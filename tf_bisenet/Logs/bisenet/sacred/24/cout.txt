INFO - bisenet-v2 - Running command 'main'
INFO - bisenet-v2 - Started run with ID "24"
INFO - root - nvidia-ml-py is not installed, automatically select gpu is disabled!
WARNING:tensorflow:From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - tensorflow - From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:88: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:88: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:100: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:100: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
INFO - root - preproces -- augment
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:108: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:108: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:218: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map__image_mirroring, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(_image_mirroring, num_parallel_calls=threads)
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:119: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:119: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:122: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:122: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:123: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:123: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:220: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map__image_scaling, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(_image_scaling, num_parallel_calls=threads)
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:148: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:148: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.

/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:223: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map_<lambda>, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  num_parallel_calls=threads)
/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:224: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map_<lambda>, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(lambda image, label: _apply_with_random_selector(image, lambda x, ordering: _distort_color
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:235: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:235: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d56c2eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d56c2eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d56c2eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d56c2eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5664208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5664208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5664208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5664208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d5664780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d5664780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d5664780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d5664780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d56644a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d56644a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d56644a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d56644a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d5664320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d5664320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d5664320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d5664320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5664ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5664ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5664ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5664ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d56644a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d56644a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d56644a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d56644a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5932438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5932438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5932438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5932438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d555d668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d555d668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d555d668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d555d668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d556c860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d556c860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d556c860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d556c860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f04d5577f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f04d5577f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f04d5577f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f04d5577f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54fb780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54fb780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54fb780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54fb780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54e7e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54e7e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54e7e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54e7e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5612e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5612e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5612e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5612e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54e7f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54e7f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54e7f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54e7f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54fac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54fac50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54fac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54fac50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54adcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54adcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54adcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54adcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5664f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5664f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5664f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5664f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d53b4048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d53b4048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d53b4048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d53b4048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5488080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5488080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5488080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5488080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54e7a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54e7a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54e7a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d54e7a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d556cbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d556cbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d556cbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d556cbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d52d04a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d52d04a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d52d04a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d52d04a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5363f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5363f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5363f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5363f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d523aeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d523aeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d523aeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d523aeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54e7a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54e7a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54e7a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54e7a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5267630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5267630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5267630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5267630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54e7f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54e7f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54e7f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d54e7f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5175eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5175eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5175eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5175eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5207eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5207eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5207eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5207eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d51758d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d51758d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d51758d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d51758d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d52d0cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d52d0cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d52d0cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d52d0cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d504fdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d504fdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d504fdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d504fdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5033940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5033940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5033940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5033940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5176438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5176438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5176438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5176438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d51fadd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d51fadd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d51fadd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d51fadd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4fd0ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4fd0ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4fd0ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4fd0ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5398358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5398358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5398358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5398358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d55fc3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d55fc3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d55fc3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d55fc3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4fd0cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4fd0cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4fd0cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4fd0cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d55fc048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d55fc048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d55fc048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d55fc048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5605908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5605908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5605908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5605908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5175dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5175dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5175dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d5175dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d51751d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d51751d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d51751d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d51751d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4e79e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4e79e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4e79e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4e79e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4f23ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4f23ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4f23ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4f23ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4d24390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4d24390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4d24390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4d24390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4caff98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4caff98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4caff98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4caff98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4df8438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4df8438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4df8438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4df8438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4d12e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4d12e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4d12e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4d12e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4fb85f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4fb85f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4fb85f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4fb85f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4c62dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4c62dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4c62dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4c62dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4b717b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4b717b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4b717b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4b717b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4b71438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4b71438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4b71438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4b71438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4b45748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4b45748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4b45748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4b45748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4b71438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4b71438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4b71438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4b71438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4abc5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4abc5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4abc5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4abc5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4df89e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4df89e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4df89e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4df89e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4a49da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4a49da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4a49da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4a49da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4abcef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4abcef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4abcef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4abcef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4a12f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4a12f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4a12f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4a12f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4b3f048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4b3f048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4b3f048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4b3f048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4919f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4919f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4919f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4919f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d49d7470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d49d7470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d49d7470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d49d7470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d492ea58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d492ea58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d492ea58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d492ea58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5932f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5932f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5932f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5932f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4bb1cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4bb1cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4bb1cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4bb1cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4a12048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4a12048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4a12048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4a12048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d47a77b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d47a77b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d47a77b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d47a77b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4900eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4900eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4900eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4900eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d478d978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d478d978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d478d978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d478d978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d47a70f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d47a70f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d47a70f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d47a70f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d482ceb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d482ceb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d482ceb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d482ceb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d48416d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d48416d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d48416d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d48416d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d45e4f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d45e4f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d45e4f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d45e4f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d478ddd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d478ddd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d478ddd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d478ddd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4ee5cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4ee5cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4ee5cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4ee5cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4abcef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4abcef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4abcef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4abcef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d46b6d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d46b6d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d46b6d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d46b6d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4e58c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4e58c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4e58c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4e58c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d452bb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d452bb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d452bb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d452bb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d458b438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d458b438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d458b438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d458b438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d442c6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d442c6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d442c6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d442c6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d45e4518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d45e4518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d45e4518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d45e4518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d442c6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d442c6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d442c6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d442c6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d461a550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d461a550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d461a550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d461a550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4320390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4320390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4320390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4320390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d43cbcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d43cbcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d43cbcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d43cbcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4406518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4406518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4406518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4406518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d450fba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d450fba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d450fba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d450fba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4274198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4274198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4274198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4274198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d41aee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d41aee48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d41aee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d41aee48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4368c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4368c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4368c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4368c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d42acc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d42acc18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d42acc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d42acc18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4294b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4294b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4294b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4294b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4276dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4276dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4276dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4276dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4841208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4841208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4841208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4841208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4841320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4841320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4841320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d4841320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406dbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406dbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406dbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406dbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d439cd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d439cd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d439cd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d439cd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406dc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406dc18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406dc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406dc18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d44062b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d44062b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d44062b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d44062b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cff4ceb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cff4ceb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cff4ceb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cff4ceb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cffce780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cffce780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cffce780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cffce780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406ed30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406ed30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406ed30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406ed30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfeed6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfeed6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfeed6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfeed6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfe21080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfe21080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfe21080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfe21080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfeedf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfeedf98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfeedf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfeedf98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfe58780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfe58780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfe58780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfe58780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfe21cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfe21cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfe21cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfe21cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfd12cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfd12cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfd12cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfd12cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfd7bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfd7bf98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfd7bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cfd7bf98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406eda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406eda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406eda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d406eda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfe58cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfe58cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfe58cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfe58cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfd40e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfd40e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfd40e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfd40e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:179: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:179: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfe07e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfe07e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfe07e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfe07e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4e7b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4e7b1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4e7b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d4e7b1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfd2c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfd2c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfd2c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfd2c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d452bb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d452bb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d452bb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04d452bb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfd40390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfd40390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfd40390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfd40390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfbb7c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfbb7c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfbb7c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfbb7c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfac2f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfac2f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfac2f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfac2f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfbb78d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfbb78d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfbb78d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfbb78d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfa177b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfa177b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfa177b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfa177b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfa17550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfa17550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfa17550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfa17550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf9f0390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf9f0390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf9f0390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf9f0390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf986ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf986ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf986ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf986ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf986ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf986ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf986ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf986ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf9867f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf9867f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf9867f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf9867f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf986630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf986630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf986630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf986630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfc31cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfc31cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfc31cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cfc31cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf978ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf978ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf978ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf978ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf8cd4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf8cd4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf8cd4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf8cd4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf9783c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf9783c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf9783c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf9783c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf8cd550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf8cd550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf8cd550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf8cd550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf8b89b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf8b89b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf8b89b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf8b89b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf8b3ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf8b3ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf8b3ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf8b3ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf7edc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf7edc18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf7edc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf7edc18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf7857b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf7857b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf7857b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf7857b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf804470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf804470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf804470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf804470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf9532e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf9532e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf9532e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf9532e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf8cd6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf8cd6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf8cd6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf8cd6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:217: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:217: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:221: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:221: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:224: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:224: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:229: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:229: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:240: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:240: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING - root - random_scale is not explicitly specified, using default value: False
WARNING - root - random_mirror is not explicitly specified, using default value: True
INFO - root - preproces -- None
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf3d6da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf3d6da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf3d6da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf3d6da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39d128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39d128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39d128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39d128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf39dba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf39dba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf39dba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf39dba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39d860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39d860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39d860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39d860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf39d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf39d828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf39d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf39d828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39deb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39deb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39deb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39deb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf39d860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf39d860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf39d860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf39d860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39df98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39df98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39df98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf39df98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf799d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf799d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf799d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf799d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3ae978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3ae978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3ae978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3ae978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f04cf3aeb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f04cf3aeb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f04cf3aeb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f04cf3aeb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf6c9d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf6c9d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf6c9d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf6c9d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3857f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3857f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3857f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3857f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf6bb0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf6bb0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf6bb0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf6bb0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3640f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3640f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3640f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3640f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5522438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5522438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5522438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04d5522438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf364630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf364630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf364630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf364630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf364630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf364630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf364630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf364630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf364da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf364da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf364da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf364da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3aab38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3aab38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3aab38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3aab38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf362fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf362fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf362fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf362fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3bf438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3bf438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3bf438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3bf438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3620f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3620f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3620f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3620f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf366c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf366c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf366c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf366c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf362828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf362828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf362828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf362828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf340630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf340630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf340630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf340630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf362780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf362780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf362780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf362780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf340748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf340748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf340748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf340748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3625f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3625f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3625f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3625f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3622e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3622e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3622e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3622e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf36eef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf36eef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf36eef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf36eef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3665f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3665f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3665f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3665f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf34fb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf34fb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf34fb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf34fb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf362780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf362780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf362780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf362780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf358da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf358da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf358da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf358da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3bfba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3bfba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3bfba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3bfba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf358c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf358c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf358c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf358c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3403c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3403c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3403c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3403c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3036a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3036a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3036a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf3036a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf358dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf358dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf358dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf358dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf317d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf317d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf317d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf317d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3bf5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3bf5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3bf5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3bf5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf317048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf317048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf317048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf317048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf34f358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf34f358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf34f358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf34f358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2c9780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2c9780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2c9780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2c9780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf325a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf325a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf325a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf325a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf333320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf333320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf333320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf333320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf325a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf325a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf325a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf325a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2c9550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2c9550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2c9550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2c9550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf333ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf333ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf333ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf333ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf340080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf340080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf340080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf340080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2c9898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2c9898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2c9898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2c9898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2d8198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2d8198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2d8198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2d8198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf340080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf340080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf340080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf340080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf290128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf290128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf290128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf290128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3174a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3174a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3174a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3174a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2e0c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2e0c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2e0c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2e0c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2d8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2d8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2d8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2d8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf29a048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf29a048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf29a048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf29a048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2e0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2e0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2e0c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2e0c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf303080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf303080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf303080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf303080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2e0fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2e0fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2e0fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2e0fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf29a3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf29a3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf29a3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf29a3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3038d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3038d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3038d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf3038d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf290d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf290d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf290d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf290d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf303080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf303080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf303080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf303080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf31c2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf31c2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf31c2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf31c2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf256438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf256438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf256438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf256438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2aa2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2aa2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2aa2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2aa2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf256d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf256d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf256d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf256d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf29ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf29ad30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf29ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf29ad30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2aa588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2aa588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2aa588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf2aa588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2a9668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2a9668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2a9668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf2a9668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf29a668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf29a668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf29a668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf29a668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf227278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf227278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf227278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf227278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf27ec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf27ec50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf27ec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf27ec50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf22d358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf22d358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf22d358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf22d358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf227278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf227278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf227278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf227278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf290400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf290400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf290400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf290400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf22d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf22d828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf22d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf22d828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf27e860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf27e860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf27e860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf27e860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf22d2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf22d2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf22d2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf22d2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1d56d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1d56d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1d56d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1d56d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf27e828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf27e828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf27e828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf27e828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1f2550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1f2550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1f2550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1f2550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c9668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c9668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c9668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c9668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1f3a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1f3a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1f3a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1f3a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1f24e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1f24e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1f24e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1f24e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1c58d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1c58d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1c58d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1c58d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c54e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c54e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c54e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c54e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1c5b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1c5b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1c5b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1c5b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c5a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c5a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c5a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c5a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf21ef28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf21ef28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf21ef28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf21ef28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c5828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c5828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c5828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c5828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf21e400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf21e400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf21e400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf21e400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c51d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c51d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c51d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c51d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf147a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf147a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf147a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf147a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf21ea20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf21ea20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf21ea20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf21ea20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1f24e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1f24e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1f24e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1f24e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf147a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf147a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf147a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf147a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1489e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1489e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1489e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1489e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c90b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c90b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c90b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1c90b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf148ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf148ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf148ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf148ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1f3550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1f3550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1f3550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1f3550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1482b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1482b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1482b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf1482b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1482b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1482b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1482b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1482b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf147a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf147a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf147a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf147a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf148400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf148400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf148400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf148400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf11fc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf11fc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf11fc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf11fc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1117f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1117f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1117f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1117f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf11f358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf11f358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf11f358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf11f358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1918d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1918d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1918d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f04cf1918d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf11fc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf11fc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf11fc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf11fc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf1f3208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf1f3208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf1f3208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf1f3208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0d97b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0d97b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0d97b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0d97b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf104e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf104e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf104e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf104e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0d97b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0d97b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0d97b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0d97b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf104a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf104a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf104a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf104a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfb857b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfb857b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfb857b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cfb857b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d458ba58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d458ba58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d458ba58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04d458ba58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf086400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf086400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf086400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf086400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf0d97b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf0d97b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf0d97b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf0d97b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf086240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf086240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf086240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf086240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf044320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf044320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf044320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf044320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0d9c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0d9c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0d9c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0d9c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf04ed30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf04ed30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf04ed30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf04ed30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf04e710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf04e710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf04e710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf04e710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf04ea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf04ea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf04ea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf04ea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf08aba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf08aba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf08aba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf08aba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf044748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf044748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf044748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf044748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf05eda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf05eda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf05eda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf05eda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf7854e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf7854e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf7854e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf7854e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf000358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf000358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf000358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf000358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf0a7390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf0a7390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf0a7390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf0a7390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0750b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0750b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0750b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf0750b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf075ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf075ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf075ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf075ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf075cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf075cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf075cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cf075cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf020128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf020128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf020128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf020128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cefe16d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cefe16d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cefe16d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04cefe16d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf0207f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf0207f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf0207f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cf0207f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cefd2a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cefd2a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cefd2a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cefd2a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cefd2b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cefd2b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cefd2b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f04cefd2b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING - tensorflow - From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING:tensorflow:From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING - tensorflow - From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING - tensorflow - From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING - tensorflow - From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

2019-11-06 19:27:07.859509: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-06 19:27:07.864201: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-11-06 19:27:07.989026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 19:27:07.989491: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558dae0bea70 executing computations on platform CUDA. Devices:
2019-11-06 19:27:07.989507: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-11-06 19:27:08.008695: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz
2019-11-06 19:27:08.009112: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558dadfecda0 executing computations on platform Host. Devices:
2019-11-06 19:27:08.009130: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-11-06 19:27:08.009350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 19:27:08.009841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-11-06 19:27:08.010042: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-06 19:27:08.011138: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-11-06 19:27:08.011839: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-11-06 19:27:08.012044: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-11-06 19:27:08.012928: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-11-06 19:27:08.013522: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-11-06 19:27:08.015476: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-11-06 19:27:08.015562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 19:27:08.016377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 19:27:08.016803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-11-06 19:27:08.016851: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-06 19:27:08.017512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-06 19:27:08.017523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-11-06 19:27:08.017529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-11-06 19:27:08.017733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 19:27:08.018207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 19:27:08.019360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6828 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
INFO - root - Restore from last checkpoint: Logs/bisenet/checkpoints/bisenet-v2/model.ckpt-45000
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from Logs/bisenet/checkpoints/bisenet-v2/model.ckpt-45000
INFO - tensorflow - Restoring parameters from Logs/bisenet/checkpoints/bisenet-v2/model.ckpt-45000
WARNING:tensorflow:From train.py:161: The name tf.train.global_step is deprecated. Please use tf.compat.v1.train.global_step instead.

WARNING - tensorflow - From train.py:161: The name tf.train.global_step is deprecated. Please use tf.compat.v1.train.global_step instead.

INFO - root - Train for 150000 steps
2019-11-06 19:27:14.179514: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
INFO - root - 2019-11-06 19:27:16.246134: step 45010, total loss = 1.89, predict loss = 0.49 (70.4 examples/sec; 0.057 sec/batch; 1h:39m:25s remains)
2019-11-06 19:27:17.520983: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
INFO - root - 2019-11-06 19:27:18.513689: step 45020, total loss = 1.78, predict loss = 0.44 (72.8 examples/sec; 0.055 sec/batch; 1h:36m:05s remains)
INFO - root - 2019-11-06 19:27:19.176089: step 45030, total loss = 1.85, predict loss = 0.50 (69.5 examples/sec; 0.058 sec/batch; 1h:40m:40s remains)
INFO - root - 2019-11-06 19:27:19.815178: step 45040, total loss = 2.95, predict loss = 0.79 (69.1 examples/sec; 0.058 sec/batch; 1h:41m:15s remains)
INFO - root - 2019-11-06 19:27:20.432047: step 45050, total loss = 2.08, predict loss = 0.52 (81.9 examples/sec; 0.049 sec/batch; 1h:25m:28s remains)
INFO - root - 2019-11-06 19:27:20.913866: step 45060, total loss = 2.51, predict loss = 0.70 (105.2 examples/sec; 0.038 sec/batch; 1h:06m:31s remains)
INFO - root - 2019-11-06 19:27:21.336971: step 45070, total loss = 3.30, predict loss = 0.99 (105.2 examples/sec; 0.038 sec/batch; 1h:06m:29s remains)
INFO - root - 2019-11-06 19:27:22.775991: step 45080, total loss = 3.17, predict loss = 0.87 (67.0 examples/sec; 0.060 sec/batch; 1h:44m:28s remains)
INFO - root - 2019-11-06 19:27:23.456511: step 45090, total loss = 3.42, predict loss = 0.97 (66.5 examples/sec; 0.060 sec/batch; 1h:45m:11s remains)
INFO - root - 2019-11-06 19:27:24.240237: step 45100, total loss = 1.36, predict loss = 0.36 (69.3 examples/sec; 0.058 sec/batch; 1h:40m:51s remains)
INFO - root - 2019-11-06 19:27:24.915052: step 45110, total loss = 2.34, predict loss = 0.53 (68.1 examples/sec; 0.059 sec/batch; 1h:42m:37s remains)
INFO - root - 2019-11-06 19:27:25.596918: step 45120, total loss = 1.79, predict loss = 0.40 (67.1 examples/sec; 0.060 sec/batch; 1h:44m:16s remains)
INFO - root - 2019-11-06 19:27:26.129275: step 45130, total loss = 2.76, predict loss = 0.75 (86.4 examples/sec; 0.046 sec/batch; 1h:20m:57s remains)
INFO - root - 2019-11-06 19:27:26.606041: step 45140, total loss = 2.07, predict loss = 0.48 (102.0 examples/sec; 0.039 sec/batch; 1h:08m:33s remains)
INFO - root - 2019-11-06 19:27:27.027082: step 45150, total loss = 2.13, predict loss = 0.49 (122.0 examples/sec; 0.033 sec/batch; 0h:57m:16s remains)
INFO - root - 2019-11-06 19:27:28.406821: step 45160, total loss = 1.92, predict loss = 0.55 (56.2 examples/sec; 0.071 sec/batch; 2h:04m:17s remains)
INFO - root - 2019-11-06 19:27:29.198443: step 45170, total loss = 2.36, predict loss = 0.62 (61.6 examples/sec; 0.065 sec/batch; 1h:53m:25s remains)
INFO - root - 2019-11-06 19:27:29.947403: step 45180, total loss = 3.22, predict loss = 0.90 (53.0 examples/sec; 0.075 sec/batch; 2h:11m:53s remains)
INFO - root - 2019-11-06 19:27:30.714127: step 45190, total loss = 2.83, predict loss = 0.75 (59.7 examples/sec; 0.067 sec/batch; 1h:57m:01s remains)
INFO - root - 2019-11-06 19:27:31.410580: step 45200, total loss = 2.72, predict loss = 0.78 (78.2 examples/sec; 0.051 sec/batch; 1h:29m:18s remains)
INFO - root - 2019-11-06 19:27:31.873308: step 45210, total loss = 2.37, predict loss = 0.62 (100.2 examples/sec; 0.040 sec/batch; 1h:09m:42s remains)
INFO - root - 2019-11-06 19:27:32.333934: step 45220, total loss = 1.64, predict loss = 0.45 (103.9 examples/sec; 0.038 sec/batch; 1h:07m:13s remains)
INFO - root - 2019-11-06 19:27:33.578042: step 45230, total loss = 1.72, predict loss = 0.39 (57.3 examples/sec; 0.070 sec/batch; 2h:01m:48s remains)
INFO - root - 2019-11-06 19:27:34.326251: step 45240, total loss = 2.47, predict loss = 0.64 (63.7 examples/sec; 0.063 sec/batch; 1h:49m:36s remains)
INFO - root - 2019-11-06 19:27:35.126443: step 45250, total loss = 2.15, predict loss = 0.54 (54.0 examples/sec; 0.074 sec/batch; 2h:09m:25s remains)
INFO - root - 2019-11-06 19:27:35.830039: step 45260, total loss = 2.78, predict loss = 0.80 (68.4 examples/sec; 0.058 sec/batch; 1h:42m:04s remains)
INFO - root - 2019-11-06 19:27:36.513570: step 45270, total loss = 1.56, predict loss = 0.34 (74.0 examples/sec; 0.054 sec/batch; 1h:34m:21s remains)
INFO - root - 2019-11-06 19:27:37.048950: step 45280, total loss = 2.23, predict loss = 0.60 (83.4 examples/sec; 0.048 sec/batch; 1h:23m:39s remains)
INFO - root - 2019-11-06 19:27:37.493072: step 45290, total loss = 1.90, predict loss = 0.50 (99.5 examples/sec; 0.040 sec/batch; 1h:10m:11s remains)
INFO - root - 2019-11-06 19:27:38.634543: step 45300, total loss = 2.30, predict loss = 0.55 (5.5 examples/sec; 0.726 sec/batch; 21h:07m:42s remains)
INFO - root - 2019-11-06 19:27:39.331683: step 45310, total loss = 2.15, predict loss = 0.62 (67.4 examples/sec; 0.059 sec/batch; 1h:43m:31s remains)
INFO - root - 2019-11-06 19:27:40.125304: step 45320, total loss = 2.39, predict loss = 0.55 (60.4 examples/sec; 0.066 sec/batch; 1h:55m:38s remains)
INFO - root - 2019-11-06 19:27:40.901710: step 45330, total loss = 2.17, predict loss = 0.50 (63.3 examples/sec; 0.063 sec/batch; 1h:50m:18s remains)
INFO - root - 2019-11-06 19:27:41.705707: step 45340, total loss = 2.70, predict loss = 0.71 (54.7 examples/sec; 0.073 sec/batch; 2h:07m:37s remains)
INFO - root - 2019-11-06 19:27:42.352097: step 45350, total loss = 3.58, predict loss = 1.09 (91.5 examples/sec; 0.044 sec/batch; 1h:16m:13s remains)
INFO - root - 2019-11-06 19:27:42.798000: step 45360, total loss = 1.77, predict loss = 0.44 (100.5 examples/sec; 0.040 sec/batch; 1h:09m:24s remains)
INFO - root - 2019-11-06 19:27:43.250985: step 45370, total loss = 2.34, predict loss = 0.58 (97.9 examples/sec; 0.041 sec/batch; 1h:11m:16s remains)
INFO - root - 2019-11-06 19:27:44.548259: step 45380, total loss = 2.49, predict loss = 0.60 (60.0 examples/sec; 0.067 sec/batch; 1h:56m:11s remains)
INFO - root - 2019-11-06 19:27:45.324245: step 45390, total loss = 2.43, predict loss = 0.68 (62.1 examples/sec; 0.064 sec/batch; 1h:52m:14s remains)
INFO - root - 2019-11-06 19:27:46.118701: step 45400, total loss = 3.55, predict loss = 1.13 (51.0 examples/sec; 0.078 sec/batch; 2h:16m:43s remains)
INFO - root - 2019-11-06 19:27:46.854887: step 45410, total loss = 1.96, predict loss = 0.46 (57.2 examples/sec; 0.070 sec/batch; 2h:01m:50s remains)
INFO - root - 2019-11-06 19:27:47.608870: step 45420, total loss = 3.50, predict loss = 0.99 (65.1 examples/sec; 0.061 sec/batch; 1h:47m:06s remains)
INFO - root - 2019-11-06 19:27:48.148599: step 45430, total loss = 1.80, predict loss = 0.45 (86.9 examples/sec; 0.046 sec/batch; 1h:20m:15s remains)
INFO - root - 2019-11-06 19:27:48.587781: step 45440, total loss = 1.62, predict loss = 0.43 (96.9 examples/sec; 0.041 sec/batch; 1h:11m:54s remains)
INFO - root - 2019-11-06 19:27:49.739234: step 45450, total loss = 2.93, predict loss = 0.82 (71.1 examples/sec; 0.056 sec/batch; 1h:38m:01s remains)
INFO - root - 2019-11-06 19:27:50.445162: step 45460, total loss = 2.36, predict loss = 0.63 (64.5 examples/sec; 0.062 sec/batch; 1h:48m:05s remains)
INFO - root - 2019-11-06 19:27:51.130690: step 45470, total loss = 1.37, predict loss = 0.34 (69.1 examples/sec; 0.058 sec/batch; 1h:40m:46s remains)
INFO - root - 2019-11-06 19:27:51.892607: step 45480, total loss = 1.87, predict loss = 0.46 (52.2 examples/sec; 0.077 sec/batch; 2h:13m:28s remains)
INFO - root - 2019-11-06 19:27:52.717158: step 45490, total loss = 2.11, predict loss = 0.54 (46.3 examples/sec; 0.086 sec/batch; 2h:30m:31s remains)
INFO - root - 2019-11-06 19:27:53.415291: step 45500, total loss = 2.35, predict loss = 0.56 (85.8 examples/sec; 0.047 sec/batch; 1h:21m:12s remains)
INFO - root - 2019-11-06 19:27:53.887866: step 45510, total loss = 2.15, predict loss = 0.55 (87.1 examples/sec; 0.046 sec/batch; 1h:19m:59s remains)
INFO - root - 2019-11-06 19:27:54.350352: step 45520, total loss = 2.18, predict loss = 0.57 (102.6 examples/sec; 0.039 sec/batch; 1h:07m:53s remains)
INFO - root - 2019-11-06 19:27:55.590848: step 45530, total loss = 2.93, predict loss = 0.84 (60.6 examples/sec; 0.066 sec/batch; 1h:54m:56s remains)
INFO - root - 2019-11-06 19:27:56.378548: step 45540, total loss = 2.16, predict loss = 0.54 (55.5 examples/sec; 0.072 sec/batch; 2h:05m:32s remains)
INFO - root - 2019-11-06 19:27:57.250413: step 45550, total loss = 2.26, predict loss = 0.60 (54.5 examples/sec; 0.073 sec/batch; 2h:07m:49s remains)
INFO - root - 2019-11-06 19:27:57.994424: step 45560, total loss = 2.11, predict loss = 0.54 (57.3 examples/sec; 0.070 sec/batch; 2h:01m:29s remains)
INFO - root - 2019-11-06 19:27:58.681253: step 45570, total loss = 1.38, predict loss = 0.35 (73.0 examples/sec; 0.055 sec/batch; 1h:35m:22s remains)
INFO - root - 2019-11-06 19:27:59.219138: step 45580, total loss = 2.54, predict loss = 0.69 (83.3 examples/sec; 0.048 sec/batch; 1h:23m:37s remains)
INFO - root - 2019-11-06 19:27:59.683667: step 45590, total loss = 3.11, predict loss = 0.85 (98.2 examples/sec; 0.041 sec/batch; 1h:10m:52s remains)
INFO - root - 2019-11-06 19:28:00.843892: step 45600, total loss = 2.26, predict loss = 0.52 (70.5 examples/sec; 0.057 sec/batch; 1h:38m:46s remains)
INFO - root - 2019-11-06 19:28:01.546584: step 45610, total loss = 2.49, predict loss = 0.63 (59.3 examples/sec; 0.067 sec/batch; 1h:57m:23s remains)
INFO - root - 2019-11-06 19:28:02.322638: step 45620, total loss = 2.07, predict loss = 0.50 (59.1 examples/sec; 0.068 sec/batch; 1h:57m:39s remains)
INFO - root - 2019-11-06 19:28:03.043070: step 45630, total loss = 3.51, predict loss = 1.02 (63.6 examples/sec; 0.063 sec/batch; 1h:49m:26s remains)
INFO - root - 2019-11-06 19:28:03.813964: step 45640, total loss = 2.21, predict loss = 0.59 (66.5 examples/sec; 0.060 sec/batch; 1h:44m:36s remains)
INFO - root - 2019-11-06 19:28:04.453640: step 45650, total loss = 2.17, predict loss = 0.60 (90.8 examples/sec; 0.044 sec/batch; 1h:16m:34s remains)
INFO - root - 2019-11-06 19:28:04.921502: step 45660, total loss = 3.08, predict loss = 0.89 (96.0 examples/sec; 0.042 sec/batch; 1h:12m:29s remains)
INFO - root - 2019-11-06 19:28:05.360243: step 45670, total loss = 3.25, predict loss = 1.05 (96.9 examples/sec; 0.041 sec/batch; 1h:11m:46s remains)
INFO - root - 2019-11-06 19:28:06.692255: step 45680, total loss = 1.80, predict loss = 0.45 (57.5 examples/sec; 0.070 sec/batch; 2h:00m:54s remains)
INFO - root - 2019-11-06 19:28:07.459307: step 45690, total loss = 1.76, predict loss = 0.45 (55.0 examples/sec; 0.073 sec/batch; 2h:06m:20s remains)
INFO - root - 2019-11-06 19:28:08.265071: step 45700, total loss = 2.66, predict loss = 0.72 (54.1 examples/sec; 0.074 sec/batch; 2h:08m:24s remains)
INFO - root - 2019-11-06 19:28:08.983033: step 45710, total loss = 1.98, predict loss = 0.56 (68.5 examples/sec; 0.058 sec/batch; 1h:41m:30s remains)
INFO - root - 2019-11-06 19:28:09.648516: step 45720, total loss = 2.11, predict loss = 0.58 (69.9 examples/sec; 0.057 sec/batch; 1h:39m:24s remains)
INFO - root - 2019-11-06 19:28:10.132004: step 45730, total loss = 1.74, predict loss = 0.40 (101.9 examples/sec; 0.039 sec/batch; 1h:08m:14s remains)
INFO - root - 2019-11-06 19:28:10.614752: step 45740, total loss = 1.67, predict loss = 0.39 (101.4 examples/sec; 0.039 sec/batch; 1h:08m:32s remains)
INFO - root - 2019-11-06 19:28:11.814927: step 45750, total loss = 2.44, predict loss = 0.61 (62.2 examples/sec; 0.064 sec/batch; 1h:51m:46s remains)
INFO - root - 2019-11-06 19:28:12.524695: step 45760, total loss = 3.12, predict loss = 0.85 (64.8 examples/sec; 0.062 sec/batch; 1h:47m:15s remains)
INFO - root - 2019-11-06 19:28:13.314329: step 45770, total loss = 2.50, predict loss = 0.68 (44.4 examples/sec; 0.090 sec/batch; 2h:36m:29s remains)
INFO - root - 2019-11-06 19:28:14.084502: step 45780, total loss = 2.78, predict loss = 0.79 (60.6 examples/sec; 0.066 sec/batch; 1h:54m:44s remains)
INFO - root - 2019-11-06 19:28:14.804196: step 45790, total loss = 2.06, predict loss = 0.51 (57.6 examples/sec; 0.069 sec/batch; 2h:00m:39s remains)
INFO - root - 2019-11-06 19:28:15.363502: step 45800, total loss = 2.31, predict loss = 0.67 (102.0 examples/sec; 0.039 sec/batch; 1h:08m:05s remains)
INFO - root - 2019-11-06 19:28:15.798956: step 45810, total loss = 2.28, predict loss = 0.60 (97.3 examples/sec; 0.041 sec/batch; 1h:11m:22s remains)
INFO - root - 2019-11-06 19:28:16.280659: step 45820, total loss = 2.49, predict loss = 0.68 (94.8 examples/sec; 0.042 sec/batch; 1h:13m:14s remains)
INFO - root - 2019-11-06 19:28:17.662603: step 45830, total loss = 1.83, predict loss = 0.44 (56.1 examples/sec; 0.071 sec/batch; 2h:03m:53s remains)
INFO - root - 2019-11-06 19:28:18.438929: step 45840, total loss = 1.91, predict loss = 0.47 (60.7 examples/sec; 0.066 sec/batch; 1h:54m:23s remains)
INFO - root - 2019-11-06 19:28:19.210365: step 45850, total loss = 2.69, predict loss = 0.78 (57.8 examples/sec; 0.069 sec/batch; 2h:00m:11s remains)
INFO - root - 2019-11-06 19:28:19.970686: step 45860, total loss = 1.99, predict loss = 0.50 (58.3 examples/sec; 0.069 sec/batch; 1h:58m:59s remains)
INFO - root - 2019-11-06 19:28:20.674183: step 45870, total loss = 2.68, predict loss = 0.87 (69.6 examples/sec; 0.057 sec/batch; 1h:39m:40s remains)
INFO - root - 2019-11-06 19:28:21.151797: step 45880, total loss = 1.30, predict loss = 0.32 (99.9 examples/sec; 0.040 sec/batch; 1h:09m:27s remains)
INFO - root - 2019-11-06 19:28:21.605088: step 45890, total loss = 2.45, predict loss = 0.68 (91.8 examples/sec; 0.044 sec/batch; 1h:15m:36s remains)
INFO - root - 2019-11-06 19:28:22.821585: step 45900, total loss = 2.75, predict loss = 0.79 (70.1 examples/sec; 0.057 sec/batch; 1h:39m:03s remains)
INFO - root - 2019-11-06 19:28:23.524856: step 45910, total loss = 2.98, predict loss = 0.80 (71.7 examples/sec; 0.056 sec/batch; 1h:36m:46s remains)
INFO - root - 2019-11-06 19:28:24.272143: step 45920, total loss = 1.47, predict loss = 0.37 (70.1 examples/sec; 0.057 sec/batch; 1h:39m:02s remains)
INFO - root - 2019-11-06 19:28:24.974006: step 45930, total loss = 1.52, predict loss = 0.39 (64.2 examples/sec; 0.062 sec/batch; 1h:48m:04s remains)
INFO - root - 2019-11-06 19:28:25.714375: step 45940, total loss = 2.25, predict loss = 0.57 (63.8 examples/sec; 0.063 sec/batch; 1h:48m:43s remains)
INFO - root - 2019-11-06 19:28:26.252867: step 45950, total loss = 2.38, predict loss = 0.61 (86.4 examples/sec; 0.046 sec/batch; 1h:20m:18s remains)
INFO - root - 2019-11-06 19:28:26.686081: step 45960, total loss = 2.41, predict loss = 0.68 (92.6 examples/sec; 0.043 sec/batch; 1h:14m:51s remains)
INFO - root - 2019-11-06 19:28:27.117954: step 45970, total loss = 1.56, predict loss = 0.41 (131.0 examples/sec; 0.031 sec/batch; 0h:52m:55s remains)
INFO - root - 2019-11-06 19:28:28.486173: step 45980, total loss = 1.92, predict loss = 0.47 (59.0 examples/sec; 0.068 sec/batch; 1h:57m:33s remains)
INFO - root - 2019-11-06 19:28:29.245094: step 45990, total loss = 1.43, predict loss = 0.31 (55.2 examples/sec; 0.072 sec/batch; 2h:05m:38s remains)
INFO - root - 2019-11-06 19:28:30.058829: step 46000, total loss = 2.69, predict loss = 0.89 (61.5 examples/sec; 0.065 sec/batch; 1h:52m:40s remains)
INFO - root - 2019-11-06 19:28:30.818128: step 46010, total loss = 2.75, predict loss = 0.76 (57.5 examples/sec; 0.070 sec/batch; 2h:00m:31s remains)
INFO - root - 2019-11-06 19:28:31.483827: step 46020, total loss = 2.35, predict loss = 0.72 (76.6 examples/sec; 0.052 sec/batch; 1h:30m:27s remains)
INFO - root - 2019-11-06 19:28:31.909181: step 46030, total loss = 1.21, predict loss = 0.31 (99.0 examples/sec; 0.040 sec/batch; 1h:10m:00s remains)
INFO - root - 2019-11-06 19:28:32.350654: step 46040, total loss = 2.05, predict loss = 0.61 (92.1 examples/sec; 0.043 sec/batch; 1h:15m:15s remains)
INFO - root - 2019-11-06 19:28:33.597095: step 46050, total loss = 3.60, predict loss = 1.06 (59.9 examples/sec; 0.067 sec/batch; 1h:55m:41s remains)
INFO - root - 2019-11-06 19:28:34.342632: step 46060, total loss = 1.97, predict loss = 0.52 (56.0 examples/sec; 0.071 sec/batch; 2h:03m:41s remains)
INFO - root - 2019-11-06 19:28:35.082030: step 46070, total loss = 2.18, predict loss = 0.55 (60.1 examples/sec; 0.067 sec/batch; 1h:55m:19s remains)
INFO - root - 2019-11-06 19:28:35.841224: step 46080, total loss = 2.89, predict loss = 0.89 (64.1 examples/sec; 0.062 sec/batch; 1h:48m:02s remains)
INFO - root - 2019-11-06 19:28:36.596786: step 46090, total loss = 1.93, predict loss = 0.51 (65.9 examples/sec; 0.061 sec/batch; 1h:45m:04s remains)
INFO - root - 2019-11-06 19:28:37.195339: step 46100, total loss = 3.06, predict loss = 0.87 (94.0 examples/sec; 0.043 sec/batch; 1h:13m:43s remains)
INFO - root - 2019-11-06 19:28:37.642466: step 46110, total loss = 2.29, predict loss = 0.60 (94.1 examples/sec; 0.042 sec/batch; 1h:13m:35s remains)
INFO - root - 2019-11-06 19:28:38.754659: step 46120, total loss = 2.44, predict loss = 0.73 (5.4 examples/sec; 0.734 sec/batch; 21h:11m:35s remains)
INFO - root - 2019-11-06 19:28:39.410516: step 46130, total loss = 2.59, predict loss = 0.76 (63.8 examples/sec; 0.063 sec/batch; 1h:48m:33s remains)
INFO - root - 2019-11-06 19:28:40.103645: step 46140, total loss = 1.52, predict loss = 0.36 (65.2 examples/sec; 0.061 sec/batch; 1h:46m:09s remains)
INFO - root - 2019-11-06 19:28:40.832943: step 46150, total loss = 2.00, predict loss = 0.50 (60.3 examples/sec; 0.066 sec/batch; 1h:54m:48s remains)
INFO - root - 2019-11-06 19:28:41.625664: step 46160, total loss = 2.11, predict loss = 0.53 (57.3 examples/sec; 0.070 sec/batch; 2h:00m:46s remains)
INFO - root - 2019-11-06 19:28:42.367707: step 46170, total loss = 2.24, predict loss = 0.65 (88.1 examples/sec; 0.045 sec/batch; 1h:18m:36s remains)
INFO - root - 2019-11-06 19:28:42.837130: step 46180, total loss = 2.14, predict loss = 0.57 (96.8 examples/sec; 0.041 sec/batch; 1h:11m:30s remains)
INFO - root - 2019-11-06 19:28:43.298841: step 46190, total loss = 2.28, predict loss = 0.61 (97.8 examples/sec; 0.041 sec/batch; 1h:10m:46s remains)
INFO - root - 2019-11-06 19:28:44.571836: step 46200, total loss = 1.47, predict loss = 0.36 (65.8 examples/sec; 0.061 sec/batch; 1h:45m:08s remains)
INFO - root - 2019-11-06 19:28:45.338699: step 46210, total loss = 2.77, predict loss = 0.79 (60.7 examples/sec; 0.066 sec/batch; 1h:53m:59s remains)
INFO - root - 2019-11-06 19:28:46.094154: step 46220, total loss = 2.18, predict loss = 0.55 (59.4 examples/sec; 0.067 sec/batch; 1h:56m:32s remains)
INFO - root - 2019-11-06 19:28:46.819867: step 46230, total loss = 2.50, predict loss = 0.68 (64.0 examples/sec; 0.063 sec/batch; 1h:48m:09s remains)
INFO - root - 2019-11-06 19:28:47.498478: step 46240, total loss = 2.09, predict loss = 0.57 (81.3 examples/sec; 0.049 sec/batch; 1h:25m:05s remains)
INFO - root - 2019-11-06 19:28:47.977515: step 46250, total loss = 1.82, predict loss = 0.47 (100.4 examples/sec; 0.040 sec/batch; 1h:08m:54s remains)
INFO - root - 2019-11-06 19:28:48.455088: step 46260, total loss = 2.45, predict loss = 0.72 (98.3 examples/sec; 0.041 sec/batch; 1h:10m:23s remains)
INFO - root - 2019-11-06 19:28:49.610987: step 46270, total loss = 2.58, predict loss = 0.77 (70.2 examples/sec; 0.057 sec/batch; 1h:38m:26s remains)
INFO - root - 2019-11-06 19:28:50.318710: step 46280, total loss = 1.33, predict loss = 0.34 (52.1 examples/sec; 0.077 sec/batch; 2h:12m:41s remains)
INFO - root - 2019-11-06 19:28:51.150860: step 46290, total loss = 1.14, predict loss = 0.31 (53.2 examples/sec; 0.075 sec/batch; 2h:10m:01s remains)
INFO - root - 2019-11-06 19:28:51.907184: step 46300, total loss = 2.58, predict loss = 0.72 (69.1 examples/sec; 0.058 sec/batch; 1h:40m:05s remains)
INFO - root - 2019-11-06 19:28:52.621858: step 46310, total loss = 1.71, predict loss = 0.46 (60.5 examples/sec; 0.066 sec/batch; 1h:54m:15s remains)
INFO - root - 2019-11-06 19:28:53.246534: step 46320, total loss = 2.01, predict loss = 0.53 (100.3 examples/sec; 0.040 sec/batch; 1h:08m:56s remains)
INFO - root - 2019-11-06 19:28:53.689715: step 46330, total loss = 1.54, predict loss = 0.40 (95.7 examples/sec; 0.042 sec/batch; 1h:12m:11s remains)
INFO - root - 2019-11-06 19:28:54.214168: step 46340, total loss = 2.28, predict loss = 0.71 (86.1 examples/sec; 0.046 sec/batch; 1h:20m:13s remains)
INFO - root - 2019-11-06 19:28:55.458161: step 46350, total loss = 1.99, predict loss = 0.54 (61.0 examples/sec; 0.066 sec/batch; 1h:53m:12s remains)
INFO - root - 2019-11-06 19:28:56.164264: step 46360, total loss = 2.78, predict loss = 0.87 (66.9 examples/sec; 0.060 sec/batch; 1h:43m:12s remains)
INFO - root - 2019-11-06 19:28:56.873897: step 46370, total loss = 1.76, predict loss = 0.50 (61.2 examples/sec; 0.065 sec/batch; 1h:52m:52s remains)
INFO - root - 2019-11-06 19:28:57.708183: step 46380, total loss = 1.50, predict loss = 0.39 (48.2 examples/sec; 0.083 sec/batch; 2h:23m:10s remains)
INFO - root - 2019-11-06 19:28:58.422940: step 46390, total loss = 1.73, predict loss = 0.43 (74.1 examples/sec; 0.054 sec/batch; 1h:33m:10s remains)
INFO - root - 2019-11-06 19:28:58.916761: step 46400, total loss = 2.20, predict loss = 0.61 (99.7 examples/sec; 0.040 sec/batch; 1h:09m:15s remains)
INFO - root - 2019-11-06 19:28:59.368891: step 46410, total loss = 2.29, predict loss = 0.62 (93.1 examples/sec; 0.043 sec/batch; 1h:14m:12s remains)
INFO - root - 2019-11-06 19:29:00.556215: step 46420, total loss = 1.08, predict loss = 0.26 (65.7 examples/sec; 0.061 sec/batch; 1h:45m:06s remains)
INFO - root - 2019-11-06 19:29:01.242656: step 46430, total loss = 2.38, predict loss = 0.67 (58.0 examples/sec; 0.069 sec/batch; 1h:58m:58s remains)
INFO - root - 2019-11-06 19:29:01.946259: step 46440, total loss = 2.65, predict loss = 0.81 (62.3 examples/sec; 0.064 sec/batch; 1h:50m:46s remains)
INFO - root - 2019-11-06 19:29:02.663592: step 46450, total loss = 1.37, predict loss = 0.38 (60.2 examples/sec; 0.066 sec/batch; 1h:54m:45s remains)
INFO - root - 2019-11-06 19:29:03.338212: step 46460, total loss = 1.28, predict loss = 0.34 (69.2 examples/sec; 0.058 sec/batch; 1h:39m:45s remains)
INFO - root - 2019-11-06 19:29:03.891452: step 46470, total loss = 3.50, predict loss = 1.11 (100.3 examples/sec; 0.040 sec/batch; 1h:08m:47s remains)
INFO - root - 2019-11-06 19:29:04.333232: step 46480, total loss = 2.25, predict loss = 0.64 (98.1 examples/sec; 0.041 sec/batch; 1h:10m:22s remains)
INFO - root - 2019-11-06 19:29:04.767387: step 46490, total loss = 1.76, predict loss = 0.43 (97.3 examples/sec; 0.041 sec/batch; 1h:10m:53s remains)
INFO - root - 2019-11-06 19:29:06.063238: step 46500, total loss = 1.88, predict loss = 0.50 (65.5 examples/sec; 0.061 sec/batch; 1h:45m:19s remains)
INFO - root - 2019-11-06 19:29:06.818371: step 46510, total loss = 1.05, predict loss = 0.25 (63.8 examples/sec; 0.063 sec/batch; 1h:48m:03s remains)
INFO - root - 2019-11-06 19:29:07.567039: step 46520, total loss = 2.16, predict loss = 0.60 (58.3 examples/sec; 0.069 sec/batch; 1h:58m:15s remains)
INFO - root - 2019-11-06 19:29:08.318160: step 46530, total loss = 1.87, predict loss = 0.46 (59.1 examples/sec; 0.068 sec/batch; 1h:56m:48s remains)
INFO - root - 2019-11-06 19:29:09.048458: step 46540, total loss = 1.96, predict loss = 0.52 (67.4 examples/sec; 0.059 sec/batch; 1h:42m:24s remains)
INFO - root - 2019-11-06 19:29:09.568064: step 46550, total loss = 1.22, predict loss = 0.33 (87.8 examples/sec; 0.046 sec/batch; 1h:18m:30s remains)
INFO - root - 2019-11-06 19:29:10.013002: step 46560, total loss = 2.64, predict loss = 0.77 (100.5 examples/sec; 0.040 sec/batch; 1h:08m:37s remains)
INFO - root - 2019-11-06 19:29:11.173023: step 46570, total loss = 2.33, predict loss = 0.69 (69.3 examples/sec; 0.058 sec/batch; 1h:39m:28s remains)
INFO - root - 2019-11-06 19:29:11.903027: step 46580, total loss = 1.04, predict loss = 0.27 (66.2 examples/sec; 0.060 sec/batch; 1h:44m:11s remains)
INFO - root - 2019-11-06 19:29:12.596987: step 46590, total loss = 1.38, predict loss = 0.37 (55.2 examples/sec; 0.072 sec/batch; 2h:04m:52s remains)
INFO - root - 2019-11-06 19:29:13.320136: step 46600, total loss = 2.04, predict loss = 0.54 (65.3 examples/sec; 0.061 sec/batch; 1h:45m:29s remains)
INFO - root - 2019-11-06 19:29:13.992527: step 46610, total loss = 1.86, predict loss = 0.50 (65.6 examples/sec; 0.061 sec/batch; 1h:45m:08s remains)
INFO - root - 2019-11-06 19:29:14.557098: step 46620, total loss = 1.40, predict loss = 0.35 (103.9 examples/sec; 0.038 sec/batch; 1h:06m:19s remains)
INFO - root - 2019-11-06 19:29:14.993165: step 46630, total loss = 0.93, predict loss = 0.28 (96.6 examples/sec; 0.041 sec/batch; 1h:11m:19s remains)
INFO - root - 2019-11-06 19:29:15.428174: step 46640, total loss = 1.20, predict loss = 0.31 (89.9 examples/sec; 0.044 sec/batch; 1h:16m:38s remains)
INFO - root - 2019-11-06 19:29:16.746283: step 46650, total loss = 1.09, predict loss = 0.30 (61.1 examples/sec; 0.065 sec/batch; 1h:52m:44s remains)
INFO - root - 2019-11-06 19:29:17.507222: step 46660, total loss = 1.59, predict loss = 0.39 (58.5 examples/sec; 0.068 sec/batch; 1h:57m:50s remains)
INFO - root - 2019-11-06 19:29:18.230668: step 46670, total loss = 2.34, predict loss = 0.66 (66.9 examples/sec; 0.060 sec/batch; 1h:43m:00s remains)
INFO - root - 2019-11-06 19:29:18.955729: step 46680, total loss = 2.16, predict loss = 0.60 (61.7 examples/sec; 0.065 sec/batch; 1h:51m:43s remains)
INFO - root - 2019-11-06 19:29:19.687041: step 46690, total loss = 2.62, predict loss = 0.79 (72.9 examples/sec; 0.055 sec/batch; 1h:34m:27s remains)
INFO - root - 2019-11-06 19:29:20.185776: step 46700, total loss = 2.14, predict loss = 0.54 (98.6 examples/sec; 0.041 sec/batch; 1h:09m:50s remains)
INFO - root - 2019-11-06 19:29:20.633574: step 46710, total loss = 1.48, predict loss = 0.35 (100.3 examples/sec; 0.040 sec/batch; 1h:08m:37s remains)
INFO - root - 2019-11-06 19:29:21.810277: step 46720, total loss = 1.58, predict loss = 0.40 (69.7 examples/sec; 0.057 sec/batch; 1h:38m:45s remains)
INFO - root - 2019-11-06 19:29:22.631806: step 46730, total loss = 2.75, predict loss = 0.80 (52.2 examples/sec; 0.077 sec/batch; 2h:11m:58s remains)
INFO - root - 2019-11-06 19:29:23.425398: step 46740, total loss = 2.11, predict loss = 0.51 (52.4 examples/sec; 0.076 sec/batch; 2h:11m:15s remains)
INFO - root - 2019-11-06 19:29:24.246901: step 46750, total loss = 2.18, predict loss = 0.60 (59.1 examples/sec; 0.068 sec/batch; 1h:56m:22s remains)
INFO - root - 2019-11-06 19:29:24.963622: step 46760, total loss = 1.78, predict loss = 0.46 (58.6 examples/sec; 0.068 sec/batch; 1h:57m:28s remains)
INFO - root - 2019-11-06 19:29:25.523035: step 46770, total loss = 2.05, predict loss = 0.58 (101.1 examples/sec; 0.040 sec/batch; 1h:08m:04s remains)
INFO - root - 2019-11-06 19:29:25.990184: step 46780, total loss = 2.21, predict loss = 0.67 (99.0 examples/sec; 0.040 sec/batch; 1h:09m:28s remains)
INFO - root - 2019-11-06 19:29:26.411422: step 46790, total loss = 3.71, predict loss = 1.33 (112.7 examples/sec; 0.035 sec/batch; 1h:01m:01s remains)
INFO - root - 2019-11-06 19:29:27.775122: step 46800, total loss = 2.40, predict loss = 0.75 (63.6 examples/sec; 0.063 sec/batch; 1h:48m:07s remains)
INFO - root - 2019-11-06 19:29:28.557995: step 46810, total loss = 2.23, predict loss = 0.63 (56.0 examples/sec; 0.071 sec/batch; 2h:02m:48s remains)
INFO - root - 2019-11-06 19:29:29.319604: step 46820, total loss = 1.45, predict loss = 0.39 (49.1 examples/sec; 0.082 sec/batch; 2h:20m:10s remains)
INFO - root - 2019-11-06 19:29:30.079777: step 46830, total loss = 1.51, predict loss = 0.40 (53.4 examples/sec; 0.075 sec/batch; 2h:08m:51s remains)
INFO - root - 2019-11-06 19:29:30.741323: step 46840, total loss = 2.32, predict loss = 0.65 (80.7 examples/sec; 0.050 sec/batch; 1h:25m:11s remains)
INFO - root - 2019-11-06 19:29:31.182883: step 46850, total loss = 2.61, predict loss = 0.76 (101.2 examples/sec; 0.040 sec/batch; 1h:07m:56s remains)
INFO - root - 2019-11-06 19:29:31.643197: step 46860, total loss = 1.69, predict loss = 0.45 (102.6 examples/sec; 0.039 sec/batch; 1h:07m:02s remains)
INFO - root - 2019-11-06 19:29:32.858196: step 46870, total loss = 2.40, predict loss = 0.73 (59.4 examples/sec; 0.067 sec/batch; 1h:55m:45s remains)
INFO - root - 2019-11-06 19:29:33.638234: step 46880, total loss = 1.63, predict loss = 0.46 (57.2 examples/sec; 0.070 sec/batch; 2h:00m:06s remains)
INFO - root - 2019-11-06 19:29:34.337381: step 46890, total loss = 1.74, predict loss = 0.44 (64.7 examples/sec; 0.062 sec/batch; 1h:46m:14s remains)
INFO - root - 2019-11-06 19:29:35.076613: step 46900, total loss = 2.17, predict loss = 0.64 (64.0 examples/sec; 0.063 sec/batch; 1h:47m:28s remains)
INFO - root - 2019-11-06 19:29:35.791859: step 46910, total loss = 1.79, predict loss = 0.49 (75.3 examples/sec; 0.053 sec/batch; 1h:31m:12s remains)
INFO - root - 2019-11-06 19:29:36.279726: step 46920, total loss = 2.48, predict loss = 0.67 (102.9 examples/sec; 0.039 sec/batch; 1h:06m:48s remains)
INFO - root - 2019-11-06 19:29:36.718958: step 46930, total loss = 1.50, predict loss = 0.38 (108.6 examples/sec; 0.037 sec/batch; 1h:03m:15s remains)
INFO - root - 2019-11-06 19:29:37.861360: step 46940, total loss = 1.70, predict loss = 0.45 (5.5 examples/sec; 0.733 sec/batch; 20h:58m:45s remains)
INFO - root - 2019-11-06 19:29:38.534221: step 46950, total loss = 1.98, predict loss = 0.54 (63.5 examples/sec; 0.063 sec/batch; 1h:48m:08s remains)
INFO - root - 2019-11-06 19:29:39.273872: step 46960, total loss = 2.68, predict loss = 0.84 (60.4 examples/sec; 0.066 sec/batch; 1h:53m:49s remains)
INFO - root - 2019-11-06 19:29:39.996823: step 46970, total loss = 1.64, predict loss = 0.54 (63.4 examples/sec; 0.063 sec/batch; 1h:48m:19s remains)
INFO - root - 2019-11-06 19:29:40.781714: step 46980, total loss = 2.79, predict loss = 0.86 (57.3 examples/sec; 0.070 sec/batch; 1h:59m:53s remains)
INFO - root - 2019-11-06 19:29:41.470435: step 46990, total loss = 1.83, predict loss = 0.47 (87.0 examples/sec; 0.046 sec/batch; 1h:18m:57s remains)
INFO - root - 2019-11-06 19:29:41.905943: step 47000, total loss = 1.60, predict loss = 0.39 (95.9 examples/sec; 0.042 sec/batch; 1h:11m:38s remains)
INFO - root - 2019-11-06 19:29:42.356384: step 47010, total loss = 0.96, predict loss = 0.27 (99.0 examples/sec; 0.040 sec/batch; 1h:09m:20s remains)
INFO - root - 2019-11-06 19:29:43.596137: step 47020, total loss = 2.12, predict loss = 0.55 (65.1 examples/sec; 0.061 sec/batch; 1h:45m:23s remains)
INFO - root - 2019-11-06 19:29:44.279350: step 47030, total loss = 1.13, predict loss = 0.30 (69.1 examples/sec; 0.058 sec/batch; 1h:39m:20s remains)
INFO - root - 2019-11-06 19:29:45.012410: step 47040, total loss = 1.55, predict loss = 0.39 (58.6 examples/sec; 0.068 sec/batch; 1h:57m:07s remains)
INFO - root - 2019-11-06 19:29:45.741493: step 47050, total loss = 1.70, predict loss = 0.47 (63.0 examples/sec; 0.064 sec/batch; 1h:49m:00s remains)
INFO - root - 2019-11-06 19:29:46.473481: step 47060, total loss = 1.48, predict loss = 0.41 (67.9 examples/sec; 0.059 sec/batch; 1h:41m:03s remains)
INFO - root - 2019-11-06 19:29:46.964570: step 47070, total loss = 1.12, predict loss = 0.30 (100.7 examples/sec; 0.040 sec/batch; 1h:08m:07s remains)
INFO - root - 2019-11-06 19:29:47.403837: step 47080, total loss = 2.46, predict loss = 0.66 (94.9 examples/sec; 0.042 sec/batch; 1h:12m:16s remains)
INFO - root - 2019-11-06 19:29:48.577168: step 47090, total loss = 1.45, predict loss = 0.34 (65.3 examples/sec; 0.061 sec/batch; 1h:45m:00s remains)
INFO - root - 2019-11-06 19:29:49.316718: step 47100, total loss = 2.32, predict loss = 0.67 (60.7 examples/sec; 0.066 sec/batch; 1h:52m:55s remains)
INFO - root - 2019-11-06 19:29:50.025738: step 47110, total loss = 2.12, predict loss = 0.60 (67.4 examples/sec; 0.059 sec/batch; 1h:41m:42s remains)
INFO - root - 2019-11-06 19:29:50.751926: step 47120, total loss = 2.20, predict loss = 0.62 (59.5 examples/sec; 0.067 sec/batch; 1h:55m:12s remains)
INFO - root - 2019-11-06 19:29:51.525232: step 47130, total loss = 2.41, predict loss = 0.73 (59.0 examples/sec; 0.068 sec/batch; 1h:56m:19s remains)
INFO - root - 2019-11-06 19:29:52.172751: step 47140, total loss = 1.55, predict loss = 0.43 (89.1 examples/sec; 0.045 sec/batch; 1h:16m:59s remains)
INFO - root - 2019-11-06 19:29:52.611508: step 47150, total loss = 2.10, predict loss = 0.65 (94.8 examples/sec; 0.042 sec/batch; 1h:12m:20s remains)
INFO - root - 2019-11-06 19:29:53.054780: step 47160, total loss = 1.33, predict loss = 0.35 (93.9 examples/sec; 0.043 sec/batch; 1h:12m:58s remains)
INFO - root - 2019-11-06 19:29:54.512037: step 47170, total loss = 1.28, predict loss = 0.33 (69.9 examples/sec; 0.057 sec/batch; 1h:38m:02s remains)
INFO - root - 2019-11-06 19:29:55.204893: step 47180, total loss = 1.98, predict loss = 0.55 (67.0 examples/sec; 0.060 sec/batch; 1h:42m:20s remains)
INFO - root - 2019-11-06 19:29:55.924657: step 47190, total loss = 1.14, predict loss = 0.30 (61.5 examples/sec; 0.065 sec/batch; 1h:51m:24s remains)
INFO - root - 2019-11-06 19:29:56.679980: step 47200, total loss = 1.07, predict loss = 0.28 (53.0 examples/sec; 0.076 sec/batch; 2h:09m:24s remains)
INFO - root - 2019-11-06 19:29:57.423919: step 47210, total loss = 2.10, predict loss = 0.55 (64.6 examples/sec; 0.062 sec/batch; 1h:46m:03s remains)
INFO - root - 2019-11-06 19:29:57.972218: step 47220, total loss = 2.09, predict loss = 0.64 (99.0 examples/sec; 0.040 sec/batch; 1h:09m:14s remains)
INFO - root - 2019-11-06 19:29:58.398841: step 47230, total loss = 2.06, predict loss = 0.58 (106.6 examples/sec; 0.038 sec/batch; 1h:04m:16s remains)
INFO - root - 2019-11-06 19:29:59.551888: step 47240, total loss = 1.51, predict loss = 0.37 (69.4 examples/sec; 0.058 sec/batch; 1h:38m:43s remains)
INFO - root - 2019-11-06 19:30:00.267944: step 47250, total loss = 2.14, predict loss = 0.62 (64.1 examples/sec; 0.062 sec/batch; 1h:46m:51s remains)
INFO - root - 2019-11-06 19:30:01.008402: step 47260, total loss = 1.33, predict loss = 0.34 (65.1 examples/sec; 0.061 sec/batch; 1h:45m:17s remains)
INFO - root - 2019-11-06 19:30:01.769894: step 47270, total loss = 0.97, predict loss = 0.29 (55.7 examples/sec; 0.072 sec/batch; 2h:02m:59s remains)
INFO - root - 2019-11-06 19:30:02.505304: step 47280, total loss = 1.28, predict loss = 0.35 (60.2 examples/sec; 0.066 sec/batch; 1h:53m:49s remains)
INFO - root - 2019-11-06 19:30:03.104789: step 47290, total loss = 1.53, predict loss = 0.41 (105.7 examples/sec; 0.038 sec/batch; 1h:04m:46s remains)
INFO - root - 2019-11-06 19:30:03.576722: step 47300, total loss = 1.11, predict loss = 0.26 (103.7 examples/sec; 0.039 sec/batch; 1h:06m:02s remains)
INFO - root - 2019-11-06 19:30:04.013274: step 47310, total loss = 1.47, predict loss = 0.44 (96.6 examples/sec; 0.041 sec/batch; 1h:10m:53s remains)
INFO - root - 2019-11-06 19:30:05.321890: step 47320, total loss = 2.00, predict loss = 0.52 (56.3 examples/sec; 0.071 sec/batch; 2h:01m:41s remains)
INFO - root - 2019-11-06 19:30:06.037173: step 47330, total loss = 1.18, predict loss = 0.30 (61.2 examples/sec; 0.065 sec/batch; 1h:51m:53s remains)
INFO - root - 2019-11-06 19:30:06.803269: step 47340, total loss = 2.53, predict loss = 0.68 (58.4 examples/sec; 0.068 sec/batch; 1h:57m:06s remains)
INFO - root - 2019-11-06 19:30:07.588682: step 47350, total loss = 1.56, predict loss = 0.43 (55.8 examples/sec; 0.072 sec/batch; 2h:02m:37s remains)
INFO - root - 2019-11-06 19:30:08.318495: step 47360, total loss = 2.66, predict loss = 0.87 (66.1 examples/sec; 0.060 sec/batch; 1h:43m:27s remains)
INFO - root - 2019-11-06 19:30:08.835374: step 47370, total loss = 1.67, predict loss = 0.40 (92.4 examples/sec; 0.043 sec/batch; 1h:14m:04s remains)
INFO - root - 2019-11-06 19:30:09.324871: step 47380, total loss = 2.07, predict loss = 0.53 (97.6 examples/sec; 0.041 sec/batch; 1h:10m:06s remains)
INFO - root - 2019-11-06 19:30:10.508200: step 47390, total loss = 1.07, predict loss = 0.26 (62.4 examples/sec; 0.064 sec/batch; 1h:49m:40s remains)
INFO - root - 2019-11-06 19:30:11.204442: step 47400, total loss = 1.38, predict loss = 0.37 (62.0 examples/sec; 0.065 sec/batch; 1h:50m:19s remains)
INFO - root - 2019-11-06 19:30:11.976932: step 47410, total loss = 1.54, predict loss = 0.38 (57.6 examples/sec; 0.069 sec/batch; 1h:58m:44s remains)
INFO - root - 2019-11-06 19:30:12.740234: step 47420, total loss = 1.93, predict loss = 0.53 (60.8 examples/sec; 0.066 sec/batch; 1h:52m:26s remains)
INFO - root - 2019-11-06 19:30:13.496790: step 47430, total loss = 1.72, predict loss = 0.46 (55.3 examples/sec; 0.072 sec/batch; 2h:03m:38s remains)
INFO - root - 2019-11-06 19:30:14.089184: step 47440, total loss = 2.34, predict loss = 0.66 (99.6 examples/sec; 0.040 sec/batch; 1h:08m:40s remains)
INFO - root - 2019-11-06 19:30:14.529380: step 47450, total loss = 2.08, predict loss = 0.61 (97.9 examples/sec; 0.041 sec/batch; 1h:09m:49s remains)
INFO - root - 2019-11-06 19:30:14.998215: step 47460, total loss = 1.02, predict loss = 0.30 (100.3 examples/sec; 0.040 sec/batch; 1h:08m:09s remains)
INFO - root - 2019-11-06 19:30:16.313683: step 47470, total loss = 1.46, predict loss = 0.39 (60.2 examples/sec; 0.066 sec/batch; 1h:53m:33s remains)
INFO - root - 2019-11-06 19:30:17.137934: step 47480, total loss = 1.57, predict loss = 0.47 (51.0 examples/sec; 0.078 sec/batch; 2h:14m:00s remains)
INFO - root - 2019-11-06 19:30:17.947734: step 47490, total loss = 0.85, predict loss = 0.24 (52.9 examples/sec; 0.076 sec/batch; 2h:09m:11s remains)
INFO - root - 2019-11-06 19:30:18.702600: step 47500, total loss = 1.90, predict loss = 0.53 (59.8 examples/sec; 0.067 sec/batch; 1h:54m:18s remains)
INFO - root - 2019-11-06 19:30:19.381296: step 47510, total loss = 0.94, predict loss = 0.24 (79.8 examples/sec; 0.050 sec/batch; 1h:25m:35s remains)
INFO - root - 2019-11-06 19:30:19.827777: step 47520, total loss = 1.94, predict loss = 0.57 (97.6 examples/sec; 0.041 sec/batch; 1h:09m:59s remains)
INFO - root - 2019-11-06 19:30:20.267346: step 47530, total loss = 1.62, predict loss = 0.43 (94.2 examples/sec; 0.042 sec/batch; 1h:12m:30s remains)
INFO - root - 2019-11-06 19:30:21.503899: step 47540, total loss = 1.74, predict loss = 0.45 (65.1 examples/sec; 0.061 sec/batch; 1h:44m:57s remains)
INFO - root - 2019-11-06 19:30:22.238029: step 47550, total loss = 1.11, predict loss = 0.28 (65.7 examples/sec; 0.061 sec/batch; 1h:43m:58s remains)
INFO - root - 2019-11-06 19:30:22.955377: step 47560, total loss = 2.14, predict loss = 0.59 (71.4 examples/sec; 0.056 sec/batch; 1h:35m:40s remains)
INFO - root - 2019-11-06 19:30:23.634816: step 47570, total loss = 1.73, predict loss = 0.43 (64.7 examples/sec; 0.062 sec/batch; 1h:45m:30s remains)
INFO - root - 2019-11-06 19:30:24.459750: step 47580, total loss = 1.31, predict loss = 0.31 (58.2 examples/sec; 0.069 sec/batch; 1h:57m:13s remains)
INFO - root - 2019-11-06 19:30:25.037437: step 47590, total loss = 2.10, predict loss = 0.62 (97.2 examples/sec; 0.041 sec/batch; 1h:10m:12s remains)
INFO - root - 2019-11-06 19:30:25.483267: step 47600, total loss = 2.38, predict loss = 0.74 (100.7 examples/sec; 0.040 sec/batch; 1h:07m:48s remains)
INFO - root - 2019-11-06 19:30:25.916362: step 47610, total loss = 1.45, predict loss = 0.34 (122.9 examples/sec; 0.033 sec/batch; 0h:55m:33s remains)
INFO - root - 2019-11-06 19:30:27.264629: step 47620, total loss = 1.40, predict loss = 0.36 (54.3 examples/sec; 0.074 sec/batch; 2h:05m:39s remains)
INFO - root - 2019-11-06 19:30:28.020198: step 47630, total loss = 2.39, predict loss = 0.73 (59.4 examples/sec; 0.067 sec/batch; 1h:54m:49s remains)
INFO - root - 2019-11-06 19:30:28.720849: step 47640, total loss = 1.84, predict loss = 0.59 (68.6 examples/sec; 0.058 sec/batch; 1h:39m:26s remains)
INFO - root - 2019-11-06 19:30:29.383727: step 47650, total loss = 1.57, predict loss = 0.41 (71.8 examples/sec; 0.056 sec/batch; 1h:35m:02s remains)
INFO - root - 2019-11-06 19:30:30.037504: step 47660, total loss = 1.81, predict loss = 0.47 (75.9 examples/sec; 0.053 sec/batch; 1h:29m:53s remains)
INFO - root - 2019-11-06 19:30:30.483222: step 47670, total loss = 1.79, predict loss = 0.48 (97.1 examples/sec; 0.041 sec/batch; 1h:10m:13s remains)
INFO - root - 2019-11-06 19:30:30.930315: step 47680, total loss = 2.08, predict loss = 0.50 (89.8 examples/sec; 0.045 sec/batch; 1h:15m:57s remains)
INFO - root - 2019-11-06 19:30:32.153336: step 47690, total loss = 2.01, predict loss = 0.58 (65.1 examples/sec; 0.061 sec/batch; 1h:44m:41s remains)
INFO - root - 2019-11-06 19:30:32.889375: step 47700, total loss = 1.66, predict loss = 0.44 (56.8 examples/sec; 0.070 sec/batch; 2h:00m:00s remains)
INFO - root - 2019-11-06 19:30:33.632284: step 47710, total loss = 1.19, predict loss = 0.32 (58.1 examples/sec; 0.069 sec/batch; 1h:57m:26s remains)
INFO - root - 2019-11-06 19:30:34.386047: step 47720, total loss = 1.16, predict loss = 0.32 (57.7 examples/sec; 0.069 sec/batch; 1h:58m:16s remains)
INFO - root - 2019-11-06 19:30:35.153220: step 47730, total loss = 1.24, predict loss = 0.33 (58.9 examples/sec; 0.068 sec/batch; 1h:55m:47s remains)
INFO - root - 2019-11-06 19:30:35.725169: step 47740, total loss = 2.54, predict loss = 0.79 (99.4 examples/sec; 0.040 sec/batch; 1h:08m:36s remains)
INFO - root - 2019-11-06 19:30:36.170237: step 47750, total loss = 0.98, predict loss = 0.27 (100.6 examples/sec; 0.040 sec/batch; 1h:07m:45s remains)
INFO - root - 2019-11-06 19:30:37.293909: step 47760, total loss = 1.02, predict loss = 0.26 (5.5 examples/sec; 0.728 sec/batch; 20h:40m:08s remains)
INFO - root - 2019-11-06 19:30:37.995216: step 47770, total loss = 1.81, predict loss = 0.54 (63.5 examples/sec; 0.063 sec/batch; 1h:47m:20s remains)
INFO - root - 2019-11-06 19:30:38.785797: step 47780, total loss = 1.70, predict loss = 0.48 (53.5 examples/sec; 0.075 sec/batch; 2h:07m:22s remains)
INFO - root - 2019-11-06 19:30:39.617976: step 47790, total loss = 1.40, predict loss = 0.34 (66.0 examples/sec; 0.061 sec/batch; 1h:43m:12s remains)
INFO - root - 2019-11-06 19:30:40.351184: step 47800, total loss = 2.14, predict loss = 0.62 (71.9 examples/sec; 0.056 sec/batch; 1h:34m:44s remains)
INFO - root - 2019-11-06 19:30:40.939209: step 47810, total loss = 2.65, predict loss = 0.74 (97.4 examples/sec; 0.041 sec/batch; 1h:09m:57s remains)
INFO - root - 2019-11-06 19:30:41.398360: step 47820, total loss = 1.40, predict loss = 0.40 (104.8 examples/sec; 0.038 sec/batch; 1h:05m:00s remains)
INFO - root - 2019-11-06 19:30:41.850907: step 47830, total loss = 1.83, predict loss = 0.51 (90.2 examples/sec; 0.044 sec/batch; 1h:15m:30s remains)
INFO - root - 2019-11-06 19:30:43.084112: step 47840, total loss = 1.31, predict loss = 0.42 (58.6 examples/sec; 0.068 sec/batch; 1h:56m:12s remains)
INFO - root - 2019-11-06 19:30:43.782317: step 47850, total loss = 1.20, predict loss = 0.37 (59.9 examples/sec; 0.067 sec/batch; 1h:53m:36s remains)
INFO - root - 2019-11-06 19:30:44.497387: step 47860, total loss = 1.88, predict loss = 0.53 (66.9 examples/sec; 0.060 sec/batch; 1h:41m:43s remains)
INFO - root - 2019-11-06 19:30:45.222278: step 47870, total loss = 2.20, predict loss = 0.65 (55.5 examples/sec; 0.072 sec/batch; 2h:02m:39s remains)
INFO - root - 2019-11-06 19:30:45.933724: step 47880, total loss = 0.79, predict loss = 0.22 (68.5 examples/sec; 0.058 sec/batch; 1h:39m:25s remains)
INFO - root - 2019-11-06 19:30:46.453680: step 47890, total loss = 2.78, predict loss = 0.91 (105.2 examples/sec; 0.038 sec/batch; 1h:04m:42s remains)
INFO - root - 2019-11-06 19:30:46.934631: step 47900, total loss = 1.55, predict loss = 0.42 (94.3 examples/sec; 0.042 sec/batch; 1h:12m:09s remains)
INFO - root - 2019-11-06 19:30:48.096872: step 47910, total loss = 2.30, predict loss = 0.68 (67.5 examples/sec; 0.059 sec/batch; 1h:40m:52s remains)
INFO - root - 2019-11-06 19:30:48.796234: step 47920, total loss = 1.58, predict loss = 0.45 (52.7 examples/sec; 0.076 sec/batch; 2h:09m:04s remains)
INFO - root - 2019-11-06 19:30:49.530706: step 47930, total loss = 0.93, predict loss = 0.27 (62.8 examples/sec; 0.064 sec/batch; 1h:48m:26s remains)
INFO - root - 2019-11-06 19:30:50.332883: step 47940, total loss = 1.27, predict loss = 0.33 (55.5 examples/sec; 0.072 sec/batch; 2h:02m:35s remains)
INFO - root - 2019-11-06 19:30:51.127292: step 47950, total loss = 2.10, predict loss = 0.64 (55.1 examples/sec; 0.073 sec/batch; 2h:03m:23s remains)
INFO - root - 2019-11-06 19:30:51.786853: step 47960, total loss = 1.98, predict loss = 0.50 (88.5 examples/sec; 0.045 sec/batch; 1h:16m:51s remains)
INFO - root - 2019-11-06 19:30:52.225157: step 47970, total loss = 1.57, predict loss = 0.40 (94.6 examples/sec; 0.042 sec/batch; 1h:11m:55s remains)
INFO - root - 2019-11-06 19:30:52.700745: step 47980, total loss = 1.41, predict loss = 0.44 (104.2 examples/sec; 0.038 sec/batch; 1h:05m:15s remains)
INFO - root - 2019-11-06 19:30:54.003648: step 47990, total loss = 2.00, predict loss = 0.56 (52.5 examples/sec; 0.076 sec/batch; 2h:09m:28s remains)
INFO - root - 2019-11-06 19:30:54.722494: step 48000, total loss = 1.54, predict loss = 0.41 (62.9 examples/sec; 0.064 sec/batch; 1h:48m:05s remains)
INFO - root - 2019-11-06 19:30:55.460130: step 48010, total loss = 1.46, predict loss = 0.37 (59.4 examples/sec; 0.067 sec/batch; 1h:54m:25s remains)
INFO - root - 2019-11-06 19:30:56.192993: step 48020, total loss = 1.95, predict loss = 0.57 (57.7 examples/sec; 0.069 sec/batch; 1h:57m:45s remains)
INFO - root - 2019-11-06 19:30:56.933143: step 48030, total loss = 1.08, predict loss = 0.30 (62.0 examples/sec; 0.064 sec/batch; 1h:49m:36s remains)
INFO - root - 2019-11-06 19:30:57.475365: step 48040, total loss = 1.56, predict loss = 0.42 (92.7 examples/sec; 0.043 sec/batch; 1h:13m:20s remains)
INFO - root - 2019-11-06 19:30:57.932110: step 48050, total loss = 1.67, predict loss = 0.47 (92.7 examples/sec; 0.043 sec/batch; 1h:13m:18s remains)
INFO - root - 2019-11-06 19:30:59.115497: step 48060, total loss = 2.39, predict loss = 0.77 (68.4 examples/sec; 0.059 sec/batch; 1h:39m:25s remains)
INFO - root - 2019-11-06 19:30:59.794472: step 48070, total loss = 1.61, predict loss = 0.44 (61.5 examples/sec; 0.065 sec/batch; 1h:50m:27s remains)
INFO - root - 2019-11-06 19:31:00.563984: step 48080, total loss = 1.30, predict loss = 0.34 (54.6 examples/sec; 0.073 sec/batch; 2h:04m:22s remains)
INFO - root - 2019-11-06 19:31:01.292101: step 48090, total loss = 1.76, predict loss = 0.54 (57.7 examples/sec; 0.069 sec/batch; 1h:57m:49s remains)
INFO - root - 2019-11-06 19:31:02.056143: step 48100, total loss = 1.35, predict loss = 0.34 (56.9 examples/sec; 0.070 sec/batch; 1h:59m:26s remains)
INFO - root - 2019-11-06 19:31:02.666606: step 48110, total loss = 1.99, predict loss = 0.56 (102.8 examples/sec; 0.039 sec/batch; 1h:06m:04s remains)
INFO - root - 2019-11-06 19:31:03.099495: step 48120, total loss = 2.24, predict loss = 0.71 (104.5 examples/sec; 0.038 sec/batch; 1h:04m:58s remains)
INFO - root - 2019-11-06 19:31:03.556278: step 48130, total loss = 2.61, predict loss = 0.85 (91.3 examples/sec; 0.044 sec/batch; 1h:14m:21s remains)
INFO - root - 2019-11-06 19:31:04.854233: step 48140, total loss = 1.86, predict loss = 0.53 (67.4 examples/sec; 0.059 sec/batch; 1h:40m:46s remains)
INFO - root - 2019-11-06 19:31:05.574462: step 48150, total loss = 1.54, predict loss = 0.45 (56.3 examples/sec; 0.071 sec/batch; 2h:00m:36s remains)
INFO - root - 2019-11-06 19:31:06.323651: step 48160, total loss = 1.57, predict loss = 0.45 (59.8 examples/sec; 0.067 sec/batch; 1h:53m:31s remains)
INFO - root - 2019-11-06 19:31:07.125109: step 48170, total loss = 1.17, predict loss = 0.36 (58.9 examples/sec; 0.068 sec/batch; 1h:55m:09s remains)
INFO - root - 2019-11-06 19:31:07.820947: step 48180, total loss = 1.55, predict loss = 0.40 (75.1 examples/sec; 0.053 sec/batch; 1h:30m:21s remains)
INFO - root - 2019-11-06 19:31:08.309099: step 48190, total loss = 1.39, predict loss = 0.37 (94.7 examples/sec; 0.042 sec/batch; 1h:11m:38s remains)
INFO - root - 2019-11-06 19:31:08.741211: step 48200, total loss = 1.41, predict loss = 0.40 (97.8 examples/sec; 0.041 sec/batch; 1h:09m:21s remains)
INFO - root - 2019-11-06 19:31:09.925991: step 48210, total loss = 1.74, predict loss = 0.50 (67.9 examples/sec; 0.059 sec/batch; 1h:39m:54s remains)
INFO - root - 2019-11-06 19:31:10.609783: step 48220, total loss = 1.38, predict loss = 0.35 (58.6 examples/sec; 0.068 sec/batch; 1h:55m:49s remains)
INFO - root - 2019-11-06 19:31:11.306305: step 48230, total loss = 2.06, predict loss = 0.69 (62.3 examples/sec; 0.064 sec/batch; 1h:48m:53s remains)
INFO - root - 2019-11-06 19:31:12.028858: step 48240, total loss = 0.63, predict loss = 0.17 (57.1 examples/sec; 0.070 sec/batch; 1h:58m:43s remains)
INFO - root - 2019-11-06 19:31:12.764200: step 48250, total loss = 0.77, predict loss = 0.21 (57.6 examples/sec; 0.069 sec/batch; 1h:57m:46s remains)
INFO - root - 2019-11-06 19:31:13.356802: step 48260, total loss = 1.88, predict loss = 0.49 (95.3 examples/sec; 0.042 sec/batch; 1h:11m:10s remains)
INFO - root - 2019-11-06 19:31:13.814419: step 48270, total loss = 2.20, predict loss = 0.67 (101.8 examples/sec; 0.039 sec/batch; 1h:06m:37s remains)
INFO - root - 2019-11-06 19:31:14.262570: step 48280, total loss = 1.86, predict loss = 0.53 (97.9 examples/sec; 0.041 sec/batch; 1h:09m:15s remains)
INFO - root - 2019-11-06 19:31:15.645470: step 48290, total loss = 2.13, predict loss = 0.62 (53.9 examples/sec; 0.074 sec/batch; 2h:05m:46s remains)
INFO - root - 2019-11-06 19:31:16.374919: step 48300, total loss = 1.51, predict loss = 0.42 (60.5 examples/sec; 0.066 sec/batch; 1h:51m:59s remains)
INFO - root - 2019-11-06 19:31:17.089362: step 48310, total loss = 0.89, predict loss = 0.21 (61.1 examples/sec; 0.065 sec/batch; 1h:50m:53s remains)
INFO - root - 2019-11-06 19:31:17.857604: step 48320, total loss = 1.77, predict loss = 0.54 (53.2 examples/sec; 0.075 sec/batch; 2h:07m:29s remains)
INFO - root - 2019-11-06 19:31:18.592796: step 48330, total loss = 1.45, predict loss = 0.37 (60.7 examples/sec; 0.066 sec/batch; 1h:51m:37s remains)
INFO - root - 2019-11-06 19:31:19.127637: step 48340, total loss = 1.35, predict loss = 0.36 (89.3 examples/sec; 0.045 sec/batch; 1h:15m:54s remains)
INFO - root - 2019-11-06 19:31:19.580035: step 48350, total loss = 1.94, predict loss = 0.50 (89.8 examples/sec; 0.045 sec/batch; 1h:15m:29s remains)
INFO - root - 2019-11-06 19:31:20.774755: step 48360, total loss = 2.13, predict loss = 0.61 (66.8 examples/sec; 0.060 sec/batch; 1h:41m:23s remains)
INFO - root - 2019-11-06 19:31:21.483611: step 48370, total loss = 0.92, predict loss = 0.24 (60.6 examples/sec; 0.066 sec/batch; 1h:51m:47s remains)
INFO - root - 2019-11-06 19:31:22.194143: step 48380, total loss = 1.48, predict loss = 0.42 (61.6 examples/sec; 0.065 sec/batch; 1h:49m:55s remains)
INFO - root - 2019-11-06 19:31:22.917448: step 48390, total loss = 1.31, predict loss = 0.33 (59.6 examples/sec; 0.067 sec/batch; 1h:53m:44s remains)
INFO - root - 2019-11-06 19:31:23.674127: step 48400, total loss = 0.81, predict loss = 0.22 (62.0 examples/sec; 0.065 sec/batch; 1h:49m:18s remains)
INFO - root - 2019-11-06 19:31:24.316631: step 48410, total loss = 2.41, predict loss = 0.75 (99.8 examples/sec; 0.040 sec/batch; 1h:07m:50s remains)
INFO - root - 2019-11-06 19:31:24.784874: step 48420, total loss = 1.37, predict loss = 0.32 (91.6 examples/sec; 0.044 sec/batch; 1h:13m:55s remains)
INFO - root - 2019-11-06 19:31:25.224811: step 48430, total loss = 1.13, predict loss = 0.29 (130.0 examples/sec; 0.031 sec/batch; 0h:52m:04s remains)
INFO - root - 2019-11-06 19:31:26.569961: step 48440, total loss = 1.48, predict loss = 0.41 (55.9 examples/sec; 0.072 sec/batch; 2h:01m:08s remains)
INFO - root - 2019-11-06 19:31:27.366475: step 48450, total loss = 1.52, predict loss = 0.37 (59.7 examples/sec; 0.067 sec/batch; 1h:53m:20s remains)
INFO - root - 2019-11-06 19:31:28.128650: step 48460, total loss = 2.04, predict loss = 0.57 (61.0 examples/sec; 0.066 sec/batch; 1h:51m:03s remains)
INFO - root - 2019-11-06 19:31:28.861572: step 48470, total loss = 0.72, predict loss = 0.22 (54.7 examples/sec; 0.073 sec/batch; 2h:03m:37s remains)
INFO - root - 2019-11-06 19:31:29.582487: step 48480, total loss = 1.91, predict loss = 0.58 (65.3 examples/sec; 0.061 sec/batch; 1h:43m:41s remains)
INFO - root - 2019-11-06 19:31:30.047781: step 48490, total loss = 1.36, predict loss = 0.37 (99.9 examples/sec; 0.040 sec/batch; 1h:07m:42s remains)
INFO - root - 2019-11-06 19:31:30.512797: step 48500, total loss = 1.50, predict loss = 0.41 (92.1 examples/sec; 0.043 sec/batch; 1h:13m:29s remains)
INFO - root - 2019-11-06 19:31:31.781133: step 48510, total loss = 1.75, predict loss = 0.50 (61.4 examples/sec; 0.065 sec/batch; 1h:50m:16s remains)
INFO - root - 2019-11-06 19:31:32.477131: step 48520, total loss = 2.53, predict loss = 0.77 (62.3 examples/sec; 0.064 sec/batch; 1h:48m:35s remains)
INFO - root - 2019-11-06 19:31:33.238984: step 48530, total loss = 1.90, predict loss = 0.55 (63.6 examples/sec; 0.063 sec/batch; 1h:46m:26s remains)
INFO - root - 2019-11-06 19:31:33.971289: step 48540, total loss = 1.74, predict loss = 0.53 (58.0 examples/sec; 0.069 sec/batch; 1h:56m:41s remains)
INFO - root - 2019-11-06 19:31:34.712988: step 48550, total loss = 1.56, predict loss = 0.37 (82.7 examples/sec; 0.048 sec/batch; 1h:21m:46s remains)
INFO - root - 2019-11-06 19:31:35.235310: step 48560, total loss = 1.94, predict loss = 0.56 (95.2 examples/sec; 0.042 sec/batch; 1h:11m:00s remains)
INFO - root - 2019-11-06 19:31:35.678230: step 48570, total loss = 1.94, predict loss = 0.51 (96.5 examples/sec; 0.041 sec/batch; 1h:10m:04s remains)
INFO - root - 2019-11-06 19:31:36.842501: step 48580, total loss = 1.59, predict loss = 0.47 (5.3 examples/sec; 0.750 sec/batch; 21h:07m:36s remains)
INFO - root - 2019-11-06 19:31:37.509095: step 48590, total loss = 1.81, predict loss = 0.60 (64.5 examples/sec; 0.062 sec/batch; 1h:44m:49s remains)
INFO - root - 2019-11-06 19:31:38.226329: step 48600, total loss = 1.10, predict loss = 0.31 (54.0 examples/sec; 0.074 sec/batch; 2h:05m:05s remains)
INFO - root - 2019-11-06 19:31:39.038466: step 48610, total loss = 1.03, predict loss = 0.28 (59.4 examples/sec; 0.067 sec/batch; 1h:53m:49s remains)
INFO - root - 2019-11-06 19:31:39.792501: step 48620, total loss = 1.60, predict loss = 0.43 (54.5 examples/sec; 0.073 sec/batch; 2h:04m:02s remains)
INFO - root - 2019-11-06 19:31:40.431484: step 48630, total loss = 1.43, predict loss = 0.40 (90.4 examples/sec; 0.044 sec/batch; 1h:14m:44s remains)
INFO - root - 2019-11-06 19:31:40.882982: step 48640, total loss = 1.12, predict loss = 0.28 (95.9 examples/sec; 0.042 sec/batch; 1h:10m:28s remains)
INFO - root - 2019-11-06 19:31:41.333441: step 48650, total loss = 1.81, predict loss = 0.46 (103.0 examples/sec; 0.039 sec/batch; 1h:05m:37s remains)
INFO - root - 2019-11-06 19:31:42.600601: step 48660, total loss = 1.88, predict loss = 0.53 (65.9 examples/sec; 0.061 sec/batch; 1h:42m:26s remains)
INFO - root - 2019-11-06 19:31:43.341798: step 48670, total loss = 1.06, predict loss = 0.27 (64.0 examples/sec; 0.062 sec/batch; 1h:45m:32s remains)
INFO - root - 2019-11-06 19:31:44.087013: step 48680, total loss = 1.70, predict loss = 0.51 (62.6 examples/sec; 0.064 sec/batch; 1h:47m:58s remains)
INFO - root - 2019-11-06 19:31:44.840452: step 48690, total loss = 1.87, predict loss = 0.46 (62.6 examples/sec; 0.064 sec/batch; 1h:47m:50s remains)
INFO - root - 2019-11-06 19:31:45.554441: step 48700, total loss = 1.38, predict loss = 0.38 (63.8 examples/sec; 0.063 sec/batch; 1h:45m:51s remains)
INFO - root - 2019-11-06 19:31:46.064779: step 48710, total loss = 1.69, predict loss = 0.48 (95.7 examples/sec; 0.042 sec/batch; 1h:10m:35s remains)
INFO - root - 2019-11-06 19:31:46.523372: step 48720, total loss = 1.67, predict loss = 0.46 (100.3 examples/sec; 0.040 sec/batch; 1h:07m:20s remains)
INFO - root - 2019-11-06 19:31:47.685873: step 48730, total loss = 2.01, predict loss = 0.61 (66.1 examples/sec; 0.061 sec/batch; 1h:42m:07s remains)
INFO - root - 2019-11-06 19:31:48.397200: step 48740, total loss = 1.12, predict loss = 0.28 (52.6 examples/sec; 0.076 sec/batch; 2h:08m:13s remains)
INFO - root - 2019-11-06 19:31:49.212605: step 48750, total loss = 1.31, predict loss = 0.33 (52.1 examples/sec; 0.077 sec/batch; 2h:09m:26s remains)
INFO - root - 2019-11-06 19:31:50.069158: step 48760, total loss = 1.68, predict loss = 0.47 (52.9 examples/sec; 0.076 sec/batch; 2h:07m:36s remains)
INFO - root - 2019-11-06 19:31:50.923533: step 48770, total loss = 1.46, predict loss = 0.38 (55.4 examples/sec; 0.072 sec/batch; 2h:01m:54s remains)
INFO - root - 2019-11-06 19:31:51.584325: step 48780, total loss = 1.60, predict loss = 0.47 (96.8 examples/sec; 0.041 sec/batch; 1h:09m:44s remains)
INFO - root - 2019-11-06 19:31:52.025075: step 48790, total loss = 2.44, predict loss = 0.74 (89.0 examples/sec; 0.045 sec/batch; 1h:15m:51s remains)
INFO - root - 2019-11-06 19:31:52.462752: step 48800, total loss = 1.04, predict loss = 0.30 (94.7 examples/sec; 0.042 sec/batch; 1h:11m:13s remains)
INFO - root - 2019-11-06 19:31:53.722693: step 48810, total loss = 0.78, predict loss = 0.21 (52.5 examples/sec; 0.076 sec/batch; 2h:08m:33s remains)
INFO - root - 2019-11-06 19:31:54.499265: step 48820, total loss = 2.22, predict loss = 0.70 (71.0 examples/sec; 0.056 sec/batch; 1h:35m:01s remains)
INFO - root - 2019-11-06 19:31:55.240810: step 48830, total loss = 0.93, predict loss = 0.24 (52.3 examples/sec; 0.076 sec/batch; 2h:08m:54s remains)
INFO - root - 2019-11-06 19:31:56.042650: step 48840, total loss = 1.39, predict loss = 0.37 (54.5 examples/sec; 0.073 sec/batch; 2h:03m:39s remains)
INFO - root - 2019-11-06 19:31:56.758756: step 48850, total loss = 2.10, predict loss = 0.63 (69.0 examples/sec; 0.058 sec/batch; 1h:37m:43s remains)
INFO - root - 2019-11-06 19:31:57.285299: step 48860, total loss = 1.70, predict loss = 0.57 (103.3 examples/sec; 0.039 sec/batch; 1h:05m:16s remains)
INFO - root - 2019-11-06 19:31:57.742393: step 48870, total loss = 1.87, predict loss = 0.53 (91.5 examples/sec; 0.044 sec/batch; 1h:13m:39s remains)
INFO - root - 2019-11-06 19:31:58.914789: step 48880, total loss = 1.88, predict loss = 0.54 (63.2 examples/sec; 0.063 sec/batch; 1h:46m:44s remains)
INFO - root - 2019-11-06 19:31:59.611841: step 48890, total loss = 1.73, predict loss = 0.52 (61.1 examples/sec; 0.066 sec/batch; 1h:50m:23s remains)
INFO - root - 2019-11-06 19:32:00.403191: step 48900, total loss = 0.89, predict loss = 0.21 (58.9 examples/sec; 0.068 sec/batch; 1h:54m:21s remains)
INFO - root - 2019-11-06 19:32:01.131922: step 48910, total loss = 1.70, predict loss = 0.44 (58.9 examples/sec; 0.068 sec/batch; 1h:54m:20s remains)
INFO - root - 2019-11-06 19:32:01.853510: step 48920, total loss = 1.20, predict loss = 0.33 (63.2 examples/sec; 0.063 sec/batch; 1h:46m:41s remains)
INFO - root - 2019-11-06 19:32:02.476718: step 48930, total loss = 1.39, predict loss = 0.36 (96.1 examples/sec; 0.042 sec/batch; 1h:10m:06s remains)
INFO - root - 2019-11-06 19:32:02.960348: step 48940, total loss = 0.98, predict loss = 0.23 (92.2 examples/sec; 0.043 sec/batch; 1h:13m:02s remains)
INFO - root - 2019-11-06 19:32:03.434110: step 48950, total loss = 1.50, predict loss = 0.46 (91.2 examples/sec; 0.044 sec/batch; 1h:13m:50s remains)
INFO - root - 2019-11-06 19:32:04.786327: step 48960, total loss = 0.99, predict loss = 0.25 (54.9 examples/sec; 0.073 sec/batch; 2h:02m:46s remains)
INFO - root - 2019-11-06 19:32:05.600141: step 48970, total loss = 1.74, predict loss = 0.56 (54.9 examples/sec; 0.073 sec/batch; 2h:02m:37s remains)
INFO - root - 2019-11-06 19:32:06.415888: step 48980, total loss = 0.79, predict loss = 0.23 (52.7 examples/sec; 0.076 sec/batch; 2h:07m:43s remains)
INFO - root - 2019-11-06 19:32:07.150152: step 48990, total loss = 0.92, predict loss = 0.20 (57.8 examples/sec; 0.069 sec/batch; 1h:56m:26s remains)
INFO - root - 2019-11-06 19:32:07.899837: step 49000, total loss = 1.92, predict loss = 0.50 (65.7 examples/sec; 0.061 sec/batch; 1h:42m:25s remains)
INFO - root - 2019-11-06 19:32:08.428650: step 49010, total loss = 1.45, predict loss = 0.44 (93.9 examples/sec; 0.043 sec/batch; 1h:11m:42s remains)
INFO - root - 2019-11-06 19:32:08.929035: step 49020, total loss = 1.26, predict loss = 0.33 (93.3 examples/sec; 0.043 sec/batch; 1h:12m:11s remains)
INFO - root - 2019-11-06 19:32:10.127309: step 49030, total loss = 1.03, predict loss = 0.27 (71.0 examples/sec; 0.056 sec/batch; 1h:34m:52s remains)
INFO - root - 2019-11-06 19:32:10.835880: step 49040, total loss = 1.20, predict loss = 0.33 (55.6 examples/sec; 0.072 sec/batch; 2h:01m:06s remains)
INFO - root - 2019-11-06 19:32:11.615333: step 49050, total loss = 1.02, predict loss = 0.27 (57.4 examples/sec; 0.070 sec/batch; 1h:57m:14s remains)
INFO - root - 2019-11-06 19:32:12.369145: step 49060, total loss = 1.39, predict loss = 0.34 (61.8 examples/sec; 0.065 sec/batch; 1h:48m:53s remains)
INFO - root - 2019-11-06 19:32:13.128682: step 49070, total loss = 1.06, predict loss = 0.28 (60.4 examples/sec; 0.066 sec/batch; 1h:51m:23s remains)
INFO - root - 2019-11-06 19:32:13.721935: step 49080, total loss = 1.22, predict loss = 0.30 (104.4 examples/sec; 0.038 sec/batch; 1h:04m:25s remains)
INFO - root - 2019-11-06 19:32:14.157718: step 49090, total loss = 1.16, predict loss = 0.32 (98.0 examples/sec; 0.041 sec/batch; 1h:08m:38s remains)
INFO - root - 2019-11-06 19:32:14.636288: step 49100, total loss = 1.59, predict loss = 0.46 (93.6 examples/sec; 0.043 sec/batch; 1h:11m:51s remains)
INFO - root - 2019-11-06 19:32:15.926922: step 49110, total loss = 0.88, predict loss = 0.26 (56.2 examples/sec; 0.071 sec/batch; 1h:59m:45s remains)
INFO - root - 2019-11-06 19:32:16.673371: step 49120, total loss = 1.98, predict loss = 0.57 (59.7 examples/sec; 0.067 sec/batch; 1h:52m:36s remains)
INFO - root - 2019-11-06 19:32:17.510328: step 49130, total loss = 1.29, predict loss = 0.33 (50.0 examples/sec; 0.080 sec/batch; 2h:14m:30s remains)
INFO - root - 2019-11-06 19:32:18.277534: step 49140, total loss = 0.93, predict loss = 0.24 (61.7 examples/sec; 0.065 sec/batch; 1h:48m:55s remains)
INFO - root - 2019-11-06 19:32:19.026762: step 49150, total loss = 1.20, predict loss = 0.31 (58.7 examples/sec; 0.068 sec/batch; 1h:54m:33s remains)
INFO - root - 2019-11-06 19:32:19.512563: step 49160, total loss = 1.33, predict loss = 0.35 (90.9 examples/sec; 0.044 sec/batch; 1h:13m:57s remains)
INFO - root - 2019-11-06 19:32:19.956826: step 49170, total loss = 2.57, predict loss = 0.84 (97.5 examples/sec; 0.041 sec/batch; 1h:08m:58s remains)
INFO - root - 2019-11-06 19:32:21.200779: step 49180, total loss = 0.89, predict loss = 0.22 (63.0 examples/sec; 0.064 sec/batch; 1h:46m:43s remains)
INFO - root - 2019-11-06 19:32:21.871715: step 49190, total loss = 2.11, predict loss = 0.62 (56.0 examples/sec; 0.071 sec/batch; 1h:59m:56s remains)
INFO - root - 2019-11-06 19:32:22.643132: step 49200, total loss = 1.19, predict loss = 0.30 (56.1 examples/sec; 0.071 sec/batch; 1h:59m:45s remains)
INFO - root - 2019-11-06 19:32:23.411241: step 49210, total loss = 1.48, predict loss = 0.40 (60.6 examples/sec; 0.066 sec/batch; 1h:50m:55s remains)
INFO - root - 2019-11-06 19:32:24.204047: step 49220, total loss = 1.44, predict loss = 0.35 (73.7 examples/sec; 0.054 sec/batch; 1h:31m:10s remains)
INFO - root - 2019-11-06 19:32:24.736353: step 49230, total loss = 2.66, predict loss = 0.83 (95.4 examples/sec; 0.042 sec/batch; 1h:10m:25s remains)
INFO - root - 2019-11-06 19:32:25.183234: step 49240, total loss = 1.40, predict loss = 0.40 (94.1 examples/sec; 0.043 sec/batch; 1h:11m:25s remains)
INFO - root - 2019-11-06 19:32:25.619203: step 49250, total loss = 0.96, predict loss = 0.25 (109.4 examples/sec; 0.037 sec/batch; 1h:01m:24s remains)
INFO - root - 2019-11-06 19:32:27.002302: step 49260, total loss = 1.60, predict loss = 0.42 (62.8 examples/sec; 0.064 sec/batch; 1h:47m:00s remains)
INFO - root - 2019-11-06 19:32:27.795397: step 49270, total loss = 1.70, predict loss = 0.48 (58.3 examples/sec; 0.069 sec/batch; 1h:55m:15s remains)
INFO - root - 2019-11-06 19:32:28.596345: step 49280, total loss = 0.98, predict loss = 0.27 (50.6 examples/sec; 0.079 sec/batch; 2h:12m:41s remains)
INFO - root - 2019-11-06 19:32:29.290177: step 49290, total loss = 1.46, predict loss = 0.38 (69.5 examples/sec; 0.058 sec/batch; 1h:36m:36s remains)
INFO - root - 2019-11-06 19:32:29.940987: step 49300, total loss = 0.96, predict loss = 0.26 (77.8 examples/sec; 0.051 sec/batch; 1h:26m:17s remains)
INFO - root - 2019-11-06 19:32:30.404228: step 49310, total loss = 1.90, predict loss = 0.59 (101.2 examples/sec; 0.040 sec/batch; 1h:06m:19s remains)
INFO - root - 2019-11-06 19:32:30.863613: step 49320, total loss = 1.75, predict loss = 0.50 (93.7 examples/sec; 0.043 sec/batch; 1h:11m:36s remains)
INFO - root - 2019-11-06 19:32:32.079352: step 49330, total loss = 2.13, predict loss = 0.63 (62.3 examples/sec; 0.064 sec/batch; 1h:47m:45s remains)
INFO - root - 2019-11-06 19:32:32.787376: step 49340, total loss = 1.11, predict loss = 0.26 (60.1 examples/sec; 0.067 sec/batch; 1h:51m:42s remains)
INFO - root - 2019-11-06 19:32:33.553606: step 49350, total loss = 1.44, predict loss = 0.41 (56.0 examples/sec; 0.071 sec/batch; 1h:59m:45s remains)
INFO - root - 2019-11-06 19:32:34.355719: step 49360, total loss = 2.23, predict loss = 0.66 (55.4 examples/sec; 0.072 sec/batch; 2h:01m:07s remains)
INFO - root - 2019-11-06 19:32:35.087714: step 49370, total loss = 1.51, predict loss = 0.40 (67.1 examples/sec; 0.060 sec/batch; 1h:39m:55s remains)
INFO - root - 2019-11-06 19:32:35.647253: step 49380, total loss = 0.88, predict loss = 0.23 (96.9 examples/sec; 0.041 sec/batch; 1h:09m:14s remains)
INFO - root - 2019-11-06 19:32:36.087990: step 49390, total loss = 0.96, predict loss = 0.27 (95.6 examples/sec; 0.042 sec/batch; 1h:10m:08s remains)
INFO - root - 2019-11-06 19:32:37.226063: step 49400, total loss = 0.89, predict loss = 0.20 (5.5 examples/sec; 0.730 sec/batch; 20h:23m:46s remains)
INFO - root - 2019-11-06 19:32:37.916908: step 49410, total loss = 1.21, predict loss = 0.28 (61.7 examples/sec; 0.065 sec/batch; 1h:48m:43s remains)
INFO - root - 2019-11-06 19:32:38.659897: step 49420, total loss = 1.96, predict loss = 0.58 (59.5 examples/sec; 0.067 sec/batch; 1h:52m:43s remains)
INFO - root - 2019-11-06 19:32:39.390989: step 49430, total loss = 0.70, predict loss = 0.20 (58.9 examples/sec; 0.068 sec/batch; 1h:53m:53s remains)
INFO - root - 2019-11-06 19:32:40.065368: step 49440, total loss = 2.42, predict loss = 0.68 (66.5 examples/sec; 0.060 sec/batch; 1h:40m:44s remains)
INFO - root - 2019-11-06 19:32:40.692913: step 49450, total loss = 1.10, predict loss = 0.29 (96.3 examples/sec; 0.042 sec/batch; 1h:09m:37s remains)
INFO - root - 2019-11-06 19:32:41.161507: step 49460, total loss = 1.17, predict loss = 0.34 (92.2 examples/sec; 0.043 sec/batch; 1h:12m:42s remains)
INFO - root - 2019-11-06 19:32:41.610890: step 49470, total loss = 0.80, predict loss = 0.20 (98.1 examples/sec; 0.041 sec/batch; 1h:08m:19s remains)
INFO - root - 2019-11-06 19:32:42.840227: step 49480, total loss = 1.56, predict loss = 0.42 (71.7 examples/sec; 0.056 sec/batch; 1h:33m:26s remains)
INFO - root - 2019-11-06 19:32:43.562942: step 49490, total loss = 1.12, predict loss = 0.32 (56.5 examples/sec; 0.071 sec/batch; 1h:58m:32s remains)
INFO - root - 2019-11-06 19:32:44.329877: step 49500, total loss = 1.66, predict loss = 0.46 (55.5 examples/sec; 0.072 sec/batch; 2h:00m:36s remains)
INFO - root - 2019-11-06 19:32:45.095471: step 49510, total loss = 0.99, predict loss = 0.28 (57.3 examples/sec; 0.070 sec/batch; 1h:56m:52s remains)
INFO - root - 2019-11-06 19:32:45.873745: step 49520, total loss = 1.07, predict loss = 0.30 (66.1 examples/sec; 0.061 sec/batch; 1h:41m:19s remains)
INFO - root - 2019-11-06 19:32:46.387905: step 49530, total loss = 1.26, predict loss = 0.33 (99.0 examples/sec; 0.040 sec/batch; 1h:07m:39s remains)
INFO - root - 2019-11-06 19:32:46.864259: step 49540, total loss = 1.47, predict loss = 0.43 (98.4 examples/sec; 0.041 sec/batch; 1h:08m:04s remains)
INFO - root - 2019-11-06 19:32:48.002636: step 49550, total loss = 0.78, predict loss = 0.23 (71.5 examples/sec; 0.056 sec/batch; 1h:33m:38s remains)
INFO - root - 2019-11-06 19:32:48.738398: step 49560, total loss = 1.56, predict loss = 0.44 (51.6 examples/sec; 0.077 sec/batch; 2h:09m:43s remains)
INFO - root - 2019-11-06 19:32:49.520792: step 49570, total loss = 1.57, predict loss = 0.45 (57.4 examples/sec; 0.070 sec/batch; 1h:56m:42s remains)
INFO - root - 2019-11-06 19:32:50.308145: step 49580, total loss = 1.66, predict loss = 0.47 (55.1 examples/sec; 0.073 sec/batch; 2h:01m:31s remains)
INFO - root - 2019-11-06 19:32:51.094158: step 49590, total loss = 1.12, predict loss = 0.30 (51.8 examples/sec; 0.077 sec/batch; 2h:09m:09s remains)
INFO - root - 2019-11-06 19:32:51.799784: step 49600, total loss = 1.30, predict loss = 0.34 (93.0 examples/sec; 0.043 sec/batch; 1h:11m:56s remains)
INFO - root - 2019-11-06 19:32:52.257081: step 49610, total loss = 1.37, predict loss = 0.34 (95.8 examples/sec; 0.042 sec/batch; 1h:09m:50s remains)
INFO - root - 2019-11-06 19:32:52.743999: step 49620, total loss = 2.06, predict loss = 0.64 (96.1 examples/sec; 0.042 sec/batch; 1h:09m:39s remains)
INFO - root - 2019-11-06 19:32:54.024720: step 49630, total loss = 1.43, predict loss = 0.39 (68.9 examples/sec; 0.058 sec/batch; 1h:37m:03s remains)
INFO - root - 2019-11-06 19:32:54.764819: step 49640, total loss = 0.88, predict loss = 0.23 (49.3 examples/sec; 0.081 sec/batch; 2h:15m:50s remains)
INFO - root - 2019-11-06 19:32:55.541712: step 49650, total loss = 1.32, predict loss = 0.45 (52.4 examples/sec; 0.076 sec/batch; 2h:07m:44s remains)
INFO - root - 2019-11-06 19:32:56.286525: step 49660, total loss = 1.16, predict loss = 0.31 (50.7 examples/sec; 0.079 sec/batch; 2h:11m:49s remains)
INFO - root - 2019-11-06 19:32:57.028099: step 49670, total loss = 1.69, predict loss = 0.44 (62.2 examples/sec; 0.064 sec/batch; 1h:47m:32s remains)
INFO - root - 2019-11-06 19:32:57.539780: step 49680, total loss = 1.50, predict loss = 0.43 (97.0 examples/sec; 0.041 sec/batch; 1h:08m:56s remains)
INFO - root - 2019-11-06 19:32:57.996228: step 49690, total loss = 1.75, predict loss = 0.59 (92.3 examples/sec; 0.043 sec/batch; 1h:12m:28s remains)
INFO - root - 2019-11-06 19:32:59.161188: step 49700, total loss = 0.62, predict loss = 0.17 (69.5 examples/sec; 0.058 sec/batch; 1h:36m:15s remains)
INFO - root - 2019-11-06 19:32:59.875556: step 49710, total loss = 0.72, predict loss = 0.19 (62.2 examples/sec; 0.064 sec/batch; 1h:47m:27s remains)
INFO - root - 2019-11-06 19:33:00.595996: step 49720, total loss = 0.99, predict loss = 0.20 (56.6 examples/sec; 0.071 sec/batch; 1h:58m:00s remains)
INFO - root - 2019-11-06 19:33:01.360244: step 49730, total loss = 1.33, predict loss = 0.35 (59.6 examples/sec; 0.067 sec/batch; 1h:52m:10s remains)
INFO - root - 2019-11-06 19:33:02.106425: step 49740, total loss = 0.75, predict loss = 0.18 (64.6 examples/sec; 0.062 sec/batch; 1h:43m:24s remains)
INFO - root - 2019-11-06 19:33:02.659926: step 49750, total loss = 2.29, predict loss = 0.71 (103.7 examples/sec; 0.039 sec/batch; 1h:04m:27s remains)
INFO - root - 2019-11-06 19:33:03.103816: step 49760, total loss = 1.15, predict loss = 0.30 (96.4 examples/sec; 0.041 sec/batch; 1h:09m:19s remains)
INFO - root - 2019-11-06 19:33:03.552520: step 49770, total loss = 1.13, predict loss = 0.27 (88.3 examples/sec; 0.045 sec/batch; 1h:15m:41s remains)
INFO - root - 2019-11-06 19:33:04.877350: step 49780, total loss = 1.60, predict loss = 0.40 (63.8 examples/sec; 0.063 sec/batch; 1h:44m:41s remains)
INFO - root - 2019-11-06 19:33:05.641433: step 49790, total loss = 1.59, predict loss = 0.46 (60.9 examples/sec; 0.066 sec/batch; 1h:49m:43s remains)
INFO - root - 2019-11-06 19:33:06.367784: step 49800, total loss = 1.27, predict loss = 0.35 (59.5 examples/sec; 0.067 sec/batch; 1h:52m:13s remains)
INFO - root - 2019-11-06 19:33:07.160075: step 49810, total loss = 1.55, predict loss = 0.42 (53.6 examples/sec; 0.075 sec/batch; 2h:04m:40s remains)
INFO - root - 2019-11-06 19:33:07.869325: step 49820, total loss = 2.13, predict loss = 0.73 (79.3 examples/sec; 0.050 sec/batch; 1h:24m:14s remains)
INFO - root - 2019-11-06 19:33:08.349698: step 49830, total loss = 1.42, predict loss = 0.35 (94.1 examples/sec; 0.042 sec/batch; 1h:10m:57s remains)
INFO - root - 2019-11-06 19:33:08.790012: step 49840, total loss = 0.82, predict loss = 0.19 (97.6 examples/sec; 0.041 sec/batch; 1h:08m:23s remains)
INFO - root - 2019-11-06 19:33:09.951081: step 49850, total loss = 2.07, predict loss = 0.66 (62.0 examples/sec; 0.065 sec/batch; 1h:47m:46s remains)
INFO - root - 2019-11-06 19:33:10.712465: step 49860, total loss = 1.27, predict loss = 0.35 (54.4 examples/sec; 0.074 sec/batch; 2h:02m:44s remains)
INFO - root - 2019-11-06 19:33:11.517622: step 49870, total loss = 0.97, predict loss = 0.26 (62.1 examples/sec; 0.064 sec/batch; 1h:47m:28s remains)
INFO - root - 2019-11-06 19:33:12.339266: step 49880, total loss = 1.63, predict loss = 0.48 (53.1 examples/sec; 0.075 sec/batch; 2h:05m:43s remains)
INFO - root - 2019-11-06 19:33:13.054550: step 49890, total loss = 2.15, predict loss = 0.71 (63.9 examples/sec; 0.063 sec/batch; 1h:44m:27s remains)
INFO - root - 2019-11-06 19:33:13.677585: step 49900, total loss = 1.49, predict loss = 0.42 (100.5 examples/sec; 0.040 sec/batch; 1h:06m:22s remains)
INFO - root - 2019-11-06 19:33:14.124015: step 49910, total loss = 1.21, predict loss = 0.33 (97.3 examples/sec; 0.041 sec/batch; 1h:08m:34s remains)
INFO - root - 2019-11-06 19:33:14.567087: step 49920, total loss = 1.06, predict loss = 0.27 (97.6 examples/sec; 0.041 sec/batch; 1h:08m:22s remains)
INFO - root - 2019-11-06 19:33:15.890377: step 49930, total loss = 1.24, predict loss = 0.31 (59.7 examples/sec; 0.067 sec/batch; 1h:51m:41s remains)
INFO - root - 2019-11-06 19:33:16.646684: step 49940, total loss = 1.94, predict loss = 0.57 (56.3 examples/sec; 0.071 sec/batch; 1h:58m:32s remains)
INFO - root - 2019-11-06 19:33:17.454750: step 49950, total loss = 1.67, predict loss = 0.48 (51.3 examples/sec; 0.078 sec/batch; 2h:10m:08s remains)
INFO - root - 2019-11-06 19:33:18.200150: step 49960, total loss = 1.37, predict loss = 0.35 (56.1 examples/sec; 0.071 sec/batch; 1h:58m:56s remains)
INFO - root - 2019-11-06 19:33:18.903155: step 49970, total loss = 1.68, predict loss = 0.47 (66.0 examples/sec; 0.061 sec/batch; 1h:41m:02s remains)
INFO - root - 2019-11-06 19:33:19.425154: step 49980, total loss = 1.05, predict loss = 0.24 (93.4 examples/sec; 0.043 sec/batch; 1h:11m:24s remains)
INFO - root - 2019-11-06 19:33:19.868772: step 49990, total loss = 1.88, predict loss = 0.52 (96.1 examples/sec; 0.042 sec/batch; 1h:09m:22s remains)
INFO - root - 2019-11-06 19:33:21.049364: step 50000, total loss = 0.83, predict loss = 0.21 (65.5 examples/sec; 0.061 sec/batch; 1h:41m:51s remains)
INFO - root - 2019-11-06 19:33:21.743994: step 50010, total loss = 1.72, predict loss = 0.50 (58.1 examples/sec; 0.069 sec/batch; 1h:54m:48s remains)
INFO - root - 2019-11-06 19:33:22.496283: step 50020, total loss = 1.39, predict loss = 0.33 (57.3 examples/sec; 0.070 sec/batch; 1h:56m:20s remains)
INFO - root - 2019-11-06 19:33:23.254292: step 50030, total loss = 1.34, predict loss = 0.33 (57.5 examples/sec; 0.070 sec/batch; 1h:55m:58s remains)
INFO - root - 2019-11-06 19:33:24.062833: step 50040, total loss = 1.31, predict loss = 0.34 (61.7 examples/sec; 0.065 sec/batch; 1h:47m:59s remains)
INFO - root - 2019-11-06 19:33:24.607565: step 50050, total loss = 1.80, predict loss = 0.48 (99.6 examples/sec; 0.040 sec/batch; 1h:06m:52s remains)
INFO - root - 2019-11-06 19:33:25.075571: step 50060, total loss = 1.41, predict loss = 0.38 (100.8 examples/sec; 0.040 sec/batch; 1h:06m:05s remains)
INFO - root - 2019-11-06 19:33:25.528752: step 50070, total loss = 1.60, predict loss = 0.47 (123.5 examples/sec; 0.032 sec/batch; 0h:53m:55s remains)
INFO - root - 2019-11-06 19:33:26.926856: step 50080, total loss = 1.28, predict loss = 0.31 (58.0 examples/sec; 0.069 sec/batch; 1h:54m:45s remains)
INFO - root - 2019-11-06 19:33:27.664843: step 50090, total loss = 1.31, predict loss = 0.38 (63.3 examples/sec; 0.063 sec/batch; 1h:45m:14s remains)
INFO - root - 2019-11-06 19:33:28.411516: step 50100, total loss = 2.19, predict loss = 0.66 (56.9 examples/sec; 0.070 sec/batch; 1h:57m:07s remains)
INFO - root - 2019-11-06 19:33:29.120241: step 50110, total loss = 1.54, predict loss = 0.45 (65.1 examples/sec; 0.061 sec/batch; 1h:42m:21s remains)
INFO - root - 2019-11-06 19:33:29.813523: step 50120, total loss = 1.48, predict loss = 0.45 (73.5 examples/sec; 0.054 sec/batch; 1h:30m:35s remains)
INFO - root - 2019-11-06 19:33:30.290748: step 50130, total loss = 1.18, predict loss = 0.34 (98.3 examples/sec; 0.041 sec/batch; 1h:07m:45s remains)
INFO - root - 2019-11-06 19:33:30.755009: step 50140, total loss = 1.15, predict loss = 0.32 (97.1 examples/sec; 0.041 sec/batch; 1h:08m:34s remains)
INFO - root - 2019-11-06 19:33:31.966589: step 50150, total loss = 1.43, predict loss = 0.39 (63.1 examples/sec; 0.063 sec/batch; 1h:45m:33s remains)
INFO - root - 2019-11-06 19:33:32.693491: step 50160, total loss = 1.73, predict loss = 0.48 (57.9 examples/sec; 0.069 sec/batch; 1h:54m:59s remains)
INFO - root - 2019-11-06 19:33:33.448977: step 50170, total loss = 1.51, predict loss = 0.40 (61.0 examples/sec; 0.066 sec/batch; 1h:49m:10s remains)
INFO - root - 2019-11-06 19:33:34.200098: step 50180, total loss = 1.96, predict loss = 0.60 (61.6 examples/sec; 0.065 sec/batch; 1h:48m:03s remains)
INFO - root - 2019-11-06 19:33:34.922904: step 50190, total loss = 1.73, predict loss = 0.51 (65.9 examples/sec; 0.061 sec/batch; 1h:40m:54s remains)
INFO - root - 2019-11-06 19:33:35.467774: step 50200, total loss = 1.44, predict loss = 0.37 (97.1 examples/sec; 0.041 sec/batch; 1h:08m:31s remains)
INFO - root - 2019-11-06 19:33:35.912347: step 50210, total loss = 1.64, predict loss = 0.45 (95.1 examples/sec; 0.042 sec/batch; 1h:09m:58s remains)
INFO - root - 2019-11-06 19:33:37.054487: step 50220, total loss = 0.99, predict loss = 0.26 (5.5 examples/sec; 0.726 sec/batch; 20h:07m:37s remains)
INFO - root - 2019-11-06 19:33:37.737453: step 50230, total loss = 1.81, predict loss = 0.51 (69.4 examples/sec; 0.058 sec/batch; 1h:35m:54s remains)
INFO - root - 2019-11-06 19:33:38.433753: step 50240, total loss = 1.36, predict loss = 0.39 (64.3 examples/sec; 0.062 sec/batch; 1h:43m:23s remains)
INFO - root - 2019-11-06 19:33:39.172953: step 50250, total loss = 1.89, predict loss = 0.56 (60.9 examples/sec; 0.066 sec/batch; 1h:49m:09s remains)
INFO - root - 2019-11-06 19:33:39.938905: step 50260, total loss = 1.79, predict loss = 0.54 (56.4 examples/sec; 0.071 sec/batch; 1h:57m:48s remains)
INFO - root - 2019-11-06 19:33:40.594226: step 50270, total loss = 2.00, predict loss = 0.53 (88.4 examples/sec; 0.045 sec/batch; 1h:15m:13s remains)
INFO - root - 2019-11-06 19:33:41.045046: step 50280, total loss = 1.43, predict loss = 0.38 (99.4 examples/sec; 0.040 sec/batch; 1h:06m:50s remains)
INFO - root - 2019-11-06 19:33:41.498813: step 50290, total loss = 1.63, predict loss = 0.50 (92.4 examples/sec; 0.043 sec/batch; 1h:11m:56s remains)
INFO - root - 2019-11-06 19:33:42.744747: step 50300, total loss = 1.30, predict loss = 0.40 (56.7 examples/sec; 0.071 sec/batch; 1h:57m:11s remains)
INFO - root - 2019-11-06 19:33:43.471033: step 50310, total loss = 1.38, predict loss = 0.34 (65.4 examples/sec; 0.061 sec/batch; 1h:41m:36s remains)
INFO - root - 2019-11-06 19:33:44.262845: step 50320, total loss = 1.36, predict loss = 0.41 (56.9 examples/sec; 0.070 sec/batch; 1h:56m:51s remains)
INFO - root - 2019-11-06 19:33:45.045253: step 50330, total loss = 1.99, predict loss = 0.57 (53.6 examples/sec; 0.075 sec/batch; 2h:03m:52s remains)
INFO - root - 2019-11-06 19:33:45.815762: step 50340, total loss = 1.17, predict loss = 0.33 (60.9 examples/sec; 0.066 sec/batch; 1h:49m:07s remains)
INFO - root - 2019-11-06 19:33:46.368751: step 50350, total loss = 0.82, predict loss = 0.22 (95.4 examples/sec; 0.042 sec/batch; 1h:09m:39s remains)
INFO - root - 2019-11-06 19:33:46.814304: step 50360, total loss = 0.86, predict loss = 0.21 (94.0 examples/sec; 0.043 sec/batch; 1h:10m:38s remains)
INFO - root - 2019-11-06 19:33:47.966896: step 50370, total loss = 1.49, predict loss = 0.40 (70.3 examples/sec; 0.057 sec/batch; 1h:34m:25s remains)
INFO - root - 2019-11-06 19:33:48.718265: step 50380, total loss = 1.00, predict loss = 0.31 (49.8 examples/sec; 0.080 sec/batch; 2h:13m:27s remains)
INFO - root - 2019-11-06 19:33:49.483470: step 50390, total loss = 2.87, predict loss = 0.85 (60.4 examples/sec; 0.066 sec/batch; 1h:50m:01s remains)
INFO - root - 2019-11-06 19:33:50.229483: step 50400, total loss = 0.68, predict loss = 0.16 (63.1 examples/sec; 0.063 sec/batch; 1h:45m:11s remains)
INFO - root - 2019-11-06 19:33:50.961304: step 50410, total loss = 0.95, predict loss = 0.25 (60.6 examples/sec; 0.066 sec/batch; 1h:49m:29s remains)
INFO - root - 2019-11-06 19:33:51.689445: step 50420, total loss = 0.70, predict loss = 0.18 (88.5 examples/sec; 0.045 sec/batch; 1h:15m:02s remains)
INFO - root - 2019-11-06 19:33:52.143332: step 50430, total loss = 1.14, predict loss = 0.28 (101.4 examples/sec; 0.039 sec/batch; 1h:05m:27s remains)
INFO - root - 2019-11-06 19:33:52.573636: step 50440, total loss = 1.24, predict loss = 0.30 (98.7 examples/sec; 0.041 sec/batch; 1h:07m:16s remains)
INFO - root - 2019-11-06 19:33:53.892977: step 50450, total loss = 2.22, predict loss = 0.75 (58.2 examples/sec; 0.069 sec/batch; 1h:53m:59s remains)
INFO - root - 2019-11-06 19:33:54.652141: step 50460, total loss = 1.60, predict loss = 0.44 (59.2 examples/sec; 0.068 sec/batch; 1h:52m:06s remains)
INFO - root - 2019-11-06 19:33:55.437999: step 50470, total loss = 1.18, predict loss = 0.31 (59.9 examples/sec; 0.067 sec/batch; 1h:50m:41s remains)
INFO - root - 2019-11-06 19:33:56.164805: step 50480, total loss = 1.05, predict loss = 0.25 (61.0 examples/sec; 0.066 sec/batch; 1h:48m:41s remains)
INFO - root - 2019-11-06 19:33:56.860860: step 50490, total loss = 1.15, predict loss = 0.31 (80.3 examples/sec; 0.050 sec/batch; 1h:22m:37s remains)
INFO - root - 2019-11-06 19:33:57.363367: step 50500, total loss = 1.89, predict loss = 0.57 (98.2 examples/sec; 0.041 sec/batch; 1h:07m:33s remains)
INFO - root - 2019-11-06 19:33:57.810885: step 50510, total loss = 1.56, predict loss = 0.42 (97.6 examples/sec; 0.041 sec/batch; 1h:07m:59s remains)
INFO - root - 2019-11-06 19:33:58.976702: step 50520, total loss = 1.17, predict loss = 0.29 (69.6 examples/sec; 0.057 sec/batch; 1h:35m:15s remains)
INFO - root - 2019-11-06 19:33:59.681729: step 50530, total loss = 1.73, predict loss = 0.54 (60.2 examples/sec; 0.066 sec/batch; 1h:50m:14s remains)
INFO - root - 2019-11-06 19:34:00.420303: step 50540, total loss = 1.15, predict loss = 0.29 (69.4 examples/sec; 0.058 sec/batch; 1h:35m:36s remains)
INFO - root - 2019-11-06 19:34:01.143285: step 50550, total loss = 1.32, predict loss = 0.37 (57.2 examples/sec; 0.070 sec/batch; 1h:55m:58s remains)
INFO - root - 2019-11-06 19:34:01.926187: step 50560, total loss = 1.87, predict loss = 0.58 (53.4 examples/sec; 0.075 sec/batch; 2h:04m:08s remains)
INFO - root - 2019-11-06 19:34:02.525219: step 50570, total loss = 1.48, predict loss = 0.43 (101.2 examples/sec; 0.040 sec/batch; 1h:05m:29s remains)
INFO - root - 2019-11-06 19:34:03.009573: step 50580, total loss = 1.88, predict loss = 0.54 (95.6 examples/sec; 0.042 sec/batch; 1h:09m:18s remains)
INFO - root - 2019-11-06 19:34:03.457154: step 50590, total loss = 1.02, predict loss = 0.26 (96.2 examples/sec; 0.042 sec/batch; 1h:08m:55s remains)
INFO - root - 2019-11-06 19:34:04.708828: step 50600, total loss = 2.04, predict loss = 0.61 (59.3 examples/sec; 0.067 sec/batch; 1h:51m:44s remains)
INFO - root - 2019-11-06 19:34:05.417096: step 50610, total loss = 0.90, predict loss = 0.22 (64.0 examples/sec; 0.062 sec/batch; 1h:43m:28s remains)
INFO - root - 2019-11-06 19:34:06.146927: step 50620, total loss = 1.47, predict loss = 0.39 (63.7 examples/sec; 0.063 sec/batch; 1h:43m:56s remains)
INFO - root - 2019-11-06 19:34:06.930567: step 50630, total loss = 1.27, predict loss = 0.30 (53.9 examples/sec; 0.074 sec/batch; 2h:03m:00s remains)
INFO - root - 2019-11-06 19:34:07.640923: step 50640, total loss = 1.23, predict loss = 0.31 (75.6 examples/sec; 0.053 sec/batch; 1h:27m:38s remains)
INFO - root - 2019-11-06 19:34:08.127715: step 50650, total loss = 0.90, predict loss = 0.21 (98.4 examples/sec; 0.041 sec/batch; 1h:07m:16s remains)
INFO - root - 2019-11-06 19:34:08.620634: step 50660, total loss = 1.58, predict loss = 0.45 (95.1 examples/sec; 0.042 sec/batch; 1h:09m:37s remains)
INFO - root - 2019-11-06 19:34:09.825605: step 50670, total loss = 1.25, predict loss = 0.33 (72.5 examples/sec; 0.055 sec/batch; 1h:31m:18s remains)
INFO - root - 2019-11-06 19:34:10.530458: step 50680, total loss = 1.75, predict loss = 0.47 (62.2 examples/sec; 0.064 sec/batch; 1h:46m:22s remains)
INFO - root - 2019-11-06 19:34:11.250757: step 50690, total loss = 0.96, predict loss = 0.24 (63.5 examples/sec; 0.063 sec/batch; 1h:44m:15s remains)
INFO - root - 2019-11-06 19:34:12.045769: step 50700, total loss = 1.29, predict loss = 0.34 (51.8 examples/sec; 0.077 sec/batch; 2h:07m:44s remains)
INFO - root - 2019-11-06 19:34:12.795283: step 50710, total loss = 1.41, predict loss = 0.38 (56.1 examples/sec; 0.071 sec/batch; 1h:58m:03s remains)
INFO - root - 2019-11-06 19:34:13.371845: step 50720, total loss = 2.22, predict loss = 0.69 (99.9 examples/sec; 0.040 sec/batch; 1h:06m:14s remains)
INFO - root - 2019-11-06 19:34:13.809129: step 50730, total loss = 1.22, predict loss = 0.34 (101.7 examples/sec; 0.039 sec/batch; 1h:05m:02s remains)
INFO - root - 2019-11-06 19:34:14.291666: step 50740, total loss = 1.43, predict loss = 0.34 (96.8 examples/sec; 0.041 sec/batch; 1h:08m:21s remains)
INFO - root - 2019-11-06 19:34:15.609198: step 50750, total loss = 0.69, predict loss = 0.18 (60.4 examples/sec; 0.066 sec/batch; 1h:49m:29s remains)
INFO - root - 2019-11-06 19:34:16.314953: step 50760, total loss = 1.50, predict loss = 0.42 (57.4 examples/sec; 0.070 sec/batch; 1h:55m:21s remains)
INFO - root - 2019-11-06 19:34:17.043894: step 50770, total loss = 1.11, predict loss = 0.30 (61.1 examples/sec; 0.066 sec/batch; 1h:48m:20s remains)
INFO - root - 2019-11-06 19:34:17.811029: step 50780, total loss = 1.71, predict loss = 0.47 (57.1 examples/sec; 0.070 sec/batch; 1h:55m:45s remains)
INFO - root - 2019-11-06 19:34:18.487121: step 50790, total loss = 1.07, predict loss = 0.28 (80.9 examples/sec; 0.049 sec/batch; 1h:21m:46s remains)
INFO - root - 2019-11-06 19:34:18.964731: step 50800, total loss = 0.86, predict loss = 0.22 (97.3 examples/sec; 0.041 sec/batch; 1h:07m:59s remains)
INFO - root - 2019-11-06 19:34:19.403209: step 50810, total loss = 0.71, predict loss = 0.19 (99.8 examples/sec; 0.040 sec/batch; 1h:06m:15s remains)
INFO - root - 2019-11-06 19:34:20.599070: step 50820, total loss = 1.92, predict loss = 0.53 (62.7 examples/sec; 0.064 sec/batch; 1h:45m:28s remains)
INFO - root - 2019-11-06 19:34:21.328051: step 50830, total loss = 1.18, predict loss = 0.31 (53.6 examples/sec; 0.075 sec/batch; 2h:03m:23s remains)
INFO - root - 2019-11-06 19:34:22.040499: step 50840, total loss = 1.28, predict loss = 0.34 (58.8 examples/sec; 0.068 sec/batch; 1h:52m:26s remains)
INFO - root - 2019-11-06 19:34:22.782459: step 50850, total loss = 1.55, predict loss = 0.47 (58.0 examples/sec; 0.069 sec/batch; 1h:54m:00s remains)
INFO - root - 2019-11-06 19:34:23.532777: step 50860, total loss = 1.91, predict loss = 0.59 (60.8 examples/sec; 0.066 sec/batch; 1h:48m:43s remains)
INFO - root - 2019-11-06 19:34:24.125376: step 50870, total loss = 0.99, predict loss = 0.26 (101.0 examples/sec; 0.040 sec/batch; 1h:05m:27s remains)
INFO - root - 2019-11-06 19:34:24.573187: step 50880, total loss = 1.50, predict loss = 0.42 (101.5 examples/sec; 0.039 sec/batch; 1h:05m:06s remains)
INFO - root - 2019-11-06 19:34:25.017408: step 50890, total loss = 0.67, predict loss = 0.19 (123.4 examples/sec; 0.032 sec/batch; 0h:53m:32s remains)
INFO - root - 2019-11-06 19:34:26.435870: step 50900, total loss = 1.90, predict loss = 0.60 (61.8 examples/sec; 0.065 sec/batch; 1h:46m:49s remains)
INFO - root - 2019-11-06 19:34:27.204512: step 50910, total loss = 1.69, predict loss = 0.49 (60.9 examples/sec; 0.066 sec/batch; 1h:48m:29s remains)
INFO - root - 2019-11-06 19:34:27.915198: step 50920, total loss = 0.91, predict loss = 0.23 (62.4 examples/sec; 0.064 sec/batch; 1h:45m:49s remains)
INFO - root - 2019-11-06 19:34:28.653436: step 50930, total loss = 1.10, predict loss = 0.32 (61.3 examples/sec; 0.065 sec/batch; 1h:47m:45s remains)
INFO - root - 2019-11-06 19:34:29.324134: step 50940, total loss = 1.84, predict loss = 0.49 (88.5 examples/sec; 0.045 sec/batch; 1h:14m:38s remains)
INFO - root - 2019-11-06 19:34:29.755844: step 50950, total loss = 0.70, predict loss = 0.18 (97.2 examples/sec; 0.041 sec/batch; 1h:07m:56s remains)
INFO - root - 2019-11-06 19:34:30.220357: step 50960, total loss = 1.60, predict loss = 0.45 (87.7 examples/sec; 0.046 sec/batch; 1h:15m:19s remains)
INFO - root - 2019-11-06 19:34:31.452209: step 50970, total loss = 1.08, predict loss = 0.27 (59.3 examples/sec; 0.067 sec/batch; 1h:51m:18s remains)
INFO - root - 2019-11-06 19:34:32.255627: step 50980, total loss = 1.47, predict loss = 0.44 (59.3 examples/sec; 0.067 sec/batch; 1h:51m:20s remains)
INFO - root - 2019-11-06 19:34:33.060669: step 50990, total loss = 1.87, predict loss = 0.56 (50.2 examples/sec; 0.080 sec/batch; 2h:11m:27s remains)
INFO - root - 2019-11-06 19:34:33.819409: step 51000, total loss = 1.42, predict loss = 0.36 (61.7 examples/sec; 0.065 sec/batch; 1h:46m:59s remains)
INFO - root - 2019-11-06 19:34:34.518281: step 51010, total loss = 1.10, predict loss = 0.22 (72.7 examples/sec; 0.055 sec/batch; 1h:30m:46s remains)
INFO - root - 2019-11-06 19:34:35.049020: step 51020, total loss = 1.47, predict loss = 0.42 (105.5 examples/sec; 0.038 sec/batch; 1h:02m:34s remains)
INFO - root - 2019-11-06 19:34:35.482333: step 51030, total loss = 1.12, predict loss = 0.29 (92.8 examples/sec; 0.043 sec/batch; 1h:11m:04s remains)
INFO - root - 2019-11-06 19:34:36.640378: step 51040, total loss = 1.67, predict loss = 0.50 (5.3 examples/sec; 0.753 sec/batch; 20h:41m:36s remains)
INFO - root - 2019-11-06 19:34:37.296630: step 51050, total loss = 1.84, predict loss = 0.59 (64.4 examples/sec; 0.062 sec/batch; 1h:42m:27s remains)
INFO - root - 2019-11-06 19:34:38.035482: step 51060, total loss = 0.95, predict loss = 0.20 (60.4 examples/sec; 0.066 sec/batch; 1h:49m:07s remains)
INFO - root - 2019-11-06 19:34:38.827648: step 51070, total loss = 0.94, predict loss = 0.22 (57.0 examples/sec; 0.070 sec/batch; 1h:55m:42s remains)
INFO - root - 2019-11-06 19:34:39.654328: step 51080, total loss = 1.17, predict loss = 0.29 (57.6 examples/sec; 0.069 sec/batch; 1h:54m:30s remains)
INFO - root - 2019-11-06 19:34:40.362014: step 51090, total loss = 0.69, predict loss = 0.18 (89.8 examples/sec; 0.045 sec/batch; 1h:13m:23s remains)
INFO - root - 2019-11-06 19:34:40.816986: step 51100, total loss = 0.83, predict loss = 0.24 (97.8 examples/sec; 0.041 sec/batch; 1h:07m:23s remains)
INFO - root - 2019-11-06 19:34:41.256748: step 51110, total loss = 0.84, predict loss = 0.23 (102.3 examples/sec; 0.039 sec/batch; 1h:04m:27s remains)
INFO - root - 2019-11-06 19:34:42.507176: step 51120, total loss = 1.22, predict loss = 0.32 (61.8 examples/sec; 0.065 sec/batch; 1h:46m:38s remains)
INFO - root - 2019-11-06 19:34:43.216918: step 51130, total loss = 1.12, predict loss = 0.30 (60.3 examples/sec; 0.066 sec/batch; 1h:49m:17s remains)
INFO - root - 2019-11-06 19:34:43.917388: step 51140, total loss = 1.56, predict loss = 0.45 (57.8 examples/sec; 0.069 sec/batch; 1h:53m:59s remains)
INFO - root - 2019-11-06 19:34:44.658503: step 51150, total loss = 1.52, predict loss = 0.44 (65.9 examples/sec; 0.061 sec/batch; 1h:40m:00s remains)
INFO - root - 2019-11-06 19:34:45.387958: step 51160, total loss = 1.59, predict loss = 0.43 (71.9 examples/sec; 0.056 sec/batch; 1h:31m:40s remains)
INFO - root - 2019-11-06 19:34:45.924958: step 51170, total loss = 1.95, predict loss = 0.55 (95.2 examples/sec; 0.042 sec/batch; 1h:09m:14s remains)
INFO - root - 2019-11-06 19:34:46.399109: step 51180, total loss = 1.40, predict loss = 0.36 (100.6 examples/sec; 0.040 sec/batch; 1h:05m:28s remains)
INFO - root - 2019-11-06 19:34:47.563858: step 51190, total loss = 1.30, predict loss = 0.40 (67.6 examples/sec; 0.059 sec/batch; 1h:37m:29s remains)
INFO - root - 2019-11-06 19:34:48.259601: step 51200, total loss = 1.28, predict loss = 0.35 (63.3 examples/sec; 0.063 sec/batch; 1h:44m:05s remains)
INFO - root - 2019-11-06 19:34:48.990473: step 51210, total loss = 0.81, predict loss = 0.20 (61.3 examples/sec; 0.065 sec/batch; 1h:47m:30s remains)
INFO - root - 2019-11-06 19:34:49.717140: step 51220, total loss = 1.28, predict loss = 0.33 (60.0 examples/sec; 0.067 sec/batch; 1h:49m:50s remains)
INFO - root - 2019-11-06 19:34:50.492083: step 51230, total loss = 1.37, predict loss = 0.38 (56.5 examples/sec; 0.071 sec/batch; 1h:56m:31s remains)
INFO - root - 2019-11-06 19:34:51.135134: step 51240, total loss = 1.03, predict loss = 0.25 (89.9 examples/sec; 0.044 sec/batch; 1h:13m:13s remains)
INFO - root - 2019-11-06 19:34:51.579438: step 51250, total loss = 0.74, predict loss = 0.18 (98.2 examples/sec; 0.041 sec/batch; 1h:07m:03s remains)
INFO - root - 2019-11-06 19:34:52.045955: step 51260, total loss = 1.45, predict loss = 0.45 (99.6 examples/sec; 0.040 sec/batch; 1h:06m:05s remains)
INFO - root - 2019-11-06 19:34:53.324059: step 51270, total loss = 1.32, predict loss = 0.37 (60.0 examples/sec; 0.067 sec/batch; 1h:49m:45s remains)
INFO - root - 2019-11-06 19:34:54.091861: step 51280, total loss = 0.69, predict loss = 0.18 (55.9 examples/sec; 0.072 sec/batch; 1h:57m:44s remains)
INFO - root - 2019-11-06 19:34:54.754875: step 51290, total loss = 1.60, predict loss = 0.46 (62.6 examples/sec; 0.064 sec/batch; 1h:45m:10s remains)
INFO - root - 2019-11-06 19:34:55.615296: step 51300, total loss = 0.66, predict loss = 0.21 (51.5 examples/sec; 0.078 sec/batch; 2h:07m:51s remains)
INFO - root - 2019-11-06 19:34:56.286872: step 51310, total loss = 1.12, predict loss = 0.29 (87.9 examples/sec; 0.046 sec/batch; 1h:14m:51s remains)
INFO - root - 2019-11-06 19:34:56.754333: step 51320, total loss = 1.62, predict loss = 0.41 (101.2 examples/sec; 0.040 sec/batch; 1h:04m:58s remains)
INFO - root - 2019-11-06 19:34:57.205849: step 51330, total loss = 1.73, predict loss = 0.46 (92.0 examples/sec; 0.043 sec/batch; 1h:11m:28s remains)
INFO - root - 2019-11-06 19:34:58.385394: step 51340, total loss = 1.01, predict loss = 0.30 (73.6 examples/sec; 0.054 sec/batch; 1h:29m:25s remains)
INFO - root - 2019-11-06 19:34:59.055803: step 51350, total loss = 2.40, predict loss = 0.72 (62.2 examples/sec; 0.064 sec/batch; 1h:45m:43s remains)
INFO - root - 2019-11-06 19:34:59.783012: step 51360, total loss = 0.65, predict loss = 0.16 (57.9 examples/sec; 0.069 sec/batch; 1h:53m:35s remains)
INFO - root - 2019-11-06 19:35:00.532191: step 51370, total loss = 1.10, predict loss = 0.28 (61.2 examples/sec; 0.065 sec/batch; 1h:47m:28s remains)
INFO - root - 2019-11-06 19:35:01.300247: step 51380, total loss = 1.27, predict loss = 0.32 (60.7 examples/sec; 0.066 sec/batch; 1h:48m:17s remains)
INFO - root - 2019-11-06 19:35:01.894244: step 51390, total loss = 1.80, predict loss = 0.50 (102.7 examples/sec; 0.039 sec/batch; 1h:03m:59s remains)
INFO - root - 2019-11-06 19:35:02.338581: step 51400, total loss = 1.77, predict loss = 0.54 (95.1 examples/sec; 0.042 sec/batch; 1h:09m:09s remains)
INFO - root - 2019-11-06 19:35:02.778541: step 51410, total loss = 0.89, predict loss = 0.24 (103.8 examples/sec; 0.039 sec/batch; 1h:03m:19s remains)
INFO - root - 2019-11-06 19:35:04.105057: step 51420, total loss = 0.92, predict loss = 0.22 (55.3 examples/sec; 0.072 sec/batch; 1h:58m:49s remains)
INFO - root - 2019-11-06 19:35:04.807226: step 51430, total loss = 1.04, predict loss = 0.26 (61.6 examples/sec; 0.065 sec/batch; 1h:46m:35s remains)
INFO - root - 2019-11-06 19:35:05.531661: step 51440, total loss = 0.84, predict loss = 0.23 (58.1 examples/sec; 0.069 sec/batch; 1h:53m:09s remains)
INFO - root - 2019-11-06 19:35:06.283965: step 51450, total loss = 1.42, predict loss = 0.41 (59.8 examples/sec; 0.067 sec/batch; 1h:49m:51s remains)
INFO - root - 2019-11-06 19:35:07.002821: step 51460, total loss = 2.31, predict loss = 0.66 (69.7 examples/sec; 0.057 sec/batch; 1h:34m:17s remains)
INFO - root - 2019-11-06 19:35:07.491842: step 51470, total loss = 0.95, predict loss = 0.25 (100.1 examples/sec; 0.040 sec/batch; 1h:05m:35s remains)
INFO - root - 2019-11-06 19:35:07.938885: step 51480, total loss = 0.89, predict loss = 0.24 (97.7 examples/sec; 0.041 sec/batch; 1h:07m:11s remains)
INFO - root - 2019-11-06 19:35:09.120008: step 51490, total loss = 1.01, predict loss = 0.26 (64.5 examples/sec; 0.062 sec/batch; 1h:41m:52s remains)
INFO - root - 2019-11-06 19:35:09.843026: step 51500, total loss = 0.87, predict loss = 0.27 (56.7 examples/sec; 0.071 sec/batch; 1h:55m:50s remains)
INFO - root - 2019-11-06 19:35:10.595834: step 51510, total loss = 1.24, predict loss = 0.38 (59.7 examples/sec; 0.067 sec/batch; 1h:50m:01s remains)
INFO - root - 2019-11-06 19:35:11.295320: step 51520, total loss = 2.06, predict loss = 0.61 (62.5 examples/sec; 0.064 sec/batch; 1h:45m:02s remains)
INFO - root - 2019-11-06 19:35:12.081264: step 51530, total loss = 1.35, predict loss = 0.37 (58.3 examples/sec; 0.069 sec/batch; 1h:52m:40s remains)
INFO - root - 2019-11-06 19:35:12.685765: step 51540, total loss = 1.07, predict loss = 0.31 (103.5 examples/sec; 0.039 sec/batch; 1h:03m:23s remains)
INFO - root - 2019-11-06 19:35:13.120665: step 51550, total loss = 1.14, predict loss = 0.30 (100.3 examples/sec; 0.040 sec/batch; 1h:05m:27s remains)
INFO - root - 2019-11-06 19:35:13.571540: step 51560, total loss = 0.99, predict loss = 0.29 (100.3 examples/sec; 0.040 sec/batch; 1h:05m:26s remains)
INFO - root - 2019-11-06 19:35:14.878851: step 51570, total loss = 1.26, predict loss = 0.37 (59.5 examples/sec; 0.067 sec/batch; 1h:50m:19s remains)
INFO - root - 2019-11-06 19:35:15.653098: step 51580, total loss = 0.96, predict loss = 0.24 (55.6 examples/sec; 0.072 sec/batch; 1h:58m:04s remains)
INFO - root - 2019-11-06 19:35:16.432178: step 51590, total loss = 0.97, predict loss = 0.24 (56.2 examples/sec; 0.071 sec/batch; 1h:56m:47s remains)
INFO - root - 2019-11-06 19:35:17.240263: step 51600, total loss = 1.93, predict loss = 0.55 (59.3 examples/sec; 0.067 sec/batch; 1h:50m:36s remains)
INFO - root - 2019-11-06 19:35:17.950914: step 51610, total loss = 1.22, predict loss = 0.32 (63.2 examples/sec; 0.063 sec/batch; 1h:43m:47s remains)
INFO - root - 2019-11-06 19:35:18.468848: step 51620, total loss = 1.47, predict loss = 0.39 (104.1 examples/sec; 0.038 sec/batch; 1h:02m:59s remains)
INFO - root - 2019-11-06 19:35:18.922965: step 51630, total loss = 1.54, predict loss = 0.37 (97.8 examples/sec; 0.041 sec/batch; 1h:07m:01s remains)
INFO - root - 2019-11-06 19:35:20.111113: step 51640, total loss = 0.82, predict loss = 0.20 (69.3 examples/sec; 0.058 sec/batch; 1h:34m:33s remains)
INFO - root - 2019-11-06 19:35:20.852116: step 51650, total loss = 1.91, predict loss = 0.53 (54.2 examples/sec; 0.074 sec/batch; 2h:00m:55s remains)
INFO - root - 2019-11-06 19:35:21.599346: step 51660, total loss = 1.37, predict loss = 0.35 (68.5 examples/sec; 0.058 sec/batch; 1h:35m:42s remains)
INFO - root - 2019-11-06 19:35:22.308157: step 51670, total loss = 1.06, predict loss = 0.29 (67.6 examples/sec; 0.059 sec/batch; 1h:36m:55s remains)
INFO - root - 2019-11-06 19:35:23.069800: step 51680, total loss = 1.12, predict loss = 0.27 (57.2 examples/sec; 0.070 sec/batch; 1h:54m:37s remains)
INFO - root - 2019-11-06 19:35:23.640954: step 51690, total loss = 1.14, predict loss = 0.29 (98.2 examples/sec; 0.041 sec/batch; 1h:06m:45s remains)
INFO - root - 2019-11-06 19:35:24.131652: step 51700, total loss = 0.92, predict loss = 0.24 (107.2 examples/sec; 0.037 sec/batch; 1h:01m:08s remains)
INFO - root - 2019-11-06 19:35:24.558535: step 51710, total loss = 1.16, predict loss = 0.29 (134.1 examples/sec; 0.030 sec/batch; 0h:48m:52s remains)
INFO - root - 2019-11-06 19:35:25.918927: step 51720, total loss = 0.90, predict loss = 0.24 (73.7 examples/sec; 0.054 sec/batch; 1h:28m:57s remains)
INFO - root - 2019-11-06 19:35:26.612260: step 51730, total loss = 1.32, predict loss = 0.35 (62.0 examples/sec; 0.064 sec/batch; 1h:45m:35s remains)
INFO - root - 2019-11-06 19:35:27.337062: step 51740, total loss = 1.23, predict loss = 0.39 (64.6 examples/sec; 0.062 sec/batch; 1h:41m:27s remains)
INFO - root - 2019-11-06 19:35:28.080758: step 51750, total loss = 1.04, predict loss = 0.32 (57.1 examples/sec; 0.070 sec/batch; 1h:54m:46s remains)
INFO - root - 2019-11-06 19:35:28.805331: step 51760, total loss = 1.06, predict loss = 0.27 (77.1 examples/sec; 0.052 sec/batch; 1h:24m:57s remains)
INFO - root - 2019-11-06 19:35:29.282353: step 51770, total loss = 0.75, predict loss = 0.18 (96.4 examples/sec; 0.041 sec/batch; 1h:07m:55s remains)
INFO - root - 2019-11-06 19:35:29.759786: step 51780, total loss = 1.14, predict loss = 0.31 (89.7 examples/sec; 0.045 sec/batch; 1h:12m:59s remains)
INFO - root - 2019-11-06 19:35:30.938564: step 51790, total loss = 0.83, predict loss = 0.23 (61.1 examples/sec; 0.066 sec/batch; 1h:47m:13s remains)
INFO - root - 2019-11-06 19:35:31.672089: step 51800, total loss = 0.82, predict loss = 0.23 (57.4 examples/sec; 0.070 sec/batch; 1h:54m:07s remains)
INFO - root - 2019-11-06 19:35:32.416224: step 51810, total loss = 1.45, predict loss = 0.36 (61.2 examples/sec; 0.065 sec/batch; 1h:46m:56s remains)
INFO - root - 2019-11-06 19:35:33.164638: step 51820, total loss = 1.07, predict loss = 0.24 (60.7 examples/sec; 0.066 sec/batch; 1h:47m:45s remains)
INFO - root - 2019-11-06 19:35:33.869902: step 51830, total loss = 2.25, predict loss = 0.69 (72.8 examples/sec; 0.055 sec/batch; 1h:29m:56s remains)
INFO - root - 2019-11-06 19:35:34.415531: step 51840, total loss = 0.93, predict loss = 0.26 (93.3 examples/sec; 0.043 sec/batch; 1h:10m:06s remains)
INFO - root - 2019-11-06 19:35:34.858775: step 51850, total loss = 1.50, predict loss = 0.44 (99.7 examples/sec; 0.040 sec/batch; 1h:05m:37s remains)
INFO - root - 2019-11-06 19:35:35.986794: step 51860, total loss = 1.83, predict loss = 0.49 (5.7 examples/sec; 0.704 sec/batch; 19h:11m:13s remains)
INFO - root - 2019-11-06 19:35:36.686409: step 51870, total loss = 1.11, predict loss = 0.29 (56.6 examples/sec; 0.071 sec/batch; 1h:55m:33s remains)
INFO - root - 2019-11-06 19:35:37.410001: step 51880, total loss = 1.96, predict loss = 0.56 (59.8 examples/sec; 0.067 sec/batch; 1h:49m:23s remains)
INFO - root - 2019-11-06 19:35:38.178969: step 51890, total loss = 1.54, predict loss = 0.49 (53.3 examples/sec; 0.075 sec/batch; 2h:02m:38s remains)
INFO - root - 2019-11-06 19:35:38.927710: step 51900, total loss = 0.91, predict loss = 0.25 (63.6 examples/sec; 0.063 sec/batch; 1h:42m:52s remains)
INFO - root - 2019-11-06 19:35:39.603157: step 51910, total loss = 0.76, predict loss = 0.18 (86.8 examples/sec; 0.046 sec/batch; 1h:15m:21s remains)
INFO - root - 2019-11-06 19:35:40.047310: step 51920, total loss = 1.96, predict loss = 0.59 (99.6 examples/sec; 0.040 sec/batch; 1h:05m:37s remains)
INFO - root - 2019-11-06 19:35:40.497580: step 51930, total loss = 1.38, predict loss = 0.37 (96.7 examples/sec; 0.041 sec/batch; 1h:07m:34s remains)
INFO - root - 2019-11-06 19:35:41.777145: step 51940, total loss = 1.25, predict loss = 0.32 (60.4 examples/sec; 0.066 sec/batch; 1h:48m:14s remains)
INFO - root - 2019-11-06 19:35:42.470479: step 51950, total loss = 1.56, predict loss = 0.45 (61.8 examples/sec; 0.065 sec/batch; 1h:45m:48s remains)
INFO - root - 2019-11-06 19:35:43.275400: step 51960, total loss = 0.96, predict loss = 0.24 (59.9 examples/sec; 0.067 sec/batch; 1h:49m:06s remains)
INFO - root - 2019-11-06 19:35:44.051152: step 51970, total loss = 1.45, predict loss = 0.39 (61.3 examples/sec; 0.065 sec/batch; 1h:46m:37s remains)
INFO - root - 2019-11-06 19:35:44.814371: step 51980, total loss = 1.13, predict loss = 0.30 (64.3 examples/sec; 0.062 sec/batch; 1h:41m:39s remains)
INFO - root - 2019-11-06 19:35:45.342927: step 51990, total loss = 1.34, predict loss = 0.34 (99.2 examples/sec; 0.040 sec/batch; 1h:05m:51s remains)
INFO - root - 2019-11-06 19:35:45.789394: step 52000, total loss = 1.32, predict loss = 0.38 (95.1 examples/sec; 0.042 sec/batch; 1h:08m:43s remains)
INFO - root - 2019-11-06 19:35:46.933044: step 52010, total loss = 1.22, predict loss = 0.35 (65.2 examples/sec; 0.061 sec/batch; 1h:40m:16s remains)
INFO - root - 2019-11-06 19:35:47.668947: step 52020, total loss = 0.97, predict loss = 0.24 (55.9 examples/sec; 0.072 sec/batch; 1h:56m:47s remains)
INFO - root - 2019-11-06 19:35:48.418288: step 52030, total loss = 0.93, predict loss = 0.23 (62.4 examples/sec; 0.064 sec/batch; 1h:44m:39s remains)
INFO - root - 2019-11-06 19:35:49.140832: step 52040, total loss = 1.13, predict loss = 0.28 (57.1 examples/sec; 0.070 sec/batch; 1h:54m:19s remains)
INFO - root - 2019-11-06 19:35:49.890682: step 52050, total loss = 1.04, predict loss = 0.28 (63.7 examples/sec; 0.063 sec/batch; 1h:42m:29s remains)
INFO - root - 2019-11-06 19:35:50.540074: step 52060, total loss = 1.48, predict loss = 0.44 (91.9 examples/sec; 0.044 sec/batch; 1h:11m:01s remains)
INFO - root - 2019-11-06 19:35:50.994912: step 52070, total loss = 0.57, predict loss = 0.18 (91.7 examples/sec; 0.044 sec/batch; 1h:11m:09s remains)
INFO - root - 2019-11-06 19:35:51.450967: step 52080, total loss = 1.43, predict loss = 0.43 (90.8 examples/sec; 0.044 sec/batch; 1h:11m:52s remains)
INFO - root - 2019-11-06 19:35:52.706482: step 52090, total loss = 0.80, predict loss = 0.20 (66.5 examples/sec; 0.060 sec/batch; 1h:38m:05s remains)
INFO - root - 2019-11-06 19:35:53.460771: step 52100, total loss = 1.26, predict loss = 0.33 (56.9 examples/sec; 0.070 sec/batch; 1h:54m:48s remains)
INFO - root - 2019-11-06 19:35:54.211168: step 52110, total loss = 1.23, predict loss = 0.33 (67.4 examples/sec; 0.059 sec/batch; 1h:36m:51s remains)
INFO - root - 2019-11-06 19:35:54.898209: step 52120, total loss = 1.41, predict loss = 0.40 (61.3 examples/sec; 0.065 sec/batch; 1h:46m:31s remains)
INFO - root - 2019-11-06 19:35:55.639886: step 52130, total loss = 1.20, predict loss = 0.28 (70.8 examples/sec; 0.056 sec/batch; 1h:32m:08s remains)
INFO - root - 2019-11-06 19:35:56.197231: step 52140, total loss = 1.02, predict loss = 0.28 (98.0 examples/sec; 0.041 sec/batch; 1h:06m:33s remains)
INFO - root - 2019-11-06 19:35:56.646119: step 52150, total loss = 0.88, predict loss = 0.24 (93.1 examples/sec; 0.043 sec/batch; 1h:10m:03s remains)
INFO - root - 2019-11-06 19:35:57.777872: step 52160, total loss = 0.87, predict loss = 0.20 (71.9 examples/sec; 0.056 sec/batch; 1h:30m:45s remains)
INFO - root - 2019-11-06 19:35:58.453001: step 52170, total loss = 1.34, predict loss = 0.41 (61.0 examples/sec; 0.066 sec/batch; 1h:46m:51s remains)
INFO - root - 2019-11-06 19:35:59.151439: step 52180, total loss = 1.01, predict loss = 0.26 (60.6 examples/sec; 0.066 sec/batch; 1h:47m:34s remains)
INFO - root - 2019-11-06 19:35:59.841557: step 52190, total loss = 0.90, predict loss = 0.23 (69.1 examples/sec; 0.058 sec/batch; 1h:34m:19s remains)
INFO - root - 2019-11-06 19:36:00.543347: step 52200, total loss = 0.86, predict loss = 0.26 (59.2 examples/sec; 0.068 sec/batch; 1h:50m:12s remains)
INFO - root - 2019-11-06 19:36:01.159757: step 52210, total loss = 1.88, predict loss = 0.53 (101.7 examples/sec; 0.039 sec/batch; 1h:04m:07s remains)
INFO - root - 2019-11-06 19:36:01.635752: step 52220, total loss = 1.70, predict loss = 0.48 (102.6 examples/sec; 0.039 sec/batch; 1h:03m:33s remains)
INFO - root - 2019-11-06 19:36:02.071165: step 52230, total loss = 1.38, predict loss = 0.43 (98.3 examples/sec; 0.041 sec/batch; 1h:06m:19s remains)
INFO - root - 2019-11-06 19:36:03.355140: step 52240, total loss = 1.59, predict loss = 0.47 (55.1 examples/sec; 0.073 sec/batch; 1h:58m:12s remains)
INFO - root - 2019-11-06 19:36:04.085398: step 52250, total loss = 0.76, predict loss = 0.23 (60.6 examples/sec; 0.066 sec/batch; 1h:47m:28s remains)
INFO - root - 2019-11-06 19:36:04.864134: step 52260, total loss = 1.72, predict loss = 0.43 (58.7 examples/sec; 0.068 sec/batch; 1h:50m:58s remains)
INFO - root - 2019-11-06 19:36:05.581054: step 52270, total loss = 1.53, predict loss = 0.42 (58.1 examples/sec; 0.069 sec/batch; 1h:52m:05s remains)
INFO - root - 2019-11-06 19:36:06.299549: step 52280, total loss = 1.18, predict loss = 0.33 (63.2 examples/sec; 0.063 sec/batch; 1h:43m:04s remains)
INFO - root - 2019-11-06 19:36:06.805375: step 52290, total loss = 0.74, predict loss = 0.18 (98.2 examples/sec; 0.041 sec/batch; 1h:06m:21s remains)
INFO - root - 2019-11-06 19:36:07.278235: step 52300, total loss = 1.31, predict loss = 0.38 (98.6 examples/sec; 0.041 sec/batch; 1h:06m:04s remains)
INFO - root - 2019-11-06 19:36:08.459886: step 52310, total loss = 1.65, predict loss = 0.54 (64.9 examples/sec; 0.062 sec/batch; 1h:40m:24s remains)
INFO - root - 2019-11-06 19:36:09.296839: step 52320, total loss = 1.41, predict loss = 0.40 (46.6 examples/sec; 0.086 sec/batch; 2h:19m:53s remains)
INFO - root - 2019-11-06 19:36:10.083448: step 52330, total loss = 1.60, predict loss = 0.52 (52.7 examples/sec; 0.076 sec/batch; 2h:03m:38s remains)
INFO - root - 2019-11-06 19:36:10.824746: step 52340, total loss = 1.01, predict loss = 0.26 (63.5 examples/sec; 0.063 sec/batch; 1h:42m:28s remains)
INFO - root - 2019-11-06 19:36:11.632723: step 52350, total loss = 1.70, predict loss = 0.48 (50.2 examples/sec; 0.080 sec/batch; 2h:09m:41s remains)
INFO - root - 2019-11-06 19:36:12.242607: step 52360, total loss = 0.98, predict loss = 0.26 (98.4 examples/sec; 0.041 sec/batch; 1h:06m:07s remains)
INFO - root - 2019-11-06 19:36:12.693883: step 52370, total loss = 1.69, predict loss = 0.48 (94.5 examples/sec; 0.042 sec/batch; 1h:08m:50s remains)
INFO - root - 2019-11-06 19:36:13.173603: step 52380, total loss = 0.90, predict loss = 0.22 (94.4 examples/sec; 0.042 sec/batch; 1h:08m:58s remains)
INFO - root - 2019-11-06 19:36:14.473444: step 52390, total loss = 0.84, predict loss = 0.22 (61.8 examples/sec; 0.065 sec/batch; 1h:45m:13s remains)
INFO - root - 2019-11-06 19:36:15.221345: step 52400, total loss = 1.37, predict loss = 0.34 (60.6 examples/sec; 0.066 sec/batch; 1h:47m:25s remains)
INFO - root - 2019-11-06 19:36:15.958675: step 52410, total loss = 1.78, predict loss = 0.58 (60.3 examples/sec; 0.066 sec/batch; 1h:47m:50s remains)
INFO - root - 2019-11-06 19:36:16.702034: step 52420, total loss = 0.86, predict loss = 0.22 (62.6 examples/sec; 0.064 sec/batch; 1h:43m:53s remains)
INFO - root - 2019-11-06 19:36:17.407595: step 52430, total loss = 1.47, predict loss = 0.41 (67.2 examples/sec; 0.060 sec/batch; 1h:36m:45s remains)
INFO - root - 2019-11-06 19:36:17.912823: step 52440, total loss = 1.12, predict loss = 0.36 (88.9 examples/sec; 0.045 sec/batch; 1h:13m:10s remains)
INFO - root - 2019-11-06 19:36:18.360768: step 52450, total loss = 1.67, predict loss = 0.44 (97.8 examples/sec; 0.041 sec/batch; 1h:06m:30s remains)
INFO - root - 2019-11-06 19:36:19.541956: step 52460, total loss = 1.28, predict loss = 0.32 (65.7 examples/sec; 0.061 sec/batch; 1h:38m:55s remains)
INFO - root - 2019-11-06 19:36:20.312909: step 52470, total loss = 1.29, predict loss = 0.31 (56.2 examples/sec; 0.071 sec/batch; 1h:55m:35s remains)
INFO - root - 2019-11-06 19:36:21.041014: step 52480, total loss = 1.18, predict loss = 0.30 (61.5 examples/sec; 0.065 sec/batch; 1h:45m:39s remains)
INFO - root - 2019-11-06 19:36:21.788362: step 52490, total loss = 1.98, predict loss = 0.63 (60.1 examples/sec; 0.067 sec/batch; 1h:48m:14s remains)
INFO - root - 2019-11-06 19:36:22.517529: step 52500, total loss = 1.15, predict loss = 0.30 (56.8 examples/sec; 0.070 sec/batch; 1h:54m:27s remains)
INFO - root - 2019-11-06 19:36:23.134623: step 52510, total loss = 1.50, predict loss = 0.42 (93.0 examples/sec; 0.043 sec/batch; 1h:09m:54s remains)
INFO - root - 2019-11-06 19:36:23.578019: step 52520, total loss = 0.91, predict loss = 0.22 (98.4 examples/sec; 0.041 sec/batch; 1h:06m:00s remains)
INFO - root - 2019-11-06 19:36:24.025746: step 52530, total loss = 1.30, predict loss = 0.38 (150.4 examples/sec; 0.027 sec/batch; 0h:43m:11s remains)
INFO - root - 2019-11-06 19:36:25.370681: step 52540, total loss = 1.22, predict loss = 0.36 (63.0 examples/sec; 0.064 sec/batch; 1h:43m:09s remains)
INFO - root - 2019-11-06 19:36:26.101219: step 52550, total loss = 1.26, predict loss = 0.34 (64.1 examples/sec; 0.062 sec/batch; 1h:41m:24s remains)
INFO - root - 2019-11-06 19:36:26.864797: step 52560, total loss = 1.36, predict loss = 0.40 (53.9 examples/sec; 0.074 sec/batch; 2h:00m:35s remains)
INFO - root - 2019-11-06 19:36:27.635163: step 52570, total loss = 1.25, predict loss = 0.33 (54.7 examples/sec; 0.073 sec/batch; 1h:58m:40s remains)
INFO - root - 2019-11-06 19:36:28.302124: step 52580, total loss = 1.11, predict loss = 0.29 (71.7 examples/sec; 0.056 sec/batch; 1h:30m:35s remains)
INFO - root - 2019-11-06 19:36:28.771835: step 52590, total loss = 0.52, predict loss = 0.13 (95.0 examples/sec; 0.042 sec/batch; 1h:08m:23s remains)
INFO - root - 2019-11-06 19:36:29.224511: step 52600, total loss = 0.82, predict loss = 0.20 (94.0 examples/sec; 0.043 sec/batch; 1h:09m:05s remains)
INFO - root - 2019-11-06 19:36:30.441924: step 52610, total loss = 0.84, predict loss = 0.23 (62.3 examples/sec; 0.064 sec/batch; 1h:44m:12s remains)
INFO - root - 2019-11-06 19:36:31.181166: step 52620, total loss = 1.44, predict loss = 0.39 (56.2 examples/sec; 0.071 sec/batch; 1h:55m:33s remains)
INFO - root - 2019-11-06 19:36:31.960387: step 52630, total loss = 0.97, predict loss = 0.24 (54.7 examples/sec; 0.073 sec/batch; 1h:58m:39s remains)
INFO - root - 2019-11-06 19:36:32.690973: step 52640, total loss = 0.78, predict loss = 0.20 (64.5 examples/sec; 0.062 sec/batch; 1h:40m:33s remains)
INFO - root - 2019-11-06 19:36:33.400971: step 52650, total loss = 0.83, predict loss = 0.22 (73.7 examples/sec; 0.054 sec/batch; 1h:28m:06s remains)
INFO - root - 2019-11-06 19:36:33.952169: step 52660, total loss = 1.36, predict loss = 0.38 (99.6 examples/sec; 0.040 sec/batch; 1h:05m:08s remains)
INFO - root - 2019-11-06 19:36:34.393674: step 52670, total loss = 1.37, predict loss = 0.38 (101.2 examples/sec; 0.040 sec/batch; 1h:04m:05s remains)
INFO - root - 2019-11-06 19:36:35.540169: step 52680, total loss = 1.69, predict loss = 0.48 (5.4 examples/sec; 0.740 sec/batch; 19h:59m:38s remains)
INFO - root - 2019-11-06 19:36:36.236403: step 52690, total loss = 1.76, predict loss = 0.59 (66.8 examples/sec; 0.060 sec/batch; 1h:37m:06s remains)
INFO - root - 2019-11-06 19:36:36.957072: step 52700, total loss = 1.04, predict loss = 0.26 (62.5 examples/sec; 0.064 sec/batch; 1h:43m:51s remains)
INFO - root - 2019-11-06 19:36:37.755818: step 52710, total loss = 1.42, predict loss = 0.34 (51.9 examples/sec; 0.077 sec/batch; 2h:05m:05s remains)
INFO - root - 2019-11-06 19:36:38.453960: step 52720, total loss = 1.23, predict loss = 0.36 (61.7 examples/sec; 0.065 sec/batch; 1h:45m:11s remains)
INFO - root - 2019-11-06 19:36:39.111447: step 52730, total loss = 0.78, predict loss = 0.17 (82.8 examples/sec; 0.048 sec/batch; 1h:18m:17s remains)
INFO - root - 2019-11-06 19:36:39.591217: step 52740, total loss = 1.65, predict loss = 0.46 (93.5 examples/sec; 0.043 sec/batch; 1h:09m:21s remains)
INFO - root - 2019-11-06 19:36:40.061051: step 52750, total loss = 1.72, predict loss = 0.52 (97.6 examples/sec; 0.041 sec/batch; 1h:06m:25s remains)
INFO - root - 2019-11-06 19:36:41.281008: step 52760, total loss = 0.72, predict loss = 0.19 (58.2 examples/sec; 0.069 sec/batch; 1h:51m:17s remains)
INFO - root - 2019-11-06 19:36:42.062297: step 52770, total loss = 1.40, predict loss = 0.41 (58.5 examples/sec; 0.068 sec/batch; 1h:50m:43s remains)
INFO - root - 2019-11-06 19:36:42.798954: step 52780, total loss = 0.81, predict loss = 0.20 (62.2 examples/sec; 0.064 sec/batch; 1h:44m:11s remains)
INFO - root - 2019-11-06 19:36:43.523554: step 52790, total loss = 1.25, predict loss = 0.33 (59.4 examples/sec; 0.067 sec/batch; 1h:49m:06s remains)
INFO - root - 2019-11-06 19:36:44.229082: step 52800, total loss = 1.34, predict loss = 0.39 (76.5 examples/sec; 0.052 sec/batch; 1h:24m:44s remains)
INFO - root - 2019-11-06 19:36:44.743283: step 52810, total loss = 0.99, predict loss = 0.27 (94.2 examples/sec; 0.042 sec/batch; 1h:08m:44s remains)
INFO - root - 2019-11-06 19:36:45.216121: step 52820, total loss = 1.50, predict loss = 0.48 (94.0 examples/sec; 0.043 sec/batch; 1h:08m:54s remains)
INFO - root - 2019-11-06 19:36:46.364570: step 52830, total loss = 1.26, predict loss = 0.32 (71.4 examples/sec; 0.056 sec/batch; 1h:30m:43s remains)
INFO - root - 2019-11-06 19:36:47.069264: step 52840, total loss = 0.90, predict loss = 0.23 (57.8 examples/sec; 0.069 sec/batch; 1h:52m:00s remains)
INFO - root - 2019-11-06 19:36:47.816954: step 52850, total loss = 1.21, predict loss = 0.32 (54.2 examples/sec; 0.074 sec/batch; 1h:59m:33s remains)
INFO - root - 2019-11-06 19:36:48.551990: step 52860, total loss = 1.39, predict loss = 0.44 (60.3 examples/sec; 0.066 sec/batch; 1h:47m:25s remains)
INFO - root - 2019-11-06 19:36:49.277062: step 52870, total loss = 1.18, predict loss = 0.29 (61.5 examples/sec; 0.065 sec/batch; 1h:45m:14s remains)
INFO - root - 2019-11-06 19:36:49.938389: step 52880, total loss = 0.51, predict loss = 0.14 (88.0 examples/sec; 0.045 sec/batch; 1h:13m:33s remains)
INFO - root - 2019-11-06 19:36:50.369532: step 52890, total loss = 1.28, predict loss = 0.36 (100.3 examples/sec; 0.040 sec/batch; 1h:04m:31s remains)
INFO - root - 2019-11-06 19:36:50.836428: step 52900, total loss = 1.33, predict loss = 0.38 (100.8 examples/sec; 0.040 sec/batch; 1h:04m:13s remains)
INFO - root - 2019-11-06 19:36:52.087066: step 52910, total loss = 1.21, predict loss = 0.33 (62.9 examples/sec; 0.064 sec/batch; 1h:42m:52s remains)
INFO - root - 2019-11-06 19:36:52.866243: step 52920, total loss = 1.51, predict loss = 0.43 (53.1 examples/sec; 0.075 sec/batch; 2h:01m:51s remains)
INFO - root - 2019-11-06 19:36:53.614575: step 52930, total loss = 1.13, predict loss = 0.27 (63.4 examples/sec; 0.063 sec/batch; 1h:42m:00s remains)
INFO - root - 2019-11-06 19:36:54.385819: step 52940, total loss = 1.14, predict loss = 0.28 (58.8 examples/sec; 0.068 sec/batch; 1h:49m:59s remains)
INFO - root - 2019-11-06 19:36:55.105550: step 52950, total loss = 0.74, predict loss = 0.22 (75.4 examples/sec; 0.053 sec/batch; 1h:25m:45s remains)
INFO - root - 2019-11-06 19:36:55.628039: step 52960, total loss = 1.15, predict loss = 0.31 (100.1 examples/sec; 0.040 sec/batch; 1h:04m:38s remains)
INFO - root - 2019-11-06 19:36:56.080695: step 52970, total loss = 1.39, predict loss = 0.40 (102.2 examples/sec; 0.039 sec/batch; 1h:03m:15s remains)
INFO - root - 2019-11-06 19:36:57.293367: step 52980, total loss = 0.75, predict loss = 0.20 (68.1 examples/sec; 0.059 sec/batch; 1h:35m:01s remains)
INFO - root - 2019-11-06 19:36:57.947016: step 52990, total loss = 1.89, predict loss = 0.52 (57.8 examples/sec; 0.069 sec/batch; 1h:51m:49s remains)
INFO - root - 2019-11-06 19:36:58.707563: step 53000, total loss = 1.18, predict loss = 0.30 (64.7 examples/sec; 0.062 sec/batch; 1h:39m:54s remains)
INFO - root - 2019-11-06 19:36:59.505526: step 53010, total loss = 1.22, predict loss = 0.33 (56.4 examples/sec; 0.071 sec/batch; 1h:54m:41s remains)
INFO - root - 2019-11-06 19:37:00.250363: step 53020, total loss = 1.37, predict loss = 0.33 (57.2 examples/sec; 0.070 sec/batch; 1h:53m:03s remains)
INFO - root - 2019-11-06 19:37:00.836625: step 53030, total loss = 1.62, predict loss = 0.48 (103.7 examples/sec; 0.039 sec/batch; 1h:02m:20s remains)
INFO - root - 2019-11-06 19:37:01.290049: step 53040, total loss = 1.07, predict loss = 0.32 (93.3 examples/sec; 0.043 sec/batch; 1h:09m:18s remains)
INFO - root - 2019-11-06 19:37:01.746816: step 53050, total loss = 1.27, predict loss = 0.34 (97.5 examples/sec; 0.041 sec/batch; 1h:06m:16s remains)
INFO - root - 2019-11-06 19:37:03.015453: step 53060, total loss = 1.12, predict loss = 0.30 (60.2 examples/sec; 0.066 sec/batch; 1h:47m:19s remains)
INFO - root - 2019-11-06 19:37:03.759886: step 53070, total loss = 1.07, predict loss = 0.28 (65.7 examples/sec; 0.061 sec/batch; 1h:38m:20s remains)
INFO - root - 2019-11-06 19:37:04.475042: step 53080, total loss = 0.67, predict loss = 0.19 (59.2 examples/sec; 0.068 sec/batch; 1h:49m:09s remains)
INFO - root - 2019-11-06 19:37:05.233875: step 53090, total loss = 1.03, predict loss = 0.30 (54.0 examples/sec; 0.074 sec/batch; 1h:59m:38s remains)
INFO - root - 2019-11-06 19:37:05.933256: step 53100, total loss = 1.35, predict loss = 0.35 (71.9 examples/sec; 0.056 sec/batch; 1h:29m:51s remains)
INFO - root - 2019-11-06 19:37:06.422320: step 53110, total loss = 2.16, predict loss = 0.61 (98.6 examples/sec; 0.041 sec/batch; 1h:05m:31s remains)
INFO - root - 2019-11-06 19:37:06.877153: step 53120, total loss = 1.00, predict loss = 0.25 (97.0 examples/sec; 0.041 sec/batch; 1h:06m:37s remains)
INFO - root - 2019-11-06 19:37:08.055286: step 53130, total loss = 1.03, predict loss = 0.23 (67.5 examples/sec; 0.059 sec/batch; 1h:35m:39s remains)
INFO - root - 2019-11-06 19:37:08.758220: step 53140, total loss = 1.54, predict loss = 0.45 (59.2 examples/sec; 0.068 sec/batch; 1h:48m:59s remains)
INFO - root - 2019-11-06 19:37:09.507390: step 53150, total loss = 1.42, predict loss = 0.40 (63.4 examples/sec; 0.063 sec/batch; 1h:41m:50s remains)
INFO - root - 2019-11-06 19:37:10.225226: step 53160, total loss = 2.04, predict loss = 0.68 (66.5 examples/sec; 0.060 sec/batch; 1h:37m:05s remains)
INFO - root - 2019-11-06 19:37:10.964178: step 53170, total loss = 1.14, predict loss = 0.31 (59.0 examples/sec; 0.068 sec/batch; 1h:49m:25s remains)
INFO - root - 2019-11-06 19:37:11.547623: step 53180, total loss = 0.91, predict loss = 0.23 (99.7 examples/sec; 0.040 sec/batch; 1h:04m:45s remains)
INFO - root - 2019-11-06 19:37:12.010052: step 53190, total loss = 1.49, predict loss = 0.42 (89.0 examples/sec; 0.045 sec/batch; 1h:12m:31s remains)
INFO - root - 2019-11-06 19:37:12.463284: step 53200, total loss = 0.88, predict loss = 0.22 (89.4 examples/sec; 0.045 sec/batch; 1h:12m:09s remains)
INFO - root - 2019-11-06 19:37:13.796873: step 53210, total loss = 0.73, predict loss = 0.18 (60.3 examples/sec; 0.066 sec/batch; 1h:47m:03s remains)
INFO - root - 2019-11-06 19:37:14.520429: step 53220, total loss = 0.58, predict loss = 0.14 (69.4 examples/sec; 0.058 sec/batch; 1h:32m:55s remains)
INFO - root - 2019-11-06 19:37:15.275462: step 53230, total loss = 1.10, predict loss = 0.28 (61.7 examples/sec; 0.065 sec/batch; 1h:44m:33s remains)
INFO - root - 2019-11-06 19:37:16.038371: step 53240, total loss = 1.07, predict loss = 0.30 (63.5 examples/sec; 0.063 sec/batch; 1h:41m:30s remains)
INFO - root - 2019-11-06 19:37:16.744629: step 53250, total loss = 1.18, predict loss = 0.33 (70.2 examples/sec; 0.057 sec/batch; 1h:31m:50s remains)
INFO - root - 2019-11-06 19:37:17.261654: step 53260, total loss = 1.54, predict loss = 0.43 (94.4 examples/sec; 0.042 sec/batch; 1h:08m:19s remains)
INFO - root - 2019-11-06 19:37:17.719742: step 53270, total loss = 0.80, predict loss = 0.21 (105.4 examples/sec; 0.038 sec/batch; 1h:01m:10s remains)
INFO - root - 2019-11-06 19:37:18.923813: step 53280, total loss = 1.81, predict loss = 0.53 (72.3 examples/sec; 0.055 sec/batch; 1h:29m:09s remains)
INFO - root - 2019-11-06 19:37:19.666042: step 53290, total loss = 1.49, predict loss = 0.43 (53.1 examples/sec; 0.075 sec/batch; 2h:01m:18s remains)
INFO - root - 2019-11-06 19:37:20.414269: step 53300, total loss = 0.86, predict loss = 0.23 (61.4 examples/sec; 0.065 sec/batch; 1h:45m:03s remains)
INFO - root - 2019-11-06 19:37:21.122553: step 53310, total loss = 0.92, predict loss = 0.24 (64.3 examples/sec; 0.062 sec/batch; 1h:40m:12s remains)
INFO - root - 2019-11-06 19:37:21.841454: step 53320, total loss = 0.68, predict loss = 0.15 (65.5 examples/sec; 0.061 sec/batch; 1h:38m:25s remains)
INFO - root - 2019-11-06 19:37:22.378543: step 53330, total loss = 1.16, predict loss = 0.28 (97.1 examples/sec; 0.041 sec/batch; 1h:06m:22s remains)
INFO - root - 2019-11-06 19:37:22.850009: step 53340, total loss = 1.34, predict loss = 0.39 (91.6 examples/sec; 0.044 sec/batch; 1h:10m:20s remains)
INFO - root - 2019-11-06 19:37:23.284765: step 53350, total loss = 0.66, predict loss = 0.18 (126.9 examples/sec; 0.032 sec/batch; 0h:50m:45s remains)
INFO - root - 2019-11-06 19:37:24.718290: step 53360, total loss = 0.83, predict loss = 0.22 (53.3 examples/sec; 0.075 sec/batch; 2h:00m:51s remains)
INFO - root - 2019-11-06 19:37:25.472402: step 53370, total loss = 0.80, predict loss = 0.20 (60.7 examples/sec; 0.066 sec/batch; 1h:46m:11s remains)
INFO - root - 2019-11-06 19:37:26.194005: step 53380, total loss = 1.14, predict loss = 0.28 (63.8 examples/sec; 0.063 sec/batch; 1h:40m:59s remains)
INFO - root - 2019-11-06 19:37:26.915976: step 53390, total loss = 1.24, predict loss = 0.34 (61.6 examples/sec; 0.065 sec/batch; 1h:44m:29s remains)
INFO - root - 2019-11-06 19:37:27.600037: step 53400, total loss = 1.83, predict loss = 0.52 (71.7 examples/sec; 0.056 sec/batch; 1h:29m:50s remains)
INFO - root - 2019-11-06 19:37:28.072271: step 53410, total loss = 1.76, predict loss = 0.44 (93.9 examples/sec; 0.043 sec/batch; 1h:08m:33s remains)
INFO - root - 2019-11-06 19:37:28.541889: step 53420, total loss = 0.50, predict loss = 0.14 (94.6 examples/sec; 0.042 sec/batch; 1h:08m:03s remains)
INFO - root - 2019-11-06 19:37:29.705321: step 53430, total loss = 1.19, predict loss = 0.33 (64.3 examples/sec; 0.062 sec/batch; 1h:40m:03s remains)
INFO - root - 2019-11-06 19:37:30.429177: step 53440, total loss = 1.23, predict loss = 0.32 (64.8 examples/sec; 0.062 sec/batch; 1h:39m:24s remains)
INFO - root - 2019-11-06 19:37:31.176370: step 53450, total loss = 0.63, predict loss = 0.16 (60.8 examples/sec; 0.066 sec/batch; 1h:45m:54s remains)
INFO - root - 2019-11-06 19:37:31.871244: step 53460, total loss = 1.04, predict loss = 0.29 (62.3 examples/sec; 0.064 sec/batch; 1h:43m:22s remains)
INFO - root - 2019-11-06 19:37:32.602740: step 53470, total loss = 0.64, predict loss = 0.16 (69.0 examples/sec; 0.058 sec/batch; 1h:33m:18s remains)
INFO - root - 2019-11-06 19:37:33.197080: step 53480, total loss = 1.75, predict loss = 0.54 (93.8 examples/sec; 0.043 sec/batch; 1h:08m:38s remains)
INFO - root - 2019-11-06 19:37:33.636473: step 53490, total loss = 1.55, predict loss = 0.41 (96.2 examples/sec; 0.042 sec/batch; 1h:06m:52s remains)
INFO - root - 2019-11-06 19:37:34.797891: step 53500, total loss = 1.78, predict loss = 0.57 (5.4 examples/sec; 0.738 sec/batch; 19h:47m:45s remains)
INFO - root - 2019-11-06 19:37:35.485834: step 53510, total loss = 1.03, predict loss = 0.27 (59.9 examples/sec; 0.067 sec/batch; 1h:47m:25s remains)
INFO - root - 2019-11-06 19:37:36.240443: step 53520, total loss = 0.84, predict loss = 0.20 (54.4 examples/sec; 0.074 sec/batch; 1h:58m:12s remains)
INFO - root - 2019-11-06 19:37:37.002556: step 53530, total loss = 1.02, predict loss = 0.24 (62.4 examples/sec; 0.064 sec/batch; 1h:43m:02s remains)
INFO - root - 2019-11-06 19:37:37.819062: step 53540, total loss = 1.40, predict loss = 0.45 (52.6 examples/sec; 0.076 sec/batch; 2h:02m:16s remains)
INFO - root - 2019-11-06 19:37:38.430033: step 53550, total loss = 0.78, predict loss = 0.20 (96.0 examples/sec; 0.042 sec/batch; 1h:06m:56s remains)
INFO - root - 2019-11-06 19:37:38.878120: step 53560, total loss = 0.80, predict loss = 0.24 (93.8 examples/sec; 0.043 sec/batch; 1h:08m:33s remains)
INFO - root - 2019-11-06 19:37:39.335876: step 53570, total loss = 1.09, predict loss = 0.29 (91.9 examples/sec; 0.044 sec/batch; 1h:09m:55s remains)
INFO - root - 2019-11-06 19:37:40.600741: step 53580, total loss = 1.37, predict loss = 0.38 (63.7 examples/sec; 0.063 sec/batch; 1h:40m:50s remains)
INFO - root - 2019-11-06 19:37:41.349953: step 53590, total loss = 1.46, predict loss = 0.38 (53.1 examples/sec; 0.075 sec/batch; 2h:00m:58s remains)
INFO - root - 2019-11-06 19:37:42.064681: step 53600, total loss = 1.49, predict loss = 0.41 (57.8 examples/sec; 0.069 sec/batch; 1h:51m:12s remains)
INFO - root - 2019-11-06 19:37:42.846372: step 53610, total loss = 1.34, predict loss = 0.37 (61.9 examples/sec; 0.065 sec/batch; 1h:43m:49s remains)
INFO - root - 2019-11-06 19:37:43.553385: step 53620, total loss = 0.91, predict loss = 0.24 (70.6 examples/sec; 0.057 sec/batch; 1h:31m:01s remains)
INFO - root - 2019-11-06 19:37:44.080768: step 53630, total loss = 1.56, predict loss = 0.47 (93.8 examples/sec; 0.043 sec/batch; 1h:08m:28s remains)
INFO - root - 2019-11-06 19:37:44.529148: step 53640, total loss = 0.82, predict loss = 0.20 (96.5 examples/sec; 0.041 sec/batch; 1h:06m:32s remains)
INFO - root - 2019-11-06 19:37:45.653068: step 53650, total loss = 1.63, predict loss = 0.48 (69.7 examples/sec; 0.057 sec/batch; 1h:32m:06s remains)
INFO - root - 2019-11-06 19:37:46.387226: step 53660, total loss = 1.07, predict loss = 0.27 (59.4 examples/sec; 0.067 sec/batch; 1h:48m:12s remains)
INFO - root - 2019-11-06 19:37:47.135221: step 53670, total loss = 2.74, predict loss = 0.90 (58.0 examples/sec; 0.069 sec/batch; 1h:50m:38s remains)
INFO - root - 2019-11-06 19:37:47.870078: step 53680, total loss = 1.97, predict loss = 0.61 (61.9 examples/sec; 0.065 sec/batch; 1h:43m:46s remains)
INFO - root - 2019-11-06 19:37:48.605998: step 53690, total loss = 0.91, predict loss = 0.23 (61.0 examples/sec; 0.066 sec/batch; 1h:45m:20s remains)
INFO - root - 2019-11-06 19:37:49.265990: step 53700, total loss = 1.13, predict loss = 0.26 (94.4 examples/sec; 0.042 sec/batch; 1h:08m:00s remains)
INFO - root - 2019-11-06 19:37:49.697012: step 53710, total loss = 1.12, predict loss = 0.31 (98.2 examples/sec; 0.041 sec/batch; 1h:05m:20s remains)
INFO - root - 2019-11-06 19:37:50.142092: step 53720, total loss = 1.15, predict loss = 0.30 (96.5 examples/sec; 0.041 sec/batch; 1h:06m:32s remains)
INFO - root - 2019-11-06 19:37:51.423878: step 53730, total loss = 1.23, predict loss = 0.36 (63.7 examples/sec; 0.063 sec/batch; 1h:40m:49s remains)
INFO - root - 2019-11-06 19:37:52.244942: step 53740, total loss = 0.79, predict loss = 0.23 (53.8 examples/sec; 0.074 sec/batch; 1h:59m:10s remains)
INFO - root - 2019-11-06 19:37:53.017070: step 53750, total loss = 0.71, predict loss = 0.17 (57.8 examples/sec; 0.069 sec/batch; 1h:50m:55s remains)
INFO - root - 2019-11-06 19:37:53.749927: step 53760, total loss = 1.28, predict loss = 0.38 (59.3 examples/sec; 0.067 sec/batch; 1h:48m:07s remains)
INFO - root - 2019-11-06 19:37:54.459912: step 53770, total loss = 2.09, predict loss = 0.56 (70.5 examples/sec; 0.057 sec/batch; 1h:31m:00s remains)
INFO - root - 2019-11-06 19:37:54.993512: step 53780, total loss = 0.77, predict loss = 0.19 (98.4 examples/sec; 0.041 sec/batch; 1h:05m:12s remains)
INFO - root - 2019-11-06 19:37:55.444053: step 53790, total loss = 1.33, predict loss = 0.38 (96.6 examples/sec; 0.041 sec/batch; 1h:06m:25s remains)
INFO - root - 2019-11-06 19:37:56.592958: step 53800, total loss = 0.92, predict loss = 0.23 (71.4 examples/sec; 0.056 sec/batch; 1h:29m:47s remains)
INFO - root - 2019-11-06 19:37:57.311119: step 53810, total loss = 1.41, predict loss = 0.42 (55.5 examples/sec; 0.072 sec/batch; 1h:55m:34s remains)
INFO - root - 2019-11-06 19:37:58.070495: step 53820, total loss = 1.17, predict loss = 0.29 (55.9 examples/sec; 0.072 sec/batch; 1h:54m:45s remains)
INFO - root - 2019-11-06 19:37:58.825352: step 53830, total loss = 1.58, predict loss = 0.41 (56.8 examples/sec; 0.070 sec/batch; 1h:52m:54s remains)
INFO - root - 2019-11-06 19:37:59.614633: step 53840, total loss = 1.60, predict loss = 0.42 (51.5 examples/sec; 0.078 sec/batch; 2h:04m:25s remains)
INFO - root - 2019-11-06 19:38:00.262359: step 53850, total loss = 1.58, predict loss = 0.42 (101.1 examples/sec; 0.040 sec/batch; 1h:03m:23s remains)
INFO - root - 2019-11-06 19:38:00.730302: step 53860, total loss = 0.60, predict loss = 0.11 (93.5 examples/sec; 0.043 sec/batch; 1h:08m:34s remains)
INFO - root - 2019-11-06 19:38:01.179758: step 53870, total loss = 1.73, predict loss = 0.56 (96.3 examples/sec; 0.042 sec/batch; 1h:06m:31s remains)
INFO - root - 2019-11-06 19:38:02.443266: step 53880, total loss = 0.85, predict loss = 0.24 (58.9 examples/sec; 0.068 sec/batch; 1h:48m:42s remains)
INFO - root - 2019-11-06 19:38:03.189050: step 53890, total loss = 1.03, predict loss = 0.22 (56.4 examples/sec; 0.071 sec/batch; 1h:53m:41s remains)
INFO - root - 2019-11-06 19:38:03.942777: step 53900, total loss = 0.75, predict loss = 0.20 (52.3 examples/sec; 0.076 sec/batch; 2h:02m:29s remains)
INFO - root - 2019-11-06 19:38:04.763270: step 53910, total loss = 1.20, predict loss = 0.35 (57.4 examples/sec; 0.070 sec/batch; 1h:51m:34s remains)
INFO - root - 2019-11-06 19:38:05.435024: step 53920, total loss = 1.65, predict loss = 0.46 (87.4 examples/sec; 0.046 sec/batch; 1h:13m:15s remains)
INFO - root - 2019-11-06 19:38:05.888508: step 53930, total loss = 1.41, predict loss = 0.40 (98.2 examples/sec; 0.041 sec/batch; 1h:05m:12s remains)
INFO - root - 2019-11-06 19:38:06.369422: step 53940, total loss = 0.74, predict loss = 0.20 (98.5 examples/sec; 0.041 sec/batch; 1h:05m:02s remains)
INFO - root - 2019-11-06 19:38:07.534545: step 53950, total loss = 1.32, predict loss = 0.37 (72.8 examples/sec; 0.055 sec/batch; 1h:27m:55s remains)
INFO - root - 2019-11-06 19:38:08.328374: step 53960, total loss = 1.43, predict loss = 0.38 (57.7 examples/sec; 0.069 sec/batch; 1h:51m:03s remains)
INFO - root - 2019-11-06 19:38:09.109074: step 53970, total loss = 1.64, predict loss = 0.49 (60.9 examples/sec; 0.066 sec/batch; 1h:45m:04s remains)
INFO - root - 2019-11-06 19:38:09.874650: step 53980, total loss = 0.97, predict loss = 0.23 (59.0 examples/sec; 0.068 sec/batch; 1h:48m:28s remains)
INFO - root - 2019-11-06 19:38:10.550593: step 53990, total loss = 0.75, predict loss = 0.18 (65.3 examples/sec; 0.061 sec/batch; 1h:38m:01s remains)
INFO - root - 2019-11-06 19:38:11.136776: step 54000, total loss = 0.55, predict loss = 0.15 (96.0 examples/sec; 0.042 sec/batch; 1h:06m:41s remains)
INFO - root - 2019-11-06 19:38:11.580622: step 54010, total loss = 2.30, predict loss = 0.76 (95.7 examples/sec; 0.042 sec/batch; 1h:06m:52s remains)
INFO - root - 2019-11-06 19:38:12.058886: step 54020, total loss = 0.85, predict loss = 0.23 (99.4 examples/sec; 0.040 sec/batch; 1h:04m:22s remains)
INFO - root - 2019-11-06 19:38:13.379048: step 54030, total loss = 1.17, predict loss = 0.31 (58.4 examples/sec; 0.068 sec/batch; 1h:49m:33s remains)
INFO - root - 2019-11-06 19:38:14.083516: step 54040, total loss = 0.92, predict loss = 0.25 (66.4 examples/sec; 0.060 sec/batch; 1h:36m:22s remains)
INFO - root - 2019-11-06 19:38:14.810723: step 54050, total loss = 2.03, predict loss = 0.64 (66.1 examples/sec; 0.061 sec/batch; 1h:36m:50s remains)
INFO - root - 2019-11-06 19:38:15.578330: step 54060, total loss = 1.03, predict loss = 0.25 (52.0 examples/sec; 0.077 sec/batch; 2h:02m:56s remains)
INFO - root - 2019-11-06 19:38:16.243029: step 54070, total loss = 1.17, predict loss = 0.31 (82.9 examples/sec; 0.048 sec/batch; 1h:17m:09s remains)
INFO - root - 2019-11-06 19:38:16.705105: step 54080, total loss = 1.08, predict loss = 0.28 (97.2 examples/sec; 0.041 sec/batch; 1h:05m:46s remains)
INFO - root - 2019-11-06 19:38:17.151856: step 54090, total loss = 0.72, predict loss = 0.18 (89.5 examples/sec; 0.045 sec/batch; 1h:11m:26s remains)
INFO - root - 2019-11-06 19:38:18.393981: step 54100, total loss = 0.76, predict loss = 0.19 (59.9 examples/sec; 0.067 sec/batch; 1h:46m:41s remains)
INFO - root - 2019-11-06 19:38:19.124520: step 54110, total loss = 1.33, predict loss = 0.34 (55.3 examples/sec; 0.072 sec/batch; 1h:55m:29s remains)
INFO - root - 2019-11-06 19:38:19.861348: step 54120, total loss = 1.28, predict loss = 0.35 (62.9 examples/sec; 0.064 sec/batch; 1h:41m:39s remains)
INFO - root - 2019-11-06 19:38:20.633243: step 54130, total loss = 1.09, predict loss = 0.29 (50.5 examples/sec; 0.079 sec/batch; 2h:06m:30s remains)
INFO - root - 2019-11-06 19:38:21.359813: step 54140, total loss = 1.01, predict loss = 0.25 (60.8 examples/sec; 0.066 sec/batch; 1h:45m:05s remains)
INFO - root - 2019-11-06 19:38:21.895411: step 54150, total loss = 1.16, predict loss = 0.30 (97.4 examples/sec; 0.041 sec/batch; 1h:05m:36s remains)
INFO - root - 2019-11-06 19:38:22.343708: step 54160, total loss = 1.23, predict loss = 0.31 (94.9 examples/sec; 0.042 sec/batch; 1h:07m:18s remains)
INFO - root - 2019-11-06 19:38:22.791860: step 54170, total loss = 1.57, predict loss = 0.47 (118.8 examples/sec; 0.034 sec/batch; 0h:53m:45s remains)
INFO - root - 2019-11-06 19:38:24.187806: step 54180, total loss = 1.48, predict loss = 0.45 (65.3 examples/sec; 0.061 sec/batch; 1h:37m:50s remains)
INFO - root - 2019-11-06 19:38:24.889596: step 54190, total loss = 0.87, predict loss = 0.22 (64.7 examples/sec; 0.062 sec/batch; 1h:38m:47s remains)
INFO - root - 2019-11-06 19:38:25.645999: step 54200, total loss = 0.93, predict loss = 0.26 (55.7 examples/sec; 0.072 sec/batch; 1h:54m:33s remains)
INFO - root - 2019-11-06 19:38:26.403154: step 54210, total loss = 1.27, predict loss = 0.31 (48.9 examples/sec; 0.082 sec/batch; 2h:10m:34s remains)
INFO - root - 2019-11-06 19:38:27.085556: step 54220, total loss = 1.02, predict loss = 0.27 (79.6 examples/sec; 0.050 sec/batch; 1h:20m:10s remains)
INFO - root - 2019-11-06 19:38:27.547943: step 54230, total loss = 1.42, predict loss = 0.36 (90.1 examples/sec; 0.044 sec/batch; 1h:10m:50s remains)
INFO - root - 2019-11-06 19:38:27.991966: step 54240, total loss = 1.04, predict loss = 0.29 (95.6 examples/sec; 0.042 sec/batch; 1h:06m:46s remains)
INFO - root - 2019-11-06 19:38:29.199378: step 54250, total loss = 1.69, predict loss = 0.46 (64.5 examples/sec; 0.062 sec/batch; 1h:39m:00s remains)
INFO - root - 2019-11-06 19:38:29.965800: step 54260, total loss = 1.02, predict loss = 0.24 (50.5 examples/sec; 0.079 sec/batch; 2h:06m:27s remains)
INFO - root - 2019-11-06 19:38:30.711124: step 54270, total loss = 0.68, predict loss = 0.18 (56.9 examples/sec; 0.070 sec/batch; 1h:52m:06s remains)
INFO - root - 2019-11-06 19:38:31.442771: step 54280, total loss = 1.16, predict loss = 0.25 (58.9 examples/sec; 0.068 sec/batch; 1h:48m:19s remains)
INFO - root - 2019-11-06 19:38:32.194052: step 54290, total loss = 0.88, predict loss = 0.21 (65.5 examples/sec; 0.061 sec/batch; 1h:37m:21s remains)
INFO - root - 2019-11-06 19:38:32.777492: step 54300, total loss = 0.94, predict loss = 0.31 (99.0 examples/sec; 0.040 sec/batch; 1h:04m:27s remains)
INFO - root - 2019-11-06 19:38:33.230155: step 54310, total loss = 0.93, predict loss = 0.25 (100.3 examples/sec; 0.040 sec/batch; 1h:03m:34s remains)
INFO - root - 2019-11-06 19:38:34.346292: step 54320, total loss = 1.41, predict loss = 0.45 (5.5 examples/sec; 0.725 sec/batch; 19h:15m:48s remains)
INFO - root - 2019-11-06 19:38:35.011501: step 54330, total loss = 0.76, predict loss = 0.21 (62.9 examples/sec; 0.064 sec/batch; 1h:41m:20s remains)
INFO - root - 2019-11-06 19:38:35.797472: step 54340, total loss = 0.75, predict loss = 0.17 (45.6 examples/sec; 0.088 sec/batch; 2h:19m:44s remains)
INFO - root - 2019-11-06 19:38:36.504151: step 54350, total loss = 0.55, predict loss = 0.14 (58.7 examples/sec; 0.068 sec/batch; 1h:48m:39s remains)
INFO - root - 2019-11-06 19:38:37.202149: step 54360, total loss = 0.62, predict loss = 0.16 (70.8 examples/sec; 0.057 sec/batch; 1h:30m:04s remains)
INFO - root - 2019-11-06 19:38:37.845616: step 54370, total loss = 0.97, predict loss = 0.23 (90.1 examples/sec; 0.044 sec/batch; 1h:10m:47s remains)
INFO - root - 2019-11-06 19:38:38.331633: step 54380, total loss = 1.80, predict loss = 0.57 (95.3 examples/sec; 0.042 sec/batch; 1h:06m:52s remains)
INFO - root - 2019-11-06 19:38:38.778523: step 54390, total loss = 0.94, predict loss = 0.24 (100.1 examples/sec; 0.040 sec/batch; 1h:03m:39s remains)
INFO - root - 2019-11-06 19:38:40.013315: step 54400, total loss = 1.15, predict loss = 0.33 (61.6 examples/sec; 0.065 sec/batch; 1h:43m:28s remains)
INFO - root - 2019-11-06 19:38:40.693324: step 54410, total loss = 1.09, predict loss = 0.30 (64.9 examples/sec; 0.062 sec/batch; 1h:38m:08s remains)
INFO - root - 2019-11-06 19:38:41.489765: step 54420, total loss = 0.80, predict loss = 0.24 (57.5 examples/sec; 0.070 sec/batch; 1h:50m:43s remains)
INFO - root - 2019-11-06 19:38:42.297534: step 54430, total loss = 0.63, predict loss = 0.14 (60.3 examples/sec; 0.066 sec/batch; 1h:45m:35s remains)
INFO - root - 2019-11-06 19:38:43.006066: step 54440, total loss = 1.37, predict loss = 0.37 (66.9 examples/sec; 0.060 sec/batch; 1h:35m:12s remains)
INFO - root - 2019-11-06 19:38:43.524021: step 54450, total loss = 0.78, predict loss = 0.22 (98.2 examples/sec; 0.041 sec/batch; 1h:04m:50s remains)
INFO - root - 2019-11-06 19:38:43.992946: step 54460, total loss = 1.26, predict loss = 0.34 (96.8 examples/sec; 0.041 sec/batch; 1h:05m:46s remains)
INFO - root - 2019-11-06 19:38:45.153364: step 54470, total loss = 0.71, predict loss = 0.19 (72.5 examples/sec; 0.055 sec/batch; 1h:27m:52s remains)
INFO - root - 2019-11-06 19:38:45.870154: step 54480, total loss = 1.64, predict loss = 0.50 (57.5 examples/sec; 0.070 sec/batch; 1h:50m:41s remains)
INFO - root - 2019-11-06 19:38:46.596606: step 54490, total loss = 0.77, predict loss = 0.18 (61.4 examples/sec; 0.065 sec/batch; 1h:43m:37s remains)
INFO - root - 2019-11-06 19:38:47.316154: step 54500, total loss = 0.60, predict loss = 0.14 (61.2 examples/sec; 0.065 sec/batch; 1h:44m:05s remains)
INFO - root - 2019-11-06 19:38:48.053492: step 54510, total loss = 1.81, predict loss = 0.48 (56.5 examples/sec; 0.071 sec/batch; 1h:52m:42s remains)
INFO - root - 2019-11-06 19:38:48.694515: step 54520, total loss = 0.78, predict loss = 0.20 (90.6 examples/sec; 0.044 sec/batch; 1h:10m:13s remains)
INFO - root - 2019-11-06 19:38:49.145665: step 54530, total loss = 0.65, predict loss = 0.17 (97.4 examples/sec; 0.041 sec/batch; 1h:05m:21s remains)
INFO - root - 2019-11-06 19:38:49.611504: step 54540, total loss = 1.29, predict loss = 0.34 (101.5 examples/sec; 0.039 sec/batch; 1h:02m:41s remains)
INFO - root - 2019-11-06 19:38:50.859762: step 54550, total loss = 0.76, predict loss = 0.19 (57.7 examples/sec; 0.069 sec/batch; 1h:50m:13s remains)
INFO - root - 2019-11-06 19:38:51.616602: step 54560, total loss = 1.75, predict loss = 0.54 (65.4 examples/sec; 0.061 sec/batch; 1h:37m:20s remains)
INFO - root - 2019-11-06 19:38:52.344524: step 54570, total loss = 0.87, predict loss = 0.23 (60.1 examples/sec; 0.067 sec/batch; 1h:45m:56s remains)
INFO - root - 2019-11-06 19:38:53.098805: step 54580, total loss = 1.19, predict loss = 0.33 (55.6 examples/sec; 0.072 sec/batch; 1h:54m:23s remains)
INFO - root - 2019-11-06 19:38:53.820691: step 54590, total loss = 1.13, predict loss = 0.33 (70.6 examples/sec; 0.057 sec/batch; 1h:30m:07s remains)
INFO - root - 2019-11-06 19:38:54.322791: step 54600, total loss = 1.07, predict loss = 0.29 (97.4 examples/sec; 0.041 sec/batch; 1h:05m:18s remains)
INFO - root - 2019-11-06 19:38:54.779354: step 54610, total loss = 0.66, predict loss = 0.18 (92.9 examples/sec; 0.043 sec/batch; 1h:08m:28s remains)
INFO - root - 2019-11-06 19:38:55.992326: step 54620, total loss = 1.18, predict loss = 0.28 (67.4 examples/sec; 0.059 sec/batch; 1h:34m:24s remains)
INFO - root - 2019-11-06 19:38:56.703880: step 54630, total loss = 0.73, predict loss = 0.18 (54.5 examples/sec; 0.073 sec/batch; 1h:56m:41s remains)
INFO - root - 2019-11-06 19:38:57.486342: step 54640, total loss = 0.82, predict loss = 0.20 (56.1 examples/sec; 0.071 sec/batch; 1h:53m:13s remains)
INFO - root - 2019-11-06 19:38:58.231486: step 54650, total loss = 1.28, predict loss = 0.33 (56.9 examples/sec; 0.070 sec/batch; 1h:51m:41s remains)
INFO - root - 2019-11-06 19:38:58.964186: step 54660, total loss = 1.18, predict loss = 0.28 (55.3 examples/sec; 0.072 sec/batch; 1h:54m:54s remains)
INFO - root - 2019-11-06 19:38:59.538544: step 54670, total loss = 1.11, predict loss = 0.28 (106.0 examples/sec; 0.038 sec/batch; 0h:59m:57s remains)
INFO - root - 2019-11-06 19:38:59.981856: step 54680, total loss = 0.81, predict loss = 0.20 (95.7 examples/sec; 0.042 sec/batch; 1h:06m:24s remains)
INFO - root - 2019-11-06 19:39:00.414435: step 54690, total loss = 1.18, predict loss = 0.33 (103.4 examples/sec; 0.039 sec/batch; 1h:01m:27s remains)
INFO - root - 2019-11-06 19:39:01.692498: step 54700, total loss = 0.76, predict loss = 0.19 (64.5 examples/sec; 0.062 sec/batch; 1h:38m:34s remains)
INFO - root - 2019-11-06 19:39:02.421806: step 54710, total loss = 0.55, predict loss = 0.13 (53.3 examples/sec; 0.075 sec/batch; 1h:59m:14s remains)
INFO - root - 2019-11-06 19:39:03.289304: step 54720, total loss = 1.47, predict loss = 0.48 (53.1 examples/sec; 0.075 sec/batch; 1h:59m:36s remains)
INFO - root - 2019-11-06 19:39:04.017779: step 54730, total loss = 0.73, predict loss = 0.17 (57.7 examples/sec; 0.069 sec/batch; 1h:50m:06s remains)
INFO - root - 2019-11-06 19:39:04.733656: step 54740, total loss = 1.57, predict loss = 0.45 (68.1 examples/sec; 0.059 sec/batch; 1h:33m:17s remains)
INFO - root - 2019-11-06 19:39:05.218205: step 54750, total loss = 2.19, predict loss = 0.69 (98.0 examples/sec; 0.041 sec/batch; 1h:04m:47s remains)
INFO - root - 2019-11-06 19:39:05.665753: step 54760, total loss = 1.26, predict loss = 0.38 (98.0 examples/sec; 0.041 sec/batch; 1h:04m:47s remains)
INFO - root - 2019-11-06 19:39:06.835227: step 54770, total loss = 0.49, predict loss = 0.13 (71.7 examples/sec; 0.056 sec/batch; 1h:28m:34s remains)
INFO - root - 2019-11-06 19:39:07.539545: step 54780, total loss = 0.87, predict loss = 0.23 (59.6 examples/sec; 0.067 sec/batch; 1h:46m:30s remains)
INFO - root - 2019-11-06 19:39:08.276973: step 54790, total loss = 0.58, predict loss = 0.12 (57.5 examples/sec; 0.070 sec/batch; 1h:50m:20s remains)
INFO - root - 2019-11-06 19:39:08.984470: step 54800, total loss = 1.01, predict loss = 0.27 (57.4 examples/sec; 0.070 sec/batch; 1h:50m:29s remains)
INFO - root - 2019-11-06 19:39:09.708771: step 54810, total loss = 0.72, predict loss = 0.19 (59.1 examples/sec; 0.068 sec/batch; 1h:47m:18s remains)
INFO - root - 2019-11-06 19:39:10.338067: step 54820, total loss = 1.03, predict loss = 0.27 (97.7 examples/sec; 0.041 sec/batch; 1h:04m:58s remains)
INFO - root - 2019-11-06 19:39:10.780417: step 54830, total loss = 2.39, predict loss = 0.80 (94.6 examples/sec; 0.042 sec/batch; 1h:07m:04s remains)
INFO - root - 2019-11-06 19:39:11.227603: step 54840, total loss = 1.48, predict loss = 0.42 (103.1 examples/sec; 0.039 sec/batch; 1h:01m:30s remains)
INFO - root - 2019-11-06 19:39:12.601575: step 54850, total loss = 1.18, predict loss = 0.30 (60.4 examples/sec; 0.066 sec/batch; 1h:45m:02s remains)
INFO - root - 2019-11-06 19:39:13.391674: step 54860, total loss = 1.76, predict loss = 0.54 (52.2 examples/sec; 0.077 sec/batch; 2h:01m:35s remains)
INFO - root - 2019-11-06 19:39:14.180573: step 54870, total loss = 1.19, predict loss = 0.33 (54.8 examples/sec; 0.073 sec/batch; 1h:55m:48s remains)
INFO - root - 2019-11-06 19:39:14.962305: step 54880, total loss = 1.05, predict loss = 0.28 (55.9 examples/sec; 0.072 sec/batch; 1h:53m:21s remains)
INFO - root - 2019-11-06 19:39:15.725715: step 54890, total loss = 1.80, predict loss = 0.53 (64.3 examples/sec; 0.062 sec/batch; 1h:38m:32s remains)
INFO - root - 2019-11-06 19:39:16.240306: step 54900, total loss = 1.98, predict loss = 0.54 (97.4 examples/sec; 0.041 sec/batch; 1h:05m:05s remains)
INFO - root - 2019-11-06 19:39:16.687970: step 54910, total loss = 0.91, predict loss = 0.26 (108.0 examples/sec; 0.037 sec/batch; 0h:58m:43s remains)
INFO - root - 2019-11-06 19:39:17.877991: step 54920, total loss = 0.84, predict loss = 0.25 (68.7 examples/sec; 0.058 sec/batch; 1h:32m:15s remains)
INFO - root - 2019-11-06 19:39:18.637071: step 54930, total loss = 1.15, predict loss = 0.30 (57.7 examples/sec; 0.069 sec/batch; 1h:49m:46s remains)
INFO - root - 2019-11-06 19:39:19.373898: step 54940, total loss = 1.00, predict loss = 0.24 (64.3 examples/sec; 0.062 sec/batch; 1h:38m:35s remains)
INFO - root - 2019-11-06 19:39:20.216493: step 54950, total loss = 1.10, predict loss = 0.29 (44.8 examples/sec; 0.089 sec/batch; 2h:21m:25s remains)
INFO - root - 2019-11-06 19:39:21.042067: step 54960, total loss = 1.83, predict loss = 0.58 (58.3 examples/sec; 0.069 sec/batch; 1h:48m:41s remains)
INFO - root - 2019-11-06 19:39:21.626405: step 54970, total loss = 0.92, predict loss = 0.22 (104.2 examples/sec; 0.038 sec/batch; 1h:00m:48s remains)
INFO - root - 2019-11-06 19:39:22.107273: step 54980, total loss = 1.36, predict loss = 0.36 (91.4 examples/sec; 0.044 sec/batch; 1h:09m:17s remains)
INFO - root - 2019-11-06 19:39:22.549048: step 54990, total loss = 1.02, predict loss = 0.25 (119.3 examples/sec; 0.034 sec/batch; 0h:53m:05s remains)
INFO - root - 2019-11-06 19:39:24.027949: step 55000, total loss = 0.87, predict loss = 0.20 (49.2 examples/sec; 0.081 sec/batch; 2h:08m:46s remains)
INFO - root - 2019-11-06 19:39:24.723125: step 55010, total loss = 1.82, predict loss = 0.61 (64.3 examples/sec; 0.062 sec/batch; 1h:38m:28s remains)
INFO - root - 2019-11-06 19:39:25.518471: step 55020, total loss = 1.57, predict loss = 0.52 (57.1 examples/sec; 0.070 sec/batch; 1h:50m:56s remains)
INFO - root - 2019-11-06 19:39:26.256009: step 55030, total loss = 1.34, predict loss = 0.36 (59.1 examples/sec; 0.068 sec/batch; 1h:47m:08s remains)
INFO - root - 2019-11-06 19:39:26.875354: step 55040, total loss = 0.71, predict loss = 0.18 (85.4 examples/sec; 0.047 sec/batch; 1h:14m:06s remains)
INFO - root - 2019-11-06 19:39:27.324462: step 55050, total loss = 1.67, predict loss = 0.50 (98.3 examples/sec; 0.041 sec/batch; 1h:04m:22s remains)
INFO - root - 2019-11-06 19:39:27.810601: step 55060, total loss = 1.00, predict loss = 0.26 (91.4 examples/sec; 0.044 sec/batch; 1h:09m:13s remains)
INFO - root - 2019-11-06 19:39:29.014524: step 55070, total loss = 0.76, predict loss = 0.19 (60.5 examples/sec; 0.066 sec/batch; 1h:44m:33s remains)
INFO - root - 2019-11-06 19:39:29.763606: step 55080, total loss = 1.30, predict loss = 0.35 (60.1 examples/sec; 0.067 sec/batch; 1h:45m:16s remains)
INFO - root - 2019-11-06 19:39:30.514231: step 55090, total loss = 1.07, predict loss = 0.30 (60.8 examples/sec; 0.066 sec/batch; 1h:44m:02s remains)
INFO - root - 2019-11-06 19:39:31.260954: step 55100, total loss = 0.49, predict loss = 0.12 (63.7 examples/sec; 0.063 sec/batch; 1h:39m:14s remains)
INFO - root - 2019-11-06 19:39:31.974082: step 55110, total loss = 0.70, predict loss = 0.16 (77.4 examples/sec; 0.052 sec/batch; 1h:21m:45s remains)
INFO - root - 2019-11-06 19:39:32.510464: step 55120, total loss = 1.43, predict loss = 0.41 (97.9 examples/sec; 0.041 sec/batch; 1h:04m:38s remains)
INFO - root - 2019-11-06 19:39:32.955224: step 55130, total loss = 0.93, predict loss = 0.22 (100.7 examples/sec; 0.040 sec/batch; 1h:02m:48s remains)
INFO - root - 2019-11-06 19:39:34.088245: step 55140, total loss = 0.91, predict loss = 0.24 (5.6 examples/sec; 0.710 sec/batch; 18h:43m:15s remains)
INFO - root - 2019-11-06 19:39:34.773512: step 55150, total loss = 1.04, predict loss = 0.30 (64.9 examples/sec; 0.062 sec/batch; 1h:37m:26s remains)
INFO - root - 2019-11-06 19:39:35.494878: step 55160, total loss = 0.93, predict loss = 0.24 (62.5 examples/sec; 0.064 sec/batch; 1h:41m:07s remains)
INFO - root - 2019-11-06 19:39:36.263192: step 55170, total loss = 1.28, predict loss = 0.30 (52.3 examples/sec; 0.077 sec/batch; 2h:00m:56s remains)
INFO - root - 2019-11-06 19:39:37.013375: step 55180, total loss = 1.01, predict loss = 0.24 (59.6 examples/sec; 0.067 sec/batch; 1h:46m:07s remains)
INFO - root - 2019-11-06 19:39:37.662004: step 55190, total loss = 0.89, predict loss = 0.23 (92.0 examples/sec; 0.044 sec/batch; 1h:08m:44s remains)
INFO - root - 2019-11-06 19:39:38.106372: step 55200, total loss = 0.87, predict loss = 0.23 (97.8 examples/sec; 0.041 sec/batch; 1h:04m:37s remains)
INFO - root - 2019-11-06 19:39:38.561722: step 55210, total loss = 1.00, predict loss = 0.22 (94.6 examples/sec; 0.042 sec/batch; 1h:06m:48s remains)
INFO - root - 2019-11-06 19:39:39.816369: step 55220, total loss = 0.46, predict loss = 0.12 (64.2 examples/sec; 0.062 sec/batch; 1h:38m:21s remains)
INFO - root - 2019-11-06 19:39:40.492144: step 55230, total loss = 0.59, predict loss = 0.15 (63.1 examples/sec; 0.063 sec/batch; 1h:40m:08s remains)
INFO - root - 2019-11-06 19:39:41.218091: step 55240, total loss = 0.43, predict loss = 0.12 (63.6 examples/sec; 0.063 sec/batch; 1h:39m:21s remains)
INFO - root - 2019-11-06 19:39:41.995447: step 55250, total loss = 1.42, predict loss = 0.42 (54.5 examples/sec; 0.073 sec/batch; 1h:55m:59s remains)
INFO - root - 2019-11-06 19:39:42.729717: step 55260, total loss = 1.52, predict loss = 0.46 (70.0 examples/sec; 0.057 sec/batch; 1h:30m:12s remains)
INFO - root - 2019-11-06 19:39:43.247491: step 55270, total loss = 1.13, predict loss = 0.32 (100.4 examples/sec; 0.040 sec/batch; 1h:02m:52s remains)
INFO - root - 2019-11-06 19:39:43.696760: step 55280, total loss = 1.03, predict loss = 0.27 (91.1 examples/sec; 0.044 sec/batch; 1h:09m:19s remains)
INFO - root - 2019-11-06 19:39:44.821520: step 55290, total loss = 0.62, predict loss = 0.17 (72.3 examples/sec; 0.055 sec/batch; 1h:27m:20s remains)
INFO - root - 2019-11-06 19:39:45.604403: step 55300, total loss = 1.76, predict loss = 0.48 (52.6 examples/sec; 0.076 sec/batch; 1h:59m:54s remains)
INFO - root - 2019-11-06 19:39:46.351492: step 55310, total loss = 1.18, predict loss = 0.35 (58.0 examples/sec; 0.069 sec/batch; 1h:48m:52s remains)
INFO - root - 2019-11-06 19:39:47.120310: step 55320, total loss = 0.96, predict loss = 0.27 (56.4 examples/sec; 0.071 sec/batch; 1h:51m:53s remains)
INFO - root - 2019-11-06 19:39:47.923923: step 55330, total loss = 0.78, predict loss = 0.20 (58.5 examples/sec; 0.068 sec/batch; 1h:47m:48s remains)
INFO - root - 2019-11-06 19:39:48.599645: step 55340, total loss = 0.64, predict loss = 0.17 (91.3 examples/sec; 0.044 sec/batch; 1h:09m:09s remains)
INFO - root - 2019-11-06 19:39:49.054085: step 55350, total loss = 1.13, predict loss = 0.33 (98.4 examples/sec; 0.041 sec/batch; 1h:04m:08s remains)
INFO - root - 2019-11-06 19:39:49.510445: step 55360, total loss = 1.27, predict loss = 0.35 (95.8 examples/sec; 0.042 sec/batch; 1h:05m:52s remains)
INFO - root - 2019-11-06 19:39:50.771141: step 55370, total loss = 1.59, predict loss = 0.49 (57.3 examples/sec; 0.070 sec/batch; 1h:50m:06s remains)
INFO - root - 2019-11-06 19:39:51.525502: step 55380, total loss = 1.00, predict loss = 0.31 (62.5 examples/sec; 0.064 sec/batch; 1h:40m:56s remains)
INFO - root - 2019-11-06 19:39:52.259638: step 55390, total loss = 0.79, predict loss = 0.19 (62.4 examples/sec; 0.064 sec/batch; 1h:41m:05s remains)
INFO - root - 2019-11-06 19:39:52.971397: step 55400, total loss = 0.48, predict loss = 0.12 (56.9 examples/sec; 0.070 sec/batch; 1h:50m:47s remains)
INFO - root - 2019-11-06 19:39:53.660757: step 55410, total loss = 1.56, predict loss = 0.48 (64.9 examples/sec; 0.062 sec/batch; 1h:37m:10s remains)
INFO - root - 2019-11-06 19:39:54.210480: step 55420, total loss = 1.45, predict loss = 0.40 (98.1 examples/sec; 0.041 sec/batch; 1h:04m:14s remains)
INFO - root - 2019-11-06 19:39:54.651760: step 55430, total loss = 0.71, predict loss = 0.18 (94.6 examples/sec; 0.042 sec/batch; 1h:06m:40s remains)
INFO - root - 2019-11-06 19:39:55.814221: step 55440, total loss = 1.07, predict loss = 0.29 (70.3 examples/sec; 0.057 sec/batch; 1h:29m:42s remains)
INFO - root - 2019-11-06 19:39:56.505786: step 55450, total loss = 1.22, predict loss = 0.32 (64.0 examples/sec; 0.062 sec/batch; 1h:38m:26s remains)
INFO - root - 2019-11-06 19:39:57.251215: step 55460, total loss = 0.59, predict loss = 0.15 (60.4 examples/sec; 0.066 sec/batch; 1h:44m:21s remains)
INFO - root - 2019-11-06 19:39:57.993193: step 55470, total loss = 1.13, predict loss = 0.30 (52.9 examples/sec; 0.076 sec/batch; 1h:59m:08s remains)
INFO - root - 2019-11-06 19:39:58.739649: step 55480, total loss = 0.64, predict loss = 0.17 (57.6 examples/sec; 0.069 sec/batch; 1h:49m:18s remains)
INFO - root - 2019-11-06 19:39:59.312429: step 55490, total loss = 0.69, predict loss = 0.17 (99.8 examples/sec; 0.040 sec/batch; 1h:03m:06s remains)
INFO - root - 2019-11-06 19:39:59.770508: step 55500, total loss = 0.57, predict loss = 0.15 (98.8 examples/sec; 0.040 sec/batch; 1h:03m:45s remains)
INFO - root - 2019-11-06 19:40:00.216091: step 55510, total loss = 0.87, predict loss = 0.22 (93.8 examples/sec; 0.043 sec/batch; 1h:07m:09s remains)
INFO - root - 2019-11-06 19:40:01.511969: step 55520, total loss = 0.80, predict loss = 0.21 (64.2 examples/sec; 0.062 sec/batch; 1h:38m:03s remains)
INFO - root - 2019-11-06 19:40:02.245859: step 55530, total loss = 0.90, predict loss = 0.28 (63.7 examples/sec; 0.063 sec/batch; 1h:38m:51s remains)
INFO - root - 2019-11-06 19:40:02.982552: step 55540, total loss = 0.74, predict loss = 0.16 (59.9 examples/sec; 0.067 sec/batch; 1h:45m:07s remains)
INFO - root - 2019-11-06 19:40:03.794429: step 55550, total loss = 0.67, predict loss = 0.14 (54.6 examples/sec; 0.073 sec/batch; 1h:55m:24s remains)
INFO - root - 2019-11-06 19:40:04.469481: step 55560, total loss = 0.93, predict loss = 0.26 (70.3 examples/sec; 0.057 sec/batch; 1h:29m:30s remains)
INFO - root - 2019-11-06 19:40:04.941153: step 55570, total loss = 0.74, predict loss = 0.18 (101.5 examples/sec; 0.039 sec/batch; 1h:02m:01s remains)
INFO - root - 2019-11-06 19:40:05.418829: step 55580, total loss = 0.65, predict loss = 0.18 (94.2 examples/sec; 0.042 sec/batch; 1h:06m:49s remains)
INFO - root - 2019-11-06 19:40:06.595448: step 55590, total loss = 1.42, predict loss = 0.42 (70.8 examples/sec; 0.057 sec/batch; 1h:28m:55s remains)
INFO - root - 2019-11-06 19:40:07.330788: step 55600, total loss = 0.82, predict loss = 0.22 (57.3 examples/sec; 0.070 sec/batch; 1h:49m:50s remains)
INFO - root - 2019-11-06 19:40:08.083207: step 55610, total loss = 0.99, predict loss = 0.29 (56.8 examples/sec; 0.070 sec/batch; 1h:50m:45s remains)
INFO - root - 2019-11-06 19:40:08.833453: step 55620, total loss = 0.97, predict loss = 0.26 (56.2 examples/sec; 0.071 sec/batch; 1h:51m:56s remains)
INFO - root - 2019-11-06 19:40:09.564206: step 55630, total loss = 1.13, predict loss = 0.26 (55.2 examples/sec; 0.072 sec/batch; 1h:54m:00s remains)
INFO - root - 2019-11-06 19:40:10.150427: step 55640, total loss = 0.87, predict loss = 0.21 (102.0 examples/sec; 0.039 sec/batch; 1h:01m:38s remains)
INFO - root - 2019-11-06 19:40:10.600682: step 55650, total loss = 0.61, predict loss = 0.13 (97.5 examples/sec; 0.041 sec/batch; 1h:04m:29s remains)
INFO - root - 2019-11-06 19:40:11.079350: step 55660, total loss = 1.29, predict loss = 0.37 (99.8 examples/sec; 0.040 sec/batch; 1h:03m:01s remains)
INFO - root - 2019-11-06 19:40:12.411329: step 55670, total loss = 1.08, predict loss = 0.28 (54.2 examples/sec; 0.074 sec/batch; 1h:55m:58s remains)
INFO - root - 2019-11-06 19:40:13.240964: step 55680, total loss = 2.02, predict loss = 0.60 (54.3 examples/sec; 0.074 sec/batch; 1h:55m:46s remains)
INFO - root - 2019-11-06 19:40:13.976155: step 55690, total loss = 0.80, predict loss = 0.20 (65.7 examples/sec; 0.061 sec/batch; 1h:35m:43s remains)
INFO - root - 2019-11-06 19:40:14.747207: step 55700, total loss = 1.53, predict loss = 0.45 (60.9 examples/sec; 0.066 sec/batch; 1h:43m:12s remains)
INFO - root - 2019-11-06 19:40:15.445292: step 55710, total loss = 0.66, predict loss = 0.17 (72.8 examples/sec; 0.055 sec/batch; 1h:26m:22s remains)
INFO - root - 2019-11-06 19:40:15.938618: step 55720, total loss = 0.92, predict loss = 0.23 (95.8 examples/sec; 0.042 sec/batch; 1h:05m:37s remains)
INFO - root - 2019-11-06 19:40:16.377296: step 55730, total loss = 0.75, predict loss = 0.19 (90.2 examples/sec; 0.044 sec/batch; 1h:09m:41s remains)
INFO - root - 2019-11-06 19:40:17.552967: step 55740, total loss = 1.00, predict loss = 0.28 (66.0 examples/sec; 0.061 sec/batch; 1h:35m:12s remains)
INFO - root - 2019-11-06 19:40:18.247500: step 55750, total loss = 0.50, predict loss = 0.12 (63.2 examples/sec; 0.063 sec/batch; 1h:39m:26s remains)
INFO - root - 2019-11-06 19:40:19.033278: step 55760, total loss = 0.67, predict loss = 0.13 (59.6 examples/sec; 0.067 sec/batch; 1h:45m:28s remains)
INFO - root - 2019-11-06 19:40:19.802196: step 55770, total loss = 1.07, predict loss = 0.28 (56.6 examples/sec; 0.071 sec/batch; 1h:50m:58s remains)
INFO - root - 2019-11-06 19:40:20.536900: step 55780, total loss = 0.99, predict loss = 0.27 (65.9 examples/sec; 0.061 sec/batch; 1h:35m:19s remains)
INFO - root - 2019-11-06 19:40:21.084805: step 55790, total loss = 0.73, predict loss = 0.19 (91.5 examples/sec; 0.044 sec/batch; 1h:08m:37s remains)
INFO - root - 2019-11-06 19:40:21.540122: step 55800, total loss = 0.88, predict loss = 0.21 (92.3 examples/sec; 0.043 sec/batch; 1h:08m:02s remains)
INFO - root - 2019-11-06 19:40:21.980125: step 55810, total loss = 0.81, predict loss = 0.23 (124.2 examples/sec; 0.032 sec/batch; 0h:50m:32s remains)
INFO - root - 2019-11-06 19:40:23.331780: step 55820, total loss = 0.81, predict loss = 0.21 (53.4 examples/sec; 0.075 sec/batch; 1h:57m:29s remains)
INFO - root - 2019-11-06 19:40:24.124418: step 55830, total loss = 0.80, predict loss = 0.23 (70.2 examples/sec; 0.057 sec/batch; 1h:29m:25s remains)
INFO - root - 2019-11-06 19:40:24.829750: step 55840, total loss = 1.69, predict loss = 0.50 (58.9 examples/sec; 0.068 sec/batch; 1h:46m:35s remains)
INFO - root - 2019-11-06 19:40:25.566904: step 55850, total loss = 1.00, predict loss = 0.26 (64.7 examples/sec; 0.062 sec/batch; 1h:36m:57s remains)
INFO - root - 2019-11-06 19:40:26.280461: step 55860, total loss = 0.92, predict loss = 0.27 (81.7 examples/sec; 0.049 sec/batch; 1h:16m:46s remains)
INFO - root - 2019-11-06 19:40:26.739575: step 55870, total loss = 0.71, predict loss = 0.17 (92.5 examples/sec; 0.043 sec/batch; 1h:07m:49s remains)
INFO - root - 2019-11-06 19:40:27.190626: step 55880, total loss = 0.65, predict loss = 0.17 (99.6 examples/sec; 0.040 sec/batch; 1h:02m:59s remains)
INFO - root - 2019-11-06 19:40:28.452908: step 55890, total loss = 1.46, predict loss = 0.43 (58.3 examples/sec; 0.069 sec/batch; 1h:47m:35s remains)
INFO - root - 2019-11-06 19:40:29.170712: step 55900, total loss = 1.55, predict loss = 0.47 (64.6 examples/sec; 0.062 sec/batch; 1h:37m:08s remains)
INFO - root - 2019-11-06 19:40:29.900285: step 55910, total loss = 1.27, predict loss = 0.38 (61.9 examples/sec; 0.065 sec/batch; 1h:41m:21s remains)
INFO - root - 2019-11-06 19:40:30.601353: step 55920, total loss = 1.00, predict loss = 0.25 (64.8 examples/sec; 0.062 sec/batch; 1h:36m:43s remains)
INFO - root - 2019-11-06 19:40:31.321716: step 55930, total loss = 0.51, predict loss = 0.11 (62.0 examples/sec; 0.064 sec/batch; 1h:41m:05s remains)
INFO - root - 2019-11-06 19:40:31.876227: step 55940, total loss = 0.62, predict loss = 0.20 (97.4 examples/sec; 0.041 sec/batch; 1h:04m:22s remains)
INFO - root - 2019-11-06 19:40:32.335219: step 55950, total loss = 1.41, predict loss = 0.43 (99.3 examples/sec; 0.040 sec/batch; 1h:03m:08s remains)
INFO - root - 2019-11-06 19:40:33.470525: step 55960, total loss = 0.85, predict loss = 0.23 (5.5 examples/sec; 0.730 sec/batch; 19h:03m:50s remains)
INFO - root - 2019-11-06 19:40:34.135504: step 55970, total loss = 1.67, predict loss = 0.47 (66.8 examples/sec; 0.060 sec/batch; 1h:33m:53s remains)
INFO - root - 2019-11-06 19:40:34.877284: step 55980, total loss = 0.81, predict loss = 0.20 (64.6 examples/sec; 0.062 sec/batch; 1h:37m:05s remains)
INFO - root - 2019-11-06 19:40:35.621605: step 55990, total loss = 0.96, predict loss = 0.25 (61.2 examples/sec; 0.065 sec/batch; 1h:42m:28s remains)
INFO - root - 2019-11-06 19:40:36.399410: step 56000, total loss = 0.90, predict loss = 0.23 (58.7 examples/sec; 0.068 sec/batch; 1h:46m:44s remains)
INFO - root - 2019-11-06 19:40:37.069215: step 56010, total loss = 1.14, predict loss = 0.29 (90.1 examples/sec; 0.044 sec/batch; 1h:09m:32s remains)
INFO - root - 2019-11-06 19:40:37.539920: step 56020, total loss = 1.29, predict loss = 0.35 (98.4 examples/sec; 0.041 sec/batch; 1h:03m:40s remains)
INFO - root - 2019-11-06 19:40:37.982479: step 56030, total loss = 1.87, predict loss = 0.55 (99.2 examples/sec; 0.040 sec/batch; 1h:03m:09s remains)
INFO - root - 2019-11-06 19:40:39.242463: step 56040, total loss = 0.64, predict loss = 0.16 (65.2 examples/sec; 0.061 sec/batch; 1h:36m:03s remains)
INFO - root - 2019-11-06 19:40:39.955999: step 56050, total loss = 1.32, predict loss = 0.36 (65.6 examples/sec; 0.061 sec/batch; 1h:35m:28s remains)
INFO - root - 2019-11-06 19:40:40.673268: step 56060, total loss = 1.55, predict loss = 0.43 (61.2 examples/sec; 0.065 sec/batch; 1h:42m:22s remains)
INFO - root - 2019-11-06 19:40:41.415299: step 56070, total loss = 1.21, predict loss = 0.33 (61.2 examples/sec; 0.065 sec/batch; 1h:42m:15s remains)
INFO - root - 2019-11-06 19:40:42.133636: step 56080, total loss = 1.91, predict loss = 0.61 (73.1 examples/sec; 0.055 sec/batch; 1h:25m:38s remains)
INFO - root - 2019-11-06 19:40:42.655368: step 56090, total loss = 0.76, predict loss = 0.19 (95.3 examples/sec; 0.042 sec/batch; 1h:05m:43s remains)
INFO - root - 2019-11-06 19:40:43.128271: step 56100, total loss = 1.04, predict loss = 0.30 (91.6 examples/sec; 0.044 sec/batch; 1h:08m:18s remains)
INFO - root - 2019-11-06 19:40:44.260383: step 56110, total loss = 0.66, predict loss = 0.16 (69.1 examples/sec; 0.058 sec/batch; 1h:30m:32s remains)
INFO - root - 2019-11-06 19:40:44.922625: step 56120, total loss = 0.73, predict loss = 0.16 (58.9 examples/sec; 0.068 sec/batch; 1h:46m:12s remains)
INFO - root - 2019-11-06 19:40:45.688738: step 56130, total loss = 1.05, predict loss = 0.26 (56.1 examples/sec; 0.071 sec/batch; 1h:51m:30s remains)
INFO - root - 2019-11-06 19:40:46.450950: step 56140, total loss = 0.53, predict loss = 0.13 (55.5 examples/sec; 0.072 sec/batch; 1h:52m:39s remains)
INFO - root - 2019-11-06 19:40:47.208182: step 56150, total loss = 0.67, predict loss = 0.16 (57.7 examples/sec; 0.069 sec/batch; 1h:48m:28s remains)
INFO - root - 2019-11-06 19:40:47.848793: step 56160, total loss = 0.88, predict loss = 0.20 (92.7 examples/sec; 0.043 sec/batch; 1h:07m:29s remains)
INFO - root - 2019-11-06 19:40:48.294012: step 56170, total loss = 0.82, predict loss = 0.22 (97.9 examples/sec; 0.041 sec/batch; 1h:03m:55s remains)
INFO - root - 2019-11-06 19:40:48.772876: step 56180, total loss = 1.21, predict loss = 0.37 (98.2 examples/sec; 0.041 sec/batch; 1h:03m:43s remains)
INFO - root - 2019-11-06 19:40:50.044860: step 56190, total loss = 1.48, predict loss = 0.34 (55.2 examples/sec; 0.072 sec/batch; 1h:53m:18s remains)
INFO - root - 2019-11-06 19:40:50.760785: step 56200, total loss = 1.05, predict loss = 0.30 (60.5 examples/sec; 0.066 sec/batch; 1h:43m:23s remains)
INFO - root - 2019-11-06 19:40:51.513094: step 56210, total loss = 0.77, predict loss = 0.19 (64.1 examples/sec; 0.062 sec/batch; 1h:37m:29s remains)
INFO - root - 2019-11-06 19:40:52.271000: step 56220, total loss = 0.48, predict loss = 0.12 (53.7 examples/sec; 0.075 sec/batch; 1h:56m:30s remains)
INFO - root - 2019-11-06 19:40:53.016675: step 56230, total loss = 0.86, predict loss = 0.22 (66.2 examples/sec; 0.060 sec/batch; 1h:34m:27s remains)
INFO - root - 2019-11-06 19:40:53.554925: step 56240, total loss = 0.81, predict loss = 0.21 (94.7 examples/sec; 0.042 sec/batch; 1h:06m:00s remains)
INFO - root - 2019-11-06 19:40:54.016355: step 56250, total loss = 1.05, predict loss = 0.29 (99.5 examples/sec; 0.040 sec/batch; 1h:02m:47s remains)
INFO - root - 2019-11-06 19:40:55.191752: step 56260, total loss = 0.68, predict loss = 0.16 (60.6 examples/sec; 0.066 sec/batch; 1h:43m:11s remains)
INFO - root - 2019-11-06 19:40:55.907146: step 56270, total loss = 1.28, predict loss = 0.39 (56.5 examples/sec; 0.071 sec/batch; 1h:50m:37s remains)
INFO - root - 2019-11-06 19:40:56.631986: step 56280, total loss = 1.12, predict loss = 0.30 (66.1 examples/sec; 0.060 sec/batch; 1h:34m:29s remains)
INFO - root - 2019-11-06 19:40:57.398288: step 56290, total loss = 1.48, predict loss = 0.43 (54.8 examples/sec; 0.073 sec/batch; 1h:53m:56s remains)
INFO - root - 2019-11-06 19:40:58.123001: step 56300, total loss = 1.14, predict loss = 0.32 (57.1 examples/sec; 0.070 sec/batch; 1h:49m:24s remains)
INFO - root - 2019-11-06 19:40:58.719094: step 56310, total loss = 1.48, predict loss = 0.47 (103.0 examples/sec; 0.039 sec/batch; 1h:00m:39s remains)
INFO - root - 2019-11-06 19:40:59.174811: step 56320, total loss = 1.30, predict loss = 0.40 (94.2 examples/sec; 0.042 sec/batch; 1h:06m:18s remains)
INFO - root - 2019-11-06 19:40:59.641530: step 56330, total loss = 1.00, predict loss = 0.28 (100.4 examples/sec; 0.040 sec/batch; 1h:02m:10s remains)
INFO - root - 2019-11-06 19:41:00.887088: step 56340, total loss = 0.97, predict loss = 0.27 (67.7 examples/sec; 0.059 sec/batch; 1h:32m:12s remains)
INFO - root - 2019-11-06 19:41:01.644933: step 56350, total loss = 0.61, predict loss = 0.15 (53.1 examples/sec; 0.075 sec/batch; 1h:57m:29s remains)
INFO - root - 2019-11-06 19:41:02.374291: step 56360, total loss = 1.37, predict loss = 0.38 (56.2 examples/sec; 0.071 sec/batch; 1h:51m:06s remains)
INFO - root - 2019-11-06 19:41:03.219710: step 56370, total loss = 1.29, predict loss = 0.34 (51.0 examples/sec; 0.078 sec/batch; 2h:02m:17s remains)
INFO - root - 2019-11-06 19:41:03.921052: step 56380, total loss = 1.26, predict loss = 0.36 (75.6 examples/sec; 0.053 sec/batch; 1h:22m:34s remains)
INFO - root - 2019-11-06 19:41:04.370888: step 56390, total loss = 1.13, predict loss = 0.30 (96.9 examples/sec; 0.041 sec/batch; 1h:04m:24s remains)
INFO - root - 2019-11-06 19:41:04.823650: step 56400, total loss = 0.85, predict loss = 0.23 (94.7 examples/sec; 0.042 sec/batch; 1h:05m:52s remains)
INFO - root - 2019-11-06 19:41:06.017831: step 56410, total loss = 0.74, predict loss = 0.20 (66.1 examples/sec; 0.060 sec/batch; 1h:34m:22s remains)
INFO - root - 2019-11-06 19:41:06.788263: step 56420, total loss = 1.26, predict loss = 0.36 (49.5 examples/sec; 0.081 sec/batch; 2h:06m:07s remains)
INFO - root - 2019-11-06 19:41:07.509451: step 56430, total loss = 1.21, predict loss = 0.34 (61.5 examples/sec; 0.065 sec/batch; 1h:41m:25s remains)
INFO - root - 2019-11-06 19:41:08.265752: step 56440, total loss = 2.14, predict loss = 0.70 (51.9 examples/sec; 0.077 sec/batch; 2h:00m:14s remains)
INFO - root - 2019-11-06 19:41:09.100412: step 56450, total loss = 0.98, predict loss = 0.26 (45.8 examples/sec; 0.087 sec/batch; 2h:16m:07s remains)
INFO - root - 2019-11-06 19:41:09.693048: step 56460, total loss = 0.98, predict loss = 0.27 (101.5 examples/sec; 0.039 sec/batch; 1h:01m:24s remains)
INFO - root - 2019-11-06 19:41:10.146634: step 56470, total loss = 0.64, predict loss = 0.16 (93.6 examples/sec; 0.043 sec/batch; 1h:06m:35s remains)
INFO - root - 2019-11-06 19:41:10.597901: step 56480, total loss = 1.22, predict loss = 0.36 (98.7 examples/sec; 0.041 sec/batch; 1h:03m:10s remains)
INFO - root - 2019-11-06 19:41:11.955963: step 56490, total loss = 0.96, predict loss = 0.25 (58.8 examples/sec; 0.068 sec/batch; 1h:45m:59s remains)
INFO - root - 2019-11-06 19:41:12.760483: step 56500, total loss = 0.66, predict loss = 0.16 (53.9 examples/sec; 0.074 sec/batch; 1h:55m:41s remains)
INFO - root - 2019-11-06 19:41:13.544218: step 56510, total loss = 1.55, predict loss = 0.46 (53.2 examples/sec; 0.075 sec/batch; 1h:57m:11s remains)
INFO - root - 2019-11-06 19:41:14.275720: step 56520, total loss = 0.86, predict loss = 0.21 (66.7 examples/sec; 0.060 sec/batch; 1h:33m:22s remains)
INFO - root - 2019-11-06 19:41:14.967400: step 56530, total loss = 1.20, predict loss = 0.33 (66.4 examples/sec; 0.060 sec/batch; 1h:33m:47s remains)
INFO - root - 2019-11-06 19:41:15.487888: step 56540, total loss = 1.17, predict loss = 0.33 (97.8 examples/sec; 0.041 sec/batch; 1h:03m:40s remains)
INFO - root - 2019-11-06 19:41:15.936885: step 56550, total loss = 1.53, predict loss = 0.41 (97.8 examples/sec; 0.041 sec/batch; 1h:03m:41s remains)
INFO - root - 2019-11-06 19:41:17.200811: step 56560, total loss = 0.64, predict loss = 0.16 (69.8 examples/sec; 0.057 sec/batch; 1h:29m:12s remains)
INFO - root - 2019-11-06 19:41:17.883849: step 56570, total loss = 0.94, predict loss = 0.25 (62.4 examples/sec; 0.064 sec/batch; 1h:39m:48s remains)
INFO - root - 2019-11-06 19:41:18.588226: step 56580, total loss = 1.71, predict loss = 0.50 (64.3 examples/sec; 0.062 sec/batch; 1h:36m:54s remains)
INFO - root - 2019-11-06 19:41:19.328755: step 56590, total loss = 1.05, predict loss = 0.30 (54.7 examples/sec; 0.073 sec/batch; 1h:53m:50s remains)
INFO - root - 2019-11-06 19:41:20.025661: step 56600, total loss = 1.15, predict loss = 0.37 (67.3 examples/sec; 0.059 sec/batch; 1h:32m:30s remains)
INFO - root - 2019-11-06 19:41:20.559204: step 56610, total loss = 1.99, predict loss = 0.59 (101.3 examples/sec; 0.039 sec/batch; 1h:01m:25s remains)
INFO - root - 2019-11-06 19:41:21.046360: step 56620, total loss = 2.00, predict loss = 0.59 (99.6 examples/sec; 0.040 sec/batch; 1h:02m:31s remains)
INFO - root - 2019-11-06 19:41:21.478752: step 56630, total loss = 0.65, predict loss = 0.19 (145.3 examples/sec; 0.028 sec/batch; 0h:42m:50s remains)
INFO - root - 2019-11-06 19:41:22.780215: step 56640, total loss = 0.61, predict loss = 0.15 (59.2 examples/sec; 0.068 sec/batch; 1h:45m:06s remains)
INFO - root - 2019-11-06 19:41:23.491441: step 56650, total loss = 0.88, predict loss = 0.23 (70.6 examples/sec; 0.057 sec/batch; 1h:28m:05s remains)
INFO - root - 2019-11-06 19:41:24.230641: step 56660, total loss = 1.23, predict loss = 0.31 (69.6 examples/sec; 0.058 sec/batch; 1h:29m:27s remains)
INFO - root - 2019-11-06 19:41:24.897352: step 56670, total loss = 1.06, predict loss = 0.30 (64.9 examples/sec; 0.062 sec/batch; 1h:35m:50s remains)
INFO - root - 2019-11-06 19:41:25.588063: step 56680, total loss = 1.44, predict loss = 0.42 (75.0 examples/sec; 0.053 sec/batch; 1h:22m:55s remains)
INFO - root - 2019-11-06 19:41:26.076714: step 56690, total loss = 1.58, predict loss = 0.56 (97.4 examples/sec; 0.041 sec/batch; 1h:03m:53s remains)
INFO - root - 2019-11-06 19:41:26.546214: step 56700, total loss = 0.96, predict loss = 0.28 (97.8 examples/sec; 0.041 sec/batch; 1h:03m:34s remains)
INFO - root - 2019-11-06 19:41:27.737262: step 56710, total loss = 0.61, predict loss = 0.17 (66.2 examples/sec; 0.060 sec/batch; 1h:33m:54s remains)
INFO - root - 2019-11-06 19:41:28.488905: step 56720, total loss = 1.46, predict loss = 0.40 (56.7 examples/sec; 0.071 sec/batch; 1h:49m:42s remains)
INFO - root - 2019-11-06 19:41:29.256212: step 56730, total loss = 1.23, predict loss = 0.35 (55.9 examples/sec; 0.072 sec/batch; 1h:51m:12s remains)
INFO - root - 2019-11-06 19:41:30.006883: step 56740, total loss = 0.56, predict loss = 0.15 (55.5 examples/sec; 0.072 sec/batch; 1h:52m:04s remains)
INFO - root - 2019-11-06 19:41:30.728641: step 56750, total loss = 1.09, predict loss = 0.30 (67.0 examples/sec; 0.060 sec/batch; 1h:32m:43s remains)
INFO - root - 2019-11-06 19:41:31.258926: step 56760, total loss = 0.75, predict loss = 0.20 (98.3 examples/sec; 0.041 sec/batch; 1h:03m:13s remains)
INFO - root - 2019-11-06 19:41:31.708548: step 56770, total loss = 0.77, predict loss = 0.20 (98.5 examples/sec; 0.041 sec/batch; 1h:03m:05s remains)
INFO - root - 2019-11-06 19:41:32.840488: step 56780, total loss = 1.73, predict loss = 0.50 (5.6 examples/sec; 0.720 sec/batch; 18h:38m:06s remains)
INFO - root - 2019-11-06 19:41:33.557364: step 56790, total loss = 0.70, predict loss = 0.21 (56.0 examples/sec; 0.071 sec/batch; 1h:50m:53s remains)
INFO - root - 2019-11-06 19:41:34.334487: step 56800, total loss = 1.58, predict loss = 0.48 (58.8 examples/sec; 0.068 sec/batch; 1h:45m:37s remains)
INFO - root - 2019-11-06 19:41:35.134223: step 56810, total loss = 0.52, predict loss = 0.14 (52.5 examples/sec; 0.076 sec/batch; 1h:58m:13s remains)
INFO - root - 2019-11-06 19:41:35.858058: step 56820, total loss = 0.60, predict loss = 0.16 (71.5 examples/sec; 0.056 sec/batch; 1h:26m:54s remains)
INFO - root - 2019-11-06 19:41:36.518051: step 56830, total loss = 1.14, predict loss = 0.30 (91.2 examples/sec; 0.044 sec/batch; 1h:08m:06s remains)
INFO - root - 2019-11-06 19:41:36.966759: step 56840, total loss = 0.55, predict loss = 0.14 (97.7 examples/sec; 0.041 sec/batch; 1h:03m:33s remains)
INFO - root - 2019-11-06 19:41:37.400766: step 56850, total loss = 0.78, predict loss = 0.20 (101.8 examples/sec; 0.039 sec/batch; 1h:01m:00s remains)
INFO - root - 2019-11-06 19:41:38.679228: step 56860, total loss = 0.92, predict loss = 0.27 (63.4 examples/sec; 0.063 sec/batch; 1h:37m:58s remains)
INFO - root - 2019-11-06 19:41:39.359897: step 56870, total loss = 0.90, predict loss = 0.24 (62.6 examples/sec; 0.064 sec/batch; 1h:39m:14s remains)
INFO - root - 2019-11-06 19:41:40.050151: step 56880, total loss = 0.55, predict loss = 0.15 (57.2 examples/sec; 0.070 sec/batch; 1h:48m:27s remains)
INFO - root - 2019-11-06 19:41:40.845634: step 56890, total loss = 1.00, predict loss = 0.30 (50.2 examples/sec; 0.080 sec/batch; 2h:03m:41s remains)
INFO - root - 2019-11-06 19:41:41.560599: step 56900, total loss = 0.61, predict loss = 0.17 (72.0 examples/sec; 0.056 sec/batch; 1h:26m:10s remains)
INFO - root - 2019-11-06 19:41:42.086137: step 56910, total loss = 0.94, predict loss = 0.25 (97.7 examples/sec; 0.041 sec/batch; 1h:03m:30s remains)
INFO - root - 2019-11-06 19:41:42.535011: step 56920, total loss = 1.13, predict loss = 0.31 (92.7 examples/sec; 0.043 sec/batch; 1h:06m:55s remains)
INFO - root - 2019-11-06 19:41:43.649019: step 56930, total loss = 0.77, predict loss = 0.21 (73.4 examples/sec; 0.054 sec/batch; 1h:24m:30s remains)
INFO - root - 2019-11-06 19:41:44.358468: step 56940, total loss = 1.26, predict loss = 0.33 (56.8 examples/sec; 0.070 sec/batch; 1h:49m:10s remains)
INFO - root - 2019-11-06 19:41:45.134052: step 56950, total loss = 0.94, predict loss = 0.26 (52.9 examples/sec; 0.076 sec/batch; 1h:57m:17s remains)
INFO - root - 2019-11-06 19:41:45.850973: step 56960, total loss = 0.70, predict loss = 0.18 (61.6 examples/sec; 0.065 sec/batch; 1h:40m:38s remains)
INFO - root - 2019-11-06 19:41:46.630820: step 56970, total loss = 0.85, predict loss = 0.21 (59.9 examples/sec; 0.067 sec/batch; 1h:43m:33s remains)
INFO - root - 2019-11-06 19:41:47.304347: step 56980, total loss = 1.18, predict loss = 0.33 (86.8 examples/sec; 0.046 sec/batch; 1h:11m:26s remains)
INFO - root - 2019-11-06 19:41:47.737750: step 56990, total loss = 1.06, predict loss = 0.29 (100.5 examples/sec; 0.040 sec/batch; 1h:01m:40s remains)
INFO - root - 2019-11-06 19:41:48.191999: step 57000, total loss = 1.05, predict loss = 0.26 (91.5 examples/sec; 0.044 sec/batch; 1h:07m:45s remains)
INFO - root - 2019-11-06 19:41:49.462972: step 57010, total loss = 1.27, predict loss = 0.28 (58.0 examples/sec; 0.069 sec/batch; 1h:46m:58s remains)
INFO - root - 2019-11-06 19:41:50.201244: step 57020, total loss = 2.01, predict loss = 0.59 (61.8 examples/sec; 0.065 sec/batch; 1h:40m:22s remains)
INFO - root - 2019-11-06 19:41:50.995039: step 57030, total loss = 0.54, predict loss = 0.14 (52.6 examples/sec; 0.076 sec/batch; 1h:57m:44s remains)
INFO - root - 2019-11-06 19:41:51.735148: step 57040, total loss = 0.60, predict loss = 0.16 (56.1 examples/sec; 0.071 sec/batch; 1h:50m:27s remains)
INFO - root - 2019-11-06 19:41:52.475187: step 57050, total loss = 1.05, predict loss = 0.29 (58.9 examples/sec; 0.068 sec/batch; 1h:45m:17s remains)
INFO - root - 2019-11-06 19:41:53.024322: step 57060, total loss = 0.71, predict loss = 0.17 (95.0 examples/sec; 0.042 sec/batch; 1h:05m:13s remains)
INFO - root - 2019-11-06 19:41:53.471226: step 57070, total loss = 1.08, predict loss = 0.30 (99.7 examples/sec; 0.040 sec/batch; 1h:02m:07s remains)
INFO - root - 2019-11-06 19:41:54.714888: step 57080, total loss = 1.19, predict loss = 0.33 (68.9 examples/sec; 0.058 sec/batch; 1h:29m:53s remains)
INFO - root - 2019-11-06 19:41:55.425043: step 57090, total loss = 0.60, predict loss = 0.15 (58.6 examples/sec; 0.068 sec/batch; 1h:45m:38s remains)
INFO - root - 2019-11-06 19:41:56.141724: step 57100, total loss = 0.93, predict loss = 0.27 (63.1 examples/sec; 0.063 sec/batch; 1h:38m:08s remains)
INFO - root - 2019-11-06 19:41:56.904684: step 57110, total loss = 1.05, predict loss = 0.26 (55.6 examples/sec; 0.072 sec/batch; 1h:51m:18s remains)
INFO - root - 2019-11-06 19:41:57.717053: step 57120, total loss = 1.45, predict loss = 0.44 (51.5 examples/sec; 0.078 sec/batch; 2h:00m:07s remains)
INFO - root - 2019-11-06 19:41:58.328342: step 57130, total loss = 1.05, predict loss = 0.30 (105.0 examples/sec; 0.038 sec/batch; 0h:58m:57s remains)
INFO - root - 2019-11-06 19:41:58.794641: step 57140, total loss = 2.04, predict loss = 0.63 (100.9 examples/sec; 0.040 sec/batch; 1h:01m:20s remains)
INFO - root - 2019-11-06 19:41:59.241912: step 57150, total loss = 0.58, predict loss = 0.13 (103.6 examples/sec; 0.039 sec/batch; 0h:59m:44s remains)
INFO - root - 2019-11-06 19:42:00.540006: step 57160, total loss = 1.16, predict loss = 0.37 (55.7 examples/sec; 0.072 sec/batch; 1h:51m:05s remains)
INFO - root - 2019-11-06 19:42:01.330444: step 57170, total loss = 0.84, predict loss = 0.25 (57.4 examples/sec; 0.070 sec/batch; 1h:47m:45s remains)
INFO - root - 2019-11-06 19:42:02.096583: step 57180, total loss = 1.00, predict loss = 0.28 (55.6 examples/sec; 0.072 sec/batch; 1h:51m:21s remains)
INFO - root - 2019-11-06 19:42:02.798946: step 57190, total loss = 0.47, predict loss = 0.13 (55.9 examples/sec; 0.072 sec/batch; 1h:50m:43s remains)
INFO - root - 2019-11-06 19:42:03.547768: step 57200, total loss = 1.19, predict loss = 0.35 (58.8 examples/sec; 0.068 sec/batch; 1h:45m:10s remains)
INFO - root - 2019-11-06 19:42:04.063230: step 57210, total loss = 0.94, predict loss = 0.28 (96.3 examples/sec; 0.042 sec/batch; 1h:04m:13s remains)
INFO - root - 2019-11-06 19:42:04.540708: step 57220, total loss = 0.64, predict loss = 0.15 (92.3 examples/sec; 0.043 sec/batch; 1h:06m:59s remains)
INFO - root - 2019-11-06 19:42:05.710777: step 57230, total loss = 1.19, predict loss = 0.32 (68.9 examples/sec; 0.058 sec/batch; 1h:29m:44s remains)
INFO - root - 2019-11-06 19:42:06.434897: step 57240, total loss = 0.83, predict loss = 0.24 (58.1 examples/sec; 0.069 sec/batch; 1h:46m:28s remains)
INFO - root - 2019-11-06 19:42:07.265859: step 57250, total loss = 1.13, predict loss = 0.29 (48.2 examples/sec; 0.083 sec/batch; 2h:08m:22s remains)
INFO - root - 2019-11-06 19:42:07.988098: step 57260, total loss = 1.09, predict loss = 0.32 (62.7 examples/sec; 0.064 sec/batch; 1h:38m:34s remains)
INFO - root - 2019-11-06 19:42:08.689545: step 57270, total loss = 0.77, predict loss = 0.19 (57.8 examples/sec; 0.069 sec/batch; 1h:46m:52s remains)
INFO - root - 2019-11-06 19:42:09.256224: step 57280, total loss = 1.51, predict loss = 0.40 (98.1 examples/sec; 0.041 sec/batch; 1h:03m:02s remains)
INFO - root - 2019-11-06 19:42:09.705643: step 57290, total loss = 1.65, predict loss = 0.42 (94.9 examples/sec; 0.042 sec/batch; 1h:05m:07s remains)
INFO - root - 2019-11-06 19:42:10.174123: step 57300, total loss = 1.31, predict loss = 0.37 (96.6 examples/sec; 0.041 sec/batch; 1h:03m:57s remains)
INFO - root - 2019-11-06 19:42:11.477188: step 57310, total loss = 1.15, predict loss = 0.32 (62.5 examples/sec; 0.064 sec/batch; 1h:38m:48s remains)
INFO - root - 2019-11-06 19:42:12.243975: step 57320, total loss = 0.88, predict loss = 0.23 (62.2 examples/sec; 0.064 sec/batch; 1h:39m:20s remains)
INFO - root - 2019-11-06 19:42:13.029451: step 57330, total loss = 2.02, predict loss = 0.60 (56.8 examples/sec; 0.070 sec/batch; 1h:48m:45s remains)
INFO - root - 2019-11-06 19:42:13.798437: step 57340, total loss = 0.62, predict loss = 0.17 (63.1 examples/sec; 0.063 sec/batch; 1h:37m:54s remains)
INFO - root - 2019-11-06 19:42:14.509426: step 57350, total loss = 2.03, predict loss = 0.56 (64.2 examples/sec; 0.062 sec/batch; 1h:36m:10s remains)
INFO - root - 2019-11-06 19:42:15.012424: step 57360, total loss = 1.30, predict loss = 0.38 (91.6 examples/sec; 0.044 sec/batch; 1h:07m:24s remains)
INFO - root - 2019-11-06 19:42:15.467902: step 57370, total loss = 0.89, predict loss = 0.27 (94.5 examples/sec; 0.042 sec/batch; 1h:05m:21s remains)
INFO - root - 2019-11-06 19:42:16.705014: step 57380, total loss = 0.79, predict loss = 0.22 (59.9 examples/sec; 0.067 sec/batch; 1h:43m:03s remains)
INFO - root - 2019-11-06 19:42:17.469138: step 57390, total loss = 1.10, predict loss = 0.29 (57.4 examples/sec; 0.070 sec/batch; 1h:47m:35s remains)
INFO - root - 2019-11-06 19:42:18.221153: step 57400, total loss = 0.99, predict loss = 0.26 (61.2 examples/sec; 0.065 sec/batch; 1h:40m:56s remains)
INFO - root - 2019-11-06 19:42:18.975689: step 57410, total loss = 0.74, predict loss = 0.20 (63.6 examples/sec; 0.063 sec/batch; 1h:37m:06s remains)
INFO - root - 2019-11-06 19:42:19.701715: step 57420, total loss = 0.86, predict loss = 0.25 (75.5 examples/sec; 0.053 sec/batch; 1h:21m:46s remains)
INFO - root - 2019-11-06 19:42:20.231735: step 57430, total loss = 0.89, predict loss = 0.24 (97.7 examples/sec; 0.041 sec/batch; 1h:03m:11s remains)
INFO - root - 2019-11-06 19:42:20.681789: step 57440, total loss = 1.41, predict loss = 0.38 (101.2 examples/sec; 0.040 sec/batch; 1h:00m:59s remains)
INFO - root - 2019-11-06 19:42:21.129584: step 57450, total loss = 0.85, predict loss = 0.24 (120.7 examples/sec; 0.033 sec/batch; 0h:51m:06s remains)
INFO - root - 2019-11-06 19:42:22.529893: step 57460, total loss = 1.36, predict loss = 0.38 (54.1 examples/sec; 0.074 sec/batch; 1h:53m:56s remains)
INFO - root - 2019-11-06 19:42:23.324903: step 57470, total loss = 1.68, predict loss = 0.54 (53.7 examples/sec; 0.075 sec/batch; 1h:54m:56s remains)
INFO - root - 2019-11-06 19:42:24.157271: step 57480, total loss = 0.97, predict loss = 0.21 (68.6 examples/sec; 0.058 sec/batch; 1h:29m:57s remains)
INFO - root - 2019-11-06 19:42:24.828173: step 57490, total loss = 2.39, predict loss = 0.74 (55.9 examples/sec; 0.072 sec/batch; 1h:50m:21s remains)
INFO - root - 2019-11-06 19:42:25.524467: step 57500, total loss = 1.15, predict loss = 0.29 (78.3 examples/sec; 0.051 sec/batch; 1h:18m:47s remains)
INFO - root - 2019-11-06 19:42:25.995983: step 57510, total loss = 0.92, predict loss = 0.24 (94.3 examples/sec; 0.042 sec/batch; 1h:05m:23s remains)
INFO - root - 2019-11-06 19:42:26.448624: step 57520, total loss = 1.37, predict loss = 0.35 (92.4 examples/sec; 0.043 sec/batch; 1h:06m:44s remains)
INFO - root - 2019-11-06 19:42:27.677504: step 57530, total loss = 0.56, predict loss = 0.15 (69.8 examples/sec; 0.057 sec/batch; 1h:28m:21s remains)
INFO - root - 2019-11-06 19:42:28.415045: step 57540, total loss = 1.07, predict loss = 0.30 (55.0 examples/sec; 0.073 sec/batch; 1h:52m:07s remains)
INFO - root - 2019-11-06 19:42:29.124105: step 57550, total loss = 0.93, predict loss = 0.25 (56.1 examples/sec; 0.071 sec/batch; 1h:49m:55s remains)
INFO - root - 2019-11-06 19:42:29.876347: step 57560, total loss = 0.58, predict loss = 0.14 (57.7 examples/sec; 0.069 sec/batch; 1h:46m:51s remains)
INFO - root - 2019-11-06 19:42:30.598141: step 57570, total loss = 1.53, predict loss = 0.36 (68.0 examples/sec; 0.059 sec/batch; 1h:30m:37s remains)
INFO - root - 2019-11-06 19:42:31.147446: step 57580, total loss = 1.20, predict loss = 0.33 (90.7 examples/sec; 0.044 sec/batch; 1h:07m:57s remains)
INFO - root - 2019-11-06 19:42:31.599242: step 57590, total loss = 0.53, predict loss = 0.16 (98.6 examples/sec; 0.041 sec/batch; 1h:02m:28s remains)
INFO - root - 2019-11-06 19:42:32.713137: step 57600, total loss = 0.60, predict loss = 0.18 (5.5 examples/sec; 0.722 sec/batch; 18h:31m:59s remains)
INFO - root - 2019-11-06 19:42:33.403458: step 57610, total loss = 1.10, predict loss = 0.30 (61.6 examples/sec; 0.065 sec/batch; 1h:39m:59s remains)
INFO - root - 2019-11-06 19:42:34.182664: step 57620, total loss = 1.52, predict loss = 0.43 (49.0 examples/sec; 0.082 sec/batch; 2h:05m:38s remains)
INFO - root - 2019-11-06 19:42:34.906736: step 57630, total loss = 0.60, predict loss = 0.15 (49.9 examples/sec; 0.080 sec/batch; 2h:03m:30s remains)
INFO - root - 2019-11-06 19:42:35.650405: step 57640, total loss = 0.84, predict loss = 0.23 (60.8 examples/sec; 0.066 sec/batch; 1h:41m:15s remains)
INFO - root - 2019-11-06 19:42:36.351289: step 57650, total loss = 0.57, predict loss = 0.16 (81.5 examples/sec; 0.049 sec/batch; 1h:15m:31s remains)
INFO - root - 2019-11-06 19:42:36.804945: step 57660, total loss = 0.98, predict loss = 0.26 (99.5 examples/sec; 0.040 sec/batch; 1h:01m:51s remains)
INFO - root - 2019-11-06 19:42:37.247838: step 57670, total loss = 0.95, predict loss = 0.24 (95.7 examples/sec; 0.042 sec/batch; 1h:04m:19s remains)
INFO - root - 2019-11-06 19:42:38.481554: step 57680, total loss = 0.88, predict loss = 0.24 (72.9 examples/sec; 0.055 sec/batch; 1h:24m:24s remains)
INFO - root - 2019-11-06 19:42:39.197882: step 57690, total loss = 0.96, predict loss = 0.26 (57.3 examples/sec; 0.070 sec/batch; 1h:47m:20s remains)
INFO - root - 2019-11-06 19:42:39.964755: step 57700, total loss = 0.89, predict loss = 0.25 (61.4 examples/sec; 0.065 sec/batch; 1h:40m:12s remains)
INFO - root - 2019-11-06 19:42:40.700729: step 57710, total loss = 1.26, predict loss = 0.36 (54.0 examples/sec; 0.074 sec/batch; 1h:54m:01s remains)
INFO - root - 2019-11-06 19:42:41.462551: step 57720, total loss = 1.73, predict loss = 0.51 (61.0 examples/sec; 0.066 sec/batch; 1h:40m:47s remains)
INFO - root - 2019-11-06 19:42:42.002680: step 57730, total loss = 0.56, predict loss = 0.13 (99.2 examples/sec; 0.040 sec/batch; 1h:02m:00s remains)
INFO - root - 2019-11-06 19:42:42.476998: step 57740, total loss = 1.08, predict loss = 0.28 (97.9 examples/sec; 0.041 sec/batch; 1h:02m:48s remains)
INFO - root - 2019-11-06 19:42:43.580316: step 57750, total loss = 0.60, predict loss = 0.16 (70.7 examples/sec; 0.057 sec/batch; 1h:26m:59s remains)
INFO - root - 2019-11-06 19:42:44.287742: step 57760, total loss = 0.48, predict loss = 0.11 (51.1 examples/sec; 0.078 sec/batch; 2h:00m:26s remains)
INFO - root - 2019-11-06 19:42:45.091700: step 57770, total loss = 0.89, predict loss = 0.24 (56.4 examples/sec; 0.071 sec/batch; 1h:48m:56s remains)
INFO - root - 2019-11-06 19:42:45.868289: step 57780, total loss = 0.80, predict loss = 0.21 (58.1 examples/sec; 0.069 sec/batch; 1h:45m:46s remains)
INFO - root - 2019-11-06 19:42:46.619542: step 57790, total loss = 1.26, predict loss = 0.34 (57.1 examples/sec; 0.070 sec/batch; 1h:47m:39s remains)
INFO - root - 2019-11-06 19:42:47.265719: step 57800, total loss = 1.03, predict loss = 0.29 (98.4 examples/sec; 0.041 sec/batch; 1h:02m:28s remains)
INFO - root - 2019-11-06 19:42:47.711882: step 57810, total loss = 1.25, predict loss = 0.34 (96.5 examples/sec; 0.041 sec/batch; 1h:03m:40s remains)
INFO - root - 2019-11-06 19:42:48.187398: step 57820, total loss = 1.69, predict loss = 0.54 (95.0 examples/sec; 0.042 sec/batch; 1h:04m:41s remains)
INFO - root - 2019-11-06 19:42:49.462767: step 57830, total loss = 1.60, predict loss = 0.48 (47.3 examples/sec; 0.085 sec/batch; 2h:10m:01s remains)
INFO - root - 2019-11-06 19:42:50.213096: step 57840, total loss = 0.90, predict loss = 0.23 (62.7 examples/sec; 0.064 sec/batch; 1h:37m:55s remains)
INFO - root - 2019-11-06 19:42:51.007616: step 57850, total loss = 1.36, predict loss = 0.38 (46.3 examples/sec; 0.086 sec/batch; 2h:12m:33s remains)
INFO - root - 2019-11-06 19:42:51.707513: step 57860, total loss = 0.85, predict loss = 0.19 (69.2 examples/sec; 0.058 sec/batch; 1h:28m:46s remains)
INFO - root - 2019-11-06 19:42:52.439228: step 57870, total loss = 1.23, predict loss = 0.32 (67.3 examples/sec; 0.059 sec/batch; 1h:31m:18s remains)
INFO - root - 2019-11-06 19:42:52.962701: step 57880, total loss = 1.24, predict loss = 0.36 (98.3 examples/sec; 0.041 sec/batch; 1h:02m:27s remains)
INFO - root - 2019-11-06 19:42:53.407383: step 57890, total loss = 0.95, predict loss = 0.23 (95.1 examples/sec; 0.042 sec/batch; 1h:04m:35s remains)
INFO - root - 2019-11-06 19:42:54.685283: step 57900, total loss = 1.09, predict loss = 0.34 (73.1 examples/sec; 0.055 sec/batch; 1h:23m:57s remains)
INFO - root - 2019-11-06 19:42:55.434907: step 57910, total loss = 0.48, predict loss = 0.12 (49.1 examples/sec; 0.081 sec/batch; 2h:05m:04s remains)
INFO - root - 2019-11-06 19:42:56.244722: step 57920, total loss = 1.12, predict loss = 0.31 (61.8 examples/sec; 0.065 sec/batch; 1h:39m:16s remains)
INFO - root - 2019-11-06 19:42:56.973340: step 57930, total loss = 1.17, predict loss = 0.31 (59.6 examples/sec; 0.067 sec/batch; 1h:43m:03s remains)
INFO - root - 2019-11-06 19:42:57.723178: step 57940, total loss = 0.92, predict loss = 0.25 (59.3 examples/sec; 0.067 sec/batch; 1h:43m:29s remains)
INFO - root - 2019-11-06 19:42:58.338976: step 57950, total loss = 0.97, predict loss = 0.22 (94.2 examples/sec; 0.042 sec/batch; 1h:05m:09s remains)
INFO - root - 2019-11-06 19:42:58.781378: step 57960, total loss = 1.05, predict loss = 0.31 (98.3 examples/sec; 0.041 sec/batch; 1h:02m:23s remains)
INFO - root - 2019-11-06 19:42:59.223037: step 57970, total loss = 0.82, predict loss = 0.23 (90.5 examples/sec; 0.044 sec/batch; 1h:07m:46s remains)
INFO - root - 2019-11-06 19:43:00.557424: step 57980, total loss = 1.04, predict loss = 0.27 (62.8 examples/sec; 0.064 sec/batch; 1h:37m:41s remains)
INFO - root - 2019-11-06 19:43:01.295117: step 57990, total loss = 0.86, predict loss = 0.25 (59.9 examples/sec; 0.067 sec/batch; 1h:42m:29s remains)
INFO - root - 2019-11-06 19:43:02.027176: step 58000, total loss = 1.71, predict loss = 0.41 (58.3 examples/sec; 0.069 sec/batch; 1h:45m:08s remains)
INFO - root - 2019-11-06 19:43:02.817938: step 58010, total loss = 0.76, predict loss = 0.19 (50.2 examples/sec; 0.080 sec/batch; 2h:02m:05s remains)
INFO - root - 2019-11-06 19:43:03.541209: step 58020, total loss = 0.90, predict loss = 0.25 (73.4 examples/sec; 0.055 sec/batch; 1h:23m:33s remains)
INFO - root - 2019-11-06 19:43:04.025829: step 58030, total loss = 1.02, predict loss = 0.29 (100.5 examples/sec; 0.040 sec/batch; 1h:01m:01s remains)
INFO - root - 2019-11-06 19:43:04.477572: step 58040, total loss = 0.54, predict loss = 0.14 (99.8 examples/sec; 0.040 sec/batch; 1h:01m:24s remains)
INFO - root - 2019-11-06 19:43:05.640797: step 58050, total loss = 0.61, predict loss = 0.16 (70.3 examples/sec; 0.057 sec/batch; 1h:27m:11s remains)
INFO - root - 2019-11-06 19:43:06.391358: step 58060, total loss = 0.91, predict loss = 0.27 (48.9 examples/sec; 0.082 sec/batch; 2h:05m:15s remains)
INFO - root - 2019-11-06 19:43:07.108917: step 58070, total loss = 0.62, predict loss = 0.16 (68.9 examples/sec; 0.058 sec/batch; 1h:28m:56s remains)
INFO - root - 2019-11-06 19:43:07.789784: step 58080, total loss = 0.50, predict loss = 0.14 (66.4 examples/sec; 0.060 sec/batch; 1h:32m:18s remains)
INFO - root - 2019-11-06 19:43:08.498739: step 58090, total loss = 1.72, predict loss = 0.50 (62.1 examples/sec; 0.064 sec/batch; 1h:38m:42s remains)
INFO - root - 2019-11-06 19:43:09.091614: step 58100, total loss = 0.47, predict loss = 0.11 (101.5 examples/sec; 0.039 sec/batch; 1h:00m:23s remains)
INFO - root - 2019-11-06 19:43:09.548833: step 58110, total loss = 1.08, predict loss = 0.28 (94.1 examples/sec; 0.042 sec/batch; 1h:05m:04s remains)
INFO - root - 2019-11-06 19:43:10.004279: step 58120, total loss = 0.90, predict loss = 0.26 (93.8 examples/sec; 0.043 sec/batch; 1h:05m:19s remains)
INFO - root - 2019-11-06 19:43:11.299951: step 58130, total loss = 0.58, predict loss = 0.16 (71.2 examples/sec; 0.056 sec/batch; 1h:25m:58s remains)
INFO - root - 2019-11-06 19:43:12.075966: step 58140, total loss = 0.75, predict loss = 0.18 (53.3 examples/sec; 0.075 sec/batch; 1h:54m:51s remains)
INFO - root - 2019-11-06 19:43:12.869689: step 58150, total loss = 0.72, predict loss = 0.21 (60.0 examples/sec; 0.067 sec/batch; 1h:42m:02s remains)
INFO - root - 2019-11-06 19:43:13.720190: step 58160, total loss = 1.10, predict loss = 0.28 (46.7 examples/sec; 0.086 sec/batch; 2h:11m:05s remains)
INFO - root - 2019-11-06 19:43:14.425648: step 58170, total loss = 0.53, predict loss = 0.13 (73.4 examples/sec; 0.055 sec/batch; 1h:23m:26s remains)
INFO - root - 2019-11-06 19:43:14.923959: step 58180, total loss = 0.96, predict loss = 0.17 (94.5 examples/sec; 0.042 sec/batch; 1h:04m:48s remains)
INFO - root - 2019-11-06 19:43:15.372887: step 58190, total loss = 0.81, predict loss = 0.23 (99.1 examples/sec; 0.040 sec/batch; 1h:01m:47s remains)
INFO - root - 2019-11-06 19:43:16.539376: step 58200, total loss = 0.50, predict loss = 0.13 (70.4 examples/sec; 0.057 sec/batch; 1h:26m:54s remains)
INFO - root - 2019-11-06 19:43:17.282733: step 58210, total loss = 1.56, predict loss = 0.46 (53.2 examples/sec; 0.075 sec/batch; 1h:55m:03s remains)
INFO - root - 2019-11-06 19:43:18.056301: step 58220, total loss = 1.31, predict loss = 0.38 (58.2 examples/sec; 0.069 sec/batch; 1h:45m:03s remains)
INFO - root - 2019-11-06 19:43:18.830622: step 58230, total loss = 0.61, predict loss = 0.13 (53.9 examples/sec; 0.074 sec/batch; 1h:53m:27s remains)
INFO - root - 2019-11-06 19:43:19.652409: step 58240, total loss = 1.10, predict loss = 0.28 (58.1 examples/sec; 0.069 sec/batch; 1h:45m:20s remains)
INFO - root - 2019-11-06 19:43:20.228651: step 58250, total loss = 1.95, predict loss = 0.60 (94.2 examples/sec; 0.042 sec/batch; 1h:04m:57s remains)
INFO - root - 2019-11-06 19:43:20.701252: step 58260, total loss = 1.45, predict loss = 0.44 (92.4 examples/sec; 0.043 sec/batch; 1h:06m:10s remains)
INFO - root - 2019-11-06 19:43:21.131111: step 58270, total loss = 0.72, predict loss = 0.19 (129.1 examples/sec; 0.031 sec/batch; 0h:47m:21s remains)
INFO - root - 2019-11-06 19:43:22.472024: step 58280, total loss = 0.63, predict loss = 0.15 (62.8 examples/sec; 0.064 sec/batch; 1h:37m:18s remains)
INFO - root - 2019-11-06 19:43:23.213985: step 58290, total loss = 0.98, predict loss = 0.27 (62.0 examples/sec; 0.065 sec/batch; 1h:38m:39s remains)
INFO - root - 2019-11-06 19:43:24.077681: step 58300, total loss = 0.86, predict loss = 0.24 (58.5 examples/sec; 0.068 sec/batch; 1h:44m:32s remains)
INFO - root - 2019-11-06 19:43:24.772567: step 58310, total loss = 0.72, predict loss = 0.18 (60.5 examples/sec; 0.066 sec/batch; 1h:41m:05s remains)
INFO - root - 2019-11-06 19:43:25.475443: step 58320, total loss = 1.05, predict loss = 0.33 (73.6 examples/sec; 0.054 sec/batch; 1h:23m:00s remains)
INFO - root - 2019-11-06 19:43:25.931870: step 58330, total loss = 0.65, predict loss = 0.16 (104.6 examples/sec; 0.038 sec/batch; 0h:58m:25s remains)
INFO - root - 2019-11-06 19:43:26.391241: step 58340, total loss = 1.37, predict loss = 0.34 (98.1 examples/sec; 0.041 sec/batch; 1h:02m:16s remains)
INFO - root - 2019-11-06 19:43:27.600482: step 58350, total loss = 0.91, predict loss = 0.25 (66.0 examples/sec; 0.061 sec/batch; 1h:32m:32s remains)
INFO - root - 2019-11-06 19:43:28.324022: step 58360, total loss = 1.63, predict loss = 0.51 (65.0 examples/sec; 0.062 sec/batch; 1h:34m:00s remains)
INFO - root - 2019-11-06 19:43:29.081083: step 58370, total loss = 1.22, predict loss = 0.38 (55.6 examples/sec; 0.072 sec/batch; 1h:49m:51s remains)
INFO - root - 2019-11-06 19:43:29.853898: step 58380, total loss = 0.55, predict loss = 0.13 (52.6 examples/sec; 0.076 sec/batch; 1h:56m:09s remains)
INFO - root - 2019-11-06 19:43:30.639084: step 58390, total loss = 1.13, predict loss = 0.33 (65.3 examples/sec; 0.061 sec/batch; 1h:33m:32s remains)
INFO - root - 2019-11-06 19:43:31.212846: step 58400, total loss = 1.38, predict loss = 0.36 (90.7 examples/sec; 0.044 sec/batch; 1h:07m:18s remains)
INFO - root - 2019-11-06 19:43:31.671254: step 58410, total loss = 1.23, predict loss = 0.33 (97.7 examples/sec; 0.041 sec/batch; 1h:02m:28s remains)
INFO - root - 2019-11-06 19:43:32.833044: step 58420, total loss = 0.83, predict loss = 0.21 (5.5 examples/sec; 0.723 sec/batch; 18h:23m:06s remains)
INFO - root - 2019-11-06 19:43:33.505171: step 58430, total loss = 0.82, predict loss = 0.22 (54.6 examples/sec; 0.073 sec/batch; 1h:51m:48s remains)
INFO - root - 2019-11-06 19:43:34.262494: step 58440, total loss = 0.80, predict loss = 0.20 (55.1 examples/sec; 0.073 sec/batch; 1h:50m:41s remains)
INFO - root - 2019-11-06 19:43:35.016465: step 58450, total loss = 1.30, predict loss = 0.34 (53.7 examples/sec; 0.075 sec/batch; 1h:53m:44s remains)
INFO - root - 2019-11-06 19:43:35.772474: step 58460, total loss = 0.70, predict loss = 0.18 (63.2 examples/sec; 0.063 sec/batch; 1h:36m:35s remains)
INFO - root - 2019-11-06 19:43:36.401500: step 58470, total loss = 0.72, predict loss = 0.18 (96.2 examples/sec; 0.042 sec/batch; 1h:03m:25s remains)
INFO - root - 2019-11-06 19:43:36.859863: step 58480, total loss = 0.53, predict loss = 0.12 (101.0 examples/sec; 0.040 sec/batch; 1h:00m:24s remains)
INFO - root - 2019-11-06 19:43:37.309491: step 58490, total loss = 1.63, predict loss = 0.48 (94.2 examples/sec; 0.042 sec/batch; 1h:04m:47s remains)
INFO - root - 2019-11-06 19:43:38.579395: step 58500, total loss = 0.63, predict loss = 0.15 (66.4 examples/sec; 0.060 sec/batch; 1h:31m:48s remains)
INFO - root - 2019-11-06 19:43:39.288299: step 58510, total loss = 1.03, predict loss = 0.25 (56.4 examples/sec; 0.071 sec/batch; 1h:48m:12s remains)
INFO - root - 2019-11-06 19:43:39.997285: step 58520, total loss = 0.70, predict loss = 0.13 (60.5 examples/sec; 0.066 sec/batch; 1h:40m:48s remains)
INFO - root - 2019-11-06 19:43:40.734956: step 58530, total loss = 1.04, predict loss = 0.28 (61.8 examples/sec; 0.065 sec/batch; 1h:38m:37s remains)
INFO - root - 2019-11-06 19:43:41.485578: step 58540, total loss = 0.71, predict loss = 0.19 (70.7 examples/sec; 0.057 sec/batch; 1h:26m:14s remains)
INFO - root - 2019-11-06 19:43:41.998499: step 58550, total loss = 1.12, predict loss = 0.31 (106.4 examples/sec; 0.038 sec/batch; 0h:57m:18s remains)
INFO - root - 2019-11-06 19:43:42.447567: step 58560, total loss = 0.62, predict loss = 0.19 (90.0 examples/sec; 0.044 sec/batch; 1h:07m:42s remains)
INFO - root - 2019-11-06 19:43:43.577626: step 58570, total loss = 1.07, predict loss = 0.28 (70.2 examples/sec; 0.057 sec/batch; 1h:26m:46s remains)
INFO - root - 2019-11-06 19:43:44.301365: step 58580, total loss = 0.82, predict loss = 0.20 (57.7 examples/sec; 0.069 sec/batch; 1h:45m:41s remains)
INFO - root - 2019-11-06 19:43:45.030616: step 58590, total loss = 1.11, predict loss = 0.29 (64.1 examples/sec; 0.062 sec/batch; 1h:35m:02s remains)
INFO - root - 2019-11-06 19:43:45.752209: step 58600, total loss = 0.79, predict loss = 0.20 (58.0 examples/sec; 0.069 sec/batch; 1h:44m:59s remains)
INFO - root - 2019-11-06 19:43:46.523804: step 58610, total loss = 0.52, predict loss = 0.11 (60.4 examples/sec; 0.066 sec/batch; 1h:40m:51s remains)
INFO - root - 2019-11-06 19:43:47.191453: step 58620, total loss = 1.67, predict loss = 0.53 (86.9 examples/sec; 0.046 sec/batch; 1h:10m:08s remains)
INFO - root - 2019-11-06 19:43:47.644042: step 58630, total loss = 0.97, predict loss = 0.28 (97.0 examples/sec; 0.041 sec/batch; 1h:02m:48s remains)
INFO - root - 2019-11-06 19:43:48.089044: step 58640, total loss = 0.68, predict loss = 0.17 (97.8 examples/sec; 0.041 sec/batch; 1h:02m:15s remains)
INFO - root - 2019-11-06 19:43:49.376246: step 58650, total loss = 0.97, predict loss = 0.25 (63.5 examples/sec; 0.063 sec/batch; 1h:35m:55s remains)
INFO - root - 2019-11-06 19:43:50.138767: step 58660, total loss = 1.40, predict loss = 0.38 (58.6 examples/sec; 0.068 sec/batch; 1h:43m:52s remains)
INFO - root - 2019-11-06 19:43:50.906482: step 58670, total loss = 0.44, predict loss = 0.10 (60.2 examples/sec; 0.066 sec/batch; 1h:41m:09s remains)
INFO - root - 2019-11-06 19:43:51.700154: step 58680, total loss = 0.87, predict loss = 0.22 (55.4 examples/sec; 0.072 sec/batch; 1h:49m:52s remains)
INFO - root - 2019-11-06 19:43:52.490859: step 58690, total loss = 0.79, predict loss = 0.19 (60.4 examples/sec; 0.066 sec/batch; 1h:40m:43s remains)
INFO - root - 2019-11-06 19:43:53.046157: step 58700, total loss = 0.77, predict loss = 0.23 (96.8 examples/sec; 0.041 sec/batch; 1h:02m:52s remains)
INFO - root - 2019-11-06 19:43:53.515905: step 58710, total loss = 1.53, predict loss = 0.53 (88.7 examples/sec; 0.045 sec/batch; 1h:08m:35s remains)
INFO - root - 2019-11-06 19:43:54.701289: step 58720, total loss = 0.91, predict loss = 0.27 (58.7 examples/sec; 0.068 sec/batch; 1h:43m:37s remains)
INFO - root - 2019-11-06 19:43:55.400236: step 58730, total loss = 0.95, predict loss = 0.29 (60.8 examples/sec; 0.066 sec/batch; 1h:40m:08s remains)
INFO - root - 2019-11-06 19:43:56.160299: step 58740, total loss = 0.97, predict loss = 0.25 (68.6 examples/sec; 0.058 sec/batch; 1h:28m:44s remains)
INFO - root - 2019-11-06 19:43:56.876081: step 58750, total loss = 0.78, predict loss = 0.19 (59.8 examples/sec; 0.067 sec/batch; 1h:41m:42s remains)
INFO - root - 2019-11-06 19:43:57.610762: step 58760, total loss = 1.33, predict loss = 0.37 (58.7 examples/sec; 0.068 sec/batch; 1h:43m:40s remains)
INFO - root - 2019-11-06 19:43:58.224999: step 58770, total loss = 0.87, predict loss = 0.23 (94.0 examples/sec; 0.043 sec/batch; 1h:04m:42s remains)
INFO - root - 2019-11-06 19:43:58.705305: step 58780, total loss = 0.67, predict loss = 0.18 (98.0 examples/sec; 0.041 sec/batch; 1h:02m:05s remains)
INFO - root - 2019-11-06 19:43:59.154915: step 58790, total loss = 1.24, predict loss = 0.34 (94.6 examples/sec; 0.042 sec/batch; 1h:04m:17s remains)
INFO - root - 2019-11-06 19:44:00.424247: step 58800, total loss = 1.37, predict loss = 0.40 (62.2 examples/sec; 0.064 sec/batch; 1h:37m:48s remains)
INFO - root - 2019-11-06 19:44:01.150517: step 58810, total loss = 0.74, predict loss = 0.20 (59.1 examples/sec; 0.068 sec/batch; 1h:42m:54s remains)
INFO - root - 2019-11-06 19:44:01.875422: step 58820, total loss = 1.38, predict loss = 0.38 (59.4 examples/sec; 0.067 sec/batch; 1h:42m:18s remains)
INFO - root - 2019-11-06 19:44:02.607431: step 58830, total loss = 0.63, predict loss = 0.16 (56.1 examples/sec; 0.071 sec/batch; 1h:48m:24s remains)
INFO - root - 2019-11-06 19:44:03.330455: step 58840, total loss = 1.72, predict loss = 0.56 (69.7 examples/sec; 0.057 sec/batch; 1h:27m:08s remains)
INFO - root - 2019-11-06 19:44:03.814423: step 58850, total loss = 1.19, predict loss = 0.32 (97.2 examples/sec; 0.041 sec/batch; 1h:02m:29s remains)
INFO - root - 2019-11-06 19:44:04.298574: step 58860, total loss = 1.14, predict loss = 0.31 (93.2 examples/sec; 0.043 sec/batch; 1h:05m:11s remains)
INFO - root - 2019-11-06 19:44:05.442795: step 58870, total loss = 0.78, predict loss = 0.21 (67.9 examples/sec; 0.059 sec/batch; 1h:29m:26s remains)
INFO - root - 2019-11-06 19:44:06.182703: step 58880, total loss = 1.19, predict loss = 0.27 (58.6 examples/sec; 0.068 sec/batch; 1h:43m:41s remains)
INFO - root - 2019-11-06 19:44:06.940961: step 58890, total loss = 0.82, predict loss = 0.22 (60.0 examples/sec; 0.067 sec/batch; 1h:41m:09s remains)
INFO - root - 2019-11-06 19:44:07.702453: step 58900, total loss = 0.77, predict loss = 0.20 (57.3 examples/sec; 0.070 sec/batch; 1h:46m:00s remains)
INFO - root - 2019-11-06 19:44:08.419168: step 58910, total loss = 0.86, predict loss = 0.22 (59.8 examples/sec; 0.067 sec/batch; 1h:41m:35s remains)
INFO - root - 2019-11-06 19:44:09.000813: step 58920, total loss = 0.73, predict loss = 0.18 (102.2 examples/sec; 0.039 sec/batch; 0h:59m:23s remains)
INFO - root - 2019-11-06 19:44:09.442887: step 58930, total loss = 1.04, predict loss = 0.29 (93.8 examples/sec; 0.043 sec/batch; 1h:04m:44s remains)
INFO - root - 2019-11-06 19:44:09.910802: step 58940, total loss = 0.91, predict loss = 0.22 (99.9 examples/sec; 0.040 sec/batch; 1h:00m:45s remains)
INFO - root - 2019-11-06 19:44:11.197160: step 58950, total loss = 1.41, predict loss = 0.41 (66.7 examples/sec; 0.060 sec/batch; 1h:31m:03s remains)
INFO - root - 2019-11-06 19:44:11.937883: step 58960, total loss = 0.51, predict loss = 0.14 (60.7 examples/sec; 0.066 sec/batch; 1h:40m:02s remains)
INFO - root - 2019-11-06 19:44:12.701973: step 58970, total loss = 0.89, predict loss = 0.22 (54.8 examples/sec; 0.073 sec/batch; 1h:50m:41s remains)
INFO - root - 2019-11-06 19:44:13.480055: step 58980, total loss = 1.46, predict loss = 0.43 (61.3 examples/sec; 0.065 sec/batch; 1h:38m:55s remains)
INFO - root - 2019-11-06 19:44:14.145126: step 58990, total loss = 0.56, predict loss = 0.13 (69.5 examples/sec; 0.058 sec/batch; 1h:27m:16s remains)
INFO - root - 2019-11-06 19:44:14.619761: step 59000, total loss = 1.26, predict loss = 0.38 (97.5 examples/sec; 0.041 sec/batch; 1h:02m:14s remains)
INFO - root - 2019-11-06 19:44:15.079008: step 59010, total loss = 0.90, predict loss = 0.24 (93.8 examples/sec; 0.043 sec/batch; 1h:04m:41s remains)
INFO - root - 2019-11-06 19:44:16.286484: step 59020, total loss = 0.51, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 1h:26m:17s remains)
INFO - root - 2019-11-06 19:44:16.983075: step 59030, total loss = 1.29, predict loss = 0.36 (60.7 examples/sec; 0.066 sec/batch; 1h:39m:55s remains)
INFO - root - 2019-11-06 19:44:17.713494: step 59040, total loss = 0.91, predict loss = 0.27 (63.6 examples/sec; 0.063 sec/batch; 1h:35m:16s remains)
INFO - root - 2019-11-06 19:44:18.458926: step 59050, total loss = 1.52, predict loss = 0.47 (57.0 examples/sec; 0.070 sec/batch; 1h:46m:26s remains)
INFO - root - 2019-11-06 19:44:19.182413: step 59060, total loss = 0.72, predict loss = 0.18 (69.0 examples/sec; 0.058 sec/batch; 1h:27m:48s remains)
INFO - root - 2019-11-06 19:44:19.722922: step 59070, total loss = 1.41, predict loss = 0.49 (98.8 examples/sec; 0.041 sec/batch; 1h:01m:23s remains)
INFO - root - 2019-11-06 19:44:20.157729: step 59080, total loss = 0.47, predict loss = 0.12 (99.0 examples/sec; 0.040 sec/batch; 1h:01m:12s remains)
INFO - root - 2019-11-06 19:44:20.603599: step 59090, total loss = 1.39, predict loss = 0.36 (116.4 examples/sec; 0.034 sec/batch; 0h:52m:04s remains)
INFO - root - 2019-11-06 19:44:21.976465: step 59100, total loss = 0.83, predict loss = 0.20 (58.4 examples/sec; 0.069 sec/batch; 1h:43m:47s remains)
INFO - root - 2019-11-06 19:44:22.734960: step 59110, total loss = 0.89, predict loss = 0.24 (56.9 examples/sec; 0.070 sec/batch; 1h:46m:33s remains)
INFO - root - 2019-11-06 19:44:23.469705: step 59120, total loss = 0.75, predict loss = 0.19 (61.7 examples/sec; 0.065 sec/batch; 1h:38m:08s remains)
INFO - root - 2019-11-06 19:44:24.231796: step 59130, total loss = 0.71, predict loss = 0.20 (71.6 examples/sec; 0.056 sec/batch; 1h:24m:38s remains)
INFO - root - 2019-11-06 19:44:24.969596: step 59140, total loss = 0.90, predict loss = 0.21 (71.1 examples/sec; 0.056 sec/batch; 1h:25m:08s remains)
INFO - root - 2019-11-06 19:44:25.441357: step 59150, total loss = 1.18, predict loss = 0.33 (97.2 examples/sec; 0.041 sec/batch; 1h:02m:19s remains)
INFO - root - 2019-11-06 19:44:25.889352: step 59160, total loss = 0.99, predict loss = 0.27 (94.2 examples/sec; 0.042 sec/batch; 1h:04m:18s remains)
INFO - root - 2019-11-06 19:44:27.124437: step 59170, total loss = 0.56, predict loss = 0.15 (54.3 examples/sec; 0.074 sec/batch; 1h:51m:34s remains)
INFO - root - 2019-11-06 19:44:27.890589: step 59180, total loss = 0.85, predict loss = 0.20 (56.1 examples/sec; 0.071 sec/batch; 1h:47m:55s remains)
INFO - root - 2019-11-06 19:44:28.627072: step 59190, total loss = 1.23, predict loss = 0.36 (59.9 examples/sec; 0.067 sec/batch; 1h:41m:01s remains)
INFO - root - 2019-11-06 19:44:29.313000: step 59200, total loss = 0.48, predict loss = 0.13 (64.8 examples/sec; 0.062 sec/batch; 1h:33m:23s remains)
INFO - root - 2019-11-06 19:44:30.057652: step 59210, total loss = 2.15, predict loss = 0.65 (69.3 examples/sec; 0.058 sec/batch; 1h:27m:22s remains)
INFO - root - 2019-11-06 19:44:30.602302: step 59220, total loss = 0.75, predict loss = 0.19 (102.6 examples/sec; 0.039 sec/batch; 0h:58m:58s remains)
INFO - root - 2019-11-06 19:44:31.050260: step 59230, total loss = 1.29, predict loss = 0.40 (93.9 examples/sec; 0.043 sec/batch; 1h:04m:26s remains)
INFO - root - 2019-11-06 19:44:32.156169: step 59240, total loss = 1.16, predict loss = 0.32 (5.7 examples/sec; 0.699 sec/batch; 17h:37m:19s remains)
INFO - root - 2019-11-06 19:44:32.837176: step 59250, total loss = 1.02, predict loss = 0.30 (57.3 examples/sec; 0.070 sec/batch; 1h:45m:31s remains)
INFO - root - 2019-11-06 19:44:33.564296: step 59260, total loss = 0.84, predict loss = 0.23 (59.3 examples/sec; 0.067 sec/batch; 1h:42m:01s remains)
INFO - root - 2019-11-06 19:44:34.340508: step 59270, total loss = 0.94, predict loss = 0.25 (55.3 examples/sec; 0.072 sec/batch; 1h:49m:19s remains)
INFO - root - 2019-11-06 19:44:35.049601: step 59280, total loss = 0.72, predict loss = 0.18 (63.7 examples/sec; 0.063 sec/batch; 1h:34m:52s remains)
INFO - root - 2019-11-06 19:44:35.725354: step 59290, total loss = 0.80, predict loss = 0.19 (91.0 examples/sec; 0.044 sec/batch; 1h:06m:26s remains)
INFO - root - 2019-11-06 19:44:36.188821: step 59300, total loss = 1.07, predict loss = 0.24 (98.1 examples/sec; 0.041 sec/batch; 1h:01m:37s remains)
INFO - root - 2019-11-06 19:44:36.643758: step 59310, total loss = 1.02, predict loss = 0.26 (97.7 examples/sec; 0.041 sec/batch; 1h:01m:54s remains)
INFO - root - 2019-11-06 19:44:37.888707: step 59320, total loss = 0.80, predict loss = 0.23 (63.5 examples/sec; 0.063 sec/batch; 1h:35m:14s remains)
INFO - root - 2019-11-06 19:44:38.614863: step 59330, total loss = 1.20, predict loss = 0.33 (57.8 examples/sec; 0.069 sec/batch; 1h:44m:34s remains)
INFO - root - 2019-11-06 19:44:39.368873: step 59340, total loss = 1.72, predict loss = 0.54 (63.3 examples/sec; 0.063 sec/batch; 1h:35m:27s remains)
INFO - root - 2019-11-06 19:44:40.096944: step 59350, total loss = 0.96, predict loss = 0.24 (64.2 examples/sec; 0.062 sec/batch; 1h:34m:09s remains)
INFO - root - 2019-11-06 19:44:40.795590: step 59360, total loss = 1.31, predict loss = 0.37 (65.0 examples/sec; 0.061 sec/batch; 1h:32m:53s remains)
INFO - root - 2019-11-06 19:44:41.304476: step 59370, total loss = 0.92, predict loss = 0.21 (100.4 examples/sec; 0.040 sec/batch; 1h:00m:11s remains)
INFO - root - 2019-11-06 19:44:41.779144: step 59380, total loss = 0.65, predict loss = 0.17 (97.5 examples/sec; 0.041 sec/batch; 1h:01m:56s remains)
INFO - root - 2019-11-06 19:44:42.882570: step 59390, total loss = 0.93, predict loss = 0.27 (73.3 examples/sec; 0.055 sec/batch; 1h:22m:23s remains)
INFO - root - 2019-11-06 19:44:43.551506: step 59400, total loss = 1.29, predict loss = 0.37 (65.5 examples/sec; 0.061 sec/batch; 1h:32m:16s remains)
INFO - root - 2019-11-06 19:44:44.242668: step 59410, total loss = 0.51, predict loss = 0.14 (59.3 examples/sec; 0.067 sec/batch; 1h:41m:53s remains)
INFO - root - 2019-11-06 19:44:44.983288: step 59420, total loss = 0.53, predict loss = 0.13 (62.9 examples/sec; 0.064 sec/batch; 1h:35m:57s remains)
INFO - root - 2019-11-06 19:44:45.703843: step 59430, total loss = 0.80, predict loss = 0.21 (58.2 examples/sec; 0.069 sec/batch; 1h:43m:49s remains)
INFO - root - 2019-11-06 19:44:46.353816: step 59440, total loss = 1.05, predict loss = 0.31 (88.8 examples/sec; 0.045 sec/batch; 1h:07m:57s remains)
INFO - root - 2019-11-06 19:44:46.797255: step 59450, total loss = 0.87, predict loss = 0.26 (96.4 examples/sec; 0.041 sec/batch; 1h:02m:35s remains)
INFO - root - 2019-11-06 19:44:47.266380: step 59460, total loss = 0.53, predict loss = 0.13 (88.3 examples/sec; 0.045 sec/batch; 1h:08m:20s remains)
INFO - root - 2019-11-06 19:44:48.483680: step 59470, total loss = 0.69, predict loss = 0.16 (69.0 examples/sec; 0.058 sec/batch; 1h:27m:29s remains)
INFO - root - 2019-11-06 19:44:49.253639: step 59480, total loss = 1.50, predict loss = 0.40 (52.7 examples/sec; 0.076 sec/batch; 1h:54m:32s remains)
INFO - root - 2019-11-06 19:44:50.001425: step 59490, total loss = 0.42, predict loss = 0.10 (56.3 examples/sec; 0.071 sec/batch; 1h:47m:12s remains)
INFO - root - 2019-11-06 19:44:50.749116: step 59500, total loss = 0.95, predict loss = 0.30 (63.0 examples/sec; 0.064 sec/batch; 1h:35m:47s remains)
INFO - root - 2019-11-06 19:44:51.423415: step 59510, total loss = 0.65, predict loss = 0.17 (66.9 examples/sec; 0.060 sec/batch; 1h:30m:10s remains)
INFO - root - 2019-11-06 19:44:51.927074: step 59520, total loss = 0.80, predict loss = 0.25 (98.0 examples/sec; 0.041 sec/batch; 1h:01m:33s remains)
INFO - root - 2019-11-06 19:44:52.373182: step 59530, total loss = 1.12, predict loss = 0.28 (97.6 examples/sec; 0.041 sec/batch; 1h:01m:46s remains)
INFO - root - 2019-11-06 19:44:53.554621: step 59540, total loss = 1.15, predict loss = 0.26 (68.3 examples/sec; 0.059 sec/batch; 1h:28m:20s remains)
INFO - root - 2019-11-06 19:44:54.300719: step 59550, total loss = 0.93, predict loss = 0.18 (70.6 examples/sec; 0.057 sec/batch; 1h:25m:25s remains)
INFO - root - 2019-11-06 19:44:55.074374: step 59560, total loss = 1.01, predict loss = 0.31 (52.7 examples/sec; 0.076 sec/batch; 1h:54m:29s remains)
INFO - root - 2019-11-06 19:44:55.807630: step 59570, total loss = 1.58, predict loss = 0.50 (55.6 examples/sec; 0.072 sec/batch; 1h:48m:22s remains)
INFO - root - 2019-11-06 19:44:56.613296: step 59580, total loss = 0.68, predict loss = 0.16 (53.5 examples/sec; 0.075 sec/batch; 1h:52m:42s remains)
INFO - root - 2019-11-06 19:44:57.187012: step 59590, total loss = 1.42, predict loss = 0.44 (103.2 examples/sec; 0.039 sec/batch; 0h:58m:24s remains)
INFO - root - 2019-11-06 19:44:57.631041: step 59600, total loss = 1.24, predict loss = 0.35 (100.0 examples/sec; 0.040 sec/batch; 1h:00m:14s remains)
INFO - root - 2019-11-06 19:44:58.070672: step 59610, total loss = 0.93, predict loss = 0.22 (97.8 examples/sec; 0.041 sec/batch; 1h:01m:36s remains)
INFO - root - 2019-11-06 19:44:59.398598: step 59620, total loss = 1.23, predict loss = 0.31 (56.6 examples/sec; 0.071 sec/batch; 1h:46m:22s remains)
INFO - root - 2019-11-06 19:45:00.104366: step 59630, total loss = 0.99, predict loss = 0.28 (58.7 examples/sec; 0.068 sec/batch; 1h:42m:36s remains)
INFO - root - 2019-11-06 19:45:00.850230: step 59640, total loss = 0.58, predict loss = 0.12 (53.1 examples/sec; 0.075 sec/batch; 1h:53m:27s remains)
INFO - root - 2019-11-06 19:45:01.580450: step 59650, total loss = 1.65, predict loss = 0.51 (57.7 examples/sec; 0.069 sec/batch; 1h:44m:25s remains)
INFO - root - 2019-11-06 19:45:02.289400: step 59660, total loss = 0.89, predict loss = 0.24 (78.3 examples/sec; 0.051 sec/batch; 1h:16m:55s remains)
INFO - root - 2019-11-06 19:45:02.775640: step 59670, total loss = 0.93, predict loss = 0.22 (91.6 examples/sec; 0.044 sec/batch; 1h:05m:43s remains)
INFO - root - 2019-11-06 19:45:03.221756: step 59680, total loss = 0.84, predict loss = 0.20 (97.3 examples/sec; 0.041 sec/batch; 1h:01m:51s remains)
INFO - root - 2019-11-06 19:45:04.442072: step 59690, total loss = 1.30, predict loss = 0.39 (63.5 examples/sec; 0.063 sec/batch; 1h:34m:46s remains)
INFO - root - 2019-11-06 19:45:05.159723: step 59700, total loss = 1.31, predict loss = 0.36 (61.9 examples/sec; 0.065 sec/batch; 1h:37m:12s remains)
INFO - root - 2019-11-06 19:45:05.852230: step 59710, total loss = 0.63, predict loss = 0.17 (66.9 examples/sec; 0.060 sec/batch; 1h:29m:58s remains)
INFO - root - 2019-11-06 19:45:06.621846: step 59720, total loss = 0.44, predict loss = 0.10 (53.5 examples/sec; 0.075 sec/batch; 1h:52m:28s remains)
INFO - root - 2019-11-06 19:45:07.369622: step 59730, total loss = 1.93, predict loss = 0.58 (62.0 examples/sec; 0.065 sec/batch; 1h:37m:04s remains)
INFO - root - 2019-11-06 19:45:08.004752: step 59740, total loss = 1.07, predict loss = 0.28 (96.7 examples/sec; 0.041 sec/batch; 1h:02m:13s remains)
INFO - root - 2019-11-06 19:45:08.460848: step 59750, total loss = 0.70, predict loss = 0.18 (94.2 examples/sec; 0.042 sec/batch; 1h:03m:50s remains)
INFO - root - 2019-11-06 19:45:08.901125: step 59760, total loss = 0.81, predict loss = 0.19 (95.9 examples/sec; 0.042 sec/batch; 1h:02m:44s remains)
INFO - root - 2019-11-06 19:45:10.261829: step 59770, total loss = 0.80, predict loss = 0.26 (56.3 examples/sec; 0.071 sec/batch; 1h:46m:53s remains)
INFO - root - 2019-11-06 19:45:11.018397: step 59780, total loss = 0.90, predict loss = 0.23 (52.8 examples/sec; 0.076 sec/batch; 1h:53m:57s remains)
INFO - root - 2019-11-06 19:45:11.838364: step 59790, total loss = 1.16, predict loss = 0.34 (50.7 examples/sec; 0.079 sec/batch; 1h:58m:35s remains)
INFO - root - 2019-11-06 19:45:12.567020: step 59800, total loss = 0.84, predict loss = 0.23 (62.7 examples/sec; 0.064 sec/batch; 1h:35m:51s remains)
INFO - root - 2019-11-06 19:45:13.253324: step 59810, total loss = 1.00, predict loss = 0.27 (70.9 examples/sec; 0.056 sec/batch; 1h:24m:51s remains)
INFO - root - 2019-11-06 19:45:13.771077: step 59820, total loss = 0.98, predict loss = 0.25 (91.4 examples/sec; 0.044 sec/batch; 1h:05m:47s remains)
INFO - root - 2019-11-06 19:45:14.213526: step 59830, total loss = 0.98, predict loss = 0.26 (101.8 examples/sec; 0.039 sec/batch; 0h:59m:03s remains)
INFO - root - 2019-11-06 19:45:15.401385: step 59840, total loss = 0.52, predict loss = 0.13 (64.9 examples/sec; 0.062 sec/batch; 1h:32m:38s remains)
INFO - root - 2019-11-06 19:45:16.095720: step 59850, total loss = 1.25, predict loss = 0.36 (60.0 examples/sec; 0.067 sec/batch; 1h:40m:09s remains)
INFO - root - 2019-11-06 19:45:16.832213: step 59860, total loss = 0.63, predict loss = 0.15 (53.5 examples/sec; 0.075 sec/batch; 1h:52m:22s remains)
INFO - root - 2019-11-06 19:45:17.589573: step 59870, total loss = 0.86, predict loss = 0.19 (59.9 examples/sec; 0.067 sec/batch; 1h:40m:20s remains)
INFO - root - 2019-11-06 19:45:18.315396: step 59880, total loss = 1.37, predict loss = 0.39 (63.5 examples/sec; 0.063 sec/batch; 1h:34m:32s remains)
INFO - root - 2019-11-06 19:45:18.891743: step 59890, total loss = 1.47, predict loss = 0.41 (100.0 examples/sec; 0.040 sec/batch; 1h:00m:06s remains)
INFO - root - 2019-11-06 19:45:19.354961: step 59900, total loss = 1.15, predict loss = 0.33 (95.7 examples/sec; 0.042 sec/batch; 1h:02m:46s remains)
INFO - root - 2019-11-06 19:45:19.786937: step 59910, total loss = 1.61, predict loss = 0.54 (136.3 examples/sec; 0.029 sec/batch; 0h:44m:04s remains)
INFO - root - 2019-11-06 19:45:21.162845: step 59920, total loss = 0.54, predict loss = 0.14 (64.2 examples/sec; 0.062 sec/batch; 1h:33m:30s remains)
INFO - root - 2019-11-06 19:45:21.900574: step 59930, total loss = 0.78, predict loss = 0.18 (62.1 examples/sec; 0.064 sec/batch; 1h:36m:44s remains)
INFO - root - 2019-11-06 19:45:22.611697: step 59940, total loss = 1.49, predict loss = 0.46 (61.8 examples/sec; 0.065 sec/batch; 1h:37m:05s remains)
INFO - root - 2019-11-06 19:45:23.421178: step 59950, total loss = 0.85, predict loss = 0.17 (54.5 examples/sec; 0.073 sec/batch; 1h:50m:09s remains)
INFO - root - 2019-11-06 19:45:24.124704: step 59960, total loss = 0.84, predict loss = 0.23 (85.6 examples/sec; 0.047 sec/batch; 1h:10m:06s remains)
INFO - root - 2019-11-06 19:45:24.616174: step 59970, total loss = 1.67, predict loss = 0.49 (83.2 examples/sec; 0.048 sec/batch; 1h:12m:08s remains)
INFO - root - 2019-11-06 19:45:25.094032: step 59980, total loss = 0.67, predict loss = 0.17 (106.4 examples/sec; 0.038 sec/batch; 0h:56m:23s remains)
INFO - root - 2019-11-06 19:45:26.274529: step 59990, total loss = 1.03, predict loss = 0.25 (61.7 examples/sec; 0.065 sec/batch; 1h:37m:18s remains)
INFO - root - 2019-11-06 19:45:27.031844: step 60000, total loss = 1.60, predict loss = 0.45 (59.6 examples/sec; 0.067 sec/batch; 1h:40m:44s remains)
INFO - root - 2019-11-06 19:45:28.751372: step 60010, total loss = 0.37, predict loss = 0.09 (73.1 examples/sec; 0.055 sec/batch; 1h:22m:01s remains)
INFO - root - 2019-11-06 19:45:29.490820: step 60020, total loss = 0.82, predict loss = 0.19 (54.7 examples/sec; 0.073 sec/batch; 1h:49m:38s remains)
INFO - root - 2019-11-06 19:45:30.233897: step 60030, total loss = 1.61, predict loss = 0.53 (69.4 examples/sec; 0.058 sec/batch; 1h:26m:26s remains)
INFO - root - 2019-11-06 19:45:30.789705: step 60040, total loss = 0.92, predict loss = 0.25 (96.0 examples/sec; 0.042 sec/batch; 1h:02m:26s remains)
INFO - root - 2019-11-06 19:45:31.239464: step 60050, total loss = 0.82, predict loss = 0.21 (95.3 examples/sec; 0.042 sec/batch; 1h:02m:55s remains)
INFO - root - 2019-11-06 19:45:32.387000: step 60060, total loss = 0.60, predict loss = 0.14 (5.7 examples/sec; 0.703 sec/batch; 17h:34m:01s remains)
INFO - root - 2019-11-06 19:45:33.049275: step 60070, total loss = 1.21, predict loss = 0.34 (63.3 examples/sec; 0.063 sec/batch; 1h:34m:43s remains)
INFO - root - 2019-11-06 19:45:33.762383: step 60080, total loss = 0.93, predict loss = 0.25 (65.6 examples/sec; 0.061 sec/batch; 1h:31m:22s remains)
INFO - root - 2019-11-06 19:45:34.504184: step 60090, total loss = 0.86, predict loss = 0.26 (58.3 examples/sec; 0.069 sec/batch; 1h:42m:50s remains)
INFO - root - 2019-11-06 19:45:35.251780: step 60100, total loss = 0.73, predict loss = 0.17 (61.9 examples/sec; 0.065 sec/batch; 1h:36m:52s remains)
INFO - root - 2019-11-06 19:45:35.922511: step 60110, total loss = 1.44, predict loss = 0.41 (87.3 examples/sec; 0.046 sec/batch; 1h:08m:39s remains)
INFO - root - 2019-11-06 19:45:36.390900: step 60120, total loss = 0.67, predict loss = 0.16 (97.7 examples/sec; 0.041 sec/batch; 1h:01m:20s remains)
INFO - root - 2019-11-06 19:45:36.848914: step 60130, total loss = 0.77, predict loss = 0.20 (95.2 examples/sec; 0.042 sec/batch; 1h:02m:56s remains)
INFO - root - 2019-11-06 19:45:38.122343: step 60140, total loss = 1.17, predict loss = 0.33 (57.7 examples/sec; 0.069 sec/batch; 1h:43m:45s remains)
INFO - root - 2019-11-06 19:45:38.878789: step 60150, total loss = 0.62, predict loss = 0.16 (57.9 examples/sec; 0.069 sec/batch; 1h:43m:27s remains)
INFO - root - 2019-11-06 19:45:39.625515: step 60160, total loss = 1.99, predict loss = 0.56 (56.6 examples/sec; 0.071 sec/batch; 1h:45m:53s remains)
INFO - root - 2019-11-06 19:45:40.382769: step 60170, total loss = 0.88, predict loss = 0.24 (57.8 examples/sec; 0.069 sec/batch; 1h:43m:33s remains)
INFO - root - 2019-11-06 19:45:41.126876: step 60180, total loss = 1.23, predict loss = 0.37 (65.4 examples/sec; 0.061 sec/batch; 1h:31m:29s remains)
INFO - root - 2019-11-06 19:45:41.662847: step 60190, total loss = 1.41, predict loss = 0.40 (98.1 examples/sec; 0.041 sec/batch; 1h:01m:01s remains)
INFO - root - 2019-11-06 19:45:42.119482: step 60200, total loss = 1.59, predict loss = 0.49 (95.5 examples/sec; 0.042 sec/batch; 1h:02m:42s remains)
INFO - root - 2019-11-06 19:45:43.224304: step 60210, total loss = 0.60, predict loss = 0.13 (71.5 examples/sec; 0.056 sec/batch; 1h:23m:42s remains)
INFO - root - 2019-11-06 19:45:43.985611: step 60220, total loss = 0.95, predict loss = 0.27 (53.3 examples/sec; 0.075 sec/batch; 1h:52m:22s remains)
INFO - root - 2019-11-06 19:45:44.692520: step 60230, total loss = 0.92, predict loss = 0.25 (67.3 examples/sec; 0.059 sec/batch; 1h:28m:59s remains)
INFO - root - 2019-11-06 19:45:45.344833: step 60240, total loss = 0.89, predict loss = 0.24 (64.1 examples/sec; 0.062 sec/batch; 1h:33m:19s remains)
INFO - root - 2019-11-06 19:45:46.024082: step 60250, total loss = 1.15, predict loss = 0.32 (66.0 examples/sec; 0.061 sec/batch; 1h:30m:36s remains)
INFO - root - 2019-11-06 19:45:46.685962: step 60260, total loss = 1.21, predict loss = 0.34 (91.8 examples/sec; 0.044 sec/batch; 1h:05m:11s remains)
INFO - root - 2019-11-06 19:45:47.136486: step 60270, total loss = 1.31, predict loss = 0.38 (95.9 examples/sec; 0.042 sec/batch; 1h:02m:20s remains)
INFO - root - 2019-11-06 19:45:47.585734: step 60280, total loss = 0.43, predict loss = 0.10 (99.3 examples/sec; 0.040 sec/batch; 1h:00m:13s remains)
INFO - root - 2019-11-06 19:45:48.857849: step 60290, total loss = 1.21, predict loss = 0.34 (67.2 examples/sec; 0.059 sec/batch; 1h:28m:56s remains)
INFO - root - 2019-11-06 19:45:49.606720: step 60300, total loss = 1.98, predict loss = 0.58 (59.8 examples/sec; 0.067 sec/batch; 1h:40m:03s remains)
INFO - root - 2019-11-06 19:45:50.337834: step 60310, total loss = 0.58, predict loss = 0.16 (59.1 examples/sec; 0.068 sec/batch; 1h:41m:15s remains)
INFO - root - 2019-11-06 19:45:51.105385: step 60320, total loss = 1.50, predict loss = 0.46 (60.6 examples/sec; 0.066 sec/batch; 1h:38m:40s remains)
INFO - root - 2019-11-06 19:45:51.845482: step 60330, total loss = 1.01, predict loss = 0.24 (62.5 examples/sec; 0.064 sec/batch; 1h:35m:42s remains)
INFO - root - 2019-11-06 19:45:52.376871: step 60340, total loss = 0.82, predict loss = 0.21 (101.4 examples/sec; 0.039 sec/batch; 0h:58m:58s remains)
INFO - root - 2019-11-06 19:45:52.823925: step 60350, total loss = 0.41, predict loss = 0.09 (99.7 examples/sec; 0.040 sec/batch; 0h:59m:55s remains)
INFO - root - 2019-11-06 19:45:54.004676: step 60360, total loss = 1.08, predict loss = 0.26 (48.0 examples/sec; 0.083 sec/batch; 2h:04m:26s remains)
INFO - root - 2019-11-06 19:45:54.712947: step 60370, total loss = 0.75, predict loss = 0.21 (57.6 examples/sec; 0.069 sec/batch; 1h:43m:43s remains)
INFO - root - 2019-11-06 19:45:55.449470: step 60380, total loss = 1.52, predict loss = 0.47 (60.2 examples/sec; 0.066 sec/batch; 1h:39m:11s remains)
INFO - root - 2019-11-06 19:45:56.131182: step 60390, total loss = 0.68, predict loss = 0.18 (71.5 examples/sec; 0.056 sec/batch; 1h:23m:30s remains)
INFO - root - 2019-11-06 19:45:56.811111: step 60400, total loss = 0.71, predict loss = 0.19 (64.1 examples/sec; 0.062 sec/batch; 1h:33m:13s remains)
INFO - root - 2019-11-06 19:45:57.461795: step 60410, total loss = 0.86, predict loss = 0.25 (96.6 examples/sec; 0.041 sec/batch; 1h:01m:51s remains)
INFO - root - 2019-11-06 19:45:57.939800: step 60420, total loss = 0.66, predict loss = 0.16 (95.5 examples/sec; 0.042 sec/batch; 1h:02m:32s remains)
INFO - root - 2019-11-06 19:45:58.386875: step 60430, total loss = 1.42, predict loss = 0.36 (97.6 examples/sec; 0.041 sec/batch; 1h:01m:12s remains)
INFO - root - 2019-11-06 19:45:59.702209: step 60440, total loss = 0.81, predict loss = 0.22 (53.4 examples/sec; 0.075 sec/batch; 1h:51m:43s remains)
INFO - root - 2019-11-06 19:46:00.489994: step 60450, total loss = 1.27, predict loss = 0.37 (61.4 examples/sec; 0.065 sec/batch; 1h:37m:14s remains)
INFO - root - 2019-11-06 19:46:01.274082: step 60460, total loss = 1.78, predict loss = 0.53 (60.7 examples/sec; 0.066 sec/batch; 1h:38m:20s remains)
INFO - root - 2019-11-06 19:46:02.044171: step 60470, total loss = 1.00, predict loss = 0.22 (57.4 examples/sec; 0.070 sec/batch; 1h:44m:03s remains)
INFO - root - 2019-11-06 19:46:02.773145: step 60480, total loss = 0.99, predict loss = 0.23 (68.9 examples/sec; 0.058 sec/batch; 1h:26m:35s remains)
INFO - root - 2019-11-06 19:46:03.283661: step 60490, total loss = 0.61, predict loss = 0.15 (100.1 examples/sec; 0.040 sec/batch; 0h:59m:35s remains)
INFO - root - 2019-11-06 19:46:03.758326: step 60500, total loss = 1.53, predict loss = 0.46 (102.9 examples/sec; 0.039 sec/batch; 0h:58m:00s remains)
INFO - root - 2019-11-06 19:46:04.970833: step 60510, total loss = 1.36, predict loss = 0.38 (66.9 examples/sec; 0.060 sec/batch; 1h:29m:13s remains)
INFO - root - 2019-11-06 19:46:05.668753: step 60520, total loss = 1.10, predict loss = 0.33 (59.1 examples/sec; 0.068 sec/batch; 1h:40m:52s remains)
INFO - root - 2019-11-06 19:46:06.396147: step 60530, total loss = 0.96, predict loss = 0.23 (56.2 examples/sec; 0.071 sec/batch; 1h:46m:06s remains)
INFO - root - 2019-11-06 19:46:07.170514: step 60540, total loss = 2.09, predict loss = 0.65 (55.7 examples/sec; 0.072 sec/batch; 1h:47m:07s remains)
INFO - root - 2019-11-06 19:46:07.932539: step 60550, total loss = 1.19, predict loss = 0.31 (56.3 examples/sec; 0.071 sec/batch; 1h:45m:59s remains)
INFO - root - 2019-11-06 19:46:08.518984: step 60560, total loss = 0.77, predict loss = 0.22 (97.7 examples/sec; 0.041 sec/batch; 1h:01m:00s remains)
INFO - root - 2019-11-06 19:46:08.979577: step 60570, total loss = 0.95, predict loss = 0.27 (91.0 examples/sec; 0.044 sec/batch; 1h:05m:32s remains)
INFO - root - 2019-11-06 19:46:09.440339: step 60580, total loss = 0.75, predict loss = 0.20 (101.8 examples/sec; 0.039 sec/batch; 0h:58m:34s remains)
INFO - root - 2019-11-06 19:46:10.704060: step 60590, total loss = 0.77, predict loss = 0.19 (59.6 examples/sec; 0.067 sec/batch; 1h:40m:00s remains)
INFO - root - 2019-11-06 19:46:11.510321: step 60600, total loss = 0.91, predict loss = 0.25 (56.5 examples/sec; 0.071 sec/batch; 1h:45m:33s remains)
INFO - root - 2019-11-06 19:46:12.268241: step 60610, total loss = 0.88, predict loss = 0.24 (61.2 examples/sec; 0.065 sec/batch; 1h:37m:18s remains)
INFO - root - 2019-11-06 19:46:13.035289: step 60620, total loss = 0.58, predict loss = 0.14 (68.3 examples/sec; 0.059 sec/batch; 1h:27m:17s remains)
INFO - root - 2019-11-06 19:46:13.668602: step 60630, total loss = 1.05, predict loss = 0.31 (72.4 examples/sec; 0.055 sec/batch; 1h:22m:18s remains)
INFO - root - 2019-11-06 19:46:14.123925: step 60640, total loss = 0.71, predict loss = 0.19 (96.2 examples/sec; 0.042 sec/batch; 1h:01m:55s remains)
INFO - root - 2019-11-06 19:46:14.579170: step 60650, total loss = 0.73, predict loss = 0.19 (90.6 examples/sec; 0.044 sec/batch; 1h:05m:46s remains)
INFO - root - 2019-11-06 19:46:15.793942: step 60660, total loss = 1.15, predict loss = 0.30 (63.7 examples/sec; 0.063 sec/batch; 1h:33m:34s remains)
INFO - root - 2019-11-06 19:46:16.543834: step 60670, total loss = 0.93, predict loss = 0.25 (54.0 examples/sec; 0.074 sec/batch; 1h:50m:15s remains)
INFO - root - 2019-11-06 19:46:17.364597: step 60680, total loss = 0.98, predict loss = 0.26 (54.3 examples/sec; 0.074 sec/batch; 1h:49m:37s remains)
INFO - root - 2019-11-06 19:46:18.106789: step 60690, total loss = 0.83, predict loss = 0.19 (52.0 examples/sec; 0.077 sec/batch; 1h:54m:30s remains)
INFO - root - 2019-11-06 19:46:18.838430: step 60700, total loss = 0.85, predict loss = 0.24 (61.7 examples/sec; 0.065 sec/batch; 1h:36m:31s remains)
INFO - root - 2019-11-06 19:46:19.387573: step 60710, total loss = 1.65, predict loss = 0.43 (101.2 examples/sec; 0.040 sec/batch; 0h:58m:49s remains)
INFO - root - 2019-11-06 19:46:19.823692: step 60720, total loss = 0.72, predict loss = 0.18 (100.1 examples/sec; 0.040 sec/batch; 0h:59m:27s remains)
INFO - root - 2019-11-06 19:46:20.261980: step 60730, total loss = 0.76, predict loss = 0.21 (130.3 examples/sec; 0.031 sec/batch; 0h:45m:40s remains)
INFO - root - 2019-11-06 19:46:21.613765: step 60740, total loss = 0.76, predict loss = 0.21 (61.1 examples/sec; 0.065 sec/batch; 1h:37m:25s remains)
INFO - root - 2019-11-06 19:46:22.349496: step 60750, total loss = 0.97, predict loss = 0.29 (60.3 examples/sec; 0.066 sec/batch; 1h:38m:42s remains)
INFO - root - 2019-11-06 19:46:23.103123: step 60760, total loss = 0.88, predict loss = 0.24 (56.8 examples/sec; 0.070 sec/batch; 1h:44m:41s remains)
INFO - root - 2019-11-06 19:46:23.865207: step 60770, total loss = 0.82, predict loss = 0.21 (56.7 examples/sec; 0.071 sec/batch; 1h:44m:55s remains)
INFO - root - 2019-11-06 19:46:24.608175: step 60780, total loss = 1.74, predict loss = 0.50 (72.2 examples/sec; 0.055 sec/batch; 1h:22m:20s remains)
INFO - root - 2019-11-06 19:46:25.065424: step 60790, total loss = 1.04, predict loss = 0.32 (100.2 examples/sec; 0.040 sec/batch; 0h:59m:22s remains)
INFO - root - 2019-11-06 19:46:25.499420: step 60800, total loss = 1.24, predict loss = 0.32 (98.9 examples/sec; 0.040 sec/batch; 1h:00m:08s remains)
INFO - root - 2019-11-06 19:46:26.720296: step 60810, total loss = 1.10, predict loss = 0.33 (58.2 examples/sec; 0.069 sec/batch; 1h:42m:05s remains)
INFO - root - 2019-11-06 19:46:27.507923: step 60820, total loss = 0.77, predict loss = 0.18 (54.5 examples/sec; 0.073 sec/batch; 1h:49m:05s remains)
INFO - root - 2019-11-06 19:46:28.257039: step 60830, total loss = 0.88, predict loss = 0.20 (60.2 examples/sec; 0.066 sec/batch; 1h:38m:47s remains)
INFO - root - 2019-11-06 19:46:29.018829: step 60840, total loss = 1.12, predict loss = 0.31 (56.1 examples/sec; 0.071 sec/batch; 1h:45m:53s remains)
INFO - root - 2019-11-06 19:46:29.832560: step 60850, total loss = 0.98, predict loss = 0.28 (70.6 examples/sec; 0.057 sec/batch; 1h:24m:14s remains)
INFO - root - 2019-11-06 19:46:30.385332: step 60860, total loss = 0.76, predict loss = 0.20 (96.9 examples/sec; 0.041 sec/batch; 1h:01m:21s remains)
INFO - root - 2019-11-06 19:46:30.842591: step 60870, total loss = 1.00, predict loss = 0.29 (94.6 examples/sec; 0.042 sec/batch; 1h:02m:49s remains)
INFO - root - 2019-11-06 19:46:31.990656: step 60880, total loss = 0.96, predict loss = 0.25 (5.3 examples/sec; 0.748 sec/batch; 18h:31m:15s remains)
INFO - root - 2019-11-06 19:46:32.667648: step 60890, total loss = 1.02, predict loss = 0.29 (61.3 examples/sec; 0.065 sec/batch; 1h:36m:57s remains)
INFO - root - 2019-11-06 19:46:33.387094: step 60900, total loss = 0.48, predict loss = 0.12 (59.6 examples/sec; 0.067 sec/batch; 1h:39m:35s remains)
INFO - root - 2019-11-06 19:46:34.159623: step 60910, total loss = 0.82, predict loss = 0.20 (56.9 examples/sec; 0.070 sec/batch; 1h:44m:19s remains)
INFO - root - 2019-11-06 19:46:34.880415: step 60920, total loss = 0.63, predict loss = 0.16 (62.5 examples/sec; 0.064 sec/batch; 1h:35m:00s remains)
INFO - root - 2019-11-06 19:46:35.508483: step 60930, total loss = 0.68, predict loss = 0.18 (98.3 examples/sec; 0.041 sec/batch; 1h:00m:23s remains)
INFO - root - 2019-11-06 19:46:35.975548: step 60940, total loss = 1.30, predict loss = 0.33 (94.7 examples/sec; 0.042 sec/batch; 1h:02m:40s remains)
INFO - root - 2019-11-06 19:46:36.414860: step 60950, total loss = 0.72, predict loss = 0.19 (99.4 examples/sec; 0.040 sec/batch; 0h:59m:42s remains)
INFO - root - 2019-11-06 19:46:37.652502: step 60960, total loss = 0.93, predict loss = 0.24 (69.8 examples/sec; 0.057 sec/batch; 1h:25m:06s remains)
INFO - root - 2019-11-06 19:46:38.385408: step 60970, total loss = 0.77, predict loss = 0.19 (66.9 examples/sec; 0.060 sec/batch; 1h:28m:40s remains)
INFO - root - 2019-11-06 19:46:39.104554: step 60980, total loss = 0.52, predict loss = 0.14 (60.5 examples/sec; 0.066 sec/batch; 1h:38m:10s remains)
INFO - root - 2019-11-06 19:46:39.797415: step 60990, total loss = 0.84, predict loss = 0.22 (58.3 examples/sec; 0.069 sec/batch; 1h:41m:49s remains)
INFO - root - 2019-11-06 19:46:40.499674: step 61000, total loss = 0.60, predict loss = 0.15 (65.2 examples/sec; 0.061 sec/batch; 1h:30m:57s remains)
INFO - root - 2019-11-06 19:46:41.012691: step 61010, total loss = 1.50, predict loss = 0.50 (98.6 examples/sec; 0.041 sec/batch; 1h:00m:11s remains)
INFO - root - 2019-11-06 19:46:41.478733: step 61020, total loss = 1.93, predict loss = 0.57 (97.2 examples/sec; 0.041 sec/batch; 1h:01m:01s remains)
INFO - root - 2019-11-06 19:46:42.639079: step 61030, total loss = 1.13, predict loss = 0.29 (69.4 examples/sec; 0.058 sec/batch; 1h:25m:30s remains)
INFO - root - 2019-11-06 19:46:43.354223: step 61040, total loss = 1.31, predict loss = 0.41 (52.2 examples/sec; 0.077 sec/batch; 1h:53m:32s remains)
INFO - root - 2019-11-06 19:46:44.155690: step 61050, total loss = 1.25, predict loss = 0.33 (56.4 examples/sec; 0.071 sec/batch; 1h:45m:08s remains)
INFO - root - 2019-11-06 19:46:44.931521: step 61060, total loss = 0.89, predict loss = 0.24 (66.8 examples/sec; 0.060 sec/batch; 1h:28m:45s remains)
INFO - root - 2019-11-06 19:46:45.696572: step 61070, total loss = 0.71, predict loss = 0.19 (61.7 examples/sec; 0.065 sec/batch; 1h:36m:04s remains)
INFO - root - 2019-11-06 19:46:46.371835: step 61080, total loss = 0.68, predict loss = 0.18 (93.8 examples/sec; 0.043 sec/batch; 1h:03m:13s remains)
INFO - root - 2019-11-06 19:46:46.826771: step 61090, total loss = 1.01, predict loss = 0.31 (93.4 examples/sec; 0.043 sec/batch; 1h:03m:27s remains)
INFO - root - 2019-11-06 19:46:47.307452: step 61100, total loss = 1.02, predict loss = 0.29 (103.3 examples/sec; 0.039 sec/batch; 0h:57m:22s remains)
INFO - root - 2019-11-06 19:46:48.561917: step 61110, total loss = 0.96, predict loss = 0.27 (60.5 examples/sec; 0.066 sec/batch; 1h:37m:55s remains)
INFO - root - 2019-11-06 19:46:49.295126: step 61120, total loss = 0.83, predict loss = 0.20 (59.8 examples/sec; 0.067 sec/batch; 1h:39m:03s remains)
INFO - root - 2019-11-06 19:46:50.067962: step 61130, total loss = 0.72, predict loss = 0.18 (52.2 examples/sec; 0.077 sec/batch; 1h:53m:24s remains)
INFO - root - 2019-11-06 19:46:50.838360: step 61140, total loss = 0.92, predict loss = 0.25 (55.3 examples/sec; 0.072 sec/batch; 1h:47m:12s remains)
INFO - root - 2019-11-06 19:46:51.610189: step 61150, total loss = 1.43, predict loss = 0.42 (68.3 examples/sec; 0.059 sec/batch; 1h:26m:45s remains)
INFO - root - 2019-11-06 19:46:52.151630: step 61160, total loss = 1.00, predict loss = 0.26 (92.9 examples/sec; 0.043 sec/batch; 1h:03m:45s remains)
INFO - root - 2019-11-06 19:46:52.597686: step 61170, total loss = 1.30, predict loss = 0.39 (93.1 examples/sec; 0.043 sec/batch; 1h:03m:36s remains)
INFO - root - 2019-11-06 19:46:53.805376: step 61180, total loss = 1.40, predict loss = 0.41 (63.5 examples/sec; 0.063 sec/batch; 1h:33m:12s remains)
INFO - root - 2019-11-06 19:46:54.530467: step 61190, total loss = 0.76, predict loss = 0.21 (64.1 examples/sec; 0.062 sec/batch; 1h:32m:24s remains)
INFO - root - 2019-11-06 19:46:55.305935: step 61200, total loss = 1.07, predict loss = 0.31 (57.7 examples/sec; 0.069 sec/batch; 1h:42m:34s remains)
INFO - root - 2019-11-06 19:46:56.115056: step 61210, total loss = 0.78, predict loss = 0.20 (56.5 examples/sec; 0.071 sec/batch; 1h:44m:49s remains)
INFO - root - 2019-11-06 19:46:56.849468: step 61220, total loss = 0.52, predict loss = 0.13 (59.3 examples/sec; 0.067 sec/batch; 1h:39m:46s remains)
INFO - root - 2019-11-06 19:46:57.459389: step 61230, total loss = 1.00, predict loss = 0.26 (97.6 examples/sec; 0.041 sec/batch; 1h:00m:39s remains)
INFO - root - 2019-11-06 19:46:57.903379: step 61240, total loss = 0.69, predict loss = 0.16 (100.4 examples/sec; 0.040 sec/batch; 0h:58m:56s remains)
INFO - root - 2019-11-06 19:46:58.373215: step 61250, total loss = 1.20, predict loss = 0.34 (93.6 examples/sec; 0.043 sec/batch; 1h:03m:12s remains)
INFO - root - 2019-11-06 19:46:59.719272: step 61260, total loss = 0.78, predict loss = 0.21 (58.8 examples/sec; 0.068 sec/batch; 1h:40m:41s remains)
INFO - root - 2019-11-06 19:47:00.425251: step 61270, total loss = 0.88, predict loss = 0.23 (61.2 examples/sec; 0.065 sec/batch; 1h:36m:35s remains)
INFO - root - 2019-11-06 19:47:01.169963: step 61280, total loss = 0.86, predict loss = 0.26 (59.0 examples/sec; 0.068 sec/batch; 1h:40m:11s remains)
INFO - root - 2019-11-06 19:47:01.942070: step 61290, total loss = 1.75, predict loss = 0.57 (52.4 examples/sec; 0.076 sec/batch; 1h:52m:54s remains)
INFO - root - 2019-11-06 19:47:02.748129: step 61300, total loss = 0.51, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 1h:27m:07s remains)
INFO - root - 2019-11-06 19:47:03.276649: step 61310, total loss = 1.01, predict loss = 0.26 (94.8 examples/sec; 0.042 sec/batch; 1h:02m:22s remains)
INFO - root - 2019-11-06 19:47:03.725294: step 61320, total loss = 0.81, predict loss = 0.23 (93.8 examples/sec; 0.043 sec/batch; 1h:03m:00s remains)
INFO - root - 2019-11-06 19:47:04.918749: step 61330, total loss = 1.28, predict loss = 0.36 (62.2 examples/sec; 0.064 sec/batch; 1h:35m:01s remains)
INFO - root - 2019-11-06 19:47:05.623125: step 61340, total loss = 0.76, predict loss = 0.20 (65.2 examples/sec; 0.061 sec/batch; 1h:30m:35s remains)
INFO - root - 2019-11-06 19:47:06.359084: step 61350, total loss = 1.12, predict loss = 0.32 (60.7 examples/sec; 0.066 sec/batch; 1h:37m:22s remains)
INFO - root - 2019-11-06 19:47:07.098274: step 61360, total loss = 1.00, predict loss = 0.24 (52.8 examples/sec; 0.076 sec/batch; 1h:51m:50s remains)
INFO - root - 2019-11-06 19:47:07.799211: step 61370, total loss = 0.46, predict loss = 0.10 (61.9 examples/sec; 0.065 sec/batch; 1h:35m:28s remains)
INFO - root - 2019-11-06 19:47:08.362355: step 61380, total loss = 0.77, predict loss = 0.21 (104.1 examples/sec; 0.038 sec/batch; 0h:56m:45s remains)
INFO - root - 2019-11-06 19:47:08.805149: step 61390, total loss = 2.00, predict loss = 0.57 (97.8 examples/sec; 0.041 sec/batch; 1h:00m:22s remains)
INFO - root - 2019-11-06 19:47:09.255104: step 61400, total loss = 0.52, predict loss = 0.12 (103.4 examples/sec; 0.039 sec/batch; 0h:57m:08s remains)
INFO - root - 2019-11-06 19:47:10.568940: step 61410, total loss = 1.49, predict loss = 0.45 (56.2 examples/sec; 0.071 sec/batch; 1h:45m:00s remains)
INFO - root - 2019-11-06 19:47:11.311585: step 61420, total loss = 0.59, predict loss = 0.14 (61.7 examples/sec; 0.065 sec/batch; 1h:35m:40s remains)
INFO - root - 2019-11-06 19:47:12.106145: step 61430, total loss = 1.13, predict loss = 0.41 (60.5 examples/sec; 0.066 sec/batch; 1h:37m:39s remains)
INFO - root - 2019-11-06 19:47:12.873795: step 61440, total loss = 1.32, predict loss = 0.37 (55.5 examples/sec; 0.072 sec/batch; 1h:46m:27s remains)
INFO - root - 2019-11-06 19:47:13.603299: step 61450, total loss = 1.11, predict loss = 0.27 (67.6 examples/sec; 0.059 sec/batch; 1h:27m:20s remains)
INFO - root - 2019-11-06 19:47:14.105376: step 61460, total loss = 1.04, predict loss = 0.25 (101.0 examples/sec; 0.040 sec/batch; 0h:58m:27s remains)
INFO - root - 2019-11-06 19:47:14.553327: step 61470, total loss = 1.08, predict loss = 0.27 (90.8 examples/sec; 0.044 sec/batch; 1h:05m:00s remains)
INFO - root - 2019-11-06 19:47:15.715724: step 61480, total loss = 0.78, predict loss = 0.18 (70.0 examples/sec; 0.057 sec/batch; 1h:24m:16s remains)
INFO - root - 2019-11-06 19:47:16.402848: step 61490, total loss = 1.11, predict loss = 0.30 (58.4 examples/sec; 0.068 sec/batch; 1h:40m:57s remains)
INFO - root - 2019-11-06 19:47:17.148517: step 61500, total loss = 0.62, predict loss = 0.15 (58.5 examples/sec; 0.068 sec/batch; 1h:40m:54s remains)
INFO - root - 2019-11-06 19:47:17.894186: step 61510, total loss = 0.53, predict loss = 0.14 (55.9 examples/sec; 0.072 sec/batch; 1h:45m:33s remains)
INFO - root - 2019-11-06 19:47:18.643675: step 61520, total loss = 0.85, predict loss = 0.21 (59.4 examples/sec; 0.067 sec/batch; 1h:39m:18s remains)
INFO - root - 2019-11-06 19:47:19.167017: step 61530, total loss = 0.76, predict loss = 0.19 (101.9 examples/sec; 0.039 sec/batch; 0h:57m:52s remains)
INFO - root - 2019-11-06 19:47:19.646809: step 61540, total loss = 1.05, predict loss = 0.30 (90.6 examples/sec; 0.044 sec/batch; 1h:05m:05s remains)
INFO - root - 2019-11-06 19:47:20.094465: step 61550, total loss = 1.54, predict loss = 0.52 (146.2 examples/sec; 0.027 sec/batch; 0h:40m:19s remains)
INFO - root - 2019-11-06 19:47:21.420569: step 61560, total loss = 0.91, predict loss = 0.26 (59.5 examples/sec; 0.067 sec/batch; 1h:39m:09s remains)
INFO - root - 2019-11-06 19:47:22.190312: step 61570, total loss = 0.69, predict loss = 0.20 (60.5 examples/sec; 0.066 sec/batch; 1h:37m:27s remains)
INFO - root - 2019-11-06 19:47:22.924220: step 61580, total loss = 0.53, predict loss = 0.13 (55.5 examples/sec; 0.072 sec/batch; 1h:46m:16s remains)
INFO - root - 2019-11-06 19:47:23.677560: step 61590, total loss = 0.70, predict loss = 0.18 (64.7 examples/sec; 0.062 sec/batch; 1h:31m:02s remains)
INFO - root - 2019-11-06 19:47:24.380527: step 61600, total loss = 1.42, predict loss = 0.37 (73.1 examples/sec; 0.055 sec/batch; 1h:20m:35s remains)
INFO - root - 2019-11-06 19:47:24.842483: step 61610, total loss = 1.60, predict loss = 0.50 (97.2 examples/sec; 0.041 sec/batch; 1h:00m:38s remains)
INFO - root - 2019-11-06 19:47:25.314739: step 61620, total loss = 0.76, predict loss = 0.20 (97.0 examples/sec; 0.041 sec/batch; 1h:00m:44s remains)
INFO - root - 2019-11-06 19:47:26.530346: step 61630, total loss = 0.96, predict loss = 0.23 (66.6 examples/sec; 0.060 sec/batch; 1h:28m:28s remains)
INFO - root - 2019-11-06 19:47:27.249618: step 61640, total loss = 0.54, predict loss = 0.13 (62.1 examples/sec; 0.064 sec/batch; 1h:34m:49s remains)
INFO - root - 2019-11-06 19:47:27.998503: step 61650, total loss = 1.19, predict loss = 0.32 (49.0 examples/sec; 0.082 sec/batch; 2h:00m:07s remains)
INFO - root - 2019-11-06 19:47:28.760982: step 61660, total loss = 0.59, predict loss = 0.15 (56.2 examples/sec; 0.071 sec/batch; 1h:44m:43s remains)
INFO - root - 2019-11-06 19:47:29.498990: step 61670, total loss = 0.81, predict loss = 0.19 (62.7 examples/sec; 0.064 sec/batch; 1h:33m:53s remains)
INFO - root - 2019-11-06 19:47:30.075567: step 61680, total loss = 0.38, predict loss = 0.09 (100.7 examples/sec; 0.040 sec/batch; 0h:58m:28s remains)
INFO - root - 2019-11-06 19:47:30.517175: step 61690, total loss = 1.26, predict loss = 0.36 (101.5 examples/sec; 0.039 sec/batch; 0h:58m:00s remains)
INFO - root - 2019-11-06 19:47:31.676209: step 61700, total loss = 1.20, predict loss = 0.34 (5.5 examples/sec; 0.731 sec/batch; 17h:56m:05s remains)
INFO - root - 2019-11-06 19:47:32.358100: step 61710, total loss = 0.87, predict loss = 0.25 (58.1 examples/sec; 0.069 sec/batch; 1h:41m:15s remains)
INFO - root - 2019-11-06 19:47:33.067696: step 61720, total loss = 0.90, predict loss = 0.24 (66.1 examples/sec; 0.061 sec/batch; 1h:29m:02s remains)
INFO - root - 2019-11-06 19:47:33.845735: step 61730, total loss = 0.81, predict loss = 0.21 (56.4 examples/sec; 0.071 sec/batch; 1h:44m:16s remains)
INFO - root - 2019-11-06 19:47:34.569005: step 61740, total loss = 1.14, predict loss = 0.30 (66.7 examples/sec; 0.060 sec/batch; 1h:28m:09s remains)
INFO - root - 2019-11-06 19:47:35.245612: step 61750, total loss = 0.60, predict loss = 0.15 (85.2 examples/sec; 0.047 sec/batch; 1h:09m:04s remains)
INFO - root - 2019-11-06 19:47:35.681016: step 61760, total loss = 1.02, predict loss = 0.26 (101.0 examples/sec; 0.040 sec/batch; 0h:58m:13s remains)
INFO - root - 2019-11-06 19:47:36.133228: step 61770, total loss = 1.11, predict loss = 0.26 (95.6 examples/sec; 0.042 sec/batch; 1h:01m:33s remains)
INFO - root - 2019-11-06 19:47:37.426536: step 61780, total loss = 0.78, predict loss = 0.21 (61.0 examples/sec; 0.066 sec/batch; 1h:36m:25s remains)
INFO - root - 2019-11-06 19:47:38.187003: step 61790, total loss = 1.65, predict loss = 0.52 (56.5 examples/sec; 0.071 sec/batch; 1h:44m:01s remains)
INFO - root - 2019-11-06 19:47:38.992634: step 61800, total loss = 0.86, predict loss = 0.24 (50.7 examples/sec; 0.079 sec/batch; 1h:56m:03s remains)
INFO - root - 2019-11-06 19:47:39.750372: step 61810, total loss = 0.98, predict loss = 0.27 (67.2 examples/sec; 0.060 sec/batch; 1h:27m:30s remains)
INFO - root - 2019-11-06 19:47:40.468591: step 61820, total loss = 0.52, predict loss = 0.14 (58.9 examples/sec; 0.068 sec/batch; 1h:39m:50s remains)
INFO - root - 2019-11-06 19:47:41.027571: step 61830, total loss = 1.18, predict loss = 0.31 (94.7 examples/sec; 0.042 sec/batch; 1h:02m:04s remains)
INFO - root - 2019-11-06 19:47:41.471035: step 61840, total loss = 0.47, predict loss = 0.12 (93.2 examples/sec; 0.043 sec/batch; 1h:03m:03s remains)
INFO - root - 2019-11-06 19:47:42.606211: step 61850, total loss = 0.70, predict loss = 0.18 (67.1 examples/sec; 0.060 sec/batch; 1h:27m:37s remains)
INFO - root - 2019-11-06 19:47:43.309675: step 61860, total loss = 0.48, predict loss = 0.10 (57.7 examples/sec; 0.069 sec/batch; 1h:41m:48s remains)
INFO - root - 2019-11-06 19:47:44.066418: step 61870, total loss = 1.03, predict loss = 0.32 (55.3 examples/sec; 0.072 sec/batch; 1h:46m:18s remains)
INFO - root - 2019-11-06 19:47:44.815203: step 61880, total loss = 0.54, predict loss = 0.14 (59.9 examples/sec; 0.067 sec/batch; 1h:38m:03s remains)
INFO - root - 2019-11-06 19:47:45.551130: step 61890, total loss = 1.38, predict loss = 0.36 (55.9 examples/sec; 0.072 sec/batch; 1h:45m:01s remains)
INFO - root - 2019-11-06 19:47:46.201103: step 61900, total loss = 0.70, predict loss = 0.18 (97.7 examples/sec; 0.041 sec/batch; 1h:00m:07s remains)
INFO - root - 2019-11-06 19:47:46.639386: step 61910, total loss = 0.63, predict loss = 0.15 (97.6 examples/sec; 0.041 sec/batch; 1h:00m:08s remains)
INFO - root - 2019-11-06 19:47:47.088555: step 61920, total loss = 1.46, predict loss = 0.46 (100.2 examples/sec; 0.040 sec/batch; 0h:58m:35s remains)
INFO - root - 2019-11-06 19:47:48.309410: step 61930, total loss = 0.70, predict loss = 0.17 (64.7 examples/sec; 0.062 sec/batch; 1h:30m:48s remains)
INFO - root - 2019-11-06 19:47:49.088640: step 61940, total loss = 0.84, predict loss = 0.20 (64.6 examples/sec; 0.062 sec/batch; 1h:30m:52s remains)
INFO - root - 2019-11-06 19:47:49.875540: step 61950, total loss = 1.24, predict loss = 0.33 (56.2 examples/sec; 0.071 sec/batch; 1h:44m:25s remains)
INFO - root - 2019-11-06 19:47:50.608472: step 61960, total loss = 0.87, predict loss = 0.28 (51.0 examples/sec; 0.078 sec/batch; 1h:55m:01s remains)
INFO - root - 2019-11-06 19:47:51.386858: step 61970, total loss = 0.78, predict loss = 0.21 (67.9 examples/sec; 0.059 sec/batch; 1h:26m:27s remains)
INFO - root - 2019-11-06 19:47:51.937624: step 61980, total loss = 0.56, predict loss = 0.14 (100.8 examples/sec; 0.040 sec/batch; 0h:58m:12s remains)
INFO - root - 2019-11-06 19:47:52.382752: step 61990, total loss = 1.51, predict loss = 0.38 (98.2 examples/sec; 0.041 sec/batch; 0h:59m:45s remains)
INFO - root - 2019-11-06 19:47:53.530313: step 62000, total loss = 0.78, predict loss = 0.19 (70.8 examples/sec; 0.057 sec/batch; 1h:22m:52s remains)
INFO - root - 2019-11-06 19:47:54.236827: step 62010, total loss = 1.12, predict loss = 0.36 (74.1 examples/sec; 0.054 sec/batch; 1h:19m:08s remains)
INFO - root - 2019-11-06 19:47:55.019559: step 62020, total loss = 0.52, predict loss = 0.13 (59.9 examples/sec; 0.067 sec/batch; 1h:37m:50s remains)
INFO - root - 2019-11-06 19:47:55.780879: step 62030, total loss = 1.20, predict loss = 0.36 (58.2 examples/sec; 0.069 sec/batch; 1h:40m:46s remains)
INFO - root - 2019-11-06 19:47:56.524141: step 62040, total loss = 1.01, predict loss = 0.28 (55.8 examples/sec; 0.072 sec/batch; 1h:45m:07s remains)
INFO - root - 2019-11-06 19:47:57.099853: step 62050, total loss = 0.51, predict loss = 0.13 (100.4 examples/sec; 0.040 sec/batch; 0h:58m:23s remains)
INFO - root - 2019-11-06 19:47:57.559939: step 62060, total loss = 0.66, predict loss = 0.20 (102.0 examples/sec; 0.039 sec/batch; 0h:57m:29s remains)
INFO - root - 2019-11-06 19:47:58.004826: step 62070, total loss = 0.63, predict loss = 0.16 (93.9 examples/sec; 0.043 sec/batch; 1h:02m:27s remains)
INFO - root - 2019-11-06 19:47:59.339456: step 62080, total loss = 0.77, predict loss = 0.20 (60.4 examples/sec; 0.066 sec/batch; 1h:36m:59s remains)
INFO - root - 2019-11-06 19:48:00.081068: step 62090, total loss = 2.14, predict loss = 0.63 (54.4 examples/sec; 0.074 sec/batch; 1h:47m:49s remains)
INFO - root - 2019-11-06 19:48:00.844794: step 62100, total loss = 1.55, predict loss = 0.43 (62.4 examples/sec; 0.064 sec/batch; 1h:33m:50s remains)
INFO - root - 2019-11-06 19:48:01.547347: step 62110, total loss = 0.42, predict loss = 0.09 (64.5 examples/sec; 0.062 sec/batch; 1h:30m:50s remains)
INFO - root - 2019-11-06 19:48:02.266157: step 62120, total loss = 1.44, predict loss = 0.38 (70.2 examples/sec; 0.057 sec/batch; 1h:23m:30s remains)
INFO - root - 2019-11-06 19:48:02.760601: step 62130, total loss = 1.07, predict loss = 0.26 (95.6 examples/sec; 0.042 sec/batch; 1h:01m:16s remains)
INFO - root - 2019-11-06 19:48:03.231473: step 62140, total loss = 0.93, predict loss = 0.22 (100.6 examples/sec; 0.040 sec/batch; 0h:58m:13s remains)
INFO - root - 2019-11-06 19:48:04.409670: step 62150, total loss = 1.31, predict loss = 0.34 (62.9 examples/sec; 0.064 sec/batch; 1h:33m:02s remains)
INFO - root - 2019-11-06 19:48:05.115581: step 62160, total loss = 1.10, predict loss = 0.29 (56.6 examples/sec; 0.071 sec/batch; 1h:43m:25s remains)
INFO - root - 2019-11-06 19:48:05.876963: step 62170, total loss = 1.58, predict loss = 0.48 (54.9 examples/sec; 0.073 sec/batch; 1h:46m:41s remains)
INFO - root - 2019-11-06 19:48:06.620815: step 62180, total loss = 0.63, predict loss = 0.16 (63.7 examples/sec; 0.063 sec/batch; 1h:31m:52s remains)
INFO - root - 2019-11-06 19:48:07.364925: step 62190, total loss = 1.35, predict loss = 0.38 (54.0 examples/sec; 0.074 sec/batch; 1h:48m:22s remains)
INFO - root - 2019-11-06 19:48:07.957126: step 62200, total loss = 0.72, predict loss = 0.18 (95.8 examples/sec; 0.042 sec/batch; 1h:01m:06s remains)
INFO - root - 2019-11-06 19:48:08.414606: step 62210, total loss = 0.92, predict loss = 0.18 (93.1 examples/sec; 0.043 sec/batch; 1h:02m:51s remains)
INFO - root - 2019-11-06 19:48:08.881365: step 62220, total loss = 1.20, predict loss = 0.36 (98.1 examples/sec; 0.041 sec/batch; 0h:59m:40s remains)
INFO - root - 2019-11-06 19:48:10.205199: step 62230, total loss = 1.07, predict loss = 0.32 (57.2 examples/sec; 0.070 sec/batch; 1h:42m:22s remains)
INFO - root - 2019-11-06 19:48:10.942394: step 62240, total loss = 0.46, predict loss = 0.10 (56.5 examples/sec; 0.071 sec/batch; 1h:43m:28s remains)
INFO - root - 2019-11-06 19:48:11.716634: step 62250, total loss = 1.02, predict loss = 0.23 (56.1 examples/sec; 0.071 sec/batch; 1h:44m:17s remains)
INFO - root - 2019-11-06 19:48:12.477097: step 62260, total loss = 1.41, predict loss = 0.40 (61.9 examples/sec; 0.065 sec/batch; 1h:34m:25s remains)
INFO - root - 2019-11-06 19:48:13.177717: step 62270, total loss = 0.90, predict loss = 0.23 (60.5 examples/sec; 0.066 sec/batch; 1h:36m:38s remains)
INFO - root - 2019-11-06 19:48:13.689703: step 62280, total loss = 0.58, predict loss = 0.14 (97.8 examples/sec; 0.041 sec/batch; 0h:59m:48s remains)
INFO - root - 2019-11-06 19:48:14.136835: step 62290, total loss = 1.13, predict loss = 0.32 (95.8 examples/sec; 0.042 sec/batch; 1h:01m:01s remains)
INFO - root - 2019-11-06 19:48:15.369875: step 62300, total loss = 0.89, predict loss = 0.20 (68.7 examples/sec; 0.058 sec/batch; 1h:25m:08s remains)
INFO - root - 2019-11-06 19:48:16.031872: step 62310, total loss = 0.59, predict loss = 0.16 (62.4 examples/sec; 0.064 sec/batch; 1h:33m:38s remains)
INFO - root - 2019-11-06 19:48:16.788406: step 62320, total loss = 0.89, predict loss = 0.20 (61.0 examples/sec; 0.066 sec/batch; 1h:35m:53s remains)
INFO - root - 2019-11-06 19:48:17.551541: step 62330, total loss = 1.57, predict loss = 0.42 (60.3 examples/sec; 0.066 sec/batch; 1h:36m:52s remains)
INFO - root - 2019-11-06 19:48:18.313885: step 62340, total loss = 0.94, predict loss = 0.28 (54.5 examples/sec; 0.073 sec/batch; 1h:47m:16s remains)
INFO - root - 2019-11-06 19:48:18.899146: step 62350, total loss = 1.53, predict loss = 0.44 (98.3 examples/sec; 0.041 sec/batch; 0h:59m:26s remains)
INFO - root - 2019-11-06 19:48:19.360441: step 62360, total loss = 1.50, predict loss = 0.45 (92.5 examples/sec; 0.043 sec/batch; 1h:03m:09s remains)
INFO - root - 2019-11-06 19:48:19.794250: step 62370, total loss = 0.69, predict loss = 0.16 (136.9 examples/sec; 0.029 sec/batch; 0h:42m:40s remains)
INFO - root - 2019-11-06 19:48:21.114672: step 62380, total loss = 0.89, predict loss = 0.25 (55.6 examples/sec; 0.072 sec/batch; 1h:45m:06s remains)
INFO - root - 2019-11-06 19:48:21.881607: step 62390, total loss = 0.56, predict loss = 0.13 (57.4 examples/sec; 0.070 sec/batch; 1h:41m:43s remains)
INFO - root - 2019-11-06 19:48:22.640526: step 62400, total loss = 1.08, predict loss = 0.28 (59.8 examples/sec; 0.067 sec/batch; 1h:37m:36s remains)
INFO - root - 2019-11-06 19:48:23.384024: step 62410, total loss = 0.93, predict loss = 0.25 (53.2 examples/sec; 0.075 sec/batch; 1h:49m:43s remains)
INFO - root - 2019-11-06 19:48:24.101775: step 62420, total loss = 0.81, predict loss = 0.19 (75.3 examples/sec; 0.053 sec/batch; 1h:17m:34s remains)
INFO - root - 2019-11-06 19:48:24.547629: step 62430, total loss = 1.02, predict loss = 0.27 (94.9 examples/sec; 0.042 sec/batch; 1h:01m:32s remains)
INFO - root - 2019-11-06 19:48:24.997877: step 62440, total loss = 1.04, predict loss = 0.31 (90.0 examples/sec; 0.044 sec/batch; 1h:04m:51s remains)
INFO - root - 2019-11-06 19:48:26.228309: step 62450, total loss = 1.69, predict loss = 0.51 (63.8 examples/sec; 0.063 sec/batch; 1h:31m:26s remains)
INFO - root - 2019-11-06 19:48:27.034326: step 62460, total loss = 1.04, predict loss = 0.27 (56.0 examples/sec; 0.071 sec/batch; 1h:44m:14s remains)
INFO - root - 2019-11-06 19:48:27.783150: step 62470, total loss = 1.66, predict loss = 0.50 (60.7 examples/sec; 0.066 sec/batch; 1h:36m:08s remains)
INFO - root - 2019-11-06 19:48:28.561735: step 62480, total loss = 0.77, predict loss = 0.20 (53.8 examples/sec; 0.074 sec/batch; 1h:48m:28s remains)
INFO - root - 2019-11-06 19:48:29.301209: step 62490, total loss = 1.16, predict loss = 0.35 (68.9 examples/sec; 0.058 sec/batch; 1h:24m:40s remains)
INFO - root - 2019-11-06 19:48:29.861825: step 62500, total loss = 0.99, predict loss = 0.27 (102.0 examples/sec; 0.039 sec/batch; 0h:57m:09s remains)
INFO - root - 2019-11-06 19:48:30.313697: step 62510, total loss = 0.37, predict loss = 0.08 (99.7 examples/sec; 0.040 sec/batch; 0h:58m:31s remains)
INFO - root - 2019-11-06 19:48:31.423016: step 62520, total loss = 1.66, predict loss = 0.45 (5.5 examples/sec; 0.723 sec/batch; 17h:34m:14s remains)
INFO - root - 2019-11-06 19:48:32.134733: step 62530, total loss = 0.65, predict loss = 0.15 (54.4 examples/sec; 0.074 sec/batch; 1h:47m:16s remains)
INFO - root - 2019-11-06 19:48:32.868087: step 62540, total loss = 1.05, predict loss = 0.34 (56.0 examples/sec; 0.071 sec/batch; 1h:44m:12s remains)
INFO - root - 2019-11-06 19:48:33.573491: step 62550, total loss = 0.46, predict loss = 0.08 (59.6 examples/sec; 0.067 sec/batch; 1h:37m:44s remains)
INFO - root - 2019-11-06 19:48:34.361567: step 62560, total loss = 0.64, predict loss = 0.14 (54.9 examples/sec; 0.073 sec/batch; 1h:46m:07s remains)
INFO - root - 2019-11-06 19:48:35.029733: step 62570, total loss = 0.63, predict loss = 0.12 (86.4 examples/sec; 0.046 sec/batch; 1h:07m:27s remains)
INFO - root - 2019-11-06 19:48:35.494011: step 62580, total loss = 1.23, predict loss = 0.33 (98.1 examples/sec; 0.041 sec/batch; 0h:59m:24s remains)
INFO - root - 2019-11-06 19:48:35.950805: step 62590, total loss = 1.00, predict loss = 0.31 (97.6 examples/sec; 0.041 sec/batch; 0h:59m:41s remains)
INFO - root - 2019-11-06 19:48:37.197173: step 62600, total loss = 1.34, predict loss = 0.34 (61.6 examples/sec; 0.065 sec/batch; 1h:34m:39s remains)
INFO - root - 2019-11-06 19:48:37.941535: step 62610, total loss = 0.47, predict loss = 0.13 (54.8 examples/sec; 0.073 sec/batch; 1h:46m:14s remains)
INFO - root - 2019-11-06 19:48:38.654685: step 62620, total loss = 0.45, predict loss = 0.10 (63.6 examples/sec; 0.063 sec/batch; 1h:31m:31s remains)
INFO - root - 2019-11-06 19:48:39.416377: step 62630, total loss = 0.61, predict loss = 0.17 (50.8 examples/sec; 0.079 sec/batch; 1h:54m:34s remains)
INFO - root - 2019-11-06 19:48:40.137853: step 62640, total loss = 1.65, predict loss = 0.50 (67.4 examples/sec; 0.059 sec/batch; 1h:26m:27s remains)
INFO - root - 2019-11-06 19:48:40.668023: step 62650, total loss = 1.17, predict loss = 0.34 (100.5 examples/sec; 0.040 sec/batch; 0h:57m:56s remains)
INFO - root - 2019-11-06 19:48:41.142494: step 62660, total loss = 1.19, predict loss = 0.33 (98.1 examples/sec; 0.041 sec/batch; 0h:59m:21s remains)
INFO - root - 2019-11-06 19:48:42.304835: step 62670, total loss = 0.89, predict loss = 0.23 (70.9 examples/sec; 0.056 sec/batch; 1h:22m:08s remains)
INFO - root - 2019-11-06 19:48:43.037568: step 62680, total loss = 0.57, predict loss = 0.16 (56.7 examples/sec; 0.071 sec/batch; 1h:42m:38s remains)
INFO - root - 2019-11-06 19:48:43.803596: step 62690, total loss = 1.08, predict loss = 0.27 (55.5 examples/sec; 0.072 sec/batch; 1h:44m:56s remains)
INFO - root - 2019-11-06 19:48:44.551757: step 62700, total loss = 1.14, predict loss = 0.37 (61.4 examples/sec; 0.065 sec/batch; 1h:34m:51s remains)
INFO - root - 2019-11-06 19:48:45.249540: step 62710, total loss = 0.54, predict loss = 0.13 (64.3 examples/sec; 0.062 sec/batch; 1h:30m:30s remains)
INFO - root - 2019-11-06 19:48:45.842320: step 62720, total loss = 0.54, predict loss = 0.14 (98.8 examples/sec; 0.040 sec/batch; 0h:58m:52s remains)
INFO - root - 2019-11-06 19:48:46.275970: step 62730, total loss = 1.16, predict loss = 0.29 (100.4 examples/sec; 0.040 sec/batch; 0h:57m:56s remains)
INFO - root - 2019-11-06 19:48:46.737920: step 62740, total loss = 0.44, predict loss = 0.11 (101.2 examples/sec; 0.040 sec/batch; 0h:57m:28s remains)
INFO - root - 2019-11-06 19:48:47.995197: step 62750, total loss = 0.95, predict loss = 0.25 (54.5 examples/sec; 0.073 sec/batch; 1h:46m:39s remains)
INFO - root - 2019-11-06 19:48:48.739049: step 62760, total loss = 0.89, predict loss = 0.21 (62.2 examples/sec; 0.064 sec/batch; 1h:33m:29s remains)
INFO - root - 2019-11-06 19:48:49.547934: step 62770, total loss = 0.38, predict loss = 0.09 (54.7 examples/sec; 0.073 sec/batch; 1h:46m:22s remains)
INFO - root - 2019-11-06 19:48:50.294728: step 62780, total loss = 1.24, predict loss = 0.35 (60.6 examples/sec; 0.066 sec/batch; 1h:35m:54s remains)
INFO - root - 2019-11-06 19:48:50.982072: step 62790, total loss = 0.38, predict loss = 0.09 (75.5 examples/sec; 0.053 sec/batch; 1h:17m:00s remains)
INFO - root - 2019-11-06 19:48:51.442348: step 62800, total loss = 0.80, predict loss = 0.19 (102.5 examples/sec; 0.039 sec/batch; 0h:56m:41s remains)
INFO - root - 2019-11-06 19:48:51.886069: step 62810, total loss = 1.65, predict loss = 0.44 (95.0 examples/sec; 0.042 sec/batch; 1h:01m:11s remains)
INFO - root - 2019-11-06 19:48:53.073855: step 62820, total loss = 0.57, predict loss = 0.15 (67.0 examples/sec; 0.060 sec/batch; 1h:26m:46s remains)
INFO - root - 2019-11-06 19:48:53.758857: step 62830, total loss = 1.04, predict loss = 0.31 (52.5 examples/sec; 0.076 sec/batch; 1h:50m:42s remains)
INFO - root - 2019-11-06 19:48:54.495552: step 62840, total loss = 1.09, predict loss = 0.31 (68.5 examples/sec; 0.058 sec/batch; 1h:24m:48s remains)
INFO - root - 2019-11-06 19:48:55.238873: step 62850, total loss = 1.61, predict loss = 0.50 (60.6 examples/sec; 0.066 sec/batch; 1h:35m:49s remains)
INFO - root - 2019-11-06 19:48:55.999133: step 62860, total loss = 0.88, predict loss = 0.24 (59.0 examples/sec; 0.068 sec/batch; 1h:38m:23s remains)
INFO - root - 2019-11-06 19:48:56.596834: step 62870, total loss = 0.78, predict loss = 0.19 (105.5 examples/sec; 0.038 sec/batch; 0h:55m:02s remains)
INFO - root - 2019-11-06 19:48:57.030482: step 62880, total loss = 1.06, predict loss = 0.31 (98.1 examples/sec; 0.041 sec/batch; 0h:59m:13s remains)
INFO - root - 2019-11-06 19:48:57.473056: step 62890, total loss = 1.06, predict loss = 0.28 (106.8 examples/sec; 0.037 sec/batch; 0h:54m:21s remains)
INFO - root - 2019-11-06 19:48:58.776193: step 62900, total loss = 0.62, predict loss = 0.16 (52.6 examples/sec; 0.076 sec/batch; 1h:50m:28s remains)
INFO - root - 2019-11-06 19:48:59.495235: step 62910, total loss = 0.78, predict loss = 0.21 (54.3 examples/sec; 0.074 sec/batch; 1h:46m:57s remains)
INFO - root - 2019-11-06 19:49:00.278401: step 62920, total loss = 1.22, predict loss = 0.35 (57.7 examples/sec; 0.069 sec/batch; 1h:40m:41s remains)
INFO - root - 2019-11-06 19:49:00.996274: step 62930, total loss = 1.47, predict loss = 0.48 (56.7 examples/sec; 0.071 sec/batch; 1h:42m:26s remains)
INFO - root - 2019-11-06 19:49:01.712895: step 62940, total loss = 1.13, predict loss = 0.36 (74.5 examples/sec; 0.054 sec/batch; 1h:17m:52s remains)
INFO - root - 2019-11-06 19:49:02.225485: step 62950, total loss = 1.31, predict loss = 0.37 (93.8 examples/sec; 0.043 sec/batch; 1h:01m:50s remains)
INFO - root - 2019-11-06 19:49:02.686784: step 62960, total loss = 2.21, predict loss = 0.71 (92.4 examples/sec; 0.043 sec/batch; 1h:02m:47s remains)
INFO - root - 2019-11-06 19:49:03.873585: step 62970, total loss = 1.19, predict loss = 0.33 (66.4 examples/sec; 0.060 sec/batch; 1h:27m:25s remains)
INFO - root - 2019-11-06 19:49:04.585962: step 62980, total loss = 0.92, predict loss = 0.22 (60.3 examples/sec; 0.066 sec/batch; 1h:36m:12s remains)
INFO - root - 2019-11-06 19:49:05.314572: step 62990, total loss = 0.53, predict loss = 0.13 (55.9 examples/sec; 0.072 sec/batch; 1h:43m:43s remains)
INFO - root - 2019-11-06 19:49:06.051855: step 63000, total loss = 0.54, predict loss = 0.13 (58.0 examples/sec; 0.069 sec/batch; 1h:40m:04s remains)
INFO - root - 2019-11-06 19:49:06.760583: step 63010, total loss = 0.68, predict loss = 0.14 (65.9 examples/sec; 0.061 sec/batch; 1h:27m:59s remains)
INFO - root - 2019-11-06 19:49:07.386744: step 63020, total loss = 1.07, predict loss = 0.31 (96.5 examples/sec; 0.041 sec/batch; 1h:00m:07s remains)
INFO - root - 2019-11-06 19:49:07.843253: step 63030, total loss = 0.83, predict loss = 0.22 (98.1 examples/sec; 0.041 sec/batch; 0h:59m:06s remains)
INFO - root - 2019-11-06 19:49:08.296400: step 63040, total loss = 0.89, predict loss = 0.25 (97.8 examples/sec; 0.041 sec/batch; 0h:59m:15s remains)
INFO - root - 2019-11-06 19:49:09.659474: step 63050, total loss = 1.24, predict loss = 0.35 (54.5 examples/sec; 0.073 sec/batch; 1h:46m:26s remains)
INFO - root - 2019-11-06 19:49:10.423951: step 63060, total loss = 0.80, predict loss = 0.19 (61.9 examples/sec; 0.065 sec/batch; 1h:33m:42s remains)
INFO - root - 2019-11-06 19:49:11.154447: step 63070, total loss = 1.33, predict loss = 0.37 (65.1 examples/sec; 0.061 sec/batch; 1h:28m:57s remains)
INFO - root - 2019-11-06 19:49:11.902827: step 63080, total loss = 0.53, predict loss = 0.15 (56.9 examples/sec; 0.070 sec/batch; 1h:41m:47s remains)
INFO - root - 2019-11-06 19:49:12.639137: step 63090, total loss = 1.14, predict loss = 0.35 (63.0 examples/sec; 0.063 sec/batch; 1h:31m:54s remains)
INFO - root - 2019-11-06 19:49:13.155417: step 63100, total loss = 0.88, predict loss = 0.19 (93.1 examples/sec; 0.043 sec/batch; 1h:02m:12s remains)
INFO - root - 2019-11-06 19:49:13.596974: step 63110, total loss = 0.76, predict loss = 0.18 (99.0 examples/sec; 0.040 sec/batch; 0h:58m:30s remains)
INFO - root - 2019-11-06 19:49:14.764159: step 63120, total loss = 0.46, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 1h:29m:00s remains)
INFO - root - 2019-11-06 19:49:15.433640: step 63130, total loss = 0.64, predict loss = 0.15 (67.9 examples/sec; 0.059 sec/batch; 1h:25m:18s remains)
INFO - root - 2019-11-06 19:49:16.135968: step 63140, total loss = 0.63, predict loss = 0.17 (65.4 examples/sec; 0.061 sec/batch; 1h:28m:34s remains)
INFO - root - 2019-11-06 19:49:16.897682: step 63150, total loss = 0.64, predict loss = 0.16 (53.4 examples/sec; 0.075 sec/batch; 1h:48m:31s remains)
INFO - root - 2019-11-06 19:49:17.633657: step 63160, total loss = 0.56, predict loss = 0.13 (55.4 examples/sec; 0.072 sec/batch; 1h:44m:32s remains)
INFO - root - 2019-11-06 19:49:18.182648: step 63170, total loss = 0.73, predict loss = 0.19 (96.5 examples/sec; 0.041 sec/batch; 0h:59m:58s remains)
INFO - root - 2019-11-06 19:49:18.658074: step 63180, total loss = 0.51, predict loss = 0.12 (88.7 examples/sec; 0.045 sec/batch; 1h:05m:16s remains)
INFO - root - 2019-11-06 19:49:19.098114: step 63190, total loss = 1.68, predict loss = 0.49 (130.2 examples/sec; 0.031 sec/batch; 0h:44m:27s remains)
INFO - root - 2019-11-06 19:49:20.529097: step 63200, total loss = 1.21, predict loss = 0.27 (47.1 examples/sec; 0.085 sec/batch; 2h:02m:56s remains)
INFO - root - 2019-11-06 19:49:21.229933: step 63210, total loss = 0.73, predict loss = 0.19 (65.6 examples/sec; 0.061 sec/batch; 1h:28m:10s remains)
INFO - root - 2019-11-06 19:49:21.977006: step 63220, total loss = 0.44, predict loss = 0.09 (61.4 examples/sec; 0.065 sec/batch; 1h:34m:09s remains)
INFO - root - 2019-11-06 19:49:22.750181: step 63230, total loss = 0.42, predict loss = 0.09 (53.8 examples/sec; 0.074 sec/batch; 1h:47m:35s remains)
INFO - root - 2019-11-06 19:49:23.432984: step 63240, total loss = 1.41, predict loss = 0.38 (80.6 examples/sec; 0.050 sec/batch; 1h:11m:46s remains)
INFO - root - 2019-11-06 19:49:23.922965: step 63250, total loss = 1.15, predict loss = 0.28 (89.9 examples/sec; 0.044 sec/batch; 1h:04m:18s remains)
INFO - root - 2019-11-06 19:49:24.402925: step 63260, total loss = 1.22, predict loss = 0.34 (100.4 examples/sec; 0.040 sec/batch; 0h:57m:35s remains)
INFO - root - 2019-11-06 19:49:25.637509: step 63270, total loss = 1.13, predict loss = 0.28 (63.7 examples/sec; 0.063 sec/batch; 1h:30m:44s remains)
INFO - root - 2019-11-06 19:49:26.410906: step 63280, total loss = 1.21, predict loss = 0.33 (59.1 examples/sec; 0.068 sec/batch; 1h:37m:49s remains)
INFO - root - 2019-11-06 19:49:27.222464: step 63290, total loss = 1.52, predict loss = 0.42 (58.0 examples/sec; 0.069 sec/batch; 1h:39m:41s remains)
INFO - root - 2019-11-06 19:49:27.969028: step 63300, total loss = 0.96, predict loss = 0.31 (54.4 examples/sec; 0.073 sec/batch; 1h:46m:10s remains)
INFO - root - 2019-11-06 19:49:28.684544: step 63310, total loss = 2.10, predict loss = 0.63 (70.3 examples/sec; 0.057 sec/batch; 1h:22m:11s remains)
INFO - root - 2019-11-06 19:49:29.235880: step 63320, total loss = 1.00, predict loss = 0.28 (102.0 examples/sec; 0.039 sec/batch; 0h:56m:38s remains)
INFO - root - 2019-11-06 19:49:29.672172: step 63330, total loss = 1.16, predict loss = 0.28 (94.5 examples/sec; 0.042 sec/batch; 1h:01m:07s remains)
INFO - root - 2019-11-06 19:49:30.815159: step 63340, total loss = 0.60, predict loss = 0.15 (5.4 examples/sec; 0.742 sec/batch; 17h:52m:15s remains)
INFO - root - 2019-11-06 19:49:31.492958: step 63350, total loss = 0.69, predict loss = 0.14 (60.3 examples/sec; 0.066 sec/batch; 1h:35m:43s remains)
INFO - root - 2019-11-06 19:49:32.195086: step 63360, total loss = 1.20, predict loss = 0.33 (62.9 examples/sec; 0.064 sec/batch; 1h:31m:47s remains)
INFO - root - 2019-11-06 19:49:32.955563: step 63370, total loss = 0.90, predict loss = 0.22 (53.2 examples/sec; 0.075 sec/batch; 1h:48m:30s remains)
INFO - root - 2019-11-06 19:49:33.683638: step 63380, total loss = 1.04, predict loss = 0.33 (60.1 examples/sec; 0.067 sec/batch; 1h:36m:04s remains)
INFO - root - 2019-11-06 19:49:34.359600: step 63390, total loss = 0.55, predict loss = 0.14 (92.1 examples/sec; 0.043 sec/batch; 1h:02m:41s remains)
INFO - root - 2019-11-06 19:49:34.808297: step 63400, total loss = 1.10, predict loss = 0.29 (95.5 examples/sec; 0.042 sec/batch; 1h:00m:28s remains)
INFO - root - 2019-11-06 19:49:35.249961: step 63410, total loss = 0.91, predict loss = 0.22 (96.9 examples/sec; 0.041 sec/batch; 0h:59m:34s remains)
INFO - root - 2019-11-06 19:49:36.518734: step 63420, total loss = 0.58, predict loss = 0.15 (63.4 examples/sec; 0.063 sec/batch; 1h:31m:02s remains)
INFO - root - 2019-11-06 19:49:37.286940: step 63430, total loss = 0.59, predict loss = 0.15 (60.7 examples/sec; 0.066 sec/batch; 1h:35m:00s remains)
INFO - root - 2019-11-06 19:49:38.074650: step 63440, total loss = 0.71, predict loss = 0.21 (56.5 examples/sec; 0.071 sec/batch; 1h:42m:03s remains)
INFO - root - 2019-11-06 19:49:38.867100: step 63450, total loss = 0.90, predict loss = 0.24 (58.4 examples/sec; 0.069 sec/batch; 1h:38m:52s remains)
INFO - root - 2019-11-06 19:49:39.581016: step 63460, total loss = 0.70, predict loss = 0.17 (72.7 examples/sec; 0.055 sec/batch; 1h:19m:20s remains)
INFO - root - 2019-11-06 19:49:40.099791: step 63470, total loss = 1.02, predict loss = 0.24 (90.4 examples/sec; 0.044 sec/batch; 1h:03m:47s remains)
INFO - root - 2019-11-06 19:49:40.550771: step 63480, total loss = 0.98, predict loss = 0.28 (97.8 examples/sec; 0.041 sec/batch; 0h:59m:00s remains)
INFO - root - 2019-11-06 19:49:41.679619: step 63490, total loss = 0.51, predict loss = 0.13 (65.9 examples/sec; 0.061 sec/batch; 1h:27m:32s remains)
INFO - root - 2019-11-06 19:49:42.391496: step 63500, total loss = 0.36, predict loss = 0.08 (63.7 examples/sec; 0.063 sec/batch; 1h:30m:29s remains)
INFO - root - 2019-11-06 19:49:43.096145: step 63510, total loss = 0.72, predict loss = 0.21 (63.3 examples/sec; 0.063 sec/batch; 1h:31m:06s remains)
INFO - root - 2019-11-06 19:49:43.862061: step 63520, total loss = 1.01, predict loss = 0.30 (56.0 examples/sec; 0.071 sec/batch; 1h:42m:57s remains)
INFO - root - 2019-11-06 19:49:44.557736: step 63530, total loss = 0.85, predict loss = 0.23 (66.4 examples/sec; 0.060 sec/batch; 1h:26m:46s remains)
INFO - root - 2019-11-06 19:49:45.211260: step 63540, total loss = 0.84, predict loss = 0.23 (96.4 examples/sec; 0.041 sec/batch; 0h:59m:47s remains)
INFO - root - 2019-11-06 19:49:45.636987: step 63550, total loss = 1.07, predict loss = 0.25 (100.8 examples/sec; 0.040 sec/batch; 0h:57m:09s remains)
INFO - root - 2019-11-06 19:49:46.088114: step 63560, total loss = 1.05, predict loss = 0.26 (92.5 examples/sec; 0.043 sec/batch; 1h:02m:18s remains)
INFO - root - 2019-11-06 19:49:47.342102: step 63570, total loss = 0.77, predict loss = 0.20 (57.8 examples/sec; 0.069 sec/batch; 1h:39m:40s remains)
INFO - root - 2019-11-06 19:49:48.088113: step 63580, total loss = 1.19, predict loss = 0.35 (59.1 examples/sec; 0.068 sec/batch; 1h:37m:30s remains)
INFO - root - 2019-11-06 19:49:48.883530: step 63590, total loss = 0.86, predict loss = 0.21 (56.4 examples/sec; 0.071 sec/batch; 1h:42m:12s remains)
INFO - root - 2019-11-06 19:49:49.630931: step 63600, total loss = 0.56, predict loss = 0.13 (61.7 examples/sec; 0.065 sec/batch; 1h:33m:20s remains)
INFO - root - 2019-11-06 19:49:50.336257: step 63610, total loss = 0.56, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 1h:21m:36s remains)
INFO - root - 2019-11-06 19:49:50.868100: step 63620, total loss = 1.62, predict loss = 0.57 (99.8 examples/sec; 0.040 sec/batch; 0h:57m:42s remains)
INFO - root - 2019-11-06 19:49:51.313307: step 63630, total loss = 0.73, predict loss = 0.20 (94.9 examples/sec; 0.042 sec/batch; 1h:00m:38s remains)
INFO - root - 2019-11-06 19:49:52.438633: step 63640, total loss = 0.59, predict loss = 0.13 (66.9 examples/sec; 0.060 sec/batch; 1h:26m:04s remains)
INFO - root - 2019-11-06 19:49:53.107969: step 63650, total loss = 0.70, predict loss = 0.20 (57.3 examples/sec; 0.070 sec/batch; 1h:40m:32s remains)
INFO - root - 2019-11-06 19:49:53.892217: step 63660, total loss = 0.68, predict loss = 0.19 (47.4 examples/sec; 0.084 sec/batch; 2h:01m:20s remains)
INFO - root - 2019-11-06 19:49:54.606498: step 63670, total loss = 0.49, predict loss = 0.13 (63.7 examples/sec; 0.063 sec/batch; 1h:30m:24s remains)
INFO - root - 2019-11-06 19:49:55.378603: step 63680, total loss = 0.37, predict loss = 0.08 (60.4 examples/sec; 0.066 sec/batch; 1h:35m:18s remains)
INFO - root - 2019-11-06 19:49:55.945326: step 63690, total loss = 1.37, predict loss = 0.34 (98.1 examples/sec; 0.041 sec/batch; 0h:58m:40s remains)
INFO - root - 2019-11-06 19:49:56.430259: step 63700, total loss = 0.70, predict loss = 0.14 (102.1 examples/sec; 0.039 sec/batch; 0h:56m:21s remains)
INFO - root - 2019-11-06 19:49:56.869371: step 63710, total loss = 0.75, predict loss = 0.21 (100.9 examples/sec; 0.040 sec/batch; 0h:57m:01s remains)
INFO - root - 2019-11-06 19:49:58.140312: step 63720, total loss = 1.25, predict loss = 0.36 (60.5 examples/sec; 0.066 sec/batch; 1h:35m:02s remains)
INFO - root - 2019-11-06 19:49:58.856804: step 63730, total loss = 1.11, predict loss = 0.31 (63.6 examples/sec; 0.063 sec/batch; 1h:30m:28s remains)
INFO - root - 2019-11-06 19:49:59.561733: step 63740, total loss = 0.67, predict loss = 0.18 (67.6 examples/sec; 0.059 sec/batch; 1h:25m:03s remains)
INFO - root - 2019-11-06 19:50:00.303244: step 63750, total loss = 0.36, predict loss = 0.10 (56.8 examples/sec; 0.070 sec/batch; 1h:41m:16s remains)
INFO - root - 2019-11-06 19:50:01.012994: step 63760, total loss = 0.65, predict loss = 0.15 (67.6 examples/sec; 0.059 sec/batch; 1h:25m:01s remains)
INFO - root - 2019-11-06 19:50:01.523557: step 63770, total loss = 0.56, predict loss = 0.15 (96.2 examples/sec; 0.042 sec/batch; 0h:59m:46s remains)
INFO - root - 2019-11-06 19:50:01.997357: step 63780, total loss = 0.49, predict loss = 0.13 (100.4 examples/sec; 0.040 sec/batch; 0h:57m:16s remains)
INFO - root - 2019-11-06 19:50:03.134504: step 63790, total loss = 0.76, predict loss = 0.20 (63.9 examples/sec; 0.063 sec/batch; 1h:30m:00s remains)
INFO - root - 2019-11-06 19:50:03.861007: step 63800, total loss = 0.79, predict loss = 0.18 (58.1 examples/sec; 0.069 sec/batch; 1h:38m:56s remains)
INFO - root - 2019-11-06 19:50:04.653166: step 63810, total loss = 0.63, predict loss = 0.18 (55.4 examples/sec; 0.072 sec/batch; 1h:43m:46s remains)
INFO - root - 2019-11-06 19:50:05.449248: step 63820, total loss = 1.15, predict loss = 0.33 (53.8 examples/sec; 0.074 sec/batch; 1h:46m:43s remains)
INFO - root - 2019-11-06 19:50:06.178802: step 63830, total loss = 0.54, predict loss = 0.13 (58.3 examples/sec; 0.069 sec/batch; 1h:38m:35s remains)
INFO - root - 2019-11-06 19:50:06.746609: step 63840, total loss = 0.90, predict loss = 0.24 (94.9 examples/sec; 0.042 sec/batch; 1h:00m:30s remains)
INFO - root - 2019-11-06 19:50:07.204735: step 63850, total loss = 0.32, predict loss = 0.07 (97.5 examples/sec; 0.041 sec/batch; 0h:58m:53s remains)
INFO - root - 2019-11-06 19:50:07.670021: step 63860, total loss = 0.83, predict loss = 0.22 (96.4 examples/sec; 0.042 sec/batch; 0h:59m:35s remains)
INFO - root - 2019-11-06 19:50:08.983441: step 63870, total loss = 0.64, predict loss = 0.16 (63.3 examples/sec; 0.063 sec/batch; 1h:30m:43s remains)
INFO - root - 2019-11-06 19:50:09.703359: step 63880, total loss = 0.87, predict loss = 0.23 (57.3 examples/sec; 0.070 sec/batch; 1h:40m:15s remains)
INFO - root - 2019-11-06 19:50:10.437195: step 63890, total loss = 0.87, predict loss = 0.22 (59.7 examples/sec; 0.067 sec/batch; 1h:36m:09s remains)
INFO - root - 2019-11-06 19:50:11.193403: step 63900, total loss = 0.66, predict loss = 0.16 (57.9 examples/sec; 0.069 sec/batch; 1h:39m:08s remains)
INFO - root - 2019-11-06 19:50:11.880448: step 63910, total loss = 0.88, predict loss = 0.25 (69.3 examples/sec; 0.058 sec/batch; 1h:22m:47s remains)
INFO - root - 2019-11-06 19:50:12.347075: step 63920, total loss = 0.50, predict loss = 0.11 (97.1 examples/sec; 0.041 sec/batch; 0h:59m:06s remains)
INFO - root - 2019-11-06 19:50:12.806537: step 63930, total loss = 0.92, predict loss = 0.22 (108.1 examples/sec; 0.037 sec/batch; 0h:53m:04s remains)
INFO - root - 2019-11-06 19:50:13.965616: step 63940, total loss = 0.92, predict loss = 0.21 (69.1 examples/sec; 0.058 sec/batch; 1h:23m:02s remains)
INFO - root - 2019-11-06 19:50:14.692157: step 63950, total loss = 0.68, predict loss = 0.16 (55.6 examples/sec; 0.072 sec/batch; 1h:43m:12s remains)
INFO - root - 2019-11-06 19:50:15.494778: step 63960, total loss = 0.76, predict loss = 0.19 (56.5 examples/sec; 0.071 sec/batch; 1h:41m:28s remains)
INFO - root - 2019-11-06 19:50:16.272403: step 63970, total loss = 0.97, predict loss = 0.24 (52.9 examples/sec; 0.076 sec/batch; 1h:48m:19s remains)
INFO - root - 2019-11-06 19:50:17.014781: step 63980, total loss = 0.73, predict loss = 0.18 (65.7 examples/sec; 0.061 sec/batch; 1h:27m:17s remains)
INFO - root - 2019-11-06 19:50:17.549765: step 63990, total loss = 0.98, predict loss = 0.25 (97.9 examples/sec; 0.041 sec/batch; 0h:58m:32s remains)
INFO - root - 2019-11-06 19:50:18.000966: step 64000, total loss = 0.64, predict loss = 0.14 (97.1 examples/sec; 0.041 sec/batch; 0h:59m:04s remains)
INFO - root - 2019-11-06 19:50:18.447263: step 64010, total loss = 1.15, predict loss = 0.31 (129.0 examples/sec; 0.031 sec/batch; 0h:44m:27s remains)
INFO - root - 2019-11-06 19:50:19.810953: step 64020, total loss = 0.48, predict loss = 0.12 (58.0 examples/sec; 0.069 sec/batch; 1h:38m:45s remains)
INFO - root - 2019-11-06 19:50:20.621862: step 64030, total loss = 0.70, predict loss = 0.19 (55.6 examples/sec; 0.072 sec/batch; 1h:43m:03s remains)
INFO - root - 2019-11-06 19:50:21.383186: step 64040, total loss = 0.49, predict loss = 0.11 (58.3 examples/sec; 0.069 sec/batch; 1h:38m:16s remains)
INFO - root - 2019-11-06 19:50:22.096242: step 64050, total loss = 0.42, predict loss = 0.08 (59.4 examples/sec; 0.067 sec/batch; 1h:36m:30s remains)
INFO - root - 2019-11-06 19:50:22.792666: step 64060, total loss = 0.64, predict loss = 0.17 (71.8 examples/sec; 0.056 sec/batch; 1h:19m:50s remains)
INFO - root - 2019-11-06 19:50:23.269493: step 64070, total loss = 0.51, predict loss = 0.16 (97.4 examples/sec; 0.041 sec/batch; 0h:58m:50s remains)
INFO - root - 2019-11-06 19:50:23.731431: step 64080, total loss = 0.62, predict loss = 0.15 (86.1 examples/sec; 0.046 sec/batch; 1h:06m:32s remains)
INFO - root - 2019-11-06 19:50:24.945965: step 64090, total loss = 0.98, predict loss = 0.26 (68.8 examples/sec; 0.058 sec/batch; 1h:23m:15s remains)
INFO - root - 2019-11-06 19:50:25.686008: step 64100, total loss = 0.96, predict loss = 0.27 (62.5 examples/sec; 0.064 sec/batch; 1h:31m:35s remains)
INFO - root - 2019-11-06 19:50:26.441139: step 64110, total loss = 0.51, predict loss = 0.12 (63.1 examples/sec; 0.063 sec/batch; 1h:30m:47s remains)
INFO - root - 2019-11-06 19:50:27.213891: step 64120, total loss = 1.74, predict loss = 0.54 (56.2 examples/sec; 0.071 sec/batch; 1h:41m:49s remains)
INFO - root - 2019-11-06 19:50:27.925006: step 64130, total loss = 0.72, predict loss = 0.18 (70.0 examples/sec; 0.057 sec/batch; 1h:21m:45s remains)
INFO - root - 2019-11-06 19:50:28.474427: step 64140, total loss = 0.37, predict loss = 0.08 (98.6 examples/sec; 0.041 sec/batch; 0h:58m:01s remains)
INFO - root - 2019-11-06 19:50:28.925042: step 64150, total loss = 0.81, predict loss = 0.21 (94.5 examples/sec; 0.042 sec/batch; 1h:00m:33s remains)
INFO - root - 2019-11-06 19:50:30.031382: step 64160, total loss = 0.52, predict loss = 0.10 (5.6 examples/sec; 0.717 sec/batch; 17h:05m:45s remains)
INFO - root - 2019-11-06 19:50:30.719853: step 64170, total loss = 0.58, predict loss = 0.15 (55.3 examples/sec; 0.072 sec/batch; 1h:43m:27s remains)
INFO - root - 2019-11-06 19:50:31.422840: step 64180, total loss = 0.98, predict loss = 0.26 (59.8 examples/sec; 0.067 sec/batch; 1h:35m:37s remains)
INFO - root - 2019-11-06 19:50:32.200042: step 64190, total loss = 0.73, predict loss = 0.14 (50.9 examples/sec; 0.079 sec/batch; 1h:52m:27s remains)
INFO - root - 2019-11-06 19:50:32.952085: step 64200, total loss = 0.93, predict loss = 0.26 (61.1 examples/sec; 0.066 sec/batch; 1h:33m:40s remains)
INFO - root - 2019-11-06 19:50:33.607114: step 64210, total loss = 1.19, predict loss = 0.35 (88.0 examples/sec; 0.045 sec/batch; 1h:04m:59s remains)
INFO - root - 2019-11-06 19:50:34.097854: step 64220, total loss = 0.79, predict loss = 0.21 (93.8 examples/sec; 0.043 sec/batch; 1h:00m:58s remains)
INFO - root - 2019-11-06 19:50:34.554988: step 64230, total loss = 0.56, predict loss = 0.16 (97.5 examples/sec; 0.041 sec/batch; 0h:58m:37s remains)
INFO - root - 2019-11-06 19:50:35.780310: step 64240, total loss = 0.49, predict loss = 0.11 (64.5 examples/sec; 0.062 sec/batch; 1h:28m:39s remains)
INFO - root - 2019-11-06 19:50:36.519722: step 64250, total loss = 1.24, predict loss = 0.34 (59.8 examples/sec; 0.067 sec/batch; 1h:35m:32s remains)
INFO - root - 2019-11-06 19:50:37.245640: step 64260, total loss = 1.08, predict loss = 0.32 (66.7 examples/sec; 0.060 sec/batch; 1h:25m:41s remains)
INFO - root - 2019-11-06 19:50:38.018599: step 64270, total loss = 0.51, predict loss = 0.18 (58.6 examples/sec; 0.068 sec/batch; 1h:37m:29s remains)
INFO - root - 2019-11-06 19:50:38.728414: step 64280, total loss = 1.79, predict loss = 0.56 (69.7 examples/sec; 0.057 sec/batch; 1h:21m:57s remains)
INFO - root - 2019-11-06 19:50:39.255348: step 64290, total loss = 1.20, predict loss = 0.33 (94.0 examples/sec; 0.043 sec/batch; 1h:00m:45s remains)
INFO - root - 2019-11-06 19:50:39.729486: step 64300, total loss = 0.58, predict loss = 0.13 (89.4 examples/sec; 0.045 sec/batch; 1h:03m:55s remains)
INFO - root - 2019-11-06 19:50:40.870801: step 64310, total loss = 0.51, predict loss = 0.13 (66.3 examples/sec; 0.060 sec/batch; 1h:26m:08s remains)
INFO - root - 2019-11-06 19:50:41.580732: step 64320, total loss = 0.33, predict loss = 0.07 (64.9 examples/sec; 0.062 sec/batch; 1h:27m:58s remains)
INFO - root - 2019-11-06 19:50:42.399365: step 64330, total loss = 0.38, predict loss = 0.09 (54.3 examples/sec; 0.074 sec/batch; 1h:45m:11s remains)
INFO - root - 2019-11-06 19:50:43.169178: step 64340, total loss = 0.51, predict loss = 0.13 (57.6 examples/sec; 0.070 sec/batch; 1h:39m:13s remains)
INFO - root - 2019-11-06 19:50:43.899252: step 64350, total loss = 0.82, predict loss = 0.23 (63.1 examples/sec; 0.063 sec/batch; 1h:30m:26s remains)
INFO - root - 2019-11-06 19:50:44.529110: step 64360, total loss = 0.74, predict loss = 0.20 (93.3 examples/sec; 0.043 sec/batch; 1h:01m:11s remains)
INFO - root - 2019-11-06 19:50:44.985210: step 64370, total loss = 0.49, predict loss = 0.13 (89.4 examples/sec; 0.045 sec/batch; 1h:03m:51s remains)
INFO - root - 2019-11-06 19:50:45.459585: step 64380, total loss = 0.36, predict loss = 0.08 (97.9 examples/sec; 0.041 sec/batch; 0h:58m:19s remains)
INFO - root - 2019-11-06 19:50:46.731286: step 64390, total loss = 0.71, predict loss = 0.20 (66.8 examples/sec; 0.060 sec/batch; 1h:25m:23s remains)
INFO - root - 2019-11-06 19:50:47.450964: step 64400, total loss = 0.76, predict loss = 0.17 (60.6 examples/sec; 0.066 sec/batch; 1h:34m:07s remains)
INFO - root - 2019-11-06 19:50:48.215015: step 64410, total loss = 1.03, predict loss = 0.28 (57.2 examples/sec; 0.070 sec/batch; 1h:39m:46s remains)
INFO - root - 2019-11-06 19:50:48.994669: step 64420, total loss = 0.60, predict loss = 0.18 (56.2 examples/sec; 0.071 sec/batch; 1h:41m:26s remains)
INFO - root - 2019-11-06 19:50:49.760673: step 64430, total loss = 1.66, predict loss = 0.54 (69.7 examples/sec; 0.057 sec/batch; 1h:21m:51s remains)
INFO - root - 2019-11-06 19:50:50.283621: step 64440, total loss = 1.00, predict loss = 0.28 (98.7 examples/sec; 0.041 sec/batch; 0h:57m:47s remains)
INFO - root - 2019-11-06 19:50:50.743169: step 64450, total loss = 1.22, predict loss = 0.32 (95.5 examples/sec; 0.042 sec/batch; 0h:59m:42s remains)
INFO - root - 2019-11-06 19:50:51.895143: step 64460, total loss = 0.75, predict loss = 0.19 (70.8 examples/sec; 0.056 sec/batch; 1h:20m:32s remains)
INFO - root - 2019-11-06 19:50:52.569877: step 64470, total loss = 0.64, predict loss = 0.16 (60.4 examples/sec; 0.066 sec/batch; 1h:34m:28s remains)
INFO - root - 2019-11-06 19:50:53.297792: step 64480, total loss = 0.80, predict loss = 0.21 (55.7 examples/sec; 0.072 sec/batch; 1h:42m:18s remains)
INFO - root - 2019-11-06 19:50:54.043545: step 64490, total loss = 0.63, predict loss = 0.18 (64.2 examples/sec; 0.062 sec/batch; 1h:28m:48s remains)
INFO - root - 2019-11-06 19:50:54.724629: step 64500, total loss = 0.73, predict loss = 0.18 (59.4 examples/sec; 0.067 sec/batch; 1h:35m:56s remains)
INFO - root - 2019-11-06 19:50:55.306543: step 64510, total loss = 0.54, predict loss = 0.13 (102.1 examples/sec; 0.039 sec/batch; 0h:55m:48s remains)
INFO - root - 2019-11-06 19:50:55.746261: step 64520, total loss = 0.89, predict loss = 0.24 (98.6 examples/sec; 0.041 sec/batch; 0h:57m:49s remains)
INFO - root - 2019-11-06 19:50:56.196482: step 64530, total loss = 0.69, predict loss = 0.18 (97.5 examples/sec; 0.041 sec/batch; 0h:58m:26s remains)
INFO - root - 2019-11-06 19:50:57.458763: step 64540, total loss = 0.79, predict loss = 0.21 (63.9 examples/sec; 0.063 sec/batch; 1h:29m:06s remains)
INFO - root - 2019-11-06 19:50:58.170314: step 64550, total loss = 0.56, predict loss = 0.12 (64.8 examples/sec; 0.062 sec/batch; 1h:27m:56s remains)
INFO - root - 2019-11-06 19:50:58.910927: step 64560, total loss = 0.60, predict loss = 0.15 (55.2 examples/sec; 0.073 sec/batch; 1h:43m:15s remains)
INFO - root - 2019-11-06 19:50:59.668554: step 64570, total loss = 0.96, predict loss = 0.28 (56.8 examples/sec; 0.070 sec/batch; 1h:40m:12s remains)
INFO - root - 2019-11-06 19:51:00.377411: step 64580, total loss = 1.77, predict loss = 0.51 (67.2 examples/sec; 0.060 sec/batch; 1h:24m:45s remains)
INFO - root - 2019-11-06 19:51:00.861742: step 64590, total loss = 1.36, predict loss = 0.39 (98.8 examples/sec; 0.041 sec/batch; 0h:57m:39s remains)
INFO - root - 2019-11-06 19:51:01.308547: step 64600, total loss = 0.86, predict loss = 0.22 (96.3 examples/sec; 0.042 sec/batch; 0h:59m:07s remains)
INFO - root - 2019-11-06 19:51:02.497523: step 64610, total loss = 0.51, predict loss = 0.13 (63.8 examples/sec; 0.063 sec/batch; 1h:29m:10s remains)
INFO - root - 2019-11-06 19:51:03.196018: step 64620, total loss = 1.05, predict loss = 0.29 (66.8 examples/sec; 0.060 sec/batch; 1h:25m:09s remains)
INFO - root - 2019-11-06 19:51:03.916087: step 64630, total loss = 0.78, predict loss = 0.19 (60.9 examples/sec; 0.066 sec/batch; 1h:33m:30s remains)
INFO - root - 2019-11-06 19:51:04.688201: step 64640, total loss = 1.41, predict loss = 0.42 (55.5 examples/sec; 0.072 sec/batch; 1h:42m:27s remains)
INFO - root - 2019-11-06 19:51:05.394478: step 64650, total loss = 0.86, predict loss = 0.22 (59.3 examples/sec; 0.067 sec/batch; 1h:35m:57s remains)
INFO - root - 2019-11-06 19:51:05.995678: step 64660, total loss = 1.35, predict loss = 0.44 (98.8 examples/sec; 0.040 sec/batch; 0h:57m:35s remains)
INFO - root - 2019-11-06 19:51:06.447762: step 64670, total loss = 0.77, predict loss = 0.21 (89.8 examples/sec; 0.045 sec/batch; 1h:03m:19s remains)
INFO - root - 2019-11-06 19:51:06.897864: step 64680, total loss = 0.56, predict loss = 0.14 (96.0 examples/sec; 0.042 sec/batch; 0h:59m:13s remains)
INFO - root - 2019-11-06 19:51:08.174761: step 64690, total loss = 0.87, predict loss = 0.21 (62.0 examples/sec; 0.065 sec/batch; 1h:31m:45s remains)
INFO - root - 2019-11-06 19:51:08.926080: step 64700, total loss = 1.03, predict loss = 0.31 (55.5 examples/sec; 0.072 sec/batch; 1h:42m:32s remains)
INFO - root - 2019-11-06 19:51:09.635928: step 64710, total loss = 0.59, predict loss = 0.15 (65.9 examples/sec; 0.061 sec/batch; 1h:26m:16s remains)
INFO - root - 2019-11-06 19:51:10.345796: step 64720, total loss = 0.76, predict loss = 0.19 (61.9 examples/sec; 0.065 sec/batch; 1h:31m:48s remains)
INFO - root - 2019-11-06 19:51:11.051947: step 64730, total loss = 0.64, predict loss = 0.15 (65.3 examples/sec; 0.061 sec/batch; 1h:26m:59s remains)
INFO - root - 2019-11-06 19:51:11.572395: step 64740, total loss = 0.82, predict loss = 0.19 (98.1 examples/sec; 0.041 sec/batch; 0h:57m:55s remains)
INFO - root - 2019-11-06 19:51:12.028733: step 64750, total loss = 0.44, predict loss = 0.12 (91.9 examples/sec; 0.044 sec/batch; 1h:01m:50s remains)
INFO - root - 2019-11-06 19:51:13.204189: step 64760, total loss = 0.70, predict loss = 0.18 (68.5 examples/sec; 0.058 sec/batch; 1h:22m:56s remains)
INFO - root - 2019-11-06 19:51:13.920797: step 64770, total loss = 1.17, predict loss = 0.31 (60.0 examples/sec; 0.067 sec/batch; 1h:34m:44s remains)
INFO - root - 2019-11-06 19:51:14.638089: step 64780, total loss = 0.71, predict loss = 0.20 (59.2 examples/sec; 0.068 sec/batch; 1h:35m:59s remains)
INFO - root - 2019-11-06 19:51:15.352249: step 64790, total loss = 0.51, predict loss = 0.14 (54.4 examples/sec; 0.073 sec/batch; 1h:44m:21s remains)
INFO - root - 2019-11-06 19:51:16.101775: step 64800, total loss = 0.98, predict loss = 0.31 (60.3 examples/sec; 0.066 sec/batch; 1h:34m:07s remains)
INFO - root - 2019-11-06 19:51:16.699737: step 64810, total loss = 1.74, predict loss = 0.51 (101.9 examples/sec; 0.039 sec/batch; 0h:55m:42s remains)
INFO - root - 2019-11-06 19:51:17.171917: step 64820, total loss = 1.02, predict loss = 0.27 (98.0 examples/sec; 0.041 sec/batch; 0h:57m:56s remains)
INFO - root - 2019-11-06 19:51:17.625253: step 64830, total loss = 0.49, predict loss = 0.12 (115.3 examples/sec; 0.035 sec/batch; 0h:49m:13s remains)
INFO - root - 2019-11-06 19:51:19.021391: step 64840, total loss = 0.79, predict loss = 0.22 (52.7 examples/sec; 0.076 sec/batch; 1h:47m:39s remains)
INFO - root - 2019-11-06 19:51:19.763787: step 64850, total loss = 0.61, predict loss = 0.16 (63.7 examples/sec; 0.063 sec/batch; 1h:29m:04s remains)
INFO - root - 2019-11-06 19:51:20.506740: step 64860, total loss = 0.83, predict loss = 0.23 (53.4 examples/sec; 0.075 sec/batch; 1h:46m:14s remains)
INFO - root - 2019-11-06 19:51:21.241951: step 64870, total loss = 0.77, predict loss = 0.20 (60.5 examples/sec; 0.066 sec/batch; 1h:33m:45s remains)
INFO - root - 2019-11-06 19:51:21.937511: step 64880, total loss = 1.01, predict loss = 0.26 (68.9 examples/sec; 0.058 sec/batch; 1h:22m:22s remains)
INFO - root - 2019-11-06 19:51:22.389219: step 64890, total loss = 1.52, predict loss = 0.44 (100.8 examples/sec; 0.040 sec/batch; 0h:56m:18s remains)
INFO - root - 2019-11-06 19:51:22.850864: step 64900, total loss = 0.93, predict loss = 0.25 (103.5 examples/sec; 0.039 sec/batch; 0h:54m:48s remains)
INFO - root - 2019-11-06 19:51:24.139962: step 64910, total loss = 0.95, predict loss = 0.25 (66.2 examples/sec; 0.060 sec/batch; 1h:25m:44s remains)
INFO - root - 2019-11-06 19:51:24.811331: step 64920, total loss = 0.94, predict loss = 0.26 (61.2 examples/sec; 0.065 sec/batch; 1h:32m:44s remains)
INFO - root - 2019-11-06 19:51:25.558598: step 64930, total loss = 0.40, predict loss = 0.09 (56.3 examples/sec; 0.071 sec/batch; 1h:40m:40s remains)
INFO - root - 2019-11-06 19:51:26.330054: step 64940, total loss = 0.94, predict loss = 0.28 (52.0 examples/sec; 0.077 sec/batch; 1h:49m:05s remains)
INFO - root - 2019-11-06 19:51:27.081555: step 64950, total loss = 0.79, predict loss = 0.22 (63.2 examples/sec; 0.063 sec/batch; 1h:29m:43s remains)
INFO - root - 2019-11-06 19:51:27.648335: step 64960, total loss = 0.85, predict loss = 0.19 (100.8 examples/sec; 0.040 sec/batch; 0h:56m:16s remains)
INFO - root - 2019-11-06 19:51:28.097150: step 64970, total loss = 1.11, predict loss = 0.29 (98.4 examples/sec; 0.041 sec/batch; 0h:57m:37s remains)
INFO - root - 2019-11-06 19:51:29.251606: step 64980, total loss = 0.79, predict loss = 0.22 (5.5 examples/sec; 0.733 sec/batch; 17h:18m:45s remains)
INFO - root - 2019-11-06 19:51:29.936706: step 64990, total loss = 0.67, predict loss = 0.18 (61.9 examples/sec; 0.065 sec/batch; 1h:31m:32s remains)
INFO - root - 2019-11-06 19:51:30.670639: step 65000, total loss = 1.26, predict loss = 0.29 (60.2 examples/sec; 0.066 sec/batch; 1h:34m:12s remains)
INFO - root - 2019-11-06 19:51:31.402484: step 65010, total loss = 0.27, predict loss = 0.06 (54.0 examples/sec; 0.074 sec/batch; 1h:44m:54s remains)
INFO - root - 2019-11-06 19:51:32.176491: step 65020, total loss = 1.11, predict loss = 0.34 (53.5 examples/sec; 0.075 sec/batch; 1h:45m:55s remains)
INFO - root - 2019-11-06 19:51:32.871731: step 65030, total loss = 0.88, predict loss = 0.26 (86.8 examples/sec; 0.046 sec/batch; 1h:05m:17s remains)
INFO - root - 2019-11-06 19:51:33.308981: step 65040, total loss = 0.64, predict loss = 0.16 (96.5 examples/sec; 0.041 sec/batch; 0h:58m:41s remains)
INFO - root - 2019-11-06 19:51:33.754544: step 65050, total loss = 1.36, predict loss = 0.38 (92.6 examples/sec; 0.043 sec/batch; 1h:01m:09s remains)
INFO - root - 2019-11-06 19:51:35.020223: step 65060, total loss = 0.94, predict loss = 0.25 (66.1 examples/sec; 0.061 sec/batch; 1h:25m:40s remains)
INFO - root - 2019-11-06 19:51:35.728956: step 65070, total loss = 0.93, predict loss = 0.25 (59.5 examples/sec; 0.067 sec/batch; 1h:35m:05s remains)
INFO - root - 2019-11-06 19:51:36.487691: step 65080, total loss = 0.89, predict loss = 0.21 (61.4 examples/sec; 0.065 sec/batch; 1h:32m:11s remains)
INFO - root - 2019-11-06 19:51:37.217538: step 65090, total loss = 0.54, predict loss = 0.12 (62.9 examples/sec; 0.064 sec/batch; 1h:30m:03s remains)
INFO - root - 2019-11-06 19:51:37.957354: step 65100, total loss = 0.88, predict loss = 0.22 (67.8 examples/sec; 0.059 sec/batch; 1h:23m:27s remains)
INFO - root - 2019-11-06 19:51:38.480197: step 65110, total loss = 0.54, predict loss = 0.15 (91.8 examples/sec; 0.044 sec/batch; 1h:01m:40s remains)
INFO - root - 2019-11-06 19:51:38.949221: step 65120, total loss = 1.16, predict loss = 0.36 (98.4 examples/sec; 0.041 sec/batch; 0h:57m:29s remains)
INFO - root - 2019-11-06 19:51:40.093081: step 65130, total loss = 0.71, predict loss = 0.17 (65.2 examples/sec; 0.061 sec/batch; 1h:26m:43s remains)
INFO - root - 2019-11-06 19:51:40.834386: step 65140, total loss = 0.86, predict loss = 0.24 (57.2 examples/sec; 0.070 sec/batch; 1h:38m:57s remains)
INFO - root - 2019-11-06 19:51:41.555051: step 65150, total loss = 1.04, predict loss = 0.26 (64.7 examples/sec; 0.062 sec/batch; 1h:27m:22s remains)
INFO - root - 2019-11-06 19:51:42.243263: step 65160, total loss = 0.55, predict loss = 0.13 (71.9 examples/sec; 0.056 sec/batch; 1h:18m:42s remains)
INFO - root - 2019-11-06 19:51:42.952637: step 65170, total loss = 1.05, predict loss = 0.31 (57.0 examples/sec; 0.070 sec/batch; 1h:39m:14s remains)
INFO - root - 2019-11-06 19:51:43.593421: step 65180, total loss = 1.00, predict loss = 0.27 (90.8 examples/sec; 0.044 sec/batch; 1h:02m:16s remains)
INFO - root - 2019-11-06 19:51:44.046689: step 65190, total loss = 1.01, predict loss = 0.26 (93.3 examples/sec; 0.043 sec/batch; 1h:00m:34s remains)
INFO - root - 2019-11-06 19:51:44.496789: step 65200, total loss = 0.80, predict loss = 0.22 (100.4 examples/sec; 0.040 sec/batch; 0h:56m:17s remains)
INFO - root - 2019-11-06 19:51:45.751026: step 65210, total loss = 1.64, predict loss = 0.51 (59.2 examples/sec; 0.068 sec/batch; 1h:35m:30s remains)
INFO - root - 2019-11-06 19:51:46.521910: step 65220, total loss = 1.25, predict loss = 0.38 (59.4 examples/sec; 0.067 sec/batch; 1h:35m:07s remains)
INFO - root - 2019-11-06 19:51:47.274542: step 65230, total loss = 1.14, predict loss = 0.35 (58.2 examples/sec; 0.069 sec/batch; 1h:37m:05s remains)
INFO - root - 2019-11-06 19:51:48.029501: step 65240, total loss = 0.68, predict loss = 0.17 (56.0 examples/sec; 0.071 sec/batch; 1h:40m:52s remains)
INFO - root - 2019-11-06 19:51:48.753952: step 65250, total loss = 0.88, predict loss = 0.22 (61.7 examples/sec; 0.065 sec/batch; 1h:31m:32s remains)
INFO - root - 2019-11-06 19:51:49.297811: step 65260, total loss = 1.53, predict loss = 0.36 (94.4 examples/sec; 0.042 sec/batch; 0h:59m:49s remains)
INFO - root - 2019-11-06 19:51:49.749260: step 65270, total loss = 1.10, predict loss = 0.29 (92.3 examples/sec; 0.043 sec/batch; 1h:01m:12s remains)
INFO - root - 2019-11-06 19:51:50.888148: step 65280, total loss = 1.13, predict loss = 0.28 (72.1 examples/sec; 0.055 sec/batch; 1h:18m:18s remains)
INFO - root - 2019-11-06 19:51:51.610442: step 65290, total loss = 1.22, predict loss = 0.30 (58.7 examples/sec; 0.068 sec/batch; 1h:36m:11s remains)
INFO - root - 2019-11-06 19:51:52.415572: step 65300, total loss = 1.22, predict loss = 0.32 (53.2 examples/sec; 0.075 sec/batch; 1h:46m:07s remains)
INFO - root - 2019-11-06 19:51:53.170282: step 65310, total loss = 0.39, predict loss = 0.09 (57.2 examples/sec; 0.070 sec/batch; 1h:38m:37s remains)
INFO - root - 2019-11-06 19:51:53.959062: step 65320, total loss = 0.59, predict loss = 0.16 (55.1 examples/sec; 0.073 sec/batch; 1h:42m:27s remains)
INFO - root - 2019-11-06 19:51:54.565701: step 65330, total loss = 1.13, predict loss = 0.32 (102.0 examples/sec; 0.039 sec/batch; 0h:55m:19s remains)
INFO - root - 2019-11-06 19:51:55.039941: step 65340, total loss = 0.88, predict loss = 0.24 (94.4 examples/sec; 0.042 sec/batch; 0h:59m:48s remains)
INFO - root - 2019-11-06 19:51:55.494727: step 65350, total loss = 0.75, predict loss = 0.20 (96.1 examples/sec; 0.042 sec/batch; 0h:58m:42s remains)
INFO - root - 2019-11-06 19:51:56.813411: step 65360, total loss = 0.48, predict loss = 0.11 (46.7 examples/sec; 0.086 sec/batch; 2h:00m:53s remains)
INFO - root - 2019-11-06 19:51:57.581042: step 65370, total loss = 1.16, predict loss = 0.32 (51.3 examples/sec; 0.078 sec/batch; 1h:49m:55s remains)
INFO - root - 2019-11-06 19:51:58.393059: step 65380, total loss = 1.37, predict loss = 0.34 (58.9 examples/sec; 0.068 sec/batch; 1h:35m:44s remains)
INFO - root - 2019-11-06 19:51:59.169360: step 65390, total loss = 1.49, predict loss = 0.45 (55.0 examples/sec; 0.073 sec/batch; 1h:42m:38s remains)
INFO - root - 2019-11-06 19:51:59.829896: step 65400, total loss = 0.58, predict loss = 0.14 (77.7 examples/sec; 0.051 sec/batch; 1h:12m:36s remains)
INFO - root - 2019-11-06 19:52:00.335546: step 65410, total loss = 1.29, predict loss = 0.39 (95.7 examples/sec; 0.042 sec/batch; 0h:58m:55s remains)
INFO - root - 2019-11-06 19:52:00.805300: step 65420, total loss = 0.91, predict loss = 0.25 (95.7 examples/sec; 0.042 sec/batch; 0h:58m:55s remains)
INFO - root - 2019-11-06 19:52:01.943383: step 65430, total loss = 0.79, predict loss = 0.20 (63.4 examples/sec; 0.063 sec/batch; 1h:28m:52s remains)
INFO - root - 2019-11-06 19:52:02.617573: step 65440, total loss = 0.98, predict loss = 0.29 (54.6 examples/sec; 0.073 sec/batch; 1h:43m:18s remains)
INFO - root - 2019-11-06 19:52:03.321893: step 65450, total loss = 1.28, predict loss = 0.40 (60.5 examples/sec; 0.066 sec/batch; 1h:33m:12s remains)
INFO - root - 2019-11-06 19:52:04.033209: step 65460, total loss = 0.50, predict loss = 0.14 (65.6 examples/sec; 0.061 sec/batch; 1h:25m:52s remains)
INFO - root - 2019-11-06 19:52:04.718425: step 65470, total loss = 0.56, predict loss = 0.13 (61.0 examples/sec; 0.066 sec/batch; 1h:32m:25s remains)
INFO - root - 2019-11-06 19:52:05.324317: step 65480, total loss = 0.64, predict loss = 0.17 (98.4 examples/sec; 0.041 sec/batch; 0h:57m:15s remains)
INFO - root - 2019-11-06 19:52:05.770785: step 65490, total loss = 0.57, predict loss = 0.14 (99.5 examples/sec; 0.040 sec/batch; 0h:56m:38s remains)
INFO - root - 2019-11-06 19:52:06.239828: step 65500, total loss = 0.92, predict loss = 0.24 (94.9 examples/sec; 0.042 sec/batch; 0h:59m:22s remains)
INFO - root - 2019-11-06 19:52:07.561685: step 65510, total loss = 0.83, predict loss = 0.22 (54.7 examples/sec; 0.073 sec/batch; 1h:43m:00s remains)
INFO - root - 2019-11-06 19:52:08.262013: step 65520, total loss = 1.01, predict loss = 0.29 (65.8 examples/sec; 0.061 sec/batch; 1h:25m:34s remains)
INFO - root - 2019-11-06 19:52:09.005891: step 65530, total loss = 0.89, predict loss = 0.25 (53.9 examples/sec; 0.074 sec/batch; 1h:44m:27s remains)
INFO - root - 2019-11-06 19:52:09.774238: step 65540, total loss = 1.25, predict loss = 0.33 (54.1 examples/sec; 0.074 sec/batch; 1h:44m:07s remains)
INFO - root - 2019-11-06 19:52:10.496359: step 65550, total loss = 0.51, predict loss = 0.14 (68.6 examples/sec; 0.058 sec/batch; 1h:22m:04s remains)
INFO - root - 2019-11-06 19:52:10.981205: step 65560, total loss = 0.76, predict loss = 0.20 (99.8 examples/sec; 0.040 sec/batch; 0h:56m:24s remains)
INFO - root - 2019-11-06 19:52:11.425018: step 65570, total loss = 0.80, predict loss = 0.19 (96.5 examples/sec; 0.041 sec/batch; 0h:58m:19s remains)
INFO - root - 2019-11-06 19:52:12.652483: step 65580, total loss = 0.34, predict loss = 0.08 (67.9 examples/sec; 0.059 sec/batch; 1h:22m:54s remains)
INFO - root - 2019-11-06 19:52:13.364245: step 65590, total loss = 1.16, predict loss = 0.30 (62.7 examples/sec; 0.064 sec/batch; 1h:29m:40s remains)
INFO - root - 2019-11-06 19:52:14.125503: step 65600, total loss = 0.66, predict loss = 0.15 (58.0 examples/sec; 0.069 sec/batch; 1h:36m:57s remains)
INFO - root - 2019-11-06 19:52:14.890134: step 65610, total loss = 0.52, predict loss = 0.14 (61.2 examples/sec; 0.065 sec/batch; 1h:31m:58s remains)
INFO - root - 2019-11-06 19:52:15.588896: step 65620, total loss = 1.58, predict loss = 0.56 (64.3 examples/sec; 0.062 sec/batch; 1h:27m:32s remains)
INFO - root - 2019-11-06 19:52:16.125608: step 65630, total loss = 0.71, predict loss = 0.17 (102.1 examples/sec; 0.039 sec/batch; 0h:55m:04s remains)
INFO - root - 2019-11-06 19:52:16.589002: step 65640, total loss = 0.73, predict loss = 0.19 (88.2 examples/sec; 0.045 sec/batch; 1h:03m:44s remains)
INFO - root - 2019-11-06 19:52:17.041710: step 65650, total loss = 0.36, predict loss = 0.09 (125.7 examples/sec; 0.032 sec/batch; 0h:44m:45s remains)
INFO - root - 2019-11-06 19:52:18.434865: step 65660, total loss = 1.31, predict loss = 0.40 (56.0 examples/sec; 0.071 sec/batch; 1h:40m:25s remains)
INFO - root - 2019-11-06 19:52:19.161005: step 65670, total loss = 1.39, predict loss = 0.44 (61.3 examples/sec; 0.065 sec/batch; 1h:31m:42s remains)
INFO - root - 2019-11-06 19:52:19.889260: step 65680, total loss = 0.91, predict loss = 0.22 (61.6 examples/sec; 0.065 sec/batch; 1h:31m:14s remains)
INFO - root - 2019-11-06 19:52:20.658817: step 65690, total loss = 0.56, predict loss = 0.12 (56.6 examples/sec; 0.071 sec/batch; 1h:39m:22s remains)
INFO - root - 2019-11-06 19:52:21.373208: step 65700, total loss = 0.72, predict loss = 0.20 (67.6 examples/sec; 0.059 sec/batch; 1h:23m:06s remains)
INFO - root - 2019-11-06 19:52:21.845232: step 65710, total loss = 0.98, predict loss = 0.25 (94.5 examples/sec; 0.042 sec/batch; 0h:59m:26s remains)
INFO - root - 2019-11-06 19:52:22.319533: step 65720, total loss = 0.63, predict loss = 0.15 (90.8 examples/sec; 0.044 sec/batch; 1h:01m:54s remains)
INFO - root - 2019-11-06 19:52:23.524560: step 65730, total loss = 0.87, predict loss = 0.25 (69.1 examples/sec; 0.058 sec/batch; 1h:21m:15s remains)
INFO - root - 2019-11-06 19:52:24.239480: step 65740, total loss = 1.13, predict loss = 0.27 (66.1 examples/sec; 0.061 sec/batch; 1h:25m:00s remains)
INFO - root - 2019-11-06 19:52:24.951295: step 65750, total loss = 0.76, predict loss = 0.17 (61.4 examples/sec; 0.065 sec/batch; 1h:31m:24s remains)
INFO - root - 2019-11-06 19:52:25.688078: step 65760, total loss = 0.95, predict loss = 0.24 (60.1 examples/sec; 0.067 sec/batch; 1h:33m:25s remains)
INFO - root - 2019-11-06 19:52:26.441622: step 65770, total loss = 0.38, predict loss = 0.12 (65.7 examples/sec; 0.061 sec/batch; 1h:25m:31s remains)
INFO - root - 2019-11-06 19:52:27.024115: step 65780, total loss = 0.54, predict loss = 0.13 (98.6 examples/sec; 0.041 sec/batch; 0h:56m:55s remains)
INFO - root - 2019-11-06 19:52:27.477870: step 65790, total loss = 1.26, predict loss = 0.36 (94.4 examples/sec; 0.042 sec/batch; 0h:59m:26s remains)
INFO - root - 2019-11-06 19:52:28.594752: step 65800, total loss = 1.20, predict loss = 0.35 (5.6 examples/sec; 0.714 sec/batch; 16h:41m:59s remains)
INFO - root - 2019-11-06 19:52:29.330083: step 65810, total loss = 0.78, predict loss = 0.19 (51.1 examples/sec; 0.078 sec/batch; 1h:49m:52s remains)
INFO - root - 2019-11-06 19:52:30.027870: step 65820, total loss = 0.69, predict loss = 0.20 (67.7 examples/sec; 0.059 sec/batch; 1h:22m:54s remains)
INFO - root - 2019-11-06 19:52:30.704528: step 65830, total loss = 0.77, predict loss = 0.21 (60.6 examples/sec; 0.066 sec/batch; 1h:32m:33s remains)
INFO - root - 2019-11-06 19:52:31.477310: step 65840, total loss = 1.08, predict loss = 0.28 (61.7 examples/sec; 0.065 sec/batch; 1h:30m:57s remains)
INFO - root - 2019-11-06 19:52:32.155188: step 65850, total loss = 1.47, predict loss = 0.43 (84.0 examples/sec; 0.048 sec/batch; 1h:06m:48s remains)
INFO - root - 2019-11-06 19:52:32.653889: step 65860, total loss = 0.83, predict loss = 0.22 (86.6 examples/sec; 0.046 sec/batch; 1h:04m:45s remains)
INFO - root - 2019-11-06 19:52:33.112957: step 65870, total loss = 0.73, predict loss = 0.19 (92.2 examples/sec; 0.043 sec/batch; 1h:00m:49s remains)
INFO - root - 2019-11-06 19:52:34.332388: step 65880, total loss = 0.77, predict loss = 0.17 (62.2 examples/sec; 0.064 sec/batch; 1h:30m:10s remains)
INFO - root - 2019-11-06 19:52:35.087444: step 65890, total loss = 1.21, predict loss = 0.39 (59.1 examples/sec; 0.068 sec/batch; 1h:34m:49s remains)
INFO - root - 2019-11-06 19:52:35.842163: step 65900, total loss = 0.78, predict loss = 0.23 (54.4 examples/sec; 0.073 sec/batch; 1h:43m:01s remains)
INFO - root - 2019-11-06 19:52:36.601607: step 65910, total loss = 0.92, predict loss = 0.24 (58.0 examples/sec; 0.069 sec/batch; 1h:36m:42s remains)
INFO - root - 2019-11-06 19:52:37.318321: step 65920, total loss = 0.42, predict loss = 0.09 (67.0 examples/sec; 0.060 sec/batch; 1h:23m:41s remains)
INFO - root - 2019-11-06 19:52:37.861105: step 65930, total loss = 0.88, predict loss = 0.25 (98.2 examples/sec; 0.041 sec/batch; 0h:57m:03s remains)
INFO - root - 2019-11-06 19:52:38.329967: step 65940, total loss = 0.82, predict loss = 0.18 (98.9 examples/sec; 0.040 sec/batch; 0h:56m:41s remains)
INFO - root - 2019-11-06 19:52:39.478235: step 65950, total loss = 1.17, predict loss = 0.29 (70.7 examples/sec; 0.057 sec/batch; 1h:19m:15s remains)
INFO - root - 2019-11-06 19:52:40.185051: step 65960, total loss = 0.53, predict loss = 0.14 (61.1 examples/sec; 0.065 sec/batch; 1h:31m:43s remains)
INFO - root - 2019-11-06 19:52:40.922275: step 65970, total loss = 0.75, predict loss = 0.19 (56.5 examples/sec; 0.071 sec/batch; 1h:39m:04s remains)
INFO - root - 2019-11-06 19:52:41.662966: step 65980, total loss = 0.80, predict loss = 0.24 (64.1 examples/sec; 0.062 sec/batch; 1h:27m:22s remains)
INFO - root - 2019-11-06 19:52:42.461620: step 65990, total loss = 1.05, predict loss = 0.29 (56.4 examples/sec; 0.071 sec/batch; 1h:39m:20s remains)
INFO - root - 2019-11-06 19:52:43.108287: step 66000, total loss = 0.53, predict loss = 0.14 (91.3 examples/sec; 0.044 sec/batch; 1h:01m:21s remains)
INFO - root - 2019-11-06 19:52:43.554731: step 66010, total loss = 0.57, predict loss = 0.14 (94.1 examples/sec; 0.043 sec/batch; 0h:59m:29s remains)
INFO - root - 2019-11-06 19:52:44.024177: step 66020, total loss = 0.52, predict loss = 0.14 (98.1 examples/sec; 0.041 sec/batch; 0h:57m:03s remains)
INFO - root - 2019-11-06 19:52:45.264846: step 66030, total loss = 0.62, predict loss = 0.17 (60.1 examples/sec; 0.067 sec/batch; 1h:33m:07s remains)
INFO - root - 2019-11-06 19:52:46.013908: step 66040, total loss = 0.60, predict loss = 0.16 (58.4 examples/sec; 0.068 sec/batch; 1h:35m:47s remains)
INFO - root - 2019-11-06 19:52:46.729247: step 66050, total loss = 0.51, predict loss = 0.13 (63.3 examples/sec; 0.063 sec/batch; 1h:28m:25s remains)
INFO - root - 2019-11-06 19:52:47.543025: step 66060, total loss = 0.50, predict loss = 0.11 (58.1 examples/sec; 0.069 sec/batch; 1h:36m:23s remains)
INFO - root - 2019-11-06 19:52:48.230627: step 66070, total loss = 1.11, predict loss = 0.36 (78.8 examples/sec; 0.051 sec/batch; 1h:10m:58s remains)
INFO - root - 2019-11-06 19:52:48.754591: step 66080, total loss = 0.70, predict loss = 0.14 (100.7 examples/sec; 0.040 sec/batch; 0h:55m:33s remains)
INFO - root - 2019-11-06 19:52:49.202078: step 66090, total loss = 0.57, predict loss = 0.14 (98.1 examples/sec; 0.041 sec/batch; 0h:57m:01s remains)
INFO - root - 2019-11-06 19:52:50.348702: step 66100, total loss = 1.01, predict loss = 0.25 (71.0 examples/sec; 0.056 sec/batch; 1h:18m:46s remains)
INFO - root - 2019-11-06 19:52:51.070540: step 66110, total loss = 0.78, predict loss = 0.20 (54.0 examples/sec; 0.074 sec/batch; 1h:43m:38s remains)
INFO - root - 2019-11-06 19:52:51.814374: step 66120, total loss = 0.70, predict loss = 0.18 (58.1 examples/sec; 0.069 sec/batch; 1h:36m:12s remains)
INFO - root - 2019-11-06 19:52:52.554872: step 66130, total loss = 0.69, predict loss = 0.15 (64.8 examples/sec; 0.062 sec/batch; 1h:26m:20s remains)
INFO - root - 2019-11-06 19:52:53.301574: step 66140, total loss = 1.51, predict loss = 0.40 (58.6 examples/sec; 0.068 sec/batch; 1h:35m:26s remains)
INFO - root - 2019-11-06 19:52:53.986211: step 66150, total loss = 1.46, predict loss = 0.43 (92.6 examples/sec; 0.043 sec/batch; 1h:00m:22s remains)
INFO - root - 2019-11-06 19:52:54.445012: step 66160, total loss = 0.74, predict loss = 0.21 (96.2 examples/sec; 0.042 sec/batch; 0h:58m:04s remains)
INFO - root - 2019-11-06 19:52:54.898251: step 66170, total loss = 0.58, predict loss = 0.14 (95.0 examples/sec; 0.042 sec/batch; 0h:58m:50s remains)
INFO - root - 2019-11-06 19:52:56.236930: step 66180, total loss = 1.23, predict loss = 0.37 (56.8 examples/sec; 0.070 sec/batch; 1h:38m:26s remains)
INFO - root - 2019-11-06 19:52:56.974025: step 66190, total loss = 0.57, predict loss = 0.15 (54.5 examples/sec; 0.073 sec/batch; 1h:42m:29s remains)
INFO - root - 2019-11-06 19:52:57.735242: step 66200, total loss = 0.64, predict loss = 0.16 (59.6 examples/sec; 0.067 sec/batch; 1h:33m:45s remains)
INFO - root - 2019-11-06 19:52:58.513828: step 66210, total loss = 0.88, predict loss = 0.24 (60.8 examples/sec; 0.066 sec/batch; 1h:31m:54s remains)
INFO - root - 2019-11-06 19:52:59.216566: step 66220, total loss = 0.75, predict loss = 0.18 (62.7 examples/sec; 0.064 sec/batch; 1h:29m:07s remains)
INFO - root - 2019-11-06 19:52:59.710548: step 66230, total loss = 1.19, predict loss = 0.36 (98.1 examples/sec; 0.041 sec/batch; 0h:56m:54s remains)
INFO - root - 2019-11-06 19:53:00.164195: step 66240, total loss = 1.09, predict loss = 0.29 (93.1 examples/sec; 0.043 sec/batch; 0h:59m:57s remains)
INFO - root - 2019-11-06 19:53:01.328451: step 66250, total loss = 0.57, predict loss = 0.16 (72.1 examples/sec; 0.055 sec/batch; 1h:17m:26s remains)
INFO - root - 2019-11-06 19:53:02.050400: step 66260, total loss = 0.78, predict loss = 0.21 (63.2 examples/sec; 0.063 sec/batch; 1h:28m:19s remains)
INFO - root - 2019-11-06 19:53:02.835290: step 66270, total loss = 1.30, predict loss = 0.37 (57.9 examples/sec; 0.069 sec/batch; 1h:36m:27s remains)
INFO - root - 2019-11-06 19:53:03.630797: step 66280, total loss = 0.86, predict loss = 0.24 (58.1 examples/sec; 0.069 sec/batch; 1h:36m:06s remains)
INFO - root - 2019-11-06 19:53:04.365743: step 66290, total loss = 0.97, predict loss = 0.30 (61.5 examples/sec; 0.065 sec/batch; 1h:30m:41s remains)
INFO - root - 2019-11-06 19:53:04.942008: step 66300, total loss = 0.86, predict loss = 0.21 (105.0 examples/sec; 0.038 sec/batch; 0h:53m:07s remains)
INFO - root - 2019-11-06 19:53:05.395689: step 66310, total loss = 0.47, predict loss = 0.11 (98.3 examples/sec; 0.041 sec/batch; 0h:56m:43s remains)
INFO - root - 2019-11-06 19:53:05.844970: step 66320, total loss = 0.93, predict loss = 0.25 (92.7 examples/sec; 0.043 sec/batch; 1h:00m:11s remains)
INFO - root - 2019-11-06 19:53:07.154394: step 66330, total loss = 0.47, predict loss = 0.12 (67.4 examples/sec; 0.059 sec/batch; 1h:22m:44s remains)
INFO - root - 2019-11-06 19:53:07.870080: step 66340, total loss = 0.52, predict loss = 0.10 (63.7 examples/sec; 0.063 sec/batch; 1h:27m:36s remains)
INFO - root - 2019-11-06 19:53:08.558645: step 66350, total loss = 0.91, predict loss = 0.25 (62.8 examples/sec; 0.064 sec/batch; 1h:28m:51s remains)
INFO - root - 2019-11-06 19:53:09.287029: step 66360, total loss = 0.87, predict loss = 0.19 (64.9 examples/sec; 0.062 sec/batch; 1h:25m:53s remains)
INFO - root - 2019-11-06 19:53:09.981471: step 66370, total loss = 0.76, predict loss = 0.20 (75.3 examples/sec; 0.053 sec/batch; 1h:14m:00s remains)
INFO - root - 2019-11-06 19:53:10.483995: step 66380, total loss = 1.60, predict loss = 0.46 (95.8 examples/sec; 0.042 sec/batch; 0h:58m:10s remains)
INFO - root - 2019-11-06 19:53:10.944182: step 66390, total loss = 0.61, predict loss = 0.16 (93.1 examples/sec; 0.043 sec/batch; 0h:59m:53s remains)
INFO - root - 2019-11-06 19:53:12.109185: step 66400, total loss = 0.33, predict loss = 0.07 (70.1 examples/sec; 0.057 sec/batch; 1h:19m:31s remains)
INFO - root - 2019-11-06 19:53:12.831880: step 66410, total loss = 0.86, predict loss = 0.23 (50.0 examples/sec; 0.080 sec/batch; 1h:51m:31s remains)
INFO - root - 2019-11-06 19:53:13.545186: step 66420, total loss = 1.06, predict loss = 0.29 (68.3 examples/sec; 0.059 sec/batch; 1h:21m:37s remains)
INFO - root - 2019-11-06 19:53:14.267661: step 66430, total loss = 1.06, predict loss = 0.25 (63.9 examples/sec; 0.063 sec/batch; 1h:27m:08s remains)
INFO - root - 2019-11-06 19:53:15.031126: step 66440, total loss = 0.53, predict loss = 0.15 (51.8 examples/sec; 0.077 sec/batch; 1h:47m:34s remains)
INFO - root - 2019-11-06 19:53:15.621709: step 66450, total loss = 0.76, predict loss = 0.17 (98.3 examples/sec; 0.041 sec/batch; 0h:56m:39s remains)
INFO - root - 2019-11-06 19:53:16.104970: step 66460, total loss = 0.95, predict loss = 0.28 (97.6 examples/sec; 0.041 sec/batch; 0h:57m:05s remains)
INFO - root - 2019-11-06 19:53:16.542105: step 66470, total loss = 1.79, predict loss = 0.55 (140.1 examples/sec; 0.029 sec/batch; 0h:39m:44s remains)
INFO - root - 2019-11-06 19:53:17.884071: step 66480, total loss = 0.73, predict loss = 0.21 (62.0 examples/sec; 0.065 sec/batch; 1h:29m:49s remains)
INFO - root - 2019-11-06 19:53:18.633075: step 66490, total loss = 1.05, predict loss = 0.30 (60.7 examples/sec; 0.066 sec/batch; 1h:31m:46s remains)
INFO - root - 2019-11-06 19:53:19.364942: step 66500, total loss = 1.30, predict loss = 0.41 (64.4 examples/sec; 0.062 sec/batch; 1h:26m:26s remains)
INFO - root - 2019-11-06 19:53:20.090126: step 66510, total loss = 0.59, predict loss = 0.12 (64.1 examples/sec; 0.062 sec/batch; 1h:26m:48s remains)
INFO - root - 2019-11-06 19:53:20.760920: step 66520, total loss = 0.67, predict loss = 0.17 (73.0 examples/sec; 0.055 sec/batch; 1h:16m:13s remains)
INFO - root - 2019-11-06 19:53:21.242423: step 66530, total loss = 0.71, predict loss = 0.20 (95.9 examples/sec; 0.042 sec/batch; 0h:58m:03s remains)
INFO - root - 2019-11-06 19:53:21.707651: step 66540, total loss = 1.11, predict loss = 0.31 (100.4 examples/sec; 0.040 sec/batch; 0h:55m:24s remains)
INFO - root - 2019-11-06 19:53:22.920361: step 66550, total loss = 1.01, predict loss = 0.31 (68.2 examples/sec; 0.059 sec/batch; 1h:21m:33s remains)
INFO - root - 2019-11-06 19:53:23.627125: step 66560, total loss = 1.33, predict loss = 0.35 (52.4 examples/sec; 0.076 sec/batch; 1h:46m:14s remains)
INFO - root - 2019-11-06 19:53:24.360495: step 66570, total loss = 0.73, predict loss = 0.14 (63.4 examples/sec; 0.063 sec/batch; 1h:27m:43s remains)
INFO - root - 2019-11-06 19:53:25.159747: step 66580, total loss = 0.57, predict loss = 0.15 (52.0 examples/sec; 0.077 sec/batch; 1h:46m:52s remains)
INFO - root - 2019-11-06 19:53:25.909587: step 66590, total loss = 0.73, predict loss = 0.20 (76.0 examples/sec; 0.053 sec/batch; 1h:13m:09s remains)
INFO - root - 2019-11-06 19:53:26.445216: step 66600, total loss = 0.89, predict loss = 0.22 (103.4 examples/sec; 0.039 sec/batch; 0h:53m:46s remains)
INFO - root - 2019-11-06 19:53:26.881189: step 66610, total loss = 0.74, predict loss = 0.18 (100.2 examples/sec; 0.040 sec/batch; 0h:55m:28s remains)
INFO - root - 2019-11-06 19:53:27.982307: step 66620, total loss = 0.72, predict loss = 0.18 (5.8 examples/sec; 0.687 sec/batch; 15h:54m:46s remains)
INFO - root - 2019-11-06 19:53:28.696802: step 66630, total loss = 0.77, predict loss = 0.21 (55.6 examples/sec; 0.072 sec/batch; 1h:39m:56s remains)
INFO - root - 2019-11-06 19:53:29.490385: step 66640, total loss = 1.00, predict loss = 0.27 (50.2 examples/sec; 0.080 sec/batch; 1h:50m:36s remains)
INFO - root - 2019-11-06 19:53:30.323815: step 66650, total loss = 0.70, predict loss = 0.19 (48.2 examples/sec; 0.083 sec/batch; 1h:55m:15s remains)
INFO - root - 2019-11-06 19:53:31.071022: step 66660, total loss = 1.45, predict loss = 0.41 (62.3 examples/sec; 0.064 sec/batch; 1h:29m:07s remains)
INFO - root - 2019-11-06 19:53:31.751356: step 66670, total loss = 0.81, predict loss = 0.19 (88.0 examples/sec; 0.045 sec/batch; 1h:03m:08s remains)
INFO - root - 2019-11-06 19:53:32.192047: step 66680, total loss = 1.32, predict loss = 0.31 (99.0 examples/sec; 0.040 sec/batch; 0h:56m:05s remains)
INFO - root - 2019-11-06 19:53:32.655896: step 66690, total loss = 1.07, predict loss = 0.30 (96.9 examples/sec; 0.041 sec/batch; 0h:57m:18s remains)
INFO - root - 2019-11-06 19:53:33.898706: step 66700, total loss = 0.54, predict loss = 0.15 (58.2 examples/sec; 0.069 sec/batch; 1h:35m:27s remains)
INFO - root - 2019-11-06 19:53:34.683676: step 66710, total loss = 1.24, predict loss = 0.38 (57.2 examples/sec; 0.070 sec/batch; 1h:37m:09s remains)
INFO - root - 2019-11-06 19:53:35.425934: step 66720, total loss = 0.59, predict loss = 0.16 (60.3 examples/sec; 0.066 sec/batch; 1h:32m:02s remains)
INFO - root - 2019-11-06 19:53:36.157286: step 66730, total loss = 0.84, predict loss = 0.24 (56.3 examples/sec; 0.071 sec/batch; 1h:38m:39s remains)
INFO - root - 2019-11-06 19:53:36.939311: step 66740, total loss = 0.39, predict loss = 0.10 (69.8 examples/sec; 0.057 sec/batch; 1h:19m:30s remains)
INFO - root - 2019-11-06 19:53:37.487852: step 66750, total loss = 0.68, predict loss = 0.18 (94.8 examples/sec; 0.042 sec/batch; 0h:58m:33s remains)
INFO - root - 2019-11-06 19:53:37.926879: step 66760, total loss = 1.02, predict loss = 0.28 (98.2 examples/sec; 0.041 sec/batch; 0h:56m:31s remains)
INFO - root - 2019-11-06 19:53:39.106786: step 66770, total loss = 1.26, predict loss = 0.36 (64.4 examples/sec; 0.062 sec/batch; 1h:26m:08s remains)
INFO - root - 2019-11-06 19:53:39.855477: step 66780, total loss = 0.50, predict loss = 0.12 (61.0 examples/sec; 0.066 sec/batch; 1h:30m:55s remains)
INFO - root - 2019-11-06 19:53:40.605061: step 66790, total loss = 1.76, predict loss = 0.55 (55.0 examples/sec; 0.073 sec/batch; 1h:40m:51s remains)
INFO - root - 2019-11-06 19:53:41.341202: step 66800, total loss = 0.72, predict loss = 0.19 (62.2 examples/sec; 0.064 sec/batch; 1h:29m:11s remains)
INFO - root - 2019-11-06 19:53:42.072685: step 66810, total loss = 0.53, predict loss = 0.14 (65.3 examples/sec; 0.061 sec/batch; 1h:24m:54s remains)
INFO - root - 2019-11-06 19:53:42.716623: step 66820, total loss = 1.26, predict loss = 0.32 (98.2 examples/sec; 0.041 sec/batch; 0h:56m:27s remains)
INFO - root - 2019-11-06 19:53:43.159393: step 66830, total loss = 0.69, predict loss = 0.20 (103.7 examples/sec; 0.039 sec/batch; 0h:53m:28s remains)
INFO - root - 2019-11-06 19:53:43.614804: step 66840, total loss = 0.62, predict loss = 0.15 (97.3 examples/sec; 0.041 sec/batch; 0h:57m:00s remains)
INFO - root - 2019-11-06 19:53:44.870060: step 66850, total loss = 0.80, predict loss = 0.21 (65.0 examples/sec; 0.062 sec/batch; 1h:25m:15s remains)
INFO - root - 2019-11-06 19:53:45.561432: step 66860, total loss = 0.46, predict loss = 0.12 (65.6 examples/sec; 0.061 sec/batch; 1h:24m:27s remains)
INFO - root - 2019-11-06 19:53:46.269277: step 66870, total loss = 1.21, predict loss = 0.39 (59.5 examples/sec; 0.067 sec/batch; 1h:33m:08s remains)
INFO - root - 2019-11-06 19:53:47.013811: step 66880, total loss = 0.95, predict loss = 0.28 (60.7 examples/sec; 0.066 sec/batch; 1h:31m:19s remains)
INFO - root - 2019-11-06 19:53:47.698938: step 66890, total loss = 1.00, predict loss = 0.26 (65.3 examples/sec; 0.061 sec/batch; 1h:24m:51s remains)
INFO - root - 2019-11-06 19:53:48.224649: step 66900, total loss = 0.93, predict loss = 0.22 (93.9 examples/sec; 0.043 sec/batch; 0h:59m:01s remains)
INFO - root - 2019-11-06 19:53:48.679239: step 66910, total loss = 0.88, predict loss = 0.23 (94.3 examples/sec; 0.042 sec/batch; 0h:58m:45s remains)
INFO - root - 2019-11-06 19:53:49.847935: step 66920, total loss = 0.66, predict loss = 0.16 (67.8 examples/sec; 0.059 sec/batch; 1h:21m:40s remains)
INFO - root - 2019-11-06 19:53:50.579630: step 66930, total loss = 0.95, predict loss = 0.25 (51.8 examples/sec; 0.077 sec/batch; 1h:46m:55s remains)
INFO - root - 2019-11-06 19:53:51.312814: step 66940, total loss = 0.77, predict loss = 0.22 (65.0 examples/sec; 0.062 sec/batch; 1h:25m:10s remains)
INFO - root - 2019-11-06 19:53:52.027158: step 66950, total loss = 1.09, predict loss = 0.31 (64.0 examples/sec; 0.062 sec/batch; 1h:26m:28s remains)
INFO - root - 2019-11-06 19:53:52.771300: step 66960, total loss = 0.95, predict loss = 0.29 (63.5 examples/sec; 0.063 sec/batch; 1h:27m:06s remains)
INFO - root - 2019-11-06 19:53:53.337146: step 66970, total loss = 0.73, predict loss = 0.20 (106.2 examples/sec; 0.038 sec/batch; 0h:52m:07s remains)
INFO - root - 2019-11-06 19:53:53.820215: step 66980, total loss = 0.89, predict loss = 0.23 (95.6 examples/sec; 0.042 sec/batch; 0h:57m:52s remains)
INFO - root - 2019-11-06 19:53:54.258423: step 66990, total loss = 0.52, predict loss = 0.13 (94.5 examples/sec; 0.042 sec/batch; 0h:58m:34s remains)
INFO - root - 2019-11-06 19:53:55.553291: step 67000, total loss = 1.16, predict loss = 0.36 (65.2 examples/sec; 0.061 sec/batch; 1h:24m:49s remains)
INFO - root - 2019-11-06 19:53:56.291244: step 67010, total loss = 1.06, predict loss = 0.30 (52.5 examples/sec; 0.076 sec/batch; 1h:45m:18s remains)
INFO - root - 2019-11-06 19:53:57.036775: step 67020, total loss = 1.05, predict loss = 0.29 (60.1 examples/sec; 0.067 sec/batch; 1h:32m:00s remains)
INFO - root - 2019-11-06 19:53:57.753716: step 67030, total loss = 0.41, predict loss = 0.10 (53.8 examples/sec; 0.074 sec/batch; 1h:42m:49s remains)
INFO - root - 2019-11-06 19:53:58.444094: step 67040, total loss = 1.96, predict loss = 0.67 (76.7 examples/sec; 0.052 sec/batch; 1h:12m:06s remains)
INFO - root - 2019-11-06 19:53:58.925272: step 67050, total loss = 1.14, predict loss = 0.33 (101.8 examples/sec; 0.039 sec/batch; 0h:54m:18s remains)
INFO - root - 2019-11-06 19:53:59.399608: step 67060, total loss = 1.58, predict loss = 0.49 (94.1 examples/sec; 0.042 sec/batch; 0h:58m:44s remains)
INFO - root - 2019-11-06 19:54:00.561722: step 67070, total loss = 1.32, predict loss = 0.39 (70.1 examples/sec; 0.057 sec/batch; 1h:18m:51s remains)
INFO - root - 2019-11-06 19:54:01.280594: step 67080, total loss = 1.07, predict loss = 0.28 (55.2 examples/sec; 0.072 sec/batch; 1h:40m:06s remains)
INFO - root - 2019-11-06 19:54:02.004477: step 67090, total loss = 0.64, predict loss = 0.16 (63.3 examples/sec; 0.063 sec/batch; 1h:27m:17s remains)
INFO - root - 2019-11-06 19:54:02.802650: step 67100, total loss = 1.51, predict loss = 0.43 (57.5 examples/sec; 0.070 sec/batch; 1h:36m:09s remains)
INFO - root - 2019-11-06 19:54:03.493281: step 67110, total loss = 0.93, predict loss = 0.28 (65.3 examples/sec; 0.061 sec/batch; 1h:24m:40s remains)
INFO - root - 2019-11-06 19:54:04.037140: step 67120, total loss = 1.35, predict loss = 0.35 (104.7 examples/sec; 0.038 sec/batch; 0h:52m:44s remains)
INFO - root - 2019-11-06 19:54:04.476394: step 67130, total loss = 0.57, predict loss = 0.13 (95.3 examples/sec; 0.042 sec/batch; 0h:57m:58s remains)
INFO - root - 2019-11-06 19:54:04.949093: step 67140, total loss = 0.69, predict loss = 0.15 (98.3 examples/sec; 0.041 sec/batch; 0h:56m:11s remains)
INFO - root - 2019-11-06 19:54:06.296039: step 67150, total loss = 0.80, predict loss = 0.25 (53.7 examples/sec; 0.075 sec/batch; 1h:42m:53s remains)
INFO - root - 2019-11-06 19:54:07.126590: step 67160, total loss = 0.83, predict loss = 0.23 (56.9 examples/sec; 0.070 sec/batch; 1h:37m:00s remains)
INFO - root - 2019-11-06 19:54:07.847737: step 67170, total loss = 0.65, predict loss = 0.19 (60.2 examples/sec; 0.066 sec/batch; 1h:31m:39s remains)
INFO - root - 2019-11-06 19:54:08.609432: step 67180, total loss = 0.85, predict loss = 0.18 (56.2 examples/sec; 0.071 sec/batch; 1h:38m:11s remains)
INFO - root - 2019-11-06 19:54:09.295476: step 67190, total loss = 0.89, predict loss = 0.24 (75.7 examples/sec; 0.053 sec/batch; 1h:12m:53s remains)
INFO - root - 2019-11-06 19:54:09.775582: step 67200, total loss = 1.07, predict loss = 0.29 (96.8 examples/sec; 0.041 sec/batch; 0h:56m:59s remains)
INFO - root - 2019-11-06 19:54:10.221313: step 67210, total loss = 0.77, predict loss = 0.20 (97.0 examples/sec; 0.041 sec/batch; 0h:56m:53s remains)
INFO - root - 2019-11-06 19:54:11.427422: step 67220, total loss = 0.52, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 1h:22m:18s remains)
INFO - root - 2019-11-06 19:54:12.109829: step 67230, total loss = 0.97, predict loss = 0.28 (60.9 examples/sec; 0.066 sec/batch; 1h:30m:38s remains)
INFO - root - 2019-11-06 19:54:12.839751: step 67240, total loss = 0.53, predict loss = 0.14 (61.9 examples/sec; 0.065 sec/batch; 1h:29m:11s remains)
INFO - root - 2019-11-06 19:54:13.597287: step 67250, total loss = 1.38, predict loss = 0.42 (57.8 examples/sec; 0.069 sec/batch; 1h:35m:28s remains)
INFO - root - 2019-11-06 19:54:14.328706: step 67260, total loss = 0.49, predict loss = 0.11 (65.3 examples/sec; 0.061 sec/batch; 1h:24m:30s remains)
INFO - root - 2019-11-06 19:54:14.900567: step 67270, total loss = 1.65, predict loss = 0.44 (99.9 examples/sec; 0.040 sec/batch; 0h:55m:11s remains)
INFO - root - 2019-11-06 19:54:15.343147: step 67280, total loss = 0.95, predict loss = 0.22 (101.4 examples/sec; 0.039 sec/batch; 0h:54m:24s remains)
INFO - root - 2019-11-06 19:54:15.774945: step 67290, total loss = 1.27, predict loss = 0.29 (132.8 examples/sec; 0.030 sec/batch; 0h:41m:30s remains)
INFO - root - 2019-11-06 19:54:17.146924: step 67300, total loss = 1.08, predict loss = 0.30 (63.0 examples/sec; 0.064 sec/batch; 1h:27m:34s remains)
INFO - root - 2019-11-06 19:54:17.831937: step 67310, total loss = 0.92, predict loss = 0.25 (63.6 examples/sec; 0.063 sec/batch; 1h:26m:40s remains)
INFO - root - 2019-11-06 19:54:18.555672: step 67320, total loss = 1.21, predict loss = 0.32 (58.5 examples/sec; 0.068 sec/batch; 1h:34m:12s remains)
INFO - root - 2019-11-06 19:54:19.324076: step 67330, total loss = 0.67, predict loss = 0.14 (62.6 examples/sec; 0.064 sec/batch; 1h:28m:04s remains)
INFO - root - 2019-11-06 19:54:20.021163: step 67340, total loss = 0.98, predict loss = 0.25 (76.1 examples/sec; 0.053 sec/batch; 1h:12m:23s remains)
INFO - root - 2019-11-06 19:54:20.474941: step 67350, total loss = 0.96, predict loss = 0.23 (94.5 examples/sec; 0.042 sec/batch; 0h:58m:18s remains)
INFO - root - 2019-11-06 19:54:20.938710: step 67360, total loss = 1.23, predict loss = 0.35 (90.8 examples/sec; 0.044 sec/batch; 1h:00m:41s remains)
INFO - root - 2019-11-06 19:54:22.103954: step 67370, total loss = 0.57, predict loss = 0.15 (70.3 examples/sec; 0.057 sec/batch; 1h:18m:18s remains)
INFO - root - 2019-11-06 19:54:22.870141: step 67380, total loss = 1.11, predict loss = 0.32 (57.7 examples/sec; 0.069 sec/batch; 1h:35m:31s remains)
INFO - root - 2019-11-06 19:54:23.619925: step 67390, total loss = 0.89, predict loss = 0.25 (63.3 examples/sec; 0.063 sec/batch; 1h:26m:58s remains)
INFO - root - 2019-11-06 19:54:24.407670: step 67400, total loss = 0.59, predict loss = 0.15 (55.8 examples/sec; 0.072 sec/batch; 1h:38m:42s remains)
INFO - root - 2019-11-06 19:54:25.163050: step 67410, total loss = 1.06, predict loss = 0.26 (62.5 examples/sec; 0.064 sec/batch; 1h:28m:03s remains)
INFO - root - 2019-11-06 19:54:25.731930: step 67420, total loss = 0.66, predict loss = 0.15 (100.5 examples/sec; 0.040 sec/batch; 0h:54m:46s remains)
INFO - root - 2019-11-06 19:54:26.183007: step 67430, total loss = 1.73, predict loss = 0.53 (97.4 examples/sec; 0.041 sec/batch; 0h:56m:32s remains)
INFO - root - 2019-11-06 19:54:27.296154: step 67440, total loss = 0.69, predict loss = 0.18 (5.6 examples/sec; 0.708 sec/batch; 16h:14m:12s remains)
INFO - root - 2019-11-06 19:54:28.010844: step 67450, total loss = 0.96, predict loss = 0.28 (59.1 examples/sec; 0.068 sec/batch; 1h:33m:02s remains)
INFO - root - 2019-11-06 19:54:28.836845: step 67460, total loss = 0.97, predict loss = 0.23 (48.6 examples/sec; 0.082 sec/batch; 1h:53m:12s remains)
INFO - root - 2019-11-06 19:54:29.618300: step 67470, total loss = 0.46, predict loss = 0.12 (53.0 examples/sec; 0.075 sec/batch; 1h:43m:48s remains)
INFO - root - 2019-11-06 19:54:30.416463: step 67480, total loss = 0.69, predict loss = 0.17 (52.1 examples/sec; 0.077 sec/batch; 1h:45m:32s remains)
INFO - root - 2019-11-06 19:54:31.110625: step 67490, total loss = 1.11, predict loss = 0.27 (85.8 examples/sec; 0.047 sec/batch; 1h:04m:08s remains)
INFO - root - 2019-11-06 19:54:31.615043: step 67500, total loss = 1.03, predict loss = 0.28 (88.5 examples/sec; 0.045 sec/batch; 1h:02m:09s remains)
INFO - root - 2019-11-06 19:54:32.066066: step 67510, total loss = 0.92, predict loss = 0.25 (96.0 examples/sec; 0.042 sec/batch; 0h:57m:16s remains)
INFO - root - 2019-11-06 19:54:33.307372: step 67520, total loss = 1.09, predict loss = 0.29 (57.1 examples/sec; 0.070 sec/batch; 1h:36m:17s remains)
INFO - root - 2019-11-06 19:54:34.062051: step 67530, total loss = 0.71, predict loss = 0.17 (52.4 examples/sec; 0.076 sec/batch; 1h:45m:01s remains)
INFO - root - 2019-11-06 19:54:34.843409: step 67540, total loss = 0.51, predict loss = 0.13 (52.7 examples/sec; 0.076 sec/batch; 1h:44m:20s remains)
INFO - root - 2019-11-06 19:54:35.639731: step 67550, total loss = 0.56, predict loss = 0.17 (57.0 examples/sec; 0.070 sec/batch; 1h:36m:25s remains)
INFO - root - 2019-11-06 19:54:36.340632: step 67560, total loss = 0.37, predict loss = 0.08 (66.7 examples/sec; 0.060 sec/batch; 1h:22m:21s remains)
INFO - root - 2019-11-06 19:54:36.856292: step 67570, total loss = 0.72, predict loss = 0.18 (95.0 examples/sec; 0.042 sec/batch; 0h:57m:51s remains)
INFO - root - 2019-11-06 19:54:37.341026: step 67580, total loss = 0.58, predict loss = 0.15 (91.9 examples/sec; 0.044 sec/batch; 0h:59m:47s remains)
INFO - root - 2019-11-06 19:54:38.433847: step 67590, total loss = 0.34, predict loss = 0.09 (70.7 examples/sec; 0.057 sec/batch; 1h:17m:40s remains)
INFO - root - 2019-11-06 19:54:39.159989: step 67600, total loss = 0.65, predict loss = 0.16 (52.0 examples/sec; 0.077 sec/batch; 1h:45m:42s remains)
INFO - root - 2019-11-06 19:54:39.955140: step 67610, total loss = 0.49, predict loss = 0.14 (57.1 examples/sec; 0.070 sec/batch; 1h:36m:12s remains)
INFO - root - 2019-11-06 19:54:40.670885: step 67620, total loss = 0.65, predict loss = 0.18 (67.1 examples/sec; 0.060 sec/batch; 1h:21m:48s remains)
INFO - root - 2019-11-06 19:54:41.416763: step 67630, total loss = 1.43, predict loss = 0.39 (62.3 examples/sec; 0.064 sec/batch; 1h:28m:10s remains)
INFO - root - 2019-11-06 19:54:42.069023: step 67640, total loss = 0.60, predict loss = 0.14 (95.4 examples/sec; 0.042 sec/batch; 0h:57m:32s remains)
INFO - root - 2019-11-06 19:54:42.497445: step 67650, total loss = 0.87, predict loss = 0.23 (97.7 examples/sec; 0.041 sec/batch; 0h:56m:10s remains)
INFO - root - 2019-11-06 19:54:42.964815: step 67660, total loss = 0.70, predict loss = 0.17 (101.0 examples/sec; 0.040 sec/batch; 0h:54m:20s remains)
INFO - root - 2019-11-06 19:54:44.264961: step 67670, total loss = 0.60, predict loss = 0.15 (57.7 examples/sec; 0.069 sec/batch; 1h:35m:11s remains)
INFO - root - 2019-11-06 19:54:45.033215: step 67680, total loss = 0.29, predict loss = 0.07 (58.0 examples/sec; 0.069 sec/batch; 1h:34m:37s remains)
INFO - root - 2019-11-06 19:54:45.788402: step 67690, total loss = 1.09, predict loss = 0.30 (53.0 examples/sec; 0.075 sec/batch; 1h:43m:32s remains)
INFO - root - 2019-11-06 19:54:46.517614: step 67700, total loss = 0.72, predict loss = 0.17 (59.5 examples/sec; 0.067 sec/batch; 1h:32m:12s remains)
INFO - root - 2019-11-06 19:54:47.276596: step 67710, total loss = 0.64, predict loss = 0.20 (69.3 examples/sec; 0.058 sec/batch; 1h:19m:08s remains)
INFO - root - 2019-11-06 19:54:47.824951: step 67720, total loss = 0.68, predict loss = 0.18 (92.4 examples/sec; 0.043 sec/batch; 0h:59m:21s remains)
INFO - root - 2019-11-06 19:54:48.269321: step 67730, total loss = 1.30, predict loss = 0.35 (98.0 examples/sec; 0.041 sec/batch; 0h:55m:59s remains)
INFO - root - 2019-11-06 19:54:49.444377: step 67740, total loss = 0.91, predict loss = 0.25 (68.1 examples/sec; 0.059 sec/batch; 1h:20m:32s remains)
INFO - root - 2019-11-06 19:54:50.142099: step 67750, total loss = 0.60, predict loss = 0.17 (64.4 examples/sec; 0.062 sec/batch; 1h:25m:11s remains)
INFO - root - 2019-11-06 19:54:50.906345: step 67760, total loss = 1.32, predict loss = 0.38 (52.8 examples/sec; 0.076 sec/batch; 1h:43m:47s remains)
INFO - root - 2019-11-06 19:54:51.673552: step 67770, total loss = 1.13, predict loss = 0.34 (58.2 examples/sec; 0.069 sec/batch; 1h:34m:08s remains)
INFO - root - 2019-11-06 19:54:52.415748: step 67780, total loss = 0.42, predict loss = 0.10 (54.7 examples/sec; 0.073 sec/batch; 1h:40m:13s remains)
INFO - root - 2019-11-06 19:54:53.026636: step 67790, total loss = 0.76, predict loss = 0.20 (96.6 examples/sec; 0.041 sec/batch; 0h:56m:44s remains)
INFO - root - 2019-11-06 19:54:53.474079: step 67800, total loss = 1.35, predict loss = 0.44 (96.1 examples/sec; 0.042 sec/batch; 0h:57m:00s remains)
INFO - root - 2019-11-06 19:54:53.938573: step 67810, total loss = 0.60, predict loss = 0.15 (90.6 examples/sec; 0.044 sec/batch; 1h:00m:28s remains)
INFO - root - 2019-11-06 19:54:55.261660: step 67820, total loss = 0.87, predict loss = 0.24 (67.4 examples/sec; 0.059 sec/batch; 1h:21m:17s remains)
INFO - root - 2019-11-06 19:54:55.997262: step 67830, total loss = 0.93, predict loss = 0.24 (61.8 examples/sec; 0.065 sec/batch; 1h:28m:36s remains)
INFO - root - 2019-11-06 19:54:56.691809: step 67840, total loss = 0.77, predict loss = 0.21 (66.6 examples/sec; 0.060 sec/batch; 1h:22m:17s remains)
INFO - root - 2019-11-06 19:54:57.410031: step 67850, total loss = 0.80, predict loss = 0.20 (57.5 examples/sec; 0.070 sec/batch; 1h:35m:17s remains)
INFO - root - 2019-11-06 19:54:58.139115: step 67860, total loss = 1.25, predict loss = 0.28 (71.8 examples/sec; 0.056 sec/batch; 1h:16m:16s remains)
INFO - root - 2019-11-06 19:54:58.622018: step 67870, total loss = 0.80, predict loss = 0.21 (97.5 examples/sec; 0.041 sec/batch; 0h:56m:08s remains)
INFO - root - 2019-11-06 19:54:59.075154: step 67880, total loss = 0.65, predict loss = 0.18 (95.1 examples/sec; 0.042 sec/batch; 0h:57m:32s remains)
INFO - root - 2019-11-06 19:55:00.263601: step 67890, total loss = 1.15, predict loss = 0.35 (66.5 examples/sec; 0.060 sec/batch; 1h:22m:18s remains)
INFO - root - 2019-11-06 19:55:00.988220: step 67900, total loss = 0.62, predict loss = 0.14 (55.9 examples/sec; 0.072 sec/batch; 1h:37m:54s remains)
INFO - root - 2019-11-06 19:55:01.782224: step 67910, total loss = 1.04, predict loss = 0.24 (55.9 examples/sec; 0.072 sec/batch; 1h:37m:49s remains)
INFO - root - 2019-11-06 19:55:02.567493: step 67920, total loss = 0.47, predict loss = 0.12 (54.9 examples/sec; 0.073 sec/batch; 1h:39m:43s remains)
INFO - root - 2019-11-06 19:55:03.304799: step 67930, total loss = 0.59, predict loss = 0.17 (58.4 examples/sec; 0.068 sec/batch; 1h:33m:39s remains)
INFO - root - 2019-11-06 19:55:03.910589: step 67940, total loss = 1.22, predict loss = 0.30 (104.3 examples/sec; 0.038 sec/batch; 0h:52m:27s remains)
INFO - root - 2019-11-06 19:55:04.369673: step 67950, total loss = 0.77, predict loss = 0.24 (96.3 examples/sec; 0.042 sec/batch; 0h:56m:48s remains)
INFO - root - 2019-11-06 19:55:04.831324: step 67960, total loss = 0.89, predict loss = 0.19 (92.9 examples/sec; 0.043 sec/batch; 0h:58m:52s remains)
INFO - root - 2019-11-06 19:55:06.133395: step 67970, total loss = 1.39, predict loss = 0.39 (62.5 examples/sec; 0.064 sec/batch; 1h:27m:25s remains)
INFO - root - 2019-11-06 19:55:06.868783: step 67980, total loss = 1.20, predict loss = 0.36 (64.3 examples/sec; 0.062 sec/batch; 1h:24m:59s remains)
INFO - root - 2019-11-06 19:55:07.612265: step 67990, total loss = 1.01, predict loss = 0.31 (59.2 examples/sec; 0.068 sec/batch; 1h:32m:25s remains)
INFO - root - 2019-11-06 19:55:08.376488: step 68000, total loss = 0.92, predict loss = 0.24 (63.0 examples/sec; 0.063 sec/batch; 1h:26m:46s remains)
INFO - root - 2019-11-06 19:55:09.071380: step 68010, total loss = 0.47, predict loss = 0.12 (62.8 examples/sec; 0.064 sec/batch; 1h:27m:04s remains)
INFO - root - 2019-11-06 19:55:09.594158: step 68020, total loss = 0.43, predict loss = 0.11 (97.9 examples/sec; 0.041 sec/batch; 0h:55m:48s remains)
INFO - root - 2019-11-06 19:55:10.041977: step 68030, total loss = 0.60, predict loss = 0.16 (91.4 examples/sec; 0.044 sec/batch; 0h:59m:46s remains)
INFO - root - 2019-11-06 19:55:11.238201: step 68040, total loss = 0.98, predict loss = 0.31 (71.3 examples/sec; 0.056 sec/batch; 1h:16m:39s remains)
INFO - root - 2019-11-06 19:55:11.908354: step 68050, total loss = 0.76, predict loss = 0.20 (58.7 examples/sec; 0.068 sec/batch; 1h:33m:07s remains)
INFO - root - 2019-11-06 19:55:12.710155: step 68060, total loss = 0.62, predict loss = 0.15 (53.1 examples/sec; 0.075 sec/batch; 1h:42m:49s remains)
INFO - root - 2019-11-06 19:55:13.510823: step 68070, total loss = 1.24, predict loss = 0.30 (49.8 examples/sec; 0.080 sec/batch; 1h:49m:35s remains)
INFO - root - 2019-11-06 19:55:14.346102: step 68080, total loss = 1.22, predict loss = 0.29 (62.3 examples/sec; 0.064 sec/batch; 1h:27m:39s remains)
INFO - root - 2019-11-06 19:55:14.880756: step 68090, total loss = 0.96, predict loss = 0.21 (98.0 examples/sec; 0.041 sec/batch; 0h:55m:42s remains)
INFO - root - 2019-11-06 19:55:15.343226: step 68100, total loss = 1.06, predict loss = 0.27 (99.0 examples/sec; 0.040 sec/batch; 0h:55m:07s remains)
INFO - root - 2019-11-06 19:55:15.790107: step 68110, total loss = 0.35, predict loss = 0.08 (118.0 examples/sec; 0.034 sec/batch; 0h:46m:16s remains)
INFO - root - 2019-11-06 19:55:17.119097: step 68120, total loss = 1.48, predict loss = 0.42 (53.9 examples/sec; 0.074 sec/batch; 1h:41m:11s remains)
INFO - root - 2019-11-06 19:55:17.829039: step 68130, total loss = 0.53, predict loss = 0.14 (62.5 examples/sec; 0.064 sec/batch; 1h:27m:22s remains)
INFO - root - 2019-11-06 19:55:18.555614: step 68140, total loss = 0.70, predict loss = 0.19 (68.1 examples/sec; 0.059 sec/batch; 1h:20m:05s remains)
INFO - root - 2019-11-06 19:55:19.273888: step 68150, total loss = 0.56, predict loss = 0.14 (59.6 examples/sec; 0.067 sec/batch; 1h:31m:32s remains)
INFO - root - 2019-11-06 19:55:19.999265: step 68160, total loss = 1.25, predict loss = 0.37 (65.9 examples/sec; 0.061 sec/batch; 1h:22m:46s remains)
INFO - root - 2019-11-06 19:55:20.466555: step 68170, total loss = 0.59, predict loss = 0.16 (101.6 examples/sec; 0.039 sec/batch; 0h:53m:41s remains)
INFO - root - 2019-11-06 19:55:20.940038: step 68180, total loss = 1.42, predict loss = 0.42 (91.7 examples/sec; 0.044 sec/batch; 0h:59m:30s remains)
INFO - root - 2019-11-06 19:55:22.185478: step 68190, total loss = 0.50, predict loss = 0.11 (57.9 examples/sec; 0.069 sec/batch; 1h:34m:12s remains)
INFO - root - 2019-11-06 19:55:22.941200: step 68200, total loss = 1.19, predict loss = 0.27 (57.8 examples/sec; 0.069 sec/batch; 1h:34m:19s remains)
INFO - root - 2019-11-06 19:55:23.724920: step 68210, total loss = 0.70, predict loss = 0.20 (53.9 examples/sec; 0.074 sec/batch; 1h:41m:09s remains)
INFO - root - 2019-11-06 19:55:24.517056: step 68220, total loss = 0.60, predict loss = 0.15 (57.4 examples/sec; 0.070 sec/batch; 1h:35m:03s remains)
INFO - root - 2019-11-06 19:55:25.266333: step 68230, total loss = 0.90, predict loss = 0.22 (70.0 examples/sec; 0.057 sec/batch; 1h:17m:52s remains)
INFO - root - 2019-11-06 19:55:25.811767: step 68240, total loss = 1.03, predict loss = 0.28 (100.4 examples/sec; 0.040 sec/batch; 0h:54m:17s remains)
INFO - root - 2019-11-06 19:55:26.254486: step 68250, total loss = 0.99, predict loss = 0.26 (98.0 examples/sec; 0.041 sec/batch; 0h:55m:36s remains)
INFO - root - 2019-11-06 19:55:27.393041: step 68260, total loss = 0.81, predict loss = 0.22 (5.6 examples/sec; 0.720 sec/batch; 16h:20m:16s remains)
INFO - root - 2019-11-06 19:55:28.094680: step 68270, total loss = 0.48, predict loss = 0.11 (61.0 examples/sec; 0.066 sec/batch; 1h:29m:22s remains)
INFO - root - 2019-11-06 19:55:28.849303: step 68280, total loss = 0.36, predict loss = 0.08 (49.8 examples/sec; 0.080 sec/batch; 1h:49m:24s remains)
INFO - root - 2019-11-06 19:55:29.600699: step 68290, total loss = 0.91, predict loss = 0.25 (64.6 examples/sec; 0.062 sec/batch; 1h:24m:19s remains)
INFO - root - 2019-11-06 19:55:30.363011: step 68300, total loss = 1.47, predict loss = 0.41 (54.8 examples/sec; 0.073 sec/batch; 1h:39m:20s remains)
INFO - root - 2019-11-06 19:55:31.029515: step 68310, total loss = 0.75, predict loss = 0.20 (97.2 examples/sec; 0.041 sec/batch; 0h:56m:03s remains)
INFO - root - 2019-11-06 19:55:31.472155: step 68320, total loss = 0.73, predict loss = 0.20 (97.5 examples/sec; 0.041 sec/batch; 0h:55m:50s remains)
INFO - root - 2019-11-06 19:55:31.919051: step 68330, total loss = 0.69, predict loss = 0.17 (93.9 examples/sec; 0.043 sec/batch; 0h:58m:00s remains)
INFO - root - 2019-11-06 19:55:33.181962: step 68340, total loss = 0.83, predict loss = 0.20 (66.7 examples/sec; 0.060 sec/batch; 1h:21m:40s remains)
INFO - root - 2019-11-06 19:55:33.869305: step 68350, total loss = 0.67, predict loss = 0.18 (56.9 examples/sec; 0.070 sec/batch; 1h:35m:41s remains)
INFO - root - 2019-11-06 19:55:34.644897: step 68360, total loss = 1.16, predict loss = 0.34 (60.7 examples/sec; 0.066 sec/batch; 1h:29m:39s remains)
INFO - root - 2019-11-06 19:55:35.384290: step 68370, total loss = 0.97, predict loss = 0.27 (59.7 examples/sec; 0.067 sec/batch; 1h:31m:05s remains)
INFO - root - 2019-11-06 19:55:36.152164: step 68380, total loss = 0.51, predict loss = 0.12 (72.9 examples/sec; 0.055 sec/batch; 1h:14m:38s remains)
INFO - root - 2019-11-06 19:55:36.674064: step 68390, total loss = 0.66, predict loss = 0.17 (99.7 examples/sec; 0.040 sec/batch; 0h:54m:33s remains)
INFO - root - 2019-11-06 19:55:37.114321: step 68400, total loss = 1.16, predict loss = 0.39 (91.7 examples/sec; 0.044 sec/batch; 0h:59m:18s remains)
INFO - root - 2019-11-06 19:55:38.229567: step 68410, total loss = 1.07, predict loss = 0.27 (71.3 examples/sec; 0.056 sec/batch; 1h:16m:16s remains)
INFO - root - 2019-11-06 19:55:38.960811: step 68420, total loss = 0.55, predict loss = 0.15 (55.1 examples/sec; 0.073 sec/batch; 1h:38m:38s remains)
INFO - root - 2019-11-06 19:55:39.773961: step 68430, total loss = 1.23, predict loss = 0.38 (62.1 examples/sec; 0.064 sec/batch; 1h:27m:33s remains)
INFO - root - 2019-11-06 19:55:40.532714: step 68440, total loss = 1.04, predict loss = 0.30 (63.1 examples/sec; 0.063 sec/batch; 1h:26m:06s remains)
INFO - root - 2019-11-06 19:55:41.281188: step 68450, total loss = 0.92, predict loss = 0.21 (57.0 examples/sec; 0.070 sec/batch; 1h:35m:26s remains)
INFO - root - 2019-11-06 19:55:41.959167: step 68460, total loss = 0.56, predict loss = 0.17 (96.2 examples/sec; 0.042 sec/batch; 0h:56m:28s remains)
INFO - root - 2019-11-06 19:55:42.399470: step 68470, total loss = 0.74, predict loss = 0.21 (98.2 examples/sec; 0.041 sec/batch; 0h:55m:20s remains)
INFO - root - 2019-11-06 19:55:42.844598: step 68480, total loss = 0.71, predict loss = 0.18 (98.2 examples/sec; 0.041 sec/batch; 0h:55m:21s remains)
INFO - root - 2019-11-06 19:55:44.086253: step 68490, total loss = 1.07, predict loss = 0.29 (62.0 examples/sec; 0.064 sec/batch; 1h:27m:36s remains)
INFO - root - 2019-11-06 19:55:44.819258: step 68500, total loss = 0.72, predict loss = 0.19 (60.6 examples/sec; 0.066 sec/batch; 1h:29m:35s remains)
INFO - root - 2019-11-06 19:55:45.567222: step 68510, total loss = 0.51, predict loss = 0.13 (58.8 examples/sec; 0.068 sec/batch; 1h:32m:20s remains)
INFO - root - 2019-11-06 19:55:46.338035: step 68520, total loss = 0.67, predict loss = 0.16 (55.4 examples/sec; 0.072 sec/batch; 1h:38m:00s remains)
INFO - root - 2019-11-06 19:55:47.100036: step 68530, total loss = 0.82, predict loss = 0.21 (70.2 examples/sec; 0.057 sec/batch; 1h:17m:20s remains)
INFO - root - 2019-11-06 19:55:47.644775: step 68540, total loss = 0.53, predict loss = 0.14 (94.9 examples/sec; 0.042 sec/batch; 0h:57m:13s remains)
INFO - root - 2019-11-06 19:55:48.097505: step 68550, total loss = 0.49, predict loss = 0.12 (93.6 examples/sec; 0.043 sec/batch; 0h:58m:01s remains)
INFO - root - 2019-11-06 19:55:49.277359: step 68560, total loss = 0.89, predict loss = 0.24 (66.3 examples/sec; 0.060 sec/batch; 1h:21m:51s remains)
INFO - root - 2019-11-06 19:55:49.991387: step 68570, total loss = 0.59, predict loss = 0.15 (53.9 examples/sec; 0.074 sec/batch; 1h:40m:40s remains)
INFO - root - 2019-11-06 19:55:50.753614: step 68580, total loss = 1.29, predict loss = 0.38 (63.0 examples/sec; 0.063 sec/batch; 1h:26m:09s remains)
INFO - root - 2019-11-06 19:55:51.474065: step 68590, total loss = 0.84, predict loss = 0.24 (59.4 examples/sec; 0.067 sec/batch; 1h:31m:21s remains)
INFO - root - 2019-11-06 19:55:52.215962: step 68600, total loss = 0.58, predict loss = 0.15 (61.0 examples/sec; 0.066 sec/batch; 1h:28m:58s remains)
INFO - root - 2019-11-06 19:55:52.789321: step 68610, total loss = 1.45, predict loss = 0.45 (106.5 examples/sec; 0.038 sec/batch; 0h:50m:57s remains)
INFO - root - 2019-11-06 19:55:53.261823: step 68620, total loss = 0.99, predict loss = 0.26 (98.4 examples/sec; 0.041 sec/batch; 0h:55m:09s remains)
INFO - root - 2019-11-06 19:55:53.723269: step 68630, total loss = 0.80, predict loss = 0.18 (96.2 examples/sec; 0.042 sec/batch; 0h:56m:25s remains)
INFO - root - 2019-11-06 19:55:55.062801: step 68640, total loss = 0.71, predict loss = 0.19 (63.9 examples/sec; 0.063 sec/batch; 1h:24m:49s remains)
INFO - root - 2019-11-06 19:55:55.732593: step 68650, total loss = 0.67, predict loss = 0.20 (67.0 examples/sec; 0.060 sec/batch; 1h:20m:55s remains)
INFO - root - 2019-11-06 19:55:56.494793: step 68660, total loss = 1.19, predict loss = 0.33 (60.7 examples/sec; 0.066 sec/batch; 1h:29m:18s remains)
INFO - root - 2019-11-06 19:55:57.190652: step 68670, total loss = 1.65, predict loss = 0.47 (62.8 examples/sec; 0.064 sec/batch; 1h:26m:20s remains)
INFO - root - 2019-11-06 19:55:57.920483: step 68680, total loss = 0.67, predict loss = 0.16 (69.4 examples/sec; 0.058 sec/batch; 1h:18m:09s remains)
INFO - root - 2019-11-06 19:55:58.429073: step 68690, total loss = 0.69, predict loss = 0.17 (98.0 examples/sec; 0.041 sec/batch; 0h:55m:17s remains)
INFO - root - 2019-11-06 19:55:58.895789: step 68700, total loss = 0.77, predict loss = 0.18 (104.8 examples/sec; 0.038 sec/batch; 0h:51m:43s remains)
INFO - root - 2019-11-06 19:56:00.082369: step 68710, total loss = 0.52, predict loss = 0.13 (62.7 examples/sec; 0.064 sec/batch; 1h:26m:26s remains)
INFO - root - 2019-11-06 19:56:00.840937: step 68720, total loss = 0.75, predict loss = 0.20 (56.3 examples/sec; 0.071 sec/batch; 1h:36m:09s remains)
INFO - root - 2019-11-06 19:56:01.701075: step 68730, total loss = 0.53, predict loss = 0.13 (56.0 examples/sec; 0.071 sec/batch; 1h:36m:48s remains)
INFO - root - 2019-11-06 19:56:02.476205: step 68740, total loss = 0.90, predict loss = 0.26 (56.7 examples/sec; 0.071 sec/batch; 1h:35m:30s remains)
INFO - root - 2019-11-06 19:56:03.195887: step 68750, total loss = 0.65, predict loss = 0.15 (65.1 examples/sec; 0.061 sec/batch; 1h:23m:11s remains)
INFO - root - 2019-11-06 19:56:03.706092: step 68760, total loss = 1.09, predict loss = 0.25 (105.2 examples/sec; 0.038 sec/batch; 0h:51m:28s remains)
INFO - root - 2019-11-06 19:56:04.151271: step 68770, total loss = 0.89, predict loss = 0.28 (94.4 examples/sec; 0.042 sec/batch; 0h:57m:21s remains)
INFO - root - 2019-11-06 19:56:04.624815: step 68780, total loss = 1.00, predict loss = 0.26 (98.3 examples/sec; 0.041 sec/batch; 0h:55m:06s remains)
INFO - root - 2019-11-06 19:56:05.913202: step 68790, total loss = 0.89, predict loss = 0.25 (65.1 examples/sec; 0.061 sec/batch; 1h:23m:06s remains)
INFO - root - 2019-11-06 19:56:06.664260: step 68800, total loss = 1.00, predict loss = 0.27 (55.6 examples/sec; 0.072 sec/batch; 1h:37m:23s remains)
INFO - root - 2019-11-06 19:56:07.470747: step 68810, total loss = 0.75, predict loss = 0.18 (52.7 examples/sec; 0.076 sec/batch; 1h:42m:41s remains)
INFO - root - 2019-11-06 19:56:08.220883: step 68820, total loss = 0.42, predict loss = 0.10 (59.1 examples/sec; 0.068 sec/batch; 1h:31m:36s remains)
INFO - root - 2019-11-06 19:56:08.956149: step 68830, total loss = 1.07, predict loss = 0.29 (68.6 examples/sec; 0.058 sec/batch; 1h:18m:54s remains)
INFO - root - 2019-11-06 19:56:09.457781: step 68840, total loss = 0.64, predict loss = 0.16 (91.4 examples/sec; 0.044 sec/batch; 0h:59m:13s remains)
INFO - root - 2019-11-06 19:56:09.919333: step 68850, total loss = 1.18, predict loss = 0.35 (93.1 examples/sec; 0.043 sec/batch; 0h:58m:05s remains)
INFO - root - 2019-11-06 19:56:11.122715: step 68860, total loss = 0.79, predict loss = 0.21 (68.8 examples/sec; 0.058 sec/batch; 1h:18m:37s remains)
INFO - root - 2019-11-06 19:56:11.878144: step 68870, total loss = 0.69, predict loss = 0.18 (68.1 examples/sec; 0.059 sec/batch; 1h:19m:26s remains)
INFO - root - 2019-11-06 19:56:12.671605: step 68880, total loss = 1.26, predict loss = 0.37 (53.6 examples/sec; 0.075 sec/batch; 1h:40m:54s remains)
INFO - root - 2019-11-06 19:56:13.399961: step 68890, total loss = 0.64, predict loss = 0.16 (58.7 examples/sec; 0.068 sec/batch; 1h:32m:06s remains)
INFO - root - 2019-11-06 19:56:14.190930: step 68900, total loss = 0.84, predict loss = 0.22 (59.6 examples/sec; 0.067 sec/batch; 1h:30m:39s remains)
INFO - root - 2019-11-06 19:56:14.741041: step 68910, total loss = 1.48, predict loss = 0.46 (98.5 examples/sec; 0.041 sec/batch; 0h:54m:53s remains)
INFO - root - 2019-11-06 19:56:15.179550: step 68920, total loss = 0.69, predict loss = 0.18 (95.3 examples/sec; 0.042 sec/batch; 0h:56m:43s remains)
INFO - root - 2019-11-06 19:56:15.630690: step 68930, total loss = 2.10, predict loss = 0.70 (113.6 examples/sec; 0.035 sec/batch; 0h:47m:33s remains)
INFO - root - 2019-11-06 19:56:16.958594: step 68940, total loss = 0.61, predict loss = 0.14 (65.9 examples/sec; 0.061 sec/batch; 1h:21m:57s remains)
INFO - root - 2019-11-06 19:56:17.654662: step 68950, total loss = 0.48, predict loss = 0.12 (59.1 examples/sec; 0.068 sec/batch; 1h:31m:21s remains)
INFO - root - 2019-11-06 19:56:18.342545: step 68960, total loss = 0.48, predict loss = 0.14 (65.2 examples/sec; 0.061 sec/batch; 1h:22m:54s remains)
INFO - root - 2019-11-06 19:56:19.074147: step 68970, total loss = 0.47, predict loss = 0.12 (61.5 examples/sec; 0.065 sec/batch; 1h:27m:47s remains)
INFO - root - 2019-11-06 19:56:19.749534: step 68980, total loss = 0.52, predict loss = 0.14 (85.1 examples/sec; 0.047 sec/batch; 1h:03m:28s remains)
INFO - root - 2019-11-06 19:56:20.208838: step 68990, total loss = 0.94, predict loss = 0.26 (107.3 examples/sec; 0.037 sec/batch; 0h:50m:19s remains)
INFO - root - 2019-11-06 19:56:20.641898: step 69000, total loss = 0.66, predict loss = 0.17 (98.5 examples/sec; 0.041 sec/batch; 0h:54m:50s remains)
INFO - root - 2019-11-06 19:56:21.872381: step 69010, total loss = 0.64, predict loss = 0.18 (67.8 examples/sec; 0.059 sec/batch; 1h:19m:36s remains)
INFO - root - 2019-11-06 19:56:22.645565: step 69020, total loss = 0.75, predict loss = 0.18 (56.6 examples/sec; 0.071 sec/batch; 1h:35m:21s remains)
INFO - root - 2019-11-06 19:56:23.424495: step 69030, total loss = 0.92, predict loss = 0.25 (56.1 examples/sec; 0.071 sec/batch; 1h:36m:14s remains)
INFO - root - 2019-11-06 19:56:24.205994: step 69040, total loss = 1.05, predict loss = 0.30 (73.5 examples/sec; 0.054 sec/batch; 1h:13m:26s remains)
INFO - root - 2019-11-06 19:56:24.878810: step 69050, total loss = 1.03, predict loss = 0.30 (73.1 examples/sec; 0.055 sec/batch; 1h:13m:48s remains)
INFO - root - 2019-11-06 19:56:25.420215: step 69060, total loss = 0.58, predict loss = 0.15 (101.2 examples/sec; 0.040 sec/batch; 0h:53m:20s remains)
INFO - root - 2019-11-06 19:56:25.872756: step 69070, total loss = 0.49, predict loss = 0.11 (88.4 examples/sec; 0.045 sec/batch; 1h:01m:03s remains)
INFO - root - 2019-11-06 19:56:26.993196: step 69080, total loss = 0.48, predict loss = 0.12 (5.5 examples/sec; 0.733 sec/batch; 16h:29m:03s remains)
INFO - root - 2019-11-06 19:56:27.694120: step 69090, total loss = 0.97, predict loss = 0.27 (61.2 examples/sec; 0.065 sec/batch; 1h:28m:06s remains)
INFO - root - 2019-11-06 19:56:28.479587: step 69100, total loss = 0.73, predict loss = 0.19 (56.6 examples/sec; 0.071 sec/batch; 1h:35m:13s remains)
INFO - root - 2019-11-06 19:56:29.173236: step 69110, total loss = 1.05, predict loss = 0.31 (68.5 examples/sec; 0.058 sec/batch; 1h:18m:43s remains)
INFO - root - 2019-11-06 19:56:29.833926: step 69120, total loss = 1.33, predict loss = 0.37 (73.6 examples/sec; 0.054 sec/batch; 1h:13m:17s remains)
INFO - root - 2019-11-06 19:56:30.392386: step 69130, total loss = 0.87, predict loss = 0.28 (99.5 examples/sec; 0.040 sec/batch; 0h:54m:11s remains)
INFO - root - 2019-11-06 19:56:30.855072: step 69140, total loss = 0.45, predict loss = 0.11 (96.4 examples/sec; 0.041 sec/batch; 0h:55m:54s remains)
INFO - root - 2019-11-06 19:56:31.312425: step 69150, total loss = 0.91, predict loss = 0.26 (99.3 examples/sec; 0.040 sec/batch; 0h:54m:15s remains)
INFO - root - 2019-11-06 19:56:32.555668: step 69160, total loss = 1.07, predict loss = 0.29 (65.5 examples/sec; 0.061 sec/batch; 1h:22m:17s remains)
INFO - root - 2019-11-06 19:56:33.258690: step 69170, total loss = 0.61, predict loss = 0.16 (60.8 examples/sec; 0.066 sec/batch; 1h:28m:38s remains)
INFO - root - 2019-11-06 19:56:34.005027: step 69180, total loss = 0.93, predict loss = 0.25 (57.6 examples/sec; 0.069 sec/batch; 1h:33m:30s remains)
INFO - root - 2019-11-06 19:56:34.709388: step 69190, total loss = 0.86, predict loss = 0.23 (61.2 examples/sec; 0.065 sec/batch; 1h:27m:59s remains)
INFO - root - 2019-11-06 19:56:35.462702: step 69200, total loss = 0.40, predict loss = 0.10 (68.3 examples/sec; 0.059 sec/batch; 1h:18m:52s remains)
INFO - root - 2019-11-06 19:56:36.010778: step 69210, total loss = 0.48, predict loss = 0.10 (106.2 examples/sec; 0.038 sec/batch; 0h:50m:43s remains)
INFO - root - 2019-11-06 19:56:36.484933: step 69220, total loss = 1.67, predict loss = 0.48 (94.5 examples/sec; 0.042 sec/batch; 0h:56m:59s remains)
INFO - root - 2019-11-06 19:56:37.628458: step 69230, total loss = 0.92, predict loss = 0.26 (66.1 examples/sec; 0.060 sec/batch; 1h:21m:24s remains)
INFO - root - 2019-11-06 19:56:38.332550: step 69240, total loss = 0.90, predict loss = 0.24 (54.1 examples/sec; 0.074 sec/batch; 1h:39m:35s remains)
INFO - root - 2019-11-06 19:56:39.033188: step 69250, total loss = 0.61, predict loss = 0.17 (58.4 examples/sec; 0.069 sec/batch; 1h:32m:14s remains)
INFO - root - 2019-11-06 19:56:39.807227: step 69260, total loss = 0.62, predict loss = 0.16 (59.8 examples/sec; 0.067 sec/batch; 1h:30m:01s remains)
INFO - root - 2019-11-06 19:56:40.575466: step 69270, total loss = 0.97, predict loss = 0.30 (56.3 examples/sec; 0.071 sec/batch; 1h:35m:34s remains)
INFO - root - 2019-11-06 19:56:41.261942: step 69280, total loss = 0.61, predict loss = 0.14 (88.2 examples/sec; 0.045 sec/batch; 1h:01m:00s remains)
INFO - root - 2019-11-06 19:56:41.692343: step 69290, total loss = 0.58, predict loss = 0.14 (100.7 examples/sec; 0.040 sec/batch; 0h:53m:26s remains)
INFO - root - 2019-11-06 19:56:42.172078: step 69300, total loss = 0.77, predict loss = 0.20 (95.8 examples/sec; 0.042 sec/batch; 0h:56m:08s remains)
INFO - root - 2019-11-06 19:56:43.451392: step 69310, total loss = 0.60, predict loss = 0.15 (62.9 examples/sec; 0.064 sec/batch; 1h:25m:33s remains)
INFO - root - 2019-11-06 19:56:44.194875: step 69320, total loss = 0.57, predict loss = 0.14 (61.9 examples/sec; 0.065 sec/batch; 1h:26m:51s remains)
INFO - root - 2019-11-06 19:56:44.919112: step 69330, total loss = 0.74, predict loss = 0.20 (60.8 examples/sec; 0.066 sec/batch; 1h:28m:26s remains)
INFO - root - 2019-11-06 19:56:45.731647: step 69340, total loss = 1.26, predict loss = 0.35 (54.2 examples/sec; 0.074 sec/batch; 1h:39m:14s remains)
INFO - root - 2019-11-06 19:56:46.419773: step 69350, total loss = 0.80, predict loss = 0.22 (72.2 examples/sec; 0.055 sec/batch; 1h:14m:26s remains)
INFO - root - 2019-11-06 19:56:46.925115: step 69360, total loss = 1.05, predict loss = 0.29 (100.5 examples/sec; 0.040 sec/batch; 0h:53m:29s remains)
INFO - root - 2019-11-06 19:56:47.364636: step 69370, total loss = 0.93, predict loss = 0.27 (96.8 examples/sec; 0.041 sec/batch; 0h:55m:32s remains)
INFO - root - 2019-11-06 19:56:48.532700: step 69380, total loss = 0.87, predict loss = 0.24 (67.4 examples/sec; 0.059 sec/batch; 1h:19m:46s remains)
INFO - root - 2019-11-06 19:56:49.282592: step 69390, total loss = 0.49, predict loss = 0.11 (56.2 examples/sec; 0.071 sec/batch; 1h:35m:38s remains)
INFO - root - 2019-11-06 19:56:50.094727: step 69400, total loss = 0.43, predict loss = 0.11 (50.3 examples/sec; 0.080 sec/batch; 1h:46m:51s remains)
INFO - root - 2019-11-06 19:56:50.797116: step 69410, total loss = 0.64, predict loss = 0.16 (70.2 examples/sec; 0.057 sec/batch; 1h:16m:29s remains)
INFO - root - 2019-11-06 19:56:51.501280: step 69420, total loss = 1.45, predict loss = 0.43 (67.5 examples/sec; 0.059 sec/batch; 1h:19m:32s remains)
INFO - root - 2019-11-06 19:56:52.092187: step 69430, total loss = 1.26, predict loss = 0.34 (101.5 examples/sec; 0.039 sec/batch; 0h:52m:54s remains)
INFO - root - 2019-11-06 19:56:52.546904: step 69440, total loss = 0.83, predict loss = 0.22 (96.2 examples/sec; 0.042 sec/batch; 0h:55m:48s remains)
INFO - root - 2019-11-06 19:56:53.009733: step 69450, total loss = 0.50, predict loss = 0.12 (93.9 examples/sec; 0.043 sec/batch; 0h:57m:11s remains)
INFO - root - 2019-11-06 19:56:54.316238: step 69460, total loss = 0.35, predict loss = 0.09 (66.6 examples/sec; 0.060 sec/batch; 1h:20m:36s remains)
INFO - root - 2019-11-06 19:56:55.076744: step 69470, total loss = 0.64, predict loss = 0.17 (60.4 examples/sec; 0.066 sec/batch; 1h:28m:48s remains)
INFO - root - 2019-11-06 19:56:55.816215: step 69480, total loss = 0.37, predict loss = 0.08 (61.5 examples/sec; 0.065 sec/batch; 1h:27m:18s remains)
INFO - root - 2019-11-06 19:56:56.532579: step 69490, total loss = 0.56, predict loss = 0.17 (57.1 examples/sec; 0.070 sec/batch; 1h:34m:01s remains)
INFO - root - 2019-11-06 19:56:57.209636: step 69500, total loss = 0.43, predict loss = 0.09 (73.3 examples/sec; 0.055 sec/batch; 1h:13m:14s remains)
INFO - root - 2019-11-06 19:56:57.686910: step 69510, total loss = 1.69, predict loss = 0.54 (98.0 examples/sec; 0.041 sec/batch; 0h:54m:46s remains)
INFO - root - 2019-11-06 19:56:58.137658: step 69520, total loss = 1.20, predict loss = 0.35 (94.0 examples/sec; 0.043 sec/batch; 0h:57m:03s remains)
INFO - root - 2019-11-06 19:56:59.324150: step 69530, total loss = 0.36, predict loss = 0.08 (66.4 examples/sec; 0.060 sec/batch; 1h:20m:50s remains)
INFO - root - 2019-11-06 19:57:00.037727: step 69540, total loss = 0.52, predict loss = 0.16 (64.9 examples/sec; 0.062 sec/batch; 1h:22m:39s remains)
INFO - root - 2019-11-06 19:57:00.808373: step 69550, total loss = 0.60, predict loss = 0.12 (55.7 examples/sec; 0.072 sec/batch; 1h:36m:16s remains)
INFO - root - 2019-11-06 19:57:01.500933: step 69560, total loss = 1.50, predict loss = 0.46 (60.4 examples/sec; 0.066 sec/batch; 1h:28m:45s remains)
INFO - root - 2019-11-06 19:57:02.230181: step 69570, total loss = 0.87, predict loss = 0.26 (57.9 examples/sec; 0.069 sec/batch; 1h:32m:40s remains)
INFO - root - 2019-11-06 19:57:02.832664: step 69580, total loss = 0.89, predict loss = 0.23 (96.5 examples/sec; 0.041 sec/batch; 0h:55m:32s remains)
INFO - root - 2019-11-06 19:57:03.277563: step 69590, total loss = 1.26, predict loss = 0.36 (101.0 examples/sec; 0.040 sec/batch; 0h:53m:04s remains)
INFO - root - 2019-11-06 19:57:03.740653: step 69600, total loss = 0.39, predict loss = 0.09 (95.7 examples/sec; 0.042 sec/batch; 0h:56m:00s remains)
INFO - root - 2019-11-06 19:57:05.107032: step 69610, total loss = 0.80, predict loss = 0.23 (57.5 examples/sec; 0.070 sec/batch; 1h:33m:11s remains)
INFO - root - 2019-11-06 19:57:05.835648: step 69620, total loss = 1.03, predict loss = 0.30 (63.3 examples/sec; 0.063 sec/batch; 1h:24m:38s remains)
INFO - root - 2019-11-06 19:57:06.628914: step 69630, total loss = 0.58, predict loss = 0.13 (52.6 examples/sec; 0.076 sec/batch; 1h:41m:48s remains)
INFO - root - 2019-11-06 19:57:07.357770: step 69640, total loss = 0.81, predict loss = 0.22 (67.5 examples/sec; 0.059 sec/batch; 1h:19m:25s remains)
INFO - root - 2019-11-06 19:57:08.056278: step 69650, total loss = 0.46, predict loss = 0.10 (72.5 examples/sec; 0.055 sec/batch; 1h:13m:53s remains)
INFO - root - 2019-11-06 19:57:08.564970: step 69660, total loss = 0.79, predict loss = 0.21 (99.4 examples/sec; 0.040 sec/batch; 0h:53m:53s remains)
INFO - root - 2019-11-06 19:57:09.021999: step 69670, total loss = 0.67, predict loss = 0.17 (94.0 examples/sec; 0.043 sec/batch; 0h:56m:56s remains)
INFO - root - 2019-11-06 19:57:10.252272: step 69680, total loss = 1.21, predict loss = 0.34 (64.2 examples/sec; 0.062 sec/batch; 1h:23m:20s remains)
INFO - root - 2019-11-06 19:57:10.922369: step 69690, total loss = 0.56, predict loss = 0.16 (66.1 examples/sec; 0.061 sec/batch; 1h:20m:58s remains)
INFO - root - 2019-11-06 19:57:11.703426: step 69700, total loss = 0.74, predict loss = 0.19 (51.0 examples/sec; 0.078 sec/batch; 1h:44m:56s remains)
INFO - root - 2019-11-06 19:57:12.447720: step 69710, total loss = 1.26, predict loss = 0.35 (57.4 examples/sec; 0.070 sec/batch; 1h:33m:17s remains)
INFO - root - 2019-11-06 19:57:13.160919: step 69720, total loss = 0.77, predict loss = 0.24 (54.1 examples/sec; 0.074 sec/batch; 1h:38m:52s remains)
INFO - root - 2019-11-06 19:57:13.730313: step 69730, total loss = 1.40, predict loss = 0.44 (98.0 examples/sec; 0.041 sec/batch; 0h:54m:36s remains)
INFO - root - 2019-11-06 19:57:14.194789: step 69740, total loss = 0.49, predict loss = 0.12 (97.6 examples/sec; 0.041 sec/batch; 0h:54m:47s remains)
INFO - root - 2019-11-06 19:57:14.619416: step 69750, total loss = 0.50, predict loss = 0.15 (149.8 examples/sec; 0.027 sec/batch; 0h:35m:42s remains)
INFO - root - 2019-11-06 19:57:15.966555: step 69760, total loss = 0.59, predict loss = 0.15 (52.1 examples/sec; 0.077 sec/batch; 1h:42m:43s remains)
INFO - root - 2019-11-06 19:57:16.701833: step 69770, total loss = 0.50, predict loss = 0.13 (61.9 examples/sec; 0.065 sec/batch; 1h:26m:24s remains)
INFO - root - 2019-11-06 19:57:17.407572: step 69780, total loss = 0.65, predict loss = 0.16 (68.5 examples/sec; 0.058 sec/batch; 1h:18m:05s remains)
INFO - root - 2019-11-06 19:57:18.094302: step 69790, total loss = 0.64, predict loss = 0.19 (55.6 examples/sec; 0.072 sec/batch; 1h:36m:08s remains)
INFO - root - 2019-11-06 19:57:18.847691: step 69800, total loss = 0.85, predict loss = 0.24 (72.6 examples/sec; 0.055 sec/batch; 1h:13m:41s remains)
INFO - root - 2019-11-06 19:57:19.296455: step 69810, total loss = 1.02, predict loss = 0.26 (103.0 examples/sec; 0.039 sec/batch; 0h:51m:54s remains)
INFO - root - 2019-11-06 19:57:19.764471: step 69820, total loss = 0.43, predict loss = 0.11 (93.9 examples/sec; 0.043 sec/batch; 0h:56m:56s remains)
INFO - root - 2019-11-06 19:57:20.965804: step 69830, total loss = 0.89, predict loss = 0.25 (61.7 examples/sec; 0.065 sec/batch; 1h:26m:41s remains)
INFO - root - 2019-11-06 19:57:21.743965: step 69840, total loss = 1.35, predict loss = 0.41 (53.9 examples/sec; 0.074 sec/batch; 1h:39m:10s remains)
INFO - root - 2019-11-06 19:57:22.510189: step 69850, total loss = 0.44, predict loss = 0.11 (60.3 examples/sec; 0.066 sec/batch; 1h:28m:40s remains)
INFO - root - 2019-11-06 19:57:23.250772: step 69860, total loss = 1.14, predict loss = 0.31 (60.1 examples/sec; 0.067 sec/batch; 1h:28m:57s remains)
INFO - root - 2019-11-06 19:57:23.972194: step 69870, total loss = 0.41, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 1h:20m:18s remains)
INFO - root - 2019-11-06 19:57:24.518925: step 69880, total loss = 0.62, predict loss = 0.17 (85.0 examples/sec; 0.047 sec/batch; 1h:02m:48s remains)
INFO - root - 2019-11-06 19:57:24.959607: step 69890, total loss = 0.47, predict loss = 0.11 (97.3 examples/sec; 0.041 sec/batch; 0h:54m:54s remains)
INFO - root - 2019-11-06 19:57:26.096787: step 69900, total loss = 0.49, predict loss = 0.12 (5.7 examples/sec; 0.708 sec/batch; 15h:44m:48s remains)
INFO - root - 2019-11-06 19:57:26.811907: step 69910, total loss = 0.44, predict loss = 0.10 (55.3 examples/sec; 0.072 sec/batch; 1h:36m:29s remains)
INFO - root - 2019-11-06 19:57:27.578236: step 69920, total loss = 0.44, predict loss = 0.11 (56.2 examples/sec; 0.071 sec/batch; 1h:35m:03s remains)
INFO - root - 2019-11-06 19:57:28.301725: step 69930, total loss = 0.57, predict loss = 0.14 (62.7 examples/sec; 0.064 sec/batch; 1h:25m:08s remains)
INFO - root - 2019-11-06 19:57:29.030632: step 69940, total loss = 0.50, predict loss = 0.12 (61.7 examples/sec; 0.065 sec/batch; 1h:26m:26s remains)
INFO - root - 2019-11-06 19:57:29.697614: step 69950, total loss = 0.44, predict loss = 0.11 (92.7 examples/sec; 0.043 sec/batch; 0h:57m:33s remains)
INFO - root - 2019-11-06 19:57:30.163156: step 69960, total loss = 1.26, predict loss = 0.38 (101.8 examples/sec; 0.039 sec/batch; 0h:52m:23s remains)
INFO - root - 2019-11-06 19:57:30.615775: step 69970, total loss = 0.62, predict loss = 0.13 (96.3 examples/sec; 0.042 sec/batch; 0h:55m:22s remains)
INFO - root - 2019-11-06 19:57:31.898736: step 69980, total loss = 0.87, predict loss = 0.22 (63.3 examples/sec; 0.063 sec/batch; 1h:24m:19s remains)
INFO - root - 2019-11-06 19:57:32.652673: step 69990, total loss = 0.61, predict loss = 0.16 (57.7 examples/sec; 0.069 sec/batch; 1h:32m:24s remains)
INFO - root - 2019-11-06 19:57:33.427306: step 70000, total loss = 0.76, predict loss = 0.20 (61.1 examples/sec; 0.065 sec/batch; 1h:27m:14s remains)
INFO - root - 2019-11-06 19:57:34.128353: step 70010, total loss = 0.60, predict loss = 0.16 (60.6 examples/sec; 0.066 sec/batch; 1h:27m:55s remains)
INFO - root - 2019-11-06 19:57:34.889141: step 70020, total loss = 0.78, predict loss = 0.21 (64.4 examples/sec; 0.062 sec/batch; 1h:22m:46s remains)
INFO - root - 2019-11-06 19:57:35.422977: step 70030, total loss = 0.89, predict loss = 0.28 (92.0 examples/sec; 0.043 sec/batch; 0h:57m:58s remains)
INFO - root - 2019-11-06 19:57:35.882761: step 70040, total loss = 1.03, predict loss = 0.28 (96.7 examples/sec; 0.041 sec/batch; 0h:55m:07s remains)
INFO - root - 2019-11-06 19:57:37.003083: step 70050, total loss = 1.33, predict loss = 0.38 (68.9 examples/sec; 0.058 sec/batch; 1h:17m:21s remains)
INFO - root - 2019-11-06 19:57:37.714798: step 70060, total loss = 0.83, predict loss = 0.19 (56.1 examples/sec; 0.071 sec/batch; 1h:35m:00s remains)
INFO - root - 2019-11-06 19:57:38.471868: step 70070, total loss = 1.70, predict loss = 0.55 (62.1 examples/sec; 0.064 sec/batch; 1h:25m:51s remains)
INFO - root - 2019-11-06 19:57:39.231857: step 70080, total loss = 0.79, predict loss = 0.20 (58.3 examples/sec; 0.069 sec/batch; 1h:31m:27s remains)
INFO - root - 2019-11-06 19:57:39.963911: step 70090, total loss = 0.62, predict loss = 0.15 (57.0 examples/sec; 0.070 sec/batch; 1h:33m:25s remains)
INFO - root - 2019-11-06 19:57:40.593291: step 70100, total loss = 0.52, predict loss = 0.13 (97.8 examples/sec; 0.041 sec/batch; 0h:54m:28s remains)
INFO - root - 2019-11-06 19:57:41.036665: step 70110, total loss = 0.95, predict loss = 0.28 (95.3 examples/sec; 0.042 sec/batch; 0h:55m:54s remains)
INFO - root - 2019-11-06 19:57:41.481907: step 70120, total loss = 0.68, predict loss = 0.14 (97.3 examples/sec; 0.041 sec/batch; 0h:54m:42s remains)
INFO - root - 2019-11-06 19:57:42.746638: step 70130, total loss = 0.57, predict loss = 0.14 (62.2 examples/sec; 0.064 sec/batch; 1h:25m:39s remains)
INFO - root - 2019-11-06 19:57:43.520649: step 70140, total loss = 0.59, predict loss = 0.14 (52.2 examples/sec; 0.077 sec/batch; 1h:41m:57s remains)
INFO - root - 2019-11-06 19:57:44.357454: step 70150, total loss = 0.88, predict loss = 0.23 (55.3 examples/sec; 0.072 sec/batch; 1h:36m:19s remains)
INFO - root - 2019-11-06 19:57:45.087267: step 70160, total loss = 0.93, predict loss = 0.25 (63.0 examples/sec; 0.064 sec/batch; 1h:24m:30s remains)
INFO - root - 2019-11-06 19:57:45.747380: step 70170, total loss = 0.85, predict loss = 0.23 (75.9 examples/sec; 0.053 sec/batch; 1h:10m:07s remains)
INFO - root - 2019-11-06 19:57:46.278812: step 70180, total loss = 0.57, predict loss = 0.14 (94.5 examples/sec; 0.042 sec/batch; 0h:56m:19s remains)
INFO - root - 2019-11-06 19:57:46.739414: step 70190, total loss = 0.78, predict loss = 0.22 (94.3 examples/sec; 0.042 sec/batch; 0h:56m:23s remains)
INFO - root - 2019-11-06 19:57:47.909032: step 70200, total loss = 1.15, predict loss = 0.33 (71.9 examples/sec; 0.056 sec/batch; 1h:14m:01s remains)
INFO - root - 2019-11-06 19:57:48.612057: step 70210, total loss = 0.39, predict loss = 0.10 (53.9 examples/sec; 0.074 sec/batch; 1h:38m:39s remains)
INFO - root - 2019-11-06 19:57:49.360974: step 70220, total loss = 0.55, predict loss = 0.15 (58.8 examples/sec; 0.068 sec/batch; 1h:30m:30s remains)
INFO - root - 2019-11-06 19:57:50.137718: step 70230, total loss = 0.77, predict loss = 0.21 (56.1 examples/sec; 0.071 sec/batch; 1h:34m:50s remains)
INFO - root - 2019-11-06 19:57:50.846546: step 70240, total loss = 1.33, predict loss = 0.38 (70.7 examples/sec; 0.057 sec/batch; 1h:15m:14s remains)
INFO - root - 2019-11-06 19:57:51.455043: step 70250, total loss = 0.82, predict loss = 0.20 (103.8 examples/sec; 0.039 sec/batch; 0h:51m:12s remains)
INFO - root - 2019-11-06 19:57:51.928225: step 70260, total loss = 0.77, predict loss = 0.20 (95.2 examples/sec; 0.042 sec/batch; 0h:55m:49s remains)
INFO - root - 2019-11-06 19:57:52.369030: step 70270, total loss = 1.25, predict loss = 0.37 (100.5 examples/sec; 0.040 sec/batch; 0h:52m:54s remains)
INFO - root - 2019-11-06 19:57:53.711472: step 70280, total loss = 0.76, predict loss = 0.20 (60.6 examples/sec; 0.066 sec/batch; 1h:27m:38s remains)
INFO - root - 2019-11-06 19:57:54.443492: step 70290, total loss = 1.59, predict loss = 0.48 (64.7 examples/sec; 0.062 sec/batch; 1h:22m:08s remains)
INFO - root - 2019-11-06 19:57:55.171940: step 70300, total loss = 0.43, predict loss = 0.11 (56.3 examples/sec; 0.071 sec/batch; 1h:34m:26s remains)
INFO - root - 2019-11-06 19:57:55.897529: step 70310, total loss = 0.44, predict loss = 0.12 (63.1 examples/sec; 0.063 sec/batch; 1h:24m:14s remains)
INFO - root - 2019-11-06 19:57:56.651919: step 70320, total loss = 0.71, predict loss = 0.19 (64.6 examples/sec; 0.062 sec/batch; 1h:22m:14s remains)
INFO - root - 2019-11-06 19:57:57.164550: step 70330, total loss = 0.61, predict loss = 0.18 (97.5 examples/sec; 0.041 sec/batch; 0h:54m:29s remains)
INFO - root - 2019-11-06 19:57:57.638380: step 70340, total loss = 1.84, predict loss = 0.50 (101.0 examples/sec; 0.040 sec/batch; 0h:52m:33s remains)
INFO - root - 2019-11-06 19:57:58.770463: step 70350, total loss = 1.10, predict loss = 0.32 (70.0 examples/sec; 0.057 sec/batch; 1h:15m:49s remains)
INFO - root - 2019-11-06 19:57:59.462592: step 70360, total loss = 1.21, predict loss = 0.34 (56.0 examples/sec; 0.071 sec/batch; 1h:34m:52s remains)
INFO - root - 2019-11-06 19:58:00.251112: step 70370, total loss = 0.91, predict loss = 0.29 (59.4 examples/sec; 0.067 sec/batch; 1h:29m:23s remains)
INFO - root - 2019-11-06 19:58:00.992563: step 70380, total loss = 1.13, predict loss = 0.36 (56.5 examples/sec; 0.071 sec/batch; 1h:34m:01s remains)
INFO - root - 2019-11-06 19:58:01.708798: step 70390, total loss = 1.01, predict loss = 0.26 (60.9 examples/sec; 0.066 sec/batch; 1h:27m:05s remains)
INFO - root - 2019-11-06 19:58:02.291210: step 70400, total loss = 0.91, predict loss = 0.24 (94.5 examples/sec; 0.042 sec/batch; 0h:56m:09s remains)
INFO - root - 2019-11-06 19:58:02.747940: step 70410, total loss = 0.89, predict loss = 0.27 (95.7 examples/sec; 0.042 sec/batch; 0h:55m:25s remains)
INFO - root - 2019-11-06 19:58:03.221319: step 70420, total loss = 1.45, predict loss = 0.39 (95.1 examples/sec; 0.042 sec/batch; 0h:55m:46s remains)
INFO - root - 2019-11-06 19:58:04.575947: step 70430, total loss = 0.88, predict loss = 0.22 (51.1 examples/sec; 0.078 sec/batch; 1h:43m:52s remains)
INFO - root - 2019-11-06 19:58:05.288149: step 70440, total loss = 0.64, predict loss = 0.17 (64.9 examples/sec; 0.062 sec/batch; 1h:21m:42s remains)
INFO - root - 2019-11-06 19:58:05.995871: step 70450, total loss = 0.91, predict loss = 0.21 (62.0 examples/sec; 0.064 sec/batch; 1h:25m:28s remains)
INFO - root - 2019-11-06 19:58:06.724526: step 70460, total loss = 0.68, predict loss = 0.15 (63.3 examples/sec; 0.063 sec/batch; 1h:23m:48s remains)
INFO - root - 2019-11-06 19:58:07.402134: step 70470, total loss = 1.25, predict loss = 0.32 (68.4 examples/sec; 0.058 sec/batch; 1h:17m:30s remains)
INFO - root - 2019-11-06 19:58:07.862214: step 70480, total loss = 0.88, predict loss = 0.26 (104.6 examples/sec; 0.038 sec/batch; 0h:50m:40s remains)
INFO - root - 2019-11-06 19:58:08.310777: step 70490, total loss = 0.66, predict loss = 0.19 (98.2 examples/sec; 0.041 sec/batch; 0h:53m:59s remains)
INFO - root - 2019-11-06 19:58:09.523798: step 70500, total loss = 1.81, predict loss = 0.59 (67.6 examples/sec; 0.059 sec/batch; 1h:18m:26s remains)
INFO - root - 2019-11-06 19:58:10.299144: step 70510, total loss = 0.71, predict loss = 0.19 (58.1 examples/sec; 0.069 sec/batch; 1h:31m:15s remains)
INFO - root - 2019-11-06 19:58:11.038522: step 70520, total loss = 0.97, predict loss = 0.27 (55.7 examples/sec; 0.072 sec/batch; 1h:35m:09s remains)
INFO - root - 2019-11-06 19:58:11.776635: step 70530, total loss = 1.27, predict loss = 0.34 (59.1 examples/sec; 0.068 sec/batch; 1h:29m:34s remains)
INFO - root - 2019-11-06 19:58:12.512705: step 70540, total loss = 0.73, predict loss = 0.20 (59.1 examples/sec; 0.068 sec/batch; 1h:29m:40s remains)
INFO - root - 2019-11-06 19:58:13.082925: step 70550, total loss = 0.56, predict loss = 0.15 (101.3 examples/sec; 0.039 sec/batch; 0h:52m:17s remains)
INFO - root - 2019-11-06 19:58:13.515148: step 70560, total loss = 1.36, predict loss = 0.38 (97.2 examples/sec; 0.041 sec/batch; 0h:54m:30s remains)
INFO - root - 2019-11-06 19:58:13.972623: step 70570, total loss = 0.53, predict loss = 0.13 (113.7 examples/sec; 0.035 sec/batch; 0h:46m:34s remains)
INFO - root - 2019-11-06 19:58:15.349881: step 70580, total loss = 0.70, predict loss = 0.18 (68.0 examples/sec; 0.059 sec/batch; 1h:17m:51s remains)
INFO - root - 2019-11-06 19:58:16.079585: step 70590, total loss = 0.92, predict loss = 0.25 (59.8 examples/sec; 0.067 sec/batch; 1h:28m:27s remains)
INFO - root - 2019-11-06 19:58:16.843606: step 70600, total loss = 0.57, predict loss = 0.15 (55.4 examples/sec; 0.072 sec/batch; 1h:35m:33s remains)
INFO - root - 2019-11-06 19:58:17.624252: step 70610, total loss = 0.83, predict loss = 0.22 (63.3 examples/sec; 0.063 sec/batch; 1h:23m:39s remains)
INFO - root - 2019-11-06 19:58:18.318677: step 70620, total loss = 0.51, predict loss = 0.13 (75.2 examples/sec; 0.053 sec/batch; 1h:10m:21s remains)
INFO - root - 2019-11-06 19:58:18.778291: step 70630, total loss = 0.65, predict loss = 0.15 (90.4 examples/sec; 0.044 sec/batch; 0h:58m:30s remains)
INFO - root - 2019-11-06 19:58:19.220667: step 70640, total loss = 0.71, predict loss = 0.17 (96.0 examples/sec; 0.042 sec/batch; 0h:55m:07s remains)
INFO - root - 2019-11-06 19:58:20.446709: step 70650, total loss = 0.86, predict loss = 0.23 (61.9 examples/sec; 0.065 sec/batch; 1h:25m:25s remains)
INFO - root - 2019-11-06 19:58:21.143847: step 70660, total loss = 0.57, predict loss = 0.16 (63.0 examples/sec; 0.064 sec/batch; 1h:23m:58s remains)
INFO - root - 2019-11-06 19:58:21.869395: step 70670, total loss = 0.54, predict loss = 0.11 (55.8 examples/sec; 0.072 sec/batch; 1h:34m:47s remains)
INFO - root - 2019-11-06 19:58:22.573730: step 70680, total loss = 0.46, predict loss = 0.12 (61.4 examples/sec; 0.065 sec/batch; 1h:26m:09s remains)
INFO - root - 2019-11-06 19:58:23.300472: step 70690, total loss = 0.87, predict loss = 0.23 (65.9 examples/sec; 0.061 sec/batch; 1h:20m:16s remains)
INFO - root - 2019-11-06 19:58:23.907138: step 70700, total loss = 0.81, predict loss = 0.21 (93.8 examples/sec; 0.043 sec/batch; 0h:56m:22s remains)
INFO - root - 2019-11-06 19:58:24.359719: step 70710, total loss = 0.87, predict loss = 0.25 (89.0 examples/sec; 0.045 sec/batch; 0h:59m:24s remains)
INFO - root - 2019-11-06 19:58:25.462887: step 70720, total loss = 0.44, predict loss = 0.11 (5.6 examples/sec; 0.709 sec/batch; 15h:37m:24s remains)
INFO - root - 2019-11-06 19:58:26.165382: step 70730, total loss = 0.92, predict loss = 0.23 (52.6 examples/sec; 0.076 sec/batch; 1h:40m:32s remains)
INFO - root - 2019-11-06 19:58:26.917864: step 70740, total loss = 0.74, predict loss = 0.23 (59.3 examples/sec; 0.067 sec/batch; 1h:29m:03s remains)
INFO - root - 2019-11-06 19:58:27.706155: step 70750, total loss = 1.07, predict loss = 0.30 (55.1 examples/sec; 0.073 sec/batch; 1h:35m:52s remains)
INFO - root - 2019-11-06 19:58:28.444053: step 70760, total loss = 0.81, predict loss = 0.21 (57.3 examples/sec; 0.070 sec/batch; 1h:32m:11s remains)
INFO - root - 2019-11-06 19:58:29.157953: step 70770, total loss = 0.82, predict loss = 0.24 (79.5 examples/sec; 0.050 sec/batch; 1h:06m:25s remains)
INFO - root - 2019-11-06 19:58:29.629924: step 70780, total loss = 0.56, predict loss = 0.13 (94.3 examples/sec; 0.042 sec/batch; 0h:56m:00s remains)
INFO - root - 2019-11-06 19:58:30.079845: step 70790, total loss = 0.53, predict loss = 0.14 (97.2 examples/sec; 0.041 sec/batch; 0h:54m:18s remains)
INFO - root - 2019-11-06 19:58:31.317461: step 70800, total loss = 0.79, predict loss = 0.21 (51.0 examples/sec; 0.078 sec/batch; 1h:43m:33s remains)
INFO - root - 2019-11-06 19:58:32.041339: step 70810, total loss = 0.84, predict loss = 0.22 (57.3 examples/sec; 0.070 sec/batch; 1h:32m:05s remains)
INFO - root - 2019-11-06 19:58:32.834570: step 70820, total loss = 0.60, predict loss = 0.16 (62.4 examples/sec; 0.064 sec/batch; 1h:24m:36s remains)
INFO - root - 2019-11-06 19:58:33.553205: step 70830, total loss = 0.43, predict loss = 0.11 (58.6 examples/sec; 0.068 sec/batch; 1h:30m:01s remains)
INFO - root - 2019-11-06 19:58:34.231710: step 70840, total loss = 1.79, predict loss = 0.55 (62.8 examples/sec; 0.064 sec/batch; 1h:24m:05s remains)
INFO - root - 2019-11-06 19:58:34.760555: step 70850, total loss = 0.79, predict loss = 0.21 (99.9 examples/sec; 0.040 sec/batch; 0h:52m:50s remains)
INFO - root - 2019-11-06 19:58:35.240027: step 70860, total loss = 1.33, predict loss = 0.38 (97.9 examples/sec; 0.041 sec/batch; 0h:53m:54s remains)
INFO - root - 2019-11-06 19:58:36.396044: step 70870, total loss = 0.77, predict loss = 0.21 (70.9 examples/sec; 0.056 sec/batch; 1h:14m:26s remains)
INFO - root - 2019-11-06 19:58:37.100579: step 70880, total loss = 0.61, predict loss = 0.18 (54.8 examples/sec; 0.073 sec/batch; 1h:36m:10s remains)
INFO - root - 2019-11-06 19:58:37.825541: step 70890, total loss = 0.65, predict loss = 0.17 (58.8 examples/sec; 0.068 sec/batch; 1h:29m:37s remains)
INFO - root - 2019-11-06 19:58:38.580138: step 70900, total loss = 0.46, predict loss = 0.09 (57.1 examples/sec; 0.070 sec/batch; 1h:32m:25s remains)
INFO - root - 2019-11-06 19:58:39.355744: step 70910, total loss = 0.63, predict loss = 0.18 (54.4 examples/sec; 0.073 sec/batch; 1h:36m:51s remains)
INFO - root - 2019-11-06 19:58:40.041730: step 70920, total loss = 0.39, predict loss = 0.09 (96.4 examples/sec; 0.041 sec/batch; 0h:54m:40s remains)
INFO - root - 2019-11-06 19:58:40.492032: step 70930, total loss = 1.18, predict loss = 0.32 (96.4 examples/sec; 0.042 sec/batch; 0h:54m:42s remains)
INFO - root - 2019-11-06 19:58:40.954722: step 70940, total loss = 0.91, predict loss = 0.21 (94.8 examples/sec; 0.042 sec/batch; 0h:55m:35s remains)
INFO - root - 2019-11-06 19:58:42.177548: step 70950, total loss = 0.85, predict loss = 0.22 (65.4 examples/sec; 0.061 sec/batch; 1h:20m:38s remains)
INFO - root - 2019-11-06 19:58:42.985318: step 70960, total loss = 0.79, predict loss = 0.26 (44.0 examples/sec; 0.091 sec/batch; 1h:59m:42s remains)
INFO - root - 2019-11-06 19:58:43.727044: step 70970, total loss = 1.23, predict loss = 0.30 (59.2 examples/sec; 0.068 sec/batch; 1h:28m:55s remains)
INFO - root - 2019-11-06 19:58:44.419638: step 70980, total loss = 1.07, predict loss = 0.29 (63.3 examples/sec; 0.063 sec/batch; 1h:23m:12s remains)
INFO - root - 2019-11-06 19:58:45.150376: step 70990, total loss = 0.69, predict loss = 0.18 (70.5 examples/sec; 0.057 sec/batch; 1h:14m:45s remains)
INFO - root - 2019-11-06 19:58:45.631553: step 71000, total loss = 0.81, predict loss = 0.23 (101.4 examples/sec; 0.039 sec/batch; 0h:51m:54s remains)
INFO - root - 2019-11-06 19:58:46.076209: step 71010, total loss = 0.35, predict loss = 0.08 (96.7 examples/sec; 0.041 sec/batch; 0h:54m:28s remains)
INFO - root - 2019-11-06 19:58:47.280674: step 71020, total loss = 1.10, predict loss = 0.29 (70.1 examples/sec; 0.057 sec/batch; 1h:15m:04s remains)
INFO - root - 2019-11-06 19:58:48.029187: step 71030, total loss = 1.32, predict loss = 0.39 (57.0 examples/sec; 0.070 sec/batch; 1h:32m:23s remains)
INFO - root - 2019-11-06 19:58:48.806024: step 71040, total loss = 0.78, predict loss = 0.19 (53.9 examples/sec; 0.074 sec/batch; 1h:37m:43s remains)
INFO - root - 2019-11-06 19:58:49.508661: step 71050, total loss = 0.77, predict loss = 0.19 (65.4 examples/sec; 0.061 sec/batch; 1h:20m:27s remains)
INFO - root - 2019-11-06 19:58:50.231656: step 71060, total loss = 0.76, predict loss = 0.21 (55.2 examples/sec; 0.072 sec/batch; 1h:35m:17s remains)
INFO - root - 2019-11-06 19:58:50.864281: step 71070, total loss = 0.66, predict loss = 0.16 (100.8 examples/sec; 0.040 sec/batch; 0h:52m:13s remains)
INFO - root - 2019-11-06 19:58:51.299507: step 71080, total loss = 0.41, predict loss = 0.10 (100.5 examples/sec; 0.040 sec/batch; 0h:52m:20s remains)
INFO - root - 2019-11-06 19:58:51.735544: step 71090, total loss = 1.34, predict loss = 0.37 (94.2 examples/sec; 0.042 sec/batch; 0h:55m:51s remains)
INFO - root - 2019-11-06 19:58:53.126521: step 71100, total loss = 0.83, predict loss = 0.23 (48.4 examples/sec; 0.083 sec/batch; 1h:48m:36s remains)
INFO - root - 2019-11-06 19:58:53.821090: step 71110, total loss = 1.30, predict loss = 0.36 (57.8 examples/sec; 0.069 sec/batch; 1h:31m:02s remains)
INFO - root - 2019-11-06 19:58:54.542648: step 71120, total loss = 0.69, predict loss = 0.21 (64.5 examples/sec; 0.062 sec/batch; 1h:21m:34s remains)
INFO - root - 2019-11-06 19:58:55.219300: step 71130, total loss = 0.66, predict loss = 0.17 (59.4 examples/sec; 0.067 sec/batch; 1h:28m:27s remains)
INFO - root - 2019-11-06 19:58:55.932982: step 71140, total loss = 0.49, predict loss = 0.12 (63.0 examples/sec; 0.064 sec/batch; 1h:23m:29s remains)
INFO - root - 2019-11-06 19:58:56.423585: step 71150, total loss = 0.73, predict loss = 0.18 (100.1 examples/sec; 0.040 sec/batch; 0h:52m:31s remains)
INFO - root - 2019-11-06 19:58:56.884867: step 71160, total loss = 0.83, predict loss = 0.21 (99.1 examples/sec; 0.040 sec/batch; 0h:53m:00s remains)
INFO - root - 2019-11-06 19:58:58.203994: step 71170, total loss = 0.50, predict loss = 0.13 (62.1 examples/sec; 0.064 sec/batch; 1h:24m:35s remains)
INFO - root - 2019-11-06 19:58:58.991425: step 71180, total loss = 0.62, predict loss = 0.19 (65.7 examples/sec; 0.061 sec/batch; 1h:19m:58s remains)
INFO - root - 2019-11-06 19:58:59.701209: step 71190, total loss = 0.56, predict loss = 0.13 (66.0 examples/sec; 0.061 sec/batch; 1h:19m:35s remains)
INFO - root - 2019-11-06 19:59:00.405175: step 71200, total loss = 0.96, predict loss = 0.23 (68.9 examples/sec; 0.058 sec/batch; 1h:16m:14s remains)
INFO - root - 2019-11-06 19:59:01.114701: step 71210, total loss = 0.67, predict loss = 0.16 (64.4 examples/sec; 0.062 sec/batch; 1h:21m:31s remains)
INFO - root - 2019-11-06 19:59:01.692328: step 71220, total loss = 0.70, predict loss = 0.17 (102.2 examples/sec; 0.039 sec/batch; 0h:51m:23s remains)
INFO - root - 2019-11-06 19:59:02.136201: step 71230, total loss = 0.50, predict loss = 0.13 (90.0 examples/sec; 0.044 sec/batch; 0h:58m:20s remains)
INFO - root - 2019-11-06 19:59:02.586295: step 71240, total loss = 0.77, predict loss = 0.20 (92.7 examples/sec; 0.043 sec/batch; 0h:56m:38s remains)
INFO - root - 2019-11-06 19:59:03.930570: step 71250, total loss = 1.00, predict loss = 0.26 (54.4 examples/sec; 0.074 sec/batch; 1h:36m:32s remains)
INFO - root - 2019-11-06 19:59:04.683844: step 71260, total loss = 1.61, predict loss = 0.48 (63.9 examples/sec; 0.063 sec/batch; 1h:22m:09s remains)
INFO - root - 2019-11-06 19:59:05.403392: step 71270, total loss = 0.79, predict loss = 0.22 (58.8 examples/sec; 0.068 sec/batch; 1h:29m:16s remains)
INFO - root - 2019-11-06 19:59:06.151038: step 71280, total loss = 0.95, predict loss = 0.24 (67.8 examples/sec; 0.059 sec/batch; 1h:17m:25s remains)
INFO - root - 2019-11-06 19:59:06.785306: step 71290, total loss = 0.88, predict loss = 0.26 (69.7 examples/sec; 0.057 sec/batch; 1h:15m:18s remains)
INFO - root - 2019-11-06 19:59:07.295239: step 71300, total loss = 0.80, predict loss = 0.20 (92.1 examples/sec; 0.043 sec/batch; 0h:56m:59s remains)
INFO - root - 2019-11-06 19:59:07.741503: step 71310, total loss = 0.82, predict loss = 0.23 (98.9 examples/sec; 0.040 sec/batch; 0h:53m:03s remains)
INFO - root - 2019-11-06 19:59:08.933096: step 71320, total loss = 0.59, predict loss = 0.15 (65.3 examples/sec; 0.061 sec/batch; 1h:20m:20s remains)
INFO - root - 2019-11-06 19:59:09.658414: step 71330, total loss = 1.71, predict loss = 0.51 (60.5 examples/sec; 0.066 sec/batch; 1h:26m:38s remains)
INFO - root - 2019-11-06 19:59:10.383597: step 71340, total loss = 0.83, predict loss = 0.23 (66.2 examples/sec; 0.060 sec/batch; 1h:19m:13s remains)
INFO - root - 2019-11-06 19:59:11.112699: step 71350, total loss = 0.62, predict loss = 0.16 (50.3 examples/sec; 0.080 sec/batch; 1h:44m:17s remains)
INFO - root - 2019-11-06 19:59:11.896176: step 71360, total loss = 1.06, predict loss = 0.31 (62.6 examples/sec; 0.064 sec/batch; 1h:23m:42s remains)
INFO - root - 2019-11-06 19:59:12.451617: step 71370, total loss = 1.28, predict loss = 0.35 (99.0 examples/sec; 0.040 sec/batch; 0h:52m:55s remains)
INFO - root - 2019-11-06 19:59:12.914066: step 71380, total loss = 0.40, predict loss = 0.09 (98.4 examples/sec; 0.041 sec/batch; 0h:53m:15s remains)
INFO - root - 2019-11-06 19:59:13.356397: step 71390, total loss = 1.93, predict loss = 0.62 (119.1 examples/sec; 0.034 sec/batch; 0h:44m:00s remains)
INFO - root - 2019-11-06 19:59:14.763764: step 71400, total loss = 0.56, predict loss = 0.14 (59.1 examples/sec; 0.068 sec/batch; 1h:28m:40s remains)
INFO - root - 2019-11-06 19:59:15.498435: step 71410, total loss = 1.12, predict loss = 0.33 (52.8 examples/sec; 0.076 sec/batch; 1h:39m:16s remains)
INFO - root - 2019-11-06 19:59:16.275434: step 71420, total loss = 0.84, predict loss = 0.20 (56.3 examples/sec; 0.071 sec/batch; 1h:33m:01s remains)
INFO - root - 2019-11-06 19:59:16.958805: step 71430, total loss = 0.50, predict loss = 0.11 (57.6 examples/sec; 0.069 sec/batch; 1h:31m:00s remains)
INFO - root - 2019-11-06 19:59:17.613961: step 71440, total loss = 0.72, predict loss = 0.17 (77.2 examples/sec; 0.052 sec/batch; 1h:07m:52s remains)
INFO - root - 2019-11-06 19:59:18.061848: step 71450, total loss = 0.63, predict loss = 0.16 (98.4 examples/sec; 0.041 sec/batch; 0h:53m:12s remains)
INFO - root - 2019-11-06 19:59:18.587023: step 71460, total loss = 0.83, predict loss = 0.25 (89.0 examples/sec; 0.045 sec/batch; 0h:58m:48s remains)
INFO - root - 2019-11-06 19:59:19.860473: step 71470, total loss = 1.30, predict loss = 0.39 (54.2 examples/sec; 0.074 sec/batch; 1h:36m:35s remains)
INFO - root - 2019-11-06 19:59:20.645022: step 71480, total loss = 0.65, predict loss = 0.15 (48.4 examples/sec; 0.083 sec/batch; 1h:48m:09s remains)
INFO - root - 2019-11-06 19:59:21.479433: step 71490, total loss = 0.85, predict loss = 0.26 (47.5 examples/sec; 0.084 sec/batch; 1h:50m:11s remains)
INFO - root - 2019-11-06 19:59:22.396853: step 71500, total loss = 0.75, predict loss = 0.20 (49.2 examples/sec; 0.081 sec/batch; 1h:46m:17s remains)
INFO - root - 2019-11-06 19:59:23.196444: step 71510, total loss = 1.48, predict loss = 0.48 (65.5 examples/sec; 0.061 sec/batch; 1h:19m:50s remains)
INFO - root - 2019-11-06 19:59:23.711959: step 71520, total loss = 0.63, predict loss = 0.17 (100.7 examples/sec; 0.040 sec/batch; 0h:51m:55s remains)
INFO - root - 2019-11-06 19:59:24.185123: step 71530, total loss = 0.94, predict loss = 0.27 (89.3 examples/sec; 0.045 sec/batch; 0h:58m:32s remains)
INFO - root - 2019-11-06 19:59:25.325305: step 71540, total loss = 1.10, predict loss = 0.30 (5.5 examples/sec; 0.726 sec/batch; 15h:49m:25s remains)
INFO - root - 2019-11-06 19:59:26.004473: step 71550, total loss = 0.67, predict loss = 0.19 (58.1 examples/sec; 0.069 sec/batch; 1h:29m:57s remains)
INFO - root - 2019-11-06 19:59:26.785404: step 71560, total loss = 0.66, predict loss = 0.13 (58.0 examples/sec; 0.069 sec/batch; 1h:30m:11s remains)
INFO - root - 2019-11-06 19:59:27.511803: step 71570, total loss = 0.80, predict loss = 0.25 (64.5 examples/sec; 0.062 sec/batch; 1h:21m:04s remains)
INFO - root - 2019-11-06 19:59:28.254369: step 71580, total loss = 0.23, predict loss = 0.04 (65.0 examples/sec; 0.062 sec/batch; 1h:20m:23s remains)
INFO - root - 2019-11-06 19:59:28.904228: step 71590, total loss = 1.18, predict loss = 0.35 (89.9 examples/sec; 0.044 sec/batch; 0h:58m:08s remains)
INFO - root - 2019-11-06 19:59:29.341654: step 71600, total loss = 0.80, predict loss = 0.21 (94.5 examples/sec; 0.042 sec/batch; 0h:55m:18s remains)
INFO - root - 2019-11-06 19:59:29.805205: step 71610, total loss = 0.76, predict loss = 0.18 (76.8 examples/sec; 0.052 sec/batch; 1h:08m:02s remains)
INFO - root - 2019-11-06 19:59:31.061872: step 71620, total loss = 0.36, predict loss = 0.08 (61.0 examples/sec; 0.066 sec/batch; 1h:25m:37s remains)
INFO - root - 2019-11-06 19:59:31.783449: step 71630, total loss = 0.73, predict loss = 0.17 (62.8 examples/sec; 0.064 sec/batch; 1h:23m:09s remains)
INFO - root - 2019-11-06 19:59:32.534972: step 71640, total loss = 0.88, predict loss = 0.25 (55.4 examples/sec; 0.072 sec/batch; 1h:34m:20s remains)
INFO - root - 2019-11-06 19:59:33.240773: step 71650, total loss = 0.43, predict loss = 0.10 (65.7 examples/sec; 0.061 sec/batch; 1h:19m:28s remains)
INFO - root - 2019-11-06 19:59:33.956169: step 71660, total loss = 0.70, predict loss = 0.18 (64.8 examples/sec; 0.062 sec/batch; 1h:20m:36s remains)
INFO - root - 2019-11-06 19:59:34.473466: step 71670, total loss = 0.71, predict loss = 0.19 (90.2 examples/sec; 0.044 sec/batch; 0h:57m:52s remains)
INFO - root - 2019-11-06 19:59:34.985261: step 71680, total loss = 0.52, predict loss = 0.12 (91.5 examples/sec; 0.044 sec/batch; 0h:57m:02s remains)
INFO - root - 2019-11-06 19:59:36.147504: step 71690, total loss = 0.87, predict loss = 0.23 (70.0 examples/sec; 0.057 sec/batch; 1h:14m:36s remains)
INFO - root - 2019-11-06 19:59:36.892197: step 71700, total loss = 0.76, predict loss = 0.20 (55.5 examples/sec; 0.072 sec/batch; 1h:34m:05s remains)
INFO - root - 2019-11-06 19:59:37.595546: step 71710, total loss = 0.82, predict loss = 0.20 (64.2 examples/sec; 0.062 sec/batch; 1h:21m:19s remains)
INFO - root - 2019-11-06 19:59:38.326621: step 71720, total loss = 1.49, predict loss = 0.40 (57.3 examples/sec; 0.070 sec/batch; 1h:31m:01s remains)
INFO - root - 2019-11-06 19:59:39.054964: step 71730, total loss = 1.59, predict loss = 0.45 (67.2 examples/sec; 0.059 sec/batch; 1h:17m:36s remains)
INFO - root - 2019-11-06 19:59:39.723419: step 71740, total loss = 1.31, predict loss = 0.35 (86.4 examples/sec; 0.046 sec/batch; 1h:00m:22s remains)
INFO - root - 2019-11-06 19:59:40.157275: step 71750, total loss = 1.21, predict loss = 0.32 (100.0 examples/sec; 0.040 sec/batch; 0h:52m:10s remains)
INFO - root - 2019-11-06 19:59:40.608815: step 71760, total loss = 0.76, predict loss = 0.19 (95.3 examples/sec; 0.042 sec/batch; 0h:54m:43s remains)
INFO - root - 2019-11-06 19:59:41.900241: step 71770, total loss = 0.98, predict loss = 0.26 (63.7 examples/sec; 0.063 sec/batch; 1h:21m:52s remains)
INFO - root - 2019-11-06 19:59:42.649962: step 71780, total loss = 1.09, predict loss = 0.33 (61.6 examples/sec; 0.065 sec/batch; 1h:24m:39s remains)
INFO - root - 2019-11-06 19:59:43.359485: step 71790, total loss = 0.26, predict loss = 0.05 (61.6 examples/sec; 0.065 sec/batch; 1h:24m:41s remains)
INFO - root - 2019-11-06 19:59:44.125583: step 71800, total loss = 0.51, predict loss = 0.11 (52.6 examples/sec; 0.076 sec/batch; 1h:39m:01s remains)
INFO - root - 2019-11-06 19:59:44.825491: step 71810, total loss = 0.60, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 1h:20m:04s remains)
INFO - root - 2019-11-06 19:59:45.377491: step 71820, total loss = 0.69, predict loss = 0.19 (100.5 examples/sec; 0.040 sec/batch; 0h:51m:51s remains)
INFO - root - 2019-11-06 19:59:45.814961: step 71830, total loss = 1.07, predict loss = 0.29 (106.1 examples/sec; 0.038 sec/batch; 0h:49m:07s remains)
INFO - root - 2019-11-06 19:59:47.008084: step 71840, total loss = 0.48, predict loss = 0.13 (66.6 examples/sec; 0.060 sec/batch; 1h:18m:14s remains)
INFO - root - 2019-11-06 19:59:47.729913: step 71850, total loss = 1.17, predict loss = 0.31 (51.4 examples/sec; 0.078 sec/batch; 1h:41m:26s remains)
INFO - root - 2019-11-06 19:59:48.465709: step 71860, total loss = 0.43, predict loss = 0.09 (55.4 examples/sec; 0.072 sec/batch; 1h:34m:05s remains)
INFO - root - 2019-11-06 19:59:49.256197: step 71870, total loss = 0.60, predict loss = 0.15 (50.2 examples/sec; 0.080 sec/batch; 1h:43m:48s remains)
INFO - root - 2019-11-06 19:59:50.083521: step 71880, total loss = 0.95, predict loss = 0.24 (54.5 examples/sec; 0.073 sec/batch; 1h:35m:31s remains)
INFO - root - 2019-11-06 19:59:50.712762: step 71890, total loss = 1.01, predict loss = 0.28 (100.5 examples/sec; 0.040 sec/batch; 0h:51m:47s remains)
INFO - root - 2019-11-06 19:59:51.181087: step 71900, total loss = 0.41, predict loss = 0.11 (100.9 examples/sec; 0.040 sec/batch; 0h:51m:36s remains)
INFO - root - 2019-11-06 19:59:51.632228: step 71910, total loss = 0.52, predict loss = 0.13 (96.1 examples/sec; 0.042 sec/batch; 0h:54m:11s remains)
INFO - root - 2019-11-06 19:59:53.007485: step 71920, total loss = 0.65, predict loss = 0.18 (64.4 examples/sec; 0.062 sec/batch; 1h:20m:46s remains)
INFO - root - 2019-11-06 19:59:53.718279: step 71930, total loss = 0.97, predict loss = 0.28 (68.4 examples/sec; 0.059 sec/batch; 1h:16m:07s remains)
INFO - root - 2019-11-06 19:59:54.447020: step 71940, total loss = 0.89, predict loss = 0.24 (66.4 examples/sec; 0.060 sec/batch; 1h:18m:19s remains)
INFO - root - 2019-11-06 19:59:55.188475: step 71950, total loss = 0.59, predict loss = 0.14 (57.2 examples/sec; 0.070 sec/batch; 1h:30m:53s remains)
INFO - root - 2019-11-06 19:59:55.913504: step 71960, total loss = 0.57, predict loss = 0.15 (76.7 examples/sec; 0.052 sec/batch; 1h:07m:49s remains)
INFO - root - 2019-11-06 19:59:56.410673: step 71970, total loss = 0.56, predict loss = 0.17 (98.6 examples/sec; 0.041 sec/batch; 0h:52m:46s remains)
INFO - root - 2019-11-06 19:59:56.890531: step 71980, total loss = 0.92, predict loss = 0.25 (103.0 examples/sec; 0.039 sec/batch; 0h:50m:30s remains)
INFO - root - 2019-11-06 19:59:58.056157: step 71990, total loss = 0.40, predict loss = 0.10 (67.4 examples/sec; 0.059 sec/batch; 1h:17m:12s remains)
INFO - root - 2019-11-06 19:59:58.718626: step 72000, total loss = 0.80, predict loss = 0.22 (62.7 examples/sec; 0.064 sec/batch; 1h:22m:58s remains)
INFO - root - 2019-11-06 19:59:59.451024: step 72010, total loss = 0.91, predict loss = 0.27 (56.0 examples/sec; 0.071 sec/batch; 1h:32m:51s remains)
INFO - root - 2019-11-06 20:00:00.201059: step 72020, total loss = 0.91, predict loss = 0.25 (56.2 examples/sec; 0.071 sec/batch; 1h:32m:27s remains)
INFO - root - 2019-11-06 20:00:00.958660: step 72030, total loss = 1.19, predict loss = 0.31 (60.3 examples/sec; 0.066 sec/batch; 1h:26m:15s remains)
INFO - root - 2019-11-06 20:00:01.539647: step 72040, total loss = 0.50, predict loss = 0.13 (100.1 examples/sec; 0.040 sec/batch; 0h:51m:54s remains)
INFO - root - 2019-11-06 20:00:01.976548: step 72050, total loss = 1.13, predict loss = 0.30 (98.6 examples/sec; 0.041 sec/batch; 0h:52m:41s remains)
INFO - root - 2019-11-06 20:00:02.456727: step 72060, total loss = 1.20, predict loss = 0.38 (88.7 examples/sec; 0.045 sec/batch; 0h:58m:36s remains)
INFO - root - 2019-11-06 20:00:03.769452: step 72070, total loss = 0.44, predict loss = 0.10 (60.0 examples/sec; 0.067 sec/batch; 1h:26m:31s remains)
INFO - root - 2019-11-06 20:00:04.528055: step 72080, total loss = 0.34, predict loss = 0.07 (61.3 examples/sec; 0.065 sec/batch; 1h:24m:43s remains)
INFO - root - 2019-11-06 20:00:05.224128: step 72090, total loss = 0.92, predict loss = 0.19 (64.1 examples/sec; 0.062 sec/batch; 1h:21m:00s remains)
INFO - root - 2019-11-06 20:00:05.982816: step 72100, total loss = 0.91, predict loss = 0.25 (52.8 examples/sec; 0.076 sec/batch; 1h:38m:21s remains)
INFO - root - 2019-11-06 20:00:06.657639: step 72110, total loss = 0.61, predict loss = 0.15 (73.7 examples/sec; 0.054 sec/batch; 1h:10m:26s remains)
INFO - root - 2019-11-06 20:00:07.115892: step 72120, total loss = 0.83, predict loss = 0.21 (99.0 examples/sec; 0.040 sec/batch; 0h:52m:26s remains)
INFO - root - 2019-11-06 20:00:07.559552: step 72130, total loss = 1.17, predict loss = 0.30 (97.6 examples/sec; 0.041 sec/batch; 0h:53m:10s remains)
INFO - root - 2019-11-06 20:00:08.779898: step 72140, total loss = 0.52, predict loss = 0.14 (68.9 examples/sec; 0.058 sec/batch; 1h:15m:22s remains)
INFO - root - 2019-11-06 20:00:09.512787: step 72150, total loss = 0.86, predict loss = 0.25 (59.3 examples/sec; 0.068 sec/batch; 1h:27m:35s remains)
INFO - root - 2019-11-06 20:00:10.217808: step 72160, total loss = 0.42, predict loss = 0.09 (69.3 examples/sec; 0.058 sec/batch; 1h:14m:52s remains)
INFO - root - 2019-11-06 20:00:10.935178: step 72170, total loss = 1.23, predict loss = 0.37 (61.5 examples/sec; 0.065 sec/batch; 1h:24m:21s remains)
INFO - root - 2019-11-06 20:00:11.686739: step 72180, total loss = 0.56, predict loss = 0.15 (63.5 examples/sec; 0.063 sec/batch; 1h:21m:43s remains)
INFO - root - 2019-11-06 20:00:12.264981: step 72190, total loss = 1.42, predict loss = 0.43 (101.7 examples/sec; 0.039 sec/batch; 0h:51m:00s remains)
INFO - root - 2019-11-06 20:00:12.702851: step 72200, total loss = 1.46, predict loss = 0.47 (99.6 examples/sec; 0.040 sec/batch; 0h:52m:03s remains)
INFO - root - 2019-11-06 20:00:13.131510: step 72210, total loss = 0.27, predict loss = 0.07 (126.4 examples/sec; 0.032 sec/batch; 0h:41m:02s remains)
INFO - root - 2019-11-06 20:00:14.478782: step 72220, total loss = 0.57, predict loss = 0.13 (56.6 examples/sec; 0.071 sec/batch; 1h:31m:34s remains)
INFO - root - 2019-11-06 20:00:15.214205: step 72230, total loss = 0.64, predict loss = 0.16 (59.9 examples/sec; 0.067 sec/batch; 1h:26m:30s remains)
INFO - root - 2019-11-06 20:00:15.968317: step 72240, total loss = 1.34, predict loss = 0.40 (48.2 examples/sec; 0.083 sec/batch; 1h:47m:28s remains)
INFO - root - 2019-11-06 20:00:16.746497: step 72250, total loss = 0.89, predict loss = 0.25 (55.9 examples/sec; 0.071 sec/batch; 1h:32m:38s remains)
INFO - root - 2019-11-06 20:00:17.415230: step 72260, total loss = 1.18, predict loss = 0.33 (82.9 examples/sec; 0.048 sec/batch; 1h:02m:33s remains)
INFO - root - 2019-11-06 20:00:17.873052: step 72270, total loss = 0.55, predict loss = 0.14 (106.2 examples/sec; 0.038 sec/batch; 0h:48m:46s remains)
INFO - root - 2019-11-06 20:00:18.332156: step 72280, total loss = 0.83, predict loss = 0.22 (97.4 examples/sec; 0.041 sec/batch; 0h:53m:10s remains)
INFO - root - 2019-11-06 20:00:19.618989: step 72290, total loss = 0.97, predict loss = 0.29 (57.7 examples/sec; 0.069 sec/batch; 1h:29m:43s remains)
INFO - root - 2019-11-06 20:00:20.337952: step 72300, total loss = 0.38, predict loss = 0.09 (60.6 examples/sec; 0.066 sec/batch; 1h:25m:30s remains)
INFO - root - 2019-11-06 20:00:21.067542: step 72310, total loss = 0.71, predict loss = 0.19 (59.6 examples/sec; 0.067 sec/batch; 1h:26m:57s remains)
INFO - root - 2019-11-06 20:00:21.815591: step 72320, total loss = 0.74, predict loss = 0.17 (61.1 examples/sec; 0.066 sec/batch; 1h:24m:48s remains)
INFO - root - 2019-11-06 20:00:22.578021: step 72330, total loss = 1.03, predict loss = 0.27 (80.3 examples/sec; 0.050 sec/batch; 1h:04m:31s remains)
INFO - root - 2019-11-06 20:00:23.110906: step 72340, total loss = 0.41, predict loss = 0.11 (91.4 examples/sec; 0.044 sec/batch; 0h:56m:39s remains)
INFO - root - 2019-11-06 20:00:23.564541: step 72350, total loss = 0.68, predict loss = 0.17 (97.5 examples/sec; 0.041 sec/batch; 0h:53m:06s remains)
INFO - root - 2019-11-06 20:00:24.697543: step 72360, total loss = 0.61, predict loss = 0.15 (5.5 examples/sec; 0.733 sec/batch; 15h:49m:06s remains)
INFO - root - 2019-11-06 20:00:25.371172: step 72370, total loss = 0.69, predict loss = 0.20 (67.7 examples/sec; 0.059 sec/batch; 1h:16m:23s remains)
INFO - root - 2019-11-06 20:00:26.141122: step 72380, total loss = 1.61, predict loss = 0.49 (54.4 examples/sec; 0.074 sec/batch; 1h:35m:10s remains)
INFO - root - 2019-11-06 20:00:26.886991: step 72390, total loss = 0.76, predict loss = 0.20 (63.1 examples/sec; 0.063 sec/batch; 1h:21m:58s remains)
INFO - root - 2019-11-06 20:00:27.649700: step 72400, total loss = 1.31, predict loss = 0.42 (56.6 examples/sec; 0.071 sec/batch; 1h:31m:20s remains)
INFO - root - 2019-11-06 20:00:28.293992: step 72410, total loss = 1.36, predict loss = 0.41 (97.8 examples/sec; 0.041 sec/batch; 0h:52m:54s remains)
INFO - root - 2019-11-06 20:00:28.749940: step 72420, total loss = 0.69, predict loss = 0.16 (100.0 examples/sec; 0.040 sec/batch; 0h:51m:42s remains)
INFO - root - 2019-11-06 20:00:29.205341: step 72430, total loss = 0.61, predict loss = 0.16 (99.0 examples/sec; 0.040 sec/batch; 0h:52m:14s remains)
INFO - root - 2019-11-06 20:00:30.420164: step 72440, total loss = 1.22, predict loss = 0.36 (65.7 examples/sec; 0.061 sec/batch; 1h:18m:44s remains)
INFO - root - 2019-11-06 20:00:31.112656: step 72450, total loss = 0.74, predict loss = 0.16 (61.4 examples/sec; 0.065 sec/batch; 1h:24m:12s remains)
INFO - root - 2019-11-06 20:00:31.836794: step 72460, total loss = 0.74, predict loss = 0.19 (55.2 examples/sec; 0.072 sec/batch; 1h:33m:38s remains)
INFO - root - 2019-11-06 20:00:32.579432: step 72470, total loss = 1.07, predict loss = 0.30 (53.4 examples/sec; 0.075 sec/batch; 1h:36m:47s remains)
INFO - root - 2019-11-06 20:00:33.332821: step 72480, total loss = 0.95, predict loss = 0.21 (65.7 examples/sec; 0.061 sec/batch; 1h:18m:41s remains)
INFO - root - 2019-11-06 20:00:33.859792: step 72490, total loss = 0.94, predict loss = 0.29 (101.2 examples/sec; 0.040 sec/batch; 0h:51m:03s remains)
INFO - root - 2019-11-06 20:00:34.328866: step 72500, total loss = 0.68, predict loss = 0.17 (98.7 examples/sec; 0.041 sec/batch; 0h:52m:22s remains)
INFO - root - 2019-11-06 20:00:35.480600: step 72510, total loss = 0.46, predict loss = 0.11 (72.2 examples/sec; 0.055 sec/batch; 1h:11m:32s remains)
INFO - root - 2019-11-06 20:00:36.171392: step 72520, total loss = 1.61, predict loss = 0.51 (54.4 examples/sec; 0.073 sec/batch; 1h:34m:53s remains)
INFO - root - 2019-11-06 20:00:36.923309: step 72530, total loss = 0.77, predict loss = 0.19 (64.9 examples/sec; 0.062 sec/batch; 1h:19m:37s remains)
INFO - root - 2019-11-06 20:00:37.668676: step 72540, total loss = 0.84, predict loss = 0.21 (63.1 examples/sec; 0.063 sec/batch; 1h:21m:47s remains)
INFO - root - 2019-11-06 20:00:38.401968: step 72550, total loss = 0.53, predict loss = 0.13 (59.5 examples/sec; 0.067 sec/batch; 1h:26m:46s remains)
INFO - root - 2019-11-06 20:00:39.033027: step 72560, total loss = 0.77, predict loss = 0.18 (95.0 examples/sec; 0.042 sec/batch; 0h:54m:21s remains)
INFO - root - 2019-11-06 20:00:39.488704: step 72570, total loss = 1.13, predict loss = 0.31 (94.1 examples/sec; 0.042 sec/batch; 0h:54m:49s remains)
INFO - root - 2019-11-06 20:00:39.969837: step 72580, total loss = 0.62, predict loss = 0.16 (102.2 examples/sec; 0.039 sec/batch; 0h:50m:29s remains)
INFO - root - 2019-11-06 20:00:41.193079: step 72590, total loss = 0.99, predict loss = 0.29 (60.7 examples/sec; 0.066 sec/batch; 1h:24m:59s remains)
INFO - root - 2019-11-06 20:00:41.874425: step 72600, total loss = 0.63, predict loss = 0.15 (58.2 examples/sec; 0.069 sec/batch; 1h:28m:37s remains)
INFO - root - 2019-11-06 20:00:42.599724: step 72610, total loss = 1.13, predict loss = 0.30 (59.0 examples/sec; 0.068 sec/batch; 1h:27m:29s remains)
INFO - root - 2019-11-06 20:00:43.353314: step 72620, total loss = 0.56, predict loss = 0.14 (64.5 examples/sec; 0.062 sec/batch; 1h:20m:01s remains)
INFO - root - 2019-11-06 20:00:44.030084: step 72630, total loss = 1.05, predict loss = 0.35 (77.7 examples/sec; 0.051 sec/batch; 1h:06m:23s remains)
INFO - root - 2019-11-06 20:00:44.525484: step 72640, total loss = 0.73, predict loss = 0.21 (97.3 examples/sec; 0.041 sec/batch; 0h:53m:01s remains)
INFO - root - 2019-11-06 20:00:44.972917: step 72650, total loss = 1.45, predict loss = 0.41 (99.3 examples/sec; 0.040 sec/batch; 0h:51m:57s remains)
INFO - root - 2019-11-06 20:00:46.174500: step 72660, total loss = 0.45, predict loss = 0.10 (66.2 examples/sec; 0.060 sec/batch; 1h:17m:55s remains)
INFO - root - 2019-11-06 20:00:46.926393: step 72670, total loss = 0.94, predict loss = 0.27 (52.4 examples/sec; 0.076 sec/batch; 1h:38m:28s remains)
INFO - root - 2019-11-06 20:00:47.668771: step 72680, total loss = 0.66, predict loss = 0.16 (60.0 examples/sec; 0.067 sec/batch; 1h:25m:58s remains)
INFO - root - 2019-11-06 20:00:48.386050: step 72690, total loss = 0.68, predict loss = 0.17 (59.7 examples/sec; 0.067 sec/batch; 1h:26m:20s remains)
INFO - root - 2019-11-06 20:00:49.124412: step 72700, total loss = 1.06, predict loss = 0.30 (63.0 examples/sec; 0.064 sec/batch; 1h:21m:50s remains)
INFO - root - 2019-11-06 20:00:49.680800: step 72710, total loss = 0.78, predict loss = 0.20 (98.0 examples/sec; 0.041 sec/batch; 0h:52m:33s remains)
INFO - root - 2019-11-06 20:00:50.144548: step 72720, total loss = 1.65, predict loss = 0.52 (89.5 examples/sec; 0.045 sec/batch; 0h:57m:32s remains)
INFO - root - 2019-11-06 20:00:50.598968: step 72730, total loss = 0.51, predict loss = 0.13 (91.0 examples/sec; 0.044 sec/batch; 0h:56m:35s remains)
INFO - root - 2019-11-06 20:00:51.960611: step 72740, total loss = 1.35, predict loss = 0.42 (48.8 examples/sec; 0.082 sec/batch; 1h:45m:37s remains)
INFO - root - 2019-11-06 20:00:52.737580: step 72750, total loss = 0.68, predict loss = 0.18 (63.9 examples/sec; 0.063 sec/batch; 1h:20m:32s remains)
INFO - root - 2019-11-06 20:00:53.483182: step 72760, total loss = 0.86, predict loss = 0.24 (58.6 examples/sec; 0.068 sec/batch; 1h:27m:49s remains)
INFO - root - 2019-11-06 20:00:54.237587: step 72770, total loss = 0.37, predict loss = 0.08 (59.3 examples/sec; 0.068 sec/batch; 1h:26m:53s remains)
INFO - root - 2019-11-06 20:00:54.970359: step 72780, total loss = 0.42, predict loss = 0.09 (72.0 examples/sec; 0.056 sec/batch; 1h:11m:31s remains)
INFO - root - 2019-11-06 20:00:55.454535: step 72790, total loss = 0.81, predict loss = 0.21 (105.6 examples/sec; 0.038 sec/batch; 0h:48m:45s remains)
INFO - root - 2019-11-06 20:00:55.897022: step 72800, total loss = 0.60, predict loss = 0.15 (94.8 examples/sec; 0.042 sec/batch; 0h:54m:16s remains)
INFO - root - 2019-11-06 20:00:57.041635: step 72810, total loss = 0.71, predict loss = 0.18 (72.7 examples/sec; 0.055 sec/batch; 1h:10m:45s remains)
INFO - root - 2019-11-06 20:00:57.747178: step 72820, total loss = 0.50, predict loss = 0.12 (61.9 examples/sec; 0.065 sec/batch; 1h:23m:04s remains)
INFO - root - 2019-11-06 20:00:58.480962: step 72830, total loss = 0.78, predict loss = 0.22 (66.8 examples/sec; 0.060 sec/batch; 1h:17m:00s remains)
INFO - root - 2019-11-06 20:00:59.179221: step 72840, total loss = 0.69, predict loss = 0.19 (69.7 examples/sec; 0.057 sec/batch; 1h:13m:47s remains)
INFO - root - 2019-11-06 20:00:59.866947: step 72850, total loss = 1.09, predict loss = 0.28 (65.8 examples/sec; 0.061 sec/batch; 1h:18m:12s remains)
INFO - root - 2019-11-06 20:01:00.457979: step 72860, total loss = 1.00, predict loss = 0.26 (103.0 examples/sec; 0.039 sec/batch; 0h:49m:56s remains)
INFO - root - 2019-11-06 20:01:00.889012: step 72870, total loss = 1.08, predict loss = 0.30 (102.6 examples/sec; 0.039 sec/batch; 0h:50m:06s remains)
INFO - root - 2019-11-06 20:01:01.309294: step 72880, total loss = 0.81, predict loss = 0.22 (103.9 examples/sec; 0.038 sec/batch; 0h:49m:28s remains)
INFO - root - 2019-11-06 20:01:02.608572: step 72890, total loss = 1.05, predict loss = 0.31 (61.5 examples/sec; 0.065 sec/batch; 1h:23m:33s remains)
INFO - root - 2019-11-06 20:01:03.344746: step 72900, total loss = 1.36, predict loss = 0.40 (63.5 examples/sec; 0.063 sec/batch; 1h:20m:56s remains)
INFO - root - 2019-11-06 20:01:04.095487: step 72910, total loss = 1.08, predict loss = 0.33 (60.1 examples/sec; 0.067 sec/batch; 1h:25m:30s remains)
INFO - root - 2019-11-06 20:01:04.857688: step 72920, total loss = 1.12, predict loss = 0.32 (56.9 examples/sec; 0.070 sec/batch; 1h:30m:22s remains)
INFO - root - 2019-11-06 20:01:05.597390: step 72930, total loss = 0.44, predict loss = 0.11 (69.2 examples/sec; 0.058 sec/batch; 1h:14m:12s remains)
INFO - root - 2019-11-06 20:01:06.115105: step 72940, total loss = 0.87, predict loss = 0.26 (100.7 examples/sec; 0.040 sec/batch; 0h:51m:02s remains)
INFO - root - 2019-11-06 20:01:06.573126: step 72950, total loss = 0.91, predict loss = 0.27 (101.4 examples/sec; 0.039 sec/batch; 0h:50m:38s remains)
INFO - root - 2019-11-06 20:01:07.830354: step 72960, total loss = 0.97, predict loss = 0.25 (64.5 examples/sec; 0.062 sec/batch; 1h:19m:36s remains)
INFO - root - 2019-11-06 20:01:08.550970: step 72970, total loss = 0.59, predict loss = 0.15 (66.5 examples/sec; 0.060 sec/batch; 1h:17m:11s remains)
INFO - root - 2019-11-06 20:01:09.319853: step 72980, total loss = 0.24, predict loss = 0.05 (53.8 examples/sec; 0.074 sec/batch; 1h:35m:29s remains)
INFO - root - 2019-11-06 20:01:10.141089: step 72990, total loss = 0.70, predict loss = 0.19 (52.7 examples/sec; 0.076 sec/batch; 1h:37m:24s remains)
INFO - root - 2019-11-06 20:01:10.930602: step 73000, total loss = 0.86, predict loss = 0.26 (60.7 examples/sec; 0.066 sec/batch; 1h:24m:33s remains)
INFO - root - 2019-11-06 20:01:11.485908: step 73010, total loss = 0.69, predict loss = 0.17 (106.4 examples/sec; 0.038 sec/batch; 0h:48m:15s remains)
INFO - root - 2019-11-06 20:01:11.952174: step 73020, total loss = 0.73, predict loss = 0.20 (97.7 examples/sec; 0.041 sec/batch; 0h:52m:31s remains)
INFO - root - 2019-11-06 20:01:12.419353: step 73030, total loss = 0.53, predict loss = 0.12 (95.1 examples/sec; 0.042 sec/batch; 0h:53m:56s remains)
INFO - root - 2019-11-06 20:01:13.773937: step 73040, total loss = 0.40, predict loss = 0.10 (61.5 examples/sec; 0.065 sec/batch; 1h:23m:29s remains)
INFO - root - 2019-11-06 20:01:14.501160: step 73050, total loss = 0.89, predict loss = 0.24 (60.9 examples/sec; 0.066 sec/batch; 1h:24m:15s remains)
INFO - root - 2019-11-06 20:01:15.206997: step 73060, total loss = 1.11, predict loss = 0.33 (63.6 examples/sec; 0.063 sec/batch; 1h:20m:38s remains)
INFO - root - 2019-11-06 20:01:15.999093: step 73070, total loss = 0.90, predict loss = 0.25 (44.2 examples/sec; 0.091 sec/batch; 1h:56m:02s remains)
INFO - root - 2019-11-06 20:01:16.687739: step 73080, total loss = 0.78, predict loss = 0.22 (76.6 examples/sec; 0.052 sec/batch; 1h:06m:55s remains)
INFO - root - 2019-11-06 20:01:17.135818: step 73090, total loss = 0.81, predict loss = 0.23 (93.2 examples/sec; 0.043 sec/batch; 0h:55m:00s remains)
INFO - root - 2019-11-06 20:01:17.603274: step 73100, total loss = 1.44, predict loss = 0.43 (100.5 examples/sec; 0.040 sec/batch; 0h:50m:59s remains)
INFO - root - 2019-11-06 20:01:18.840714: step 73110, total loss = 0.82, predict loss = 0.22 (60.9 examples/sec; 0.066 sec/batch; 1h:24m:09s remains)
INFO - root - 2019-11-06 20:01:19.570345: step 73120, total loss = 0.99, predict loss = 0.24 (61.0 examples/sec; 0.066 sec/batch; 1h:24m:00s remains)
INFO - root - 2019-11-06 20:01:20.333166: step 73130, total loss = 0.75, predict loss = 0.19 (58.8 examples/sec; 0.068 sec/batch; 1h:27m:09s remains)
INFO - root - 2019-11-06 20:01:21.057484: step 73140, total loss = 0.45, predict loss = 0.10 (57.0 examples/sec; 0.070 sec/batch; 1h:29m:56s remains)
INFO - root - 2019-11-06 20:01:21.810129: step 73150, total loss = 0.92, predict loss = 0.19 (60.3 examples/sec; 0.066 sec/batch; 1h:24m:58s remains)
INFO - root - 2019-11-06 20:01:22.383265: step 73160, total loss = 1.08, predict loss = 0.31 (101.9 examples/sec; 0.039 sec/batch; 0h:50m:16s remains)
INFO - root - 2019-11-06 20:01:22.816229: step 73170, total loss = 0.83, predict loss = 0.22 (100.6 examples/sec; 0.040 sec/batch; 0h:50m:53s remains)
INFO - root - 2019-11-06 20:01:23.957666: step 73180, total loss = 0.97, predict loss = 0.28 (5.5 examples/sec; 0.727 sec/batch; 15h:31m:20s remains)
INFO - root - 2019-11-06 20:01:24.638049: step 73190, total loss = 0.77, predict loss = 0.19 (58.6 examples/sec; 0.068 sec/batch; 1h:27m:24s remains)
INFO - root - 2019-11-06 20:01:25.352763: step 73200, total loss = 1.16, predict loss = 0.29 (59.2 examples/sec; 0.068 sec/batch; 1h:26m:26s remains)
INFO - root - 2019-11-06 20:01:26.108765: step 73210, total loss = 0.48, predict loss = 0.12 (55.5 examples/sec; 0.072 sec/batch; 1h:32m:10s remains)
INFO - root - 2019-11-06 20:01:26.824547: step 73220, total loss = 1.95, predict loss = 0.66 (61.3 examples/sec; 0.065 sec/batch; 1h:23m:30s remains)
INFO - root - 2019-11-06 20:01:27.439420: step 73230, total loss = 0.76, predict loss = 0.21 (100.3 examples/sec; 0.040 sec/batch; 0h:51m:00s remains)
INFO - root - 2019-11-06 20:01:27.878700: step 73240, total loss = 0.66, predict loss = 0.17 (96.5 examples/sec; 0.041 sec/batch; 0h:53m:00s remains)
INFO - root - 2019-11-06 20:01:28.324068: step 73250, total loss = 0.85, predict loss = 0.23 (93.4 examples/sec; 0.043 sec/batch; 0h:54m:45s remains)
INFO - root - 2019-11-06 20:01:29.627589: step 73260, total loss = 0.83, predict loss = 0.22 (64.1 examples/sec; 0.062 sec/batch; 1h:19m:49s remains)
INFO - root - 2019-11-06 20:01:30.378750: step 73270, total loss = 1.54, predict loss = 0.52 (55.1 examples/sec; 0.073 sec/batch; 1h:32m:48s remains)
INFO - root - 2019-11-06 20:01:31.115376: step 73280, total loss = 0.36, predict loss = 0.07 (62.8 examples/sec; 0.064 sec/batch; 1h:21m:30s remains)
INFO - root - 2019-11-06 20:01:31.809651: step 73290, total loss = 0.57, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 1h:16m:13s remains)
INFO - root - 2019-11-06 20:01:32.529056: step 73300, total loss = 0.65, predict loss = 0.16 (65.1 examples/sec; 0.061 sec/batch; 1h:18m:29s remains)
INFO - root - 2019-11-06 20:01:33.053497: step 73310, total loss = 1.42, predict loss = 0.39 (94.7 examples/sec; 0.042 sec/batch; 0h:53m:58s remains)
INFO - root - 2019-11-06 20:01:33.503790: step 73320, total loss = 1.74, predict loss = 0.51 (96.3 examples/sec; 0.042 sec/batch; 0h:53m:05s remains)
INFO - root - 2019-11-06 20:01:34.695575: step 73330, total loss = 1.30, predict loss = 0.40 (66.2 examples/sec; 0.060 sec/batch; 1h:17m:11s remains)
INFO - root - 2019-11-06 20:01:35.422174: step 73340, total loss = 0.89, predict loss = 0.25 (58.4 examples/sec; 0.068 sec/batch; 1h:27m:31s remains)
INFO - root - 2019-11-06 20:01:36.141557: step 73350, total loss = 1.31, predict loss = 0.39 (55.3 examples/sec; 0.072 sec/batch; 1h:32m:27s remains)
INFO - root - 2019-11-06 20:01:36.927112: step 73360, total loss = 0.61, predict loss = 0.16 (52.6 examples/sec; 0.076 sec/batch; 1h:37m:08s remains)
INFO - root - 2019-11-06 20:01:37.747905: step 73370, total loss = 1.19, predict loss = 0.28 (54.7 examples/sec; 0.073 sec/batch; 1h:33m:20s remains)
INFO - root - 2019-11-06 20:01:38.403222: step 73380, total loss = 1.37, predict loss = 0.37 (95.3 examples/sec; 0.042 sec/batch; 0h:53m:36s remains)
INFO - root - 2019-11-06 20:01:38.836885: step 73390, total loss = 0.67, predict loss = 0.14 (101.0 examples/sec; 0.040 sec/batch; 0h:50m:35s remains)
INFO - root - 2019-11-06 20:01:39.278304: step 73400, total loss = 1.40, predict loss = 0.42 (97.5 examples/sec; 0.041 sec/batch; 0h:52m:23s remains)
INFO - root - 2019-11-06 20:01:40.563908: step 73410, total loss = 1.78, predict loss = 0.52 (55.8 examples/sec; 0.072 sec/batch; 1h:31m:31s remains)
INFO - root - 2019-11-06 20:01:41.376977: step 73420, total loss = 0.51, predict loss = 0.13 (54.3 examples/sec; 0.074 sec/batch; 1h:33m:59s remains)
INFO - root - 2019-11-06 20:01:42.092864: step 73430, total loss = 0.88, predict loss = 0.23 (60.2 examples/sec; 0.066 sec/batch; 1h:24m:47s remains)
INFO - root - 2019-11-06 20:01:42.799673: step 73440, total loss = 0.94, predict loss = 0.29 (63.6 examples/sec; 0.063 sec/batch; 1h:20m:16s remains)
INFO - root - 2019-11-06 20:01:43.483226: step 73450, total loss = 0.33, predict loss = 0.07 (70.0 examples/sec; 0.057 sec/batch; 1h:12m:56s remains)
INFO - root - 2019-11-06 20:01:44.011870: step 73460, total loss = 0.97, predict loss = 0.31 (101.8 examples/sec; 0.039 sec/batch; 0h:50m:07s remains)
INFO - root - 2019-11-06 20:01:44.457362: step 73470, total loss = 0.65, predict loss = 0.18 (102.1 examples/sec; 0.039 sec/batch; 0h:49m:57s remains)
INFO - root - 2019-11-06 20:01:45.637091: step 73480, total loss = 1.11, predict loss = 0.31 (69.7 examples/sec; 0.057 sec/batch; 1h:13m:14s remains)
INFO - root - 2019-11-06 20:01:46.312161: step 73490, total loss = 0.68, predict loss = 0.17 (53.1 examples/sec; 0.075 sec/batch; 1h:36m:01s remains)
INFO - root - 2019-11-06 20:01:47.088657: step 73500, total loss = 0.81, predict loss = 0.20 (67.7 examples/sec; 0.059 sec/batch; 1h:15m:23s remains)
INFO - root - 2019-11-06 20:01:47.820066: step 73510, total loss = 0.54, predict loss = 0.14 (63.0 examples/sec; 0.064 sec/batch; 1h:20m:59s remains)
INFO - root - 2019-11-06 20:01:48.586509: step 73520, total loss = 1.03, predict loss = 0.29 (50.3 examples/sec; 0.079 sec/batch; 1h:41m:16s remains)
INFO - root - 2019-11-06 20:01:49.232489: step 73530, total loss = 0.44, predict loss = 0.12 (96.9 examples/sec; 0.041 sec/batch; 0h:52m:38s remains)
INFO - root - 2019-11-06 20:01:49.696237: step 73540, total loss = 0.67, predict loss = 0.17 (97.6 examples/sec; 0.041 sec/batch; 0h:52m:13s remains)
INFO - root - 2019-11-06 20:01:50.132597: step 73550, total loss = 0.97, predict loss = 0.25 (103.8 examples/sec; 0.039 sec/batch; 0h:49m:06s remains)
INFO - root - 2019-11-06 20:01:51.412495: step 73560, total loss = 0.36, predict loss = 0.08 (67.7 examples/sec; 0.059 sec/batch; 1h:15m:17s remains)
INFO - root - 2019-11-06 20:01:52.140943: step 73570, total loss = 0.65, predict loss = 0.17 (58.1 examples/sec; 0.069 sec/batch; 1h:27m:43s remains)
INFO - root - 2019-11-06 20:01:52.896835: step 73580, total loss = 1.10, predict loss = 0.32 (69.2 examples/sec; 0.058 sec/batch; 1h:13m:37s remains)
INFO - root - 2019-11-06 20:01:53.628391: step 73590, total loss = 0.41, predict loss = 0.09 (61.8 examples/sec; 0.065 sec/batch; 1h:22m:28s remains)
INFO - root - 2019-11-06 20:01:54.308173: step 73600, total loss = 1.02, predict loss = 0.26 (72.7 examples/sec; 0.055 sec/batch; 1h:10m:04s remains)
INFO - root - 2019-11-06 20:01:54.790536: step 73610, total loss = 1.61, predict loss = 0.44 (92.6 examples/sec; 0.043 sec/batch; 0h:54m:58s remains)
INFO - root - 2019-11-06 20:01:55.274837: step 73620, total loss = 0.90, predict loss = 0.25 (97.5 examples/sec; 0.041 sec/batch; 0h:52m:12s remains)
INFO - root - 2019-11-06 20:01:56.459633: step 73630, total loss = 0.85, predict loss = 0.23 (73.2 examples/sec; 0.055 sec/batch; 1h:09m:33s remains)
INFO - root - 2019-11-06 20:01:57.294406: step 73640, total loss = 0.47, predict loss = 0.13 (51.0 examples/sec; 0.078 sec/batch; 1h:39m:49s remains)
INFO - root - 2019-11-06 20:01:58.026754: step 73650, total loss = 0.52, predict loss = 0.15 (57.9 examples/sec; 0.069 sec/batch; 1h:27m:56s remains)
INFO - root - 2019-11-06 20:01:58.792408: step 73660, total loss = 0.73, predict loss = 0.17 (61.1 examples/sec; 0.066 sec/batch; 1h:23m:21s remains)
INFO - root - 2019-11-06 20:01:59.548102: step 73670, total loss = 0.54, predict loss = 0.13 (59.7 examples/sec; 0.067 sec/batch; 1h:25m:15s remains)
INFO - root - 2019-11-06 20:02:00.124155: step 73680, total loss = 0.65, predict loss = 0.14 (99.0 examples/sec; 0.040 sec/batch; 0h:51m:22s remains)
INFO - root - 2019-11-06 20:02:00.570066: step 73690, total loss = 0.56, predict loss = 0.14 (99.2 examples/sec; 0.040 sec/batch; 0h:51m:18s remains)
INFO - root - 2019-11-06 20:02:01.035300: step 73700, total loss = 0.83, predict loss = 0.22 (100.2 examples/sec; 0.040 sec/batch; 0h:50m:45s remains)
INFO - root - 2019-11-06 20:02:02.391179: step 73710, total loss = 0.85, predict loss = 0.25 (61.6 examples/sec; 0.065 sec/batch; 1h:22m:31s remains)
INFO - root - 2019-11-06 20:02:03.110735: step 73720, total loss = 0.68, predict loss = 0.16 (71.0 examples/sec; 0.056 sec/batch; 1h:11m:39s remains)
INFO - root - 2019-11-06 20:02:03.805750: step 73730, total loss = 1.26, predict loss = 0.35 (66.5 examples/sec; 0.060 sec/batch; 1h:16m:29s remains)
INFO - root - 2019-11-06 20:02:04.520632: step 73740, total loss = 0.79, predict loss = 0.22 (59.5 examples/sec; 0.067 sec/batch; 1h:25m:23s remains)
INFO - root - 2019-11-06 20:02:05.174784: step 73750, total loss = 0.57, predict loss = 0.13 (71.7 examples/sec; 0.056 sec/batch; 1h:10m:54s remains)
INFO - root - 2019-11-06 20:02:05.623121: step 73760, total loss = 0.38, predict loss = 0.08 (95.6 examples/sec; 0.042 sec/batch; 0h:53m:09s remains)
INFO - root - 2019-11-06 20:02:06.058124: step 73770, total loss = 1.56, predict loss = 0.49 (87.3 examples/sec; 0.046 sec/batch; 0h:58m:14s remains)
INFO - root - 2019-11-06 20:02:07.287489: step 73780, total loss = 0.68, predict loss = 0.17 (70.5 examples/sec; 0.057 sec/batch; 1h:12m:02s remains)
INFO - root - 2019-11-06 20:02:07.992083: step 73790, total loss = 0.75, predict loss = 0.23 (57.5 examples/sec; 0.070 sec/batch; 1h:28m:19s remains)
INFO - root - 2019-11-06 20:02:08.675035: step 73800, total loss = 0.72, predict loss = 0.19 (65.4 examples/sec; 0.061 sec/batch; 1h:17m:40s remains)
INFO - root - 2019-11-06 20:02:09.402726: step 73810, total loss = 0.90, predict loss = 0.25 (52.3 examples/sec; 0.077 sec/batch; 1h:37m:10s remains)
INFO - root - 2019-11-06 20:02:10.127435: step 73820, total loss = 1.07, predict loss = 0.28 (66.8 examples/sec; 0.060 sec/batch; 1h:16m:03s remains)
INFO - root - 2019-11-06 20:02:10.659530: step 73830, total loss = 0.69, predict loss = 0.17 (102.1 examples/sec; 0.039 sec/batch; 0h:49m:44s remains)
INFO - root - 2019-11-06 20:02:11.096314: step 73840, total loss = 2.02, predict loss = 0.62 (96.4 examples/sec; 0.042 sec/batch; 0h:52m:41s remains)
INFO - root - 2019-11-06 20:02:11.528900: step 73850, total loss = 0.36, predict loss = 0.07 (126.7 examples/sec; 0.032 sec/batch; 0h:40m:04s remains)
INFO - root - 2019-11-06 20:02:12.875601: step 73860, total loss = 1.41, predict loss = 0.38 (55.2 examples/sec; 0.072 sec/batch; 1h:31m:57s remains)
INFO - root - 2019-11-06 20:02:13.583132: step 73870, total loss = 0.58, predict loss = 0.19 (64.4 examples/sec; 0.062 sec/batch; 1h:18m:45s remains)
INFO - root - 2019-11-06 20:02:14.306617: step 73880, total loss = 0.70, predict loss = 0.19 (60.4 examples/sec; 0.066 sec/batch; 1h:23m:57s remains)
INFO - root - 2019-11-06 20:02:15.035891: step 73890, total loss = 0.52, predict loss = 0.12 (60.8 examples/sec; 0.066 sec/batch; 1h:23m:27s remains)
INFO - root - 2019-11-06 20:02:15.694591: step 73900, total loss = 1.25, predict loss = 0.31 (91.3 examples/sec; 0.044 sec/batch; 0h:55m:32s remains)
INFO - root - 2019-11-06 20:02:16.139809: step 73910, total loss = 0.69, predict loss = 0.17 (97.2 examples/sec; 0.041 sec/batch; 0h:52m:11s remains)
INFO - root - 2019-11-06 20:02:16.591701: step 73920, total loss = 0.78, predict loss = 0.17 (97.0 examples/sec; 0.041 sec/batch; 0h:52m:18s remains)
INFO - root - 2019-11-06 20:02:17.864776: step 73930, total loss = 0.97, predict loss = 0.24 (75.1 examples/sec; 0.053 sec/batch; 1h:07m:32s remains)
INFO - root - 2019-11-06 20:02:18.590709: step 73940, total loss = 0.75, predict loss = 0.18 (59.8 examples/sec; 0.067 sec/batch; 1h:24m:44s remains)
INFO - root - 2019-11-06 20:02:19.415355: step 73950, total loss = 0.97, predict loss = 0.28 (61.7 examples/sec; 0.065 sec/batch; 1h:22m:06s remains)
INFO - root - 2019-11-06 20:02:20.260164: step 73960, total loss = 0.96, predict loss = 0.27 (57.9 examples/sec; 0.069 sec/batch; 1h:27m:31s remains)
INFO - root - 2019-11-06 20:02:21.002062: step 73970, total loss = 0.46, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 1h:11m:18s remains)
INFO - root - 2019-11-06 20:02:21.546092: step 73980, total loss = 0.94, predict loss = 0.26 (101.9 examples/sec; 0.039 sec/batch; 0h:49m:42s remains)
INFO - root - 2019-11-06 20:02:21.983648: step 73990, total loss = 0.93, predict loss = 0.25 (101.5 examples/sec; 0.039 sec/batch; 0h:49m:55s remains)
INFO - root - 2019-11-06 20:02:23.125169: step 74000, total loss = 0.77, predict loss = 0.22 (5.5 examples/sec; 0.727 sec/batch; 15h:21m:26s remains)
INFO - root - 2019-11-06 20:02:23.814315: step 74010, total loss = 0.84, predict loss = 0.22 (62.0 examples/sec; 0.064 sec/batch; 1h:21m:40s remains)
INFO - root - 2019-11-06 20:02:24.532654: step 74020, total loss = 1.03, predict loss = 0.27 (64.9 examples/sec; 0.062 sec/batch; 1h:18m:01s remains)
INFO - root - 2019-11-06 20:02:25.269450: step 74030, total loss = 0.91, predict loss = 0.25 (63.8 examples/sec; 0.063 sec/batch; 1h:19m:23s remains)
INFO - root - 2019-11-06 20:02:26.019182: step 74040, total loss = 1.25, predict loss = 0.37 (55.3 examples/sec; 0.072 sec/batch; 1h:31m:31s remains)
INFO - root - 2019-11-06 20:02:26.699136: step 74050, total loss = 0.73, predict loss = 0.18 (88.9 examples/sec; 0.045 sec/batch; 0h:56m:58s remains)
INFO - root - 2019-11-06 20:02:27.184195: step 74060, total loss = 0.42, predict loss = 0.08 (100.4 examples/sec; 0.040 sec/batch; 0h:50m:25s remains)
INFO - root - 2019-11-06 20:02:27.632524: step 74070, total loss = 1.14, predict loss = 0.33 (101.5 examples/sec; 0.039 sec/batch; 0h:49m:53s remains)
INFO - root - 2019-11-06 20:02:28.867026: step 74080, total loss = 0.81, predict loss = 0.21 (71.8 examples/sec; 0.056 sec/batch; 1h:10m:31s remains)
INFO - root - 2019-11-06 20:02:29.599693: step 74090, total loss = 0.44, predict loss = 0.13 (54.7 examples/sec; 0.073 sec/batch; 1h:32m:35s remains)
INFO - root - 2019-11-06 20:02:30.303802: step 74100, total loss = 0.55, predict loss = 0.14 (62.5 examples/sec; 0.064 sec/batch; 1h:20m:55s remains)
INFO - root - 2019-11-06 20:02:31.031139: step 74110, total loss = 0.60, predict loss = 0.14 (55.9 examples/sec; 0.072 sec/batch; 1h:30m:34s remains)
INFO - root - 2019-11-06 20:02:31.703512: step 74120, total loss = 1.00, predict loss = 0.27 (74.1 examples/sec; 0.054 sec/batch; 1h:08m:16s remains)
INFO - root - 2019-11-06 20:02:32.212069: step 74130, total loss = 0.54, predict loss = 0.12 (96.5 examples/sec; 0.041 sec/batch; 0h:52m:26s remains)
INFO - root - 2019-11-06 20:02:32.678009: step 74140, total loss = 1.18, predict loss = 0.38 (90.5 examples/sec; 0.044 sec/batch; 0h:55m:51s remains)
INFO - root - 2019-11-06 20:02:33.793068: step 74150, total loss = 0.91, predict loss = 0.22 (71.0 examples/sec; 0.056 sec/batch; 1h:11m:11s remains)
INFO - root - 2019-11-06 20:02:34.479858: step 74160, total loss = 0.67, predict loss = 0.17 (64.1 examples/sec; 0.062 sec/batch; 1h:18m:51s remains)
INFO - root - 2019-11-06 20:02:35.210111: step 74170, total loss = 0.94, predict loss = 0.23 (57.9 examples/sec; 0.069 sec/batch; 1h:27m:21s remains)
INFO - root - 2019-11-06 20:02:35.961558: step 74180, total loss = 0.88, predict loss = 0.24 (55.6 examples/sec; 0.072 sec/batch; 1h:30m:52s remains)
INFO - root - 2019-11-06 20:02:36.690088: step 74190, total loss = 0.48, predict loss = 0.14 (58.0 examples/sec; 0.069 sec/batch; 1h:27m:11s remains)
INFO - root - 2019-11-06 20:02:37.320654: step 74200, total loss = 0.90, predict loss = 0.19 (96.5 examples/sec; 0.041 sec/batch; 0h:52m:21s remains)
INFO - root - 2019-11-06 20:02:37.761794: step 74210, total loss = 0.85, predict loss = 0.21 (101.5 examples/sec; 0.039 sec/batch; 0h:49m:47s remains)
INFO - root - 2019-11-06 20:02:38.240875: step 74220, total loss = 0.71, predict loss = 0.19 (99.5 examples/sec; 0.040 sec/batch; 0h:50m:47s remains)
INFO - root - 2019-11-06 20:02:39.535108: step 74230, total loss = 1.61, predict loss = 0.51 (64.8 examples/sec; 0.062 sec/batch; 1h:17m:57s remains)
INFO - root - 2019-11-06 20:02:40.296032: step 74240, total loss = 0.37, predict loss = 0.15 (53.9 examples/sec; 0.074 sec/batch; 1h:33m:46s remains)
INFO - root - 2019-11-06 20:02:41.041109: step 74250, total loss = 0.98, predict loss = 0.25 (55.9 examples/sec; 0.072 sec/batch; 1h:30m:16s remains)
INFO - root - 2019-11-06 20:02:41.768056: step 74260, total loss = 0.46, predict loss = 0.11 (67.2 examples/sec; 0.060 sec/batch; 1h:15m:09s remains)
INFO - root - 2019-11-06 20:02:42.437326: step 74270, total loss = 1.19, predict loss = 0.34 (74.5 examples/sec; 0.054 sec/batch; 1h:07m:46s remains)
