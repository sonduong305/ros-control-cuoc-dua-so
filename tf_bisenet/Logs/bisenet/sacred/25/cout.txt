INFO - bisenet-v2 - Running command 'main'
INFO - bisenet-v2 - Started run with ID "25"
INFO - root - nvidia-ml-py is not installed, automatically select gpu is disabled!
WARNING:tensorflow:From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - tensorflow - From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:88: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:88: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:100: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:100: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
INFO - root - preproces -- augment
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:108: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:108: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:185: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map__image_mirroring, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(_image_mirroring, num_parallel_calls=threads)
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:119: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:119: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:122: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:122: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:123: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:123: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:187: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map__image_scaling, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(_image_scaling, num_parallel_calls=threads)
/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:190: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map_<lambda>, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(lambda image, label: _apply_with_random_selector(image, lambda x, ordering: _distort_color
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:201: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:201: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:157: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:157: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1aee36d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1aee36d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1aee36d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1aee36d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae9d9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae9d9e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae9d9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae9d9e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae4c588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae4c588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae4c588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae4c588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae7aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae7aa90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae7aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae7aa90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae4c940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae4c940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae4c940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae4c940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4cb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4cb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4cb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae4cb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f4d1ae4c588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f4d1ae4c588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f4d1ae4c588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f4d1ae4c588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ab4def0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ab4def0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ab4def0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ab4def0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae02b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae02b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae02b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae02b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ab16d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ab16d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ab16d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ab16d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae09e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae09e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae09e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae09e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1aba0a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1aba0a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1aba0a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1aba0a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae0710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae0710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae0710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae0710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1aa9e400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1aa9e400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1aa9e400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1aa9e400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae0fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae0fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae0fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1aae0fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1abfee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1abfee10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1abfee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1abfee10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a916128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a916128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a916128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a916128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1aee3978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1aee3978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1aee3978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1aee3978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a9c87b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a9c87b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a9c87b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a9c87b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a9160f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a9160f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a9160f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a9160f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a83aef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a83aef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a83aef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a83aef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a928b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a928b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a928b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a928b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a8b14e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a8b14e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a8b14e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a8b14e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a84f940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a84f940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a84f940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a84f940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a7889e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a7889e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a7889e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a7889e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a83a828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a83a828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a83a828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a83a828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a72bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a72bd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a72bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a72bd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a83aeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a83aeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a83aeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a83aeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a7a3550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a7a3550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a7a3550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a7a3550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ac49c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ac49c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ac49c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ac49c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a5b9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a5b9e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a5b9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a5b9e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a8d0470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a8d0470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a8d0470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a8d0470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a53a2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a53a2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a53a2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a53a2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a6a1f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a6a1f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a6a1f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a6a1f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a46f908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a46f908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a46f908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a46f908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a6a1f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a6a1f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a6a1f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a6a1f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a46f2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a46f2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a46f2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a46f2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ac99b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ac99b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ac99b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1ac99b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae99b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae99b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae99b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1ae99b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a581cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a581cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a581cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a581cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a384400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a384400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a384400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a384400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a42a630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a42a630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a42a630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a42a630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a287208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a287208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a287208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a287208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a2edc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a2edc18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a2edc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a2edc18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a287080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a287080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a287080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a287080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a2ed9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a2ed9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a2ed9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a2ed9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a1a6e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a1a6e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a1a6e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a1a6e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a23df60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a23df60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a23df60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a23df60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a25b2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a25b2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a25b2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a25b2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a25bac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a25bac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a25bac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a25bac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a1977b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a1977b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a1977b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a1977b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a09a780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a09a780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a09a780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a09a780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a1efa58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a1efa58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a1efa58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1a1efa58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a2d6c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a2d6c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a2d6c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a2d6c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ff0b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ff0b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ff0b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ff0b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19fbcef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19fbcef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19fbcef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19fbcef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ff0c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ff0c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ff0c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ff0c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19f41400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19f41400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19f41400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19f41400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19eabd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19eabd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19eabd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19eabd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a0fa780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a0fa780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a0fa780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a0fa780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19edf710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19edf710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19edf710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19edf710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a1c5dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a1c5dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a1c5dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1a1c5dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ee9518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ee9518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ee9518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ee9518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19e66ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19e66ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19e66ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19e66ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ff02b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ff02b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ff02b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ff02b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19e66240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19e66240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19e66240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19e66240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19e32748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19e32748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19e32748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19e32748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19ff0240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19ff0240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19ff0240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19ff0240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19cd7940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19cd7940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19cd7940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19cd7940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19ce9f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19ce9f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19ce9f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19ce9f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19cd70f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19cd70f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19cd70f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19cd70f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19db7b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19db7b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19db7b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19db7b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1abf71d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1abf71d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1abf71d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1abf71d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19bb0da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19bb0da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19bb0da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19bb0da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19aaccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19aaccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19aaccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19aaccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19ac4d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19ac4d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19ac4d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19ac4d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19c51320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19c51320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19c51320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19c51320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d199efe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d199efe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d199efe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d199efe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ae25c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ae25c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ae25c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19ae25c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19bf7eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19bf7eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19bf7eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19bf7eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19a26908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19a26908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19a26908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19a26908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1997db38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1997db38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1997db38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1997db38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19cd72e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19cd72e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19cd72e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19cd72e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d199bf828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d199bf828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d199bf828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d199bf828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19991be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19991be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19991be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19991be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19900ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19900ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19900ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19900ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19806320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19806320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19806320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19806320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19b472e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19b472e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19b472e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19b472e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19806eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19806eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19806eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19806eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d198362b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d198362b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d198362b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d198362b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19b472e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19b472e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19b472e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19b472e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1983a4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1983a4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1983a4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1983a4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d198060b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d198060b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d198060b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d198060b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19712780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19712780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19712780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19712780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d199bf710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d199bf710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d199bf710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d199bf710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d196aba20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d196aba20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d196aba20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d196aba20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19623668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19623668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19623668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19623668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d195eac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d195eac50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d195eac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d195eac50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19603d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19603d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19603d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19603d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d195eac88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d195eac88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d195eac88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d195eac88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1956b4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1956b4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1956b4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1956b4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19835f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19835f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19835f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19835f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d195179e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d195179e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d195179e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d195179e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d193689e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d193689e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d193689e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d193689e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d195ba470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d195ba470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d195ba470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d195ba470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19712b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19712b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19712b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d19712b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19384fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19384fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19384fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19384fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d195ba470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d195ba470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d195ba470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d195ba470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d194dcdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d194dcdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d194dcdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d194dcdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae4c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae4c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae4c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1ae4c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1925d160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1925d160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1925d160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1925d160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:179: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:179: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d193a3a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d193a3a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d193a3a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d193a3a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d191f8dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d191f8dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d191f8dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d191f8dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d196037b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d196037b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d196037b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d196037b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19a70208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19a70208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19a70208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19a70208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1a423ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1a423ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1a423ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1a423ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19aac7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19aac7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19aac7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d19aac7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d191f8278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d191f8278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d191f8278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d191f8278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d190cff28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d190cff28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d190cff28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d190cff28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d190346a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d190346a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d190346a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d190346a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18fee390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18fee390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18fee390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18fee390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18fc8da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18fc8da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18fc8da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18fc8da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18ffea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18ffea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18ffea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18ffea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18ffeb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18ffeb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18ffeb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18ffeb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18fc8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18fc8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18fc8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18fc8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18fc8cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18fc8cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18fc8cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18fc8cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18f95ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18f95ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18f95ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18f95ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18f28668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18f28668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18f28668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18f28668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18eee978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18eee978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18eee978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18eee978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18fc8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18fc8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18fc8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18fc8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18ecadd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18ecadd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18ecadd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18ecadd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18ecaef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18ecaef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18ecaef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18ecaef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18e309e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18e309e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18e309e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18e309e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18da7198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18da7198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18da7198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18da7198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18daab70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18daab70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18daab70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18daab70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18daaf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18daaf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18daaf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18daaf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18d94828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18d94828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18d94828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18d94828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18eee828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18eee828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18eee828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18eee828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:217: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:217: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:221: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:221: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:224: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:224: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:229: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:229: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:240: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:240: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING - root - random_scale is not explicitly specified, using default value: False
WARNING - root - random_mirror is not explicitly specified, using default value: True
INFO - root - preproces -- None
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189ad6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189ad6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189ad6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189ad6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189e4e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189e4e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189e4e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189e4e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189e46d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189e46d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189e46d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189e46d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189e42b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189e42b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189e42b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189e42b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189e4d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189e4d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189e4d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189e4d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189e4470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189e4470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189e4470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189e4470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189e4358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189e4358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189e4358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d189e4358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897ec18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897ec18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897ec18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897ec18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18d944a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18d944a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18d944a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18d944a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189bd940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189bd940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189bd940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189bd940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f4d189bdb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f4d189bdb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f4d189bdb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f4d189bdb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897e3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897e3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897e3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897e3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189ccb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189ccb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189ccb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189ccb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18c26668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18c26668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18c26668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18c26668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897d7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897d7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897d7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897d7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897def0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897def0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897def0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897def0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897d828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897d828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897dfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897dfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897dfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897dfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18927358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18927358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18927358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18927358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897e668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897e668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897e668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897e668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18927160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18927160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18927160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18927160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189bd7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189bd7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189bd7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189bd7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189bd7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189bd7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189bd7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189bd7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18975e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18975e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18975e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18975e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189ccac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189ccac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189ccac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189ccac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189ad1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189ad1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189ad1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189ad1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189757f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189757f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189757f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189757f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897d2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897d2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897d2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897d2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897b048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897b048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897b048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897b048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189cc0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189cc0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189cc0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189cc0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18927cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18927cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18927cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18927cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189d2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189d2390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189d2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189d2390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1898a2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1898a2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1898a2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1898a2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18927dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18927dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18927dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18927dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1898aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1898aa90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1898aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1898aa90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189bdba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189bdba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189bdba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189bdba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897bfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897bfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897e0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897e0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897e0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897e0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188e7cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188e7cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188e7cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188e7cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1898a438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1898a438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1898a438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1898a438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897bb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897bb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897bb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1897bb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897bfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897bfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188e7320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188e7320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188e7320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188e7320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897b7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897b7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897b7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1897b7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189256d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189256d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189256d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189256d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188e75c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188e75c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188e75c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188e75c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189212e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189212e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189212e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189212e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18925390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18925390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18925390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18925390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18921400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18921400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18921400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18921400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189dc240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189dc240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189dc240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d189dc240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18921668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18921668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18921668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18921668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18940fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18940fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18940fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18940fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189212e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189212e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189212e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189212e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18997c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18997c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18997c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18997c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18922748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18922748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18922748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18922748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188b77b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188b77b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188b77b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188b77b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1887d588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1887d588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1887d588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1887d588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188b8e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188b8e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188b8e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188b8e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18922978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18922978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18922978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18922978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1887dc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1887dc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1887dc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1887dc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189221d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189221d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189221d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d189221d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18925b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18925b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18925b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18925b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18921c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18921c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18921c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18921c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18925898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18925898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18925898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18925898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18846a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18846a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18846a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18846a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18922a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18922a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18922a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18922a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188460f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188460f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188460f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188460f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188460b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188460b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188460b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188460b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188c1c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188c1c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188c1c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188c1c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1882ed68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1882ed68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1882ed68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1882ed68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18846ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18846ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18846ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18846ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18846f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18846f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18846f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18846f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188e4e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188e4e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188e4e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d188e4e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188462b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188462b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188462b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188462b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187fa780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187fa780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187fa780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187fa780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188e4e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188e4e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188e4e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d188e4e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1881dda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1881dda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1881dda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1881dda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18835780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18835780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18835780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18835780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187fab70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187fab70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187fab70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187fab70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18997208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18997208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18997208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18997208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1881dac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1881dac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1881dac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1881dac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187fa320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187fa320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187fa320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187fa320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187f23c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187f23c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187f23c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187f23c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187fa320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187fa320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187fa320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187fa320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187ff9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187ff9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187ff9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187ff9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1881d160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1881d160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1881d160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1881d160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187ff710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187ff710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187ff710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187ff710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187f2a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187f2a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187f2a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187f2a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1872c940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1872c940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1872c940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1872c940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187ff710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187ff710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187ff710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187ff710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18798208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18798208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18798208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18798208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1881d5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1881d5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1881d5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1881d5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18798400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18798400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18798400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18798400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1881d6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1881d6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1881d6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1881d6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187749e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187749e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187749e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187749e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187c9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187c9e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187c9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187c9e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187a8f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187a8f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187a8f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187a8f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18774470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18774470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18774470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18774470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187fff28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187fff28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187fff28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187fff28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18774358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18774358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18774358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18774358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187e2a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187e2a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187e2a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187e2a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187a8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187a8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187a8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187a8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187987f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187987f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187987f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187987f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187e2ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187e2ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187e2ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187e2ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187c9978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187c9978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187c9978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d187c9978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18798438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18798438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18798438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d18798438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1874e8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1874e8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1874e8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1874e8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187c9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187c9e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187c9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187c9e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1874ee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1874ee48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1874ee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1874ee48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187c9da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187c9da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187c9da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d187c9da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18799860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18799860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18799860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18799860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1874ea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1874ea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1874ea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f4d1874ea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18799d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18799d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18799d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18799d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1872c240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1872c240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1872c240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1872c240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1878f7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1878f7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1878f7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1878f7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18799208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18799208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18799208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18799208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186c9668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186c9668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186c9668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186c9668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1876c780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1876c780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1876c780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1876c780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1871ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1871ad30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1871ad30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1871ad30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1870f9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1870f9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1870f9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1870f9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1871a7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1871a7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1871a7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1871a7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1871a898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1871a898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1871a898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1871a898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186d7fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186d7fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186d7fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186d7fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d187128d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d187128d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d187128d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d187128d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186c9438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186c9438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186c9438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186c9438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d186436d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d186436d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d186436d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d186436d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18643eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18643eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18643eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18643eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18643898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18643898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18643898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18643898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186775c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186775c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186775c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186775c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18677438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18677438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18677438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18677438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1868ec18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1868ec18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1868ec18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d1868ec18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18677400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18677400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18677400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18677400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186614a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186614a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186614a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d186614a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d19aac278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d19aac278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d19aac278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d19aac278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18677d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18677d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18677d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d18677d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18677400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18677400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18677400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18677400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1915d438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1915d438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1915d438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d1915d438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d186030f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d186030f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d186030f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d186030f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d185c2518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d185c2518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d185c2518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4d185c2518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18603358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18603358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18603358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18603358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d186116a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d186116a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d186116a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d186116a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18611b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18611b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18611b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f4d18611b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING - tensorflow - From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING:tensorflow:From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING - tensorflow - From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING - tensorflow - From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING - tensorflow - From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

2019-11-06 20:08:21.158058: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-06 20:08:21.163052: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-11-06 20:08:21.273162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 20:08:21.273628: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55676562e9c0 executing computations on platform CUDA. Devices:
2019-11-06 20:08:21.273643: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-11-06 20:08:21.292798: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz
2019-11-06 20:08:21.293043: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5567657129f0 executing computations on platform Host. Devices:
2019-11-06 20:08:21.293057: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-11-06 20:08:21.293299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 20:08:21.293707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-11-06 20:08:21.293841: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-06 20:08:21.294569: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-11-06 20:08:21.295201: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-11-06 20:08:21.295358: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-11-06 20:08:21.296149: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-11-06 20:08:21.296734: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-11-06 20:08:21.299226: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-11-06 20:08:21.299374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 20:08:21.299859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 20:08:21.300249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-11-06 20:08:21.300285: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-06 20:08:21.300973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-06 20:08:21.300985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-11-06 20:08:21.300991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-11-06 20:08:21.301118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 20:08:21.301507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 20:08:21.301912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6877 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-11-06 20:08:22.607224: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
INFO - root - Train for 150000 steps
2019-11-06 20:08:27.410771: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
INFO - root - 2019-11-06 20:08:28.957952: step 0, total loss = 6.21, predict loss = 2.22 (0.7 examples/sec; 5.745 sec/batch; 239h:23m:14s remains)
2019-11-06 20:08:30.203231: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
INFO - root - 2019-11-06 20:08:32.194698: step 10, total loss = 6.22, predict loss = 2.24 (80.0 examples/sec; 0.050 sec/batch; 2h:04m:59s remains)
INFO - root - 2019-11-06 20:08:32.768298: step 20, total loss = 6.09, predict loss = 2.08 (79.9 examples/sec; 0.050 sec/batch; 2h:05m:10s remains)
INFO - root - 2019-11-06 20:08:33.348086: step 30, total loss = 5.36, predict loss = 1.88 (80.7 examples/sec; 0.050 sec/batch; 2h:03m:50s remains)
INFO - root - 2019-11-06 20:08:33.931018: step 40, total loss = 5.34, predict loss = 1.78 (78.9 examples/sec; 0.051 sec/batch; 2h:06m:41s remains)
INFO - root - 2019-11-06 20:08:34.534117: step 50, total loss = 5.06, predict loss = 1.71 (78.0 examples/sec; 0.051 sec/batch; 2h:08m:08s remains)
INFO - root - 2019-11-06 20:08:35.113331: step 60, total loss = 4.93, predict loss = 1.71 (78.9 examples/sec; 0.051 sec/batch; 2h:06m:39s remains)
INFO - root - 2019-11-06 20:08:35.704750: step 70, total loss = 4.21, predict loss = 1.46 (82.3 examples/sec; 0.049 sec/batch; 2h:01m:30s remains)
INFO - root - 2019-11-06 20:08:36.296271: step 80, total loss = 4.06, predict loss = 1.39 (80.2 examples/sec; 0.050 sec/batch; 2h:04m:41s remains)
INFO - root - 2019-11-06 20:08:36.898844: step 90, total loss = 4.33, predict loss = 1.37 (80.3 examples/sec; 0.050 sec/batch; 2h:04m:26s remains)
INFO - root - 2019-11-06 20:08:37.470697: step 100, total loss = 3.61, predict loss = 1.24 (71.6 examples/sec; 0.056 sec/batch; 2h:19m:32s remains)
INFO - root - 2019-11-06 20:08:38.058443: step 110, total loss = 3.48, predict loss = 1.21 (75.5 examples/sec; 0.053 sec/batch; 2h:12m:19s remains)
INFO - root - 2019-11-06 20:08:38.634611: step 120, total loss = 3.26, predict loss = 1.08 (84.0 examples/sec; 0.048 sec/batch; 1h:58m:56s remains)
INFO - root - 2019-11-06 20:08:39.119462: step 130, total loss = 3.22, predict loss = 1.09 (104.2 examples/sec; 0.038 sec/batch; 1h:35m:50s remains)
INFO - root - 2019-11-06 20:08:39.575366: step 140, total loss = 3.12, predict loss = 1.12 (96.5 examples/sec; 0.041 sec/batch; 1h:43m:32s remains)
INFO - root - 2019-11-06 20:08:40.487380: step 150, total loss = 3.14, predict loss = 1.01 (81.5 examples/sec; 0.049 sec/batch; 2h:02m:32s remains)
INFO - root - 2019-11-06 20:08:41.111858: step 160, total loss = 2.74, predict loss = 0.96 (69.6 examples/sec; 0.058 sec/batch; 2h:23m:37s remains)
INFO - root - 2019-11-06 20:08:41.746060: step 170, total loss = 3.07, predict loss = 1.04 (76.2 examples/sec; 0.053 sec/batch; 2h:11m:10s remains)
INFO - root - 2019-11-06 20:08:42.312914: step 180, total loss = 2.50, predict loss = 0.94 (75.7 examples/sec; 0.053 sec/batch; 2h:11m:55s remains)
INFO - root - 2019-11-06 20:08:42.885790: step 190, total loss = 2.62, predict loss = 0.93 (79.3 examples/sec; 0.050 sec/batch; 2h:05m:51s remains)
INFO - root - 2019-11-06 20:08:43.473882: step 200, total loss = 2.37, predict loss = 0.84 (76.9 examples/sec; 0.052 sec/batch; 2h:09m:56s remains)
INFO - root - 2019-11-06 20:08:44.073009: step 210, total loss = 2.87, predict loss = 0.90 (81.9 examples/sec; 0.049 sec/batch; 2h:01m:52s remains)
INFO - root - 2019-11-06 20:08:44.647960: step 220, total loss = 2.31, predict loss = 0.82 (81.7 examples/sec; 0.049 sec/batch; 2h:02m:14s remains)
INFO - root - 2019-11-06 20:08:45.224215: step 230, total loss = 2.46, predict loss = 0.81 (78.9 examples/sec; 0.051 sec/batch; 2h:06m:31s remains)
INFO - root - 2019-11-06 20:08:45.817693: step 240, total loss = 2.19, predict loss = 0.79 (82.5 examples/sec; 0.048 sec/batch; 2h:01m:02s remains)
INFO - root - 2019-11-06 20:08:46.413822: step 250, total loss = 1.75, predict loss = 0.65 (77.8 examples/sec; 0.051 sec/batch; 2h:08m:15s remains)
INFO - root - 2019-11-06 20:08:46.991166: step 260, total loss = 1.91, predict loss = 0.68 (74.5 examples/sec; 0.054 sec/batch; 2h:14m:01s remains)
INFO - root - 2019-11-06 20:08:47.570414: step 270, total loss = 1.80, predict loss = 0.66 (91.0 examples/sec; 0.044 sec/batch; 1h:49m:38s remains)
INFO - root - 2019-11-06 20:08:48.027288: step 280, total loss = 1.86, predict loss = 0.74 (98.6 examples/sec; 0.041 sec/batch; 1h:41m:14s remains)
INFO - root - 2019-11-06 20:08:48.502026: step 290, total loss = 1.94, predict loss = 0.65 (96.4 examples/sec; 0.042 sec/batch; 1h:43m:34s remains)
INFO - root - 2019-11-06 20:08:49.474901: step 300, total loss = 1.61, predict loss = 0.53 (74.3 examples/sec; 0.054 sec/batch; 2h:14m:15s remains)
INFO - root - 2019-11-06 20:08:50.163691: step 310, total loss = 1.67, predict loss = 0.61 (65.2 examples/sec; 0.061 sec/batch; 2h:33m:09s remains)
INFO - root - 2019-11-06 20:08:50.774535: step 320, total loss = 1.71, predict loss = 0.59 (73.6 examples/sec; 0.054 sec/batch; 2h:15m:34s remains)
INFO - root - 2019-11-06 20:08:51.369847: step 330, total loss = 1.54, predict loss = 0.54 (81.2 examples/sec; 0.049 sec/batch; 2h:02m:49s remains)
INFO - root - 2019-11-06 20:08:51.969948: step 340, total loss = 1.82, predict loss = 0.61 (79.8 examples/sec; 0.050 sec/batch; 2h:05m:04s remains)
INFO - root - 2019-11-06 20:08:52.644297: step 350, total loss = 1.82, predict loss = 0.59 (76.8 examples/sec; 0.052 sec/batch; 2h:09m:54s remains)
INFO - root - 2019-11-06 20:08:53.229996: step 360, total loss = 1.35, predict loss = 0.55 (73.5 examples/sec; 0.054 sec/batch; 2h:15m:47s remains)
INFO - root - 2019-11-06 20:08:53.834399: step 370, total loss = 1.88, predict loss = 0.58 (79.1 examples/sec; 0.051 sec/batch; 2h:06m:07s remains)
INFO - root - 2019-11-06 20:08:54.433525: step 380, total loss = 1.33, predict loss = 0.51 (74.6 examples/sec; 0.054 sec/batch; 2h:13m:46s remains)
INFO - root - 2019-11-06 20:08:55.031367: step 390, total loss = 1.46, predict loss = 0.55 (77.5 examples/sec; 0.052 sec/batch; 2h:08m:44s remains)
INFO - root - 2019-11-06 20:08:55.624323: step 400, total loss = 1.33, predict loss = 0.59 (75.9 examples/sec; 0.053 sec/batch; 2h:11m:24s remains)
INFO - root - 2019-11-06 20:08:56.220836: step 410, total loss = 1.33, predict loss = 0.46 (77.5 examples/sec; 0.052 sec/batch; 2h:08m:45s remains)
INFO - root - 2019-11-06 20:08:56.769411: step 420, total loss = 1.25, predict loss = 0.49 (83.1 examples/sec; 0.048 sec/batch; 1h:59m:55s remains)
INFO - root - 2019-11-06 20:08:57.217905: step 430, total loss = 1.92, predict loss = 0.60 (91.0 examples/sec; 0.044 sec/batch; 1h:49m:37s remains)
INFO - root - 2019-11-06 20:08:57.678586: step 440, total loss = 1.99, predict loss = 0.67 (90.0 examples/sec; 0.044 sec/batch; 1h:50m:45s remains)
INFO - root - 2019-11-06 20:08:58.703372: step 450, total loss = 1.34, predict loss = 0.45 (53.5 examples/sec; 0.075 sec/batch; 3h:06m:15s remains)
INFO - root - 2019-11-06 20:08:59.342242: step 460, total loss = 1.66, predict loss = 0.53 (79.7 examples/sec; 0.050 sec/batch; 2h:05m:04s remains)
INFO - root - 2019-11-06 20:08:59.928777: step 470, total loss = 1.35, predict loss = 0.47 (73.7 examples/sec; 0.054 sec/batch; 2h:15m:12s remains)
INFO - root - 2019-11-06 20:09:00.517239: step 480, total loss = 1.08, predict loss = 0.40 (79.9 examples/sec; 0.050 sec/batch; 2h:04m:49s remains)
INFO - root - 2019-11-06 20:09:01.111949: step 490, total loss = 1.25, predict loss = 0.50 (78.0 examples/sec; 0.051 sec/batch; 2h:07m:45s remains)
INFO - root - 2019-11-06 20:09:01.709973: step 500, total loss = 1.19, predict loss = 0.40 (76.7 examples/sec; 0.052 sec/batch; 2h:09m:57s remains)
INFO - root - 2019-11-06 20:09:02.290169: step 510, total loss = 1.00, predict loss = 0.37 (78.5 examples/sec; 0.051 sec/batch; 2h:06m:56s remains)
INFO - root - 2019-11-06 20:09:02.862930: step 520, total loss = 1.04, predict loss = 0.35 (77.8 examples/sec; 0.051 sec/batch; 2h:08m:03s remains)
INFO - root - 2019-11-06 20:09:03.456378: step 530, total loss = 1.08, predict loss = 0.38 (76.3 examples/sec; 0.052 sec/batch; 2h:10m:40s remains)
INFO - root - 2019-11-06 20:09:04.053120: step 540, total loss = 0.86, predict loss = 0.31 (71.4 examples/sec; 0.056 sec/batch; 2h:19m:35s remains)
INFO - root - 2019-11-06 20:09:04.644233: step 550, total loss = 1.10, predict loss = 0.36 (81.5 examples/sec; 0.049 sec/batch; 2h:02m:15s remains)
INFO - root - 2019-11-06 20:09:05.241402: step 560, total loss = 0.98, predict loss = 0.33 (81.6 examples/sec; 0.049 sec/batch; 2h:02m:07s remains)
INFO - root - 2019-11-06 20:09:05.776865: step 570, total loss = 1.25, predict loss = 0.39 (93.6 examples/sec; 0.043 sec/batch; 1h:46m:29s remains)
INFO - root - 2019-11-06 20:09:06.224188: step 580, total loss = 1.27, predict loss = 0.43 (89.6 examples/sec; 0.045 sec/batch; 1h:51m:08s remains)
INFO - root - 2019-11-06 20:09:06.675136: step 590, total loss = 1.27, predict loss = 0.46 (95.9 examples/sec; 0.042 sec/batch; 1h:43m:50s remains)
INFO - root - 2019-11-06 20:09:07.729138: step 600, total loss = 1.09, predict loss = 0.39 (52.3 examples/sec; 0.077 sec/batch; 3h:10m:31s remains)
INFO - root - 2019-11-06 20:09:08.404156: step 610, total loss = 1.09, predict loss = 0.35 (72.9 examples/sec; 0.055 sec/batch; 2h:16m:41s remains)
INFO - root - 2019-11-06 20:09:09.002802: step 620, total loss = 1.00, predict loss = 0.33 (75.4 examples/sec; 0.053 sec/batch; 2h:12m:02s remains)
INFO - root - 2019-11-06 20:09:09.578927: step 630, total loss = 0.90, predict loss = 0.32 (79.9 examples/sec; 0.050 sec/batch; 2h:04m:37s remains)
INFO - root - 2019-11-06 20:09:10.151315: step 640, total loss = 1.31, predict loss = 0.44 (75.7 examples/sec; 0.053 sec/batch; 2h:11m:33s remains)
INFO - root - 2019-11-06 20:09:10.765419: step 650, total loss = 1.03, predict loss = 0.34 (79.3 examples/sec; 0.050 sec/batch; 2h:05m:34s remains)
INFO - root - 2019-11-06 20:09:11.355692: step 660, total loss = 1.47, predict loss = 0.40 (76.0 examples/sec; 0.053 sec/batch; 2h:10m:55s remains)
INFO - root - 2019-11-06 20:09:11.939351: step 670, total loss = 0.93, predict loss = 0.31 (79.8 examples/sec; 0.050 sec/batch; 2h:04m:45s remains)
INFO - root - 2019-11-06 20:09:12.504168: step 680, total loss = 0.95, predict loss = 0.33 (77.0 examples/sec; 0.052 sec/batch; 2h:09m:16s remains)
INFO - root - 2019-11-06 20:09:13.102261: step 690, total loss = 0.98, predict loss = 0.30 (78.4 examples/sec; 0.051 sec/batch; 2h:06m:58s remains)
INFO - root - 2019-11-06 20:09:13.699867: step 700, total loss = 1.16, predict loss = 0.35 (79.7 examples/sec; 0.050 sec/batch; 2h:04m:54s remains)
INFO - root - 2019-11-06 20:09:14.285441: step 710, total loss = 1.20, predict loss = 0.39 (79.8 examples/sec; 0.050 sec/batch; 2h:04m:45s remains)
INFO - root - 2019-11-06 20:09:14.775543: step 720, total loss = 0.96, predict loss = 0.30 (104.7 examples/sec; 0.038 sec/batch; 1h:35m:00s remains)
INFO - root - 2019-11-06 20:09:15.245102: step 730, total loss = 0.90, predict loss = 0.31 (94.0 examples/sec; 0.043 sec/batch; 1h:45m:50s remains)
INFO - root - 2019-11-06 20:09:15.695826: step 740, total loss = 1.19, predict loss = 0.39 (105.5 examples/sec; 0.038 sec/batch; 1h:34m:18s remains)
INFO - root - 2019-11-06 20:09:16.803623: step 750, total loss = 1.19, predict loss = 0.31 (64.4 examples/sec; 0.062 sec/batch; 2h:34m:26s remains)
INFO - root - 2019-11-06 20:09:17.437417: step 760, total loss = 1.06, predict loss = 0.35 (78.6 examples/sec; 0.051 sec/batch; 2h:06m:31s remains)
INFO - root - 2019-11-06 20:09:18.053830: step 770, total loss = 1.28, predict loss = 0.35 (79.2 examples/sec; 0.050 sec/batch; 2h:05m:33s remains)
INFO - root - 2019-11-06 20:09:18.642139: step 780, total loss = 1.33, predict loss = 0.39 (80.2 examples/sec; 0.050 sec/batch; 2h:04m:05s remains)
INFO - root - 2019-11-06 20:09:19.234899: step 790, total loss = 1.49, predict loss = 0.42 (77.6 examples/sec; 0.052 sec/batch; 2h:08m:07s remains)
INFO - root - 2019-11-06 20:09:19.823104: step 800, total loss = 1.09, predict loss = 0.33 (70.0 examples/sec; 0.057 sec/batch; 2h:22m:04s remains)
INFO - root - 2019-11-06 20:09:20.436832: step 810, total loss = 0.85, predict loss = 0.30 (68.6 examples/sec; 0.058 sec/batch; 2h:24m:57s remains)
INFO - root - 2019-11-06 20:09:21.110084: step 820, total loss = 1.21, predict loss = 0.35 (79.5 examples/sec; 0.050 sec/batch; 2h:05m:02s remains)
INFO - root - 2019-11-06 20:09:21.726325: step 830, total loss = 1.05, predict loss = 0.29 (74.1 examples/sec; 0.054 sec/batch; 2h:14m:15s remains)
INFO - root - 2019-11-06 20:09:22.392377: step 840, total loss = 0.91, predict loss = 0.29 (49.1 examples/sec; 0.081 sec/batch; 3h:22m:21s remains)
INFO - root - 2019-11-06 20:09:23.128736: step 850, total loss = 0.84, predict loss = 0.23 (61.2 examples/sec; 0.065 sec/batch; 2h:42m:25s remains)
INFO - root - 2019-11-06 20:09:23.724872: step 860, total loss = 1.41, predict loss = 0.41 (74.8 examples/sec; 0.054 sec/batch; 2h:12m:59s remains)
INFO - root - 2019-11-06 20:09:24.252413: step 870, total loss = 1.23, predict loss = 0.38 (92.7 examples/sec; 0.043 sec/batch; 1h:47m:14s remains)
INFO - root - 2019-11-06 20:09:24.733208: step 880, total loss = 0.70, predict loss = 0.25 (92.9 examples/sec; 0.043 sec/batch; 1h:46m:59s remains)
INFO - root - 2019-11-06 20:09:25.742740: step 890, total loss = 0.85, predict loss = 0.26 (64.3 examples/sec; 0.062 sec/batch; 2h:34m:36s remains)
INFO - root - 2019-11-06 20:09:26.460592: step 900, total loss = 0.90, predict loss = 0.29 (63.9 examples/sec; 0.063 sec/batch; 2h:35m:30s remains)
INFO - root - 2019-11-06 20:09:27.211257: step 910, total loss = 0.94, predict loss = 0.29 (68.2 examples/sec; 0.059 sec/batch; 2h:25m:45s remains)
INFO - root - 2019-11-06 20:09:28.000415: step 920, total loss = 1.06, predict loss = 0.34 (66.4 examples/sec; 0.060 sec/batch; 2h:29m:41s remains)
INFO - root - 2019-11-06 20:09:28.608563: step 930, total loss = 0.76, predict loss = 0.25 (83.1 examples/sec; 0.048 sec/batch; 1h:59m:38s remains)
INFO - root - 2019-11-06 20:09:29.197588: step 940, total loss = 0.87, predict loss = 0.27 (75.2 examples/sec; 0.053 sec/batch; 2h:12m:05s remains)
INFO - root - 2019-11-06 20:09:29.765217: step 950, total loss = 0.72, predict loss = 0.23 (77.7 examples/sec; 0.051 sec/batch; 2h:07m:55s remains)
INFO - root - 2019-11-06 20:09:30.417303: step 960, total loss = 0.95, predict loss = 0.29 (79.0 examples/sec; 0.051 sec/batch; 2h:05m:47s remains)
INFO - root - 2019-11-06 20:09:31.026657: step 970, total loss = 0.79, predict loss = 0.24 (67.7 examples/sec; 0.059 sec/batch; 2h:26m:49s remains)
INFO - root - 2019-11-06 20:09:31.668270: step 980, total loss = 1.18, predict loss = 0.34 (73.6 examples/sec; 0.054 sec/batch; 2h:14m:55s remains)
INFO - root - 2019-11-06 20:09:32.266788: step 990, total loss = 0.91, predict loss = 0.29 (64.8 examples/sec; 0.062 sec/batch; 2h:33m:16s remains)
INFO - root - 2019-11-06 20:09:32.860997: step 1000, total loss = 0.70, predict loss = 0.23 (77.5 examples/sec; 0.052 sec/batch; 2h:08m:15s remains)
INFO - root - 2019-11-06 20:09:33.423986: step 1010, total loss = 1.01, predict loss = 0.30 (93.9 examples/sec; 0.043 sec/batch; 1h:45m:46s remains)
INFO - root - 2019-11-06 20:09:33.875684: step 1020, total loss = 0.97, predict loss = 0.28 (100.0 examples/sec; 0.040 sec/batch; 1h:39m:18s remains)
INFO - root - 2019-11-06 20:09:34.320325: step 1030, total loss = 0.71, predict loss = 0.22 (93.6 examples/sec; 0.043 sec/batch; 1h:46m:07s remains)
INFO - root - 2019-11-06 20:09:35.277041: step 1040, total loss = 0.77, predict loss = 0.22 (68.2 examples/sec; 0.059 sec/batch; 2h:25m:33s remains)
INFO - root - 2019-11-06 20:09:36.014648: step 1050, total loss = 0.95, predict loss = 0.30 (51.8 examples/sec; 0.077 sec/batch; 3h:11m:52s remains)
INFO - root - 2019-11-06 20:09:36.718142: step 1060, total loss = 0.98, predict loss = 0.28 (73.1 examples/sec; 0.055 sec/batch; 2h:15m:47s remains)
INFO - root - 2019-11-06 20:09:37.301972: step 1070, total loss = 1.07, predict loss = 0.30 (77.5 examples/sec; 0.052 sec/batch; 2h:08m:05s remains)
INFO - root - 2019-11-06 20:09:37.874982: step 1080, total loss = 1.13, predict loss = 0.34 (77.2 examples/sec; 0.052 sec/batch; 2h:08m:40s remains)
INFO - root - 2019-11-06 20:09:38.485866: step 1090, total loss = 0.93, predict loss = 0.29 (69.6 examples/sec; 0.057 sec/batch; 2h:22m:34s remains)
INFO - root - 2019-11-06 20:09:39.130483: step 1100, total loss = 0.74, predict loss = 0.22 (73.3 examples/sec; 0.055 sec/batch; 2h:15m:25s remains)
INFO - root - 2019-11-06 20:09:39.725388: step 1110, total loss = 0.69, predict loss = 0.21 (78.6 examples/sec; 0.051 sec/batch; 2h:06m:14s remains)
INFO - root - 2019-11-06 20:09:40.287976: step 1120, total loss = 1.03, predict loss = 0.31 (78.9 examples/sec; 0.051 sec/batch; 2h:05m:51s remains)
INFO - root - 2019-11-06 20:09:40.869230: step 1130, total loss = 0.73, predict loss = 0.23 (79.9 examples/sec; 0.050 sec/batch; 2h:04m:14s remains)
INFO - root - 2019-11-06 20:09:41.434663: step 1140, total loss = 0.57, predict loss = 0.18 (80.4 examples/sec; 0.050 sec/batch; 2h:03m:23s remains)
INFO - root - 2019-11-06 20:09:42.014237: step 1150, total loss = 0.88, predict loss = 0.26 (78.7 examples/sec; 0.051 sec/batch; 2h:06m:00s remains)
INFO - root - 2019-11-06 20:09:42.575863: step 1160, total loss = 1.41, predict loss = 0.41 (93.4 examples/sec; 0.043 sec/batch; 1h:46m:16s remains)
INFO - root - 2019-11-06 20:09:43.094253: step 1170, total loss = 0.80, predict loss = 0.25 (98.8 examples/sec; 0.040 sec/batch; 1h:40m:26s remains)
INFO - root - 2019-11-06 20:09:43.669625: step 1180, total loss = 0.90, predict loss = 0.27 (86.8 examples/sec; 0.046 sec/batch; 1h:54m:20s remains)
INFO - root - 2019-11-06 20:09:44.939915: step 1190, total loss = 0.72, predict loss = 0.23 (49.3 examples/sec; 0.081 sec/batch; 3h:21m:17s remains)
INFO - root - 2019-11-06 20:09:45.694652: step 1200, total loss = 0.76, predict loss = 0.23 (64.5 examples/sec; 0.062 sec/batch; 2h:33m:46s remains)
INFO - root - 2019-11-06 20:09:46.338224: step 1210, total loss = 0.92, predict loss = 0.28 (72.8 examples/sec; 0.055 sec/batch; 2h:16m:15s remains)
INFO - root - 2019-11-06 20:09:46.927455: step 1220, total loss = 0.71, predict loss = 0.21 (75.7 examples/sec; 0.053 sec/batch; 2h:10m:57s remains)
INFO - root - 2019-11-06 20:09:47.504903: step 1230, total loss = 0.55, predict loss = 0.16 (80.0 examples/sec; 0.050 sec/batch; 2h:04m:02s remains)
INFO - root - 2019-11-06 20:09:48.085039: step 1240, total loss = 0.91, predict loss = 0.27 (79.1 examples/sec; 0.051 sec/batch; 2h:05m:27s remains)
INFO - root - 2019-11-06 20:09:48.675267: step 1250, total loss = 0.85, predict loss = 0.25 (77.8 examples/sec; 0.051 sec/batch; 2h:07m:31s remains)
INFO - root - 2019-11-06 20:09:49.245997: step 1260, total loss = 0.67, predict loss = 0.22 (71.7 examples/sec; 0.056 sec/batch; 2h:18m:13s remains)
INFO - root - 2019-11-06 20:09:49.805244: step 1270, total loss = 0.70, predict loss = 0.24 (77.5 examples/sec; 0.052 sec/batch; 2h:07m:53s remains)
INFO - root - 2019-11-06 20:09:50.379727: step 1280, total loss = 0.70, predict loss = 0.21 (77.8 examples/sec; 0.051 sec/batch; 2h:07m:21s remains)
INFO - root - 2019-11-06 20:09:50.989408: step 1290, total loss = 1.07, predict loss = 0.31 (75.6 examples/sec; 0.053 sec/batch; 2h:11m:04s remains)
INFO - root - 2019-11-06 20:09:51.574323: step 1300, total loss = 0.99, predict loss = 0.28 (79.5 examples/sec; 0.050 sec/batch; 2h:04m:41s remains)
INFO - root - 2019-11-06 20:09:52.141884: step 1310, total loss = 0.50, predict loss = 0.15 (82.1 examples/sec; 0.049 sec/batch; 2h:00m:46s remains)
INFO - root - 2019-11-06 20:09:52.641671: step 1320, total loss = 1.18, predict loss = 0.32 (84.5 examples/sec; 0.047 sec/batch; 1h:57m:16s remains)
INFO - root - 2019-11-06 20:09:53.155392: step 1330, total loss = 1.04, predict loss = 0.32 (96.1 examples/sec; 0.042 sec/batch; 1h:43m:08s remains)
INFO - root - 2019-11-06 20:09:54.182485: step 1340, total loss = 0.71, predict loss = 0.21 (56.5 examples/sec; 0.071 sec/batch; 2h:55m:16s remains)
INFO - root - 2019-11-06 20:09:54.885627: step 1350, total loss = 0.60, predict loss = 0.18 (58.9 examples/sec; 0.068 sec/batch; 2h:48m:16s remains)
INFO - root - 2019-11-06 20:09:55.606794: step 1360, total loss = 1.25, predict loss = 0.35 (64.7 examples/sec; 0.062 sec/batch; 2h:33m:07s remains)
INFO - root - 2019-11-06 20:09:56.222526: step 1370, total loss = 0.91, predict loss = 0.25 (75.5 examples/sec; 0.053 sec/batch; 2h:11m:19s remains)
INFO - root - 2019-11-06 20:09:56.810823: step 1380, total loss = 0.80, predict loss = 0.23 (78.1 examples/sec; 0.051 sec/batch; 2h:06m:48s remains)
INFO - root - 2019-11-06 20:09:57.392009: step 1390, total loss = 0.71, predict loss = 0.20 (78.7 examples/sec; 0.051 sec/batch; 2h:05m:49s remains)
INFO - root - 2019-11-06 20:09:57.963975: step 1400, total loss = 0.93, predict loss = 0.28 (80.2 examples/sec; 0.050 sec/batch; 2h:03m:28s remains)
INFO - root - 2019-11-06 20:09:58.565397: step 1410, total loss = 1.44, predict loss = 0.42 (76.9 examples/sec; 0.052 sec/batch; 2h:08m:53s remains)
INFO - root - 2019-11-06 20:09:59.154976: step 1420, total loss = 1.16, predict loss = 0.32 (82.3 examples/sec; 0.049 sec/batch; 2h:00m:18s remains)
INFO - root - 2019-11-06 20:09:59.741135: step 1430, total loss = 0.76, predict loss = 0.21 (79.9 examples/sec; 0.050 sec/batch; 2h:03m:59s remains)
INFO - root - 2019-11-06 20:10:00.332579: step 1440, total loss = 0.61, predict loss = 0.20 (79.4 examples/sec; 0.050 sec/batch; 2h:04m:46s remains)
INFO - root - 2019-11-06 20:10:00.981677: step 1450, total loss = 0.60, predict loss = 0.17 (80.7 examples/sec; 0.050 sec/batch; 2h:02m:45s remains)
INFO - root - 2019-11-06 20:10:01.545459: step 1460, total loss = 0.94, predict loss = 0.25 (106.3 examples/sec; 0.038 sec/batch; 1h:33m:09s remains)
INFO - root - 2019-11-06 20:10:01.995280: step 1470, total loss = 0.78, predict loss = 0.22 (94.6 examples/sec; 0.042 sec/batch; 1h:44m:40s remains)
INFO - root - 2019-11-06 20:10:02.457658: step 1480, total loss = 0.56, predict loss = 0.18 (94.6 examples/sec; 0.042 sec/batch; 1h:44m:39s remains)
INFO - root - 2019-11-06 20:10:03.552088: step 1490, total loss = 0.64, predict loss = 0.22 (58.3 examples/sec; 0.069 sec/batch; 2h:49m:43s remains)
INFO - root - 2019-11-06 20:10:04.162895: step 1500, total loss = 0.93, predict loss = 0.29 (77.2 examples/sec; 0.052 sec/batch; 2h:08m:12s remains)
INFO - root - 2019-11-06 20:10:04.727255: step 1510, total loss = 0.84, predict loss = 0.23 (81.9 examples/sec; 0.049 sec/batch; 2h:00m:50s remains)
INFO - root - 2019-11-06 20:10:05.300486: step 1520, total loss = 1.18, predict loss = 0.35 (76.9 examples/sec; 0.052 sec/batch; 2h:08m:38s remains)
INFO - root - 2019-11-06 20:10:05.883612: step 1530, total loss = 0.70, predict loss = 0.20 (76.8 examples/sec; 0.052 sec/batch; 2h:08m:50s remains)
INFO - root - 2019-11-06 20:10:06.466079: step 1540, total loss = 0.72, predict loss = 0.24 (72.1 examples/sec; 0.055 sec/batch; 2h:17m:12s remains)
INFO - root - 2019-11-06 20:10:07.035670: step 1550, total loss = 0.52, predict loss = 0.17 (80.5 examples/sec; 0.050 sec/batch; 2h:02m:59s remains)
INFO - root - 2019-11-06 20:10:07.617598: step 1560, total loss = 0.86, predict loss = 0.22 (74.5 examples/sec; 0.054 sec/batch; 2h:12m:52s remains)
INFO - root - 2019-11-06 20:10:08.214139: step 1570, total loss = 0.85, predict loss = 0.25 (78.6 examples/sec; 0.051 sec/batch; 2h:05m:52s remains)
INFO - root - 2019-11-06 20:10:08.786838: step 1580, total loss = 0.65, predict loss = 0.18 (82.2 examples/sec; 0.049 sec/batch; 2h:00m:24s remains)
INFO - root - 2019-11-06 20:10:09.363407: step 1590, total loss = 1.20, predict loss = 0.34 (77.4 examples/sec; 0.052 sec/batch; 2h:07m:54s remains)
INFO - root - 2019-11-06 20:10:09.931003: step 1600, total loss = 0.55, predict loss = 0.15 (83.4 examples/sec; 0.048 sec/batch; 1h:58m:35s remains)
INFO - root - 2019-11-06 20:10:10.431160: step 1610, total loss = 1.29, predict loss = 0.36 (95.8 examples/sec; 0.042 sec/batch; 1h:43m:16s remains)
INFO - root - 2019-11-06 20:10:10.885485: step 1620, total loss = 0.70, predict loss = 0.19 (96.3 examples/sec; 0.042 sec/batch; 1h:42m:43s remains)
INFO - root - 2019-11-06 20:10:11.771093: step 1630, total loss = 1.04, predict loss = 0.32 (8.6 examples/sec; 0.466 sec/batch; 19h:12m:03s remains)
INFO - root - 2019-11-06 20:10:12.401709: step 1640, total loss = 0.70, predict loss = 0.21 (70.2 examples/sec; 0.057 sec/batch; 2h:20m:52s remains)
INFO - root - 2019-11-06 20:10:13.012042: step 1650, total loss = 0.87, predict loss = 0.25 (75.0 examples/sec; 0.053 sec/batch; 2h:11m:49s remains)
INFO - root - 2019-11-06 20:10:13.593115: step 1660, total loss = 0.50, predict loss = 0.14 (79.0 examples/sec; 0.051 sec/batch; 2h:05m:10s remains)
INFO - root - 2019-11-06 20:10:14.163810: step 1670, total loss = 0.75, predict loss = 0.23 (79.6 examples/sec; 0.050 sec/batch; 2h:04m:10s remains)
INFO - root - 2019-11-06 20:10:14.737190: step 1680, total loss = 0.65, predict loss = 0.19 (77.3 examples/sec; 0.052 sec/batch; 2h:07m:54s remains)
INFO - root - 2019-11-06 20:10:15.370570: step 1690, total loss = 0.97, predict loss = 0.27 (77.5 examples/sec; 0.052 sec/batch; 2h:07m:29s remains)
INFO - root - 2019-11-06 20:10:15.946191: step 1700, total loss = 0.56, predict loss = 0.17 (78.6 examples/sec; 0.051 sec/batch; 2h:05m:51s remains)
INFO - root - 2019-11-06 20:10:16.516237: step 1710, total loss = 0.72, predict loss = 0.20 (82.6 examples/sec; 0.048 sec/batch; 1h:59m:41s remains)
INFO - root - 2019-11-06 20:10:17.107523: step 1720, total loss = 0.57, predict loss = 0.15 (74.1 examples/sec; 0.054 sec/batch; 2h:13m:21s remains)
INFO - root - 2019-11-06 20:10:17.700422: step 1730, total loss = 0.53, predict loss = 0.17 (76.1 examples/sec; 0.053 sec/batch; 2h:09m:49s remains)
INFO - root - 2019-11-06 20:10:18.284065: step 1740, total loss = 0.82, predict loss = 0.21 (79.2 examples/sec; 0.050 sec/batch; 2h:04m:44s remains)
INFO - root - 2019-11-06 20:10:18.847041: step 1750, total loss = 1.30, predict loss = 0.37 (88.1 examples/sec; 0.045 sec/batch; 1h:52m:09s remains)
INFO - root - 2019-11-06 20:10:19.320822: step 1760, total loss = 0.58, predict loss = 0.18 (97.4 examples/sec; 0.041 sec/batch; 1h:41m:28s remains)
INFO - root - 2019-11-06 20:10:19.799859: step 1770, total loss = 0.77, predict loss = 0.19 (96.7 examples/sec; 0.041 sec/batch; 1h:42m:12s remains)
INFO - root - 2019-11-06 20:10:20.711284: step 1780, total loss = 0.67, predict loss = 0.20 (78.4 examples/sec; 0.051 sec/batch; 2h:06m:03s remains)
INFO - root - 2019-11-06 20:10:21.354818: step 1790, total loss = 0.67, predict loss = 0.19 (68.0 examples/sec; 0.059 sec/batch; 2h:25m:12s remains)
INFO - root - 2019-11-06 20:10:21.957600: step 1800, total loss = 0.56, predict loss = 0.17 (76.1 examples/sec; 0.053 sec/batch; 2h:09m:53s remains)
INFO - root - 2019-11-06 20:10:22.534649: step 1810, total loss = 1.10, predict loss = 0.31 (78.5 examples/sec; 0.051 sec/batch; 2h:05m:47s remains)
INFO - root - 2019-11-06 20:10:23.107001: step 1820, total loss = 0.84, predict loss = 0.24 (74.1 examples/sec; 0.054 sec/batch; 2h:13m:13s remains)
INFO - root - 2019-11-06 20:10:23.665108: step 1830, total loss = 0.89, predict loss = 0.27 (82.4 examples/sec; 0.049 sec/batch; 1h:59m:49s remains)
INFO - root - 2019-11-06 20:10:24.226677: step 1840, total loss = 0.53, predict loss = 0.15 (74.3 examples/sec; 0.054 sec/batch; 2h:12m:53s remains)
INFO - root - 2019-11-06 20:10:24.816783: step 1850, total loss = 0.69, predict loss = 0.19 (83.7 examples/sec; 0.048 sec/batch; 1h:58m:03s remains)
INFO - root - 2019-11-06 20:10:25.390105: step 1860, total loss = 0.73, predict loss = 0.20 (79.5 examples/sec; 0.050 sec/batch; 2h:04m:15s remains)
INFO - root - 2019-11-06 20:10:25.972343: step 1870, total loss = 0.62, predict loss = 0.18 (76.9 examples/sec; 0.052 sec/batch; 2h:08m:27s remains)
INFO - root - 2019-11-06 20:10:26.538712: step 1880, total loss = 0.90, predict loss = 0.25 (82.9 examples/sec; 0.048 sec/batch; 1h:59m:07s remains)
INFO - root - 2019-11-06 20:10:27.132212: step 1890, total loss = 0.89, predict loss = 0.27 (80.5 examples/sec; 0.050 sec/batch; 2h:02m:42s remains)
INFO - root - 2019-11-06 20:10:27.679424: step 1900, total loss = 0.80, predict loss = 0.24 (96.0 examples/sec; 0.042 sec/batch; 1h:42m:50s remains)
INFO - root - 2019-11-06 20:10:28.137756: step 1910, total loss = 0.60, predict loss = 0.16 (104.2 examples/sec; 0.038 sec/batch; 1h:34m:43s remains)
INFO - root - 2019-11-06 20:10:28.595424: step 1920, total loss = 0.58, predict loss = 0.17 (99.7 examples/sec; 0.040 sec/batch; 1h:39m:03s remains)
INFO - root - 2019-11-06 20:10:29.569660: step 1930, total loss = 0.73, predict loss = 0.21 (64.4 examples/sec; 0.062 sec/batch; 2h:33m:21s remains)
INFO - root - 2019-11-06 20:10:30.290502: step 1940, total loss = 0.52, predict loss = 0.15 (63.1 examples/sec; 0.063 sec/batch; 2h:36m:30s remains)
INFO - root - 2019-11-06 20:10:30.937565: step 1950, total loss = 1.09, predict loss = 0.32 (68.2 examples/sec; 0.059 sec/batch; 2h:24m:45s remains)
INFO - root - 2019-11-06 20:10:31.531926: step 1960, total loss = 1.00, predict loss = 0.28 (78.7 examples/sec; 0.051 sec/batch; 2h:05m:23s remains)
INFO - root - 2019-11-06 20:10:32.111236: step 1970, total loss = 0.59, predict loss = 0.16 (80.2 examples/sec; 0.050 sec/batch; 2h:03m:01s remains)
INFO - root - 2019-11-06 20:10:32.672134: step 1980, total loss = 0.59, predict loss = 0.18 (82.4 examples/sec; 0.049 sec/batch; 1h:59m:49s remains)
INFO - root - 2019-11-06 20:10:33.241954: step 1990, total loss = 0.67, predict loss = 0.18 (77.1 examples/sec; 0.052 sec/batch; 2h:08m:00s remains)
INFO - root - 2019-11-06 20:10:33.809300: step 2000, total loss = 0.82, predict loss = 0.22 (79.4 examples/sec; 0.050 sec/batch; 2h:04m:11s remains)
INFO - root - 2019-11-06 20:10:34.400278: step 2010, total loss = 0.66, predict loss = 0.19 (78.0 examples/sec; 0.051 sec/batch; 2h:06m:29s remains)
INFO - root - 2019-11-06 20:10:34.968859: step 2020, total loss = 0.87, predict loss = 0.27 (80.6 examples/sec; 0.050 sec/batch; 2h:02m:23s remains)
INFO - root - 2019-11-06 20:10:35.551902: step 2030, total loss = 0.63, predict loss = 0.19 (76.2 examples/sec; 0.052 sec/batch; 2h:09m:22s remains)
INFO - root - 2019-11-06 20:10:36.109870: step 2040, total loss = 0.57, predict loss = 0.16 (79.0 examples/sec; 0.051 sec/batch; 2h:04m:56s remains)
INFO - root - 2019-11-06 20:10:36.645267: step 2050, total loss = 0.76, predict loss = 0.21 (95.8 examples/sec; 0.042 sec/batch; 1h:42m:55s remains)
INFO - root - 2019-11-06 20:10:37.085230: step 2060, total loss = 1.12, predict loss = 0.32 (99.1 examples/sec; 0.040 sec/batch; 1h:39m:29s remains)
INFO - root - 2019-11-06 20:10:37.538420: step 2070, total loss = 0.78, predict loss = 0.22 (90.5 examples/sec; 0.044 sec/batch; 1h:48m:57s remains)
INFO - root - 2019-11-06 20:10:38.559453: step 2080, total loss = 0.71, predict loss = 0.18 (59.7 examples/sec; 0.067 sec/batch; 2h:45m:15s remains)
INFO - root - 2019-11-06 20:10:39.289241: step 2090, total loss = 0.57, predict loss = 0.17 (64.9 examples/sec; 0.062 sec/batch; 2h:31m:57s remains)
INFO - root - 2019-11-06 20:10:39.899957: step 2100, total loss = 0.71, predict loss = 0.20 (78.0 examples/sec; 0.051 sec/batch; 2h:06m:26s remains)
INFO - root - 2019-11-06 20:10:40.485756: step 2110, total loss = 1.25, predict loss = 0.35 (80.3 examples/sec; 0.050 sec/batch; 2h:02m:49s remains)
INFO - root - 2019-11-06 20:10:41.059992: step 2120, total loss = 0.66, predict loss = 0.18 (80.2 examples/sec; 0.050 sec/batch; 2h:02m:55s remains)
INFO - root - 2019-11-06 20:10:41.636376: step 2130, total loss = 1.13, predict loss = 0.33 (75.3 examples/sec; 0.053 sec/batch; 2h:10m:54s remains)
INFO - root - 2019-11-06 20:10:42.204288: step 2140, total loss = 0.94, predict loss = 0.24 (78.0 examples/sec; 0.051 sec/batch; 2h:06m:18s remains)
INFO - root - 2019-11-06 20:10:42.789510: step 2150, total loss = 0.86, predict loss = 0.20 (76.7 examples/sec; 0.052 sec/batch; 2h:08m:33s remains)
INFO - root - 2019-11-06 20:10:43.363834: step 2160, total loss = 0.66, predict loss = 0.18 (75.7 examples/sec; 0.053 sec/batch; 2h:10m:15s remains)
INFO - root - 2019-11-06 20:10:43.953104: step 2170, total loss = 0.71, predict loss = 0.18 (80.7 examples/sec; 0.050 sec/batch; 2h:02m:09s remains)
INFO - root - 2019-11-06 20:10:44.528959: step 2180, total loss = 0.55, predict loss = 0.16 (78.2 examples/sec; 0.051 sec/batch; 2h:05m:57s remains)
INFO - root - 2019-11-06 20:10:45.137359: step 2190, total loss = 0.95, predict loss = 0.27 (63.0 examples/sec; 0.063 sec/batch; 2h:36m:25s remains)
INFO - root - 2019-11-06 20:10:45.669747: step 2200, total loss = 0.70, predict loss = 0.22 (100.3 examples/sec; 0.040 sec/batch; 1h:38m:14s remains)
INFO - root - 2019-11-06 20:10:46.151055: step 2210, total loss = 1.01, predict loss = 0.27 (95.7 examples/sec; 0.042 sec/batch; 1h:42m:54s remains)
INFO - root - 2019-11-06 20:10:46.610314: step 2220, total loss = 1.17, predict loss = 0.35 (95.5 examples/sec; 0.042 sec/batch; 1h:43m:12s remains)
INFO - root - 2019-11-06 20:10:47.631676: step 2230, total loss = 0.52, predict loss = 0.14 (62.4 examples/sec; 0.064 sec/batch; 2h:37m:55s remains)
INFO - root - 2019-11-06 20:10:48.274661: step 2240, total loss = 0.68, predict loss = 0.18 (78.6 examples/sec; 0.051 sec/batch; 2h:05m:15s remains)
INFO - root - 2019-11-06 20:10:48.901754: step 2250, total loss = 1.14, predict loss = 0.34 (73.4 examples/sec; 0.055 sec/batch; 2h:14m:17s remains)
INFO - root - 2019-11-06 20:10:49.486225: step 2260, total loss = 0.70, predict loss = 0.19 (76.4 examples/sec; 0.052 sec/batch; 2h:09m:00s remains)
INFO - root - 2019-11-06 20:10:50.046150: step 2270, total loss = 0.44, predict loss = 0.11 (77.1 examples/sec; 0.052 sec/batch; 2h:07m:43s remains)
INFO - root - 2019-11-06 20:10:50.626383: step 2280, total loss = 0.52, predict loss = 0.15 (77.0 examples/sec; 0.052 sec/batch; 2h:07m:52s remains)
INFO - root - 2019-11-06 20:10:51.212186: step 2290, total loss = 0.64, predict loss = 0.21 (81.5 examples/sec; 0.049 sec/batch; 2h:00m:47s remains)
INFO - root - 2019-11-06 20:10:51.791182: step 2300, total loss = 0.57, predict loss = 0.16 (78.2 examples/sec; 0.051 sec/batch; 2h:05m:54s remains)
INFO - root - 2019-11-06 20:10:52.370804: step 2310, total loss = 0.69, predict loss = 0.18 (77.2 examples/sec; 0.052 sec/batch; 2h:07m:35s remains)
INFO - root - 2019-11-06 20:10:52.950622: step 2320, total loss = 0.41, predict loss = 0.11 (78.5 examples/sec; 0.051 sec/batch; 2h:05m:21s remains)
INFO - root - 2019-11-06 20:10:53.544738: step 2330, total loss = 0.70, predict loss = 0.19 (78.5 examples/sec; 0.051 sec/batch; 2h:05m:26s remains)
INFO - root - 2019-11-06 20:10:54.121985: step 2340, total loss = 0.44, predict loss = 0.12 (77.5 examples/sec; 0.052 sec/batch; 2h:07m:04s remains)
INFO - root - 2019-11-06 20:10:54.614999: step 2350, total loss = 0.58, predict loss = 0.16 (96.4 examples/sec; 0.042 sec/batch; 1h:42m:08s remains)
INFO - root - 2019-11-06 20:10:55.066865: step 2360, total loss = 0.48, predict loss = 0.14 (92.5 examples/sec; 0.043 sec/batch; 1h:46m:24s remains)
INFO - root - 2019-11-06 20:10:55.548216: step 2370, total loss = 0.71, predict loss = 0.19 (89.7 examples/sec; 0.045 sec/batch; 1h:49m:39s remains)
INFO - root - 2019-11-06 20:10:56.608670: step 2380, total loss = 0.51, predict loss = 0.15 (67.4 examples/sec; 0.059 sec/batch; 2h:26m:01s remains)
INFO - root - 2019-11-06 20:10:57.212872: step 2390, total loss = 0.92, predict loss = 0.31 (79.6 examples/sec; 0.050 sec/batch; 2h:03m:40s remains)
INFO - root - 2019-11-06 20:10:57.779952: step 2400, total loss = 0.61, predict loss = 0.15 (78.7 examples/sec; 0.051 sec/batch; 2h:04m:59s remains)
INFO - root - 2019-11-06 20:10:58.371446: step 2410, total loss = 0.79, predict loss = 0.19 (80.1 examples/sec; 0.050 sec/batch; 2h:02m:53s remains)
INFO - root - 2019-11-06 20:10:58.950022: step 2420, total loss = 0.57, predict loss = 0.17 (78.3 examples/sec; 0.051 sec/batch; 2h:05m:39s remains)
INFO - root - 2019-11-06 20:10:59.521716: step 2430, total loss = 0.61, predict loss = 0.18 (83.0 examples/sec; 0.048 sec/batch; 1h:58m:33s remains)
INFO - root - 2019-11-06 20:11:00.091272: step 2440, total loss = 1.08, predict loss = 0.28 (77.2 examples/sec; 0.052 sec/batch; 2h:07m:25s remains)
INFO - root - 2019-11-06 20:11:00.685894: step 2450, total loss = 0.42, predict loss = 0.11 (81.2 examples/sec; 0.049 sec/batch; 2h:01m:08s remains)
INFO - root - 2019-11-06 20:11:01.257402: step 2460, total loss = 0.48, predict loss = 0.12 (78.7 examples/sec; 0.051 sec/batch; 2h:04m:57s remains)
INFO - root - 2019-11-06 20:11:01.824616: step 2470, total loss = 0.72, predict loss = 0.21 (78.9 examples/sec; 0.051 sec/batch; 2h:04m:41s remains)
INFO - root - 2019-11-06 20:11:02.398296: step 2480, total loss = 0.72, predict loss = 0.19 (72.9 examples/sec; 0.055 sec/batch; 2h:14m:52s remains)
INFO - root - 2019-11-06 20:11:02.976807: step 2490, total loss = 0.58, predict loss = 0.16 (81.9 examples/sec; 0.049 sec/batch; 2h:00m:06s remains)
INFO - root - 2019-11-06 20:11:03.446088: step 2500, total loss = 1.17, predict loss = 0.34 (97.5 examples/sec; 0.041 sec/batch; 1h:40m:52s remains)
INFO - root - 2019-11-06 20:11:03.916434: step 2510, total loss = 0.61, predict loss = 0.16 (91.8 examples/sec; 0.044 sec/batch; 1h:47m:09s remains)
INFO - root - 2019-11-06 20:11:04.830456: step 2520, total loss = 0.48, predict loss = 0.14 (75.6 examples/sec; 0.053 sec/batch; 2h:09m:59s remains)
INFO - root - 2019-11-06 20:11:05.513963: step 2530, total loss = 0.54, predict loss = 0.15 (62.9 examples/sec; 0.064 sec/batch; 2h:36m:17s remains)
INFO - root - 2019-11-06 20:11:06.126898: step 2540, total loss = 0.64, predict loss = 0.17 (72.9 examples/sec; 0.055 sec/batch; 2h:14m:51s remains)
INFO - root - 2019-11-06 20:11:06.695813: step 2550, total loss = 1.18, predict loss = 0.32 (84.2 examples/sec; 0.048 sec/batch; 1h:56m:43s remains)
INFO - root - 2019-11-06 20:11:07.268906: step 2560, total loss = 0.51, predict loss = 0.14 (78.1 examples/sec; 0.051 sec/batch; 2h:05m:50s remains)
INFO - root - 2019-11-06 20:11:07.856185: step 2570, total loss = 0.57, predict loss = 0.16 (73.6 examples/sec; 0.054 sec/batch; 2h:13m:27s remains)
INFO - root - 2019-11-06 20:11:08.430709: step 2580, total loss = 1.11, predict loss = 0.29 (75.5 examples/sec; 0.053 sec/batch; 2h:10m:07s remains)
INFO - root - 2019-11-06 20:11:08.999948: step 2590, total loss = 0.46, predict loss = 0.12 (81.1 examples/sec; 0.049 sec/batch; 2h:01m:12s remains)
INFO - root - 2019-11-06 20:11:09.575024: step 2600, total loss = 0.78, predict loss = 0.22 (80.3 examples/sec; 0.050 sec/batch; 2h:02m:26s remains)
INFO - root - 2019-11-06 20:11:10.163170: step 2610, total loss = 0.44, predict loss = 0.13 (77.5 examples/sec; 0.052 sec/batch; 2h:06m:47s remains)
INFO - root - 2019-11-06 20:11:10.736229: step 2620, total loss = 0.68, predict loss = 0.18 (80.6 examples/sec; 0.050 sec/batch; 2h:01m:50s remains)
INFO - root - 2019-11-06 20:11:11.298504: step 2630, total loss = 0.51, predict loss = 0.15 (81.4 examples/sec; 0.049 sec/batch; 2h:00m:39s remains)
INFO - root - 2019-11-06 20:11:11.868406: step 2640, total loss = 0.73, predict loss = 0.19 (91.1 examples/sec; 0.044 sec/batch; 1h:47m:53s remains)
INFO - root - 2019-11-06 20:11:12.344627: step 2650, total loss = 0.42, predict loss = 0.11 (100.5 examples/sec; 0.040 sec/batch; 1h:37m:43s remains)
INFO - root - 2019-11-06 20:11:12.810581: step 2660, total loss = 0.56, predict loss = 0.16 (93.8 examples/sec; 0.043 sec/batch; 1h:44m:41s remains)
INFO - root - 2019-11-06 20:11:13.740984: step 2670, total loss = 0.60, predict loss = 0.16 (73.9 examples/sec; 0.054 sec/batch; 2h:12m:50s remains)
INFO - root - 2019-11-06 20:11:14.382774: step 2680, total loss = 0.76, predict loss = 0.21 (75.5 examples/sec; 0.053 sec/batch; 2h:10m:10s remains)
INFO - root - 2019-11-06 20:11:14.994591: step 2690, total loss = 0.60, predict loss = 0.16 (73.8 examples/sec; 0.054 sec/batch; 2h:13m:08s remains)
INFO - root - 2019-11-06 20:11:15.614415: step 2700, total loss = 0.52, predict loss = 0.15 (73.8 examples/sec; 0.054 sec/batch; 2h:13m:01s remains)
INFO - root - 2019-11-06 20:11:16.191724: step 2710, total loss = 0.38, predict loss = 0.10 (75.8 examples/sec; 0.053 sec/batch; 2h:09m:28s remains)
INFO - root - 2019-11-06 20:11:16.773633: step 2720, total loss = 0.84, predict loss = 0.25 (75.7 examples/sec; 0.053 sec/batch; 2h:09m:41s remains)
INFO - root - 2019-11-06 20:11:17.353925: step 2730, total loss = 0.63, predict loss = 0.16 (76.3 examples/sec; 0.052 sec/batch; 2h:08m:44s remains)
INFO - root - 2019-11-06 20:11:17.926261: step 2740, total loss = 0.65, predict loss = 0.16 (77.1 examples/sec; 0.052 sec/batch; 2h:07m:16s remains)
INFO - root - 2019-11-06 20:11:18.492604: step 2750, total loss = 1.51, predict loss = 0.45 (78.6 examples/sec; 0.051 sec/batch; 2h:04m:48s remains)
INFO - root - 2019-11-06 20:11:19.058242: step 2760, total loss = 0.91, predict loss = 0.25 (77.7 examples/sec; 0.051 sec/batch; 2h:06m:22s remains)
INFO - root - 2019-11-06 20:11:19.647401: step 2770, total loss = 0.44, predict loss = 0.12 (78.1 examples/sec; 0.051 sec/batch; 2h:05m:37s remains)
INFO - root - 2019-11-06 20:11:20.218805: step 2780, total loss = 1.13, predict loss = 0.31 (77.8 examples/sec; 0.051 sec/batch; 2h:06m:04s remains)
INFO - root - 2019-11-06 20:11:20.751739: step 2790, total loss = 0.78, predict loss = 0.20 (97.0 examples/sec; 0.041 sec/batch; 1h:41m:11s remains)
INFO - root - 2019-11-06 20:11:21.208311: step 2800, total loss = 1.09, predict loss = 0.30 (97.1 examples/sec; 0.041 sec/batch; 1h:41m:01s remains)
INFO - root - 2019-11-06 20:11:21.688371: step 2810, total loss = 0.49, predict loss = 0.11 (91.6 examples/sec; 0.044 sec/batch; 1h:47m:08s remains)
INFO - root - 2019-11-06 20:11:22.662485: step 2820, total loss = 0.57, predict loss = 0.14 (66.5 examples/sec; 0.060 sec/batch; 2h:27m:27s remains)
INFO - root - 2019-11-06 20:11:23.302223: step 2830, total loss = 0.98, predict loss = 0.29 (72.6 examples/sec; 0.055 sec/batch; 2h:15m:06s remains)
INFO - root - 2019-11-06 20:11:23.882767: step 2840, total loss = 0.62, predict loss = 0.16 (79.7 examples/sec; 0.050 sec/batch; 2h:03m:01s remains)
INFO - root - 2019-11-06 20:11:24.464035: step 2850, total loss = 0.77, predict loss = 0.23 (80.2 examples/sec; 0.050 sec/batch; 2h:02m:22s remains)
INFO - root - 2019-11-06 20:11:25.039585: step 2860, total loss = 0.54, predict loss = 0.16 (82.3 examples/sec; 0.049 sec/batch; 1h:59m:08s remains)
INFO - root - 2019-11-06 20:11:25.606231: step 2870, total loss = 0.47, predict loss = 0.13 (79.5 examples/sec; 0.050 sec/batch; 2h:03m:24s remains)
INFO - root - 2019-11-06 20:11:26.174235: step 2880, total loss = 0.50, predict loss = 0.14 (79.5 examples/sec; 0.050 sec/batch; 2h:03m:26s remains)
INFO - root - 2019-11-06 20:11:26.774047: step 2890, total loss = 0.69, predict loss = 0.19 (73.9 examples/sec; 0.054 sec/batch; 2h:12m:37s remains)
INFO - root - 2019-11-06 20:11:27.351879: step 2900, total loss = 0.58, predict loss = 0.16 (79.1 examples/sec; 0.051 sec/batch; 2h:03m:59s remains)
INFO - root - 2019-11-06 20:11:27.913506: step 2910, total loss = 0.42, predict loss = 0.11 (82.0 examples/sec; 0.049 sec/batch; 1h:59m:36s remains)
INFO - root - 2019-11-06 20:11:28.480861: step 2920, total loss = 0.85, predict loss = 0.23 (78.3 examples/sec; 0.051 sec/batch; 2h:05m:16s remains)
INFO - root - 2019-11-06 20:11:29.080842: step 2930, total loss = 0.46, predict loss = 0.12 (72.9 examples/sec; 0.055 sec/batch; 2h:14m:26s remains)
INFO - root - 2019-11-06 20:11:29.611849: step 2940, total loss = 0.71, predict loss = 0.19 (95.3 examples/sec; 0.042 sec/batch; 1h:42m:51s remains)
INFO - root - 2019-11-06 20:11:30.060645: step 2950, total loss = 0.60, predict loss = 0.17 (93.6 examples/sec; 0.043 sec/batch; 1h:44m:42s remains)
INFO - root - 2019-11-06 20:11:30.515984: step 2960, total loss = 0.81, predict loss = 0.24 (93.8 examples/sec; 0.043 sec/batch; 1h:44m:33s remains)
INFO - root - 2019-11-06 20:11:31.563680: step 2970, total loss = 0.45, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 2h:34m:24s remains)
INFO - root - 2019-11-06 20:11:32.191103: step 2980, total loss = 0.62, predict loss = 0.16 (77.0 examples/sec; 0.052 sec/batch; 2h:07m:16s remains)
INFO - root - 2019-11-06 20:11:32.762996: step 2990, total loss = 0.76, predict loss = 0.20 (78.0 examples/sec; 0.051 sec/batch; 2h:05m:37s remains)
INFO - root - 2019-11-06 20:11:33.345001: step 3000, total loss = 0.50, predict loss = 0.12 (76.8 examples/sec; 0.052 sec/batch; 2h:07m:37s remains)
INFO - root - 2019-11-06 20:11:33.931156: step 3010, total loss = 0.54, predict loss = 0.13 (83.2 examples/sec; 0.048 sec/batch; 1h:57m:43s remains)
INFO - root - 2019-11-06 20:11:34.501283: step 3020, total loss = 0.60, predict loss = 0.18 (78.6 examples/sec; 0.051 sec/batch; 2h:04m:41s remains)
INFO - root - 2019-11-06 20:11:35.080832: step 3030, total loss = 0.54, predict loss = 0.16 (75.6 examples/sec; 0.053 sec/batch; 2h:09m:32s remains)
INFO - root - 2019-11-06 20:11:35.655808: step 3040, total loss = 0.66, predict loss = 0.17 (79.2 examples/sec; 0.051 sec/batch; 2h:03m:46s remains)
INFO - root - 2019-11-06 20:11:36.242331: step 3050, total loss = 0.51, predict loss = 0.14 (78.4 examples/sec; 0.051 sec/batch; 2h:04m:54s remains)
INFO - root - 2019-11-06 20:11:36.825331: step 3060, total loss = 0.76, predict loss = 0.20 (81.4 examples/sec; 0.049 sec/batch; 2h:00m:20s remains)
INFO - root - 2019-11-06 20:11:37.390017: step 3070, total loss = 0.85, predict loss = 0.23 (79.6 examples/sec; 0.050 sec/batch; 2h:03m:07s remains)
INFO - root - 2019-11-06 20:11:37.965384: step 3080, total loss = 0.41, predict loss = 0.11 (78.2 examples/sec; 0.051 sec/batch; 2h:05m:13s remains)
INFO - root - 2019-11-06 20:11:38.493530: step 3090, total loss = 0.47, predict loss = 0.13 (99.4 examples/sec; 0.040 sec/batch; 1h:38m:29s remains)
INFO - root - 2019-11-06 20:11:38.931499: step 3100, total loss = 0.48, predict loss = 0.13 (103.6 examples/sec; 0.039 sec/batch; 1h:34m:29s remains)
INFO - root - 2019-11-06 20:11:39.380943: step 3110, total loss = 0.68, predict loss = 0.18 (91.4 examples/sec; 0.044 sec/batch; 1h:47m:09s remains)
INFO - root - 2019-11-06 20:11:40.505532: step 3120, total loss = 0.57, predict loss = 0.16 (57.8 examples/sec; 0.069 sec/batch; 2h:49m:26s remains)
INFO - root - 2019-11-06 20:11:41.218352: step 3130, total loss = 0.58, predict loss = 0.18 (65.9 examples/sec; 0.061 sec/batch; 2h:28m:32s remains)
INFO - root - 2019-11-06 20:11:41.824332: step 3140, total loss = 0.90, predict loss = 0.25 (82.3 examples/sec; 0.049 sec/batch; 1h:58m:59s remains)
INFO - root - 2019-11-06 20:11:42.414546: step 3150, total loss = 0.49, predict loss = 0.15 (74.6 examples/sec; 0.054 sec/batch; 2h:11m:18s remains)
INFO - root - 2019-11-06 20:11:42.989630: step 3160, total loss = 0.86, predict loss = 0.23 (79.7 examples/sec; 0.050 sec/batch; 2h:02m:47s remains)
INFO - root - 2019-11-06 20:11:43.584711: step 3170, total loss = 0.49, predict loss = 0.13 (80.4 examples/sec; 0.050 sec/batch; 2h:01m:40s remains)
INFO - root - 2019-11-06 20:11:44.144622: step 3180, total loss = 0.55, predict loss = 0.16 (76.7 examples/sec; 0.052 sec/batch; 2h:07m:41s remains)
INFO - root - 2019-11-06 20:11:44.721205: step 3190, total loss = 0.58, predict loss = 0.15 (79.8 examples/sec; 0.050 sec/batch; 2h:02m:34s remains)
INFO - root - 2019-11-06 20:11:45.327298: step 3200, total loss = 0.64, predict loss = 0.18 (78.9 examples/sec; 0.051 sec/batch; 2h:04m:05s remains)
INFO - root - 2019-11-06 20:11:45.927823: step 3210, total loss = 0.51, predict loss = 0.14 (78.4 examples/sec; 0.051 sec/batch; 2h:04m:49s remains)
INFO - root - 2019-11-06 20:11:46.495934: step 3220, total loss = 0.55, predict loss = 0.13 (74.8 examples/sec; 0.053 sec/batch; 2h:10m:52s remains)
INFO - root - 2019-11-06 20:11:47.070951: step 3230, total loss = 0.47, predict loss = 0.13 (78.4 examples/sec; 0.051 sec/batch; 2h:04m:46s remains)
INFO - root - 2019-11-06 20:11:47.556877: step 3240, total loss = 0.43, predict loss = 0.11 (98.4 examples/sec; 0.041 sec/batch; 1h:39m:27s remains)
INFO - root - 2019-11-06 20:11:48.018930: step 3250, total loss = 0.47, predict loss = 0.13 (98.6 examples/sec; 0.041 sec/batch; 1h:39m:16s remains)
INFO - root - 2019-11-06 20:11:48.907084: step 3260, total loss = 0.57, predict loss = 0.17 (8.4 examples/sec; 0.477 sec/batch; 19h:26m:30s remains)
INFO - root - 2019-11-06 20:11:49.621290: step 3270, total loss = 0.74, predict loss = 0.21 (54.0 examples/sec; 0.074 sec/batch; 3h:01m:01s remains)
INFO - root - 2019-11-06 20:11:50.252000: step 3280, total loss = 0.77, predict loss = 0.22 (82.0 examples/sec; 0.049 sec/batch; 1h:59m:12s remains)
INFO - root - 2019-11-06 20:11:50.839215: step 3290, total loss = 0.42, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 2h:06m:04s remains)
INFO - root - 2019-11-06 20:11:51.401307: step 3300, total loss = 0.50, predict loss = 0.14 (82.3 examples/sec; 0.049 sec/batch; 1h:58m:49s remains)
INFO - root - 2019-11-06 20:11:51.968114: step 3310, total loss = 0.68, predict loss = 0.19 (80.3 examples/sec; 0.050 sec/batch; 2h:01m:49s remains)
INFO - root - 2019-11-06 20:11:52.533542: step 3320, total loss = 0.54, predict loss = 0.12 (79.1 examples/sec; 0.051 sec/batch; 2h:03m:33s remains)
INFO - root - 2019-11-06 20:11:53.124885: step 3330, total loss = 0.81, predict loss = 0.23 (77.5 examples/sec; 0.052 sec/batch; 2h:06m:14s remains)
INFO - root - 2019-11-06 20:11:53.704182: step 3340, total loss = 0.55, predict loss = 0.15 (80.5 examples/sec; 0.050 sec/batch; 2h:01m:23s remains)
INFO - root - 2019-11-06 20:11:54.269347: step 3350, total loss = 0.66, predict loss = 0.17 (78.1 examples/sec; 0.051 sec/batch; 2h:05m:11s remains)
INFO - root - 2019-11-06 20:11:54.839528: step 3360, total loss = 0.73, predict loss = 0.18 (82.5 examples/sec; 0.048 sec/batch; 1h:58m:26s remains)
INFO - root - 2019-11-06 20:11:55.419377: step 3370, total loss = 0.55, predict loss = 0.14 (79.1 examples/sec; 0.051 sec/batch; 2h:03m:36s remains)
INFO - root - 2019-11-06 20:11:56.001476: step 3380, total loss = 0.74, predict loss = 0.20 (85.9 examples/sec; 0.047 sec/batch; 1h:53m:49s remains)
INFO - root - 2019-11-06 20:11:56.464600: step 3390, total loss = 0.55, predict loss = 0.14 (96.6 examples/sec; 0.041 sec/batch; 1h:41m:08s remains)
INFO - root - 2019-11-06 20:11:56.903862: step 3400, total loss = 0.79, predict loss = 0.22 (98.3 examples/sec; 0.041 sec/batch; 1h:39m:25s remains)
INFO - root - 2019-11-06 20:11:57.835525: step 3410, total loss = 0.98, predict loss = 0.27 (79.3 examples/sec; 0.050 sec/batch; 2h:03m:11s remains)
INFO - root - 2019-11-06 20:11:58.488159: step 3420, total loss = 0.43, predict loss = 0.13 (64.2 examples/sec; 0.062 sec/batch; 2h:32m:17s remains)
INFO - root - 2019-11-06 20:11:59.081111: step 3430, total loss = 0.73, predict loss = 0.20 (85.2 examples/sec; 0.047 sec/batch; 1h:54m:44s remains)
INFO - root - 2019-11-06 20:11:59.652542: step 3440, total loss = 0.73, predict loss = 0.17 (81.9 examples/sec; 0.049 sec/batch; 1h:59m:20s remains)
INFO - root - 2019-11-06 20:12:00.236188: step 3450, total loss = 1.10, predict loss = 0.29 (80.4 examples/sec; 0.050 sec/batch; 2h:01m:29s remains)
INFO - root - 2019-11-06 20:12:00.798472: step 3460, total loss = 0.64, predict loss = 0.17 (80.2 examples/sec; 0.050 sec/batch; 2h:01m:49s remains)
INFO - root - 2019-11-06 20:12:01.388743: step 3470, total loss = 0.84, predict loss = 0.25 (77.3 examples/sec; 0.052 sec/batch; 2h:06m:23s remains)
INFO - root - 2019-11-06 20:12:01.966610: step 3480, total loss = 0.43, predict loss = 0.13 (76.2 examples/sec; 0.053 sec/batch; 2h:08m:15s remains)
INFO - root - 2019-11-06 20:12:02.556883: step 3490, total loss = 0.73, predict loss = 0.16 (76.6 examples/sec; 0.052 sec/batch; 2h:07m:28s remains)
INFO - root - 2019-11-06 20:12:03.133808: step 3500, total loss = 0.55, predict loss = 0.17 (81.2 examples/sec; 0.049 sec/batch; 2h:00m:19s remains)
INFO - root - 2019-11-06 20:12:03.715178: step 3510, total loss = 0.91, predict loss = 0.26 (81.7 examples/sec; 0.049 sec/batch; 1h:59m:36s remains)
INFO - root - 2019-11-06 20:12:04.286775: step 3520, total loss = 0.66, predict loss = 0.18 (80.9 examples/sec; 0.049 sec/batch; 2h:00m:44s remains)
INFO - root - 2019-11-06 20:12:04.852036: step 3530, total loss = 0.69, predict loss = 0.18 (96.6 examples/sec; 0.041 sec/batch; 1h:41m:07s remains)
INFO - root - 2019-11-06 20:12:05.317128: step 3540, total loss = 0.45, predict loss = 0.11 (94.4 examples/sec; 0.042 sec/batch; 1h:43m:29s remains)
INFO - root - 2019-11-06 20:12:05.790430: step 3550, total loss = 0.56, predict loss = 0.16 (92.3 examples/sec; 0.043 sec/batch; 1h:45m:44s remains)
INFO - root - 2019-11-06 20:12:06.746166: step 3560, total loss = 0.61, predict loss = 0.16 (69.8 examples/sec; 0.057 sec/batch; 2h:19m:49s remains)
INFO - root - 2019-11-06 20:12:07.433713: step 3570, total loss = 0.47, predict loss = 0.13 (68.2 examples/sec; 0.059 sec/batch; 2h:23m:06s remains)
INFO - root - 2019-11-06 20:12:08.048071: step 3580, total loss = 0.41, predict loss = 0.10 (79.4 examples/sec; 0.050 sec/batch; 2h:02m:55s remains)
INFO - root - 2019-11-06 20:12:08.641215: step 3590, total loss = 0.47, predict loss = 0.12 (79.1 examples/sec; 0.051 sec/batch; 2h:03m:22s remains)
INFO - root - 2019-11-06 20:12:09.219856: step 3600, total loss = 0.69, predict loss = 0.19 (77.6 examples/sec; 0.052 sec/batch; 2h:05m:48s remains)
INFO - root - 2019-11-06 20:12:09.810413: step 3610, total loss = 0.66, predict loss = 0.18 (78.8 examples/sec; 0.051 sec/batch; 2h:03m:48s remains)
INFO - root - 2019-11-06 20:12:10.384634: step 3620, total loss = 0.75, predict loss = 0.20 (77.3 examples/sec; 0.052 sec/batch; 2h:06m:11s remains)
INFO - root - 2019-11-06 20:12:10.946337: step 3630, total loss = 0.43, predict loss = 0.11 (80.1 examples/sec; 0.050 sec/batch; 2h:01m:47s remains)
INFO - root - 2019-11-06 20:12:11.509709: step 3640, total loss = 0.61, predict loss = 0.17 (83.4 examples/sec; 0.048 sec/batch; 1h:56m:56s remains)
INFO - root - 2019-11-06 20:12:12.098561: step 3650, total loss = 0.54, predict loss = 0.13 (78.6 examples/sec; 0.051 sec/batch; 2h:04m:09s remains)
INFO - root - 2019-11-06 20:12:12.678158: step 3660, total loss = 0.54, predict loss = 0.12 (78.5 examples/sec; 0.051 sec/batch; 2h:04m:19s remains)
INFO - root - 2019-11-06 20:12:13.244046: step 3670, total loss = 0.40, predict loss = 0.10 (76.1 examples/sec; 0.053 sec/batch; 2h:08m:08s remains)
INFO - root - 2019-11-06 20:12:13.776350: step 3680, total loss = 0.59, predict loss = 0.17 (96.7 examples/sec; 0.041 sec/batch; 1h:40m:55s remains)
INFO - root - 2019-11-06 20:12:14.239353: step 3690, total loss = 0.48, predict loss = 0.14 (94.2 examples/sec; 0.042 sec/batch; 1h:43m:33s remains)
INFO - root - 2019-11-06 20:12:14.691883: step 3700, total loss = 1.08, predict loss = 0.31 (97.2 examples/sec; 0.041 sec/batch; 1h:40m:22s remains)
INFO - root - 2019-11-06 20:12:15.744235: step 3710, total loss = 0.58, predict loss = 0.14 (64.9 examples/sec; 0.062 sec/batch; 2h:30m:19s remains)
INFO - root - 2019-11-06 20:12:16.363158: step 3720, total loss = 0.76, predict loss = 0.21 (76.3 examples/sec; 0.052 sec/batch; 2h:07m:47s remains)
INFO - root - 2019-11-06 20:12:16.955835: step 3730, total loss = 0.46, predict loss = 0.11 (76.5 examples/sec; 0.052 sec/batch; 2h:07m:25s remains)
INFO - root - 2019-11-06 20:12:17.533991: step 3740, total loss = 0.69, predict loss = 0.18 (83.3 examples/sec; 0.048 sec/batch; 1h:57m:05s remains)
INFO - root - 2019-11-06 20:12:18.099858: step 3750, total loss = 0.45, predict loss = 0.12 (80.1 examples/sec; 0.050 sec/batch; 2h:01m:40s remains)
INFO - root - 2019-11-06 20:12:18.668872: step 3760, total loss = 0.92, predict loss = 0.24 (80.1 examples/sec; 0.050 sec/batch; 2h:01m:41s remains)
INFO - root - 2019-11-06 20:12:19.265789: step 3770, total loss = 0.60, predict loss = 0.14 (76.2 examples/sec; 0.052 sec/batch; 2h:07m:56s remains)
INFO - root - 2019-11-06 20:12:19.839749: step 3780, total loss = 1.02, predict loss = 0.27 (79.9 examples/sec; 0.050 sec/batch; 2h:02m:01s remains)
INFO - root - 2019-11-06 20:12:20.415952: step 3790, total loss = 1.07, predict loss = 0.29 (76.8 examples/sec; 0.052 sec/batch; 2h:07m:00s remains)
INFO - root - 2019-11-06 20:12:20.996405: step 3800, total loss = 0.80, predict loss = 0.24 (78.9 examples/sec; 0.051 sec/batch; 2h:03m:31s remains)
INFO - root - 2019-11-06 20:12:21.585664: step 3810, total loss = 0.60, predict loss = 0.16 (79.1 examples/sec; 0.051 sec/batch; 2h:03m:15s remains)
INFO - root - 2019-11-06 20:12:22.167937: step 3820, total loss = 0.60, predict loss = 0.16 (80.0 examples/sec; 0.050 sec/batch; 2h:01m:47s remains)
INFO - root - 2019-11-06 20:12:22.687419: step 3830, total loss = 0.85, predict loss = 0.26 (100.0 examples/sec; 0.040 sec/batch; 1h:37m:29s remains)
INFO - root - 2019-11-06 20:12:23.132116: step 3840, total loss = 0.42, predict loss = 0.11 (96.3 examples/sec; 0.042 sec/batch; 1h:41m:13s remains)
INFO - root - 2019-11-06 20:12:23.612726: step 3850, total loss = 0.91, predict loss = 0.25 (99.6 examples/sec; 0.040 sec/batch; 1h:37m:49s remains)
INFO - root - 2019-11-06 20:12:24.724980: step 3860, total loss = 0.73, predict loss = 0.22 (58.7 examples/sec; 0.068 sec/batch; 2h:45m:51s remains)
INFO - root - 2019-11-06 20:12:25.416176: step 3870, total loss = 0.44, predict loss = 0.10 (65.1 examples/sec; 0.061 sec/batch; 2h:29m:43s remains)
INFO - root - 2019-11-06 20:12:26.018803: step 3880, total loss = 0.81, predict loss = 0.20 (79.3 examples/sec; 0.050 sec/batch; 2h:02m:48s remains)
INFO - root - 2019-11-06 20:12:26.586353: step 3890, total loss = 0.48, predict loss = 0.14 (83.3 examples/sec; 0.048 sec/batch; 1h:56m:58s remains)
INFO - root - 2019-11-06 20:12:27.149455: step 3900, total loss = 0.48, predict loss = 0.12 (77.0 examples/sec; 0.052 sec/batch; 2h:06m:28s remains)
INFO - root - 2019-11-06 20:12:27.717626: step 3910, total loss = 0.51, predict loss = 0.14 (78.5 examples/sec; 0.051 sec/batch; 2h:04m:08s remains)
INFO - root - 2019-11-06 20:12:28.300460: step 3920, total loss = 0.76, predict loss = 0.21 (72.9 examples/sec; 0.055 sec/batch; 2h:13m:34s remains)
INFO - root - 2019-11-06 20:12:28.893424: step 3930, total loss = 0.74, predict loss = 0.20 (78.1 examples/sec; 0.051 sec/batch; 2h:04m:36s remains)
INFO - root - 2019-11-06 20:12:29.472522: step 3940, total loss = 0.48, predict loss = 0.13 (76.4 examples/sec; 0.052 sec/batch; 2h:07m:25s remains)
INFO - root - 2019-11-06 20:12:30.041530: step 3950, total loss = 0.88, predict loss = 0.28 (83.5 examples/sec; 0.048 sec/batch; 1h:56m:37s remains)
INFO - root - 2019-11-06 20:12:30.597397: step 3960, total loss = 0.60, predict loss = 0.16 (79.1 examples/sec; 0.051 sec/batch; 2h:03m:05s remains)
INFO - root - 2019-11-06 20:12:31.191998: step 3970, total loss = 0.39, predict loss = 0.12 (77.4 examples/sec; 0.052 sec/batch; 2h:05m:44s remains)
INFO - root - 2019-11-06 20:12:31.685571: step 3980, total loss = 0.91, predict loss = 0.24 (98.1 examples/sec; 0.041 sec/batch; 1h:39m:13s remains)
INFO - root - 2019-11-06 20:12:32.147923: step 3990, total loss = 0.49, predict loss = 0.15 (94.1 examples/sec; 0.043 sec/batch; 1h:43m:26s remains)
INFO - root - 2019-11-06 20:12:32.611001: step 4000, total loss = 0.53, predict loss = 0.14 (91.7 examples/sec; 0.044 sec/batch; 1h:46m:08s remains)
INFO - root - 2019-11-06 20:12:33.731214: step 4010, total loss = 0.59, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 2h:29m:28s remains)
INFO - root - 2019-11-06 20:12:34.354897: step 4020, total loss = 0.47, predict loss = 0.13 (79.2 examples/sec; 0.051 sec/batch; 2h:02m:52s remains)
INFO - root - 2019-11-06 20:12:34.937073: step 4030, total loss = 0.46, predict loss = 0.11 (78.9 examples/sec; 0.051 sec/batch; 2h:03m:20s remains)
INFO - root - 2019-11-06 20:12:35.513533: step 4040, total loss = 0.49, predict loss = 0.14 (75.9 examples/sec; 0.053 sec/batch; 2h:08m:15s remains)
INFO - root - 2019-11-06 20:12:36.116064: step 4050, total loss = 0.79, predict loss = 0.22 (76.0 examples/sec; 0.053 sec/batch; 2h:08m:02s remains)
INFO - root - 2019-11-06 20:12:36.664791: step 4060, total loss = 0.58, predict loss = 0.15 (78.8 examples/sec; 0.051 sec/batch; 2h:03m:31s remains)
INFO - root - 2019-11-06 20:12:37.246747: step 4070, total loss = 0.65, predict loss = 0.17 (74.7 examples/sec; 0.054 sec/batch; 2h:10m:16s remains)
INFO - root - 2019-11-06 20:12:37.819379: step 4080, total loss = 0.68, predict loss = 0.20 (76.8 examples/sec; 0.052 sec/batch; 2h:06m:39s remains)
INFO - root - 2019-11-06 20:12:38.406102: step 4090, total loss = 0.58, predict loss = 0.15 (76.7 examples/sec; 0.052 sec/batch; 2h:06m:52s remains)
INFO - root - 2019-11-06 20:12:38.987256: step 4100, total loss = 0.71, predict loss = 0.18 (76.8 examples/sec; 0.052 sec/batch; 2h:06m:35s remains)
INFO - root - 2019-11-06 20:12:39.559357: step 4110, total loss = 0.56, predict loss = 0.14 (80.3 examples/sec; 0.050 sec/batch; 2h:01m:04s remains)
INFO - root - 2019-11-06 20:12:40.140210: step 4120, total loss = 0.52, predict loss = 0.15 (79.2 examples/sec; 0.050 sec/batch; 2h:02m:45s remains)
INFO - root - 2019-11-06 20:12:40.639481: step 4130, total loss = 0.52, predict loss = 0.15 (89.0 examples/sec; 0.045 sec/batch; 1h:49m:16s remains)
INFO - root - 2019-11-06 20:12:41.111027: step 4140, total loss = 0.65, predict loss = 0.18 (91.3 examples/sec; 0.044 sec/batch; 1h:46m:29s remains)
INFO - root - 2019-11-06 20:12:42.005738: step 4150, total loss = 0.56, predict loss = 0.17 (78.8 examples/sec; 0.051 sec/batch; 2h:03m:22s remains)
INFO - root - 2019-11-06 20:12:42.685567: step 4160, total loss = 0.52, predict loss = 0.12 (65.2 examples/sec; 0.061 sec/batch; 2h:29m:10s remains)
INFO - root - 2019-11-06 20:12:43.386086: step 4170, total loss = 0.69, predict loss = 0.19 (67.7 examples/sec; 0.059 sec/batch; 2h:23m:32s remains)
INFO - root - 2019-11-06 20:12:43.992777: step 4180, total loss = 0.84, predict loss = 0.26 (77.9 examples/sec; 0.051 sec/batch; 2h:04m:46s remains)
INFO - root - 2019-11-06 20:12:44.558713: step 4190, total loss = 0.39, predict loss = 0.11 (79.1 examples/sec; 0.051 sec/batch; 2h:02m:50s remains)
INFO - root - 2019-11-06 20:12:45.179432: step 4200, total loss = 0.62, predict loss = 0.17 (71.9 examples/sec; 0.056 sec/batch; 2h:15m:15s remains)
INFO - root - 2019-11-06 20:12:45.770816: step 4210, total loss = 0.63, predict loss = 0.17 (80.7 examples/sec; 0.050 sec/batch; 2h:00m:29s remains)
INFO - root - 2019-11-06 20:12:46.348913: step 4220, total loss = 0.43, predict loss = 0.12 (72.7 examples/sec; 0.055 sec/batch; 2h:13m:45s remains)
INFO - root - 2019-11-06 20:12:46.923782: step 4230, total loss = 0.60, predict loss = 0.15 (78.1 examples/sec; 0.051 sec/batch; 2h:04m:29s remains)
INFO - root - 2019-11-06 20:12:47.493505: step 4240, total loss = 0.57, predict loss = 0.14 (77.5 examples/sec; 0.052 sec/batch; 2h:05m:24s remains)
INFO - root - 2019-11-06 20:12:48.093637: step 4250, total loss = 0.38, predict loss = 0.09 (77.2 examples/sec; 0.052 sec/batch; 2h:05m:52s remains)
INFO - root - 2019-11-06 20:12:48.667074: step 4260, total loss = 0.42, predict loss = 0.10 (79.8 examples/sec; 0.050 sec/batch; 2h:01m:48s remains)
INFO - root - 2019-11-06 20:12:49.227486: step 4270, total loss = 0.44, predict loss = 0.13 (87.1 examples/sec; 0.046 sec/batch; 1h:51m:35s remains)
INFO - root - 2019-11-06 20:12:49.689596: step 4280, total loss = 0.84, predict loss = 0.23 (94.5 examples/sec; 0.042 sec/batch; 1h:42m:48s remains)
INFO - root - 2019-11-06 20:12:50.165402: step 4290, total loss = 0.43, predict loss = 0.11 (96.7 examples/sec; 0.041 sec/batch; 1h:40m:25s remains)
INFO - root - 2019-11-06 20:12:51.087512: step 4300, total loss = 0.65, predict loss = 0.16 (77.0 examples/sec; 0.052 sec/batch; 2h:06m:12s remains)
INFO - root - 2019-11-06 20:12:51.809678: step 4310, total loss = 0.48, predict loss = 0.13 (63.5 examples/sec; 0.063 sec/batch; 2h:32m:53s remains)
INFO - root - 2019-11-06 20:12:52.414777: step 4320, total loss = 0.50, predict loss = 0.13 (82.3 examples/sec; 0.049 sec/batch; 1h:58m:01s remains)
INFO - root - 2019-11-06 20:12:53.010568: step 4330, total loss = 0.36, predict loss = 0.10 (78.2 examples/sec; 0.051 sec/batch; 2h:04m:13s remains)
INFO - root - 2019-11-06 20:12:53.583462: step 4340, total loss = 0.41, predict loss = 0.11 (78.6 examples/sec; 0.051 sec/batch; 2h:03m:28s remains)
INFO - root - 2019-11-06 20:12:54.154116: step 4350, total loss = 0.61, predict loss = 0.15 (77.2 examples/sec; 0.052 sec/batch; 2h:05m:42s remains)
INFO - root - 2019-11-06 20:12:54.728850: step 4360, total loss = 0.31, predict loss = 0.08 (79.7 examples/sec; 0.050 sec/batch; 2h:01m:46s remains)
INFO - root - 2019-11-06 20:12:55.320486: step 4370, total loss = 0.67, predict loss = 0.18 (79.7 examples/sec; 0.050 sec/batch; 2h:01m:52s remains)
INFO - root - 2019-11-06 20:12:55.899587: step 4380, total loss = 0.60, predict loss = 0.17 (80.5 examples/sec; 0.050 sec/batch; 2h:00m:32s remains)
INFO - root - 2019-11-06 20:12:56.478154: step 4390, total loss = 0.41, predict loss = 0.10 (76.0 examples/sec; 0.053 sec/batch; 2h:07m:45s remains)
INFO - root - 2019-11-06 20:12:57.046534: step 4400, total loss = 0.37, predict loss = 0.10 (76.7 examples/sec; 0.052 sec/batch; 2h:06m:36s remains)
INFO - root - 2019-11-06 20:12:57.641449: step 4410, total loss = 0.52, predict loss = 0.16 (79.6 examples/sec; 0.050 sec/batch; 2h:01m:59s remains)
INFO - root - 2019-11-06 20:12:58.174397: step 4420, total loss = 0.48, predict loss = 0.12 (96.7 examples/sec; 0.041 sec/batch; 1h:40m:23s remains)
INFO - root - 2019-11-06 20:12:58.621946: step 4430, total loss = 0.84, predict loss = 0.21 (100.5 examples/sec; 0.040 sec/batch; 1h:36m:33s remains)
INFO - root - 2019-11-06 20:12:59.072971: step 4440, total loss = 0.82, predict loss = 0.25 (96.2 examples/sec; 0.042 sec/batch; 1h:40m:52s remains)
INFO - root - 2019-11-06 20:13:00.055812: step 4450, total loss = 0.52, predict loss = 0.13 (58.4 examples/sec; 0.069 sec/batch; 2h:46m:16s remains)
INFO - root - 2019-11-06 20:13:00.734517: step 4460, total loss = 0.64, predict loss = 0.15 (70.3 examples/sec; 0.057 sec/batch; 2h:18m:04s remains)
INFO - root - 2019-11-06 20:13:01.337110: step 4470, total loss = 0.73, predict loss = 0.19 (78.4 examples/sec; 0.051 sec/batch; 2h:03m:45s remains)
INFO - root - 2019-11-06 20:13:01.917972: step 4480, total loss = 0.44, predict loss = 0.11 (78.9 examples/sec; 0.051 sec/batch; 2h:02m:55s remains)
INFO - root - 2019-11-06 20:13:02.494675: step 4490, total loss = 0.49, predict loss = 0.14 (83.9 examples/sec; 0.048 sec/batch; 1h:55m:33s remains)
INFO - root - 2019-11-06 20:13:03.063977: step 4500, total loss = 0.42, predict loss = 0.13 (77.8 examples/sec; 0.051 sec/batch; 2h:04m:39s remains)
INFO - root - 2019-11-06 20:13:03.644337: step 4510, total loss = 0.52, predict loss = 0.13 (79.6 examples/sec; 0.050 sec/batch; 2h:01m:47s remains)
INFO - root - 2019-11-06 20:13:04.219879: step 4520, total loss = 0.70, predict loss = 0.19 (76.6 examples/sec; 0.052 sec/batch; 2h:06m:39s remains)
INFO - root - 2019-11-06 20:13:04.809425: step 4530, total loss = 0.51, predict loss = 0.13 (82.2 examples/sec; 0.049 sec/batch; 1h:58m:02s remains)
INFO - root - 2019-11-06 20:13:05.388696: step 4540, total loss = 0.63, predict loss = 0.15 (77.7 examples/sec; 0.051 sec/batch; 2h:04m:47s remains)
INFO - root - 2019-11-06 20:13:05.962684: step 4550, total loss = 0.65, predict loss = 0.18 (81.7 examples/sec; 0.049 sec/batch; 1h:58m:42s remains)
INFO - root - 2019-11-06 20:13:06.534398: step 4560, total loss = 0.49, predict loss = 0.12 (80.1 examples/sec; 0.050 sec/batch; 2h:00m:59s remains)
INFO - root - 2019-11-06 20:13:07.073224: step 4570, total loss = 0.60, predict loss = 0.14 (94.5 examples/sec; 0.042 sec/batch; 1h:42m:37s remains)
INFO - root - 2019-11-06 20:13:07.506772: step 4580, total loss = 0.74, predict loss = 0.20 (99.8 examples/sec; 0.040 sec/batch; 1h:37m:06s remains)
INFO - root - 2019-11-06 20:13:07.959169: step 4590, total loss = 0.39, predict loss = 0.10 (90.2 examples/sec; 0.044 sec/batch; 1h:47m:29s remains)
INFO - root - 2019-11-06 20:13:08.952426: step 4600, total loss = 0.42, predict loss = 0.10 (58.2 examples/sec; 0.069 sec/batch; 2h:46m:35s remains)
INFO - root - 2019-11-06 20:13:09.624006: step 4610, total loss = 0.51, predict loss = 0.14 (73.2 examples/sec; 0.055 sec/batch; 2h:12m:25s remains)
INFO - root - 2019-11-06 20:13:10.202720: step 4620, total loss = 0.69, predict loss = 0.20 (79.9 examples/sec; 0.050 sec/batch; 2h:01m:21s remains)
INFO - root - 2019-11-06 20:13:10.773133: step 4630, total loss = 0.53, predict loss = 0.13 (77.9 examples/sec; 0.051 sec/batch; 2h:04m:27s remains)
INFO - root - 2019-11-06 20:13:11.341714: step 4640, total loss = 0.40, predict loss = 0.09 (81.5 examples/sec; 0.049 sec/batch; 1h:58m:52s remains)
INFO - root - 2019-11-06 20:13:11.931595: step 4650, total loss = 0.62, predict loss = 0.18 (78.1 examples/sec; 0.051 sec/batch; 2h:04m:03s remains)
INFO - root - 2019-11-06 20:13:12.512254: step 4660, total loss = 0.34, predict loss = 0.09 (77.9 examples/sec; 0.051 sec/batch; 2h:04m:18s remains)
INFO - root - 2019-11-06 20:13:13.079326: step 4670, total loss = 0.51, predict loss = 0.14 (78.4 examples/sec; 0.051 sec/batch; 2h:03m:34s remains)
INFO - root - 2019-11-06 20:13:13.652659: step 4680, total loss = 0.51, predict loss = 0.12 (81.5 examples/sec; 0.049 sec/batch; 1h:58m:56s remains)
INFO - root - 2019-11-06 20:13:14.242620: step 4690, total loss = 0.64, predict loss = 0.17 (78.0 examples/sec; 0.051 sec/batch; 2h:04m:14s remains)
INFO - root - 2019-11-06 20:13:14.811419: step 4700, total loss = 0.52, predict loss = 0.12 (78.0 examples/sec; 0.051 sec/batch; 2h:04m:07s remains)
INFO - root - 2019-11-06 20:13:15.437141: step 4710, total loss = 0.36, predict loss = 0.09 (76.8 examples/sec; 0.052 sec/batch; 2h:06m:02s remains)
INFO - root - 2019-11-06 20:13:15.933395: step 4720, total loss = 0.38, predict loss = 0.10 (97.9 examples/sec; 0.041 sec/batch; 1h:38m:56s remains)
INFO - root - 2019-11-06 20:13:16.407666: step 4730, total loss = 0.60, predict loss = 0.14 (91.0 examples/sec; 0.044 sec/batch; 1h:46m:22s remains)
INFO - root - 2019-11-06 20:13:16.869601: step 4740, total loss = 0.54, predict loss = 0.12 (93.0 examples/sec; 0.043 sec/batch; 1h:44m:07s remains)
INFO - root - 2019-11-06 20:13:17.917688: step 4750, total loss = 0.53, predict loss = 0.13 (63.3 examples/sec; 0.063 sec/batch; 2h:32m:56s remains)
INFO - root - 2019-11-06 20:13:18.528885: step 4760, total loss = 0.73, predict loss = 0.20 (78.5 examples/sec; 0.051 sec/batch; 2h:03m:16s remains)
INFO - root - 2019-11-06 20:13:19.106202: step 4770, total loss = 0.51, predict loss = 0.13 (82.6 examples/sec; 0.048 sec/batch; 1h:57m:15s remains)
INFO - root - 2019-11-06 20:13:19.694117: step 4780, total loss = 0.42, predict loss = 0.11 (77.0 examples/sec; 0.052 sec/batch; 2h:05m:40s remains)
INFO - root - 2019-11-06 20:13:20.266925: step 4790, total loss = 0.58, predict loss = 0.17 (79.4 examples/sec; 0.050 sec/batch; 2h:01m:53s remains)
INFO - root - 2019-11-06 20:13:20.842099: step 4800, total loss = 0.56, predict loss = 0.15 (74.2 examples/sec; 0.054 sec/batch; 2h:10m:31s remains)
INFO - root - 2019-11-06 20:13:21.431063: step 4810, total loss = 0.52, predict loss = 0.13 (79.5 examples/sec; 0.050 sec/batch; 2h:01m:43s remains)
INFO - root - 2019-11-06 20:13:21.995299: step 4820, total loss = 0.75, predict loss = 0.23 (81.4 examples/sec; 0.049 sec/batch; 1h:58m:58s remains)
INFO - root - 2019-11-06 20:13:22.569619: step 4830, total loss = 1.08, predict loss = 0.34 (78.2 examples/sec; 0.051 sec/batch; 2h:03m:43s remains)
INFO - root - 2019-11-06 20:13:23.139772: step 4840, total loss = 0.39, predict loss = 0.10 (77.8 examples/sec; 0.051 sec/batch; 2h:04m:19s remains)
INFO - root - 2019-11-06 20:13:23.726388: step 4850, total loss = 0.40, predict loss = 0.11 (83.1 examples/sec; 0.048 sec/batch; 1h:56m:24s remains)
INFO - root - 2019-11-06 20:13:24.296591: step 4860, total loss = 0.46, predict loss = 0.11 (79.8 examples/sec; 0.050 sec/batch; 2h:01m:16s remains)
INFO - root - 2019-11-06 20:13:24.778549: step 4870, total loss = 0.63, predict loss = 0.17 (102.0 examples/sec; 0.039 sec/batch; 1h:34m:53s remains)
INFO - root - 2019-11-06 20:13:25.248148: step 4880, total loss = 0.54, predict loss = 0.15 (94.8 examples/sec; 0.042 sec/batch; 1h:42m:06s remains)
INFO - root - 2019-11-06 20:13:26.201287: step 4890, total loss = 0.79, predict loss = 0.23 (7.7 examples/sec; 0.517 sec/batch; 20h:50m:53s remains)
INFO - root - 2019-11-06 20:13:26.837133: step 4900, total loss = 0.39, predict loss = 0.10 (65.5 examples/sec; 0.061 sec/batch; 2h:27m:34s remains)
INFO - root - 2019-11-06 20:13:27.503271: step 4910, total loss = 0.50, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 2h:18m:31s remains)
INFO - root - 2019-11-06 20:13:28.102049: step 4920, total loss = 0.59, predict loss = 0.16 (73.7 examples/sec; 0.054 sec/batch; 2h:11m:17s remains)
INFO - root - 2019-11-06 20:13:28.695498: step 4930, total loss = 0.95, predict loss = 0.26 (77.8 examples/sec; 0.051 sec/batch; 2h:04m:16s remains)
INFO - root - 2019-11-06 20:13:29.267312: step 4940, total loss = 0.33, predict loss = 0.09 (80.0 examples/sec; 0.050 sec/batch; 2h:00m:51s remains)
INFO - root - 2019-11-06 20:13:29.841440: step 4950, total loss = 0.85, predict loss = 0.27 (82.2 examples/sec; 0.049 sec/batch; 1h:57m:38s remains)
INFO - root - 2019-11-06 20:13:30.416710: step 4960, total loss = 1.06, predict loss = 0.35 (79.2 examples/sec; 0.050 sec/batch; 2h:02m:02s remains)
INFO - root - 2019-11-06 20:13:31.007110: step 4970, total loss = 0.64, predict loss = 0.19 (81.4 examples/sec; 0.049 sec/batch; 1h:58m:44s remains)
INFO - root - 2019-11-06 20:13:31.569886: step 4980, total loss = 0.40, predict loss = 0.09 (75.1 examples/sec; 0.053 sec/batch; 2h:08m:45s remains)
INFO - root - 2019-11-06 20:13:32.135584: step 4990, total loss = 0.41, predict loss = 0.10 (77.5 examples/sec; 0.052 sec/batch; 2h:04m:41s remains)
INFO - root - 2019-11-06 20:13:32.714798: step 5000, total loss = 0.45, predict loss = 0.09 (78.0 examples/sec; 0.051 sec/batch; 2h:03m:53s remains)
INFO - root - 2019-11-06 20:13:33.295999: step 5010, total loss = 0.98, predict loss = 0.30 (87.0 examples/sec; 0.046 sec/batch; 1h:51m:04s remains)
INFO - root - 2019-11-06 20:13:33.769796: step 5020, total loss = 0.78, predict loss = 0.21 (100.3 examples/sec; 0.040 sec/batch; 1h:36m:21s remains)
INFO - root - 2019-11-06 20:13:34.211846: step 5030, total loss = 0.53, predict loss = 0.15 (99.9 examples/sec; 0.040 sec/batch; 1h:36m:42s remains)
INFO - root - 2019-11-06 20:13:35.133436: step 5040, total loss = 0.65, predict loss = 0.16 (77.7 examples/sec; 0.051 sec/batch; 2h:04m:24s remains)
INFO - root - 2019-11-06 20:13:35.771267: step 5050, total loss = 0.83, predict loss = 0.24 (70.4 examples/sec; 0.057 sec/batch; 2h:17m:17s remains)
INFO - root - 2019-11-06 20:13:36.366756: step 5060, total loss = 0.48, predict loss = 0.15 (78.3 examples/sec; 0.051 sec/batch; 2h:03m:22s remains)
INFO - root - 2019-11-06 20:13:36.937666: step 5070, total loss = 0.81, predict loss = 0.24 (78.2 examples/sec; 0.051 sec/batch; 2h:03m:29s remains)
INFO - root - 2019-11-06 20:13:37.515843: step 5080, total loss = 0.47, predict loss = 0.11 (79.7 examples/sec; 0.050 sec/batch; 2h:01m:11s remains)
INFO - root - 2019-11-06 20:13:38.108082: step 5090, total loss = 0.73, predict loss = 0.21 (77.2 examples/sec; 0.052 sec/batch; 2h:05m:05s remains)
INFO - root - 2019-11-06 20:13:38.683369: step 5100, total loss = 0.85, predict loss = 0.28 (79.5 examples/sec; 0.050 sec/batch; 2h:01m:31s remains)
INFO - root - 2019-11-06 20:13:39.255247: step 5110, total loss = 0.50, predict loss = 0.12 (80.2 examples/sec; 0.050 sec/batch; 2h:00m:22s remains)
INFO - root - 2019-11-06 20:13:39.843391: step 5120, total loss = 0.38, predict loss = 0.10 (79.5 examples/sec; 0.050 sec/batch; 2h:01m:28s remains)
INFO - root - 2019-11-06 20:13:40.432641: step 5130, total loss = 0.85, predict loss = 0.23 (78.9 examples/sec; 0.051 sec/batch; 2h:02m:21s remains)
INFO - root - 2019-11-06 20:13:41.006455: step 5140, total loss = 0.44, predict loss = 0.11 (76.8 examples/sec; 0.052 sec/batch; 2h:05m:44s remains)
INFO - root - 2019-11-06 20:13:41.591902: step 5150, total loss = 1.03, predict loss = 0.25 (77.1 examples/sec; 0.052 sec/batch; 2h:05m:16s remains)
INFO - root - 2019-11-06 20:13:42.140600: step 5160, total loss = 0.59, predict loss = 0.15 (91.5 examples/sec; 0.044 sec/batch; 1h:45m:30s remains)
INFO - root - 2019-11-06 20:13:42.619884: step 5170, total loss = 0.60, predict loss = 0.16 (93.9 examples/sec; 0.043 sec/batch; 1h:42m:47s remains)
INFO - root - 2019-11-06 20:13:43.082429: step 5180, total loss = 0.48, predict loss = 0.14 (95.5 examples/sec; 0.042 sec/batch; 1h:41m:03s remains)
INFO - root - 2019-11-06 20:13:44.010508: step 5190, total loss = 0.39, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 2h:16m:48s remains)
INFO - root - 2019-11-06 20:13:44.739393: step 5200, total loss = 0.72, predict loss = 0.20 (67.0 examples/sec; 0.060 sec/batch; 2h:24m:06s remains)
INFO - root - 2019-11-06 20:13:45.401852: step 5210, total loss = 0.49, predict loss = 0.13 (74.9 examples/sec; 0.053 sec/batch; 2h:08m:48s remains)
INFO - root - 2019-11-06 20:13:45.972517: step 5220, total loss = 0.71, predict loss = 0.19 (78.5 examples/sec; 0.051 sec/batch; 2h:02m:56s remains)
INFO - root - 2019-11-06 20:13:46.540628: step 5230, total loss = 0.39, predict loss = 0.09 (80.4 examples/sec; 0.050 sec/batch; 2h:00m:01s remains)
INFO - root - 2019-11-06 20:13:47.103881: step 5240, total loss = 0.79, predict loss = 0.23 (78.5 examples/sec; 0.051 sec/batch; 2h:02m:54s remains)
INFO - root - 2019-11-06 20:13:47.694027: step 5250, total loss = 0.43, predict loss = 0.11 (78.5 examples/sec; 0.051 sec/batch; 2h:02m:56s remains)
INFO - root - 2019-11-06 20:13:48.267877: step 5260, total loss = 0.66, predict loss = 0.17 (79.2 examples/sec; 0.050 sec/batch; 2h:01m:46s remains)
INFO - root - 2019-11-06 20:13:48.836295: step 5270, total loss = 0.49, predict loss = 0.13 (77.8 examples/sec; 0.051 sec/batch; 2h:04m:04s remains)
INFO - root - 2019-11-06 20:13:49.408846: step 5280, total loss = 0.77, predict loss = 0.20 (82.7 examples/sec; 0.048 sec/batch; 1h:56m:42s remains)
INFO - root - 2019-11-06 20:13:49.988576: step 5290, total loss = 0.59, predict loss = 0.15 (74.2 examples/sec; 0.054 sec/batch; 2h:10m:03s remains)
INFO - root - 2019-11-06 20:13:50.567673: step 5300, total loss = 0.53, predict loss = 0.15 (83.6 examples/sec; 0.048 sec/batch; 1h:55m:24s remains)
INFO - root - 2019-11-06 20:13:51.097480: step 5310, total loss = 0.40, predict loss = 0.11 (91.5 examples/sec; 0.044 sec/batch; 1h:45m:28s remains)
INFO - root - 2019-11-06 20:13:51.545765: step 5320, total loss = 0.35, predict loss = 0.09 (99.2 examples/sec; 0.040 sec/batch; 1h:37m:16s remains)
INFO - root - 2019-11-06 20:13:52.046146: step 5330, total loss = 0.96, predict loss = 0.20 (92.3 examples/sec; 0.043 sec/batch; 1h:44m:27s remains)
INFO - root - 2019-11-06 20:13:53.076195: step 5340, total loss = 0.42, predict loss = 0.11 (55.1 examples/sec; 0.073 sec/batch; 2h:55m:05s remains)
INFO - root - 2019-11-06 20:13:53.718859: step 5350, total loss = 0.41, predict loss = 0.11 (80.1 examples/sec; 0.050 sec/batch; 2h:00m:22s remains)
INFO - root - 2019-11-06 20:13:54.290061: step 5360, total loss = 0.35, predict loss = 0.10 (83.1 examples/sec; 0.048 sec/batch; 1h:56m:02s remains)
INFO - root - 2019-11-06 20:13:54.868089: step 5370, total loss = 0.44, predict loss = 0.12 (81.8 examples/sec; 0.049 sec/batch; 1h:57m:53s remains)
INFO - root - 2019-11-06 20:13:55.454296: step 5380, total loss = 0.56, predict loss = 0.17 (74.4 examples/sec; 0.054 sec/batch; 2h:09m:36s remains)
INFO - root - 2019-11-06 20:13:56.028837: step 5390, total loss = 0.53, predict loss = 0.13 (73.8 examples/sec; 0.054 sec/batch; 2h:10m:42s remains)
INFO - root - 2019-11-06 20:13:56.595408: step 5400, total loss = 0.45, predict loss = 0.12 (80.2 examples/sec; 0.050 sec/batch; 2h:00m:15s remains)
INFO - root - 2019-11-06 20:13:57.197357: step 5410, total loss = 0.63, predict loss = 0.16 (80.2 examples/sec; 0.050 sec/batch; 2h:00m:15s remains)
INFO - root - 2019-11-06 20:13:57.790998: step 5420, total loss = 0.68, predict loss = 0.19 (75.5 examples/sec; 0.053 sec/batch; 2h:07m:41s remains)
INFO - root - 2019-11-06 20:13:58.362151: step 5430, total loss = 0.78, predict loss = 0.20 (81.8 examples/sec; 0.049 sec/batch; 1h:57m:50s remains)
INFO - root - 2019-11-06 20:13:58.932140: step 5440, total loss = 0.51, predict loss = 0.14 (78.6 examples/sec; 0.051 sec/batch; 2h:02m:40s remains)
INFO - root - 2019-11-06 20:13:59.512273: step 5450, total loss = 0.41, predict loss = 0.10 (80.5 examples/sec; 0.050 sec/batch; 1h:59m:43s remains)
INFO - root - 2019-11-06 20:14:00.025379: step 5460, total loss = 1.01, predict loss = 0.28 (101.1 examples/sec; 0.040 sec/batch; 1h:35m:17s remains)
INFO - root - 2019-11-06 20:14:00.474257: step 5470, total loss = 0.53, predict loss = 0.13 (98.7 examples/sec; 0.041 sec/batch; 1h:37m:34s remains)
INFO - root - 2019-11-06 20:14:00.939224: step 5480, total loss = 0.30, predict loss = 0.08 (100.4 examples/sec; 0.040 sec/batch; 1h:35m:56s remains)
INFO - root - 2019-11-06 20:14:02.004004: step 5490, total loss = 0.38, predict loss = 0.10 (56.9 examples/sec; 0.070 sec/batch; 2h:49m:24s remains)
INFO - root - 2019-11-06 20:14:02.634175: step 5500, total loss = 0.75, predict loss = 0.23 (82.4 examples/sec; 0.049 sec/batch; 1h:56m:56s remains)
INFO - root - 2019-11-06 20:14:03.208779: step 5510, total loss = 0.79, predict loss = 0.24 (81.9 examples/sec; 0.049 sec/batch; 1h:57m:33s remains)
INFO - root - 2019-11-06 20:14:03.783036: step 5520, total loss = 0.38, predict loss = 0.10 (75.7 examples/sec; 0.053 sec/batch; 2h:07m:14s remains)
INFO - root - 2019-11-06 20:14:04.383845: step 5530, total loss = 0.52, predict loss = 0.13 (78.2 examples/sec; 0.051 sec/batch; 2h:03m:09s remains)
INFO - root - 2019-11-06 20:14:04.945820: step 5540, total loss = 0.48, predict loss = 0.14 (82.6 examples/sec; 0.048 sec/batch; 1h:56m:33s remains)
INFO - root - 2019-11-06 20:14:05.507641: step 5550, total loss = 0.97, predict loss = 0.28 (73.5 examples/sec; 0.054 sec/batch; 2h:11m:00s remains)
INFO - root - 2019-11-06 20:14:06.079330: step 5560, total loss = 0.62, predict loss = 0.16 (75.0 examples/sec; 0.053 sec/batch; 2h:08m:25s remains)
INFO - root - 2019-11-06 20:14:06.671118: step 5570, total loss = 0.57, predict loss = 0.12 (77.1 examples/sec; 0.052 sec/batch; 2h:04m:53s remains)
INFO - root - 2019-11-06 20:14:07.232811: step 5580, total loss = 0.41, predict loss = 0.09 (77.7 examples/sec; 0.051 sec/batch; 2h:03m:55s remains)
INFO - root - 2019-11-06 20:14:07.800072: step 5590, total loss = 0.39, predict loss = 0.10 (73.5 examples/sec; 0.054 sec/batch; 2h:10m:55s remains)
INFO - root - 2019-11-06 20:14:08.386422: step 5600, total loss = 0.63, predict loss = 0.20 (74.6 examples/sec; 0.054 sec/batch; 2h:09m:04s remains)
INFO - root - 2019-11-06 20:14:08.893466: step 5610, total loss = 0.61, predict loss = 0.15 (96.6 examples/sec; 0.041 sec/batch; 1h:39m:37s remains)
INFO - root - 2019-11-06 20:14:09.359054: step 5620, total loss = 0.46, predict loss = 0.10 (93.2 examples/sec; 0.043 sec/batch; 1h:43m:17s remains)
INFO - root - 2019-11-06 20:14:09.822344: step 5630, total loss = 0.85, predict loss = 0.24 (97.4 examples/sec; 0.041 sec/batch; 1h:38m:46s remains)
INFO - root - 2019-11-06 20:14:10.885055: step 5640, total loss = 0.37, predict loss = 0.09 (66.5 examples/sec; 0.060 sec/batch; 2h:24m:48s remains)
INFO - root - 2019-11-06 20:14:11.550948: step 5650, total loss = 0.49, predict loss = 0.14 (75.5 examples/sec; 0.053 sec/batch; 2h:07m:22s remains)
INFO - root - 2019-11-06 20:14:12.141304: step 5660, total loss = 0.67, predict loss = 0.18 (74.8 examples/sec; 0.053 sec/batch; 2h:08m:33s remains)
INFO - root - 2019-11-06 20:14:12.713106: step 5670, total loss = 0.45, predict loss = 0.12 (78.7 examples/sec; 0.051 sec/batch; 2h:02m:17s remains)
INFO - root - 2019-11-06 20:14:13.281715: step 5680, total loss = 0.47, predict loss = 0.13 (77.9 examples/sec; 0.051 sec/batch; 2h:03m:29s remains)
INFO - root - 2019-11-06 20:14:13.873388: step 5690, total loss = 0.66, predict loss = 0.20 (78.3 examples/sec; 0.051 sec/batch; 2h:02m:49s remains)
INFO - root - 2019-11-06 20:14:14.448551: step 5700, total loss = 0.45, predict loss = 0.12 (75.0 examples/sec; 0.053 sec/batch; 2h:08m:17s remains)
INFO - root - 2019-11-06 20:14:15.053030: step 5710, total loss = 0.51, predict loss = 0.13 (56.0 examples/sec; 0.071 sec/batch; 2h:51m:38s remains)
INFO - root - 2019-11-06 20:14:15.661950: step 5720, total loss = 0.93, predict loss = 0.25 (79.5 examples/sec; 0.050 sec/batch; 2h:01m:01s remains)
INFO - root - 2019-11-06 20:14:16.252128: step 5730, total loss = 0.41, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 2h:08m:14s remains)
INFO - root - 2019-11-06 20:14:16.826628: step 5740, total loss = 0.42, predict loss = 0.10 (82.4 examples/sec; 0.049 sec/batch; 1h:56m:40s remains)
INFO - root - 2019-11-06 20:14:17.408773: step 5750, total loss = 0.36, predict loss = 0.11 (76.2 examples/sec; 0.052 sec/batch; 2h:06m:12s remains)
INFO - root - 2019-11-06 20:14:17.878756: step 5760, total loss = 0.68, predict loss = 0.14 (94.0 examples/sec; 0.043 sec/batch; 1h:42m:17s remains)
INFO - root - 2019-11-06 20:14:18.367499: step 5770, total loss = 0.53, predict loss = 0.13 (90.8 examples/sec; 0.044 sec/batch; 1h:45m:57s remains)
INFO - root - 2019-11-06 20:14:19.271901: step 5780, total loss = 0.72, predict loss = 0.21 (80.0 examples/sec; 0.050 sec/batch; 2h:00m:09s remains)
INFO - root - 2019-11-06 20:14:19.926597: step 5790, total loss = 0.66, predict loss = 0.16 (67.7 examples/sec; 0.059 sec/batch; 2h:22m:00s remains)
INFO - root - 2019-11-06 20:14:20.540317: step 5800, total loss = 0.38, predict loss = 0.09 (78.7 examples/sec; 0.051 sec/batch; 2h:02m:05s remains)
INFO - root - 2019-11-06 20:14:21.136728: step 5810, total loss = 0.64, predict loss = 0.13 (75.5 examples/sec; 0.053 sec/batch; 2h:07m:14s remains)
INFO - root - 2019-11-06 20:14:21.707233: step 5820, total loss = 0.33, predict loss = 0.09 (81.2 examples/sec; 0.049 sec/batch; 1h:58m:18s remains)
INFO - root - 2019-11-06 20:14:22.283447: step 5830, total loss = 0.34, predict loss = 0.09 (80.1 examples/sec; 0.050 sec/batch; 1h:59m:57s remains)
INFO - root - 2019-11-06 20:14:22.866623: step 5840, total loss = 0.60, predict loss = 0.13 (79.0 examples/sec; 0.051 sec/batch; 2h:01m:41s remains)
INFO - root - 2019-11-06 20:14:23.455973: step 5850, total loss = 0.73, predict loss = 0.23 (76.3 examples/sec; 0.052 sec/batch; 2h:06m:00s remains)
INFO - root - 2019-11-06 20:14:24.038785: step 5860, total loss = 0.36, predict loss = 0.09 (82.2 examples/sec; 0.049 sec/batch; 1h:56m:56s remains)
INFO - root - 2019-11-06 20:14:24.601544: step 5870, total loss = 0.35, predict loss = 0.09 (78.9 examples/sec; 0.051 sec/batch; 2h:01m:46s remains)
INFO - root - 2019-11-06 20:14:25.165487: step 5880, total loss = 0.37, predict loss = 0.10 (78.7 examples/sec; 0.051 sec/batch; 2h:02m:08s remains)
INFO - root - 2019-11-06 20:14:25.766104: step 5890, total loss = 0.28, predict loss = 0.08 (74.8 examples/sec; 0.053 sec/batch; 2h:08m:26s remains)
INFO - root - 2019-11-06 20:14:26.328162: step 5900, total loss = 0.62, predict loss = 0.12 (95.3 examples/sec; 0.042 sec/batch; 1h:40m:45s remains)
INFO - root - 2019-11-06 20:14:26.780652: step 5910, total loss = 0.43, predict loss = 0.13 (95.3 examples/sec; 0.042 sec/batch; 1h:40m:45s remains)
INFO - root - 2019-11-06 20:14:27.233112: step 5920, total loss = 0.60, predict loss = 0.14 (91.3 examples/sec; 0.044 sec/batch; 1h:45m:15s remains)
INFO - root - 2019-11-06 20:14:28.192499: step 5930, total loss = 0.63, predict loss = 0.19 (75.0 examples/sec; 0.053 sec/batch; 2h:08m:00s remains)
INFO - root - 2019-11-06 20:14:28.876741: step 5940, total loss = 0.90, predict loss = 0.27 (67.7 examples/sec; 0.059 sec/batch; 2h:21m:47s remains)
INFO - root - 2019-11-06 20:14:29.468375: step 5950, total loss = 0.64, predict loss = 0.15 (78.2 examples/sec; 0.051 sec/batch; 2h:02m:51s remains)
INFO - root - 2019-11-06 20:14:30.068203: step 5960, total loss = 0.41, predict loss = 0.10 (75.4 examples/sec; 0.053 sec/batch; 2h:07m:22s remains)
INFO - root - 2019-11-06 20:14:30.652294: step 5970, total loss = 0.41, predict loss = 0.10 (77.6 examples/sec; 0.052 sec/batch; 2h:03m:43s remains)
INFO - root - 2019-11-06 20:14:31.230639: step 5980, total loss = 0.56, predict loss = 0.15 (81.0 examples/sec; 0.049 sec/batch; 1h:58m:32s remains)
INFO - root - 2019-11-06 20:14:31.808896: step 5990, total loss = 0.67, predict loss = 0.20 (71.4 examples/sec; 0.056 sec/batch; 2h:14m:32s remains)
INFO - root - 2019-11-06 20:14:32.393245: step 6000, total loss = 0.63, predict loss = 0.19 (75.5 examples/sec; 0.053 sec/batch; 2h:07m:11s remains)
INFO - root - 2019-11-06 20:14:32.969205: step 6010, total loss = 0.44, predict loss = 0.12 (78.6 examples/sec; 0.051 sec/batch; 2h:02m:05s remains)
INFO - root - 2019-11-06 20:14:33.537009: step 6020, total loss = 0.71, predict loss = 0.20 (77.7 examples/sec; 0.051 sec/batch; 2h:03m:29s remains)
INFO - root - 2019-11-06 20:14:34.103758: step 6030, total loss = 0.60, predict loss = 0.18 (81.2 examples/sec; 0.049 sec/batch; 1h:58m:12s remains)
INFO - root - 2019-11-06 20:14:34.687118: step 6040, total loss = 0.51, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 2h:11m:23s remains)
INFO - root - 2019-11-06 20:14:35.249939: step 6050, total loss = 0.54, predict loss = 0.12 (93.1 examples/sec; 0.043 sec/batch; 1h:43m:04s remains)
INFO - root - 2019-11-06 20:14:35.696056: step 6060, total loss = 0.47, predict loss = 0.12 (98.0 examples/sec; 0.041 sec/batch; 1h:37m:56s remains)
INFO - root - 2019-11-06 20:14:36.155460: step 6070, total loss = 0.67, predict loss = 0.16 (94.8 examples/sec; 0.042 sec/batch; 1h:41m:14s remains)
INFO - root - 2019-11-06 20:14:37.158266: step 6080, total loss = 0.56, predict loss = 0.12 (59.5 examples/sec; 0.067 sec/batch; 2h:41m:23s remains)
INFO - root - 2019-11-06 20:14:37.861791: step 6090, total loss = 1.08, predict loss = 0.34 (70.7 examples/sec; 0.057 sec/batch; 2h:15m:40s remains)
INFO - root - 2019-11-06 20:14:38.461174: step 6100, total loss = 0.43, predict loss = 0.12 (81.4 examples/sec; 0.049 sec/batch; 1h:57m:48s remains)
INFO - root - 2019-11-06 20:14:39.034477: step 6110, total loss = 0.49, predict loss = 0.13 (80.6 examples/sec; 0.050 sec/batch; 1h:58m:59s remains)
INFO - root - 2019-11-06 20:14:39.608803: step 6120, total loss = 0.48, predict loss = 0.10 (75.0 examples/sec; 0.053 sec/batch; 2h:07m:51s remains)
INFO - root - 2019-11-06 20:14:40.205072: step 6130, total loss = 0.49, predict loss = 0.13 (74.4 examples/sec; 0.054 sec/batch; 2h:08m:57s remains)
INFO - root - 2019-11-06 20:14:40.787257: step 6140, total loss = 0.52, predict loss = 0.13 (77.6 examples/sec; 0.052 sec/batch; 2h:03m:36s remains)
INFO - root - 2019-11-06 20:14:41.359468: step 6150, total loss = 0.47, predict loss = 0.11 (79.7 examples/sec; 0.050 sec/batch; 2h:00m:18s remains)
INFO - root - 2019-11-06 20:14:41.929290: step 6160, total loss = 0.71, predict loss = 0.20 (78.7 examples/sec; 0.051 sec/batch; 2h:01m:46s remains)
INFO - root - 2019-11-06 20:14:42.503517: step 6170, total loss = 0.47, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 2h:03m:36s remains)
INFO - root - 2019-11-06 20:14:43.073563: step 6180, total loss = 0.42, predict loss = 0.11 (77.3 examples/sec; 0.052 sec/batch; 2h:04m:02s remains)
INFO - root - 2019-11-06 20:14:43.646569: step 6190, total loss = 0.38, predict loss = 0.09 (79.8 examples/sec; 0.050 sec/batch; 2h:00m:09s remains)
INFO - root - 2019-11-06 20:14:44.177696: step 6200, total loss = 0.72, predict loss = 0.20 (93.8 examples/sec; 0.043 sec/batch; 1h:42m:10s remains)
INFO - root - 2019-11-06 20:14:44.651238: step 6210, total loss = 0.95, predict loss = 0.24 (94.3 examples/sec; 0.042 sec/batch; 1h:41m:40s remains)
INFO - root - 2019-11-06 20:14:45.106412: step 6220, total loss = 0.69, predict loss = 0.18 (102.8 examples/sec; 0.039 sec/batch; 1h:33m:12s remains)
INFO - root - 2019-11-06 20:14:46.146762: step 6230, total loss = 0.62, predict loss = 0.15 (54.3 examples/sec; 0.074 sec/batch; 2h:56m:39s remains)
INFO - root - 2019-11-06 20:14:46.788305: step 6240, total loss = 0.75, predict loss = 0.25 (75.3 examples/sec; 0.053 sec/batch; 2h:07m:18s remains)
INFO - root - 2019-11-06 20:14:47.367947: step 6250, total loss = 0.36, predict loss = 0.09 (77.7 examples/sec; 0.051 sec/batch; 2h:03m:17s remains)
INFO - root - 2019-11-06 20:14:47.926345: step 6260, total loss = 0.60, predict loss = 0.16 (81.2 examples/sec; 0.049 sec/batch; 1h:57m:58s remains)
INFO - root - 2019-11-06 20:14:48.504166: step 6270, total loss = 0.58, predict loss = 0.14 (75.6 examples/sec; 0.053 sec/batch; 2h:06m:45s remains)
INFO - root - 2019-11-06 20:14:49.080745: step 6280, total loss = 0.72, predict loss = 0.22 (78.2 examples/sec; 0.051 sec/batch; 2h:02m:30s remains)
INFO - root - 2019-11-06 20:14:49.668611: step 6290, total loss = 0.44, predict loss = 0.13 (77.0 examples/sec; 0.052 sec/batch; 2h:04m:29s remains)
INFO - root - 2019-11-06 20:14:50.237301: step 6300, total loss = 0.60, predict loss = 0.16 (80.9 examples/sec; 0.049 sec/batch; 1h:58m:26s remains)
INFO - root - 2019-11-06 20:14:50.810552: step 6310, total loss = 0.61, predict loss = 0.16 (79.7 examples/sec; 0.050 sec/batch; 2h:00m:07s remains)
INFO - root - 2019-11-06 20:14:51.379068: step 6320, total loss = 0.60, predict loss = 0.16 (81.5 examples/sec; 0.049 sec/batch; 1h:57m:30s remains)
INFO - root - 2019-11-06 20:14:51.977111: step 6330, total loss = 0.31, predict loss = 0.08 (76.8 examples/sec; 0.052 sec/batch; 2h:04m:38s remains)
INFO - root - 2019-11-06 20:14:52.553292: step 6340, total loss = 1.42, predict loss = 0.40 (79.3 examples/sec; 0.050 sec/batch; 2h:00m:42s remains)
INFO - root - 2019-11-06 20:14:53.056224: step 6350, total loss = 0.49, predict loss = 0.12 (98.5 examples/sec; 0.041 sec/batch; 1h:37m:15s remains)
INFO - root - 2019-11-06 20:14:53.518968: step 6360, total loss = 0.52, predict loss = 0.12 (97.2 examples/sec; 0.041 sec/batch; 1h:38m:31s remains)
INFO - root - 2019-11-06 20:14:54.002745: step 6370, total loss = 0.87, predict loss = 0.24 (97.2 examples/sec; 0.041 sec/batch; 1h:38m:29s remains)
INFO - root - 2019-11-06 20:14:55.125002: step 6380, total loss = 0.43, predict loss = 0.11 (58.0 examples/sec; 0.069 sec/batch; 2h:44m:57s remains)
INFO - root - 2019-11-06 20:14:55.777688: step 6390, total loss = 0.42, predict loss = 0.11 (76.0 examples/sec; 0.053 sec/batch; 2h:05m:55s remains)
INFO - root - 2019-11-06 20:14:56.386432: step 6400, total loss = 0.39, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 2h:09m:33s remains)
INFO - root - 2019-11-06 20:14:56.995190: step 6410, total loss = 0.38, predict loss = 0.10 (78.4 examples/sec; 0.051 sec/batch; 2h:02m:10s remains)
INFO - root - 2019-11-06 20:14:57.588609: step 6420, total loss = 0.44, predict loss = 0.12 (76.3 examples/sec; 0.052 sec/batch; 2h:05m:27s remains)
INFO - root - 2019-11-06 20:14:58.184599: step 6430, total loss = 0.45, predict loss = 0.11 (79.9 examples/sec; 0.050 sec/batch; 1h:59m:49s remains)
INFO - root - 2019-11-06 20:14:58.779728: step 6440, total loss = 0.54, predict loss = 0.16 (74.1 examples/sec; 0.054 sec/batch; 2h:09m:13s remains)
INFO - root - 2019-11-06 20:14:59.387641: step 6450, total loss = 0.52, predict loss = 0.14 (74.1 examples/sec; 0.054 sec/batch; 2h:09m:13s remains)
INFO - root - 2019-11-06 20:14:59.984814: step 6460, total loss = 0.42, predict loss = 0.09 (78.0 examples/sec; 0.051 sec/batch; 2h:02m:40s remains)
INFO - root - 2019-11-06 20:15:00.587334: step 6470, total loss = 0.43, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 2h:09m:30s remains)
INFO - root - 2019-11-06 20:15:01.170323: step 6480, total loss = 0.63, predict loss = 0.18 (72.8 examples/sec; 0.055 sec/batch; 2h:11m:21s remains)
INFO - root - 2019-11-06 20:15:01.777959: step 6490, total loss = 0.46, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 2h:09m:26s remains)
INFO - root - 2019-11-06 20:15:02.285870: step 6500, total loss = 0.40, predict loss = 0.10 (100.0 examples/sec; 0.040 sec/batch; 1h:35m:42s remains)
INFO - root - 2019-11-06 20:15:02.750595: step 6510, total loss = 0.49, predict loss = 0.13 (96.3 examples/sec; 0.042 sec/batch; 1h:39m:22s remains)
INFO - root - 2019-11-06 20:15:03.678414: step 6520, total loss = 0.50, predict loss = 0.12 (7.9 examples/sec; 0.504 sec/batch; 20h:04m:37s remains)
INFO - root - 2019-11-06 20:15:04.367286: step 6530, total loss = 0.46, predict loss = 0.11 (59.6 examples/sec; 0.067 sec/batch; 2h:40m:20s remains)
INFO - root - 2019-11-06 20:15:05.041951: step 6540, total loss = 0.58, predict loss = 0.15 (69.3 examples/sec; 0.058 sec/batch; 2h:18m:04s remains)
INFO - root - 2019-11-06 20:15:05.649697: step 6550, total loss = 0.48, predict loss = 0.12 (75.9 examples/sec; 0.053 sec/batch; 2h:05m:54s remains)
INFO - root - 2019-11-06 20:15:06.225666: step 6560, total loss = 0.68, predict loss = 0.17 (78.9 examples/sec; 0.051 sec/batch; 2h:01m:09s remains)
INFO - root - 2019-11-06 20:15:06.870955: step 6570, total loss = 0.83, predict loss = 0.24 (61.8 examples/sec; 0.065 sec/batch; 2h:34m:46s remains)
INFO - root - 2019-11-06 20:15:07.455034: step 6580, total loss = 0.45, predict loss = 0.12 (79.8 examples/sec; 0.050 sec/batch; 1h:59m:47s remains)
INFO - root - 2019-11-06 20:15:08.028404: step 6590, total loss = 0.62, predict loss = 0.17 (77.2 examples/sec; 0.052 sec/batch; 2h:03m:48s remains)
INFO - root - 2019-11-06 20:15:08.601610: step 6600, total loss = 0.47, predict loss = 0.10 (76.7 examples/sec; 0.052 sec/batch; 2h:04m:36s remains)
INFO - root - 2019-11-06 20:15:09.187334: step 6610, total loss = 0.32, predict loss = 0.08 (77.8 examples/sec; 0.051 sec/batch; 2h:02m:53s remains)
INFO - root - 2019-11-06 20:15:09.763307: step 6620, total loss = 0.59, predict loss = 0.17 (81.3 examples/sec; 0.049 sec/batch; 1h:57m:35s remains)
INFO - root - 2019-11-06 20:15:10.338944: step 6630, total loss = 0.49, predict loss = 0.12 (78.6 examples/sec; 0.051 sec/batch; 2h:01m:32s remains)
INFO - root - 2019-11-06 20:15:10.904275: step 6640, total loss = 0.49, predict loss = 0.13 (87.7 examples/sec; 0.046 sec/batch; 1h:49m:00s remains)
INFO - root - 2019-11-06 20:15:11.378067: step 6650, total loss = 0.47, predict loss = 0.13 (99.4 examples/sec; 0.040 sec/batch; 1h:36m:08s remains)
INFO - root - 2019-11-06 20:15:11.842964: step 6660, total loss = 0.53, predict loss = 0.13 (93.1 examples/sec; 0.043 sec/batch; 1h:42m:37s remains)
INFO - root - 2019-11-06 20:15:12.782006: step 6670, total loss = 0.47, predict loss = 0.10 (74.9 examples/sec; 0.053 sec/batch; 2h:07m:34s remains)
INFO - root - 2019-11-06 20:15:13.475355: step 6680, total loss = 0.59, predict loss = 0.15 (68.6 examples/sec; 0.058 sec/batch; 2h:19m:16s remains)
INFO - root - 2019-11-06 20:15:14.137974: step 6690, total loss = 0.43, predict loss = 0.09 (74.5 examples/sec; 0.054 sec/batch; 2h:08m:16s remains)
INFO - root - 2019-11-06 20:15:14.770919: step 6700, total loss = 0.32, predict loss = 0.09 (73.3 examples/sec; 0.055 sec/batch; 2h:10m:24s remains)
INFO - root - 2019-11-06 20:15:15.392552: step 6710, total loss = 0.63, predict loss = 0.16 (75.9 examples/sec; 0.053 sec/batch; 2h:05m:53s remains)
INFO - root - 2019-11-06 20:15:15.954978: step 6720, total loss = 0.46, predict loss = 0.13 (87.3 examples/sec; 0.046 sec/batch; 1h:49m:28s remains)
INFO - root - 2019-11-06 20:15:16.532164: step 6730, total loss = 0.55, predict loss = 0.14 (75.5 examples/sec; 0.053 sec/batch; 2h:06m:31s remains)
INFO - root - 2019-11-06 20:15:17.108208: step 6740, total loss = 0.61, predict loss = 0.17 (79.9 examples/sec; 0.050 sec/batch; 1h:59m:29s remains)
INFO - root - 2019-11-06 20:15:17.678267: step 6750, total loss = 0.36, predict loss = 0.09 (77.6 examples/sec; 0.052 sec/batch; 2h:03m:06s remains)
INFO - root - 2019-11-06 20:15:18.260522: step 6760, total loss = 0.52, predict loss = 0.12 (74.3 examples/sec; 0.054 sec/batch; 2h:08m:30s remains)
INFO - root - 2019-11-06 20:15:18.844046: step 6770, total loss = 0.30, predict loss = 0.08 (82.3 examples/sec; 0.049 sec/batch; 1h:56m:02s remains)
INFO - root - 2019-11-06 20:15:19.410100: step 6780, total loss = 0.31, predict loss = 0.07 (86.1 examples/sec; 0.046 sec/batch; 1h:50m:53s remains)
INFO - root - 2019-11-06 20:15:19.978035: step 6790, total loss = 0.48, predict loss = 0.12 (95.9 examples/sec; 0.042 sec/batch; 1h:39m:33s remains)
INFO - root - 2019-11-06 20:15:20.431241: step 6800, total loss = 0.39, predict loss = 0.10 (89.0 examples/sec; 0.045 sec/batch; 1h:47m:15s remains)
INFO - root - 2019-11-06 20:15:20.918699: step 6810, total loss = 0.77, predict loss = 0.26 (93.2 examples/sec; 0.043 sec/batch; 1h:42m:22s remains)
INFO - root - 2019-11-06 20:15:21.844041: step 6820, total loss = 0.82, predict loss = 0.25 (67.5 examples/sec; 0.059 sec/batch; 2h:21m:21s remains)
INFO - root - 2019-11-06 20:15:22.605581: step 6830, total loss = 0.69, predict loss = 0.22 (61.4 examples/sec; 0.065 sec/batch; 2h:35m:30s remains)
INFO - root - 2019-11-06 20:15:23.228164: step 6840, total loss = 0.40, predict loss = 0.11 (78.6 examples/sec; 0.051 sec/batch; 2h:01m:29s remains)
INFO - root - 2019-11-06 20:15:23.827499: step 6850, total loss = 0.33, predict loss = 0.07 (75.0 examples/sec; 0.053 sec/batch; 2h:07m:13s remains)
INFO - root - 2019-11-06 20:15:24.401723: step 6860, total loss = 0.36, predict loss = 0.09 (79.3 examples/sec; 0.050 sec/batch; 2h:00m:17s remains)
INFO - root - 2019-11-06 20:15:24.961095: step 6870, total loss = 0.29, predict loss = 0.08 (79.7 examples/sec; 0.050 sec/batch; 1h:59m:46s remains)
INFO - root - 2019-11-06 20:15:25.539410: step 6880, total loss = 0.49, predict loss = 0.13 (80.0 examples/sec; 0.050 sec/batch; 1h:59m:17s remains)
INFO - root - 2019-11-06 20:15:26.122134: step 6890, total loss = 0.61, predict loss = 0.16 (79.7 examples/sec; 0.050 sec/batch; 1h:59m:40s remains)
INFO - root - 2019-11-06 20:15:26.697911: step 6900, total loss = 0.70, predict loss = 0.21 (78.9 examples/sec; 0.051 sec/batch; 2h:00m:52s remains)
INFO - root - 2019-11-06 20:15:27.279398: step 6910, total loss = 0.39, predict loss = 0.10 (73.3 examples/sec; 0.055 sec/batch; 2h:10m:13s remains)
INFO - root - 2019-11-06 20:15:27.863375: step 6920, total loss = 0.43, predict loss = 0.12 (76.5 examples/sec; 0.052 sec/batch; 2h:04m:40s remains)
INFO - root - 2019-11-06 20:15:28.448077: step 6930, total loss = 0.51, predict loss = 0.15 (83.1 examples/sec; 0.048 sec/batch; 1h:54m:45s remains)
INFO - root - 2019-11-06 20:15:28.986333: step 6940, total loss = 0.42, predict loss = 0.11 (94.5 examples/sec; 0.042 sec/batch; 1h:40m:57s remains)
INFO - root - 2019-11-06 20:15:29.435481: step 6950, total loss = 0.28, predict loss = 0.08 (93.6 examples/sec; 0.043 sec/batch; 1h:41m:52s remains)
INFO - root - 2019-11-06 20:15:29.897757: step 6960, total loss = 0.51, predict loss = 0.12 (93.6 examples/sec; 0.043 sec/batch; 1h:41m:55s remains)
INFO - root - 2019-11-06 20:15:30.944962: step 6970, total loss = 0.63, predict loss = 0.18 (53.3 examples/sec; 0.075 sec/batch; 2h:58m:44s remains)
INFO - root - 2019-11-06 20:15:31.592411: step 6980, total loss = 0.66, predict loss = 0.16 (80.3 examples/sec; 0.050 sec/batch; 1h:58m:41s remains)
INFO - root - 2019-11-06 20:15:32.171048: step 6990, total loss = 0.35, predict loss = 0.08 (80.1 examples/sec; 0.050 sec/batch; 1h:58m:58s remains)
INFO - root - 2019-11-06 20:15:32.742401: step 7000, total loss = 0.62, predict loss = 0.15 (80.9 examples/sec; 0.049 sec/batch; 1h:57m:53s remains)
INFO - root - 2019-11-06 20:15:33.331110: step 7010, total loss = 0.55, predict loss = 0.15 (76.4 examples/sec; 0.052 sec/batch; 2h:04m:44s remains)
INFO - root - 2019-11-06 20:15:33.898911: step 7020, total loss = 0.87, predict loss = 0.27 (75.8 examples/sec; 0.053 sec/batch; 2h:05m:42s remains)
INFO - root - 2019-11-06 20:15:34.483673: step 7030, total loss = 0.35, predict loss = 0.08 (77.7 examples/sec; 0.051 sec/batch; 2h:02m:36s remains)
INFO - root - 2019-11-06 20:15:35.066670: step 7040, total loss = 0.37, predict loss = 0.10 (77.4 examples/sec; 0.052 sec/batch; 2h:03m:11s remains)
INFO - root - 2019-11-06 20:15:35.668393: step 7050, total loss = 0.59, predict loss = 0.16 (79.4 examples/sec; 0.050 sec/batch; 2h:00m:05s remains)
INFO - root - 2019-11-06 20:15:36.245361: step 7060, total loss = 0.78, predict loss = 0.22 (79.2 examples/sec; 0.050 sec/batch; 2h:00m:17s remains)
INFO - root - 2019-11-06 20:15:36.818716: step 7070, total loss = 0.40, predict loss = 0.10 (79.5 examples/sec; 0.050 sec/batch; 1h:59m:51s remains)
INFO - root - 2019-11-06 20:15:37.396630: step 7080, total loss = 0.59, predict loss = 0.14 (79.1 examples/sec; 0.051 sec/batch; 2h:00m:28s remains)
INFO - root - 2019-11-06 20:15:37.927052: step 7090, total loss = 0.67, predict loss = 0.16 (97.2 examples/sec; 0.041 sec/batch; 1h:38m:00s remains)
INFO - root - 2019-11-06 20:15:38.379821: step 7100, total loss = 0.29, predict loss = 0.08 (97.5 examples/sec; 0.041 sec/batch; 1h:37m:45s remains)
INFO - root - 2019-11-06 20:15:38.839073: step 7110, total loss = 0.86, predict loss = 0.23 (91.8 examples/sec; 0.044 sec/batch; 1h:43m:48s remains)
INFO - root - 2019-11-06 20:15:39.896294: step 7120, total loss = 0.46, predict loss = 0.12 (64.9 examples/sec; 0.062 sec/batch; 2h:26m:52s remains)
INFO - root - 2019-11-06 20:15:40.527719: step 7130, total loss = 0.43, predict loss = 0.13 (80.3 examples/sec; 0.050 sec/batch; 1h:58m:40s remains)
INFO - root - 2019-11-06 20:15:41.107934: step 7140, total loss = 0.51, predict loss = 0.13 (77.7 examples/sec; 0.051 sec/batch; 2h:02m:33s remains)
INFO - root - 2019-11-06 20:15:41.673436: step 7150, total loss = 0.58, predict loss = 0.11 (80.0 examples/sec; 0.050 sec/batch; 1h:59m:06s remains)
INFO - root - 2019-11-06 20:15:42.244543: step 7160, total loss = 0.42, predict loss = 0.12 (76.9 examples/sec; 0.052 sec/batch; 2h:03m:53s remains)
INFO - root - 2019-11-06 20:15:42.837660: step 7170, total loss = 0.38, predict loss = 0.10 (76.1 examples/sec; 0.053 sec/batch; 2h:05m:12s remains)
INFO - root - 2019-11-06 20:15:43.404386: step 7180, total loss = 0.41, predict loss = 0.09 (73.4 examples/sec; 0.055 sec/batch; 2h:09m:46s remains)
INFO - root - 2019-11-06 20:15:43.978091: step 7190, total loss = 0.33, predict loss = 0.08 (76.2 examples/sec; 0.052 sec/batch; 2h:04m:55s remains)
INFO - root - 2019-11-06 20:15:44.560707: step 7200, total loss = 0.57, predict loss = 0.15 (79.1 examples/sec; 0.051 sec/batch; 2h:00m:17s remains)
INFO - root - 2019-11-06 20:15:45.178277: step 7210, total loss = 0.79, predict loss = 0.24 (65.3 examples/sec; 0.061 sec/batch; 2h:25m:44s remains)
INFO - root - 2019-11-06 20:15:45.759501: step 7220, total loss = 0.45, predict loss = 0.11 (76.3 examples/sec; 0.052 sec/batch; 2h:04m:42s remains)
INFO - root - 2019-11-06 20:15:46.340327: step 7230, total loss = 0.37, predict loss = 0.10 (78.3 examples/sec; 0.051 sec/batch; 2h:01m:31s remains)
INFO - root - 2019-11-06 20:15:46.838962: step 7240, total loss = 0.55, predict loss = 0.15 (94.1 examples/sec; 0.043 sec/batch; 1h:41m:08s remains)
INFO - root - 2019-11-06 20:15:47.315951: step 7250, total loss = 0.80, predict loss = 0.24 (99.6 examples/sec; 0.040 sec/batch; 1h:35m:30s remains)
INFO - root - 2019-11-06 20:15:47.777176: step 7260, total loss = 0.69, predict loss = 0.17 (92.7 examples/sec; 0.043 sec/batch; 1h:42m:41s remains)
INFO - root - 2019-11-06 20:15:48.885119: step 7270, total loss = 0.61, predict loss = 0.19 (59.1 examples/sec; 0.068 sec/batch; 2h:41m:06s remains)
INFO - root - 2019-11-06 20:15:49.518662: step 7280, total loss = 0.37, predict loss = 0.08 (77.7 examples/sec; 0.051 sec/batch; 2h:02m:27s remains)
INFO - root - 2019-11-06 20:15:50.107001: step 7290, total loss = 0.35, predict loss = 0.09 (75.1 examples/sec; 0.053 sec/batch; 2h:06m:36s remains)
INFO - root - 2019-11-06 20:15:50.689458: step 7300, total loss = 0.27, predict loss = 0.07 (76.8 examples/sec; 0.052 sec/batch; 2h:03m:56s remains)
INFO - root - 2019-11-06 20:15:51.259092: step 7310, total loss = 0.47, predict loss = 0.12 (77.6 examples/sec; 0.052 sec/batch; 2h:02m:37s remains)
INFO - root - 2019-11-06 20:15:51.831551: step 7320, total loss = 0.48, predict loss = 0.12 (78.1 examples/sec; 0.051 sec/batch; 2h:01m:43s remains)
INFO - root - 2019-11-06 20:15:52.406572: step 7330, total loss = 0.44, predict loss = 0.09 (78.0 examples/sec; 0.051 sec/batch; 2h:01m:59s remains)
INFO - root - 2019-11-06 20:15:52.980870: step 7340, total loss = 0.53, predict loss = 0.14 (77.7 examples/sec; 0.051 sec/batch; 2h:02m:26s remains)
INFO - root - 2019-11-06 20:15:53.554160: step 7350, total loss = 0.77, predict loss = 0.20 (78.1 examples/sec; 0.051 sec/batch; 2h:01m:49s remains)
INFO - root - 2019-11-06 20:15:54.127457: step 7360, total loss = 0.29, predict loss = 0.07 (80.9 examples/sec; 0.049 sec/batch; 1h:57m:36s remains)
INFO - root - 2019-11-06 20:15:54.720730: step 7370, total loss = 0.65, predict loss = 0.15 (75.7 examples/sec; 0.053 sec/batch; 2h:05m:41s remains)
INFO - root - 2019-11-06 20:15:55.298480: step 7380, total loss = 0.58, predict loss = 0.17 (79.9 examples/sec; 0.050 sec/batch; 1h:58m:59s remains)
INFO - root - 2019-11-06 20:15:55.759606: step 7390, total loss = 0.93, predict loss = 0.28 (104.5 examples/sec; 0.038 sec/batch; 1h:30m:59s remains)
INFO - root - 2019-11-06 20:15:56.213098: step 7400, total loss = 0.42, predict loss = 0.09 (99.3 examples/sec; 0.040 sec/batch; 1h:35m:41s remains)
INFO - root - 2019-11-06 20:15:57.120369: step 7410, total loss = 0.33, predict loss = 0.08 (81.4 examples/sec; 0.049 sec/batch; 1h:56m:42s remains)
INFO - root - 2019-11-06 20:15:57.776299: step 7420, total loss = 0.75, predict loss = 0.18 (66.2 examples/sec; 0.060 sec/batch; 2h:23m:29s remains)
INFO - root - 2019-11-06 20:15:58.391374: step 7430, total loss = 0.73, predict loss = 0.19 (75.7 examples/sec; 0.053 sec/batch; 2h:05m:33s remains)
INFO - root - 2019-11-06 20:15:58.975538: step 7440, total loss = 0.27, predict loss = 0.06 (74.6 examples/sec; 0.054 sec/batch; 2h:07m:22s remains)
INFO - root - 2019-11-06 20:15:59.573570: step 7450, total loss = 0.33, predict loss = 0.09 (80.9 examples/sec; 0.049 sec/batch; 1h:57m:29s remains)
INFO - root - 2019-11-06 20:16:00.147323: step 7460, total loss = 0.32, predict loss = 0.08 (82.6 examples/sec; 0.048 sec/batch; 1h:55m:02s remains)
INFO - root - 2019-11-06 20:16:00.720934: step 7470, total loss = 0.75, predict loss = 0.21 (81.2 examples/sec; 0.049 sec/batch; 1h:57m:03s remains)
INFO - root - 2019-11-06 20:16:01.280070: step 7480, total loss = 0.79, predict loss = 0.22 (80.1 examples/sec; 0.050 sec/batch; 1h:58m:38s remains)
INFO - root - 2019-11-06 20:16:01.862747: step 7490, total loss = 0.48, predict loss = 0.11 (76.9 examples/sec; 0.052 sec/batch; 2h:03m:30s remains)
INFO - root - 2019-11-06 20:16:02.425567: step 7500, total loss = 0.38, predict loss = 0.10 (82.4 examples/sec; 0.049 sec/batch; 1h:55m:14s remains)
INFO - root - 2019-11-06 20:16:03.004660: step 7510, total loss = 0.62, predict loss = 0.18 (76.6 examples/sec; 0.052 sec/batch; 2h:03m:58s remains)
INFO - root - 2019-11-06 20:16:03.593988: step 7520, total loss = 0.64, predict loss = 0.18 (78.9 examples/sec; 0.051 sec/batch; 2h:00m:24s remains)
INFO - root - 2019-11-06 20:16:04.175743: step 7530, total loss = 0.34, predict loss = 0.08 (93.1 examples/sec; 0.043 sec/batch; 1h:42m:00s remains)
INFO - root - 2019-11-06 20:16:04.632175: step 7540, total loss = 0.50, predict loss = 0.14 (97.4 examples/sec; 0.041 sec/batch; 1h:37m:29s remains)
INFO - root - 2019-11-06 20:16:05.084768: step 7550, total loss = 0.45, predict loss = 0.11 (99.2 examples/sec; 0.040 sec/batch; 1h:35m:43s remains)
INFO - root - 2019-11-06 20:16:06.008190: step 7560, total loss = 0.62, predict loss = 0.18 (77.3 examples/sec; 0.052 sec/batch; 2h:02m:47s remains)
INFO - root - 2019-11-06 20:16:06.780835: step 7570, total loss = 0.30, predict loss = 0.07 (55.4 examples/sec; 0.072 sec/batch; 2h:51m:27s remains)
INFO - root - 2019-11-06 20:16:07.406016: step 7580, total loss = 0.49, predict loss = 0.10 (78.1 examples/sec; 0.051 sec/batch; 2h:01m:32s remains)
INFO - root - 2019-11-06 20:16:07.974184: step 7590, total loss = 0.49, predict loss = 0.13 (79.3 examples/sec; 0.050 sec/batch; 1h:59m:47s remains)
INFO - root - 2019-11-06 20:16:08.539353: step 7600, total loss = 0.67, predict loss = 0.18 (74.1 examples/sec; 0.054 sec/batch; 2h:08m:05s remains)
INFO - root - 2019-11-06 20:16:09.134084: step 7610, total loss = 0.36, predict loss = 0.10 (77.1 examples/sec; 0.052 sec/batch; 2h:03m:04s remains)
INFO - root - 2019-11-06 20:16:09.719515: step 7620, total loss = 0.52, predict loss = 0.12 (74.3 examples/sec; 0.054 sec/batch; 2h:07m:40s remains)
INFO - root - 2019-11-06 20:16:10.295037: step 7630, total loss = 0.34, predict loss = 0.08 (80.1 examples/sec; 0.050 sec/batch; 1h:58m:33s remains)
INFO - root - 2019-11-06 20:16:10.877773: step 7640, total loss = 0.42, predict loss = 0.10 (76.9 examples/sec; 0.052 sec/batch; 2h:03m:24s remains)
INFO - root - 2019-11-06 20:16:11.464500: step 7650, total loss = 0.61, predict loss = 0.19 (81.5 examples/sec; 0.049 sec/batch; 1h:56m:25s remains)
INFO - root - 2019-11-06 20:16:12.043114: step 7660, total loss = 0.33, predict loss = 0.09 (78.1 examples/sec; 0.051 sec/batch; 2h:01m:30s remains)
INFO - root - 2019-11-06 20:16:12.624733: step 7670, total loss = 0.51, predict loss = 0.14 (78.9 examples/sec; 0.051 sec/batch; 2h:00m:11s remains)
INFO - root - 2019-11-06 20:16:13.174026: step 7680, total loss = 0.63, predict loss = 0.19 (96.8 examples/sec; 0.041 sec/batch; 1h:38m:00s remains)
INFO - root - 2019-11-06 20:16:13.653046: step 7690, total loss = 0.39, predict loss = 0.10 (96.1 examples/sec; 0.042 sec/batch; 1h:38m:43s remains)
INFO - root - 2019-11-06 20:16:14.102023: step 7700, total loss = 0.77, predict loss = 0.25 (94.8 examples/sec; 0.042 sec/batch; 1h:40m:01s remains)
INFO - root - 2019-11-06 20:16:15.058319: step 7710, total loss = 0.30, predict loss = 0.08 (57.2 examples/sec; 0.070 sec/batch; 2h:45m:57s remains)
INFO - root - 2019-11-06 20:16:15.693802: step 7720, total loss = 0.49, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 2h:17m:57s remains)
INFO - root - 2019-11-06 20:16:16.350058: step 7730, total loss = 0.56, predict loss = 0.14 (74.6 examples/sec; 0.054 sec/batch; 2h:07m:04s remains)
INFO - root - 2019-11-06 20:16:16.941492: step 7740, total loss = 0.55, predict loss = 0.12 (72.7 examples/sec; 0.055 sec/batch; 2h:10m:23s remains)
INFO - root - 2019-11-06 20:16:17.512484: step 7750, total loss = 0.45, predict loss = 0.14 (80.2 examples/sec; 0.050 sec/batch; 1h:58m:11s remains)
INFO - root - 2019-11-06 20:16:18.091445: step 7760, total loss = 0.37, predict loss = 0.10 (74.9 examples/sec; 0.053 sec/batch; 2h:06m:31s remains)
INFO - root - 2019-11-06 20:16:18.681977: step 7770, total loss = 0.58, predict loss = 0.15 (76.2 examples/sec; 0.053 sec/batch; 2h:04m:30s remains)
INFO - root - 2019-11-06 20:16:19.246179: step 7780, total loss = 0.42, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 2h:02m:09s remains)
INFO - root - 2019-11-06 20:16:19.826996: step 7790, total loss = 0.31, predict loss = 0.08 (77.6 examples/sec; 0.052 sec/batch; 2h:02m:09s remains)
INFO - root - 2019-11-06 20:16:20.394926: step 7800, total loss = 0.83, predict loss = 0.22 (74.2 examples/sec; 0.054 sec/batch; 2h:07m:46s remains)
INFO - root - 2019-11-06 20:16:20.989311: step 7810, total loss = 0.58, predict loss = 0.13 (81.6 examples/sec; 0.049 sec/batch; 1h:56m:08s remains)
INFO - root - 2019-11-06 20:16:21.559058: step 7820, total loss = 0.31, predict loss = 0.08 (82.1 examples/sec; 0.049 sec/batch; 1h:55m:23s remains)
INFO - root - 2019-11-06 20:16:22.081146: step 7830, total loss = 0.36, predict loss = 0.09 (94.0 examples/sec; 0.043 sec/batch; 1h:40m:51s remains)
INFO - root - 2019-11-06 20:16:22.541595: step 7840, total loss = 0.45, predict loss = 0.12 (93.4 examples/sec; 0.043 sec/batch; 1h:41m:27s remains)
INFO - root - 2019-11-06 20:16:23.015075: step 7850, total loss = 0.74, predict loss = 0.24 (98.9 examples/sec; 0.040 sec/batch; 1h:35m:48s remains)
INFO - root - 2019-11-06 20:16:24.013400: step 7860, total loss = 0.46, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 2h:19m:56s remains)
INFO - root - 2019-11-06 20:16:24.721328: step 7870, total loss = 0.50, predict loss = 0.14 (63.3 examples/sec; 0.063 sec/batch; 2h:29m:47s remains)
INFO - root - 2019-11-06 20:16:25.328939: step 7880, total loss = 0.36, predict loss = 0.09 (81.3 examples/sec; 0.049 sec/batch; 1h:56m:29s remains)
INFO - root - 2019-11-06 20:16:25.919597: step 7890, total loss = 0.36, predict loss = 0.11 (76.1 examples/sec; 0.053 sec/batch; 2h:04m:33s remains)
INFO - root - 2019-11-06 20:16:26.501842: step 7900, total loss = 0.42, predict loss = 0.10 (79.5 examples/sec; 0.050 sec/batch; 1h:59m:05s remains)
INFO - root - 2019-11-06 20:16:27.072233: step 7910, total loss = 0.34, predict loss = 0.09 (76.3 examples/sec; 0.052 sec/batch; 2h:04m:05s remains)
INFO - root - 2019-11-06 20:16:27.661851: step 7920, total loss = 0.57, predict loss = 0.17 (77.1 examples/sec; 0.052 sec/batch; 2h:02m:51s remains)
INFO - root - 2019-11-06 20:16:28.246830: step 7930, total loss = 0.45, predict loss = 0.11 (81.1 examples/sec; 0.049 sec/batch; 1h:56m:49s remains)
INFO - root - 2019-11-06 20:16:28.810603: step 7940, total loss = 0.33, predict loss = 0.07 (81.3 examples/sec; 0.049 sec/batch; 1h:56m:27s remains)
INFO - root - 2019-11-06 20:16:29.385851: step 7950, total loss = 0.51, predict loss = 0.11 (76.2 examples/sec; 0.053 sec/batch; 2h:04m:21s remains)
INFO - root - 2019-11-06 20:16:29.967462: step 7960, total loss = 0.42, predict loss = 0.10 (77.8 examples/sec; 0.051 sec/batch; 2h:01m:41s remains)
INFO - root - 2019-11-06 20:16:30.552086: step 7970, total loss = 0.70, predict loss = 0.19 (81.8 examples/sec; 0.049 sec/batch; 1h:55m:41s remains)
INFO - root - 2019-11-06 20:16:31.069299: step 7980, total loss = 0.91, predict loss = 0.29 (100.9 examples/sec; 0.040 sec/batch; 1h:33m:49s remains)
INFO - root - 2019-11-06 20:16:31.527493: step 7990, total loss = 0.51, predict loss = 0.15 (95.7 examples/sec; 0.042 sec/batch; 1h:38m:54s remains)
INFO - root - 2019-11-06 20:16:31.992840: step 8000, total loss = 0.29, predict loss = 0.08 (91.7 examples/sec; 0.044 sec/batch; 1h:43m:11s remains)
INFO - root - 2019-11-06 20:16:33.056836: step 8010, total loss = 0.34, predict loss = 0.09 (65.3 examples/sec; 0.061 sec/batch; 2h:24m:56s remains)
INFO - root - 2019-11-06 20:16:33.670343: step 8020, total loss = 0.33, predict loss = 0.09 (74.7 examples/sec; 0.054 sec/batch; 2h:06m:44s remains)
INFO - root - 2019-11-06 20:16:34.223768: step 8030, total loss = 0.49, predict loss = 0.15 (78.6 examples/sec; 0.051 sec/batch; 2h:00m:28s remains)
INFO - root - 2019-11-06 20:16:34.790757: step 8040, total loss = 0.82, predict loss = 0.22 (79.8 examples/sec; 0.050 sec/batch; 1h:58m:39s remains)
INFO - root - 2019-11-06 20:16:35.388746: step 8050, total loss = 0.53, predict loss = 0.12 (79.8 examples/sec; 0.050 sec/batch; 1h:58m:34s remains)
INFO - root - 2019-11-06 20:16:35.955934: step 8060, total loss = 0.53, predict loss = 0.12 (80.4 examples/sec; 0.050 sec/batch; 1h:57m:42s remains)
INFO - root - 2019-11-06 20:16:36.517971: step 8070, total loss = 0.68, predict loss = 0.19 (80.2 examples/sec; 0.050 sec/batch; 1h:58m:01s remains)
INFO - root - 2019-11-06 20:16:37.099388: step 8080, total loss = 0.59, predict loss = 0.21 (80.9 examples/sec; 0.049 sec/batch; 1h:56m:58s remains)
INFO - root - 2019-11-06 20:16:37.691016: step 8090, total loss = 0.66, predict loss = 0.19 (76.7 examples/sec; 0.052 sec/batch; 2h:03m:23s remains)
INFO - root - 2019-11-06 20:16:38.262780: step 8100, total loss = 0.68, predict loss = 0.18 (78.8 examples/sec; 0.051 sec/batch; 2h:00m:04s remains)
INFO - root - 2019-11-06 20:16:38.841627: step 8110, total loss = 0.54, predict loss = 0.12 (78.5 examples/sec; 0.051 sec/batch; 2h:00m:32s remains)
INFO - root - 2019-11-06 20:16:39.423470: step 8120, total loss = 0.37, predict loss = 0.09 (78.3 examples/sec; 0.051 sec/batch; 2h:00m:52s remains)
INFO - root - 2019-11-06 20:16:39.914210: step 8130, total loss = 0.55, predict loss = 0.14 (97.7 examples/sec; 0.041 sec/batch; 1h:36m:48s remains)
INFO - root - 2019-11-06 20:16:40.360386: step 8140, total loss = 1.01, predict loss = 0.30 (99.6 examples/sec; 0.040 sec/batch; 1h:34m:57s remains)
INFO - root - 2019-11-06 20:16:41.252763: step 8150, total loss = 0.24, predict loss = 0.07 (8.4 examples/sec; 0.476 sec/batch; 18h:45m:49s remains)
INFO - root - 2019-11-06 20:16:41.923201: step 8160, total loss = 0.38, predict loss = 0.08 (59.1 examples/sec; 0.068 sec/batch; 2h:40m:00s remains)
INFO - root - 2019-11-06 20:16:42.583763: step 8170, total loss = 0.31, predict loss = 0.08 (74.1 examples/sec; 0.054 sec/batch; 2h:07m:32s remains)
INFO - root - 2019-11-06 20:16:43.174048: step 8180, total loss = 0.33, predict loss = 0.10 (77.5 examples/sec; 0.052 sec/batch; 2h:01m:55s remains)
INFO - root - 2019-11-06 20:16:43.747285: step 8190, total loss = 0.28, predict loss = 0.07 (80.1 examples/sec; 0.050 sec/batch; 1h:57m:57s remains)
INFO - root - 2019-11-06 20:16:44.314338: step 8200, total loss = 0.45, predict loss = 0.12 (79.6 examples/sec; 0.050 sec/batch; 1h:58m:49s remains)
INFO - root - 2019-11-06 20:16:44.903198: step 8210, total loss = 0.61, predict loss = 0.16 (74.6 examples/sec; 0.054 sec/batch; 2h:06m:46s remains)
INFO - root - 2019-11-06 20:16:45.527515: step 8220, total loss = 0.58, predict loss = 0.16 (81.7 examples/sec; 0.049 sec/batch; 1h:55m:38s remains)
INFO - root - 2019-11-06 20:16:46.104632: step 8230, total loss = 0.35, predict loss = 0.08 (79.6 examples/sec; 0.050 sec/batch; 1h:58m:45s remains)
INFO - root - 2019-11-06 20:16:46.684405: step 8240, total loss = 0.41, predict loss = 0.10 (80.9 examples/sec; 0.049 sec/batch; 1h:56m:51s remains)
INFO - root - 2019-11-06 20:16:47.269496: step 8250, total loss = 0.32, predict loss = 0.07 (79.2 examples/sec; 0.051 sec/batch; 1h:59m:20s remains)
INFO - root - 2019-11-06 20:16:47.836409: step 8260, total loss = 0.74, predict loss = 0.24 (78.3 examples/sec; 0.051 sec/batch; 2h:00m:42s remains)
INFO - root - 2019-11-06 20:16:48.406122: step 8270, total loss = 0.82, predict loss = 0.21 (86.3 examples/sec; 0.046 sec/batch; 1h:49m:31s remains)
INFO - root - 2019-11-06 20:16:48.874495: step 8280, total loss = 0.46, predict loss = 0.12 (98.2 examples/sec; 0.041 sec/batch; 1h:36m:14s remains)
INFO - root - 2019-11-06 20:16:49.359918: step 8290, total loss = 0.35, predict loss = 0.09 (93.5 examples/sec; 0.043 sec/batch; 1h:41m:04s remains)
INFO - root - 2019-11-06 20:16:50.273238: step 8300, total loss = 0.34, predict loss = 0.09 (76.6 examples/sec; 0.052 sec/batch; 2h:03m:15s remains)
INFO - root - 2019-11-06 20:16:51.021476: step 8310, total loss = 0.50, predict loss = 0.12 (53.8 examples/sec; 0.074 sec/batch; 2h:55m:27s remains)
INFO - root - 2019-11-06 20:16:51.652078: step 8320, total loss = 0.38, predict loss = 0.09 (79.8 examples/sec; 0.050 sec/batch; 1h:58m:23s remains)
INFO - root - 2019-11-06 20:16:52.241489: step 8330, total loss = 0.41, predict loss = 0.10 (78.7 examples/sec; 0.051 sec/batch; 1h:59m:58s remains)
INFO - root - 2019-11-06 20:16:52.811133: step 8340, total loss = 0.35, predict loss = 0.08 (80.1 examples/sec; 0.050 sec/batch; 1h:57m:54s remains)
INFO - root - 2019-11-06 20:16:53.389541: step 8350, total loss = 0.30, predict loss = 0.08 (75.4 examples/sec; 0.053 sec/batch; 2h:05m:11s remains)
INFO - root - 2019-11-06 20:16:53.960419: step 8360, total loss = 0.47, predict loss = 0.10 (76.9 examples/sec; 0.052 sec/batch; 2h:02m:51s remains)
INFO - root - 2019-11-06 20:16:54.538761: step 8370, total loss = 0.43, predict loss = 0.09 (82.7 examples/sec; 0.048 sec/batch; 1h:54m:10s remains)
INFO - root - 2019-11-06 20:16:55.113737: step 8380, total loss = 0.66, predict loss = 0.17 (77.2 examples/sec; 0.052 sec/batch; 2h:02m:13s remains)
INFO - root - 2019-11-06 20:16:55.680633: step 8390, total loss = 0.27, predict loss = 0.07 (79.3 examples/sec; 0.050 sec/batch; 1h:59m:03s remains)
INFO - root - 2019-11-06 20:16:56.265058: step 8400, total loss = 0.25, predict loss = 0.07 (79.0 examples/sec; 0.051 sec/batch; 1h:59m:28s remains)
INFO - root - 2019-11-06 20:16:56.859719: step 8410, total loss = 0.87, predict loss = 0.27 (79.9 examples/sec; 0.050 sec/batch; 1h:58m:09s remains)
INFO - root - 2019-11-06 20:16:57.416309: step 8420, total loss = 0.30, predict loss = 0.07 (90.7 examples/sec; 0.044 sec/batch; 1h:44m:05s remains)
INFO - root - 2019-11-06 20:16:57.867679: step 8430, total loss = 0.39, predict loss = 0.10 (96.6 examples/sec; 0.041 sec/batch; 1h:37m:44s remains)
INFO - root - 2019-11-06 20:16:58.318650: step 8440, total loss = 0.27, predict loss = 0.06 (94.5 examples/sec; 0.042 sec/batch; 1h:39m:49s remains)
INFO - root - 2019-11-06 20:16:59.305124: step 8450, total loss = 0.37, predict loss = 0.08 (64.5 examples/sec; 0.062 sec/batch; 2h:26m:19s remains)
INFO - root - 2019-11-06 20:16:59.926396: step 8460, total loss = 0.39, predict loss = 0.08 (79.4 examples/sec; 0.050 sec/batch; 1h:58m:52s remains)
INFO - root - 2019-11-06 20:17:00.516989: step 8470, total loss = 0.46, predict loss = 0.10 (75.0 examples/sec; 0.053 sec/batch; 2h:05m:44s remains)
INFO - root - 2019-11-06 20:17:01.090550: step 8480, total loss = 0.55, predict loss = 0.16 (79.5 examples/sec; 0.050 sec/batch; 1h:58m:37s remains)
INFO - root - 2019-11-06 20:17:01.672011: step 8490, total loss = 0.31, predict loss = 0.09 (79.4 examples/sec; 0.050 sec/batch; 1h:58m:45s remains)
INFO - root - 2019-11-06 20:17:02.232037: step 8500, total loss = 0.56, predict loss = 0.14 (78.2 examples/sec; 0.051 sec/batch; 2h:00m:38s remains)
INFO - root - 2019-11-06 20:17:02.804106: step 8510, total loss = 0.74, predict loss = 0.21 (78.7 examples/sec; 0.051 sec/batch; 1h:59m:52s remains)
INFO - root - 2019-11-06 20:17:03.375093: step 8520, total loss = 0.30, predict loss = 0.07 (78.9 examples/sec; 0.051 sec/batch; 1h:59m:31s remains)
INFO - root - 2019-11-06 20:17:03.962792: step 8530, total loss = 0.54, predict loss = 0.13 (78.3 examples/sec; 0.051 sec/batch; 2h:00m:23s remains)
INFO - root - 2019-11-06 20:17:04.526731: step 8540, total loss = 0.59, predict loss = 0.18 (78.4 examples/sec; 0.051 sec/batch; 2h:00m:15s remains)
INFO - root - 2019-11-06 20:17:05.098440: step 8550, total loss = 0.34, predict loss = 0.09 (81.2 examples/sec; 0.049 sec/batch; 1h:56m:09s remains)
INFO - root - 2019-11-06 20:17:05.668009: step 8560, total loss = 0.33, predict loss = 0.08 (80.3 examples/sec; 0.050 sec/batch; 1h:57m:28s remains)
INFO - root - 2019-11-06 20:17:06.206260: step 8570, total loss = 0.46, predict loss = 0.11 (91.8 examples/sec; 0.044 sec/batch; 1h:42m:44s remains)
INFO - root - 2019-11-06 20:17:06.664361: step 8580, total loss = 0.73, predict loss = 0.18 (97.2 examples/sec; 0.041 sec/batch; 1h:36m:57s remains)
INFO - root - 2019-11-06 20:17:07.123299: step 8590, total loss = 0.33, predict loss = 0.08 (96.2 examples/sec; 0.042 sec/batch; 1h:37m:59s remains)
INFO - root - 2019-11-06 20:17:08.117207: step 8600, total loss = 0.51, predict loss = 0.14 (65.5 examples/sec; 0.061 sec/batch; 2h:23m:57s remains)
INFO - root - 2019-11-06 20:17:08.757743: step 8610, total loss = 0.76, predict loss = 0.23 (76.9 examples/sec; 0.052 sec/batch; 2h:02m:38s remains)
INFO - root - 2019-11-06 20:17:09.329657: step 8620, total loss = 0.78, predict loss = 0.19 (81.2 examples/sec; 0.049 sec/batch; 1h:56m:05s remains)
INFO - root - 2019-11-06 20:17:09.907136: step 8630, total loss = 0.28, predict loss = 0.07 (79.0 examples/sec; 0.051 sec/batch; 1h:59m:16s remains)
INFO - root - 2019-11-06 20:17:10.492869: step 8640, total loss = 0.31, predict loss = 0.08 (80.4 examples/sec; 0.050 sec/batch; 1h:57m:13s remains)
INFO - root - 2019-11-06 20:17:11.093640: step 8650, total loss = 0.31, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:59m:08s remains)
INFO - root - 2019-11-06 20:17:11.668917: step 8660, total loss = 0.92, predict loss = 0.26 (75.2 examples/sec; 0.053 sec/batch; 2h:05m:19s remains)
INFO - root - 2019-11-06 20:17:12.232480: step 8670, total loss = 0.25, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:59m:24s remains)
INFO - root - 2019-11-06 20:17:12.805824: step 8680, total loss = 0.45, predict loss = 0.09 (79.7 examples/sec; 0.050 sec/batch; 1h:58m:15s remains)
INFO - root - 2019-11-06 20:17:13.392387: step 8690, total loss = 0.32, predict loss = 0.07 (78.1 examples/sec; 0.051 sec/batch; 2h:00m:38s remains)
INFO - root - 2019-11-06 20:17:13.974425: step 8700, total loss = 0.75, predict loss = 0.23 (76.3 examples/sec; 0.052 sec/batch; 2h:03m:22s remains)
INFO - root - 2019-11-06 20:17:14.553237: step 8710, total loss = 0.75, predict loss = 0.20 (78.2 examples/sec; 0.051 sec/batch; 2h:00m:30s remains)
INFO - root - 2019-11-06 20:17:15.086251: step 8720, total loss = 0.53, predict loss = 0.17 (97.0 examples/sec; 0.041 sec/batch; 1h:37m:04s remains)
INFO - root - 2019-11-06 20:17:15.578401: step 8730, total loss = 0.48, predict loss = 0.11 (94.1 examples/sec; 0.043 sec/batch; 1h:40m:06s remains)
INFO - root - 2019-11-06 20:17:16.022760: step 8740, total loss = 0.34, predict loss = 0.08 (97.0 examples/sec; 0.041 sec/batch; 1h:37m:08s remains)
INFO - root - 2019-11-06 20:17:17.110816: step 8750, total loss = 0.59, predict loss = 0.16 (48.4 examples/sec; 0.083 sec/batch; 3h:14m:34s remains)
INFO - root - 2019-11-06 20:17:17.744985: step 8760, total loss = 0.29, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 2h:00m:14s remains)
INFO - root - 2019-11-06 20:17:18.332738: step 8770, total loss = 0.33, predict loss = 0.08 (73.9 examples/sec; 0.054 sec/batch; 2h:07m:21s remains)
INFO - root - 2019-11-06 20:17:18.914495: step 8780, total loss = 0.36, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 2h:00m:24s remains)
INFO - root - 2019-11-06 20:17:19.488482: step 8790, total loss = 0.27, predict loss = 0.08 (79.2 examples/sec; 0.051 sec/batch; 1h:58m:55s remains)
INFO - root - 2019-11-06 20:17:20.054664: step 8800, total loss = 0.80, predict loss = 0.23 (76.9 examples/sec; 0.052 sec/batch; 2h:02m:24s remains)
INFO - root - 2019-11-06 20:17:20.629599: step 8810, total loss = 0.30, predict loss = 0.08 (83.1 examples/sec; 0.048 sec/batch; 1h:53m:13s remains)
INFO - root - 2019-11-06 20:17:21.194429: step 8820, total loss = 0.39, predict loss = 0.10 (78.0 examples/sec; 0.051 sec/batch; 2h:00m:36s remains)
INFO - root - 2019-11-06 20:17:21.783022: step 8830, total loss = 0.37, predict loss = 0.09 (73.8 examples/sec; 0.054 sec/batch; 2h:07m:31s remains)
INFO - root - 2019-11-06 20:17:22.364972: step 8840, total loss = 0.68, predict loss = 0.20 (78.3 examples/sec; 0.051 sec/batch; 2h:00m:10s remains)
INFO - root - 2019-11-06 20:17:22.946233: step 8850, total loss = 0.40, predict loss = 0.10 (75.2 examples/sec; 0.053 sec/batch; 2h:05m:03s remains)
INFO - root - 2019-11-06 20:17:23.523236: step 8860, total loss = 0.35, predict loss = 0.09 (75.1 examples/sec; 0.053 sec/batch; 2h:05m:16s remains)
INFO - root - 2019-11-06 20:17:24.016514: step 8870, total loss = 0.58, predict loss = 0.13 (101.3 examples/sec; 0.039 sec/batch; 1h:32m:53s remains)
INFO - root - 2019-11-06 20:17:24.469692: step 8880, total loss = 0.65, predict loss = 0.18 (101.0 examples/sec; 0.040 sec/batch; 1h:33m:07s remains)
INFO - root - 2019-11-06 20:17:24.946748: step 8890, total loss = 0.62, predict loss = 0.19 (95.5 examples/sec; 0.042 sec/batch; 1h:38m:33s remains)
INFO - root - 2019-11-06 20:17:26.062018: step 8900, total loss = 0.54, predict loss = 0.12 (57.5 examples/sec; 0.070 sec/batch; 2h:43m:27s remains)
INFO - root - 2019-11-06 20:17:26.680999: step 8910, total loss = 0.39, predict loss = 0.09 (74.7 examples/sec; 0.054 sec/batch; 2h:05m:55s remains)
INFO - root - 2019-11-06 20:17:27.244344: step 8920, total loss = 0.45, predict loss = 0.11 (76.9 examples/sec; 0.052 sec/batch; 2h:02m:14s remains)
INFO - root - 2019-11-06 20:17:27.824956: step 8930, total loss = 0.66, predict loss = 0.18 (83.1 examples/sec; 0.048 sec/batch; 1h:53m:07s remains)
INFO - root - 2019-11-06 20:17:28.389675: step 8940, total loss = 0.51, predict loss = 0.13 (79.0 examples/sec; 0.051 sec/batch; 1h:59m:04s remains)
INFO - root - 2019-11-06 20:17:28.958312: step 8950, total loss = 0.53, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 2h:03m:56s remains)
INFO - root - 2019-11-06 20:17:29.533797: step 8960, total loss = 0.61, predict loss = 0.16 (80.5 examples/sec; 0.050 sec/batch; 1h:56m:47s remains)
INFO - root - 2019-11-06 20:17:30.120374: step 8970, total loss = 0.37, predict loss = 0.08 (76.8 examples/sec; 0.052 sec/batch; 2h:02m:21s remains)
INFO - root - 2019-11-06 20:17:30.696965: step 8980, total loss = 0.39, predict loss = 0.11 (77.0 examples/sec; 0.052 sec/batch; 2h:02m:04s remains)
INFO - root - 2019-11-06 20:17:31.271580: step 8990, total loss = 0.52, predict loss = 0.13 (80.3 examples/sec; 0.050 sec/batch; 1h:57m:07s remains)
INFO - root - 2019-11-06 20:17:31.845450: step 9000, total loss = 0.65, predict loss = 0.19 (77.5 examples/sec; 0.052 sec/batch; 2h:01m:19s remains)
INFO - root - 2019-11-06 20:17:32.442066: step 9010, total loss = 0.65, predict loss = 0.17 (80.2 examples/sec; 0.050 sec/batch; 1h:57m:15s remains)
INFO - root - 2019-11-06 20:17:32.913285: step 9020, total loss = 0.54, predict loss = 0.14 (99.6 examples/sec; 0.040 sec/batch; 1h:34m:19s remains)
INFO - root - 2019-11-06 20:17:33.359729: step 9030, total loss = 0.38, predict loss = 0.09 (98.7 examples/sec; 0.041 sec/batch; 1h:35m:10s remains)
INFO - root - 2019-11-06 20:17:34.263077: step 9040, total loss = 0.56, predict loss = 0.16 (79.2 examples/sec; 0.050 sec/batch; 1h:58m:37s remains)
INFO - root - 2019-11-06 20:17:34.966680: step 9050, total loss = 0.38, predict loss = 0.11 (60.8 examples/sec; 0.066 sec/batch; 2h:34m:38s remains)
INFO - root - 2019-11-06 20:17:35.587405: step 9060, total loss = 0.35, predict loss = 0.09 (80.2 examples/sec; 0.050 sec/batch; 1h:57m:13s remains)
INFO - root - 2019-11-06 20:17:36.161195: step 9070, total loss = 0.54, predict loss = 0.16 (84.0 examples/sec; 0.048 sec/batch; 1h:51m:52s remains)
INFO - root - 2019-11-06 20:17:36.742681: step 9080, total loss = 0.39, predict loss = 0.09 (76.9 examples/sec; 0.052 sec/batch; 2h:02m:08s remains)
INFO - root - 2019-11-06 20:17:37.327477: step 9090, total loss = 0.46, predict loss = 0.13 (80.4 examples/sec; 0.050 sec/batch; 1h:56m:51s remains)
INFO - root - 2019-11-06 20:17:37.902971: step 9100, total loss = 0.41, predict loss = 0.10 (80.2 examples/sec; 0.050 sec/batch; 1h:57m:03s remains)
INFO - root - 2019-11-06 20:17:38.474783: step 9110, total loss = 0.34, predict loss = 0.08 (79.6 examples/sec; 0.050 sec/batch; 1h:57m:59s remains)
INFO - root - 2019-11-06 20:17:39.039219: step 9120, total loss = 0.34, predict loss = 0.10 (79.2 examples/sec; 0.050 sec/batch; 1h:58m:33s remains)
INFO - root - 2019-11-06 20:17:39.627302: step 9130, total loss = 0.34, predict loss = 0.08 (80.5 examples/sec; 0.050 sec/batch; 1h:56m:37s remains)
INFO - root - 2019-11-06 20:17:40.192765: step 9140, total loss = 0.41, predict loss = 0.10 (80.3 examples/sec; 0.050 sec/batch; 1h:56m:59s remains)
INFO - root - 2019-11-06 20:17:40.778724: step 9150, total loss = 0.36, predict loss = 0.09 (77.2 examples/sec; 0.052 sec/batch; 2h:01m:38s remains)
INFO - root - 2019-11-06 20:17:41.351063: step 9160, total loss = 0.39, predict loss = 0.09 (88.4 examples/sec; 0.045 sec/batch; 1h:46m:14s remains)
INFO - root - 2019-11-06 20:17:41.827559: step 9170, total loss = 0.48, predict loss = 0.13 (95.1 examples/sec; 0.042 sec/batch; 1h:38m:45s remains)
INFO - root - 2019-11-06 20:17:42.288897: step 9180, total loss = 0.82, predict loss = 0.21 (89.8 examples/sec; 0.045 sec/batch; 1h:44m:31s remains)
INFO - root - 2019-11-06 20:17:43.231251: step 9190, total loss = 0.32, predict loss = 0.08 (74.4 examples/sec; 0.054 sec/batch; 2h:06m:09s remains)
INFO - root - 2019-11-06 20:17:43.859171: step 9200, total loss = 0.37, predict loss = 0.10 (80.0 examples/sec; 0.050 sec/batch; 1h:57m:18s remains)
INFO - root - 2019-11-06 20:17:44.457817: step 9210, total loss = 0.33, predict loss = 0.08 (75.0 examples/sec; 0.053 sec/batch; 2h:05m:12s remains)
INFO - root - 2019-11-06 20:17:45.064953: step 9220, total loss = 0.42, predict loss = 0.11 (61.2 examples/sec; 0.065 sec/batch; 2h:33m:16s remains)
INFO - root - 2019-11-06 20:17:45.657548: step 9230, total loss = 1.06, predict loss = 0.29 (77.5 examples/sec; 0.052 sec/batch; 2h:01m:04s remains)
INFO - root - 2019-11-06 20:17:46.237747: step 9240, total loss = 0.35, predict loss = 0.10 (75.7 examples/sec; 0.053 sec/batch; 2h:03m:53s remains)
INFO - root - 2019-11-06 20:17:46.829006: step 9250, total loss = 0.66, predict loss = 0.18 (76.1 examples/sec; 0.053 sec/batch; 2h:03m:15s remains)
INFO - root - 2019-11-06 20:17:47.402389: step 9260, total loss = 0.30, predict loss = 0.07 (79.3 examples/sec; 0.050 sec/batch; 1h:58m:21s remains)
INFO - root - 2019-11-06 20:17:47.974552: step 9270, total loss = 0.41, predict loss = 0.11 (77.5 examples/sec; 0.052 sec/batch; 2h:01m:00s remains)
INFO - root - 2019-11-06 20:17:48.535150: step 9280, total loss = 0.25, predict loss = 0.06 (79.4 examples/sec; 0.050 sec/batch; 1h:58m:12s remains)
INFO - root - 2019-11-06 20:17:49.123605: step 9290, total loss = 0.38, predict loss = 0.11 (76.6 examples/sec; 0.052 sec/batch; 2h:02m:32s remains)
INFO - root - 2019-11-06 20:17:49.697187: step 9300, total loss = 0.39, predict loss = 0.12 (79.2 examples/sec; 0.051 sec/batch; 1h:58m:29s remains)
INFO - root - 2019-11-06 20:17:50.244108: step 9310, total loss = 0.62, predict loss = 0.17 (93.6 examples/sec; 0.043 sec/batch; 1h:40m:10s remains)
INFO - root - 2019-11-06 20:17:50.689025: step 9320, total loss = 0.53, predict loss = 0.13 (96.9 examples/sec; 0.041 sec/batch; 1h:36m:47s remains)
INFO - root - 2019-11-06 20:17:51.166465: step 9330, total loss = 0.38, predict loss = 0.10 (90.0 examples/sec; 0.044 sec/batch; 1h:44m:11s remains)
INFO - root - 2019-11-06 20:17:52.132782: step 9340, total loss = 0.37, predict loss = 0.08 (61.0 examples/sec; 0.066 sec/batch; 2h:33m:38s remains)
INFO - root - 2019-11-06 20:17:52.780019: step 9350, total loss = 0.41, predict loss = 0.10 (73.2 examples/sec; 0.055 sec/batch; 2h:08m:06s remains)
INFO - root - 2019-11-06 20:17:53.363083: step 9360, total loss = 0.45, predict loss = 0.12 (82.6 examples/sec; 0.048 sec/batch; 1h:53m:32s remains)
INFO - root - 2019-11-06 20:17:53.958452: step 9370, total loss = 0.67, predict loss = 0.20 (76.6 examples/sec; 0.052 sec/batch; 2h:02m:26s remains)
INFO - root - 2019-11-06 20:17:54.522666: step 9380, total loss = 0.74, predict loss = 0.22 (76.9 examples/sec; 0.052 sec/batch; 2h:01m:55s remains)
INFO - root - 2019-11-06 20:17:55.100640: step 9390, total loss = 0.39, predict loss = 0.10 (76.9 examples/sec; 0.052 sec/batch; 2h:01m:54s remains)
INFO - root - 2019-11-06 20:17:55.700562: step 9400, total loss = 0.47, predict loss = 0.13 (73.6 examples/sec; 0.054 sec/batch; 2h:07m:20s remains)
INFO - root - 2019-11-06 20:17:56.321328: step 9410, total loss = 0.40, predict loss = 0.08 (77.6 examples/sec; 0.052 sec/batch; 2h:00m:48s remains)
INFO - root - 2019-11-06 20:17:56.913099: step 9420, total loss = 0.35, predict loss = 0.11 (83.6 examples/sec; 0.048 sec/batch; 1h:52m:06s remains)
INFO - root - 2019-11-06 20:17:57.492612: step 9430, total loss = 0.50, predict loss = 0.15 (78.2 examples/sec; 0.051 sec/batch; 1h:59m:53s remains)
INFO - root - 2019-11-06 20:17:58.058789: step 9440, total loss = 0.54, predict loss = 0.15 (81.3 examples/sec; 0.049 sec/batch; 1h:55m:13s remains)
INFO - root - 2019-11-06 20:17:58.642611: step 9450, total loss = 0.34, predict loss = 0.08 (78.2 examples/sec; 0.051 sec/batch; 1h:59m:50s remains)
INFO - root - 2019-11-06 20:17:59.168494: step 9460, total loss = 0.53, predict loss = 0.12 (94.7 examples/sec; 0.042 sec/batch; 1h:38m:55s remains)
INFO - root - 2019-11-06 20:17:59.631616: step 9470, total loss = 0.52, predict loss = 0.14 (93.6 examples/sec; 0.043 sec/batch; 1h:40m:05s remains)
INFO - root - 2019-11-06 20:18:00.082858: step 9480, total loss = 0.71, predict loss = 0.20 (97.3 examples/sec; 0.041 sec/batch; 1h:36m:14s remains)
INFO - root - 2019-11-06 20:18:01.158231: step 9490, total loss = 0.54, predict loss = 0.15 (56.1 examples/sec; 0.071 sec/batch; 2h:46m:51s remains)
INFO - root - 2019-11-06 20:18:01.785080: step 9500, total loss = 0.48, predict loss = 0.14 (79.2 examples/sec; 0.051 sec/batch; 1h:58m:16s remains)
INFO - root - 2019-11-06 20:18:02.357783: step 9510, total loss = 0.66, predict loss = 0.18 (79.5 examples/sec; 0.050 sec/batch; 1h:57m:45s remains)
INFO - root - 2019-11-06 20:18:02.925026: step 9520, total loss = 0.33, predict loss = 0.08 (77.7 examples/sec; 0.051 sec/batch; 2h:00m:27s remains)
INFO - root - 2019-11-06 20:18:03.509268: step 9530, total loss = 0.37, predict loss = 0.09 (80.0 examples/sec; 0.050 sec/batch; 1h:57m:00s remains)
INFO - root - 2019-11-06 20:18:04.102421: step 9540, total loss = 0.55, predict loss = 0.19 (78.7 examples/sec; 0.051 sec/batch; 1h:58m:56s remains)
INFO - root - 2019-11-06 20:18:04.668361: step 9550, total loss = 0.49, predict loss = 0.12 (81.8 examples/sec; 0.049 sec/batch; 1h:54m:26s remains)
INFO - root - 2019-11-06 20:18:05.246437: step 9560, total loss = 0.53, predict loss = 0.12 (81.0 examples/sec; 0.049 sec/batch; 1h:55m:39s remains)
INFO - root - 2019-11-06 20:18:05.833310: step 9570, total loss = 0.30, predict loss = 0.07 (80.3 examples/sec; 0.050 sec/batch; 1h:56m:31s remains)
INFO - root - 2019-11-06 20:18:06.419940: step 9580, total loss = 0.32, predict loss = 0.08 (77.3 examples/sec; 0.052 sec/batch; 2h:01m:06s remains)
INFO - root - 2019-11-06 20:18:06.990188: step 9590, total loss = 0.74, predict loss = 0.22 (77.7 examples/sec; 0.051 sec/batch; 2h:00m:27s remains)
INFO - root - 2019-11-06 20:18:07.552854: step 9600, total loss = 0.47, predict loss = 0.10 (79.5 examples/sec; 0.050 sec/batch; 1h:57m:44s remains)
INFO - root - 2019-11-06 20:18:08.066341: step 9610, total loss = 0.42, predict loss = 0.11 (107.3 examples/sec; 0.037 sec/batch; 1h:27m:15s remains)
INFO - root - 2019-11-06 20:18:08.521368: step 9620, total loss = 0.27, predict loss = 0.07 (96.2 examples/sec; 0.042 sec/batch; 1h:37m:17s remains)
INFO - root - 2019-11-06 20:18:08.989300: step 9630, total loss = 0.57, predict loss = 0.14 (96.2 examples/sec; 0.042 sec/batch; 1h:37m:15s remains)
INFO - root - 2019-11-06 20:18:10.046794: step 9640, total loss = 0.46, predict loss = 0.11 (58.3 examples/sec; 0.069 sec/batch; 2h:40m:25s remains)
INFO - root - 2019-11-06 20:18:10.695481: step 9650, total loss = 0.45, predict loss = 0.11 (79.6 examples/sec; 0.050 sec/batch; 1h:57m:30s remains)
INFO - root - 2019-11-06 20:18:11.275875: step 9660, total loss = 0.60, predict loss = 0.14 (78.8 examples/sec; 0.051 sec/batch; 1h:58m:40s remains)
INFO - root - 2019-11-06 20:18:11.851378: step 9670, total loss = 0.37, predict loss = 0.09 (82.2 examples/sec; 0.049 sec/batch; 1h:53m:48s remains)
INFO - root - 2019-11-06 20:18:12.430087: step 9680, total loss = 0.49, predict loss = 0.14 (75.4 examples/sec; 0.053 sec/batch; 2h:04m:08s remains)
INFO - root - 2019-11-06 20:18:13.025558: step 9690, total loss = 0.56, predict loss = 0.15 (77.7 examples/sec; 0.052 sec/batch; 2h:00m:27s remains)
INFO - root - 2019-11-06 20:18:13.581645: step 9700, total loss = 0.45, predict loss = 0.12 (89.2 examples/sec; 0.045 sec/batch; 1h:44m:49s remains)
INFO - root - 2019-11-06 20:18:14.157178: step 9710, total loss = 0.42, predict loss = 0.09 (77.9 examples/sec; 0.051 sec/batch; 2h:00m:06s remains)
INFO - root - 2019-11-06 20:18:14.721915: step 9720, total loss = 0.35, predict loss = 0.08 (76.5 examples/sec; 0.052 sec/batch; 2h:02m:18s remains)
INFO - root - 2019-11-06 20:18:15.359014: step 9730, total loss = 0.33, predict loss = 0.08 (76.5 examples/sec; 0.052 sec/batch; 2h:02m:12s remains)
INFO - root - 2019-11-06 20:18:15.935024: step 9740, total loss = 0.49, predict loss = 0.14 (75.6 examples/sec; 0.053 sec/batch; 2h:03m:37s remains)
INFO - root - 2019-11-06 20:18:16.509258: step 9750, total loss = 0.33, predict loss = 0.07 (81.6 examples/sec; 0.049 sec/batch; 1h:54m:32s remains)
INFO - root - 2019-11-06 20:18:16.984924: step 9760, total loss = 0.62, predict loss = 0.11 (102.0 examples/sec; 0.039 sec/batch; 1h:31m:39s remains)
INFO - root - 2019-11-06 20:18:17.463125: step 9770, total loss = 0.36, predict loss = 0.08 (93.2 examples/sec; 0.043 sec/batch; 1h:40m:18s remains)
INFO - root - 2019-11-06 20:18:18.367576: step 9780, total loss = 0.63, predict loss = 0.19 (8.2 examples/sec; 0.489 sec/batch; 19h:01m:52s remains)
INFO - root - 2019-11-06 20:18:19.013419: step 9790, total loss = 0.28, predict loss = 0.06 (71.6 examples/sec; 0.056 sec/batch; 2h:10m:32s remains)
INFO - root - 2019-11-06 20:18:19.603639: step 9800, total loss = 0.44, predict loss = 0.11 (76.2 examples/sec; 0.052 sec/batch; 2h:02m:39s remains)
INFO - root - 2019-11-06 20:18:20.203920: step 9810, total loss = 0.71, predict loss = 0.22 (72.4 examples/sec; 0.055 sec/batch; 2h:09m:00s remains)
INFO - root - 2019-11-06 20:18:20.779580: step 9820, total loss = 0.66, predict loss = 0.18 (79.7 examples/sec; 0.050 sec/batch; 1h:57m:17s remains)
INFO - root - 2019-11-06 20:18:21.357328: step 9830, total loss = 0.48, predict loss = 0.13 (81.9 examples/sec; 0.049 sec/batch; 1h:54m:02s remains)
INFO - root - 2019-11-06 20:18:21.930260: step 9840, total loss = 0.59, predict loss = 0.18 (74.2 examples/sec; 0.054 sec/batch; 2h:05m:54s remains)
INFO - root - 2019-11-06 20:18:22.517312: step 9850, total loss = 0.54, predict loss = 0.17 (80.6 examples/sec; 0.050 sec/batch; 1h:55m:52s remains)
INFO - root - 2019-11-06 20:18:23.091790: step 9860, total loss = 0.44, predict loss = 0.12 (79.3 examples/sec; 0.050 sec/batch; 1h:57m:47s remains)
INFO - root - 2019-11-06 20:18:23.673850: step 9870, total loss = 0.51, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 2h:08m:43s remains)
INFO - root - 2019-11-06 20:18:24.251216: step 9880, total loss = 0.41, predict loss = 0.12 (76.2 examples/sec; 0.052 sec/batch; 2h:02m:33s remains)
INFO - root - 2019-11-06 20:18:24.835216: step 9890, total loss = 0.27, predict loss = 0.07 (78.7 examples/sec; 0.051 sec/batch; 1h:58m:39s remains)
INFO - root - 2019-11-06 20:18:25.415111: step 9900, total loss = 0.65, predict loss = 0.16 (87.7 examples/sec; 0.046 sec/batch; 1h:46m:26s remains)
INFO - root - 2019-11-06 20:18:25.893115: step 9910, total loss = 0.50, predict loss = 0.12 (92.7 examples/sec; 0.043 sec/batch; 1h:40m:44s remains)
INFO - root - 2019-11-06 20:18:26.350766: step 9920, total loss = 0.43, predict loss = 0.11 (91.1 examples/sec; 0.044 sec/batch; 1h:42m:31s remains)
INFO - root - 2019-11-06 20:18:27.298641: step 9930, total loss = 0.38, predict loss = 0.09 (75.6 examples/sec; 0.053 sec/batch; 2h:03m:26s remains)
INFO - root - 2019-11-06 20:18:28.071303: step 9940, total loss = 0.40, predict loss = 0.12 (54.5 examples/sec; 0.073 sec/batch; 2h:51m:11s remains)
INFO - root - 2019-11-06 20:18:28.720999: step 9950, total loss = 0.60, predict loss = 0.18 (76.7 examples/sec; 0.052 sec/batch; 2h:01m:47s remains)
INFO - root - 2019-11-06 20:18:29.300562: step 9960, total loss = 0.46, predict loss = 0.13 (75.5 examples/sec; 0.053 sec/batch; 2h:03m:39s remains)
INFO - root - 2019-11-06 20:18:29.888584: step 9970, total loss = 0.43, predict loss = 0.13 (81.6 examples/sec; 0.049 sec/batch; 1h:54m:26s remains)
INFO - root - 2019-11-06 20:18:30.446252: step 9980, total loss = 0.69, predict loss = 0.19 (78.2 examples/sec; 0.051 sec/batch; 1h:59m:21s remains)
INFO - root - 2019-11-06 20:18:31.006487: step 9990, total loss = 0.48, predict loss = 0.11 (77.0 examples/sec; 0.052 sec/batch; 2h:01m:15s remains)
INFO - root - 2019-11-06 20:18:31.580069: step 10000, total loss = 0.28, predict loss = 0.06 (78.5 examples/sec; 0.051 sec/batch; 1h:58m:50s remains)
INFO - root - 2019-11-06 20:18:32.185141: step 10010, total loss = 0.72, predict loss = 0.22 (79.9 examples/sec; 0.050 sec/batch; 1h:56m:50s remains)
INFO - root - 2019-11-06 20:18:32.752734: step 10020, total loss = 0.26, predict loss = 0.06 (83.4 examples/sec; 0.048 sec/batch; 1h:51m:53s remains)
INFO - root - 2019-11-06 20:18:33.325833: step 10030, total loss = 0.46, predict loss = 0.13 (78.4 examples/sec; 0.051 sec/batch; 1h:59m:01s remains)
INFO - root - 2019-11-06 20:18:33.897256: step 10040, total loss = 0.30, predict loss = 0.06 (80.8 examples/sec; 0.049 sec/batch; 1h:55m:27s remains)
INFO - root - 2019-11-06 20:18:34.466199: step 10050, total loss = 0.42, predict loss = 0.11 (96.1 examples/sec; 0.042 sec/batch; 1h:37m:05s remains)
INFO - root - 2019-11-06 20:18:34.931910: step 10060, total loss = 0.36, predict loss = 0.09 (95.7 examples/sec; 0.042 sec/batch; 1h:37m:28s remains)
INFO - root - 2019-11-06 20:18:35.394980: step 10070, total loss = 0.37, predict loss = 0.10 (93.7 examples/sec; 0.043 sec/batch; 1h:39m:32s remains)
INFO - root - 2019-11-06 20:18:36.338292: step 10080, total loss = 0.31, predict loss = 0.07 (63.5 examples/sec; 0.063 sec/batch; 2h:26m:50s remains)
INFO - root - 2019-11-06 20:18:36.995142: step 10090, total loss = 0.34, predict loss = 0.09 (71.9 examples/sec; 0.056 sec/batch; 2h:09m:42s remains)
INFO - root - 2019-11-06 20:18:37.608800: step 10100, total loss = 0.50, predict loss = 0.15 (73.1 examples/sec; 0.055 sec/batch; 2h:07m:39s remains)
INFO - root - 2019-11-06 20:18:38.196820: step 10110, total loss = 0.34, predict loss = 0.09 (78.8 examples/sec; 0.051 sec/batch; 1h:58m:17s remains)
INFO - root - 2019-11-06 20:18:38.769693: step 10120, total loss = 0.39, predict loss = 0.09 (83.0 examples/sec; 0.048 sec/batch; 1h:52m:25s remains)
INFO - root - 2019-11-06 20:18:39.355882: step 10130, total loss = 0.48, predict loss = 0.12 (75.2 examples/sec; 0.053 sec/batch; 2h:04m:02s remains)
INFO - root - 2019-11-06 20:18:39.923835: step 10140, total loss = 0.28, predict loss = 0.07 (80.7 examples/sec; 0.050 sec/batch; 1h:55m:28s remains)
INFO - root - 2019-11-06 20:18:40.491116: step 10150, total loss = 0.44, predict loss = 0.11 (76.6 examples/sec; 0.052 sec/batch; 2h:01m:38s remains)
INFO - root - 2019-11-06 20:18:41.062662: step 10160, total loss = 0.87, predict loss = 0.22 (77.7 examples/sec; 0.051 sec/batch; 1h:59m:58s remains)
INFO - root - 2019-11-06 20:18:41.650556: step 10170, total loss = 0.43, predict loss = 0.12 (79.5 examples/sec; 0.050 sec/batch; 1h:57m:14s remains)
INFO - root - 2019-11-06 20:18:42.227553: step 10180, total loss = 0.28, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 2h:00m:26s remains)
INFO - root - 2019-11-06 20:18:42.801879: step 10190, total loss = 0.50, predict loss = 0.16 (76.8 examples/sec; 0.052 sec/batch; 2h:01m:23s remains)
INFO - root - 2019-11-06 20:18:43.327709: step 10200, total loss = 0.50, predict loss = 0.13 (97.7 examples/sec; 0.041 sec/batch; 1h:35m:23s remains)
INFO - root - 2019-11-06 20:18:43.799870: step 10210, total loss = 0.34, predict loss = 0.08 (89.8 examples/sec; 0.045 sec/batch; 1h:43m:44s remains)
INFO - root - 2019-11-06 20:18:44.261555: step 10220, total loss = 0.44, predict loss = 0.15 (91.9 examples/sec; 0.044 sec/batch; 1h:41m:21s remains)
INFO - root - 2019-11-06 20:18:45.280485: step 10230, total loss = 0.49, predict loss = 0.12 (77.8 examples/sec; 0.051 sec/batch; 1h:59m:46s remains)
INFO - root - 2019-11-06 20:18:45.922258: step 10240, total loss = 0.29, predict loss = 0.08 (72.3 examples/sec; 0.055 sec/batch; 2h:08m:51s remains)
INFO - root - 2019-11-06 20:18:46.560591: step 10250, total loss = 0.30, predict loss = 0.09 (76.6 examples/sec; 0.052 sec/batch; 2h:01m:40s remains)
INFO - root - 2019-11-06 20:18:47.143757: step 10260, total loss = 0.41, predict loss = 0.10 (76.7 examples/sec; 0.052 sec/batch; 2h:01m:30s remains)
INFO - root - 2019-11-06 20:18:47.711962: step 10270, total loss = 0.38, predict loss = 0.10 (77.0 examples/sec; 0.052 sec/batch; 2h:01m:02s remains)
INFO - root - 2019-11-06 20:18:48.286889: step 10280, total loss = 0.26, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:56m:54s remains)
INFO - root - 2019-11-06 20:18:48.872908: step 10290, total loss = 0.36, predict loss = 0.08 (76.5 examples/sec; 0.052 sec/batch; 2h:01m:46s remains)
INFO - root - 2019-11-06 20:18:49.456822: step 10300, total loss = 0.56, predict loss = 0.14 (75.4 examples/sec; 0.053 sec/batch; 2h:03m:32s remains)
INFO - root - 2019-11-06 20:18:50.022000: step 10310, total loss = 0.66, predict loss = 0.18 (80.1 examples/sec; 0.050 sec/batch; 1h:56m:13s remains)
INFO - root - 2019-11-06 20:18:50.588210: step 10320, total loss = 0.58, predict loss = 0.13 (79.2 examples/sec; 0.051 sec/batch; 1h:57m:38s remains)
INFO - root - 2019-11-06 20:18:51.163581: step 10330, total loss = 0.49, predict loss = 0.16 (79.8 examples/sec; 0.050 sec/batch; 1h:56m:39s remains)
INFO - root - 2019-11-06 20:18:51.746739: step 10340, total loss = 0.27, predict loss = 0.07 (81.6 examples/sec; 0.049 sec/batch; 1h:54m:09s remains)
INFO - root - 2019-11-06 20:18:52.250591: step 10350, total loss = 0.40, predict loss = 0.10 (103.5 examples/sec; 0.039 sec/batch; 1h:29m:57s remains)
INFO - root - 2019-11-06 20:18:52.691357: step 10360, total loss = 0.33, predict loss = 0.08 (97.6 examples/sec; 0.041 sec/batch; 1h:35m:22s remains)
INFO - root - 2019-11-06 20:18:53.180800: step 10370, total loss = 0.52, predict loss = 0.15 (96.1 examples/sec; 0.042 sec/batch; 1h:36m:54s remains)
INFO - root - 2019-11-06 20:18:54.233832: step 10380, total loss = 0.30, predict loss = 0.08 (62.6 examples/sec; 0.064 sec/batch; 2h:28m:34s remains)
INFO - root - 2019-11-06 20:18:54.850419: step 10390, total loss = 0.34, predict loss = 0.08 (78.8 examples/sec; 0.051 sec/batch; 1h:58m:02s remains)
INFO - root - 2019-11-06 20:18:55.421613: step 10400, total loss = 0.45, predict loss = 0.13 (77.0 examples/sec; 0.052 sec/batch; 2h:00m:53s remains)
INFO - root - 2019-11-06 20:18:56.017037: step 10410, total loss = 0.47, predict loss = 0.12 (73.8 examples/sec; 0.054 sec/batch; 2h:06m:07s remains)
INFO - root - 2019-11-06 20:18:56.588225: step 10420, total loss = 0.32, predict loss = 0.07 (79.9 examples/sec; 0.050 sec/batch; 1h:56m:26s remains)
INFO - root - 2019-11-06 20:18:57.166063: step 10430, total loss = 0.23, predict loss = 0.06 (76.9 examples/sec; 0.052 sec/batch; 2h:00m:55s remains)
INFO - root - 2019-11-06 20:18:57.737559: step 10440, total loss = 0.75, predict loss = 0.25 (79.1 examples/sec; 0.051 sec/batch; 1h:57m:41s remains)
INFO - root - 2019-11-06 20:18:58.328935: step 10450, total loss = 0.61, predict loss = 0.15 (80.0 examples/sec; 0.050 sec/batch; 1h:56m:16s remains)
INFO - root - 2019-11-06 20:18:58.906129: step 10460, total loss = 0.25, predict loss = 0.07 (77.5 examples/sec; 0.052 sec/batch; 2h:00m:02s remains)
INFO - root - 2019-11-06 20:18:59.471368: step 10470, total loss = 0.49, predict loss = 0.13 (78.6 examples/sec; 0.051 sec/batch; 1h:58m:24s remains)
INFO - root - 2019-11-06 20:19:00.048110: step 10480, total loss = 0.40, predict loss = 0.09 (75.6 examples/sec; 0.053 sec/batch; 2h:02m:58s remains)
INFO - root - 2019-11-06 20:19:00.645951: step 10490, total loss = 0.75, predict loss = 0.19 (74.2 examples/sec; 0.054 sec/batch; 2h:05m:20s remains)
INFO - root - 2019-11-06 20:19:01.124039: step 10500, total loss = 0.57, predict loss = 0.16 (102.6 examples/sec; 0.039 sec/batch; 1h:30m:40s remains)
INFO - root - 2019-11-06 20:19:01.590054: step 10510, total loss = 0.31, predict loss = 0.08 (93.2 examples/sec; 0.043 sec/batch; 1h:39m:44s remains)
INFO - root - 2019-11-06 20:19:02.047830: step 10520, total loss = 0.38, predict loss = 0.08 (95.6 examples/sec; 0.042 sec/batch; 1h:37m:19s remains)
INFO - root - 2019-11-06 20:19:03.180198: step 10530, total loss = 0.37, predict loss = 0.11 (58.4 examples/sec; 0.069 sec/batch; 2h:39m:15s remains)
INFO - root - 2019-11-06 20:19:03.826124: step 10540, total loss = 0.45, predict loss = 0.12 (78.6 examples/sec; 0.051 sec/batch; 1h:58m:18s remains)
INFO - root - 2019-11-06 20:19:04.393867: step 10550, total loss = 0.39, predict loss = 0.11 (80.9 examples/sec; 0.049 sec/batch; 1h:54m:55s remains)
INFO - root - 2019-11-06 20:19:04.972736: step 10560, total loss = 0.40, predict loss = 0.09 (75.6 examples/sec; 0.053 sec/batch; 2h:02m:58s remains)
INFO - root - 2019-11-06 20:19:05.567176: step 10570, total loss = 0.42, predict loss = 0.10 (80.6 examples/sec; 0.050 sec/batch; 1h:55m:17s remains)
INFO - root - 2019-11-06 20:19:06.141259: step 10580, total loss = 0.63, predict loss = 0.19 (76.0 examples/sec; 0.053 sec/batch; 2h:02m:20s remains)
INFO - root - 2019-11-06 20:19:06.707929: step 10590, total loss = 0.43, predict loss = 0.11 (80.3 examples/sec; 0.050 sec/batch; 1h:55m:41s remains)
INFO - root - 2019-11-06 20:19:07.280071: step 10600, total loss = 0.37, predict loss = 0.08 (79.5 examples/sec; 0.050 sec/batch; 1h:56m:49s remains)
INFO - root - 2019-11-06 20:19:07.888594: step 10610, total loss = 0.31, predict loss = 0.07 (80.7 examples/sec; 0.050 sec/batch; 1h:55m:06s remains)
INFO - root - 2019-11-06 20:19:08.473728: step 10620, total loss = 0.80, predict loss = 0.20 (76.5 examples/sec; 0.052 sec/batch; 2h:01m:28s remains)
INFO - root - 2019-11-06 20:19:09.047133: step 10630, total loss = 0.37, predict loss = 0.09 (81.0 examples/sec; 0.049 sec/batch; 1h:54m:46s remains)
INFO - root - 2019-11-06 20:19:09.622500: step 10640, total loss = 0.31, predict loss = 0.08 (82.8 examples/sec; 0.048 sec/batch; 1h:52m:13s remains)
INFO - root - 2019-11-06 20:19:10.112463: step 10650, total loss = 0.37, predict loss = 0.09 (91.2 examples/sec; 0.044 sec/batch; 1h:41m:53s remains)
INFO - root - 2019-11-06 20:19:10.562252: step 10660, total loss = 0.48, predict loss = 0.13 (103.3 examples/sec; 0.039 sec/batch; 1h:29m:55s remains)
INFO - root - 2019-11-06 20:19:11.465921: step 10670, total loss = 0.58, predict loss = 0.17 (78.3 examples/sec; 0.051 sec/batch; 1h:58m:34s remains)
INFO - root - 2019-11-06 20:19:12.164687: step 10680, total loss = 0.43, predict loss = 0.11 (63.7 examples/sec; 0.063 sec/batch; 2h:25m:54s remains)
INFO - root - 2019-11-06 20:19:12.801089: step 10690, total loss = 0.36, predict loss = 0.08 (79.4 examples/sec; 0.050 sec/batch; 1h:56m:55s remains)
INFO - root - 2019-11-06 20:19:13.377505: step 10700, total loss = 0.44, predict loss = 0.13 (79.3 examples/sec; 0.050 sec/batch; 1h:57m:02s remains)
INFO - root - 2019-11-06 20:19:13.953871: step 10710, total loss = 0.42, predict loss = 0.10 (78.8 examples/sec; 0.051 sec/batch; 1h:57m:51s remains)
INFO - root - 2019-11-06 20:19:14.528431: step 10720, total loss = 0.28, predict loss = 0.07 (75.8 examples/sec; 0.053 sec/batch; 2h:02m:34s remains)
INFO - root - 2019-11-06 20:19:15.158722: step 10730, total loss = 0.33, predict loss = 0.08 (68.1 examples/sec; 0.059 sec/batch; 2h:16m:14s remains)
INFO - root - 2019-11-06 20:19:15.742644: step 10740, total loss = 0.27, predict loss = 0.07 (79.3 examples/sec; 0.050 sec/batch; 1h:57m:01s remains)
INFO - root - 2019-11-06 20:19:16.317666: step 10750, total loss = 0.27, predict loss = 0.07 (77.7 examples/sec; 0.051 sec/batch; 1h:59m:25s remains)
INFO - root - 2019-11-06 20:19:16.883871: step 10760, total loss = 0.51, predict loss = 0.13 (81.6 examples/sec; 0.049 sec/batch; 1h:53m:43s remains)
INFO - root - 2019-11-06 20:19:17.488313: step 10770, total loss = 0.32, predict loss = 0.07 (74.8 examples/sec; 0.053 sec/batch; 2h:04m:00s remains)
INFO - root - 2019-11-06 20:19:18.070187: step 10780, total loss = 0.55, predict loss = 0.14 (77.7 examples/sec; 0.051 sec/batch; 1h:59m:27s remains)
INFO - root - 2019-11-06 20:19:18.628332: step 10790, total loss = 0.24, predict loss = 0.06 (91.1 examples/sec; 0.044 sec/batch; 1h:41m:53s remains)
INFO - root - 2019-11-06 20:19:19.090900: step 10800, total loss = 0.41, predict loss = 0.11 (96.2 examples/sec; 0.042 sec/batch; 1h:36m:30s remains)
INFO - root - 2019-11-06 20:19:19.562900: step 10810, total loss = 1.21, predict loss = 0.39 (97.5 examples/sec; 0.041 sec/batch; 1h:35m:09s remains)
INFO - root - 2019-11-06 20:19:20.512769: step 10820, total loss = 0.29, predict loss = 0.07 (70.2 examples/sec; 0.057 sec/batch; 2h:12m:11s remains)
INFO - root - 2019-11-06 20:19:21.158347: step 10830, total loss = 0.46, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 2h:02m:15s remains)
INFO - root - 2019-11-06 20:19:21.769569: step 10840, total loss = 0.55, predict loss = 0.15 (77.9 examples/sec; 0.051 sec/batch; 1h:59m:09s remains)
INFO - root - 2019-11-06 20:19:22.404196: step 10850, total loss = 0.42, predict loss = 0.11 (75.8 examples/sec; 0.053 sec/batch; 2h:02m:25s remains)
INFO - root - 2019-11-06 20:19:22.979297: step 10860, total loss = 0.40, predict loss = 0.10 (74.1 examples/sec; 0.054 sec/batch; 2h:05m:14s remains)
INFO - root - 2019-11-06 20:19:23.550075: step 10870, total loss = 0.53, predict loss = 0.14 (78.4 examples/sec; 0.051 sec/batch; 1h:58m:14s remains)
INFO - root - 2019-11-06 20:19:24.121540: step 10880, total loss = 0.41, predict loss = 0.10 (77.9 examples/sec; 0.051 sec/batch; 1h:58m:59s remains)
INFO - root - 2019-11-06 20:19:24.717912: step 10890, total loss = 0.25, predict loss = 0.07 (77.2 examples/sec; 0.052 sec/batch; 2h:00m:08s remains)
INFO - root - 2019-11-06 20:19:25.295570: step 10900, total loss = 0.40, predict loss = 0.08 (77.1 examples/sec; 0.052 sec/batch; 2h:00m:20s remains)
INFO - root - 2019-11-06 20:19:25.863022: step 10910, total loss = 0.41, predict loss = 0.11 (82.6 examples/sec; 0.048 sec/batch; 1h:52m:12s remains)
INFO - root - 2019-11-06 20:19:26.441381: step 10920, total loss = 0.30, predict loss = 0.07 (79.5 examples/sec; 0.050 sec/batch; 1h:56m:34s remains)
INFO - root - 2019-11-06 20:19:27.027967: step 10930, total loss = 0.31, predict loss = 0.08 (76.9 examples/sec; 0.052 sec/batch; 2h:00m:29s remains)
INFO - root - 2019-11-06 20:19:27.574585: step 10940, total loss = 0.53, predict loss = 0.15 (94.7 examples/sec; 0.042 sec/batch; 1h:37m:55s remains)
INFO - root - 2019-11-06 20:19:28.045178: step 10950, total loss = 0.39, predict loss = 0.08 (103.1 examples/sec; 0.039 sec/batch; 1h:29m:52s remains)
INFO - root - 2019-11-06 20:19:28.469781: step 10960, total loss = 0.41, predict loss = 0.11 (102.8 examples/sec; 0.039 sec/batch; 1h:30m:11s remains)
INFO - root - 2019-11-06 20:19:29.463971: step 10970, total loss = 0.27, predict loss = 0.07 (65.6 examples/sec; 0.061 sec/batch; 2h:21m:20s remains)
INFO - root - 2019-11-06 20:19:30.144141: step 10980, total loss = 0.26, predict loss = 0.07 (67.1 examples/sec; 0.060 sec/batch; 2h:18m:07s remains)
INFO - root - 2019-11-06 20:19:30.740931: step 10990, total loss = 0.30, predict loss = 0.07 (76.1 examples/sec; 0.053 sec/batch; 2h:01m:49s remains)
INFO - root - 2019-11-06 20:19:31.343298: step 11000, total loss = 0.81, predict loss = 0.25 (77.0 examples/sec; 0.052 sec/batch; 2h:00m:25s remains)
INFO - root - 2019-11-06 20:19:31.937801: step 11010, total loss = 0.34, predict loss = 0.08 (81.0 examples/sec; 0.049 sec/batch; 1h:54m:21s remains)
INFO - root - 2019-11-06 20:19:32.523032: step 11020, total loss = 0.43, predict loss = 0.13 (76.5 examples/sec; 0.052 sec/batch; 2h:01m:04s remains)
INFO - root - 2019-11-06 20:19:33.094281: step 11030, total loss = 0.31, predict loss = 0.06 (76.9 examples/sec; 0.052 sec/batch; 2h:00m:28s remains)
INFO - root - 2019-11-06 20:19:33.658741: step 11040, total loss = 0.50, predict loss = 0.13 (81.3 examples/sec; 0.049 sec/batch; 1h:53m:59s remains)
INFO - root - 2019-11-06 20:19:34.240384: step 11050, total loss = 0.28, predict loss = 0.07 (81.7 examples/sec; 0.049 sec/batch; 1h:53m:21s remains)
INFO - root - 2019-11-06 20:19:34.814820: step 11060, total loss = 0.23, predict loss = 0.07 (78.3 examples/sec; 0.051 sec/batch; 1h:58m:13s remains)
INFO - root - 2019-11-06 20:19:35.393959: step 11070, total loss = 0.46, predict loss = 0.13 (80.7 examples/sec; 0.050 sec/batch; 1h:54m:47s remains)
INFO - root - 2019-11-06 20:19:35.972893: step 11080, total loss = 0.76, predict loss = 0.22 (74.3 examples/sec; 0.054 sec/batch; 2h:04m:43s remains)
INFO - root - 2019-11-06 20:19:36.512442: step 11090, total loss = 0.40, predict loss = 0.09 (96.7 examples/sec; 0.041 sec/batch; 1h:35m:45s remains)
INFO - root - 2019-11-06 20:19:36.964816: step 11100, total loss = 0.40, predict loss = 0.09 (94.0 examples/sec; 0.043 sec/batch; 1h:38m:30s remains)
INFO - root - 2019-11-06 20:19:37.406282: step 11110, total loss = 0.71, predict loss = 0.23 (95.2 examples/sec; 0.042 sec/batch; 1h:37m:15s remains)
INFO - root - 2019-11-06 20:19:38.535830: step 11120, total loss = 0.45, predict loss = 0.11 (51.3 examples/sec; 0.078 sec/batch; 3h:00m:30s remains)
INFO - root - 2019-11-06 20:19:39.175391: step 11130, total loss = 0.40, predict loss = 0.09 (78.9 examples/sec; 0.051 sec/batch; 1h:57m:21s remains)
INFO - root - 2019-11-06 20:19:39.732309: step 11140, total loss = 0.46, predict loss = 0.12 (85.6 examples/sec; 0.047 sec/batch; 1h:48m:11s remains)
INFO - root - 2019-11-06 20:19:40.308168: step 11150, total loss = 0.37, predict loss = 0.11 (74.5 examples/sec; 0.054 sec/batch; 2h:04m:19s remains)
INFO - root - 2019-11-06 20:19:40.875441: step 11160, total loss = 0.63, predict loss = 0.18 (77.1 examples/sec; 0.052 sec/batch; 2h:00m:02s remains)
INFO - root - 2019-11-06 20:19:41.463043: step 11170, total loss = 0.32, predict loss = 0.08 (76.2 examples/sec; 0.053 sec/batch; 2h:01m:29s remains)
INFO - root - 2019-11-06 20:19:42.036748: step 11180, total loss = 0.29, predict loss = 0.08 (81.0 examples/sec; 0.049 sec/batch; 1h:54m:12s remains)
INFO - root - 2019-11-06 20:19:42.610290: step 11190, total loss = 0.67, predict loss = 0.16 (77.5 examples/sec; 0.052 sec/batch; 1h:59m:27s remains)
INFO - root - 2019-11-06 20:19:43.189862: step 11200, total loss = 0.34, predict loss = 0.07 (80.6 examples/sec; 0.050 sec/batch; 1h:54m:49s remains)
INFO - root - 2019-11-06 20:19:43.777128: step 11210, total loss = 0.44, predict loss = 0.12 (77.9 examples/sec; 0.051 sec/batch; 1h:58m:43s remains)
INFO - root - 2019-11-06 20:19:44.372430: step 11220, total loss = 0.50, predict loss = 0.14 (77.5 examples/sec; 0.052 sec/batch; 1h:59m:23s remains)
INFO - root - 2019-11-06 20:19:44.949899: step 11230, total loss = 0.48, predict loss = 0.12 (77.8 examples/sec; 0.051 sec/batch; 1h:58m:53s remains)
INFO - root - 2019-11-06 20:19:45.491891: step 11240, total loss = 0.39, predict loss = 0.10 (102.1 examples/sec; 0.039 sec/batch; 1h:30m:34s remains)
INFO - root - 2019-11-06 20:19:45.950974: step 11250, total loss = 0.83, predict loss = 0.23 (101.8 examples/sec; 0.039 sec/batch; 1h:30m:49s remains)
INFO - root - 2019-11-06 20:19:46.408622: step 11260, total loss = 0.31, predict loss = 0.07 (93.1 examples/sec; 0.043 sec/batch; 1h:39m:21s remains)
INFO - root - 2019-11-06 20:19:47.506734: step 11270, total loss = 0.36, predict loss = 0.09 (54.2 examples/sec; 0.074 sec/batch; 2h:50m:46s remains)
INFO - root - 2019-11-06 20:19:48.139682: step 11280, total loss = 0.34, predict loss = 0.08 (76.4 examples/sec; 0.052 sec/batch; 2h:01m:00s remains)
INFO - root - 2019-11-06 20:19:48.730460: step 11290, total loss = 0.41, predict loss = 0.12 (81.6 examples/sec; 0.049 sec/batch; 1h:53m:20s remains)
INFO - root - 2019-11-06 20:19:49.319612: step 11300, total loss = 0.44, predict loss = 0.10 (78.8 examples/sec; 0.051 sec/batch; 1h:57m:23s remains)
INFO - root - 2019-11-06 20:19:49.896225: step 11310, total loss = 0.27, predict loss = 0.07 (78.1 examples/sec; 0.051 sec/batch; 1h:58m:23s remains)
INFO - root - 2019-11-06 20:19:50.468010: step 11320, total loss = 0.53, predict loss = 0.14 (83.2 examples/sec; 0.048 sec/batch; 1h:51m:11s remains)
INFO - root - 2019-11-06 20:19:51.065611: step 11330, total loss = 0.34, predict loss = 0.08 (84.0 examples/sec; 0.048 sec/batch; 1h:50m:01s remains)
INFO - root - 2019-11-06 20:19:51.629560: step 11340, total loss = 0.40, predict loss = 0.10 (77.1 examples/sec; 0.052 sec/batch; 1h:59m:56s remains)
INFO - root - 2019-11-06 20:19:52.206001: step 11350, total loss = 0.39, predict loss = 0.10 (81.2 examples/sec; 0.049 sec/batch; 1h:53m:50s remains)
INFO - root - 2019-11-06 20:19:52.773089: step 11360, total loss = 0.31, predict loss = 0.08 (79.2 examples/sec; 0.050 sec/batch; 1h:56m:39s remains)
INFO - root - 2019-11-06 20:19:53.355654: step 11370, total loss = 0.47, predict loss = 0.12 (79.2 examples/sec; 0.051 sec/batch; 1h:56m:45s remains)
INFO - root - 2019-11-06 20:19:53.924419: step 11380, total loss = 0.35, predict loss = 0.08 (82.2 examples/sec; 0.049 sec/batch; 1h:52m:22s remains)
INFO - root - 2019-11-06 20:19:54.404246: step 11390, total loss = 0.35, predict loss = 0.07 (102.6 examples/sec; 0.039 sec/batch; 1h:30m:05s remains)
INFO - root - 2019-11-06 20:19:54.862805: step 11400, total loss = 0.40, predict loss = 0.11 (95.8 examples/sec; 0.042 sec/batch; 1h:36m:27s remains)
INFO - root - 2019-11-06 20:19:55.794578: step 11410, total loss = 0.54, predict loss = 0.17 (8.0 examples/sec; 0.499 sec/batch; 19h:12m:52s remains)
INFO - root - 2019-11-06 20:19:56.485889: step 11420, total loss = 0.25, predict loss = 0.05 (59.6 examples/sec; 0.067 sec/batch; 2h:34m:57s remains)
INFO - root - 2019-11-06 20:19:57.112932: step 11430, total loss = 0.28, predict loss = 0.07 (81.0 examples/sec; 0.049 sec/batch; 1h:54m:01s remains)
INFO - root - 2019-11-06 20:19:57.668520: step 11440, total loss = 0.30, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:56m:43s remains)
INFO - root - 2019-11-06 20:19:58.267071: step 11450, total loss = 0.29, predict loss = 0.08 (82.4 examples/sec; 0.049 sec/batch; 1h:52m:04s remains)
INFO - root - 2019-11-06 20:19:58.846812: step 11460, total loss = 0.31, predict loss = 0.10 (76.9 examples/sec; 0.052 sec/batch; 2h:00m:06s remains)
INFO - root - 2019-11-06 20:19:59.433909: step 11470, total loss = 0.52, predict loss = 0.16 (79.9 examples/sec; 0.050 sec/batch; 1h:55m:32s remains)
INFO - root - 2019-11-06 20:20:00.008535: step 11480, total loss = 0.24, predict loss = 0.05 (79.5 examples/sec; 0.050 sec/batch; 1h:56m:09s remains)
INFO - root - 2019-11-06 20:20:00.604270: step 11490, total loss = 0.35, predict loss = 0.10 (77.5 examples/sec; 0.052 sec/batch; 1h:59m:09s remains)
INFO - root - 2019-11-06 20:20:01.169119: step 11500, total loss = 0.35, predict loss = 0.08 (78.5 examples/sec; 0.051 sec/batch; 1h:57m:33s remains)
INFO - root - 2019-11-06 20:20:01.754600: step 11510, total loss = 0.71, predict loss = 0.22 (76.1 examples/sec; 0.053 sec/batch; 2h:01m:23s remains)
INFO - root - 2019-11-06 20:20:02.335381: step 11520, total loss = 0.65, predict loss = 0.18 (74.8 examples/sec; 0.053 sec/batch; 2h:03m:22s remains)
INFO - root - 2019-11-06 20:20:02.928794: step 11530, total loss = 0.30, predict loss = 0.07 (82.0 examples/sec; 0.049 sec/batch; 1h:52m:33s remains)
INFO - root - 2019-11-06 20:20:03.389490: step 11540, total loss = 0.57, predict loss = 0.17 (98.1 examples/sec; 0.041 sec/batch; 1h:34m:02s remains)
INFO - root - 2019-11-06 20:20:03.844967: step 11550, total loss = 0.33, predict loss = 0.07 (92.8 examples/sec; 0.043 sec/batch; 1h:39m:28s remains)
INFO - root - 2019-11-06 20:20:04.786859: step 11560, total loss = 0.44, predict loss = 0.11 (77.8 examples/sec; 0.051 sec/batch; 1h:58m:38s remains)
INFO - root - 2019-11-06 20:20:05.445151: step 11570, total loss = 0.26, predict loss = 0.06 (65.4 examples/sec; 0.061 sec/batch; 2h:21m:08s remains)
INFO - root - 2019-11-06 20:20:06.060583: step 11580, total loss = 0.55, predict loss = 0.13 (81.3 examples/sec; 0.049 sec/batch; 1h:53m:28s remains)
INFO - root - 2019-11-06 20:20:06.635788: step 11590, total loss = 0.38, predict loss = 0.09 (79.1 examples/sec; 0.051 sec/batch; 1h:56m:34s remains)
INFO - root - 2019-11-06 20:20:07.214855: step 11600, total loss = 0.26, predict loss = 0.07 (77.1 examples/sec; 0.052 sec/batch; 1h:59m:44s remains)
INFO - root - 2019-11-06 20:20:07.816040: step 11610, total loss = 0.38, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 2h:02m:15s remains)
INFO - root - 2019-11-06 20:20:08.384130: step 11620, total loss = 0.65, predict loss = 0.18 (78.9 examples/sec; 0.051 sec/batch; 1h:56m:58s remains)
INFO - root - 2019-11-06 20:20:08.953838: step 11630, total loss = 0.43, predict loss = 0.13 (74.1 examples/sec; 0.054 sec/batch; 2h:04m:27s remains)
INFO - root - 2019-11-06 20:20:09.533650: step 11640, total loss = 0.48, predict loss = 0.14 (80.7 examples/sec; 0.050 sec/batch; 1h:54m:15s remains)
INFO - root - 2019-11-06 20:20:10.119156: step 11650, total loss = 0.51, predict loss = 0.12 (76.5 examples/sec; 0.052 sec/batch; 2h:00m:34s remains)
INFO - root - 2019-11-06 20:20:10.696037: step 11660, total loss = 0.70, predict loss = 0.22 (78.8 examples/sec; 0.051 sec/batch; 1h:57m:00s remains)
INFO - root - 2019-11-06 20:20:11.260543: step 11670, total loss = 0.36, predict loss = 0.09 (79.8 examples/sec; 0.050 sec/batch; 1h:55m:35s remains)
INFO - root - 2019-11-06 20:20:11.814273: step 11680, total loss = 0.30, predict loss = 0.07 (95.6 examples/sec; 0.042 sec/batch; 1h:36m:29s remains)
INFO - root - 2019-11-06 20:20:12.295060: step 11690, total loss = 0.33, predict loss = 0.08 (94.0 examples/sec; 0.043 sec/batch; 1h:38m:05s remains)
INFO - root - 2019-11-06 20:20:12.739276: step 11700, total loss = 0.31, predict loss = 0.08 (97.6 examples/sec; 0.041 sec/batch; 1h:34m:29s remains)
INFO - root - 2019-11-06 20:20:13.674148: step 11710, total loss = 0.32, predict loss = 0.08 (71.8 examples/sec; 0.056 sec/batch; 2h:08m:26s remains)
INFO - root - 2019-11-06 20:20:14.308852: step 11720, total loss = 0.45, predict loss = 0.11 (78.7 examples/sec; 0.051 sec/batch; 1h:57m:12s remains)
INFO - root - 2019-11-06 20:20:14.901557: step 11730, total loss = 0.26, predict loss = 0.07 (81.5 examples/sec; 0.049 sec/batch; 1h:53m:02s remains)
INFO - root - 2019-11-06 20:20:15.515715: step 11740, total loss = 0.60, predict loss = 0.16 (75.5 examples/sec; 0.053 sec/batch; 2h:02m:01s remains)
INFO - root - 2019-11-06 20:20:16.078877: step 11750, total loss = 0.45, predict loss = 0.14 (80.9 examples/sec; 0.049 sec/batch; 1h:53m:56s remains)
INFO - root - 2019-11-06 20:20:16.649570: step 11760, total loss = 0.56, predict loss = 0.15 (78.8 examples/sec; 0.051 sec/batch; 1h:56m:59s remains)
INFO - root - 2019-11-06 20:20:17.240341: step 11770, total loss = 0.57, predict loss = 0.17 (75.6 examples/sec; 0.053 sec/batch; 2h:01m:50s remains)
INFO - root - 2019-11-06 20:20:17.796117: step 11780, total loss = 0.37, predict loss = 0.08 (78.1 examples/sec; 0.051 sec/batch; 1h:57m:57s remains)
INFO - root - 2019-11-06 20:20:18.367356: step 11790, total loss = 0.30, predict loss = 0.08 (82.2 examples/sec; 0.049 sec/batch; 1h:52m:02s remains)
INFO - root - 2019-11-06 20:20:18.944192: step 11800, total loss = 0.33, predict loss = 0.08 (77.7 examples/sec; 0.051 sec/batch; 1h:58m:31s remains)
INFO - root - 2019-11-06 20:20:19.529432: step 11810, total loss = 0.47, predict loss = 0.11 (77.4 examples/sec; 0.052 sec/batch; 1h:59m:04s remains)
INFO - root - 2019-11-06 20:20:20.109836: step 11820, total loss = 0.72, predict loss = 0.22 (77.3 examples/sec; 0.052 sec/batch; 1h:59m:09s remains)
INFO - root - 2019-11-06 20:20:20.642669: step 11830, total loss = 0.39, predict loss = 0.09 (96.1 examples/sec; 0.042 sec/batch; 1h:35m:49s remains)
INFO - root - 2019-11-06 20:20:21.098986: step 11840, total loss = 0.44, predict loss = 0.11 (89.5 examples/sec; 0.045 sec/batch; 1h:42m:55s remains)
INFO - root - 2019-11-06 20:20:21.565182: step 11850, total loss = 0.56, predict loss = 0.17 (98.6 examples/sec; 0.041 sec/batch; 1h:33m:24s remains)
INFO - root - 2019-11-06 20:20:22.570498: step 11860, total loss = 0.34, predict loss = 0.08 (59.0 examples/sec; 0.068 sec/batch; 2h:35m:58s remains)
INFO - root - 2019-11-06 20:20:23.203824: step 11870, total loss = 0.27, predict loss = 0.06 (78.5 examples/sec; 0.051 sec/batch; 1h:57m:20s remains)
INFO - root - 2019-11-06 20:20:23.779393: step 11880, total loss = 0.22, predict loss = 0.05 (79.4 examples/sec; 0.050 sec/batch; 1h:55m:55s remains)
INFO - root - 2019-11-06 20:20:24.362547: step 11890, total loss = 0.26, predict loss = 0.06 (81.8 examples/sec; 0.049 sec/batch; 1h:52m:29s remains)
INFO - root - 2019-11-06 20:20:24.936264: step 11900, total loss = 0.40, predict loss = 0.10 (79.8 examples/sec; 0.050 sec/batch; 1h:55m:21s remains)
INFO - root - 2019-11-06 20:20:25.508389: step 11910, total loss = 0.59, predict loss = 0.16 (79.0 examples/sec; 0.051 sec/batch; 1h:56m:30s remains)
INFO - root - 2019-11-06 20:20:26.079913: step 11920, total loss = 0.29, predict loss = 0.08 (81.3 examples/sec; 0.049 sec/batch; 1h:53m:11s remains)
INFO - root - 2019-11-06 20:20:26.672803: step 11930, total loss = 0.48, predict loss = 0.14 (79.9 examples/sec; 0.050 sec/batch; 1h:55m:09s remains)
INFO - root - 2019-11-06 20:20:27.238159: step 11940, total loss = 0.39, predict loss = 0.10 (77.4 examples/sec; 0.052 sec/batch; 1h:58m:51s remains)
INFO - root - 2019-11-06 20:20:27.808471: step 11950, total loss = 0.36, predict loss = 0.11 (78.8 examples/sec; 0.051 sec/batch; 1h:56m:50s remains)
INFO - root - 2019-11-06 20:20:28.382489: step 11960, total loss = 0.26, predict loss = 0.07 (78.7 examples/sec; 0.051 sec/batch; 1h:56m:54s remains)
INFO - root - 2019-11-06 20:20:28.971962: step 11970, total loss = 0.26, predict loss = 0.07 (76.2 examples/sec; 0.053 sec/batch; 2h:00m:47s remains)
INFO - root - 2019-11-06 20:20:29.486748: step 11980, total loss = 0.33, predict loss = 0.07 (99.9 examples/sec; 0.040 sec/batch; 1h:32m:07s remains)
INFO - root - 2019-11-06 20:20:29.932313: step 11990, total loss = 0.73, predict loss = 0.20 (97.2 examples/sec; 0.041 sec/batch; 1h:34m:38s remains)
INFO - root - 2019-11-06 20:20:30.390594: step 12000, total loss = 0.34, predict loss = 0.08 (91.6 examples/sec; 0.044 sec/batch; 1h:40m:28s remains)
INFO - root - 2019-11-06 20:20:31.511126: step 12010, total loss = 0.50, predict loss = 0.14 (52.1 examples/sec; 0.077 sec/batch; 2h:56m:27s remains)
INFO - root - 2019-11-06 20:20:32.196079: step 12020, total loss = 0.27, predict loss = 0.06 (72.0 examples/sec; 0.056 sec/batch; 2h:07m:46s remains)
INFO - root - 2019-11-06 20:20:32.789274: step 12030, total loss = 0.43, predict loss = 0.13 (77.5 examples/sec; 0.052 sec/batch; 1h:58m:42s remains)
INFO - root - 2019-11-06 20:20:33.358670: step 12040, total loss = 0.21, predict loss = 0.05 (77.5 examples/sec; 0.052 sec/batch; 1h:58m:44s remains)
INFO - root - 2019-11-06 20:20:33.947181: step 12050, total loss = 0.36, predict loss = 0.09 (77.7 examples/sec; 0.051 sec/batch; 1h:58m:21s remains)
INFO - root - 2019-11-06 20:20:34.525711: step 12060, total loss = 0.28, predict loss = 0.07 (79.4 examples/sec; 0.050 sec/batch; 1h:55m:49s remains)
INFO - root - 2019-11-06 20:20:35.108396: step 12070, total loss = 0.30, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:55m:24s remains)
INFO - root - 2019-11-06 20:20:35.685153: step 12080, total loss = 0.49, predict loss = 0.12 (74.9 examples/sec; 0.053 sec/batch; 2h:02m:46s remains)
INFO - root - 2019-11-06 20:20:36.273150: step 12090, total loss = 0.41, predict loss = 0.10 (80.1 examples/sec; 0.050 sec/batch; 1h:54m:50s remains)
INFO - root - 2019-11-06 20:20:36.854746: step 12100, total loss = 0.47, predict loss = 0.13 (77.2 examples/sec; 0.052 sec/batch; 1h:59m:05s remains)
INFO - root - 2019-11-06 20:20:37.423034: step 12110, total loss = 0.29, predict loss = 0.07 (78.7 examples/sec; 0.051 sec/batch; 1h:56m:44s remains)
INFO - root - 2019-11-06 20:20:38.000923: step 12120, total loss = 0.54, predict loss = 0.15 (79.6 examples/sec; 0.050 sec/batch; 1h:55m:30s remains)
INFO - root - 2019-11-06 20:20:38.514387: step 12130, total loss = 0.26, predict loss = 0.06 (100.9 examples/sec; 0.040 sec/batch; 1h:31m:06s remains)
INFO - root - 2019-11-06 20:20:38.969818: step 12140, total loss = 0.63, predict loss = 0.16 (92.7 examples/sec; 0.043 sec/batch; 1h:39m:09s remains)
INFO - root - 2019-11-06 20:20:39.433320: step 12150, total loss = 0.45, predict loss = 0.12 (91.0 examples/sec; 0.044 sec/batch; 1h:40m:56s remains)
INFO - root - 2019-11-06 20:20:40.554671: step 12160, total loss = 0.33, predict loss = 0.09 (59.2 examples/sec; 0.068 sec/batch; 2h:35m:20s remains)
INFO - root - 2019-11-06 20:20:41.208428: step 12170, total loss = 0.47, predict loss = 0.14 (73.1 examples/sec; 0.055 sec/batch; 2h:05m:42s remains)
INFO - root - 2019-11-06 20:20:41.810944: step 12180, total loss = 0.32, predict loss = 0.07 (74.5 examples/sec; 0.054 sec/batch; 2h:03m:19s remains)
INFO - root - 2019-11-06 20:20:42.373148: step 12190, total loss = 0.21, predict loss = 0.04 (78.0 examples/sec; 0.051 sec/batch; 1h:57m:45s remains)
INFO - root - 2019-11-06 20:20:42.945674: step 12200, total loss = 0.39, predict loss = 0.10 (75.5 examples/sec; 0.053 sec/batch; 2h:01m:39s remains)
INFO - root - 2019-11-06 20:20:43.532087: step 12210, total loss = 0.56, predict loss = 0.16 (79.9 examples/sec; 0.050 sec/batch; 1h:54m:57s remains)
INFO - root - 2019-11-06 20:20:44.099073: step 12220, total loss = 0.26, predict loss = 0.06 (74.9 examples/sec; 0.053 sec/batch; 2h:02m:40s remains)
INFO - root - 2019-11-06 20:20:44.676051: step 12230, total loss = 0.33, predict loss = 0.08 (76.1 examples/sec; 0.053 sec/batch; 2h:00m:40s remains)
INFO - root - 2019-11-06 20:20:45.298004: step 12240, total loss = 0.42, predict loss = 0.11 (77.3 examples/sec; 0.052 sec/batch; 1h:58m:44s remains)
INFO - root - 2019-11-06 20:20:45.880877: step 12250, total loss = 0.69, predict loss = 0.17 (78.2 examples/sec; 0.051 sec/batch; 1h:57m:29s remains)
INFO - root - 2019-11-06 20:20:46.445279: step 12260, total loss = 0.35, predict loss = 0.09 (80.7 examples/sec; 0.050 sec/batch; 1h:53m:43s remains)
INFO - root - 2019-11-06 20:20:47.018333: step 12270, total loss = 0.21, predict loss = 0.05 (80.8 examples/sec; 0.050 sec/batch; 1h:53m:38s remains)
INFO - root - 2019-11-06 20:20:47.480980: step 12280, total loss = 0.33, predict loss = 0.08 (100.4 examples/sec; 0.040 sec/batch; 1h:31m:25s remains)
INFO - root - 2019-11-06 20:20:47.956762: step 12290, total loss = 0.28, predict loss = 0.07 (91.9 examples/sec; 0.044 sec/batch; 1h:39m:50s remains)
INFO - root - 2019-11-06 20:20:48.875133: step 12300, total loss = 0.27, predict loss = 0.07 (77.8 examples/sec; 0.051 sec/batch; 1h:57m:55s remains)
INFO - root - 2019-11-06 20:20:49.523213: step 12310, total loss = 0.26, predict loss = 0.07 (57.0 examples/sec; 0.070 sec/batch; 2h:40m:59s remains)
INFO - root - 2019-11-06 20:20:50.140790: step 12320, total loss = 0.35, predict loss = 0.08 (78.5 examples/sec; 0.051 sec/batch; 1h:56m:57s remains)
INFO - root - 2019-11-06 20:20:50.744672: step 12330, total loss = 0.35, predict loss = 0.09 (75.8 examples/sec; 0.053 sec/batch; 2h:01m:02s remains)
INFO - root - 2019-11-06 20:20:51.313583: step 12340, total loss = 0.60, predict loss = 0.17 (75.4 examples/sec; 0.053 sec/batch; 2h:01m:39s remains)
INFO - root - 2019-11-06 20:20:51.874310: step 12350, total loss = 0.46, predict loss = 0.12 (81.3 examples/sec; 0.049 sec/batch; 1h:52m:50s remains)
INFO - root - 2019-11-06 20:20:52.447642: step 12360, total loss = 0.23, predict loss = 0.06 (79.5 examples/sec; 0.050 sec/batch; 1h:55m:22s remains)
INFO - root - 2019-11-06 20:20:53.039079: step 12370, total loss = 0.44, predict loss = 0.14 (77.7 examples/sec; 0.051 sec/batch; 1h:58m:04s remains)
INFO - root - 2019-11-06 20:20:53.615604: step 12380, total loss = 0.34, predict loss = 0.08 (81.3 examples/sec; 0.049 sec/batch; 1h:52m:53s remains)
INFO - root - 2019-11-06 20:20:54.199383: step 12390, total loss = 0.50, predict loss = 0.15 (75.3 examples/sec; 0.053 sec/batch; 2h:01m:51s remains)
INFO - root - 2019-11-06 20:20:54.766807: step 12400, total loss = 0.30, predict loss = 0.07 (78.9 examples/sec; 0.051 sec/batch; 1h:56m:16s remains)
INFO - root - 2019-11-06 20:20:55.361154: step 12410, total loss = 0.57, predict loss = 0.17 (77.2 examples/sec; 0.052 sec/batch; 1h:58m:47s remains)
INFO - root - 2019-11-06 20:20:55.924510: step 12420, total loss = 0.39, predict loss = 0.08 (90.4 examples/sec; 0.044 sec/batch; 1h:41m:29s remains)
INFO - root - 2019-11-06 20:20:56.393154: step 12430, total loss = 0.43, predict loss = 0.10 (96.9 examples/sec; 0.041 sec/batch; 1h:34m:36s remains)
INFO - root - 2019-11-06 20:20:56.854600: step 12440, total loss = 0.34, predict loss = 0.09 (98.7 examples/sec; 0.041 sec/batch; 1h:32m:57s remains)
INFO - root - 2019-11-06 20:20:57.801479: step 12450, total loss = 0.43, predict loss = 0.11 (77.1 examples/sec; 0.052 sec/batch; 1h:58m:57s remains)
INFO - root - 2019-11-06 20:20:58.522086: step 12460, total loss = 0.36, predict loss = 0.09 (65.5 examples/sec; 0.061 sec/batch; 2h:19m:55s remains)
INFO - root - 2019-11-06 20:20:59.119361: step 12470, total loss = 0.32, predict loss = 0.09 (77.9 examples/sec; 0.051 sec/batch; 1h:57m:39s remains)
INFO - root - 2019-11-06 20:20:59.694331: step 12480, total loss = 0.30, predict loss = 0.07 (76.5 examples/sec; 0.052 sec/batch; 1h:59m:48s remains)
INFO - root - 2019-11-06 20:21:00.290848: step 12490, total loss = 0.46, predict loss = 0.12 (77.9 examples/sec; 0.051 sec/batch; 1h:57m:38s remains)
INFO - root - 2019-11-06 20:21:00.864600: step 12500, total loss = 0.48, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 2h:05m:28s remains)
INFO - root - 2019-11-06 20:21:01.426159: step 12510, total loss = 0.32, predict loss = 0.07 (79.0 examples/sec; 0.051 sec/batch; 1h:56m:02s remains)
INFO - root - 2019-11-06 20:21:02.004752: step 12520, total loss = 0.43, predict loss = 0.13 (77.9 examples/sec; 0.051 sec/batch; 1h:57m:40s remains)
INFO - root - 2019-11-06 20:21:02.591746: step 12530, total loss = 0.48, predict loss = 0.14 (78.6 examples/sec; 0.051 sec/batch; 1h:56m:35s remains)
INFO - root - 2019-11-06 20:21:03.163668: step 12540, total loss = 0.33, predict loss = 0.08 (76.1 examples/sec; 0.053 sec/batch; 2h:00m:23s remains)
INFO - root - 2019-11-06 20:21:03.727166: step 12550, total loss = 0.25, predict loss = 0.07 (86.1 examples/sec; 0.046 sec/batch; 1h:46m:23s remains)
INFO - root - 2019-11-06 20:21:04.299212: step 12560, total loss = 0.30, predict loss = 0.09 (74.9 examples/sec; 0.053 sec/batch; 2h:02m:18s remains)
INFO - root - 2019-11-06 20:21:04.855563: step 12570, total loss = 0.50, predict loss = 0.12 (92.8 examples/sec; 0.043 sec/batch; 1h:38m:43s remains)
INFO - root - 2019-11-06 20:21:05.329575: step 12580, total loss = 0.41, predict loss = 0.11 (91.5 examples/sec; 0.044 sec/batch; 1h:40m:04s remains)
INFO - root - 2019-11-06 20:21:05.781251: step 12590, total loss = 0.44, predict loss = 0.13 (94.8 examples/sec; 0.042 sec/batch; 1h:36m:37s remains)
INFO - root - 2019-11-06 20:21:06.744720: step 12600, total loss = 0.34, predict loss = 0.09 (76.3 examples/sec; 0.052 sec/batch; 2h:00m:00s remains)
INFO - root - 2019-11-06 20:21:07.388202: step 12610, total loss = 0.48, predict loss = 0.13 (76.7 examples/sec; 0.052 sec/batch; 1h:59m:27s remains)
INFO - root - 2019-11-06 20:21:07.975608: step 12620, total loss = 0.43, predict loss = 0.14 (74.8 examples/sec; 0.053 sec/batch; 2h:02m:28s remains)
INFO - root - 2019-11-06 20:21:08.544511: step 12630, total loss = 0.37, predict loss = 0.09 (81.8 examples/sec; 0.049 sec/batch; 1h:51m:58s remains)
INFO - root - 2019-11-06 20:21:09.119109: step 12640, total loss = 0.53, predict loss = 0.12 (77.7 examples/sec; 0.051 sec/batch; 1h:57m:51s remains)
INFO - root - 2019-11-06 20:21:09.700867: step 12650, total loss = 0.34, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:54m:49s remains)
INFO - root - 2019-11-06 20:21:10.275216: step 12660, total loss = 0.51, predict loss = 0.13 (81.9 examples/sec; 0.049 sec/batch; 1h:51m:49s remains)
INFO - root - 2019-11-06 20:21:10.826316: step 12670, total loss = 0.28, predict loss = 0.07 (79.2 examples/sec; 0.051 sec/batch; 1h:55m:37s remains)
INFO - root - 2019-11-06 20:21:11.385583: step 12680, total loss = 0.35, predict loss = 0.08 (80.1 examples/sec; 0.050 sec/batch; 1h:54m:15s remains)
INFO - root - 2019-11-06 20:21:11.996554: step 12690, total loss = 0.41, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 1h:57m:58s remains)
INFO - root - 2019-11-06 20:21:12.569811: step 12700, total loss = 0.39, predict loss = 0.11 (78.7 examples/sec; 0.051 sec/batch; 1h:56m:21s remains)
INFO - root - 2019-11-06 20:21:13.130345: step 12710, total loss = 0.27, predict loss = 0.05 (81.5 examples/sec; 0.049 sec/batch; 1h:52m:16s remains)
INFO - root - 2019-11-06 20:21:13.657436: step 12720, total loss = 0.54, predict loss = 0.13 (96.5 examples/sec; 0.041 sec/batch; 1h:34m:48s remains)
INFO - root - 2019-11-06 20:21:14.120773: step 12730, total loss = 0.31, predict loss = 0.08 (99.0 examples/sec; 0.040 sec/batch; 1h:32m:26s remains)
INFO - root - 2019-11-06 20:21:14.572679: step 12740, total loss = 0.36, predict loss = 0.09 (97.8 examples/sec; 0.041 sec/batch; 1h:33m:34s remains)
INFO - root - 2019-11-06 20:21:15.700118: step 12750, total loss = 0.31, predict loss = 0.07 (58.8 examples/sec; 0.068 sec/batch; 2h:35m:42s remains)
INFO - root - 2019-11-06 20:21:16.331804: step 12760, total loss = 0.62, predict loss = 0.16 (79.6 examples/sec; 0.050 sec/batch; 1h:54m:52s remains)
INFO - root - 2019-11-06 20:21:16.920929: step 12770, total loss = 0.29, predict loss = 0.08 (81.7 examples/sec; 0.049 sec/batch; 1h:52m:00s remains)
INFO - root - 2019-11-06 20:21:17.502291: step 12780, total loss = 0.48, predict loss = 0.12 (77.4 examples/sec; 0.052 sec/batch; 1h:58m:11s remains)
INFO - root - 2019-11-06 20:21:18.085358: step 12790, total loss = 0.61, predict loss = 0.17 (81.7 examples/sec; 0.049 sec/batch; 1h:51m:58s remains)
INFO - root - 2019-11-06 20:21:18.650714: step 12800, total loss = 0.53, predict loss = 0.14 (78.5 examples/sec; 0.051 sec/batch; 1h:56m:34s remains)
INFO - root - 2019-11-06 20:21:19.242381: step 12810, total loss = 0.33, predict loss = 0.09 (75.5 examples/sec; 0.053 sec/batch; 2h:01m:04s remains)
INFO - root - 2019-11-06 20:21:19.801831: step 12820, total loss = 0.30, predict loss = 0.08 (76.3 examples/sec; 0.052 sec/batch; 1h:59m:49s remains)
INFO - root - 2019-11-06 20:21:20.374966: step 12830, total loss = 0.29, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:54m:46s remains)
INFO - root - 2019-11-06 20:21:20.937303: step 12840, total loss = 0.27, predict loss = 0.06 (79.7 examples/sec; 0.050 sec/batch; 1h:54m:41s remains)
INFO - root - 2019-11-06 20:21:21.520079: step 12850, total loss = 0.41, predict loss = 0.11 (81.8 examples/sec; 0.049 sec/batch; 1h:51m:48s remains)
INFO - root - 2019-11-06 20:21:22.100330: step 12860, total loss = 0.47, predict loss = 0.13 (80.7 examples/sec; 0.050 sec/batch; 1h:53m:17s remains)
INFO - root - 2019-11-06 20:21:22.603200: step 12870, total loss = 0.22, predict loss = 0.06 (94.9 examples/sec; 0.042 sec/batch; 1h:36m:19s remains)
INFO - root - 2019-11-06 20:21:23.076405: step 12880, total loss = 0.26, predict loss = 0.06 (91.1 examples/sec; 0.044 sec/batch; 1h:40m:20s remains)
INFO - root - 2019-11-06 20:21:23.563945: step 12890, total loss = 0.40, predict loss = 0.10 (96.2 examples/sec; 0.042 sec/batch; 1h:35m:02s remains)
INFO - root - 2019-11-06 20:21:24.712854: step 12900, total loss = 0.61, predict loss = 0.20 (45.9 examples/sec; 0.087 sec/batch; 3h:19m:16s remains)
INFO - root - 2019-11-06 20:21:25.341125: step 12910, total loss = 0.24, predict loss = 0.06 (81.3 examples/sec; 0.049 sec/batch; 1h:52m:21s remains)
INFO - root - 2019-11-06 20:21:25.922822: step 12920, total loss = 0.43, predict loss = 0.10 (77.9 examples/sec; 0.051 sec/batch; 1h:57m:20s remains)
INFO - root - 2019-11-06 20:21:26.518675: step 12930, total loss = 0.33, predict loss = 0.08 (81.2 examples/sec; 0.049 sec/batch; 1h:52m:34s remains)
INFO - root - 2019-11-06 20:21:27.104215: step 12940, total loss = 0.39, predict loss = 0.10 (74.7 examples/sec; 0.054 sec/batch; 2h:02m:17s remains)
INFO - root - 2019-11-06 20:21:27.681016: step 12950, total loss = 0.45, predict loss = 0.12 (77.2 examples/sec; 0.052 sec/batch; 1h:58m:23s remains)
INFO - root - 2019-11-06 20:21:28.255459: step 12960, total loss = 0.31, predict loss = 0.08 (80.8 examples/sec; 0.050 sec/batch; 1h:53m:07s remains)
INFO - root - 2019-11-06 20:21:28.845233: step 12970, total loss = 0.43, predict loss = 0.09 (77.9 examples/sec; 0.051 sec/batch; 1h:57m:17s remains)
INFO - root - 2019-11-06 20:21:29.412541: step 12980, total loss = 0.65, predict loss = 0.17 (74.8 examples/sec; 0.053 sec/batch; 2h:02m:03s remains)
INFO - root - 2019-11-06 20:21:29.980127: step 12990, total loss = 0.36, predict loss = 0.09 (79.5 examples/sec; 0.050 sec/batch; 1h:54m:57s remains)
INFO - root - 2019-11-06 20:21:30.552957: step 13000, total loss = 0.44, predict loss = 0.12 (80.8 examples/sec; 0.050 sec/batch; 1h:53m:04s remains)
INFO - root - 2019-11-06 20:21:31.154730: step 13010, total loss = 0.28, predict loss = 0.08 (75.2 examples/sec; 0.053 sec/batch; 2h:01m:27s remains)
INFO - root - 2019-11-06 20:21:31.644798: step 13020, total loss = 0.30, predict loss = 0.08 (93.6 examples/sec; 0.043 sec/batch; 1h:37m:31s remains)
INFO - root - 2019-11-06 20:21:32.119051: step 13030, total loss = 0.33, predict loss = 0.09 (95.9 examples/sec; 0.042 sec/batch; 1h:35m:12s remains)
INFO - root - 2019-11-06 20:21:33.005821: step 13040, total loss = 0.30, predict loss = 0.08 (8.4 examples/sec; 0.476 sec/batch; 18h:06m:29s remains)
INFO - root - 2019-11-06 20:21:33.626200: step 13050, total loss = 0.34, predict loss = 0.08 (69.7 examples/sec; 0.057 sec/batch; 2h:11m:01s remains)
INFO - root - 2019-11-06 20:21:34.228748: step 13060, total loss = 0.54, predict loss = 0.15 (77.4 examples/sec; 0.052 sec/batch; 1h:57m:55s remains)
INFO - root - 2019-11-06 20:21:34.797813: step 13070, total loss = 0.42, predict loss = 0.11 (80.3 examples/sec; 0.050 sec/batch; 1h:53m:44s remains)
INFO - root - 2019-11-06 20:21:35.371303: step 13080, total loss = 0.28, predict loss = 0.06 (76.3 examples/sec; 0.052 sec/batch; 1h:59m:37s remains)
INFO - root - 2019-11-06 20:21:35.968257: step 13090, total loss = 0.42, predict loss = 0.11 (80.5 examples/sec; 0.050 sec/batch; 1h:53m:22s remains)
INFO - root - 2019-11-06 20:21:36.543359: step 13100, total loss = 0.33, predict loss = 0.09 (76.8 examples/sec; 0.052 sec/batch; 1h:58m:52s remains)
INFO - root - 2019-11-06 20:21:37.115176: step 13110, total loss = 0.35, predict loss = 0.09 (78.8 examples/sec; 0.051 sec/batch; 1h:55m:52s remains)
INFO - root - 2019-11-06 20:21:37.686904: step 13120, total loss = 0.42, predict loss = 0.11 (73.5 examples/sec; 0.054 sec/batch; 2h:04m:12s remains)
INFO - root - 2019-11-06 20:21:38.286169: step 13130, total loss = 0.33, predict loss = 0.09 (76.6 examples/sec; 0.052 sec/batch; 1h:59m:09s remains)
INFO - root - 2019-11-06 20:21:38.844520: step 13140, total loss = 0.24, predict loss = 0.05 (83.9 examples/sec; 0.048 sec/batch; 1h:48m:45s remains)
INFO - root - 2019-11-06 20:21:39.419649: step 13150, total loss = 0.54, predict loss = 0.15 (76.7 examples/sec; 0.052 sec/batch; 1h:58m:53s remains)
INFO - root - 2019-11-06 20:21:40.000786: step 13160, total loss = 0.33, predict loss = 0.07 (86.3 examples/sec; 0.046 sec/batch; 1h:45m:43s remains)
INFO - root - 2019-11-06 20:21:40.488393: step 13170, total loss = 0.53, predict loss = 0.14 (94.3 examples/sec; 0.042 sec/batch; 1h:36m:46s remains)
INFO - root - 2019-11-06 20:21:40.930119: step 13180, total loss = 0.44, predict loss = 0.10 (99.5 examples/sec; 0.040 sec/batch; 1h:31m:38s remains)
INFO - root - 2019-11-06 20:21:41.847412: step 13190, total loss = 0.43, predict loss = 0.13 (78.8 examples/sec; 0.051 sec/batch; 1h:55m:44s remains)
INFO - root - 2019-11-06 20:21:42.500440: step 13200, total loss = 0.39, predict loss = 0.10 (68.5 examples/sec; 0.058 sec/batch; 2h:13m:10s remains)
INFO - root - 2019-11-06 20:21:43.154660: step 13210, total loss = 0.23, predict loss = 0.06 (73.3 examples/sec; 0.055 sec/batch; 2h:04m:25s remains)
INFO - root - 2019-11-06 20:21:43.738582: step 13220, total loss = 0.27, predict loss = 0.07 (78.4 examples/sec; 0.051 sec/batch; 1h:56m:17s remains)
INFO - root - 2019-11-06 20:21:44.317482: step 13230, total loss = 0.28, predict loss = 0.07 (74.1 examples/sec; 0.054 sec/batch; 2h:02m:58s remains)
INFO - root - 2019-11-06 20:21:44.883802: step 13240, total loss = 0.30, predict loss = 0.08 (78.2 examples/sec; 0.051 sec/batch; 1h:56m:32s remains)
INFO - root - 2019-11-06 20:21:45.499413: step 13250, total loss = 0.49, predict loss = 0.13 (77.9 examples/sec; 0.051 sec/batch; 1h:57m:04s remains)
INFO - root - 2019-11-06 20:21:46.064711: step 13260, total loss = 0.41, predict loss = 0.10 (77.0 examples/sec; 0.052 sec/batch; 1h:58m:22s remains)
INFO - root - 2019-11-06 20:21:46.636191: step 13270, total loss = 0.41, predict loss = 0.12 (74.4 examples/sec; 0.054 sec/batch; 2h:02m:33s remains)
INFO - root - 2019-11-06 20:21:47.213167: step 13280, total loss = 0.55, predict loss = 0.14 (78.1 examples/sec; 0.051 sec/batch; 1h:56m:41s remains)
INFO - root - 2019-11-06 20:21:47.806524: step 13290, total loss = 0.49, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 2h:06m:52s remains)
INFO - root - 2019-11-06 20:21:48.372870: step 13300, total loss = 0.34, predict loss = 0.07 (81.4 examples/sec; 0.049 sec/batch; 1h:51m:54s remains)
INFO - root - 2019-11-06 20:21:48.934005: step 13310, total loss = 0.55, predict loss = 0.15 (95.1 examples/sec; 0.042 sec/batch; 1h:35m:48s remains)
INFO - root - 2019-11-06 20:21:49.394483: step 13320, total loss = 0.30, predict loss = 0.08 (94.3 examples/sec; 0.042 sec/batch; 1h:36m:36s remains)
INFO - root - 2019-11-06 20:21:49.862842: step 13330, total loss = 0.25, predict loss = 0.07 (102.6 examples/sec; 0.039 sec/batch; 1h:28m:49s remains)
INFO - root - 2019-11-06 20:21:50.823178: step 13340, total loss = 0.31, predict loss = 0.07 (67.5 examples/sec; 0.059 sec/batch; 2h:15m:01s remains)
INFO - root - 2019-11-06 20:21:51.497765: step 13350, total loss = 0.31, predict loss = 0.09 (74.7 examples/sec; 0.054 sec/batch; 2h:01m:56s remains)
INFO - root - 2019-11-06 20:21:52.111517: step 13360, total loss = 0.43, predict loss = 0.12 (74.1 examples/sec; 0.054 sec/batch; 2h:02m:55s remains)
INFO - root - 2019-11-06 20:21:52.738537: step 13370, total loss = 0.51, predict loss = 0.12 (73.1 examples/sec; 0.055 sec/batch; 2h:04m:40s remains)
INFO - root - 2019-11-06 20:21:53.318108: step 13380, total loss = 0.30, predict loss = 0.08 (80.7 examples/sec; 0.050 sec/batch; 1h:52m:51s remains)
INFO - root - 2019-11-06 20:21:53.891388: step 13390, total loss = 0.53, predict loss = 0.14 (78.0 examples/sec; 0.051 sec/batch; 1h:56m:45s remains)
INFO - root - 2019-11-06 20:21:54.472062: step 13400, total loss = 0.34, predict loss = 0.10 (78.4 examples/sec; 0.051 sec/batch; 1h:56m:08s remains)
INFO - root - 2019-11-06 20:21:55.067352: step 13410, total loss = 0.36, predict loss = 0.09 (78.7 examples/sec; 0.051 sec/batch; 1h:55m:43s remains)
INFO - root - 2019-11-06 20:21:55.634942: step 13420, total loss = 0.41, predict loss = 0.11 (76.0 examples/sec; 0.053 sec/batch; 1h:59m:51s remains)
INFO - root - 2019-11-06 20:21:56.214242: step 13430, total loss = 0.47, predict loss = 0.13 (81.9 examples/sec; 0.049 sec/batch; 1h:51m:12s remains)
INFO - root - 2019-11-06 20:21:56.790911: step 13440, total loss = 0.28, predict loss = 0.07 (76.8 examples/sec; 0.052 sec/batch; 1h:58m:30s remains)
INFO - root - 2019-11-06 20:21:57.390680: step 13450, total loss = 0.31, predict loss = 0.07 (82.9 examples/sec; 0.048 sec/batch; 1h:49m:44s remains)
INFO - root - 2019-11-06 20:21:57.927467: step 13460, total loss = 0.51, predict loss = 0.13 (95.4 examples/sec; 0.042 sec/batch; 1h:35m:24s remains)
INFO - root - 2019-11-06 20:21:58.392363: step 13470, total loss = 0.31, predict loss = 0.09 (96.4 examples/sec; 0.041 sec/batch; 1h:34m:25s remains)
INFO - root - 2019-11-06 20:21:58.856040: step 13480, total loss = 0.43, predict loss = 0.12 (93.5 examples/sec; 0.043 sec/batch; 1h:37m:22s remains)
INFO - root - 2019-11-06 20:21:59.871350: step 13490, total loss = 0.30, predict loss = 0.07 (62.2 examples/sec; 0.064 sec/batch; 2h:26m:25s remains)
INFO - root - 2019-11-06 20:22:00.508500: step 13500, total loss = 0.31, predict loss = 0.09 (69.0 examples/sec; 0.058 sec/batch; 2h:11m:53s remains)
INFO - root - 2019-11-06 20:22:01.070620: step 13510, total loss = 0.64, predict loss = 0.19 (78.7 examples/sec; 0.051 sec/batch; 1h:55m:39s remains)
INFO - root - 2019-11-06 20:22:01.632361: step 13520, total loss = 0.19, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:55m:23s remains)
INFO - root - 2019-11-06 20:22:02.211237: step 13530, total loss = 0.44, predict loss = 0.14 (79.2 examples/sec; 0.051 sec/batch; 1h:54m:55s remains)
INFO - root - 2019-11-06 20:22:02.790258: step 13540, total loss = 0.45, predict loss = 0.12 (76.4 examples/sec; 0.052 sec/batch; 1h:59m:04s remains)
INFO - root - 2019-11-06 20:22:03.370860: step 13550, total loss = 0.24, predict loss = 0.06 (78.0 examples/sec; 0.051 sec/batch; 1h:56m:35s remains)
INFO - root - 2019-11-06 20:22:03.950658: step 13560, total loss = 0.27, predict loss = 0.06 (73.2 examples/sec; 0.055 sec/batch; 2h:04m:15s remains)
INFO - root - 2019-11-06 20:22:04.541222: step 13570, total loss = 0.25, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:58m:29s remains)
INFO - root - 2019-11-06 20:22:05.128192: step 13580, total loss = 0.31, predict loss = 0.08 (76.8 examples/sec; 0.052 sec/batch; 1h:58m:23s remains)
INFO - root - 2019-11-06 20:22:05.701648: step 13590, total loss = 0.39, predict loss = 0.08 (80.8 examples/sec; 0.049 sec/batch; 1h:52m:30s remains)
INFO - root - 2019-11-06 20:22:06.277256: step 13600, total loss = 0.32, predict loss = 0.09 (74.4 examples/sec; 0.054 sec/batch; 2h:02m:18s remains)
INFO - root - 2019-11-06 20:22:06.818209: step 13610, total loss = 0.40, predict loss = 0.10 (104.5 examples/sec; 0.038 sec/batch; 1h:27m:00s remains)
INFO - root - 2019-11-06 20:22:07.280010: step 13620, total loss = 0.25, predict loss = 0.06 (101.0 examples/sec; 0.040 sec/batch; 1h:30m:03s remains)
INFO - root - 2019-11-06 20:22:07.723092: step 13630, total loss = 0.41, predict loss = 0.11 (96.4 examples/sec; 0.042 sec/batch; 1h:34m:19s remains)
INFO - root - 2019-11-06 20:22:08.731468: step 13640, total loss = 0.40, predict loss = 0.10 (62.4 examples/sec; 0.064 sec/batch; 2h:25m:47s remains)
INFO - root - 2019-11-06 20:22:09.418486: step 13650, total loss = 0.28, predict loss = 0.07 (71.8 examples/sec; 0.056 sec/batch; 2h:06m:32s remains)
INFO - root - 2019-11-06 20:22:10.010735: step 13660, total loss = 0.47, predict loss = 0.11 (78.3 examples/sec; 0.051 sec/batch; 1h:56m:02s remains)
INFO - root - 2019-11-06 20:22:10.573797: step 13670, total loss = 0.65, predict loss = 0.16 (77.8 examples/sec; 0.051 sec/batch; 1h:56m:44s remains)
INFO - root - 2019-11-06 20:22:11.149409: step 13680, total loss = 0.32, predict loss = 0.08 (78.0 examples/sec; 0.051 sec/batch; 1h:56m:28s remains)
INFO - root - 2019-11-06 20:22:11.738243: step 13690, total loss = 0.35, predict loss = 0.07 (81.4 examples/sec; 0.049 sec/batch; 1h:51m:34s remains)
INFO - root - 2019-11-06 20:22:12.308908: step 13700, total loss = 0.47, predict loss = 0.12 (75.9 examples/sec; 0.053 sec/batch; 1h:59m:39s remains)
INFO - root - 2019-11-06 20:22:12.886291: step 13710, total loss = 0.36, predict loss = 0.09 (77.3 examples/sec; 0.052 sec/batch; 1h:57m:30s remains)
INFO - root - 2019-11-06 20:22:13.458060: step 13720, total loss = 0.61, predict loss = 0.21 (80.5 examples/sec; 0.050 sec/batch; 1h:52m:51s remains)
INFO - root - 2019-11-06 20:22:14.067101: step 13730, total loss = 0.41, predict loss = 0.11 (77.1 examples/sec; 0.052 sec/batch; 1h:57m:45s remains)
INFO - root - 2019-11-06 20:22:14.642539: step 13740, total loss = 0.28, predict loss = 0.07 (78.6 examples/sec; 0.051 sec/batch; 1h:55m:34s remains)
INFO - root - 2019-11-06 20:22:15.254362: step 13750, total loss = 0.51, predict loss = 0.14 (71.6 examples/sec; 0.056 sec/batch; 2h:06m:46s remains)
INFO - root - 2019-11-06 20:22:15.749583: step 13760, total loss = 0.41, predict loss = 0.11 (98.3 examples/sec; 0.041 sec/batch; 1h:32m:22s remains)
INFO - root - 2019-11-06 20:22:16.218896: step 13770, total loss = 0.48, predict loss = 0.14 (94.5 examples/sec; 0.042 sec/batch; 1h:36m:03s remains)
INFO - root - 2019-11-06 20:22:16.678198: step 13780, total loss = 0.48, predict loss = 0.14 (88.9 examples/sec; 0.045 sec/batch; 1h:42m:06s remains)
INFO - root - 2019-11-06 20:22:17.775947: step 13790, total loss = 0.45, predict loss = 0.12 (59.0 examples/sec; 0.068 sec/batch; 2h:33m:47s remains)
INFO - root - 2019-11-06 20:22:18.396057: step 13800, total loss = 0.31, predict loss = 0.08 (79.3 examples/sec; 0.050 sec/batch; 1h:54m:33s remains)
INFO - root - 2019-11-06 20:22:18.989233: step 13810, total loss = 0.22, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:57m:37s remains)
INFO - root - 2019-11-06 20:22:19.559447: step 13820, total loss = 0.49, predict loss = 0.13 (79.8 examples/sec; 0.050 sec/batch; 1h:53m:49s remains)
INFO - root - 2019-11-06 20:22:20.127816: step 13830, total loss = 0.64, predict loss = 0.20 (77.1 examples/sec; 0.052 sec/batch; 1h:57m:47s remains)
INFO - root - 2019-11-06 20:22:20.699849: step 13840, total loss = 0.26, predict loss = 0.07 (77.1 examples/sec; 0.052 sec/batch; 1h:57m:46s remains)
INFO - root - 2019-11-06 20:22:21.289506: step 13850, total loss = 0.63, predict loss = 0.17 (77.1 examples/sec; 0.052 sec/batch; 1h:57m:44s remains)
INFO - root - 2019-11-06 20:22:21.852706: step 13860, total loss = 0.31, predict loss = 0.09 (77.0 examples/sec; 0.052 sec/batch; 1h:57m:53s remains)
INFO - root - 2019-11-06 20:22:22.427199: step 13870, total loss = 0.31, predict loss = 0.09 (78.6 examples/sec; 0.051 sec/batch; 1h:55m:23s remains)
INFO - root - 2019-11-06 20:22:22.992460: step 13880, total loss = 0.56, predict loss = 0.18 (79.0 examples/sec; 0.051 sec/batch; 1h:54m:49s remains)
INFO - root - 2019-11-06 20:22:23.587730: step 13890, total loss = 0.62, predict loss = 0.18 (71.0 examples/sec; 0.056 sec/batch; 2h:07m:49s remains)
INFO - root - 2019-11-06 20:22:24.164132: step 13900, total loss = 0.38, predict loss = 0.11 (77.4 examples/sec; 0.052 sec/batch; 1h:57m:15s remains)
INFO - root - 2019-11-06 20:22:24.627868: step 13910, total loss = 0.31, predict loss = 0.08 (94.0 examples/sec; 0.043 sec/batch; 1h:36m:31s remains)
INFO - root - 2019-11-06 20:22:25.088551: step 13920, total loss = 0.39, predict loss = 0.10 (94.6 examples/sec; 0.042 sec/batch; 1h:35m:52s remains)
INFO - root - 2019-11-06 20:22:26.024048: step 13930, total loss = 0.36, predict loss = 0.09 (78.5 examples/sec; 0.051 sec/batch; 1h:55m:30s remains)
INFO - root - 2019-11-06 20:22:26.640600: step 13940, total loss = 0.40, predict loss = 0.10 (71.7 examples/sec; 0.056 sec/batch; 2h:06m:28s remains)
INFO - root - 2019-11-06 20:22:27.261179: step 13950, total loss = 0.46, predict loss = 0.12 (78.1 examples/sec; 0.051 sec/batch; 1h:56m:12s remains)
INFO - root - 2019-11-06 20:22:27.838902: step 13960, total loss = 0.30, predict loss = 0.07 (76.3 examples/sec; 0.052 sec/batch; 1h:58m:48s remains)
INFO - root - 2019-11-06 20:22:28.426878: step 13970, total loss = 0.34, predict loss = 0.07 (79.3 examples/sec; 0.050 sec/batch; 1h:54m:18s remains)
INFO - root - 2019-11-06 20:22:28.997955: step 13980, total loss = 0.45, predict loss = 0.12 (82.0 examples/sec; 0.049 sec/batch; 1h:50m:31s remains)
INFO - root - 2019-11-06 20:22:29.580454: step 13990, total loss = 0.50, predict loss = 0.14 (74.4 examples/sec; 0.054 sec/batch; 2h:01m:54s remains)
INFO - root - 2019-11-06 20:22:30.150494: step 14000, total loss = 0.31, predict loss = 0.07 (81.2 examples/sec; 0.049 sec/batch; 1h:51m:39s remains)
INFO - root - 2019-11-06 20:22:30.748202: step 14010, total loss = 0.45, predict loss = 0.12 (77.5 examples/sec; 0.052 sec/batch; 1h:56m:58s remains)
INFO - root - 2019-11-06 20:22:31.328513: step 14020, total loss = 0.39, predict loss = 0.08 (80.1 examples/sec; 0.050 sec/batch; 1h:53m:08s remains)
INFO - root - 2019-11-06 20:22:31.887759: step 14030, total loss = 0.46, predict loss = 0.11 (76.3 examples/sec; 0.052 sec/batch; 1h:58m:51s remains)
INFO - root - 2019-11-06 20:22:32.463644: step 14040, total loss = 0.29, predict loss = 0.07 (75.7 examples/sec; 0.053 sec/batch; 1h:59m:48s remains)
INFO - root - 2019-11-06 20:22:33.044090: step 14050, total loss = 0.36, predict loss = 0.09 (87.1 examples/sec; 0.046 sec/batch; 1h:44m:00s remains)
INFO - root - 2019-11-06 20:22:33.523167: step 14060, total loss = 0.57, predict loss = 0.14 (93.5 examples/sec; 0.043 sec/batch; 1h:36m:55s remains)
INFO - root - 2019-11-06 20:22:33.971525: step 14070, total loss = 0.29, predict loss = 0.07 (94.8 examples/sec; 0.042 sec/batch; 1h:35m:36s remains)
INFO - root - 2019-11-06 20:22:34.890347: step 14080, total loss = 0.54, predict loss = 0.17 (72.3 examples/sec; 0.055 sec/batch; 2h:05m:23s remains)
INFO - root - 2019-11-06 20:22:35.544964: step 14090, total loss = 0.34, predict loss = 0.09 (69.3 examples/sec; 0.058 sec/batch; 2h:10m:47s remains)
INFO - root - 2019-11-06 20:22:36.150141: step 14100, total loss = 0.50, predict loss = 0.13 (74.5 examples/sec; 0.054 sec/batch; 2h:01m:34s remains)
INFO - root - 2019-11-06 20:22:36.720886: step 14110, total loss = 0.34, predict loss = 0.08 (80.0 examples/sec; 0.050 sec/batch; 1h:53m:18s remains)
INFO - root - 2019-11-06 20:22:37.293427: step 14120, total loss = 0.57, predict loss = 0.16 (79.6 examples/sec; 0.050 sec/batch; 1h:53m:51s remains)
INFO - root - 2019-11-06 20:22:37.873389: step 14130, total loss = 0.46, predict loss = 0.13 (81.3 examples/sec; 0.049 sec/batch; 1h:51m:24s remains)
INFO - root - 2019-11-06 20:22:38.439208: step 14140, total loss = 0.31, predict loss = 0.08 (81.2 examples/sec; 0.049 sec/batch; 1h:51m:34s remains)
INFO - root - 2019-11-06 20:22:39.009016: step 14150, total loss = 0.44, predict loss = 0.12 (77.4 examples/sec; 0.052 sec/batch; 1h:56m:56s remains)
INFO - root - 2019-11-06 20:22:39.577928: step 14160, total loss = 0.38, predict loss = 0.09 (76.5 examples/sec; 0.052 sec/batch; 1h:58m:20s remains)
INFO - root - 2019-11-06 20:22:40.162066: step 14170, total loss = 0.27, predict loss = 0.07 (76.9 examples/sec; 0.052 sec/batch; 1h:57m:47s remains)
INFO - root - 2019-11-06 20:22:40.744011: step 14180, total loss = 0.37, predict loss = 0.11 (76.3 examples/sec; 0.052 sec/batch; 1h:58m:37s remains)
INFO - root - 2019-11-06 20:22:41.318341: step 14190, total loss = 0.29, predict loss = 0.07 (80.4 examples/sec; 0.050 sec/batch; 1h:52m:39s remains)
INFO - root - 2019-11-06 20:22:41.880275: step 14200, total loss = 0.31, predict loss = 0.09 (92.1 examples/sec; 0.043 sec/batch; 1h:38m:19s remains)
INFO - root - 2019-11-06 20:22:42.358873: step 14210, total loss = 0.27, predict loss = 0.07 (96.8 examples/sec; 0.041 sec/batch; 1h:33m:32s remains)
INFO - root - 2019-11-06 20:22:42.822317: step 14220, total loss = 0.32, predict loss = 0.07 (94.1 examples/sec; 0.043 sec/batch; 1h:36m:12s remains)
INFO - root - 2019-11-06 20:22:43.776930: step 14230, total loss = 0.28, predict loss = 0.07 (73.8 examples/sec; 0.054 sec/batch; 2h:02m:36s remains)
INFO - root - 2019-11-06 20:22:44.400587: step 14240, total loss = 0.35, predict loss = 0.09 (73.5 examples/sec; 0.054 sec/batch; 2h:03m:10s remains)
INFO - root - 2019-11-06 20:22:45.027305: step 14250, total loss = 0.44, predict loss = 0.13 (61.2 examples/sec; 0.065 sec/batch; 2h:27m:54s remains)
INFO - root - 2019-11-06 20:22:45.634698: step 14260, total loss = 0.29, predict loss = 0.07 (77.8 examples/sec; 0.051 sec/batch; 1h:56m:14s remains)
INFO - root - 2019-11-06 20:22:46.211543: step 14270, total loss = 0.42, predict loss = 0.12 (77.7 examples/sec; 0.051 sec/batch; 1h:56m:27s remains)
INFO - root - 2019-11-06 20:22:46.781978: step 14280, total loss = 0.48, predict loss = 0.12 (80.4 examples/sec; 0.050 sec/batch; 1h:52m:34s remains)
INFO - root - 2019-11-06 20:22:47.369149: step 14290, total loss = 0.45, predict loss = 0.13 (80.6 examples/sec; 0.050 sec/batch; 1h:52m:16s remains)
INFO - root - 2019-11-06 20:22:47.928000: step 14300, total loss = 0.19, predict loss = 0.05 (82.4 examples/sec; 0.049 sec/batch; 1h:49m:46s remains)
INFO - root - 2019-11-06 20:22:48.500415: step 14310, total loss = 0.38, predict loss = 0.09 (80.9 examples/sec; 0.049 sec/batch; 1h:51m:48s remains)
INFO - root - 2019-11-06 20:22:49.068227: step 14320, total loss = 0.55, predict loss = 0.15 (79.8 examples/sec; 0.050 sec/batch; 1h:53m:20s remains)
INFO - root - 2019-11-06 20:22:49.651630: step 14330, total loss = 0.57, predict loss = 0.14 (75.5 examples/sec; 0.053 sec/batch; 1h:59m:46s remains)
INFO - root - 2019-11-06 20:22:50.233612: step 14340, total loss = 0.31, predict loss = 0.08 (77.9 examples/sec; 0.051 sec/batch; 1h:56m:04s remains)
INFO - root - 2019-11-06 20:22:50.767649: step 14350, total loss = 0.42, predict loss = 0.11 (95.8 examples/sec; 0.042 sec/batch; 1h:34m:23s remains)
INFO - root - 2019-11-06 20:22:51.218636: step 14360, total loss = 0.31, predict loss = 0.08 (93.9 examples/sec; 0.043 sec/batch; 1h:36m:15s remains)
INFO - root - 2019-11-06 20:22:51.714858: step 14370, total loss = 0.42, predict loss = 0.11 (96.4 examples/sec; 0.041 sec/batch; 1h:33m:48s remains)
INFO - root - 2019-11-06 20:22:52.717765: step 14380, total loss = 0.27, predict loss = 0.07 (63.2 examples/sec; 0.063 sec/batch; 2h:22m:59s remains)
INFO - root - 2019-11-06 20:22:53.415360: step 14390, total loss = 0.37, predict loss = 0.09 (67.6 examples/sec; 0.059 sec/batch; 2h:13m:40s remains)
INFO - root - 2019-11-06 20:22:54.016856: step 14400, total loss = 0.50, predict loss = 0.12 (81.6 examples/sec; 0.049 sec/batch; 1h:50m:49s remains)
INFO - root - 2019-11-06 20:22:54.611145: step 14410, total loss = 0.46, predict loss = 0.11 (76.5 examples/sec; 0.052 sec/batch; 1h:58m:09s remains)
INFO - root - 2019-11-06 20:22:55.186815: step 14420, total loss = 0.40, predict loss = 0.09 (78.3 examples/sec; 0.051 sec/batch; 1h:55m:24s remains)
INFO - root - 2019-11-06 20:22:55.758460: step 14430, total loss = 0.39, predict loss = 0.08 (77.9 examples/sec; 0.051 sec/batch; 1h:56m:00s remains)
INFO - root - 2019-11-06 20:22:56.335092: step 14440, total loss = 0.31, predict loss = 0.08 (75.7 examples/sec; 0.053 sec/batch; 1h:59m:19s remains)
INFO - root - 2019-11-06 20:22:56.916722: step 14450, total loss = 0.40, predict loss = 0.11 (76.9 examples/sec; 0.052 sec/batch; 1h:57m:32s remains)
INFO - root - 2019-11-06 20:22:57.468938: step 14460, total loss = 0.30, predict loss = 0.08 (78.0 examples/sec; 0.051 sec/batch; 1h:55m:46s remains)
INFO - root - 2019-11-06 20:22:58.052287: step 14470, total loss = 0.47, predict loss = 0.13 (76.1 examples/sec; 0.053 sec/batch; 1h:58m:47s remains)
INFO - root - 2019-11-06 20:22:58.624138: step 14480, total loss = 0.46, predict loss = 0.12 (80.1 examples/sec; 0.050 sec/batch; 1h:52m:48s remains)
INFO - root - 2019-11-06 20:22:59.214681: step 14490, total loss = 0.25, predict loss = 0.06 (79.9 examples/sec; 0.050 sec/batch; 1h:53m:02s remains)
INFO - root - 2019-11-06 20:22:59.721980: step 14500, total loss = 0.38, predict loss = 0.10 (100.7 examples/sec; 0.040 sec/batch; 1h:29m:44s remains)
INFO - root - 2019-11-06 20:23:00.168899: step 14510, total loss = 0.70, predict loss = 0.23 (98.5 examples/sec; 0.041 sec/batch; 1h:31m:41s remains)
INFO - root - 2019-11-06 20:23:00.625378: step 14520, total loss = 0.32, predict loss = 0.08 (94.9 examples/sec; 0.042 sec/batch; 1h:35m:07s remains)
INFO - root - 2019-11-06 20:23:01.724138: step 14530, total loss = 0.41, predict loss = 0.11 (61.3 examples/sec; 0.065 sec/batch; 2h:27m:19s remains)
INFO - root - 2019-11-06 20:23:02.363650: step 14540, total loss = 0.44, predict loss = 0.14 (74.3 examples/sec; 0.054 sec/batch; 2h:01m:37s remains)
INFO - root - 2019-11-06 20:23:02.939054: step 14550, total loss = 0.33, predict loss = 0.09 (80.6 examples/sec; 0.050 sec/batch; 1h:51m:59s remains)
INFO - root - 2019-11-06 20:23:03.506086: step 14560, total loss = 0.41, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 1h:56m:25s remains)
INFO - root - 2019-11-06 20:23:04.095279: step 14570, total loss = 0.34, predict loss = 0.08 (81.8 examples/sec; 0.049 sec/batch; 1h:50m:22s remains)
INFO - root - 2019-11-06 20:23:04.668001: step 14580, total loss = 0.26, predict loss = 0.06 (81.0 examples/sec; 0.049 sec/batch; 1h:51m:26s remains)
INFO - root - 2019-11-06 20:23:05.251434: step 14590, total loss = 0.60, predict loss = 0.16 (76.0 examples/sec; 0.053 sec/batch; 1h:58m:43s remains)
INFO - root - 2019-11-06 20:23:05.824131: step 14600, total loss = 0.42, predict loss = 0.11 (78.3 examples/sec; 0.051 sec/batch; 1h:55m:19s remains)
INFO - root - 2019-11-06 20:23:06.403579: step 14610, total loss = 0.36, predict loss = 0.12 (79.9 examples/sec; 0.050 sec/batch; 1h:53m:01s remains)
INFO - root - 2019-11-06 20:23:06.972866: step 14620, total loss = 0.49, predict loss = 0.12 (79.1 examples/sec; 0.051 sec/batch; 1h:54m:04s remains)
INFO - root - 2019-11-06 20:23:07.532609: step 14630, total loss = 0.36, predict loss = 0.08 (79.8 examples/sec; 0.050 sec/batch; 1h:53m:04s remains)
INFO - root - 2019-11-06 20:23:08.102349: step 14640, total loss = 0.23, predict loss = 0.06 (76.6 examples/sec; 0.052 sec/batch; 1h:57m:47s remains)
INFO - root - 2019-11-06 20:23:08.609004: step 14650, total loss = 0.46, predict loss = 0.11 (97.2 examples/sec; 0.041 sec/batch; 1h:32m:51s remains)
INFO - root - 2019-11-06 20:23:09.071754: step 14660, total loss = 0.24, predict loss = 0.06 (92.5 examples/sec; 0.043 sec/batch; 1h:37m:32s remains)
INFO - root - 2019-11-06 20:23:09.972244: step 14670, total loss = 0.25, predict loss = 0.06 (8.4 examples/sec; 0.478 sec/batch; 17h:59m:00s remains)
INFO - root - 2019-11-06 20:23:10.600067: step 14680, total loss = 0.36, predict loss = 0.10 (68.5 examples/sec; 0.058 sec/batch; 2h:11m:41s remains)
INFO - root - 2019-11-06 20:23:11.220827: step 14690, total loss = 0.30, predict loss = 0.08 (78.0 examples/sec; 0.051 sec/batch; 1h:55m:42s remains)
INFO - root - 2019-11-06 20:23:11.798887: step 14700, total loss = 0.51, predict loss = 0.13 (75.2 examples/sec; 0.053 sec/batch; 1h:59m:55s remains)
INFO - root - 2019-11-06 20:23:12.364325: step 14710, total loss = 0.56, predict loss = 0.15 (77.1 examples/sec; 0.052 sec/batch; 1h:57m:01s remains)
INFO - root - 2019-11-06 20:23:12.936644: step 14720, total loss = 0.27, predict loss = 0.07 (78.1 examples/sec; 0.051 sec/batch; 1h:55m:26s remains)
INFO - root - 2019-11-06 20:23:13.515086: step 14730, total loss = 0.25, predict loss = 0.06 (81.5 examples/sec; 0.049 sec/batch; 1h:50m:39s remains)
INFO - root - 2019-11-06 20:23:14.089339: step 14740, total loss = 0.32, predict loss = 0.08 (78.7 examples/sec; 0.051 sec/batch; 1h:54m:37s remains)
INFO - root - 2019-11-06 20:23:14.669728: step 14750, total loss = 0.41, predict loss = 0.10 (77.3 examples/sec; 0.052 sec/batch; 1h:56m:40s remains)
INFO - root - 2019-11-06 20:23:15.293049: step 14760, total loss = 0.31, predict loss = 0.08 (79.0 examples/sec; 0.051 sec/batch; 1h:54m:11s remains)
INFO - root - 2019-11-06 20:23:15.877110: step 14770, total loss = 0.29, predict loss = 0.07 (77.0 examples/sec; 0.052 sec/batch; 1h:57m:00s remains)
INFO - root - 2019-11-06 20:23:16.445471: step 14780, total loss = 0.34, predict loss = 0.08 (79.2 examples/sec; 0.051 sec/batch; 1h:53m:51s remains)
INFO - root - 2019-11-06 20:23:17.016542: step 14790, total loss = 0.45, predict loss = 0.12 (90.3 examples/sec; 0.044 sec/batch; 1h:39m:46s remains)
INFO - root - 2019-11-06 20:23:17.480868: step 14800, total loss = 0.28, predict loss = 0.07 (98.3 examples/sec; 0.041 sec/batch; 1h:31m:40s remains)
INFO - root - 2019-11-06 20:23:17.957921: step 14810, total loss = 0.36, predict loss = 0.09 (96.1 examples/sec; 0.042 sec/batch; 1h:33m:49s remains)
INFO - root - 2019-11-06 20:23:18.876783: step 14820, total loss = 0.44, predict loss = 0.14 (74.5 examples/sec; 0.054 sec/batch; 2h:00m:58s remains)
INFO - root - 2019-11-06 20:23:19.545257: step 14830, total loss = 0.34, predict loss = 0.09 (58.1 examples/sec; 0.069 sec/batch; 2h:35m:07s remains)
INFO - root - 2019-11-06 20:23:20.164908: step 14840, total loss = 0.31, predict loss = 0.07 (82.2 examples/sec; 0.049 sec/batch; 1h:49m:37s remains)
INFO - root - 2019-11-06 20:23:20.766190: step 14850, total loss = 0.58, predict loss = 0.16 (77.9 examples/sec; 0.051 sec/batch; 1h:55m:37s remains)
INFO - root - 2019-11-06 20:23:21.329378: step 14860, total loss = 0.44, predict loss = 0.13 (78.3 examples/sec; 0.051 sec/batch; 1h:55m:05s remains)
INFO - root - 2019-11-06 20:23:21.892972: step 14870, total loss = 0.39, predict loss = 0.10 (80.2 examples/sec; 0.050 sec/batch; 1h:52m:23s remains)
INFO - root - 2019-11-06 20:23:22.478482: step 14880, total loss = 0.23, predict loss = 0.06 (79.4 examples/sec; 0.050 sec/batch; 1h:53m:25s remains)
INFO - root - 2019-11-06 20:23:23.056427: step 14890, total loss = 0.26, predict loss = 0.07 (81.1 examples/sec; 0.049 sec/batch; 1h:51m:04s remains)
INFO - root - 2019-11-06 20:23:23.611076: step 14900, total loss = 0.21, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:55m:32s remains)
INFO - root - 2019-11-06 20:23:24.185899: step 14910, total loss = 0.29, predict loss = 0.07 (77.0 examples/sec; 0.052 sec/batch; 1h:56m:57s remains)
INFO - root - 2019-11-06 20:23:24.763512: step 14920, total loss = 0.46, predict loss = 0.13 (82.7 examples/sec; 0.048 sec/batch; 1h:48m:55s remains)
INFO - root - 2019-11-06 20:23:25.352287: step 14930, total loss = 0.36, predict loss = 0.09 (76.6 examples/sec; 0.052 sec/batch; 1h:57m:31s remains)
INFO - root - 2019-11-06 20:23:25.921732: step 14940, total loss = 0.44, predict loss = 0.12 (91.8 examples/sec; 0.044 sec/batch; 1h:38m:06s remains)
INFO - root - 2019-11-06 20:23:26.375395: step 14950, total loss = 0.39, predict loss = 0.12 (94.8 examples/sec; 0.042 sec/batch; 1h:34m:55s remains)
INFO - root - 2019-11-06 20:23:26.836627: step 14960, total loss = 0.36, predict loss = 0.08 (93.7 examples/sec; 0.043 sec/batch; 1h:36m:07s remains)
INFO - root - 2019-11-06 20:23:27.812328: step 14970, total loss = 0.79, predict loss = 0.18 (73.7 examples/sec; 0.054 sec/batch; 2h:02m:12s remains)
INFO - root - 2019-11-06 20:23:28.471668: step 14980, total loss = 0.33, predict loss = 0.09 (67.6 examples/sec; 0.059 sec/batch; 2h:13m:13s remains)
INFO - root - 2019-11-06 20:23:29.135369: step 14990, total loss = 0.35, predict loss = 0.09 (70.6 examples/sec; 0.057 sec/batch; 2h:07m:33s remains)
INFO - root - 2019-11-06 20:23:29.738947: step 15000, total loss = 0.26, predict loss = 0.06 (81.1 examples/sec; 0.049 sec/batch; 1h:50m:58s remains)
INFO - root - 2019-11-06 20:23:30.950843: step 15010, total loss = 0.44, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 2h:10m:10s remains)
INFO - root - 2019-11-06 20:23:31.522760: step 15020, total loss = 0.40, predict loss = 0.12 (79.1 examples/sec; 0.051 sec/batch; 1h:53m:46s remains)
INFO - root - 2019-11-06 20:23:32.113011: step 15030, total loss = 0.27, predict loss = 0.08 (80.5 examples/sec; 0.050 sec/batch; 1h:51m:45s remains)
INFO - root - 2019-11-06 20:23:32.684881: step 15040, total loss = 0.30, predict loss = 0.07 (80.4 examples/sec; 0.050 sec/batch; 1h:51m:58s remains)
INFO - root - 2019-11-06 20:23:33.265717: step 15050, total loss = 0.36, predict loss = 0.10 (80.8 examples/sec; 0.049 sec/batch; 1h:51m:19s remains)
INFO - root - 2019-11-06 20:23:33.851028: step 15060, total loss = 0.32, predict loss = 0.08 (78.1 examples/sec; 0.051 sec/batch; 1h:55m:14s remains)
INFO - root - 2019-11-06 20:23:34.432269: step 15070, total loss = 0.37, predict loss = 0.10 (77.6 examples/sec; 0.052 sec/batch; 1h:55m:58s remains)
INFO - root - 2019-11-06 20:23:35.004220: step 15080, total loss = 0.35, predict loss = 0.08 (79.6 examples/sec; 0.050 sec/batch; 1h:53m:03s remains)
INFO - root - 2019-11-06 20:23:35.561442: step 15090, total loss = 0.61, predict loss = 0.21 (91.4 examples/sec; 0.044 sec/batch; 1h:38m:21s remains)
INFO - root - 2019-11-06 20:23:36.015538: step 15100, total loss = 0.51, predict loss = 0.13 (100.2 examples/sec; 0.040 sec/batch; 1h:29m:43s remains)
INFO - root - 2019-11-06 20:23:36.472775: step 15110, total loss = 0.55, predict loss = 0.15 (96.6 examples/sec; 0.041 sec/batch; 1h:33m:05s remains)
INFO - root - 2019-11-06 20:23:37.486014: step 15120, total loss = 0.62, predict loss = 0.14 (58.3 examples/sec; 0.069 sec/batch; 2h:34m:08s remains)
INFO - root - 2019-11-06 20:23:38.142243: step 15130, total loss = 0.39, predict loss = 0.09 (77.6 examples/sec; 0.052 sec/batch; 1h:55m:51s remains)
INFO - root - 2019-11-06 20:23:38.706561: step 15140, total loss = 0.54, predict loss = 0.16 (79.8 examples/sec; 0.050 sec/batch; 1h:52m:39s remains)
INFO - root - 2019-11-06 20:23:39.282846: step 15150, total loss = 0.32, predict loss = 0.09 (87.6 examples/sec; 0.046 sec/batch; 1h:42m:34s remains)
INFO - root - 2019-11-06 20:23:39.865401: step 15160, total loss = 0.30, predict loss = 0.08 (79.5 examples/sec; 0.050 sec/batch; 1h:53m:08s remains)
INFO - root - 2019-11-06 20:23:40.457075: step 15170, total loss = 0.48, predict loss = 0.14 (78.1 examples/sec; 0.051 sec/batch; 1h:55m:08s remains)
INFO - root - 2019-11-06 20:23:41.028201: step 15180, total loss = 0.60, predict loss = 0.14 (78.3 examples/sec; 0.051 sec/batch; 1h:54m:43s remains)
INFO - root - 2019-11-06 20:23:41.603920: step 15190, total loss = 0.36, predict loss = 0.10 (78.3 examples/sec; 0.051 sec/batch; 1h:54m:46s remains)
INFO - root - 2019-11-06 20:23:42.179871: step 15200, total loss = 0.61, predict loss = 0.15 (82.0 examples/sec; 0.049 sec/batch; 1h:49m:38s remains)
INFO - root - 2019-11-06 20:23:42.770253: step 15210, total loss = 0.34, predict loss = 0.10 (79.6 examples/sec; 0.050 sec/batch; 1h:52m:56s remains)
INFO - root - 2019-11-06 20:23:43.341165: step 15220, total loss = 0.48, predict loss = 0.13 (78.7 examples/sec; 0.051 sec/batch; 1h:54m:12s remains)
INFO - root - 2019-11-06 20:23:43.911103: step 15230, total loss = 0.39, predict loss = 0.12 (76.4 examples/sec; 0.052 sec/batch; 1h:57m:38s remains)
INFO - root - 2019-11-06 20:23:44.422083: step 15240, total loss = 0.51, predict loss = 0.14 (101.8 examples/sec; 0.039 sec/batch; 1h:28m:14s remains)
INFO - root - 2019-11-06 20:23:44.903648: step 15250, total loss = 0.35, predict loss = 0.10 (93.0 examples/sec; 0.043 sec/batch; 1h:36m:33s remains)
INFO - root - 2019-11-06 20:23:45.375420: step 15260, total loss = 0.30, predict loss = 0.08 (99.3 examples/sec; 0.040 sec/batch; 1h:30m:26s remains)
INFO - root - 2019-11-06 20:23:46.411441: step 15270, total loss = 0.56, predict loss = 0.14 (67.6 examples/sec; 0.059 sec/batch; 2h:12m:47s remains)
INFO - root - 2019-11-06 20:23:47.010182: step 15280, total loss = 0.54, predict loss = 0.12 (81.6 examples/sec; 0.049 sec/batch; 1h:50m:07s remains)
INFO - root - 2019-11-06 20:23:47.591599: step 15290, total loss = 0.38, predict loss = 0.07 (78.3 examples/sec; 0.051 sec/batch; 1h:54m:44s remains)
INFO - root - 2019-11-06 20:23:48.152463: step 15300, total loss = 0.48, predict loss = 0.14 (76.0 examples/sec; 0.053 sec/batch; 1h:58m:08s remains)
INFO - root - 2019-11-06 20:23:48.735063: step 15310, total loss = 0.33, predict loss = 0.08 (77.1 examples/sec; 0.052 sec/batch; 1h:56m:26s remains)
INFO - root - 2019-11-06 20:23:49.325768: step 15320, total loss = 0.24, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:56m:20s remains)
INFO - root - 2019-11-06 20:23:49.909168: step 15330, total loss = 0.45, predict loss = 0.11 (77.8 examples/sec; 0.051 sec/batch; 1h:55m:25s remains)
INFO - root - 2019-11-06 20:23:50.481254: step 15340, total loss = 0.29, predict loss = 0.07 (80.6 examples/sec; 0.050 sec/batch; 1h:51m:23s remains)
INFO - root - 2019-11-06 20:23:51.040471: step 15350, total loss = 0.29, predict loss = 0.07 (76.7 examples/sec; 0.052 sec/batch; 1h:56m:59s remains)
INFO - root - 2019-11-06 20:23:51.607980: step 15360, total loss = 0.65, predict loss = 0.17 (81.4 examples/sec; 0.049 sec/batch; 1h:50m:16s remains)
INFO - root - 2019-11-06 20:23:52.200927: step 15370, total loss = 0.27, predict loss = 0.06 (79.5 examples/sec; 0.050 sec/batch; 1h:52m:50s remains)
INFO - root - 2019-11-06 20:23:52.778983: step 15380, total loss = 0.28, predict loss = 0.07 (82.5 examples/sec; 0.048 sec/batch; 1h:48m:43s remains)
INFO - root - 2019-11-06 20:23:53.274801: step 15390, total loss = 0.42, predict loss = 0.12 (104.6 examples/sec; 0.038 sec/batch; 1h:25m:46s remains)
INFO - root - 2019-11-06 20:23:53.745081: step 15400, total loss = 0.35, predict loss = 0.10 (95.1 examples/sec; 0.042 sec/batch; 1h:34m:23s remains)
INFO - root - 2019-11-06 20:23:54.220717: step 15410, total loss = 0.46, predict loss = 0.12 (96.8 examples/sec; 0.041 sec/batch; 1h:32m:43s remains)
INFO - root - 2019-11-06 20:23:55.313014: step 15420, total loss = 0.44, predict loss = 0.12 (62.1 examples/sec; 0.064 sec/batch; 2h:24m:24s remains)
INFO - root - 2019-11-06 20:23:55.922548: step 15430, total loss = 0.42, predict loss = 0.10 (81.6 examples/sec; 0.049 sec/batch; 1h:49m:57s remains)
INFO - root - 2019-11-06 20:23:56.484037: step 15440, total loss = 0.45, predict loss = 0.12 (78.7 examples/sec; 0.051 sec/batch; 1h:54m:02s remains)
INFO - root - 2019-11-06 20:23:57.062125: step 15450, total loss = 0.32, predict loss = 0.09 (79.3 examples/sec; 0.050 sec/batch; 1h:53m:11s remains)
INFO - root - 2019-11-06 20:23:57.642435: step 15460, total loss = 0.45, predict loss = 0.12 (81.3 examples/sec; 0.049 sec/batch; 1h:50m:20s remains)
INFO - root - 2019-11-06 20:23:58.203257: step 15470, total loss = 0.37, predict loss = 0.10 (80.8 examples/sec; 0.049 sec/batch; 1h:50m:57s remains)
INFO - root - 2019-11-06 20:23:58.773456: step 15480, total loss = 0.38, predict loss = 0.10 (79.4 examples/sec; 0.050 sec/batch; 1h:52m:59s remains)
INFO - root - 2019-11-06 20:23:59.353750: step 15490, total loss = 0.30, predict loss = 0.09 (83.2 examples/sec; 0.048 sec/batch; 1h:47m:47s remains)
INFO - root - 2019-11-06 20:23:59.928167: step 15500, total loss = 0.41, predict loss = 0.12 (79.8 examples/sec; 0.050 sec/batch; 1h:52m:17s remains)
INFO - root - 2019-11-06 20:24:00.512973: step 15510, total loss = 0.32, predict loss = 0.08 (79.9 examples/sec; 0.050 sec/batch; 1h:52m:12s remains)
INFO - root - 2019-11-06 20:24:01.091137: step 15520, total loss = 0.25, predict loss = 0.06 (77.3 examples/sec; 0.052 sec/batch; 1h:55m:59s remains)
INFO - root - 2019-11-06 20:24:01.662413: step 15530, total loss = 0.24, predict loss = 0.06 (78.5 examples/sec; 0.051 sec/batch; 1h:54m:09s remains)
INFO - root - 2019-11-06 20:24:02.131762: step 15540, total loss = 0.28, predict loss = 0.08 (92.0 examples/sec; 0.043 sec/batch; 1h:37m:23s remains)
INFO - root - 2019-11-06 20:24:02.580758: step 15550, total loss = 0.38, predict loss = 0.09 (96.1 examples/sec; 0.042 sec/batch; 1h:33m:17s remains)
INFO - root - 2019-11-06 20:24:03.465266: step 15560, total loss = 0.43, predict loss = 0.11 (74.8 examples/sec; 0.053 sec/batch; 1h:59m:45s remains)
INFO - root - 2019-11-06 20:24:04.146164: step 15570, total loss = 0.39, predict loss = 0.09 (66.4 examples/sec; 0.060 sec/batch; 2h:14m:53s remains)
INFO - root - 2019-11-06 20:24:04.769884: step 15580, total loss = 0.35, predict loss = 0.10 (79.2 examples/sec; 0.051 sec/batch; 1h:53m:08s remains)
INFO - root - 2019-11-06 20:24:05.333752: step 15590, total loss = 0.26, predict loss = 0.07 (82.2 examples/sec; 0.049 sec/batch; 1h:49m:00s remains)
INFO - root - 2019-11-06 20:24:05.913760: step 15600, total loss = 0.34, predict loss = 0.10 (78.4 examples/sec; 0.051 sec/batch; 1h:54m:18s remains)
INFO - root - 2019-11-06 20:24:06.497764: step 15610, total loss = 0.34, predict loss = 0.10 (80.7 examples/sec; 0.050 sec/batch; 1h:51m:00s remains)
INFO - root - 2019-11-06 20:24:07.062503: step 15620, total loss = 0.34, predict loss = 0.09 (81.3 examples/sec; 0.049 sec/batch; 1h:50m:13s remains)
INFO - root - 2019-11-06 20:24:07.649335: step 15630, total loss = 0.27, predict loss = 0.07 (75.7 examples/sec; 0.053 sec/batch; 1h:58m:17s remains)
INFO - root - 2019-11-06 20:24:08.222072: step 15640, total loss = 0.41, predict loss = 0.11 (80.6 examples/sec; 0.050 sec/batch; 1h:51m:09s remains)
INFO - root - 2019-11-06 20:24:08.814412: step 15650, total loss = 0.43, predict loss = 0.13 (81.2 examples/sec; 0.049 sec/batch; 1h:50m:16s remains)
INFO - root - 2019-11-06 20:24:09.374825: step 15660, total loss = 0.26, predict loss = 0.08 (77.4 examples/sec; 0.052 sec/batch; 1h:55m:41s remains)
INFO - root - 2019-11-06 20:24:09.948715: step 15670, total loss = 0.46, predict loss = 0.13 (74.4 examples/sec; 0.054 sec/batch; 2h:00m:19s remains)
INFO - root - 2019-11-06 20:24:10.504818: step 15680, total loss = 0.27, predict loss = 0.06 (92.2 examples/sec; 0.043 sec/batch; 1h:37m:08s remains)
INFO - root - 2019-11-06 20:24:10.977066: step 15690, total loss = 0.55, predict loss = 0.15 (93.7 examples/sec; 0.043 sec/batch; 1h:35m:34s remains)
INFO - root - 2019-11-06 20:24:11.411784: step 15700, total loss = 0.41, predict loss = 0.11 (106.1 examples/sec; 0.038 sec/batch; 1h:24m:23s remains)
INFO - root - 2019-11-06 20:24:12.319350: step 15710, total loss = 0.48, predict loss = 0.14 (70.2 examples/sec; 0.057 sec/batch; 2h:07m:35s remains)
INFO - root - 2019-11-06 20:24:13.031343: step 15720, total loss = 0.30, predict loss = 0.07 (65.1 examples/sec; 0.061 sec/batch; 2h:17m:25s remains)
INFO - root - 2019-11-06 20:24:13.653392: step 15730, total loss = 0.35, predict loss = 0.10 (79.3 examples/sec; 0.050 sec/batch; 1h:52m:49s remains)
INFO - root - 2019-11-06 20:24:14.214087: step 15740, total loss = 0.26, predict loss = 0.06 (78.2 examples/sec; 0.051 sec/batch; 1h:54m:26s remains)
INFO - root - 2019-11-06 20:24:14.786839: step 15750, total loss = 0.26, predict loss = 0.07 (79.6 examples/sec; 0.050 sec/batch; 1h:52m:27s remains)
INFO - root - 2019-11-06 20:24:15.414035: step 15760, total loss = 0.36, predict loss = 0.09 (74.7 examples/sec; 0.054 sec/batch; 1h:59m:50s remains)
INFO - root - 2019-11-06 20:24:15.987359: step 15770, total loss = 0.29, predict loss = 0.08 (82.8 examples/sec; 0.048 sec/batch; 1h:48m:08s remains)
INFO - root - 2019-11-06 20:24:16.565715: step 15780, total loss = 0.53, predict loss = 0.15 (74.1 examples/sec; 0.054 sec/batch; 2h:00m:44s remains)
INFO - root - 2019-11-06 20:24:17.143735: step 15790, total loss = 0.25, predict loss = 0.06 (72.7 examples/sec; 0.055 sec/batch; 2h:03m:01s remains)
INFO - root - 2019-11-06 20:24:17.731620: step 15800, total loss = 0.31, predict loss = 0.07 (78.6 examples/sec; 0.051 sec/batch; 1h:53m:52s remains)
INFO - root - 2019-11-06 20:24:18.318465: step 15810, total loss = 0.55, predict loss = 0.14 (79.1 examples/sec; 0.051 sec/batch; 1h:53m:03s remains)
INFO - root - 2019-11-06 20:24:18.892892: step 15820, total loss = 0.30, predict loss = 0.07 (81.7 examples/sec; 0.049 sec/batch; 1h:49m:32s remains)
INFO - root - 2019-11-06 20:24:19.433711: step 15830, total loss = 0.39, predict loss = 0.10 (96.6 examples/sec; 0.041 sec/batch; 1h:32m:33s remains)
INFO - root - 2019-11-06 20:24:19.885527: step 15840, total loss = 0.21, predict loss = 0.06 (91.5 examples/sec; 0.044 sec/batch; 1h:37m:44s remains)
INFO - root - 2019-11-06 20:24:20.369322: step 15850, total loss = 0.42, predict loss = 0.12 (95.6 examples/sec; 0.042 sec/batch; 1h:33m:33s remains)
INFO - root - 2019-11-06 20:24:21.349039: step 15860, total loss = 0.27, predict loss = 0.06 (62.8 examples/sec; 0.064 sec/batch; 2h:22m:18s remains)
INFO - root - 2019-11-06 20:24:22.102897: step 15870, total loss = 0.43, predict loss = 0.11 (59.1 examples/sec; 0.068 sec/batch; 2h:31m:13s remains)
INFO - root - 2019-11-06 20:24:22.714235: step 15880, total loss = 0.61, predict loss = 0.19 (77.7 examples/sec; 0.051 sec/batch; 1h:55m:04s remains)
INFO - root - 2019-11-06 20:24:23.317717: step 15890, total loss = 0.27, predict loss = 0.07 (74.6 examples/sec; 0.054 sec/batch; 1h:59m:49s remains)
INFO - root - 2019-11-06 20:24:23.900442: step 15900, total loss = 0.35, predict loss = 0.09 (79.7 examples/sec; 0.050 sec/batch; 1h:52m:10s remains)
INFO - root - 2019-11-06 20:24:24.467506: step 15910, total loss = 0.53, predict loss = 0.12 (76.8 examples/sec; 0.052 sec/batch; 1h:56m:24s remains)
INFO - root - 2019-11-06 20:24:25.048842: step 15920, total loss = 0.22, predict loss = 0.06 (75.3 examples/sec; 0.053 sec/batch; 1h:58m:43s remains)
INFO - root - 2019-11-06 20:24:25.640517: step 15930, total loss = 0.37, predict loss = 0.11 (78.5 examples/sec; 0.051 sec/batch; 1h:53m:50s remains)
INFO - root - 2019-11-06 20:24:26.215084: step 15940, total loss = 0.53, predict loss = 0.14 (77.1 examples/sec; 0.052 sec/batch; 1h:55m:53s remains)
INFO - root - 2019-11-06 20:24:26.791547: step 15950, total loss = 0.34, predict loss = 0.08 (80.9 examples/sec; 0.049 sec/batch; 1h:50m:26s remains)
INFO - root - 2019-11-06 20:24:27.361055: step 15960, total loss = 0.34, predict loss = 0.09 (77.5 examples/sec; 0.052 sec/batch; 1h:55m:18s remains)
INFO - root - 2019-11-06 20:24:27.956968: step 15970, total loss = 0.35, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 1h:54m:17s remains)
INFO - root - 2019-11-06 20:24:28.479186: step 15980, total loss = 0.57, predict loss = 0.15 (97.1 examples/sec; 0.041 sec/batch; 1h:31m:58s remains)
INFO - root - 2019-11-06 20:24:28.926923: step 15990, total loss = 0.32, predict loss = 0.08 (92.9 examples/sec; 0.043 sec/batch; 1h:36m:12s remains)
INFO - root - 2019-11-06 20:24:29.379637: step 16000, total loss = 0.27, predict loss = 0.07 (92.6 examples/sec; 0.043 sec/batch; 1h:36m:30s remains)
INFO - root - 2019-11-06 20:24:30.412728: step 16010, total loss = 0.32, predict loss = 0.08 (66.4 examples/sec; 0.060 sec/batch; 2h:14m:26s remains)
INFO - root - 2019-11-06 20:24:31.015837: step 16020, total loss = 0.27, predict loss = 0.07 (78.3 examples/sec; 0.051 sec/batch; 1h:54m:02s remains)
INFO - root - 2019-11-06 20:24:31.584799: step 16030, total loss = 0.76, predict loss = 0.21 (74.5 examples/sec; 0.054 sec/batch; 1h:59m:54s remains)
INFO - root - 2019-11-06 20:24:32.160888: step 16040, total loss = 0.32, predict loss = 0.09 (77.9 examples/sec; 0.051 sec/batch; 1h:54m:36s remains)
INFO - root - 2019-11-06 20:24:32.760681: step 16050, total loss = 0.35, predict loss = 0.10 (72.5 examples/sec; 0.055 sec/batch; 2h:03m:08s remains)
INFO - root - 2019-11-06 20:24:33.328732: step 16060, total loss = 0.61, predict loss = 0.17 (78.5 examples/sec; 0.051 sec/batch; 1h:53m:44s remains)
INFO - root - 2019-11-06 20:24:33.904351: step 16070, total loss = 0.50, predict loss = 0.14 (78.2 examples/sec; 0.051 sec/batch; 1h:54m:09s remains)
INFO - root - 2019-11-06 20:24:34.466993: step 16080, total loss = 0.19, predict loss = 0.05 (79.4 examples/sec; 0.050 sec/batch; 1h:52m:29s remains)
INFO - root - 2019-11-06 20:24:35.068625: step 16090, total loss = 0.40, predict loss = 0.11 (80.4 examples/sec; 0.050 sec/batch; 1h:50m:59s remains)
INFO - root - 2019-11-06 20:24:35.640159: step 16100, total loss = 0.55, predict loss = 0.18 (77.9 examples/sec; 0.051 sec/batch; 1h:54m:32s remains)
INFO - root - 2019-11-06 20:24:36.216307: step 16110, total loss = 0.35, predict loss = 0.08 (79.3 examples/sec; 0.050 sec/batch; 1h:52m:35s remains)
INFO - root - 2019-11-06 20:24:36.800897: step 16120, total loss = 0.25, predict loss = 0.06 (78.8 examples/sec; 0.051 sec/batch; 1h:53m:13s remains)
INFO - root - 2019-11-06 20:24:37.324917: step 16130, total loss = 0.27, predict loss = 0.07 (99.2 examples/sec; 0.040 sec/batch; 1h:29m:55s remains)
INFO - root - 2019-11-06 20:24:37.787055: step 16140, total loss = 0.43, predict loss = 0.12 (90.9 examples/sec; 0.044 sec/batch; 1h:38m:09s remains)
INFO - root - 2019-11-06 20:24:38.253861: step 16150, total loss = 0.28, predict loss = 0.07 (94.7 examples/sec; 0.042 sec/batch; 1h:34m:12s remains)
INFO - root - 2019-11-06 20:24:39.343427: step 16160, total loss = 0.26, predict loss = 0.08 (61.3 examples/sec; 0.065 sec/batch; 2h:25m:36s remains)
INFO - root - 2019-11-06 20:24:39.982938: step 16170, total loss = 0.27, predict loss = 0.07 (75.9 examples/sec; 0.053 sec/batch; 1h:57m:35s remains)
INFO - root - 2019-11-06 20:24:40.565448: step 16180, total loss = 0.18, predict loss = 0.04 (80.7 examples/sec; 0.050 sec/batch; 1h:50m:34s remains)
INFO - root - 2019-11-06 20:24:41.149906: step 16190, total loss = 0.29, predict loss = 0.07 (78.8 examples/sec; 0.051 sec/batch; 1h:53m:11s remains)
INFO - root - 2019-11-06 20:24:41.714090: step 16200, total loss = 0.26, predict loss = 0.07 (83.5 examples/sec; 0.048 sec/batch; 1h:46m:48s remains)
INFO - root - 2019-11-06 20:24:42.305473: step 16210, total loss = 0.49, predict loss = 0.13 (78.5 examples/sec; 0.051 sec/batch; 1h:53m:40s remains)
INFO - root - 2019-11-06 20:24:42.882898: step 16220, total loss = 0.31, predict loss = 0.08 (76.4 examples/sec; 0.052 sec/batch; 1h:56m:43s remains)
INFO - root - 2019-11-06 20:24:43.455779: step 16230, total loss = 0.32, predict loss = 0.07 (81.3 examples/sec; 0.049 sec/batch; 1h:49m:44s remains)
INFO - root - 2019-11-06 20:24:44.024333: step 16240, total loss = 0.25, predict loss = 0.06 (77.9 examples/sec; 0.051 sec/batch; 1h:54m:30s remains)
INFO - root - 2019-11-06 20:24:44.615069: step 16250, total loss = 0.22, predict loss = 0.06 (76.4 examples/sec; 0.052 sec/batch; 1h:56m:39s remains)
INFO - root - 2019-11-06 20:24:45.231099: step 16260, total loss = 0.26, predict loss = 0.07 (69.9 examples/sec; 0.057 sec/batch; 2h:07m:32s remains)
INFO - root - 2019-11-06 20:24:45.795531: step 16270, total loss = 0.19, predict loss = 0.05 (81.2 examples/sec; 0.049 sec/batch; 1h:49m:47s remains)
INFO - root - 2019-11-06 20:24:46.272315: step 16280, total loss = 0.35, predict loss = 0.10 (100.1 examples/sec; 0.040 sec/batch; 1h:29m:05s remains)
INFO - root - 2019-11-06 20:24:46.755094: step 16290, total loss = 0.28, predict loss = 0.08 (95.0 examples/sec; 0.042 sec/batch; 1h:33m:51s remains)
INFO - root - 2019-11-06 20:24:47.656229: step 16300, total loss = 0.40, predict loss = 0.12 (8.2 examples/sec; 0.487 sec/batch; 18h:05m:12s remains)
INFO - root - 2019-11-06 20:24:48.395329: step 16310, total loss = 0.25, predict loss = 0.05 (53.9 examples/sec; 0.074 sec/batch; 2h:45m:24s remains)
INFO - root - 2019-11-06 20:24:49.024273: step 16320, total loss = 0.34, predict loss = 0.08 (80.0 examples/sec; 0.050 sec/batch; 1h:51m:19s remains)
INFO - root - 2019-11-06 20:24:49.624712: step 16330, total loss = 0.33, predict loss = 0.09 (80.0 examples/sec; 0.050 sec/batch; 1h:51m:20s remains)
INFO - root - 2019-11-06 20:24:50.195713: step 16340, total loss = 0.24, predict loss = 0.05 (77.3 examples/sec; 0.052 sec/batch; 1h:55m:19s remains)
INFO - root - 2019-11-06 20:24:50.767895: step 16350, total loss = 0.41, predict loss = 0.13 (80.5 examples/sec; 0.050 sec/batch; 1h:50m:37s remains)
INFO - root - 2019-11-06 20:24:51.344387: step 16360, total loss = 0.44, predict loss = 0.11 (81.0 examples/sec; 0.049 sec/batch; 1h:50m:02s remains)
INFO - root - 2019-11-06 20:24:51.931008: step 16370, total loss = 0.33, predict loss = 0.08 (81.4 examples/sec; 0.049 sec/batch; 1h:49m:28s remains)
INFO - root - 2019-11-06 20:24:52.508758: step 16380, total loss = 0.45, predict loss = 0.12 (80.2 examples/sec; 0.050 sec/batch; 1h:51m:07s remains)
INFO - root - 2019-11-06 20:24:53.081569: step 16390, total loss = 0.31, predict loss = 0.08 (80.6 examples/sec; 0.050 sec/batch; 1h:50m:34s remains)
INFO - root - 2019-11-06 20:24:53.655284: step 16400, total loss = 0.19, predict loss = 0.05 (81.2 examples/sec; 0.049 sec/batch; 1h:49m:41s remains)
INFO - root - 2019-11-06 20:24:54.245915: step 16410, total loss = 0.44, predict loss = 0.13 (75.2 examples/sec; 0.053 sec/batch; 1h:58m:26s remains)
INFO - root - 2019-11-06 20:24:54.820250: step 16420, total loss = 0.40, predict loss = 0.11 (85.9 examples/sec; 0.047 sec/batch; 1h:43m:40s remains)
INFO - root - 2019-11-06 20:24:55.277426: step 16430, total loss = 0.25, predict loss = 0.06 (102.5 examples/sec; 0.039 sec/batch; 1h:26m:51s remains)
INFO - root - 2019-11-06 20:24:55.719368: step 16440, total loss = 0.33, predict loss = 0.09 (95.1 examples/sec; 0.042 sec/batch; 1h:33m:40s remains)
INFO - root - 2019-11-06 20:24:56.660331: step 16450, total loss = 0.29, predict loss = 0.07 (80.2 examples/sec; 0.050 sec/batch; 1h:50m:59s remains)
INFO - root - 2019-11-06 20:24:57.339021: step 16460, total loss = 0.28, predict loss = 0.07 (66.1 examples/sec; 0.061 sec/batch; 2h:14m:44s remains)
INFO - root - 2019-11-06 20:24:58.010496: step 16470, total loss = 0.47, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 2h:08m:41s remains)
INFO - root - 2019-11-06 20:24:58.611716: step 16480, total loss = 0.27, predict loss = 0.07 (76.4 examples/sec; 0.052 sec/batch; 1h:56m:26s remains)
INFO - root - 2019-11-06 20:24:59.192301: step 16490, total loss = 0.51, predict loss = 0.14 (78.7 examples/sec; 0.051 sec/batch; 1h:53m:05s remains)
INFO - root - 2019-11-06 20:24:59.762831: step 16500, total loss = 0.63, predict loss = 0.16 (77.5 examples/sec; 0.052 sec/batch; 1h:54m:49s remains)
INFO - root - 2019-11-06 20:25:00.339984: step 16510, total loss = 0.56, predict loss = 0.16 (79.0 examples/sec; 0.051 sec/batch; 1h:52m:42s remains)
INFO - root - 2019-11-06 20:25:00.918331: step 16520, total loss = 0.42, predict loss = 0.11 (79.5 examples/sec; 0.050 sec/batch; 1h:51m:53s remains)
INFO - root - 2019-11-06 20:25:01.506974: step 16530, total loss = 0.35, predict loss = 0.10 (78.0 examples/sec; 0.051 sec/batch; 1h:54m:07s remains)
INFO - root - 2019-11-06 20:25:02.081874: step 16540, total loss = 0.18, predict loss = 0.04 (77.5 examples/sec; 0.052 sec/batch; 1h:54m:51s remains)
INFO - root - 2019-11-06 20:25:02.652257: step 16550, total loss = 0.37, predict loss = 0.09 (73.0 examples/sec; 0.055 sec/batch; 2h:01m:54s remains)
INFO - root - 2019-11-06 20:25:03.226170: step 16560, total loss = 0.58, predict loss = 0.17 (78.5 examples/sec; 0.051 sec/batch; 1h:53m:18s remains)
INFO - root - 2019-11-06 20:25:03.801491: step 16570, total loss = 0.44, predict loss = 0.12 (94.2 examples/sec; 0.042 sec/batch; 1h:34m:27s remains)
INFO - root - 2019-11-06 20:25:04.264921: step 16580, total loss = 0.39, predict loss = 0.10 (94.0 examples/sec; 0.043 sec/batch; 1h:34m:36s remains)
INFO - root - 2019-11-06 20:25:04.716391: step 16590, total loss = 0.47, predict loss = 0.12 (96.4 examples/sec; 0.041 sec/batch; 1h:32m:14s remains)
INFO - root - 2019-11-06 20:25:05.672067: step 16600, total loss = 0.39, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 2h:09m:43s remains)
INFO - root - 2019-11-06 20:25:06.345015: step 16610, total loss = 0.23, predict loss = 0.05 (79.5 examples/sec; 0.050 sec/batch; 1h:51m:48s remains)
INFO - root - 2019-11-06 20:25:06.918097: step 16620, total loss = 0.35, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:52m:27s remains)
INFO - root - 2019-11-06 20:25:07.495295: step 16630, total loss = 0.31, predict loss = 0.07 (81.7 examples/sec; 0.049 sec/batch; 1h:48m:50s remains)
INFO - root - 2019-11-06 20:25:08.075800: step 16640, total loss = 0.41, predict loss = 0.10 (79.3 examples/sec; 0.050 sec/batch; 1h:52m:10s remains)
INFO - root - 2019-11-06 20:25:08.680263: step 16650, total loss = 0.52, predict loss = 0.14 (75.5 examples/sec; 0.053 sec/batch; 1h:57m:45s remains)
INFO - root - 2019-11-06 20:25:09.258191: step 16660, total loss = 0.20, predict loss = 0.05 (77.7 examples/sec; 0.051 sec/batch; 1h:54m:23s remains)
INFO - root - 2019-11-06 20:25:09.838062: step 16670, total loss = 0.34, predict loss = 0.09 (78.3 examples/sec; 0.051 sec/batch; 1h:53m:30s remains)
INFO - root - 2019-11-06 20:25:10.404023: step 16680, total loss = 0.28, predict loss = 0.07 (78.7 examples/sec; 0.051 sec/batch; 1h:52m:58s remains)
INFO - root - 2019-11-06 20:25:10.984329: step 16690, total loss = 0.25, predict loss = 0.08 (77.5 examples/sec; 0.052 sec/batch; 1h:54m:43s remains)
INFO - root - 2019-11-06 20:25:11.546894: step 16700, total loss = 0.24, predict loss = 0.05 (82.1 examples/sec; 0.049 sec/batch; 1h:48m:16s remains)
INFO - root - 2019-11-06 20:25:12.121961: step 16710, total loss = 0.41, predict loss = 0.11 (82.5 examples/sec; 0.048 sec/batch; 1h:47m:39s remains)
INFO - root - 2019-11-06 20:25:12.656147: step 16720, total loss = 0.46, predict loss = 0.14 (94.3 examples/sec; 0.042 sec/batch; 1h:34m:16s remains)
INFO - root - 2019-11-06 20:25:13.126071: step 16730, total loss = 0.45, predict loss = 0.10 (94.8 examples/sec; 0.042 sec/batch; 1h:33m:44s remains)
INFO - root - 2019-11-06 20:25:13.576030: step 16740, total loss = 0.56, predict loss = 0.14 (101.5 examples/sec; 0.039 sec/batch; 1h:27m:32s remains)
INFO - root - 2019-11-06 20:25:14.545237: step 16750, total loss = 0.49, predict loss = 0.13 (64.2 examples/sec; 0.062 sec/batch; 2h:18m:23s remains)
INFO - root - 2019-11-06 20:25:15.224036: step 16760, total loss = 0.25, predict loss = 0.05 (53.6 examples/sec; 0.075 sec/batch; 2h:45m:37s remains)
INFO - root - 2019-11-06 20:25:15.821618: step 16770, total loss = 0.23, predict loss = 0.06 (76.0 examples/sec; 0.053 sec/batch; 1h:56m:55s remains)
INFO - root - 2019-11-06 20:25:16.393292: step 16780, total loss = 0.23, predict loss = 0.07 (78.2 examples/sec; 0.051 sec/batch; 1h:53m:30s remains)
INFO - root - 2019-11-06 20:25:16.965170: step 16790, total loss = 0.30, predict loss = 0.08 (77.0 examples/sec; 0.052 sec/batch; 1h:55m:16s remains)
INFO - root - 2019-11-06 20:25:17.539170: step 16800, total loss = 0.26, predict loss = 0.08 (76.2 examples/sec; 0.053 sec/batch; 1h:56m:34s remains)
INFO - root - 2019-11-06 20:25:18.123687: step 16810, total loss = 0.27, predict loss = 0.06 (82.3 examples/sec; 0.049 sec/batch; 1h:47m:55s remains)
INFO - root - 2019-11-06 20:25:18.683885: step 16820, total loss = 0.50, predict loss = 0.14 (80.7 examples/sec; 0.050 sec/batch; 1h:50m:02s remains)
INFO - root - 2019-11-06 20:25:19.261779: step 16830, total loss = 0.41, predict loss = 0.12 (81.0 examples/sec; 0.049 sec/batch; 1h:49m:34s remains)
INFO - root - 2019-11-06 20:25:19.838900: step 16840, total loss = 0.25, predict loss = 0.06 (81.3 examples/sec; 0.049 sec/batch; 1h:49m:08s remains)
INFO - root - 2019-11-06 20:25:20.421216: step 16850, total loss = 0.38, predict loss = 0.09 (77.3 examples/sec; 0.052 sec/batch; 1h:54m:52s remains)
INFO - root - 2019-11-06 20:25:20.990396: step 16860, total loss = 0.33, predict loss = 0.09 (77.9 examples/sec; 0.051 sec/batch; 1h:54m:00s remains)
INFO - root - 2019-11-06 20:25:21.500329: step 16870, total loss = 0.47, predict loss = 0.13 (102.7 examples/sec; 0.039 sec/batch; 1h:26m:27s remains)
INFO - root - 2019-11-06 20:25:21.955948: step 16880, total loss = 0.24, predict loss = 0.05 (97.3 examples/sec; 0.041 sec/batch; 1h:31m:14s remains)
INFO - root - 2019-11-06 20:25:22.438513: step 16890, total loss = 0.33, predict loss = 0.10 (90.4 examples/sec; 0.044 sec/batch; 1h:38m:09s remains)
INFO - root - 2019-11-06 20:25:23.465021: step 16900, total loss = 0.30, predict loss = 0.05 (61.6 examples/sec; 0.065 sec/batch; 2h:24m:03s remains)
INFO - root - 2019-11-06 20:25:24.091470: step 16910, total loss = 0.23, predict loss = 0.06 (78.4 examples/sec; 0.051 sec/batch; 1h:53m:10s remains)
INFO - root - 2019-11-06 20:25:24.663724: step 16920, total loss = 0.50, predict loss = 0.12 (82.6 examples/sec; 0.048 sec/batch; 1h:47m:27s remains)
INFO - root - 2019-11-06 20:25:25.251003: step 16930, total loss = 0.84, predict loss = 0.27 (81.1 examples/sec; 0.049 sec/batch; 1h:49m:24s remains)
INFO - root - 2019-11-06 20:25:25.824592: step 16940, total loss = 0.27, predict loss = 0.07 (77.2 examples/sec; 0.052 sec/batch; 1h:54m:53s remains)
INFO - root - 2019-11-06 20:25:26.392507: step 16950, total loss = 0.36, predict loss = 0.13 (81.0 examples/sec; 0.049 sec/batch; 1h:49m:29s remains)
INFO - root - 2019-11-06 20:25:26.962233: step 16960, total loss = 0.35, predict loss = 0.08 (78.9 examples/sec; 0.051 sec/batch; 1h:52m:25s remains)
INFO - root - 2019-11-06 20:25:27.555685: step 16970, total loss = 0.31, predict loss = 0.08 (77.3 examples/sec; 0.052 sec/batch; 1h:54m:45s remains)
INFO - root - 2019-11-06 20:25:28.129818: step 16980, total loss = 0.27, predict loss = 0.06 (80.2 examples/sec; 0.050 sec/batch; 1h:50m:32s remains)
INFO - root - 2019-11-06 20:25:28.701275: step 16990, total loss = 0.35, predict loss = 0.09 (74.4 examples/sec; 0.054 sec/batch; 1h:59m:09s remains)
INFO - root - 2019-11-06 20:25:29.279187: step 17000, total loss = 0.43, predict loss = 0.10 (77.2 examples/sec; 0.052 sec/batch; 1h:54m:48s remains)
INFO - root - 2019-11-06 20:25:29.866606: step 17010, total loss = 0.21, predict loss = 0.05 (78.4 examples/sec; 0.051 sec/batch; 1h:53m:06s remains)
INFO - root - 2019-11-06 20:25:30.365412: step 17020, total loss = 0.34, predict loss = 0.09 (101.9 examples/sec; 0.039 sec/batch; 1h:27m:02s remains)
INFO - root - 2019-11-06 20:25:30.814177: step 17030, total loss = 0.44, predict loss = 0.11 (100.1 examples/sec; 0.040 sec/batch; 1h:28m:34s remains)
INFO - root - 2019-11-06 20:25:31.281281: step 17040, total loss = 0.22, predict loss = 0.06 (94.1 examples/sec; 0.042 sec/batch; 1h:34m:08s remains)
INFO - root - 2019-11-06 20:25:32.364241: step 17050, total loss = 0.21, predict loss = 0.05 (66.6 examples/sec; 0.060 sec/batch; 2h:13m:03s remains)
INFO - root - 2019-11-06 20:25:32.960495: step 17060, total loss = 0.28, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:51m:15s remains)
INFO - root - 2019-11-06 20:25:33.530993: step 17070, total loss = 0.32, predict loss = 0.08 (77.9 examples/sec; 0.051 sec/batch; 1h:53m:46s remains)
INFO - root - 2019-11-06 20:25:34.109943: step 17080, total loss = 0.23, predict loss = 0.05 (78.2 examples/sec; 0.051 sec/batch; 1h:53m:22s remains)
INFO - root - 2019-11-06 20:25:34.702909: step 17090, total loss = 0.45, predict loss = 0.12 (77.8 examples/sec; 0.051 sec/batch; 1h:53m:56s remains)
INFO - root - 2019-11-06 20:25:35.273526: step 17100, total loss = 0.48, predict loss = 0.14 (78.6 examples/sec; 0.051 sec/batch; 1h:52m:41s remains)
INFO - root - 2019-11-06 20:25:35.835312: step 17110, total loss = 0.56, predict loss = 0.14 (81.1 examples/sec; 0.049 sec/batch; 1h:49m:11s remains)
INFO - root - 2019-11-06 20:25:36.397228: step 17120, total loss = 0.39, predict loss = 0.10 (75.5 examples/sec; 0.053 sec/batch; 1h:57m:21s remains)
INFO - root - 2019-11-06 20:25:36.982051: step 17130, total loss = 0.38, predict loss = 0.08 (82.1 examples/sec; 0.049 sec/batch; 1h:47m:56s remains)
INFO - root - 2019-11-06 20:25:37.553894: step 17140, total loss = 0.35, predict loss = 0.09 (79.3 examples/sec; 0.050 sec/batch; 1h:51m:39s remains)
INFO - root - 2019-11-06 20:25:38.121466: step 17150, total loss = 0.52, predict loss = 0.13 (79.4 examples/sec; 0.050 sec/batch; 1h:51m:36s remains)
INFO - root - 2019-11-06 20:25:38.696641: step 17160, total loss = 0.29, predict loss = 0.07 (75.8 examples/sec; 0.053 sec/batch; 1h:56m:46s remains)
INFO - root - 2019-11-06 20:25:39.194999: step 17170, total loss = 0.41, predict loss = 0.10 (92.4 examples/sec; 0.043 sec/batch; 1h:35m:50s remains)
INFO - root - 2019-11-06 20:25:39.647551: step 17180, total loss = 0.22, predict loss = 0.06 (95.2 examples/sec; 0.042 sec/batch; 1h:33m:03s remains)
INFO - root - 2019-11-06 20:25:40.554505: step 17190, total loss = 0.39, predict loss = 0.10 (79.3 examples/sec; 0.050 sec/batch; 1h:51m:43s remains)
INFO - root - 2019-11-06 20:25:41.247398: step 17200, total loss = 0.28, predict loss = 0.06 (59.7 examples/sec; 0.067 sec/batch; 2h:28m:14s remains)
INFO - root - 2019-11-06 20:25:41.884552: step 17210, total loss = 0.45, predict loss = 0.17 (79.5 examples/sec; 0.050 sec/batch; 1h:51m:17s remains)
INFO - root - 2019-11-06 20:25:42.462461: step 17220, total loss = 0.28, predict loss = 0.07 (76.6 examples/sec; 0.052 sec/batch; 1h:55m:30s remains)
INFO - root - 2019-11-06 20:25:43.035336: step 17230, total loss = 0.52, predict loss = 0.14 (74.1 examples/sec; 0.054 sec/batch; 1h:59m:30s remains)
INFO - root - 2019-11-06 20:25:43.608058: step 17240, total loss = 0.32, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:51m:54s remains)
INFO - root - 2019-11-06 20:25:44.188603: step 17250, total loss = 0.37, predict loss = 0.10 (81.2 examples/sec; 0.049 sec/batch; 1h:48m:57s remains)
INFO - root - 2019-11-06 20:25:44.762039: step 17260, total loss = 0.32, predict loss = 0.08 (79.0 examples/sec; 0.051 sec/batch; 1h:51m:57s remains)
INFO - root - 2019-11-06 20:25:45.385716: step 17270, total loss = 0.41, predict loss = 0.10 (77.3 examples/sec; 0.052 sec/batch; 1h:54m:27s remains)
INFO - root - 2019-11-06 20:25:45.960387: step 17280, total loss = 0.21, predict loss = 0.04 (79.2 examples/sec; 0.051 sec/batch; 1h:51m:46s remains)
INFO - root - 2019-11-06 20:25:46.549269: step 17290, total loss = 0.30, predict loss = 0.08 (79.4 examples/sec; 0.050 sec/batch; 1h:51m:24s remains)
INFO - root - 2019-11-06 20:25:47.108240: step 17300, total loss = 0.22, predict loss = 0.05 (76.0 examples/sec; 0.053 sec/batch; 1h:56m:20s remains)
INFO - root - 2019-11-06 20:25:47.673698: step 17310, total loss = 0.38, predict loss = 0.12 (86.2 examples/sec; 0.046 sec/batch; 1h:42m:36s remains)
INFO - root - 2019-11-06 20:25:48.125786: step 17320, total loss = 0.48, predict loss = 0.13 (97.6 examples/sec; 0.041 sec/batch; 1h:30m:36s remains)
INFO - root - 2019-11-06 20:25:48.609511: step 17330, total loss = 0.36, predict loss = 0.10 (94.1 examples/sec; 0.042 sec/batch; 1h:33m:58s remains)
INFO - root - 2019-11-06 20:25:49.565101: step 17340, total loss = 0.39, predict loss = 0.11 (77.7 examples/sec; 0.051 sec/batch; 1h:53m:45s remains)
INFO - root - 2019-11-06 20:25:50.232036: step 17350, total loss = 0.43, predict loss = 0.10 (67.0 examples/sec; 0.060 sec/batch; 2h:12m:01s remains)
INFO - root - 2019-11-06 20:25:50.833287: step 17360, total loss = 0.24, predict loss = 0.05 (81.8 examples/sec; 0.049 sec/batch; 1h:48m:04s remains)
INFO - root - 2019-11-06 20:25:51.425129: step 17370, total loss = 0.37, predict loss = 0.09 (74.9 examples/sec; 0.053 sec/batch; 1h:58m:01s remains)
INFO - root - 2019-11-06 20:25:51.991967: step 17380, total loss = 0.35, predict loss = 0.09 (79.9 examples/sec; 0.050 sec/batch; 1h:50m:37s remains)
INFO - root - 2019-11-06 20:25:52.557845: step 17390, total loss = 0.34, predict loss = 0.09 (82.8 examples/sec; 0.048 sec/batch; 1h:46m:45s remains)
INFO - root - 2019-11-06 20:25:53.135955: step 17400, total loss = 0.39, predict loss = 0.10 (74.7 examples/sec; 0.054 sec/batch; 1h:58m:24s remains)
INFO - root - 2019-11-06 20:25:53.719967: step 17410, total loss = 0.44, predict loss = 0.10 (79.1 examples/sec; 0.051 sec/batch; 1h:51m:41s remains)
INFO - root - 2019-11-06 20:25:54.290872: step 17420, total loss = 0.49, predict loss = 0.14 (80.2 examples/sec; 0.050 sec/batch; 1h:50m:15s remains)
INFO - root - 2019-11-06 20:25:54.855505: step 17430, total loss = 0.30, predict loss = 0.09 (76.9 examples/sec; 0.052 sec/batch; 1h:54m:53s remains)
INFO - root - 2019-11-06 20:25:55.429503: step 17440, total loss = 0.21, predict loss = 0.05 (81.7 examples/sec; 0.049 sec/batch; 1h:48m:12s remains)
INFO - root - 2019-11-06 20:25:56.011551: step 17450, total loss = 0.40, predict loss = 0.11 (75.2 examples/sec; 0.053 sec/batch; 1h:57m:34s remains)
INFO - root - 2019-11-06 20:25:56.540647: step 17460, total loss = 0.31, predict loss = 0.09 (96.9 examples/sec; 0.041 sec/batch; 1h:31m:09s remains)
INFO - root - 2019-11-06 20:25:57.001773: step 17470, total loss = 0.39, predict loss = 0.11 (97.6 examples/sec; 0.041 sec/batch; 1h:30m:29s remains)
INFO - root - 2019-11-06 20:25:57.450875: step 17480, total loss = 0.31, predict loss = 0.07 (99.4 examples/sec; 0.040 sec/batch; 1h:28m:53s remains)
INFO - root - 2019-11-06 20:25:58.446086: step 17490, total loss = 0.60, predict loss = 0.17 (56.5 examples/sec; 0.071 sec/batch; 2h:36m:21s remains)
INFO - root - 2019-11-06 20:25:59.092231: step 17500, total loss = 0.28, predict loss = 0.06 (81.1 examples/sec; 0.049 sec/batch; 1h:48m:52s remains)
INFO - root - 2019-11-06 20:25:59.664437: step 17510, total loss = 0.40, predict loss = 0.12 (79.6 examples/sec; 0.050 sec/batch; 1h:50m:57s remains)
INFO - root - 2019-11-06 20:26:00.243367: step 17520, total loss = 0.31, predict loss = 0.08 (79.4 examples/sec; 0.050 sec/batch; 1h:51m:13s remains)
INFO - root - 2019-11-06 20:26:00.839588: step 17530, total loss = 0.31, predict loss = 0.08 (79.8 examples/sec; 0.050 sec/batch; 1h:50m:40s remains)
INFO - root - 2019-11-06 20:26:01.393606: step 17540, total loss = 0.27, predict loss = 0.08 (83.1 examples/sec; 0.048 sec/batch; 1h:46m:12s remains)
INFO - root - 2019-11-06 20:26:01.971590: step 17550, total loss = 0.35, predict loss = 0.10 (79.9 examples/sec; 0.050 sec/batch; 1h:50m:27s remains)
INFO - root - 2019-11-06 20:26:02.540001: step 17560, total loss = 0.25, predict loss = 0.07 (79.6 examples/sec; 0.050 sec/batch; 1h:50m:58s remains)
INFO - root - 2019-11-06 20:26:03.127745: step 17570, total loss = 0.36, predict loss = 0.09 (79.1 examples/sec; 0.051 sec/batch; 1h:51m:40s remains)
INFO - root - 2019-11-06 20:26:03.703675: step 17580, total loss = 0.41, predict loss = 0.10 (75.2 examples/sec; 0.053 sec/batch; 1h:57m:25s remains)
INFO - root - 2019-11-06 20:26:04.280233: step 17590, total loss = 0.31, predict loss = 0.09 (80.9 examples/sec; 0.049 sec/batch; 1h:49m:06s remains)
INFO - root - 2019-11-06 20:26:04.849055: step 17600, total loss = 0.26, predict loss = 0.06 (81.1 examples/sec; 0.049 sec/batch; 1h:48m:54s remains)
INFO - root - 2019-11-06 20:26:05.388217: step 17610, total loss = 0.76, predict loss = 0.24 (94.9 examples/sec; 0.042 sec/batch; 1h:33m:00s remains)
INFO - root - 2019-11-06 20:26:05.849516: step 17620, total loss = 0.29, predict loss = 0.08 (89.9 examples/sec; 0.044 sec/batch; 1h:38m:07s remains)
INFO - root - 2019-11-06 20:26:06.301946: step 17630, total loss = 0.21, predict loss = 0.05 (102.1 examples/sec; 0.039 sec/batch; 1h:26m:23s remains)
INFO - root - 2019-11-06 20:26:07.373171: step 17640, total loss = 0.27, predict loss = 0.06 (51.0 examples/sec; 0.078 sec/batch; 2h:52m:51s remains)
INFO - root - 2019-11-06 20:26:08.087753: step 17650, total loss = 0.36, predict loss = 0.10 (62.5 examples/sec; 0.064 sec/batch; 2h:21m:12s remains)
INFO - root - 2019-11-06 20:26:08.695854: step 17660, total loss = 0.39, predict loss = 0.12 (80.6 examples/sec; 0.050 sec/batch; 1h:49m:26s remains)
INFO - root - 2019-11-06 20:26:09.259840: step 17670, total loss = 0.48, predict loss = 0.12 (78.7 examples/sec; 0.051 sec/batch; 1h:52m:08s remains)
INFO - root - 2019-11-06 20:26:09.837201: step 17680, total loss = 0.26, predict loss = 0.07 (78.5 examples/sec; 0.051 sec/batch; 1h:52m:25s remains)
INFO - root - 2019-11-06 20:26:10.423827: step 17690, total loss = 0.32, predict loss = 0.09 (82.7 examples/sec; 0.048 sec/batch; 1h:46m:38s remains)
INFO - root - 2019-11-06 20:26:11.000489: step 17700, total loss = 0.39, predict loss = 0.13 (75.7 examples/sec; 0.053 sec/batch; 1h:56m:30s remains)
INFO - root - 2019-11-06 20:26:11.573302: step 17710, total loss = 0.26, predict loss = 0.07 (81.0 examples/sec; 0.049 sec/batch; 1h:48m:50s remains)
INFO - root - 2019-11-06 20:26:12.154120: step 17720, total loss = 0.32, predict loss = 0.07 (82.8 examples/sec; 0.048 sec/batch; 1h:46m:29s remains)
INFO - root - 2019-11-06 20:26:12.749445: step 17730, total loss = 0.36, predict loss = 0.08 (76.6 examples/sec; 0.052 sec/batch; 1h:55m:09s remains)
INFO - root - 2019-11-06 20:26:13.336510: step 17740, total loss = 0.36, predict loss = 0.09 (77.0 examples/sec; 0.052 sec/batch; 1h:54m:28s remains)
INFO - root - 2019-11-06 20:26:13.903770: step 17750, total loss = 0.32, predict loss = 0.09 (77.5 examples/sec; 0.052 sec/batch; 1h:53m:47s remains)
INFO - root - 2019-11-06 20:26:14.406673: step 17760, total loss = 0.22, predict loss = 0.06 (104.5 examples/sec; 0.038 sec/batch; 1h:24m:22s remains)
INFO - root - 2019-11-06 20:26:14.886864: step 17770, total loss = 0.23, predict loss = 0.05 (96.5 examples/sec; 0.041 sec/batch; 1h:31m:20s remains)
INFO - root - 2019-11-06 20:26:15.348729: step 17780, total loss = 0.35, predict loss = 0.08 (101.2 examples/sec; 0.040 sec/batch; 1h:27m:05s remains)
INFO - root - 2019-11-06 20:26:16.435331: step 17790, total loss = 0.47, predict loss = 0.12 (59.8 examples/sec; 0.067 sec/batch; 2h:27m:19s remains)
INFO - root - 2019-11-06 20:26:17.059461: step 17800, total loss = 0.32, predict loss = 0.11 (84.1 examples/sec; 0.048 sec/batch; 1h:44m:49s remains)
INFO - root - 2019-11-06 20:26:17.650018: step 17810, total loss = 0.36, predict loss = 0.10 (80.5 examples/sec; 0.050 sec/batch; 1h:49m:32s remains)
INFO - root - 2019-11-06 20:26:18.228747: step 17820, total loss = 0.27, predict loss = 0.07 (73.5 examples/sec; 0.054 sec/batch; 1h:59m:57s remains)
INFO - root - 2019-11-06 20:26:18.796072: step 17830, total loss = 0.50, predict loss = 0.13 (75.8 examples/sec; 0.053 sec/batch; 1h:56m:10s remains)
INFO - root - 2019-11-06 20:26:19.369864: step 17840, total loss = 0.32, predict loss = 0.08 (74.8 examples/sec; 0.053 sec/batch; 1h:57m:47s remains)
INFO - root - 2019-11-06 20:26:19.964847: step 17850, total loss = 0.31, predict loss = 0.07 (74.2 examples/sec; 0.054 sec/batch; 1h:58m:39s remains)
INFO - root - 2019-11-06 20:26:20.543496: step 17860, total loss = 0.43, predict loss = 0.13 (79.7 examples/sec; 0.050 sec/batch; 1h:50m:31s remains)
INFO - root - 2019-11-06 20:26:21.120806: step 17870, total loss = 0.46, predict loss = 0.15 (75.9 examples/sec; 0.053 sec/batch; 1h:56m:01s remains)
INFO - root - 2019-11-06 20:26:21.686078: step 17880, total loss = 0.34, predict loss = 0.09 (79.4 examples/sec; 0.050 sec/batch; 1h:51m:00s remains)
INFO - root - 2019-11-06 20:26:22.277709: step 17890, total loss = 0.30, predict loss = 0.08 (82.8 examples/sec; 0.048 sec/batch; 1h:46m:22s remains)
INFO - root - 2019-11-06 20:26:22.834923: step 17900, total loss = 0.19, predict loss = 0.06 (73.2 examples/sec; 0.055 sec/batch; 2h:00m:16s remains)
INFO - root - 2019-11-06 20:26:23.322534: step 17910, total loss = 0.39, predict loss = 0.10 (96.6 examples/sec; 0.041 sec/batch; 1h:31m:06s remains)
INFO - root - 2019-11-06 20:26:23.779106: step 17920, total loss = 0.45, predict loss = 0.13 (94.0 examples/sec; 0.043 sec/batch; 1h:33m:38s remains)
INFO - root - 2019-11-06 20:26:24.692799: step 17930, total loss = 0.25, predict loss = 0.06 (8.4 examples/sec; 0.475 sec/batch; 17h:26m:36s remains)
INFO - root - 2019-11-06 20:26:25.339399: step 17940, total loss = 0.47, predict loss = 0.12 (64.5 examples/sec; 0.062 sec/batch; 2h:16m:33s remains)
INFO - root - 2019-11-06 20:26:25.978363: step 17950, total loss = 0.35, predict loss = 0.10 (74.6 examples/sec; 0.054 sec/batch; 1h:57m:58s remains)
INFO - root - 2019-11-06 20:26:26.592928: step 17960, total loss = 0.24, predict loss = 0.06 (75.2 examples/sec; 0.053 sec/batch; 1h:57m:02s remains)
INFO - root - 2019-11-06 20:26:27.210508: step 17970, total loss = 0.28, predict loss = 0.07 (80.2 examples/sec; 0.050 sec/batch; 1h:49m:44s remains)
INFO - root - 2019-11-06 20:26:27.794191: step 17980, total loss = 0.36, predict loss = 0.09 (75.3 examples/sec; 0.053 sec/batch; 1h:56m:49s remains)
INFO - root - 2019-11-06 20:26:28.374921: step 17990, total loss = 0.46, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 1h:54m:45s remains)
INFO - root - 2019-11-06 20:26:28.938466: step 18000, total loss = 0.38, predict loss = 0.09 (83.9 examples/sec; 0.048 sec/batch; 1h:44m:55s remains)
INFO - root - 2019-11-06 20:26:29.527992: step 18010, total loss = 0.31, predict loss = 0.09 (80.4 examples/sec; 0.050 sec/batch; 1h:49m:28s remains)
INFO - root - 2019-11-06 20:26:30.103868: step 18020, total loss = 0.38, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 1h:54m:40s remains)
INFO - root - 2019-11-06 20:26:30.668620: step 18030, total loss = 0.27, predict loss = 0.07 (76.7 examples/sec; 0.052 sec/batch; 1h:54m:43s remains)
INFO - root - 2019-11-06 20:26:31.244037: step 18040, total loss = 0.22, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:50m:57s remains)
INFO - root - 2019-11-06 20:26:31.815329: step 18050, total loss = 0.55, predict loss = 0.16 (89.3 examples/sec; 0.045 sec/batch; 1h:38m:33s remains)
INFO - root - 2019-11-06 20:26:32.283116: step 18060, total loss = 0.35, predict loss = 0.09 (98.8 examples/sec; 0.041 sec/batch; 1h:29m:04s remains)
INFO - root - 2019-11-06 20:26:32.743243: step 18070, total loss = 0.26, predict loss = 0.07 (101.4 examples/sec; 0.039 sec/batch; 1h:26m:44s remains)
INFO - root - 2019-11-06 20:26:33.659752: step 18080, total loss = 0.48, predict loss = 0.14 (82.9 examples/sec; 0.048 sec/batch; 1h:46m:03s remains)
INFO - root - 2019-11-06 20:26:34.329929: step 18090, total loss = 0.34, predict loss = 0.08 (64.4 examples/sec; 0.062 sec/batch; 2h:16m:37s remains)
INFO - root - 2019-11-06 20:26:34.974042: step 18100, total loss = 0.33, predict loss = 0.08 (69.0 examples/sec; 0.058 sec/batch; 2h:07m:26s remains)
INFO - root - 2019-11-06 20:26:35.560824: step 18110, total loss = 0.53, predict loss = 0.15 (81.0 examples/sec; 0.049 sec/batch; 1h:48m:36s remains)
INFO - root - 2019-11-06 20:26:36.133665: step 18120, total loss = 0.28, predict loss = 0.09 (78.9 examples/sec; 0.051 sec/batch; 1h:51m:23s remains)
INFO - root - 2019-11-06 20:26:36.719592: step 18130, total loss = 0.25, predict loss = 0.07 (79.1 examples/sec; 0.051 sec/batch; 1h:51m:08s remains)
INFO - root - 2019-11-06 20:26:37.293948: step 18140, total loss = 0.32, predict loss = 0.08 (76.7 examples/sec; 0.052 sec/batch; 1h:54m:36s remains)
INFO - root - 2019-11-06 20:26:37.875613: step 18150, total loss = 0.26, predict loss = 0.06 (79.0 examples/sec; 0.051 sec/batch; 1h:51m:15s remains)
INFO - root - 2019-11-06 20:26:38.448416: step 18160, total loss = 0.27, predict loss = 0.07 (80.4 examples/sec; 0.050 sec/batch; 1h:49m:18s remains)
INFO - root - 2019-11-06 20:26:39.036906: step 18170, total loss = 0.29, predict loss = 0.08 (80.6 examples/sec; 0.050 sec/batch; 1h:49m:05s remains)
INFO - root - 2019-11-06 20:26:39.612600: step 18180, total loss = 0.24, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 1h:51m:45s remains)
INFO - root - 2019-11-06 20:26:40.180083: step 18190, total loss = 0.36, predict loss = 0.09 (77.7 examples/sec; 0.051 sec/batch; 1h:53m:04s remains)
INFO - root - 2019-11-06 20:26:40.730123: step 18200, total loss = 0.26, predict loss = 0.06 (95.2 examples/sec; 0.042 sec/batch; 1h:32m:14s remains)
INFO - root - 2019-11-06 20:26:41.205561: step 18210, total loss = 0.58, predict loss = 0.16 (94.8 examples/sec; 0.042 sec/batch; 1h:32m:43s remains)
INFO - root - 2019-11-06 20:26:41.657918: step 18220, total loss = 0.34, predict loss = 0.10 (98.9 examples/sec; 0.040 sec/batch; 1h:28m:47s remains)
INFO - root - 2019-11-06 20:26:42.629740: step 18230, total loss = 0.45, predict loss = 0.11 (70.2 examples/sec; 0.057 sec/batch; 2h:05m:06s remains)
INFO - root - 2019-11-06 20:26:43.322370: step 18240, total loss = 0.41, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 2h:13m:05s remains)
INFO - root - 2019-11-06 20:26:43.949175: step 18250, total loss = 0.59, predict loss = 0.17 (82.2 examples/sec; 0.049 sec/batch; 1h:46m:54s remains)
INFO - root - 2019-11-06 20:26:44.543431: step 18260, total loss = 0.28, predict loss = 0.08 (77.8 examples/sec; 0.051 sec/batch; 1h:52m:50s remains)
INFO - root - 2019-11-06 20:26:45.154907: step 18270, total loss = 0.39, predict loss = 0.11 (61.4 examples/sec; 0.065 sec/batch; 2h:23m:08s remains)
INFO - root - 2019-11-06 20:26:45.739770: step 18280, total loss = 0.40, predict loss = 0.08 (77.0 examples/sec; 0.052 sec/batch; 1h:54m:03s remains)
INFO - root - 2019-11-06 20:26:46.355159: step 18290, total loss = 0.25, predict loss = 0.06 (75.3 examples/sec; 0.053 sec/batch; 1h:56m:33s remains)
INFO - root - 2019-11-06 20:26:46.933539: step 18300, total loss = 0.29, predict loss = 0.07 (77.0 examples/sec; 0.052 sec/batch; 1h:54m:03s remains)
INFO - root - 2019-11-06 20:26:47.515760: step 18310, total loss = 0.33, predict loss = 0.08 (81.8 examples/sec; 0.049 sec/batch; 1h:47m:17s remains)
INFO - root - 2019-11-06 20:26:48.092748: step 18320, total loss = 0.30, predict loss = 0.06 (77.6 examples/sec; 0.052 sec/batch; 1h:53m:07s remains)
INFO - root - 2019-11-06 20:26:48.686558: step 18330, total loss = 0.47, predict loss = 0.13 (77.5 examples/sec; 0.052 sec/batch; 1h:53m:16s remains)
INFO - root - 2019-11-06 20:26:49.267242: step 18340, total loss = 0.30, predict loss = 0.06 (76.7 examples/sec; 0.052 sec/batch; 1h:54m:25s remains)
INFO - root - 2019-11-06 20:26:49.794530: step 18350, total loss = 0.33, predict loss = 0.07 (96.8 examples/sec; 0.041 sec/batch; 1h:30m:41s remains)
INFO - root - 2019-11-06 20:26:50.265683: step 18360, total loss = 0.31, predict loss = 0.08 (88.2 examples/sec; 0.045 sec/batch; 1h:39m:31s remains)
INFO - root - 2019-11-06 20:26:50.736555: step 18370, total loss = 0.60, predict loss = 0.17 (100.1 examples/sec; 0.040 sec/batch; 1h:27m:42s remains)
INFO - root - 2019-11-06 20:26:51.726470: step 18380, total loss = 0.21, predict loss = 0.05 (61.7 examples/sec; 0.065 sec/batch; 2h:22m:15s remains)
INFO - root - 2019-11-06 20:26:52.403884: step 18390, total loss = 0.24, predict loss = 0.05 (77.3 examples/sec; 0.052 sec/batch; 1h:53m:33s remains)
INFO - root - 2019-11-06 20:26:52.995372: step 18400, total loss = 0.29, predict loss = 0.08 (77.0 examples/sec; 0.052 sec/batch; 1h:53m:52s remains)
INFO - root - 2019-11-06 20:26:53.591853: step 18410, total loss = 0.33, predict loss = 0.09 (77.5 examples/sec; 0.052 sec/batch; 1h:53m:09s remains)
INFO - root - 2019-11-06 20:26:54.166442: step 18420, total loss = 0.35, predict loss = 0.10 (78.1 examples/sec; 0.051 sec/batch; 1h:52m:17s remains)
INFO - root - 2019-11-06 20:26:54.749219: step 18430, total loss = 0.31, predict loss = 0.08 (78.5 examples/sec; 0.051 sec/batch; 1h:51m:47s remains)
INFO - root - 2019-11-06 20:26:55.322336: step 18440, total loss = 0.21, predict loss = 0.05 (77.8 examples/sec; 0.051 sec/batch; 1h:52m:48s remains)
INFO - root - 2019-11-06 20:26:55.910565: step 18450, total loss = 0.30, predict loss = 0.08 (79.2 examples/sec; 0.051 sec/batch; 1h:50m:47s remains)
INFO - root - 2019-11-06 20:26:56.485738: step 18460, total loss = 0.30, predict loss = 0.08 (76.0 examples/sec; 0.053 sec/batch; 1h:55m:20s remains)
INFO - root - 2019-11-06 20:26:57.058459: step 18470, total loss = 0.49, predict loss = 0.15 (74.9 examples/sec; 0.053 sec/batch; 1h:57m:00s remains)
INFO - root - 2019-11-06 20:26:57.634344: step 18480, total loss = 0.44, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 1h:54m:17s remains)
INFO - root - 2019-11-06 20:26:58.233152: step 18490, total loss = 0.29, predict loss = 0.08 (75.8 examples/sec; 0.053 sec/batch; 1h:55m:38s remains)
INFO - root - 2019-11-06 20:26:58.750975: step 18500, total loss = 0.95, predict loss = 0.29 (104.8 examples/sec; 0.038 sec/batch; 1h:23m:40s remains)
INFO - root - 2019-11-06 20:26:59.193480: step 18510, total loss = 0.26, predict loss = 0.06 (97.1 examples/sec; 0.041 sec/batch; 1h:30m:19s remains)
INFO - root - 2019-11-06 20:26:59.654064: step 18520, total loss = 0.24, predict loss = 0.06 (93.1 examples/sec; 0.043 sec/batch; 1h:34m:07s remains)
INFO - root - 2019-11-06 20:27:00.720288: step 18530, total loss = 0.38, predict loss = 0.11 (62.1 examples/sec; 0.064 sec/batch; 2h:21m:11s remains)
INFO - root - 2019-11-06 20:27:01.336961: step 18540, total loss = 0.26, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:53m:28s remains)
INFO - root - 2019-11-06 20:27:01.909147: step 18550, total loss = 0.25, predict loss = 0.07 (76.7 examples/sec; 0.052 sec/batch; 1h:54m:14s remains)
INFO - root - 2019-11-06 20:27:02.494212: step 18560, total loss = 0.39, predict loss = 0.11 (73.8 examples/sec; 0.054 sec/batch; 1h:58m:43s remains)
INFO - root - 2019-11-06 20:27:03.077303: step 18570, total loss = 0.34, predict loss = 0.10 (78.3 examples/sec; 0.051 sec/batch; 1h:51m:57s remains)
INFO - root - 2019-11-06 20:27:03.640856: step 18580, total loss = 0.27, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 1h:53m:07s remains)
INFO - root - 2019-11-06 20:27:04.212050: step 18590, total loss = 0.23, predict loss = 0.06 (77.6 examples/sec; 0.052 sec/batch; 1h:52m:56s remains)
INFO - root - 2019-11-06 20:27:04.785284: step 18600, total loss = 0.41, predict loss = 0.12 (74.1 examples/sec; 0.054 sec/batch; 1h:58m:10s remains)
INFO - root - 2019-11-06 20:27:05.374485: step 18610, total loss = 0.31, predict loss = 0.07 (82.1 examples/sec; 0.049 sec/batch; 1h:46m:37s remains)
INFO - root - 2019-11-06 20:27:05.943340: step 18620, total loss = 0.45, predict loss = 0.13 (78.4 examples/sec; 0.051 sec/batch; 1h:51m:45s remains)
INFO - root - 2019-11-06 20:27:06.512143: step 18630, total loss = 0.26, predict loss = 0.06 (76.5 examples/sec; 0.052 sec/batch; 1h:54m:33s remains)
INFO - root - 2019-11-06 20:27:07.083868: step 18640, total loss = 0.34, predict loss = 0.10 (81.7 examples/sec; 0.049 sec/batch; 1h:47m:14s remains)
INFO - root - 2019-11-06 20:27:07.593256: step 18650, total loss = 0.52, predict loss = 0.15 (98.9 examples/sec; 0.040 sec/batch; 1h:28m:33s remains)
INFO - root - 2019-11-06 20:27:08.041830: step 18660, total loss = 0.31, predict loss = 0.08 (101.4 examples/sec; 0.039 sec/batch; 1h:26m:18s remains)
INFO - root - 2019-11-06 20:27:08.487236: step 18670, total loss = 0.48, predict loss = 0.12 (95.2 examples/sec; 0.042 sec/batch; 1h:32m:00s remains)
INFO - root - 2019-11-06 20:27:09.574069: step 18680, total loss = 0.47, predict loss = 0.10 (61.4 examples/sec; 0.065 sec/batch; 2h:22m:40s remains)
INFO - root - 2019-11-06 20:27:10.222806: step 18690, total loss = 0.41, predict loss = 0.14 (72.9 examples/sec; 0.055 sec/batch; 2h:00m:00s remains)
INFO - root - 2019-11-06 20:27:10.806724: step 18700, total loss = 0.33, predict loss = 0.10 (78.7 examples/sec; 0.051 sec/batch; 1h:51m:13s remains)
INFO - root - 2019-11-06 20:27:11.373571: step 18710, total loss = 0.30, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 1h:51m:43s remains)
INFO - root - 2019-11-06 20:27:11.941756: step 18720, total loss = 0.29, predict loss = 0.07 (78.1 examples/sec; 0.051 sec/batch; 1h:52m:01s remains)
INFO - root - 2019-11-06 20:27:12.527227: step 18730, total loss = 0.63, predict loss = 0.17 (79.4 examples/sec; 0.050 sec/batch; 1h:50m:15s remains)
INFO - root - 2019-11-06 20:27:13.083542: step 18740, total loss = 0.33, predict loss = 0.08 (78.0 examples/sec; 0.051 sec/batch; 1h:52m:15s remains)
INFO - root - 2019-11-06 20:27:13.673046: step 18750, total loss = 0.28, predict loss = 0.08 (76.8 examples/sec; 0.052 sec/batch; 1h:53m:57s remains)
INFO - root - 2019-11-06 20:27:14.244679: step 18760, total loss = 0.30, predict loss = 0.09 (79.0 examples/sec; 0.051 sec/batch; 1h:50m:41s remains)
INFO - root - 2019-11-06 20:27:14.834541: step 18770, total loss = 0.28, predict loss = 0.07 (78.4 examples/sec; 0.051 sec/batch; 1h:51m:36s remains)
INFO - root - 2019-11-06 20:27:15.436334: step 18780, total loss = 0.35, predict loss = 0.10 (76.9 examples/sec; 0.052 sec/batch; 1h:53m:41s remains)
INFO - root - 2019-11-06 20:27:16.004838: step 18790, total loss = 0.41, predict loss = 0.11 (75.6 examples/sec; 0.053 sec/batch; 1h:55m:38s remains)
INFO - root - 2019-11-06 20:27:16.476894: step 18800, total loss = 0.38, predict loss = 0.11 (99.9 examples/sec; 0.040 sec/batch; 1h:27m:35s remains)
INFO - root - 2019-11-06 20:27:16.959124: step 18810, total loss = 0.24, predict loss = 0.06 (95.2 examples/sec; 0.042 sec/batch; 1h:31m:52s remains)
INFO - root - 2019-11-06 20:27:17.854426: step 18820, total loss = 0.45, predict loss = 0.13 (78.0 examples/sec; 0.051 sec/batch; 1h:52m:06s remains)
INFO - root - 2019-11-06 20:27:18.500980: step 18830, total loss = 0.26, predict loss = 0.07 (66.4 examples/sec; 0.060 sec/batch; 2h:11m:38s remains)
INFO - root - 2019-11-06 20:27:19.118592: step 18840, total loss = 0.38, predict loss = 0.09 (78.8 examples/sec; 0.051 sec/batch; 1h:50m:56s remains)
INFO - root - 2019-11-06 20:27:19.701726: step 18850, total loss = 0.23, predict loss = 0.05 (77.8 examples/sec; 0.051 sec/batch; 1h:52m:25s remains)
INFO - root - 2019-11-06 20:27:20.269463: step 18860, total loss = 0.26, predict loss = 0.06 (75.9 examples/sec; 0.053 sec/batch; 1h:55m:14s remains)
INFO - root - 2019-11-06 20:27:20.847938: step 18870, total loss = 0.26, predict loss = 0.07 (76.6 examples/sec; 0.052 sec/batch; 1h:54m:05s remains)
INFO - root - 2019-11-06 20:27:21.426082: step 18880, total loss = 0.43, predict loss = 0.12 (78.6 examples/sec; 0.051 sec/batch; 1h:51m:10s remains)
INFO - root - 2019-11-06 20:27:22.025542: step 18890, total loss = 0.22, predict loss = 0.05 (81.0 examples/sec; 0.049 sec/batch; 1h:47m:53s remains)
INFO - root - 2019-11-06 20:27:22.594455: step 18900, total loss = 0.22, predict loss = 0.05 (79.5 examples/sec; 0.050 sec/batch; 1h:49m:53s remains)
INFO - root - 2019-11-06 20:27:23.169867: step 18910, total loss = 0.23, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:50m:44s remains)
INFO - root - 2019-11-06 20:27:23.733260: step 18920, total loss = 0.42, predict loss = 0.13 (76.0 examples/sec; 0.053 sec/batch; 1h:54m:58s remains)
INFO - root - 2019-11-06 20:27:24.316069: step 18930, total loss = 0.26, predict loss = 0.06 (81.0 examples/sec; 0.049 sec/batch; 1h:47m:54s remains)
INFO - root - 2019-11-06 20:27:24.893201: step 18940, total loss = 0.24, predict loss = 0.07 (87.0 examples/sec; 0.046 sec/batch; 1h:40m:27s remains)
INFO - root - 2019-11-06 20:27:25.361887: step 18950, total loss = 0.42, predict loss = 0.12 (93.1 examples/sec; 0.043 sec/batch; 1h:33m:51s remains)
INFO - root - 2019-11-06 20:27:25.816548: step 18960, total loss = 0.21, predict loss = 0.05 (95.0 examples/sec; 0.042 sec/batch; 1h:31m:59s remains)
INFO - root - 2019-11-06 20:27:26.776108: step 18970, total loss = 0.27, predict loss = 0.06 (73.4 examples/sec; 0.054 sec/batch; 1h:59m:00s remains)
INFO - root - 2019-11-06 20:27:27.411170: step 18980, total loss = 0.35, predict loss = 0.09 (78.1 examples/sec; 0.051 sec/batch; 1h:51m:49s remains)
INFO - root - 2019-11-06 20:27:27.987428: step 18990, total loss = 0.42, predict loss = 0.10 (77.5 examples/sec; 0.052 sec/batch; 1h:52m:41s remains)
INFO - root - 2019-11-06 20:27:28.570567: step 19000, total loss = 0.28, predict loss = 0.07 (73.1 examples/sec; 0.055 sec/batch; 1h:59m:29s remains)
INFO - root - 2019-11-06 20:27:29.167154: step 19010, total loss = 0.26, predict loss = 0.07 (80.8 examples/sec; 0.050 sec/batch; 1h:48m:05s remains)
INFO - root - 2019-11-06 20:27:29.742100: step 19020, total loss = 0.62, predict loss = 0.19 (79.4 examples/sec; 0.050 sec/batch; 1h:49m:58s remains)
INFO - root - 2019-11-06 20:27:30.297189: step 19030, total loss = 0.28, predict loss = 0.07 (77.7 examples/sec; 0.052 sec/batch; 1h:52m:26s remains)
INFO - root - 2019-11-06 20:27:30.878367: step 19040, total loss = 0.35, predict loss = 0.08 (74.0 examples/sec; 0.054 sec/batch; 1h:57m:59s remains)
INFO - root - 2019-11-06 20:27:31.477014: step 19050, total loss = 0.22, predict loss = 0.06 (75.4 examples/sec; 0.053 sec/batch; 1h:55m:45s remains)
INFO - root - 2019-11-06 20:27:32.051914: step 19060, total loss = 0.23, predict loss = 0.07 (79.2 examples/sec; 0.051 sec/batch; 1h:50m:17s remains)
INFO - root - 2019-11-06 20:27:32.629061: step 19070, total loss = 0.30, predict loss = 0.08 (79.7 examples/sec; 0.050 sec/batch; 1h:49m:31s remains)
INFO - root - 2019-11-06 20:27:33.207475: step 19080, total loss = 0.29, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 1h:51m:39s remains)
INFO - root - 2019-11-06 20:27:33.761309: step 19090, total loss = 0.35, predict loss = 0.09 (92.3 examples/sec; 0.043 sec/batch; 1h:34m:32s remains)
INFO - root - 2019-11-06 20:27:34.230925: step 19100, total loss = 0.33, predict loss = 0.09 (91.2 examples/sec; 0.044 sec/batch; 1h:35m:39s remains)
INFO - root - 2019-11-06 20:27:34.682733: step 19110, total loss = 0.42, predict loss = 0.13 (97.5 examples/sec; 0.041 sec/batch; 1h:29m:28s remains)
INFO - root - 2019-11-06 20:27:35.670117: step 19120, total loss = 0.42, predict loss = 0.15 (62.9 examples/sec; 0.064 sec/batch; 2h:18m:38s remains)
INFO - root - 2019-11-06 20:27:36.339202: step 19130, total loss = 0.22, predict loss = 0.06 (71.0 examples/sec; 0.056 sec/batch; 2h:02m:53s remains)
INFO - root - 2019-11-06 20:27:36.923823: step 19140, total loss = 0.40, predict loss = 0.11 (78.4 examples/sec; 0.051 sec/batch; 1h:51m:20s remains)
INFO - root - 2019-11-06 20:27:37.502375: step 19150, total loss = 0.37, predict loss = 0.09 (78.4 examples/sec; 0.051 sec/batch; 1h:51m:16s remains)
INFO - root - 2019-11-06 20:27:38.071057: step 19160, total loss = 0.72, predict loss = 0.22 (77.4 examples/sec; 0.052 sec/batch; 1h:52m:44s remains)
INFO - root - 2019-11-06 20:27:38.670632: step 19170, total loss = 0.19, predict loss = 0.05 (79.0 examples/sec; 0.051 sec/batch; 1h:50m:27s remains)
INFO - root - 2019-11-06 20:27:39.232679: step 19180, total loss = 0.33, predict loss = 0.08 (78.6 examples/sec; 0.051 sec/batch; 1h:50m:57s remains)
INFO - root - 2019-11-06 20:27:39.815304: step 19190, total loss = 0.27, predict loss = 0.07 (77.0 examples/sec; 0.052 sec/batch; 1h:53m:12s remains)
INFO - root - 2019-11-06 20:27:40.389078: step 19200, total loss = 0.39, predict loss = 0.11 (80.2 examples/sec; 0.050 sec/batch; 1h:48m:40s remains)
INFO - root - 2019-11-06 20:27:40.975700: step 19210, total loss = 0.27, predict loss = 0.08 (75.4 examples/sec; 0.053 sec/batch; 1h:55m:39s remains)
INFO - root - 2019-11-06 20:27:41.542828: step 19220, total loss = 0.22, predict loss = 0.05 (79.8 examples/sec; 0.050 sec/batch; 1h:49m:12s remains)
INFO - root - 2019-11-06 20:27:42.110775: step 19230, total loss = 0.31, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 1h:51m:29s remains)
INFO - root - 2019-11-06 20:27:42.634858: step 19240, total loss = 0.36, predict loss = 0.09 (92.3 examples/sec; 0.043 sec/batch; 1h:34m:28s remains)
INFO - root - 2019-11-06 20:27:43.094431: step 19250, total loss = 0.33, predict loss = 0.10 (95.4 examples/sec; 0.042 sec/batch; 1h:31m:24s remains)
INFO - root - 2019-11-06 20:27:43.561034: step 19260, total loss = 0.30, predict loss = 0.08 (88.4 examples/sec; 0.045 sec/batch; 1h:38m:37s remains)
INFO - root - 2019-11-06 20:27:44.596333: step 19270, total loss = 0.28, predict loss = 0.07 (62.9 examples/sec; 0.064 sec/batch; 2h:18m:34s remains)
INFO - root - 2019-11-06 20:27:45.257481: step 19280, total loss = 0.31, predict loss = 0.06 (67.8 examples/sec; 0.059 sec/batch; 2h:08m:26s remains)
INFO - root - 2019-11-06 20:27:45.856068: step 19290, total loss = 0.60, predict loss = 0.19 (81.8 examples/sec; 0.049 sec/batch; 1h:46m:32s remains)
INFO - root - 2019-11-06 20:27:46.435148: step 19300, total loss = 0.36, predict loss = 0.10 (81.0 examples/sec; 0.049 sec/batch; 1h:47m:35s remains)
INFO - root - 2019-11-06 20:27:47.013858: step 19310, total loss = 0.18, predict loss = 0.05 (75.4 examples/sec; 0.053 sec/batch; 1h:55m:32s remains)
INFO - root - 2019-11-06 20:27:47.584754: step 19320, total loss = 0.41, predict loss = 0.11 (79.5 examples/sec; 0.050 sec/batch; 1h:49m:38s remains)
INFO - root - 2019-11-06 20:27:48.167679: step 19330, total loss = 0.29, predict loss = 0.07 (76.6 examples/sec; 0.052 sec/batch; 1h:53m:41s remains)
INFO - root - 2019-11-06 20:27:48.747460: step 19340, total loss = 0.33, predict loss = 0.09 (79.2 examples/sec; 0.050 sec/batch; 1h:49m:57s remains)
INFO - root - 2019-11-06 20:27:49.326926: step 19350, total loss = 0.53, predict loss = 0.17 (77.0 examples/sec; 0.052 sec/batch; 1h:53m:08s remains)
INFO - root - 2019-11-06 20:27:49.904342: step 19360, total loss = 0.27, predict loss = 0.07 (76.5 examples/sec; 0.052 sec/batch; 1h:53m:53s remains)
INFO - root - 2019-11-06 20:27:50.482996: step 19370, total loss = 0.42, predict loss = 0.09 (84.6 examples/sec; 0.047 sec/batch; 1h:42m:53s remains)
INFO - root - 2019-11-06 20:27:51.049239: step 19380, total loss = 0.28, predict loss = 0.06 (77.6 examples/sec; 0.052 sec/batch; 1h:52m:11s remains)
INFO - root - 2019-11-06 20:27:51.558916: step 19390, total loss = 0.26, predict loss = 0.07 (101.5 examples/sec; 0.039 sec/batch; 1h:25m:46s remains)
INFO - root - 2019-11-06 20:27:52.004703: step 19400, total loss = 0.59, predict loss = 0.17 (97.0 examples/sec; 0.041 sec/batch; 1h:29m:46s remains)
INFO - root - 2019-11-06 20:27:52.490692: step 19410, total loss = 0.37, predict loss = 0.10 (95.0 examples/sec; 0.042 sec/batch; 1h:31m:36s remains)
INFO - root - 2019-11-06 20:27:53.603760: step 19420, total loss = 0.35, predict loss = 0.09 (50.4 examples/sec; 0.079 sec/batch; 2h:52m:53s remains)
INFO - root - 2019-11-06 20:27:54.245734: step 19430, total loss = 0.43, predict loss = 0.16 (77.8 examples/sec; 0.051 sec/batch; 1h:51m:53s remains)
INFO - root - 2019-11-06 20:27:54.810147: step 19440, total loss = 0.28, predict loss = 0.08 (76.9 examples/sec; 0.052 sec/batch; 1h:53m:07s remains)
INFO - root - 2019-11-06 20:27:55.402343: step 19450, total loss = 0.30, predict loss = 0.08 (82.5 examples/sec; 0.048 sec/batch; 1h:45m:30s remains)
INFO - root - 2019-11-06 20:27:55.970561: step 19460, total loss = 0.36, predict loss = 0.11 (80.4 examples/sec; 0.050 sec/batch; 1h:48m:15s remains)
INFO - root - 2019-11-06 20:27:56.542838: step 19470, total loss = 0.34, predict loss = 0.09 (78.8 examples/sec; 0.051 sec/batch; 1h:50m:29s remains)
INFO - root - 2019-11-06 20:27:57.122955: step 19480, total loss = 0.38, predict loss = 0.12 (76.4 examples/sec; 0.052 sec/batch; 1h:53m:54s remains)
INFO - root - 2019-11-06 20:27:57.716053: step 19490, total loss = 0.18, predict loss = 0.04 (79.1 examples/sec; 0.051 sec/batch; 1h:49m:55s remains)
INFO - root - 2019-11-06 20:27:58.298131: step 19500, total loss = 0.29, predict loss = 0.07 (77.2 examples/sec; 0.052 sec/batch; 1h:52m:42s remains)
INFO - root - 2019-11-06 20:27:58.875284: step 19510, total loss = 0.59, predict loss = 0.14 (76.7 examples/sec; 0.052 sec/batch; 1h:53m:28s remains)
INFO - root - 2019-11-06 20:27:59.465795: step 19520, total loss = 0.45, predict loss = 0.12 (77.2 examples/sec; 0.052 sec/batch; 1h:52m:37s remains)
INFO - root - 2019-11-06 20:28:00.060279: step 19530, total loss = 0.53, predict loss = 0.14 (79.6 examples/sec; 0.050 sec/batch; 1h:49m:17s remains)
INFO - root - 2019-11-06 20:28:00.551432: step 19540, total loss = 0.34, predict loss = 0.09 (90.4 examples/sec; 0.044 sec/batch; 1h:36m:10s remains)
INFO - root - 2019-11-06 20:28:01.016839: step 19550, total loss = 0.53, predict loss = 0.16 (91.8 examples/sec; 0.044 sec/batch; 1h:34m:42s remains)
INFO - root - 2019-11-06 20:28:01.932849: step 19560, total loss = 0.31, predict loss = 0.07 (8.3 examples/sec; 0.484 sec/batch; 17h:33m:10s remains)
INFO - root - 2019-11-06 20:28:02.570326: step 19570, total loss = 0.25, predict loss = 0.07 (65.0 examples/sec; 0.062 sec/batch; 2h:13m:41s remains)
INFO - root - 2019-11-06 20:28:03.196358: step 19580, total loss = 0.47, predict loss = 0.10 (74.0 examples/sec; 0.054 sec/batch; 1h:57m:29s remains)
INFO - root - 2019-11-06 20:28:03.764175: step 19590, total loss = 0.40, predict loss = 0.11 (81.6 examples/sec; 0.049 sec/batch; 1h:46m:29s remains)
INFO - root - 2019-11-06 20:28:04.339205: step 19600, total loss = 0.35, predict loss = 0.10 (77.0 examples/sec; 0.052 sec/batch; 1h:52m:54s remains)
INFO - root - 2019-11-06 20:28:04.936047: step 19610, total loss = 0.41, predict loss = 0.09 (74.2 examples/sec; 0.054 sec/batch; 1h:57m:04s remains)
INFO - root - 2019-11-06 20:28:05.510610: step 19620, total loss = 0.54, predict loss = 0.15 (78.8 examples/sec; 0.051 sec/batch; 1h:50m:21s remains)
INFO - root - 2019-11-06 20:28:06.077463: step 19630, total loss = 0.56, predict loss = 0.16 (75.6 examples/sec; 0.053 sec/batch; 1h:54m:54s remains)
INFO - root - 2019-11-06 20:28:06.653345: step 19640, total loss = 0.30, predict loss = 0.08 (76.4 examples/sec; 0.052 sec/batch; 1h:53m:47s remains)
INFO - root - 2019-11-06 20:28:07.250942: step 19650, total loss = 0.26, predict loss = 0.08 (79.8 examples/sec; 0.050 sec/batch; 1h:48m:56s remains)
INFO - root - 2019-11-06 20:28:07.826487: step 19660, total loss = 0.24, predict loss = 0.05 (82.6 examples/sec; 0.048 sec/batch; 1h:45m:09s remains)
INFO - root - 2019-11-06 20:28:08.391963: step 19670, total loss = 0.26, predict loss = 0.07 (80.5 examples/sec; 0.050 sec/batch; 1h:47m:57s remains)
INFO - root - 2019-11-06 20:28:08.955892: step 19680, total loss = 0.46, predict loss = 0.12 (84.9 examples/sec; 0.047 sec/batch; 1h:42m:20s remains)
INFO - root - 2019-11-06 20:28:09.448282: step 19690, total loss = 0.32, predict loss = 0.09 (91.6 examples/sec; 0.044 sec/batch; 1h:34m:51s remains)
INFO - root - 2019-11-06 20:28:09.905081: step 19700, total loss = 0.19, predict loss = 0.05 (95.7 examples/sec; 0.042 sec/batch; 1h:30m:46s remains)
INFO - root - 2019-11-06 20:28:10.814350: step 19710, total loss = 0.36, predict loss = 0.09 (81.3 examples/sec; 0.049 sec/batch; 1h:46m:47s remains)
INFO - root - 2019-11-06 20:28:11.470615: step 19720, total loss = 0.31, predict loss = 0.09 (65.5 examples/sec; 0.061 sec/batch; 2h:12m:30s remains)
INFO - root - 2019-11-06 20:28:12.101024: step 19730, total loss = 0.40, predict loss = 0.11 (82.0 examples/sec; 0.049 sec/batch; 1h:45m:54s remains)
INFO - root - 2019-11-06 20:28:12.671629: step 19740, total loss = 0.24, predict loss = 0.06 (80.4 examples/sec; 0.050 sec/batch; 1h:48m:01s remains)
INFO - root - 2019-11-06 20:28:13.250578: step 19750, total loss = 0.29, predict loss = 0.06 (76.1 examples/sec; 0.053 sec/batch; 1h:54m:07s remains)
INFO - root - 2019-11-06 20:28:13.834217: step 19760, total loss = 0.29, predict loss = 0.08 (75.7 examples/sec; 0.053 sec/batch; 1h:54m:37s remains)
INFO - root - 2019-11-06 20:28:14.422658: step 19770, total loss = 0.19, predict loss = 0.05 (82.7 examples/sec; 0.048 sec/batch; 1h:44m:56s remains)
INFO - root - 2019-11-06 20:28:15.002764: step 19780, total loss = 0.49, predict loss = 0.12 (72.8 examples/sec; 0.055 sec/batch; 1h:59m:16s remains)
INFO - root - 2019-11-06 20:28:15.629936: step 19790, total loss = 0.28, predict loss = 0.08 (77.7 examples/sec; 0.051 sec/batch; 1h:51m:40s remains)
INFO - root - 2019-11-06 20:28:16.200927: step 19800, total loss = 0.36, predict loss = 0.09 (73.8 examples/sec; 0.054 sec/batch; 1h:57m:32s remains)
INFO - root - 2019-11-06 20:28:16.777316: step 19810, total loss = 0.26, predict loss = 0.07 (80.3 examples/sec; 0.050 sec/batch; 1h:48m:08s remains)
INFO - root - 2019-11-06 20:28:17.342929: step 19820, total loss = 0.20, predict loss = 0.05 (81.2 examples/sec; 0.049 sec/batch; 1h:46m:54s remains)
INFO - root - 2019-11-06 20:28:17.891863: step 19830, total loss = 0.44, predict loss = 0.12 (95.5 examples/sec; 0.042 sec/batch; 1h:30m:50s remains)
INFO - root - 2019-11-06 20:28:18.354184: step 19840, total loss = 0.61, predict loss = 0.17 (98.4 examples/sec; 0.041 sec/batch; 1h:28m:11s remains)
INFO - root - 2019-11-06 20:28:18.815817: step 19850, total loss = 0.28, predict loss = 0.07 (98.0 examples/sec; 0.041 sec/batch; 1h:28m:32s remains)
INFO - root - 2019-11-06 20:28:19.742212: step 19860, total loss = 0.22, predict loss = 0.06 (71.9 examples/sec; 0.056 sec/batch; 2h:00m:40s remains)
INFO - root - 2019-11-06 20:28:20.419129: step 19870, total loss = 0.37, predict loss = 0.10 (74.9 examples/sec; 0.053 sec/batch; 1h:55m:51s remains)
INFO - root - 2019-11-06 20:28:20.988974: step 19880, total loss = 0.51, predict loss = 0.13 (77.8 examples/sec; 0.051 sec/batch; 1h:51m:32s remains)
INFO - root - 2019-11-06 20:28:21.575082: step 19890, total loss = 0.27, predict loss = 0.07 (77.3 examples/sec; 0.052 sec/batch; 1h:52m:11s remains)
INFO - root - 2019-11-06 20:28:22.136991: step 19900, total loss = 0.56, predict loss = 0.17 (77.3 examples/sec; 0.052 sec/batch; 1h:52m:14s remains)
INFO - root - 2019-11-06 20:28:22.697131: step 19910, total loss = 0.29, predict loss = 0.07 (82.3 examples/sec; 0.049 sec/batch; 1h:45m:19s remains)
INFO - root - 2019-11-06 20:28:23.258492: step 19920, total loss = 0.24, predict loss = 0.05 (75.7 examples/sec; 0.053 sec/batch; 1h:54m:29s remains)
INFO - root - 2019-11-06 20:28:23.859156: step 19930, total loss = 0.33, predict loss = 0.08 (80.5 examples/sec; 0.050 sec/batch; 1h:47m:39s remains)
INFO - root - 2019-11-06 20:28:24.444111: step 19940, total loss = 0.40, predict loss = 0.11 (78.0 examples/sec; 0.051 sec/batch; 1h:51m:08s remains)
INFO - root - 2019-11-06 20:28:25.015571: step 19950, total loss = 0.26, predict loss = 0.07 (82.0 examples/sec; 0.049 sec/batch; 1h:45m:44s remains)
INFO - root - 2019-11-06 20:28:25.596760: step 19960, total loss = 0.19, predict loss = 0.04 (76.4 examples/sec; 0.052 sec/batch; 1h:53m:27s remains)
INFO - root - 2019-11-06 20:28:26.195295: step 19970, total loss = 0.20, predict loss = 0.05 (84.1 examples/sec; 0.048 sec/batch; 1h:43m:01s remains)
INFO - root - 2019-11-06 20:28:26.735633: step 19980, total loss = 0.38, predict loss = 0.10 (93.7 examples/sec; 0.043 sec/batch; 1h:32m:28s remains)
INFO - root - 2019-11-06 20:28:27.185885: step 19990, total loss = 0.27, predict loss = 0.06 (100.9 examples/sec; 0.040 sec/batch; 1h:25m:54s remains)
INFO - root - 2019-11-06 20:28:27.639320: step 20000, total loss = 0.39, predict loss = 0.12 (93.5 examples/sec; 0.043 sec/batch; 1h:32m:43s remains)
INFO - root - 2019-11-06 20:28:28.709534: step 20010, total loss = 0.34, predict loss = 0.06 (53.8 examples/sec; 0.074 sec/batch; 2h:41m:03s remains)
INFO - root - 2019-11-06 20:28:29.332987: step 20020, total loss = 0.29, predict loss = 0.09 (76.2 examples/sec; 0.052 sec/batch; 1h:53m:42s remains)
INFO - root - 2019-11-06 20:28:29.910692: step 20030, total loss = 0.24, predict loss = 0.06 (74.8 examples/sec; 0.054 sec/batch; 1h:55m:54s remains)
INFO - root - 2019-11-06 20:28:30.483581: step 20040, total loss = 0.49, predict loss = 0.13 (78.6 examples/sec; 0.051 sec/batch; 1h:50m:10s remains)
INFO - root - 2019-11-06 20:28:31.063978: step 20050, total loss = 0.31, predict loss = 0.07 (80.6 examples/sec; 0.050 sec/batch; 1h:47m:26s remains)
INFO - root - 2019-11-06 20:28:31.632161: step 20060, total loss = 0.19, predict loss = 0.05 (78.6 examples/sec; 0.051 sec/batch; 1h:50m:08s remains)
INFO - root - 2019-11-06 20:28:32.207008: step 20070, total loss = 0.21, predict loss = 0.06 (84.0 examples/sec; 0.048 sec/batch; 1h:43m:06s remains)
INFO - root - 2019-11-06 20:28:32.788678: step 20080, total loss = 0.25, predict loss = 0.06 (78.0 examples/sec; 0.051 sec/batch; 1h:51m:06s remains)
INFO - root - 2019-11-06 20:28:33.365586: step 20090, total loss = 0.40, predict loss = 0.11 (77.0 examples/sec; 0.052 sec/batch; 1h:52m:31s remains)
INFO - root - 2019-11-06 20:28:33.943330: step 20100, total loss = 0.36, predict loss = 0.10 (76.9 examples/sec; 0.052 sec/batch; 1h:52m:35s remains)
INFO - root - 2019-11-06 20:28:34.519619: step 20110, total loss = 0.26, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:49m:32s remains)
INFO - root - 2019-11-06 20:28:35.097571: step 20120, total loss = 0.33, predict loss = 0.08 (80.2 examples/sec; 0.050 sec/batch; 1h:47m:56s remains)
INFO - root - 2019-11-06 20:28:35.630445: step 20130, total loss = 0.26, predict loss = 0.07 (97.8 examples/sec; 0.041 sec/batch; 1h:28m:33s remains)
INFO - root - 2019-11-06 20:28:36.082479: step 20140, total loss = 0.32, predict loss = 0.08 (93.5 examples/sec; 0.043 sec/batch; 1h:32m:33s remains)
INFO - root - 2019-11-06 20:28:36.538825: step 20150, total loss = 0.34, predict loss = 0.08 (103.9 examples/sec; 0.038 sec/batch; 1h:23m:17s remains)
INFO - root - 2019-11-06 20:28:37.555034: step 20160, total loss = 0.41, predict loss = 0.12 (63.3 examples/sec; 0.063 sec/batch; 2h:16m:38s remains)
INFO - root - 2019-11-06 20:28:38.187897: step 20170, total loss = 0.31, predict loss = 0.07 (78.3 examples/sec; 0.051 sec/batch; 1h:50m:29s remains)
INFO - root - 2019-11-06 20:28:38.755007: step 20180, total loss = 0.28, predict loss = 0.07 (77.3 examples/sec; 0.052 sec/batch; 1h:51m:56s remains)
INFO - root - 2019-11-06 20:28:39.319753: step 20190, total loss = 0.28, predict loss = 0.07 (82.0 examples/sec; 0.049 sec/batch; 1h:45m:33s remains)
INFO - root - 2019-11-06 20:28:39.893883: step 20200, total loss = 0.56, predict loss = 0.17 (77.3 examples/sec; 0.052 sec/batch; 1h:51m:55s remains)
INFO - root - 2019-11-06 20:28:40.489318: step 20210, total loss = 0.33, predict loss = 0.08 (78.4 examples/sec; 0.051 sec/batch; 1h:50m:20s remains)
INFO - root - 2019-11-06 20:28:41.071039: step 20220, total loss = 0.37, predict loss = 0.10 (79.6 examples/sec; 0.050 sec/batch; 1h:48m:45s remains)
INFO - root - 2019-11-06 20:28:41.642289: step 20230, total loss = 0.24, predict loss = 0.06 (77.0 examples/sec; 0.052 sec/batch; 1h:52m:23s remains)
INFO - root - 2019-11-06 20:28:42.214237: step 20240, total loss = 0.19, predict loss = 0.05 (76.1 examples/sec; 0.053 sec/batch; 1h:53m:43s remains)
INFO - root - 2019-11-06 20:28:42.806188: step 20250, total loss = 0.53, predict loss = 0.17 (77.4 examples/sec; 0.052 sec/batch; 1h:51m:47s remains)
INFO - root - 2019-11-06 20:28:43.376677: step 20260, total loss = 0.21, predict loss = 0.05 (76.3 examples/sec; 0.052 sec/batch; 1h:53m:25s remains)
INFO - root - 2019-11-06 20:28:43.947593: step 20270, total loss = 0.48, predict loss = 0.12 (77.4 examples/sec; 0.052 sec/batch; 1h:51m:44s remains)
INFO - root - 2019-11-06 20:28:44.441819: step 20280, total loss = 0.24, predict loss = 0.06 (96.4 examples/sec; 0.041 sec/batch; 1h:29m:42s remains)
INFO - root - 2019-11-06 20:28:44.936397: step 20290, total loss = 0.29, predict loss = 0.07 (94.3 examples/sec; 0.042 sec/batch; 1h:31m:39s remains)
INFO - root - 2019-11-06 20:28:45.400928: step 20300, total loss = 0.44, predict loss = 0.11 (92.9 examples/sec; 0.043 sec/batch; 1h:33m:02s remains)
INFO - root - 2019-11-06 20:28:46.580560: step 20310, total loss = 0.26, predict loss = 0.07 (59.4 examples/sec; 0.067 sec/batch; 2h:25m:33s remains)
INFO - root - 2019-11-06 20:28:47.207767: step 20320, total loss = 0.29, predict loss = 0.07 (81.6 examples/sec; 0.049 sec/batch; 1h:45m:57s remains)
INFO - root - 2019-11-06 20:28:47.790582: step 20330, total loss = 0.35, predict loss = 0.08 (77.6 examples/sec; 0.052 sec/batch; 1h:51m:23s remains)
INFO - root - 2019-11-06 20:28:48.351585: step 20340, total loss = 0.31, predict loss = 0.07 (76.7 examples/sec; 0.052 sec/batch; 1h:52m:37s remains)
INFO - root - 2019-11-06 20:28:48.925963: step 20350, total loss = 0.64, predict loss = 0.17 (80.8 examples/sec; 0.050 sec/batch; 1h:47m:00s remains)
INFO - root - 2019-11-06 20:28:49.499865: step 20360, total loss = 0.32, predict loss = 0.09 (72.5 examples/sec; 0.055 sec/batch; 1h:59m:15s remains)
INFO - root - 2019-11-06 20:28:50.082286: step 20370, total loss = 0.33, predict loss = 0.08 (78.0 examples/sec; 0.051 sec/batch; 1h:50m:48s remains)
INFO - root - 2019-11-06 20:28:50.660069: step 20380, total loss = 0.33, predict loss = 0.09 (76.9 examples/sec; 0.052 sec/batch; 1h:52m:25s remains)
INFO - root - 2019-11-06 20:28:51.226196: step 20390, total loss = 0.33, predict loss = 0.08 (80.8 examples/sec; 0.050 sec/batch; 1h:46m:59s remains)
INFO - root - 2019-11-06 20:28:51.817487: step 20400, total loss = 0.45, predict loss = 0.12 (82.4 examples/sec; 0.049 sec/batch; 1h:44m:51s remains)
INFO - root - 2019-11-06 20:28:52.396093: step 20410, total loss = 0.43, predict loss = 0.11 (80.2 examples/sec; 0.050 sec/batch; 1h:47m:40s remains)
INFO - root - 2019-11-06 20:28:52.975534: step 20420, total loss = 0.50, predict loss = 0.15 (79.9 examples/sec; 0.050 sec/batch; 1h:48m:08s remains)
INFO - root - 2019-11-06 20:28:53.449016: step 20430, total loss = 0.26, predict loss = 0.07 (96.4 examples/sec; 0.042 sec/batch; 1h:29m:38s remains)
INFO - root - 2019-11-06 20:28:53.898940: step 20440, total loss = 0.25, predict loss = 0.06 (96.1 examples/sec; 0.042 sec/batch; 1h:29m:50s remains)
INFO - root - 2019-11-06 20:28:54.821874: step 20450, total loss = 0.36, predict loss = 0.12 (78.5 examples/sec; 0.051 sec/batch; 1h:49m:58s remains)
INFO - root - 2019-11-06 20:28:55.449415: step 20460, total loss = 0.25, predict loss = 0.06 (61.2 examples/sec; 0.065 sec/batch; 2h:21m:12s remains)
INFO - root - 2019-11-06 20:28:56.047651: step 20470, total loss = 0.26, predict loss = 0.06 (80.3 examples/sec; 0.050 sec/batch; 1h:47m:29s remains)
INFO - root - 2019-11-06 20:28:56.612399: step 20480, total loss = 0.37, predict loss = 0.12 (79.7 examples/sec; 0.050 sec/batch; 1h:48m:18s remains)
INFO - root - 2019-11-06 20:28:57.200364: step 20490, total loss = 0.29, predict loss = 0.07 (81.4 examples/sec; 0.049 sec/batch; 1h:46m:06s remains)
INFO - root - 2019-11-06 20:28:57.778212: step 20500, total loss = 0.29, predict loss = 0.08 (79.4 examples/sec; 0.050 sec/batch; 1h:48m:41s remains)
INFO - root - 2019-11-06 20:28:58.342354: step 20510, total loss = 0.35, predict loss = 0.11 (79.1 examples/sec; 0.051 sec/batch; 1h:49m:10s remains)
INFO - root - 2019-11-06 20:28:58.906341: step 20520, total loss = 0.24, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:48m:11s remains)
INFO - root - 2019-11-06 20:28:59.489592: step 20530, total loss = 0.35, predict loss = 0.10 (77.1 examples/sec; 0.052 sec/batch; 1h:52m:01s remains)
INFO - root - 2019-11-06 20:29:00.068180: step 20540, total loss = 0.28, predict loss = 0.08 (80.0 examples/sec; 0.050 sec/batch; 1h:47m:54s remains)
INFO - root - 2019-11-06 20:29:00.632745: step 20550, total loss = 0.26, predict loss = 0.06 (86.8 examples/sec; 0.046 sec/batch; 1h:39m:25s remains)
INFO - root - 2019-11-06 20:29:01.202942: step 20560, total loss = 0.30, predict loss = 0.07 (78.9 examples/sec; 0.051 sec/batch; 1h:49m:18s remains)
INFO - root - 2019-11-06 20:29:01.777370: step 20570, total loss = 0.23, predict loss = 0.06 (87.3 examples/sec; 0.046 sec/batch; 1h:38m:51s remains)
INFO - root - 2019-11-06 20:29:02.235066: step 20580, total loss = 0.38, predict loss = 0.11 (96.3 examples/sec; 0.042 sec/batch; 1h:29m:36s remains)
INFO - root - 2019-11-06 20:29:02.681976: step 20590, total loss = 0.40, predict loss = 0.11 (94.7 examples/sec; 0.042 sec/batch; 1h:31m:06s remains)
INFO - root - 2019-11-06 20:29:03.617233: step 20600, total loss = 0.24, predict loss = 0.06 (77.9 examples/sec; 0.051 sec/batch; 1h:50m:48s remains)
INFO - root - 2019-11-06 20:29:04.273016: step 20610, total loss = 0.26, predict loss = 0.07 (71.7 examples/sec; 0.056 sec/batch; 2h:00m:19s remains)
INFO - root - 2019-11-06 20:29:04.864943: step 20620, total loss = 0.31, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:49m:04s remains)
INFO - root - 2019-11-06 20:29:05.436295: step 20630, total loss = 0.32, predict loss = 0.08 (82.0 examples/sec; 0.049 sec/batch; 1h:45m:13s remains)
INFO - root - 2019-11-06 20:29:05.993582: step 20640, total loss = 0.21, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:49m:21s remains)
INFO - root - 2019-11-06 20:29:06.590107: step 20650, total loss = 0.20, predict loss = 0.05 (79.4 examples/sec; 0.050 sec/batch; 1h:48m:35s remains)
INFO - root - 2019-11-06 20:29:07.164839: step 20660, total loss = 0.37, predict loss = 0.10 (73.0 examples/sec; 0.055 sec/batch; 1h:58m:06s remains)
INFO - root - 2019-11-06 20:29:07.737542: step 20670, total loss = 0.28, predict loss = 0.07 (77.9 examples/sec; 0.051 sec/batch; 1h:50m:40s remains)
INFO - root - 2019-11-06 20:29:08.312151: step 20680, total loss = 0.46, predict loss = 0.12 (74.9 examples/sec; 0.053 sec/batch; 1h:55m:10s remains)
INFO - root - 2019-11-06 20:29:08.908580: step 20690, total loss = 0.36, predict loss = 0.10 (74.9 examples/sec; 0.053 sec/batch; 1h:55m:03s remains)
INFO - root - 2019-11-06 20:29:09.476789: step 20700, total loss = 0.41, predict loss = 0.13 (77.6 examples/sec; 0.052 sec/batch; 1h:51m:03s remains)
INFO - root - 2019-11-06 20:29:10.047062: step 20710, total loss = 0.22, predict loss = 0.06 (80.1 examples/sec; 0.050 sec/batch; 1h:47m:39s remains)
INFO - root - 2019-11-06 20:29:10.586355: step 20720, total loss = 0.29, predict loss = 0.08 (96.0 examples/sec; 0.042 sec/batch; 1h:29m:48s remains)
INFO - root - 2019-11-06 20:29:11.072680: step 20730, total loss = 0.36, predict loss = 0.11 (89.9 examples/sec; 0.044 sec/batch; 1h:35m:51s remains)
INFO - root - 2019-11-06 20:29:11.534457: step 20740, total loss = 0.34, predict loss = 0.08 (92.8 examples/sec; 0.043 sec/batch; 1h:32m:49s remains)
INFO - root - 2019-11-06 20:29:12.525737: step 20750, total loss = 0.23, predict loss = 0.05 (56.4 examples/sec; 0.071 sec/batch; 2h:32m:53s remains)
INFO - root - 2019-11-06 20:29:13.161636: step 20760, total loss = 0.27, predict loss = 0.07 (74.9 examples/sec; 0.053 sec/batch; 1h:55m:04s remains)
INFO - root - 2019-11-06 20:29:13.753814: step 20770, total loss = 0.34, predict loss = 0.12 (76.6 examples/sec; 0.052 sec/batch; 1h:52m:31s remains)
INFO - root - 2019-11-06 20:29:14.333698: step 20780, total loss = 0.23, predict loss = 0.06 (78.8 examples/sec; 0.051 sec/batch; 1h:49m:16s remains)
INFO - root - 2019-11-06 20:29:14.896869: step 20790, total loss = 0.21, predict loss = 0.05 (80.3 examples/sec; 0.050 sec/batch; 1h:47m:16s remains)
INFO - root - 2019-11-06 20:29:15.506725: step 20800, total loss = 0.30, predict loss = 0.09 (80.3 examples/sec; 0.050 sec/batch; 1h:47m:18s remains)
INFO - root - 2019-11-06 20:29:16.096359: step 20810, total loss = 0.30, predict loss = 0.07 (83.0 examples/sec; 0.048 sec/batch; 1h:43m:45s remains)
INFO - root - 2019-11-06 20:29:16.680913: step 20820, total loss = 0.35, predict loss = 0.10 (76.3 examples/sec; 0.052 sec/batch; 1h:52m:50s remains)
INFO - root - 2019-11-06 20:29:17.248940: step 20830, total loss = 0.35, predict loss = 0.10 (76.1 examples/sec; 0.053 sec/batch; 1h:53m:13s remains)
INFO - root - 2019-11-06 20:29:17.826433: step 20840, total loss = 0.23, predict loss = 0.05 (77.3 examples/sec; 0.052 sec/batch; 1h:51m:22s remains)
INFO - root - 2019-11-06 20:29:18.415423: step 20850, total loss = 0.26, predict loss = 0.06 (76.1 examples/sec; 0.053 sec/batch; 1h:53m:10s remains)
INFO - root - 2019-11-06 20:29:18.989144: step 20860, total loss = 0.21, predict loss = 0.05 (72.7 examples/sec; 0.055 sec/batch; 1h:58m:29s remains)
INFO - root - 2019-11-06 20:29:19.509831: step 20870, total loss = 0.22, predict loss = 0.05 (99.8 examples/sec; 0.040 sec/batch; 1h:26m:17s remains)
INFO - root - 2019-11-06 20:29:19.966033: step 20880, total loss = 0.25, predict loss = 0.07 (90.1 examples/sec; 0.044 sec/batch; 1h:35m:34s remains)
INFO - root - 2019-11-06 20:29:20.448490: step 20890, total loss = 0.23, predict loss = 0.06 (96.1 examples/sec; 0.042 sec/batch; 1h:29m:34s remains)
INFO - root - 2019-11-06 20:29:21.414044: step 20900, total loss = 0.19, predict loss = 0.05 (59.7 examples/sec; 0.067 sec/batch; 2h:24m:03s remains)
INFO - root - 2019-11-06 20:29:22.034312: step 20910, total loss = 0.41, predict loss = 0.11 (77.9 examples/sec; 0.051 sec/batch; 1h:50m:24s remains)
INFO - root - 2019-11-06 20:29:22.610740: step 20920, total loss = 0.38, predict loss = 0.11 (77.5 examples/sec; 0.052 sec/batch; 1h:51m:03s remains)
INFO - root - 2019-11-06 20:29:23.215296: step 20930, total loss = 0.27, predict loss = 0.07 (74.0 examples/sec; 0.054 sec/batch; 1h:56m:15s remains)
INFO - root - 2019-11-06 20:29:23.795845: step 20940, total loss = 0.34, predict loss = 0.07 (80.3 examples/sec; 0.050 sec/batch; 1h:47m:10s remains)
INFO - root - 2019-11-06 20:29:24.370566: step 20950, total loss = 0.36, predict loss = 0.09 (80.8 examples/sec; 0.049 sec/batch; 1h:46m:24s remains)
INFO - root - 2019-11-06 20:29:24.934523: step 20960, total loss = 0.19, predict loss = 0.04 (80.5 examples/sec; 0.050 sec/batch; 1h:46m:49s remains)
INFO - root - 2019-11-06 20:29:25.510417: step 20970, total loss = 0.29, predict loss = 0.08 (77.0 examples/sec; 0.052 sec/batch; 1h:51m:43s remains)
INFO - root - 2019-11-06 20:29:26.085462: step 20980, total loss = 0.34, predict loss = 0.08 (79.0 examples/sec; 0.051 sec/batch; 1h:48m:48s remains)
INFO - root - 2019-11-06 20:29:26.657777: step 20990, total loss = 0.47, predict loss = 0.11 (85.8 examples/sec; 0.047 sec/batch; 1h:40m:16s remains)
INFO - root - 2019-11-06 20:29:27.232512: step 21000, total loss = 0.24, predict loss = 0.06 (78.1 examples/sec; 0.051 sec/batch; 1h:50m:07s remains)
INFO - root - 2019-11-06 20:29:27.812322: step 21010, total loss = 0.24, predict loss = 0.06 (77.1 examples/sec; 0.052 sec/batch; 1h:51m:33s remains)
INFO - root - 2019-11-06 20:29:28.317164: step 21020, total loss = 0.28, predict loss = 0.08 (103.7 examples/sec; 0.039 sec/batch; 1h:22m:53s remains)
INFO - root - 2019-11-06 20:29:28.759927: step 21030, total loss = 0.24, predict loss = 0.06 (93.3 examples/sec; 0.043 sec/batch; 1h:32m:10s remains)
INFO - root - 2019-11-06 20:29:29.225739: step 21040, total loss = 0.24, predict loss = 0.06 (90.6 examples/sec; 0.044 sec/batch; 1h:34m:54s remains)
INFO - root - 2019-11-06 20:29:30.419800: step 21050, total loss = 0.22, predict loss = 0.06 (46.3 examples/sec; 0.086 sec/batch; 3h:05m:33s remains)
INFO - root - 2019-11-06 20:29:31.052128: step 21060, total loss = 0.38, predict loss = 0.11 (74.6 examples/sec; 0.054 sec/batch; 1h:55m:11s remains)
INFO - root - 2019-11-06 20:29:31.615737: step 21070, total loss = 0.40, predict loss = 0.11 (80.0 examples/sec; 0.050 sec/batch; 1h:47m:27s remains)
INFO - root - 2019-11-06 20:29:32.179648: step 21080, total loss = 0.35, predict loss = 0.09 (83.1 examples/sec; 0.048 sec/batch; 1h:43m:24s remains)
INFO - root - 2019-11-06 20:29:32.767286: step 21090, total loss = 0.37, predict loss = 0.09 (78.5 examples/sec; 0.051 sec/batch; 1h:49m:25s remains)
INFO - root - 2019-11-06 20:29:33.355128: step 21100, total loss = 0.39, predict loss = 0.12 (81.5 examples/sec; 0.049 sec/batch; 1h:45m:26s remains)
INFO - root - 2019-11-06 20:29:33.928763: step 21110, total loss = 0.27, predict loss = 0.07 (74.0 examples/sec; 0.054 sec/batch; 1h:56m:06s remains)
INFO - root - 2019-11-06 20:29:34.504724: step 21120, total loss = 0.46, predict loss = 0.11 (80.2 examples/sec; 0.050 sec/batch; 1h:47m:04s remains)
INFO - root - 2019-11-06 20:29:35.104057: step 21130, total loss = 0.31, predict loss = 0.09 (76.9 examples/sec; 0.052 sec/batch; 1h:51m:44s remains)
INFO - root - 2019-11-06 20:29:35.668039: step 21140, total loss = 0.33, predict loss = 0.12 (82.0 examples/sec; 0.049 sec/batch; 1h:44m:46s remains)
INFO - root - 2019-11-06 20:29:36.241726: step 21150, total loss = 0.24, predict loss = 0.05 (84.8 examples/sec; 0.047 sec/batch; 1h:41m:16s remains)
INFO - root - 2019-11-06 20:29:36.819476: step 21160, total loss = 0.24, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 1h:49m:13s remains)
INFO - root - 2019-11-06 20:29:37.312528: step 21170, total loss = 0.36, predict loss = 0.10 (97.0 examples/sec; 0.041 sec/batch; 1h:28m:32s remains)
INFO - root - 2019-11-06 20:29:37.771260: step 21180, total loss = 0.20, predict loss = 0.05 (92.5 examples/sec; 0.043 sec/batch; 1h:32m:51s remains)
INFO - root - 2019-11-06 20:29:38.654007: step 21190, total loss = 0.25, predict loss = 0.06 (8.5 examples/sec; 0.469 sec/batch; 16h:47m:03s remains)
INFO - root - 2019-11-06 20:29:39.387222: step 21200, total loss = 0.27, predict loss = 0.07 (58.0 examples/sec; 0.069 sec/batch; 2h:27m:59s remains)
INFO - root - 2019-11-06 20:29:40.021163: step 21210, total loss = 0.17, predict loss = 0.04 (77.2 examples/sec; 0.052 sec/batch; 1h:51m:13s remains)
INFO - root - 2019-11-06 20:29:40.589405: step 21220, total loss = 0.25, predict loss = 0.06 (80.8 examples/sec; 0.050 sec/batch; 1h:46m:15s remains)
INFO - root - 2019-11-06 20:29:41.160106: step 21230, total loss = 0.65, predict loss = 0.19 (79.9 examples/sec; 0.050 sec/batch; 1h:47m:29s remains)
INFO - root - 2019-11-06 20:29:41.737584: step 21240, total loss = 0.31, predict loss = 0.09 (81.3 examples/sec; 0.049 sec/batch; 1h:45m:34s remains)
INFO - root - 2019-11-06 20:29:42.323081: step 21250, total loss = 0.21, predict loss = 0.06 (80.0 examples/sec; 0.050 sec/batch; 1h:47m:16s remains)
INFO - root - 2019-11-06 20:29:42.890726: step 21260, total loss = 0.27, predict loss = 0.08 (75.7 examples/sec; 0.053 sec/batch; 1h:53m:23s remains)
INFO - root - 2019-11-06 20:29:43.469090: step 21270, total loss = 0.27, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:47m:38s remains)
INFO - root - 2019-11-06 20:29:44.045741: step 21280, total loss = 0.45, predict loss = 0.12 (78.2 examples/sec; 0.051 sec/batch; 1h:49m:47s remains)
INFO - root - 2019-11-06 20:29:44.629523: step 21290, total loss = 0.55, predict loss = 0.15 (76.8 examples/sec; 0.052 sec/batch; 1h:51m:46s remains)
INFO - root - 2019-11-06 20:29:45.254770: step 21300, total loss = 0.32, predict loss = 0.08 (66.0 examples/sec; 0.061 sec/batch; 2h:10m:01s remains)
INFO - root - 2019-11-06 20:29:45.821177: step 21310, total loss = 0.31, predict loss = 0.08 (87.2 examples/sec; 0.046 sec/batch; 1h:38m:23s remains)
INFO - root - 2019-11-06 20:29:46.298355: step 21320, total loss = 0.23, predict loss = 0.06 (95.0 examples/sec; 0.042 sec/batch; 1h:30m:17s remains)
INFO - root - 2019-11-06 20:29:46.779706: step 21330, total loss = 0.42, predict loss = 0.12 (90.8 examples/sec; 0.044 sec/batch; 1h:34m:28s remains)
INFO - root - 2019-11-06 20:29:47.709263: step 21340, total loss = 0.23, predict loss = 0.05 (75.6 examples/sec; 0.053 sec/batch; 1h:53m:30s remains)
INFO - root - 2019-11-06 20:29:48.375444: step 21350, total loss = 0.44, predict loss = 0.11 (68.8 examples/sec; 0.058 sec/batch; 2h:04m:42s remains)
INFO - root - 2019-11-06 20:29:49.046666: step 21360, total loss = 0.35, predict loss = 0.10 (66.4 examples/sec; 0.060 sec/batch; 2h:09m:12s remains)
INFO - root - 2019-11-06 20:29:49.677255: step 21370, total loss = 0.35, predict loss = 0.12 (77.0 examples/sec; 0.052 sec/batch; 1h:51m:20s remains)
INFO - root - 2019-11-06 20:29:50.265106: step 21380, total loss = 0.31, predict loss = 0.08 (74.4 examples/sec; 0.054 sec/batch; 1h:55m:15s remains)
INFO - root - 2019-11-06 20:29:50.834621: step 21390, total loss = 0.28, predict loss = 0.07 (83.4 examples/sec; 0.048 sec/batch; 1h:42m:48s remains)
INFO - root - 2019-11-06 20:29:51.400289: step 21400, total loss = 0.43, predict loss = 0.11 (77.0 examples/sec; 0.052 sec/batch; 1h:51m:16s remains)
INFO - root - 2019-11-06 20:29:51.995445: step 21410, total loss = 0.23, predict loss = 0.05 (79.9 examples/sec; 0.050 sec/batch; 1h:47m:19s remains)
INFO - root - 2019-11-06 20:29:52.571881: step 21420, total loss = 0.26, predict loss = 0.06 (76.1 examples/sec; 0.053 sec/batch; 1h:52m:36s remains)
INFO - root - 2019-11-06 20:29:53.147017: step 21430, total loss = 0.29, predict loss = 0.07 (74.5 examples/sec; 0.054 sec/batch; 1h:55m:07s remains)
INFO - root - 2019-11-06 20:29:53.719207: step 21440, total loss = 0.23, predict loss = 0.07 (78.4 examples/sec; 0.051 sec/batch; 1h:49m:15s remains)
INFO - root - 2019-11-06 20:29:54.310232: step 21450, total loss = 0.25, predict loss = 0.06 (83.6 examples/sec; 0.048 sec/batch; 1h:42m:32s remains)
INFO - root - 2019-11-06 20:29:54.857730: step 21460, total loss = 0.28, predict loss = 0.07 (92.6 examples/sec; 0.043 sec/batch; 1h:32m:30s remains)
INFO - root - 2019-11-06 20:29:55.321705: step 21470, total loss = 0.43, predict loss = 0.15 (96.5 examples/sec; 0.041 sec/batch; 1h:28m:50s remains)
INFO - root - 2019-11-06 20:29:55.785431: step 21480, total loss = 0.48, predict loss = 0.14 (89.8 examples/sec; 0.045 sec/batch; 1h:35m:26s remains)
INFO - root - 2019-11-06 20:29:56.740766: step 21490, total loss = 0.34, predict loss = 0.08 (67.7 examples/sec; 0.059 sec/batch; 2h:06m:35s remains)
INFO - root - 2019-11-06 20:29:57.394865: step 21500, total loss = 0.25, predict loss = 0.07 (77.8 examples/sec; 0.051 sec/batch; 1h:50m:07s remains)
INFO - root - 2019-11-06 20:29:57.977607: step 21510, total loss = 0.19, predict loss = 0.05 (74.5 examples/sec; 0.054 sec/batch; 1h:54m:56s remains)
INFO - root - 2019-11-06 20:29:58.556456: step 21520, total loss = 0.23, predict loss = 0.06 (74.5 examples/sec; 0.054 sec/batch; 1h:54m:57s remains)
INFO - root - 2019-11-06 20:29:59.156865: step 21530, total loss = 0.30, predict loss = 0.08 (74.6 examples/sec; 0.054 sec/batch; 1h:54m:46s remains)
INFO - root - 2019-11-06 20:29:59.724585: step 21540, total loss = 0.40, predict loss = 0.14 (76.6 examples/sec; 0.052 sec/batch; 1h:51m:46s remains)
INFO - root - 2019-11-06 20:30:00.295871: step 21550, total loss = 0.47, predict loss = 0.11 (79.8 examples/sec; 0.050 sec/batch; 1h:47m:14s remains)
INFO - root - 2019-11-06 20:30:00.865387: step 21560, total loss = 0.36, predict loss = 0.12 (82.9 examples/sec; 0.048 sec/batch; 1h:43m:20s remains)
INFO - root - 2019-11-06 20:30:01.453110: step 21570, total loss = 0.26, predict loss = 0.07 (79.5 examples/sec; 0.050 sec/batch; 1h:47m:39s remains)
INFO - root - 2019-11-06 20:30:02.029559: step 21580, total loss = 0.42, predict loss = 0.12 (79.0 examples/sec; 0.051 sec/batch; 1h:48m:20s remains)
INFO - root - 2019-11-06 20:30:02.593294: step 21590, total loss = 0.20, predict loss = 0.05 (81.5 examples/sec; 0.049 sec/batch; 1h:44m:58s remains)
INFO - root - 2019-11-06 20:30:03.167151: step 21600, total loss = 0.23, predict loss = 0.06 (73.9 examples/sec; 0.054 sec/batch; 1h:55m:53s remains)
INFO - root - 2019-11-06 20:30:03.735212: step 21610, total loss = 0.24, predict loss = 0.06 (99.2 examples/sec; 0.040 sec/batch; 1h:26m:17s remains)
INFO - root - 2019-11-06 20:30:04.189691: step 21620, total loss = 0.28, predict loss = 0.08 (93.3 examples/sec; 0.043 sec/batch; 1h:31m:46s remains)
INFO - root - 2019-11-06 20:30:04.649570: step 21630, total loss = 0.25, predict loss = 0.06 (95.2 examples/sec; 0.042 sec/batch; 1h:29m:55s remains)
INFO - root - 2019-11-06 20:30:05.674728: step 21640, total loss = 0.38, predict loss = 0.10 (56.1 examples/sec; 0.071 sec/batch; 2h:32m:32s remains)
INFO - root - 2019-11-06 20:30:06.346558: step 21650, total loss = 0.37, predict loss = 0.11 (69.0 examples/sec; 0.058 sec/batch; 2h:03m:57s remains)
INFO - root - 2019-11-06 20:30:06.940616: step 21660, total loss = 0.18, predict loss = 0.04 (78.0 examples/sec; 0.051 sec/batch; 1h:49m:38s remains)
INFO - root - 2019-11-06 20:30:07.514140: step 21670, total loss = 0.29, predict loss = 0.08 (81.3 examples/sec; 0.049 sec/batch; 1h:45m:13s remains)
INFO - root - 2019-11-06 20:30:08.080501: step 21680, total loss = 0.28, predict loss = 0.07 (81.1 examples/sec; 0.049 sec/batch; 1h:45m:28s remains)
INFO - root - 2019-11-06 20:30:08.680970: step 21690, total loss = 0.27, predict loss = 0.07 (64.0 examples/sec; 0.062 sec/batch; 2h:13m:33s remains)
INFO - root - 2019-11-06 20:30:09.251717: step 21700, total loss = 0.23, predict loss = 0.06 (80.1 examples/sec; 0.050 sec/batch; 1h:46m:44s remains)
INFO - root - 2019-11-06 20:30:09.816829: step 21710, total loss = 0.34, predict loss = 0.09 (75.0 examples/sec; 0.053 sec/batch; 1h:53m:57s remains)
INFO - root - 2019-11-06 20:30:10.395518: step 21720, total loss = 0.43, predict loss = 0.10 (76.3 examples/sec; 0.052 sec/batch; 1h:52m:06s remains)
INFO - root - 2019-11-06 20:30:10.981066: step 21730, total loss = 0.27, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:47m:19s remains)
INFO - root - 2019-11-06 20:30:11.559562: step 21740, total loss = 0.44, predict loss = 0.12 (74.4 examples/sec; 0.054 sec/batch; 1h:54m:51s remains)
INFO - root - 2019-11-06 20:30:12.128369: step 21750, total loss = 0.30, predict loss = 0.08 (81.6 examples/sec; 0.049 sec/batch; 1h:44m:47s remains)
INFO - root - 2019-11-06 20:30:12.645712: step 21760, total loss = 0.37, predict loss = 0.10 (101.1 examples/sec; 0.040 sec/batch; 1h:24m:33s remains)
INFO - root - 2019-11-06 20:30:13.121027: step 21770, total loss = 0.34, predict loss = 0.09 (94.3 examples/sec; 0.042 sec/batch; 1h:30m:36s remains)
INFO - root - 2019-11-06 20:30:13.584188: step 21780, total loss = 0.44, predict loss = 0.12 (87.4 examples/sec; 0.046 sec/batch; 1h:37m:49s remains)
INFO - root - 2019-11-06 20:30:14.677597: step 21790, total loss = 0.39, predict loss = 0.09 (48.0 examples/sec; 0.083 sec/batch; 2h:57m:59s remains)
INFO - root - 2019-11-06 20:30:15.351494: step 21800, total loss = 0.34, predict loss = 0.09 (75.5 examples/sec; 0.053 sec/batch; 1h:53m:16s remains)
INFO - root - 2019-11-06 20:30:15.943517: step 21810, total loss = 0.39, predict loss = 0.09 (79.9 examples/sec; 0.050 sec/batch; 1h:46m:54s remains)
INFO - root - 2019-11-06 20:30:16.502558: step 21820, total loss = 0.48, predict loss = 0.13 (77.1 examples/sec; 0.052 sec/batch; 1h:50m:51s remains)
INFO - root - 2019-11-06 20:30:17.077905: step 21830, total loss = 0.92, predict loss = 0.26 (81.9 examples/sec; 0.049 sec/batch; 1h:44m:23s remains)
INFO - root - 2019-11-06 20:30:17.654816: step 21840, total loss = 0.38, predict loss = 0.09 (77.2 examples/sec; 0.052 sec/batch; 1h:50m:43s remains)
INFO - root - 2019-11-06 20:30:18.237035: step 21850, total loss = 0.21, predict loss = 0.05 (79.7 examples/sec; 0.050 sec/batch; 1h:47m:12s remains)
INFO - root - 2019-11-06 20:30:18.820882: step 21860, total loss = 0.42, predict loss = 0.10 (78.4 examples/sec; 0.051 sec/batch; 1h:48m:55s remains)
INFO - root - 2019-11-06 20:30:19.397744: step 21870, total loss = 0.42, predict loss = 0.13 (76.1 examples/sec; 0.053 sec/batch; 1h:52m:13s remains)
INFO - root - 2019-11-06 20:30:19.971070: step 21880, total loss = 0.37, predict loss = 0.10 (78.7 examples/sec; 0.051 sec/batch; 1h:48m:32s remains)
INFO - root - 2019-11-06 20:30:20.564597: step 21890, total loss = 0.27, predict loss = 0.07 (76.2 examples/sec; 0.052 sec/batch; 1h:52m:04s remains)
INFO - root - 2019-11-06 20:30:21.124594: step 21900, total loss = 0.24, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:50m:37s remains)
INFO - root - 2019-11-06 20:30:21.617948: step 21910, total loss = 0.58, predict loss = 0.15 (96.6 examples/sec; 0.041 sec/batch; 1h:28m:25s remains)
INFO - root - 2019-11-06 20:30:22.071820: step 21920, total loss = 0.29, predict loss = 0.08 (96.2 examples/sec; 0.042 sec/batch; 1h:28m:44s remains)
INFO - root - 2019-11-06 20:30:22.551340: step 21930, total loss = 0.24, predict loss = 0.06 (94.0 examples/sec; 0.043 sec/batch; 1h:30m:48s remains)
INFO - root - 2019-11-06 20:30:23.628391: step 21940, total loss = 0.42, predict loss = 0.10 (60.2 examples/sec; 0.066 sec/batch; 2h:21m:52s remains)
INFO - root - 2019-11-06 20:30:24.246558: step 21950, total loss = 0.48, predict loss = 0.15 (78.6 examples/sec; 0.051 sec/batch; 1h:48m:37s remains)
INFO - root - 2019-11-06 20:30:24.815553: step 21960, total loss = 0.41, predict loss = 0.12 (81.6 examples/sec; 0.049 sec/batch; 1h:44m:38s remains)
INFO - root - 2019-11-06 20:30:25.388719: step 21970, total loss = 0.42, predict loss = 0.11 (79.5 examples/sec; 0.050 sec/batch; 1h:47m:20s remains)
INFO - root - 2019-11-06 20:30:25.970836: step 21980, total loss = 0.44, predict loss = 0.14 (79.4 examples/sec; 0.050 sec/batch; 1h:47m:26s remains)
INFO - root - 2019-11-06 20:30:26.538781: step 21990, total loss = 0.25, predict loss = 0.06 (81.1 examples/sec; 0.049 sec/batch; 1h:45m:11s remains)
INFO - root - 2019-11-06 20:30:27.111659: step 22000, total loss = 0.19, predict loss = 0.04 (80.4 examples/sec; 0.050 sec/batch; 1h:46m:07s remains)
INFO - root - 2019-11-06 20:30:27.700912: step 22010, total loss = 0.28, predict loss = 0.08 (80.6 examples/sec; 0.050 sec/batch; 1h:45m:48s remains)
INFO - root - 2019-11-06 20:30:28.269817: step 22020, total loss = 0.29, predict loss = 0.08 (79.7 examples/sec; 0.050 sec/batch; 1h:47m:03s remains)
INFO - root - 2019-11-06 20:30:28.847734: step 22030, total loss = 0.28, predict loss = 0.08 (79.8 examples/sec; 0.050 sec/batch; 1h:46m:58s remains)
INFO - root - 2019-11-06 20:30:29.427025: step 22040, total loss = 0.31, predict loss = 0.11 (80.5 examples/sec; 0.050 sec/batch; 1h:46m:01s remains)
INFO - root - 2019-11-06 20:30:30.020537: step 22050, total loss = 0.27, predict loss = 0.07 (73.3 examples/sec; 0.055 sec/batch; 1h:56m:26s remains)
INFO - root - 2019-11-06 20:30:30.494042: step 22060, total loss = 0.39, predict loss = 0.11 (93.8 examples/sec; 0.043 sec/batch; 1h:30m:56s remains)
INFO - root - 2019-11-06 20:30:30.938819: step 22070, total loss = 0.34, predict loss = 0.10 (96.0 examples/sec; 0.042 sec/batch; 1h:28m:48s remains)
INFO - root - 2019-11-06 20:30:31.858663: step 22080, total loss = 0.30, predict loss = 0.08 (78.2 examples/sec; 0.051 sec/batch; 1h:48m:59s remains)
INFO - root - 2019-11-06 20:30:32.567083: step 22090, total loss = 0.40, predict loss = 0.10 (57.5 examples/sec; 0.070 sec/batch; 2h:28m:23s remains)
INFO - root - 2019-11-06 20:30:33.180046: step 22100, total loss = 0.39, predict loss = 0.09 (79.9 examples/sec; 0.050 sec/batch; 1h:46m:41s remains)
INFO - root - 2019-11-06 20:30:33.742908: step 22110, total loss = 0.21, predict loss = 0.05 (82.7 examples/sec; 0.048 sec/batch; 1h:43m:05s remains)
INFO - root - 2019-11-06 20:30:34.312559: step 22120, total loss = 0.27, predict loss = 0.07 (76.5 examples/sec; 0.052 sec/batch; 1h:51m:30s remains)
INFO - root - 2019-11-06 20:30:34.899139: step 22130, total loss = 0.36, predict loss = 0.09 (76.4 examples/sec; 0.052 sec/batch; 1h:51m:36s remains)
INFO - root - 2019-11-06 20:30:35.476417: step 22140, total loss = 0.41, predict loss = 0.11 (75.3 examples/sec; 0.053 sec/batch; 1h:53m:13s remains)
INFO - root - 2019-11-06 20:30:36.043253: step 22150, total loss = 0.40, predict loss = 0.10 (77.6 examples/sec; 0.052 sec/batch; 1h:49m:50s remains)
INFO - root - 2019-11-06 20:30:36.604680: step 22160, total loss = 0.22, predict loss = 0.06 (83.0 examples/sec; 0.048 sec/batch; 1h:42m:44s remains)
INFO - root - 2019-11-06 20:30:37.187950: step 22170, total loss = 0.36, predict loss = 0.10 (79.4 examples/sec; 0.050 sec/batch; 1h:47m:19s remains)
INFO - root - 2019-11-06 20:30:37.747382: step 22180, total loss = 0.33, predict loss = 0.10 (79.6 examples/sec; 0.050 sec/batch; 1h:46m:59s remains)
INFO - root - 2019-11-06 20:30:38.320431: step 22190, total loss = 0.29, predict loss = 0.07 (79.2 examples/sec; 0.051 sec/batch; 1h:47m:37s remains)
INFO - root - 2019-11-06 20:30:38.885804: step 22200, total loss = 0.22, predict loss = 0.06 (88.8 examples/sec; 0.045 sec/batch; 1h:35m:59s remains)
INFO - root - 2019-11-06 20:30:39.359752: step 22210, total loss = 0.22, predict loss = 0.05 (99.7 examples/sec; 0.040 sec/batch; 1h:25m:26s remains)
INFO - root - 2019-11-06 20:30:39.826574: step 22220, total loss = 0.31, predict loss = 0.11 (94.8 examples/sec; 0.042 sec/batch; 1h:29m:54s remains)
INFO - root - 2019-11-06 20:30:40.731250: step 22230, total loss = 0.28, predict loss = 0.07 (71.0 examples/sec; 0.056 sec/batch; 1h:59m:56s remains)
INFO - root - 2019-11-06 20:30:41.414658: step 22240, total loss = 0.21, predict loss = 0.05 (68.1 examples/sec; 0.059 sec/batch; 2h:05m:08s remains)
INFO - root - 2019-11-06 20:30:42.032715: step 22250, total loss = 0.36, predict loss = 0.10 (80.0 examples/sec; 0.050 sec/batch; 1h:46m:25s remains)
INFO - root - 2019-11-06 20:30:42.600759: step 22260, total loss = 0.43, predict loss = 0.14 (80.9 examples/sec; 0.049 sec/batch; 1h:45m:16s remains)
INFO - root - 2019-11-06 20:30:43.163930: step 22270, total loss = 0.33, predict loss = 0.09 (80.1 examples/sec; 0.050 sec/batch; 1h:46m:21s remains)
INFO - root - 2019-11-06 20:30:43.744049: step 22280, total loss = 0.34, predict loss = 0.08 (79.3 examples/sec; 0.050 sec/batch; 1h:47m:23s remains)
INFO - root - 2019-11-06 20:30:44.344928: step 22290, total loss = 0.43, predict loss = 0.11 (75.6 examples/sec; 0.053 sec/batch; 1h:52m:38s remains)
INFO - root - 2019-11-06 20:30:44.923133: step 22300, total loss = 0.39, predict loss = 0.12 (81.6 examples/sec; 0.049 sec/batch; 1h:44m:20s remains)
INFO - root - 2019-11-06 20:30:45.534010: step 22310, total loss = 0.24, predict loss = 0.07 (77.9 examples/sec; 0.051 sec/batch; 1h:49m:17s remains)
INFO - root - 2019-11-06 20:30:46.119177: step 22320, total loss = 0.22, predict loss = 0.05 (75.4 examples/sec; 0.053 sec/batch; 1h:52m:52s remains)
INFO - root - 2019-11-06 20:30:46.714278: step 22330, total loss = 0.27, predict loss = 0.06 (77.0 examples/sec; 0.052 sec/batch; 1h:50m:29s remains)
INFO - root - 2019-11-06 20:30:47.286893: step 22340, total loss = 0.23, predict loss = 0.05 (74.4 examples/sec; 0.054 sec/batch; 1h:54m:27s remains)
INFO - root - 2019-11-06 20:30:47.839400: step 22350, total loss = 0.32, predict loss = 0.09 (92.7 examples/sec; 0.043 sec/batch; 1h:31m:46s remains)
INFO - root - 2019-11-06 20:30:48.302109: step 22360, total loss = 0.49, predict loss = 0.13 (94.1 examples/sec; 0.043 sec/batch; 1h:30m:25s remains)
INFO - root - 2019-11-06 20:30:48.787358: step 22370, total loss = 0.41, predict loss = 0.12 (98.2 examples/sec; 0.041 sec/batch; 1h:26m:40s remains)
INFO - root - 2019-11-06 20:30:49.767549: step 22380, total loss = 0.18, predict loss = 0.04 (57.8 examples/sec; 0.069 sec/batch; 2h:27m:16s remains)
INFO - root - 2019-11-06 20:30:50.416349: step 22390, total loss = 0.29, predict loss = 0.07 (77.7 examples/sec; 0.051 sec/batch; 1h:49m:29s remains)
INFO - root - 2019-11-06 20:30:50.989626: step 22400, total loss = 0.45, predict loss = 0.13 (78.0 examples/sec; 0.051 sec/batch; 1h:49m:00s remains)
INFO - root - 2019-11-06 20:30:51.593461: step 22410, total loss = 0.39, predict loss = 0.10 (79.9 examples/sec; 0.050 sec/batch; 1h:46m:26s remains)
INFO - root - 2019-11-06 20:30:52.161633: step 22420, total loss = 0.40, predict loss = 0.11 (83.2 examples/sec; 0.048 sec/batch; 1h:42m:15s remains)
INFO - root - 2019-11-06 20:30:52.737159: step 22430, total loss = 0.42, predict loss = 0.10 (81.3 examples/sec; 0.049 sec/batch; 1h:44m:38s remains)
INFO - root - 2019-11-06 20:30:53.305821: step 22440, total loss = 0.22, predict loss = 0.06 (78.4 examples/sec; 0.051 sec/batch; 1h:48m:31s remains)
INFO - root - 2019-11-06 20:30:53.896408: step 22450, total loss = 0.27, predict loss = 0.08 (77.2 examples/sec; 0.052 sec/batch; 1h:50m:04s remains)
INFO - root - 2019-11-06 20:30:54.467500: step 22460, total loss = 0.23, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:46m:36s remains)
INFO - root - 2019-11-06 20:30:55.045351: step 22470, total loss = 0.35, predict loss = 0.09 (76.3 examples/sec; 0.052 sec/batch; 1h:51m:26s remains)
INFO - root - 2019-11-06 20:30:55.601537: step 22480, total loss = 0.28, predict loss = 0.08 (83.5 examples/sec; 0.048 sec/batch; 1h:41m:46s remains)
INFO - root - 2019-11-06 20:30:56.173990: step 22490, total loss = 0.32, predict loss = 0.08 (79.2 examples/sec; 0.051 sec/batch; 1h:47m:22s remains)
INFO - root - 2019-11-06 20:30:56.693366: step 22500, total loss = 0.35, predict loss = 0.10 (92.8 examples/sec; 0.043 sec/batch; 1h:31m:35s remains)
INFO - root - 2019-11-06 20:30:57.159686: step 22510, total loss = 0.29, predict loss = 0.08 (94.7 examples/sec; 0.042 sec/batch; 1h:29m:42s remains)
INFO - root - 2019-11-06 20:30:57.610613: step 22520, total loss = 0.38, predict loss = 0.08 (97.1 examples/sec; 0.041 sec/batch; 1h:27m:32s remains)
INFO - root - 2019-11-06 20:30:58.633638: step 22530, total loss = 0.20, predict loss = 0.05 (65.0 examples/sec; 0.062 sec/batch; 2h:10m:45s remains)
INFO - root - 2019-11-06 20:30:59.240815: step 22540, total loss = 0.28, predict loss = 0.07 (72.1 examples/sec; 0.055 sec/batch; 1h:57m:51s remains)
INFO - root - 2019-11-06 20:30:59.807530: step 22550, total loss = 0.23, predict loss = 0.06 (80.5 examples/sec; 0.050 sec/batch; 1h:45m:32s remains)
INFO - root - 2019-11-06 20:31:00.369946: step 22560, total loss = 0.27, predict loss = 0.07 (79.1 examples/sec; 0.051 sec/batch; 1h:47m:24s remains)
INFO - root - 2019-11-06 20:31:00.972291: step 22570, total loss = 0.27, predict loss = 0.08 (76.6 examples/sec; 0.052 sec/batch; 1h:50m:58s remains)
INFO - root - 2019-11-06 20:31:01.556984: step 22580, total loss = 0.43, predict loss = 0.11 (77.8 examples/sec; 0.051 sec/batch; 1h:49m:10s remains)
INFO - root - 2019-11-06 20:31:02.124566: step 22590, total loss = 0.31, predict loss = 0.08 (79.8 examples/sec; 0.050 sec/batch; 1h:46m:26s remains)
INFO - root - 2019-11-06 20:31:02.689207: step 22600, total loss = 0.69, predict loss = 0.18 (79.1 examples/sec; 0.051 sec/batch; 1h:47m:22s remains)
INFO - root - 2019-11-06 20:31:03.284131: step 22610, total loss = 0.49, predict loss = 0.13 (76.5 examples/sec; 0.052 sec/batch; 1h:51m:04s remains)
INFO - root - 2019-11-06 20:31:03.857913: step 22620, total loss = 0.27, predict loss = 0.07 (77.8 examples/sec; 0.051 sec/batch; 1h:49m:06s remains)
INFO - root - 2019-11-06 20:31:04.435288: step 22630, total loss = 0.27, predict loss = 0.07 (78.3 examples/sec; 0.051 sec/batch; 1h:48m:23s remains)
INFO - root - 2019-11-06 20:31:05.011722: step 22640, total loss = 0.31, predict loss = 0.08 (78.5 examples/sec; 0.051 sec/batch; 1h:48m:07s remains)
INFO - root - 2019-11-06 20:31:05.527182: step 22650, total loss = 0.23, predict loss = 0.05 (103.6 examples/sec; 0.039 sec/batch; 1h:21m:57s remains)
INFO - root - 2019-11-06 20:31:05.976702: step 22660, total loss = 0.25, predict loss = 0.06 (94.6 examples/sec; 0.042 sec/batch; 1h:29m:42s remains)
INFO - root - 2019-11-06 20:31:06.427802: step 22670, total loss = 0.46, predict loss = 0.15 (96.1 examples/sec; 0.042 sec/batch; 1h:28m:21s remains)
INFO - root - 2019-11-06 20:31:07.517341: step 22680, total loss = 0.20, predict loss = 0.05 (56.1 examples/sec; 0.071 sec/batch; 2h:31m:17s remains)
INFO - root - 2019-11-06 20:31:08.154985: step 22690, total loss = 0.24, predict loss = 0.06 (80.9 examples/sec; 0.049 sec/batch; 1h:44m:55s remains)
INFO - root - 2019-11-06 20:31:08.718896: step 22700, total loss = 0.26, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:46m:18s remains)
INFO - root - 2019-11-06 20:31:09.301565: step 22710, total loss = 0.24, predict loss = 0.06 (81.3 examples/sec; 0.049 sec/batch; 1h:44m:21s remains)
INFO - root - 2019-11-06 20:31:09.880246: step 22720, total loss = 0.23, predict loss = 0.06 (78.7 examples/sec; 0.051 sec/batch; 1h:47m:48s remains)
INFO - root - 2019-11-06 20:31:10.463110: step 22730, total loss = 0.34, predict loss = 0.10 (79.3 examples/sec; 0.050 sec/batch; 1h:46m:59s remains)
INFO - root - 2019-11-06 20:31:11.034434: step 22740, total loss = 0.19, predict loss = 0.05 (78.2 examples/sec; 0.051 sec/batch; 1h:48m:28s remains)
INFO - root - 2019-11-06 20:31:11.623815: step 22750, total loss = 0.28, predict loss = 0.07 (74.7 examples/sec; 0.054 sec/batch; 1h:53m:36s remains)
INFO - root - 2019-11-06 20:31:12.178311: step 22760, total loss = 0.26, predict loss = 0.06 (76.7 examples/sec; 0.052 sec/batch; 1h:50m:34s remains)
INFO - root - 2019-11-06 20:31:12.774715: step 22770, total loss = 0.31, predict loss = 0.08 (77.4 examples/sec; 0.052 sec/batch; 1h:49m:36s remains)
INFO - root - 2019-11-06 20:31:13.353506: step 22780, total loss = 0.37, predict loss = 0.10 (80.0 examples/sec; 0.050 sec/batch; 1h:46m:00s remains)
INFO - root - 2019-11-06 20:31:13.927243: step 22790, total loss = 0.32, predict loss = 0.08 (75.1 examples/sec; 0.053 sec/batch; 1h:52m:52s remains)
INFO - root - 2019-11-06 20:31:14.400283: step 22800, total loss = 0.28, predict loss = 0.06 (101.3 examples/sec; 0.039 sec/batch; 1h:23m:40s remains)
INFO - root - 2019-11-06 20:31:14.881828: step 22810, total loss = 0.40, predict loss = 0.15 (91.3 examples/sec; 0.044 sec/batch; 1h:32m:53s remains)
INFO - root - 2019-11-06 20:31:15.794716: step 22820, total loss = 0.28, predict loss = 0.07 (8.4 examples/sec; 0.479 sec/batch; 16h:55m:09s remains)
INFO - root - 2019-11-06 20:31:16.480417: step 22830, total loss = 0.25, predict loss = 0.05 (55.4 examples/sec; 0.072 sec/batch; 2h:32m:57s remains)
INFO - root - 2019-11-06 20:31:17.105186: step 22840, total loss = 0.34, predict loss = 0.09 (83.1 examples/sec; 0.048 sec/batch; 1h:42m:00s remains)
INFO - root - 2019-11-06 20:31:17.696099: step 22850, total loss = 0.32, predict loss = 0.08 (79.8 examples/sec; 0.050 sec/batch; 1h:46m:16s remains)
INFO - root - 2019-11-06 20:31:18.265436: step 22860, total loss = 0.24, predict loss = 0.05 (76.4 examples/sec; 0.052 sec/batch; 1h:50m:55s remains)
INFO - root - 2019-11-06 20:31:18.837274: step 22870, total loss = 0.33, predict loss = 0.10 (76.4 examples/sec; 0.052 sec/batch; 1h:50m:52s remains)
INFO - root - 2019-11-06 20:31:19.424123: step 22880, total loss = 0.22, predict loss = 0.05 (79.4 examples/sec; 0.050 sec/batch; 1h:46m:42s remains)
INFO - root - 2019-11-06 20:31:20.007681: step 22890, total loss = 0.31, predict loss = 0.08 (79.9 examples/sec; 0.050 sec/batch; 1h:46m:02s remains)
INFO - root - 2019-11-06 20:31:20.585649: step 22900, total loss = 0.22, predict loss = 0.06 (82.7 examples/sec; 0.048 sec/batch; 1h:42m:29s remains)
INFO - root - 2019-11-06 20:31:21.160105: step 22910, total loss = 0.17, predict loss = 0.04 (75.4 examples/sec; 0.053 sec/batch; 1h:52m:22s remains)
INFO - root - 2019-11-06 20:31:21.736492: step 22920, total loss = 0.25, predict loss = 0.06 (80.2 examples/sec; 0.050 sec/batch; 1h:45m:36s remains)
INFO - root - 2019-11-06 20:31:22.326434: step 22930, total loss = 0.27, predict loss = 0.07 (78.0 examples/sec; 0.051 sec/batch; 1h:48m:34s remains)
INFO - root - 2019-11-06 20:31:22.889850: step 22940, total loss = 0.26, predict loss = 0.08 (86.7 examples/sec; 0.046 sec/batch; 1h:37m:40s remains)
INFO - root - 2019-11-06 20:31:23.347422: step 22950, total loss = 0.34, predict loss = 0.10 (96.1 examples/sec; 0.042 sec/batch; 1h:28m:07s remains)
INFO - root - 2019-11-06 20:31:23.803808: step 22960, total loss = 0.29, predict loss = 0.08 (93.7 examples/sec; 0.043 sec/batch; 1h:30m:25s remains)
INFO - root - 2019-11-06 20:31:24.727510: step 22970, total loss = 0.24, predict loss = 0.07 (74.0 examples/sec; 0.054 sec/batch; 1h:54m:24s remains)
INFO - root - 2019-11-06 20:31:25.470673: step 22980, total loss = 0.34, predict loss = 0.10 (58.2 examples/sec; 0.069 sec/batch; 2h:25m:30s remains)
INFO - root - 2019-11-06 20:31:26.107307: step 22990, total loss = 0.20, predict loss = 0.05 (72.4 examples/sec; 0.055 sec/batch; 1h:56m:57s remains)
INFO - root - 2019-11-06 20:31:26.683479: step 23000, total loss = 0.27, predict loss = 0.08 (75.9 examples/sec; 0.053 sec/batch; 1h:51m:37s remains)
INFO - root - 2019-11-06 20:31:27.262863: step 23010, total loss = 0.32, predict loss = 0.09 (81.1 examples/sec; 0.049 sec/batch; 1h:44m:20s remains)
INFO - root - 2019-11-06 20:31:27.825863: step 23020, total loss = 0.42, predict loss = 0.11 (83.0 examples/sec; 0.048 sec/batch; 1h:41m:59s remains)
INFO - root - 2019-11-06 20:31:28.406907: step 23030, total loss = 0.25, predict loss = 0.07 (78.2 examples/sec; 0.051 sec/batch; 1h:48m:15s remains)
INFO - root - 2019-11-06 20:31:28.976833: step 23040, total loss = 0.35, predict loss = 0.08 (74.7 examples/sec; 0.054 sec/batch; 1h:53m:19s remains)
INFO - root - 2019-11-06 20:31:29.559038: step 23050, total loss = 0.31, predict loss = 0.08 (78.1 examples/sec; 0.051 sec/batch; 1h:48m:18s remains)
INFO - root - 2019-11-06 20:31:30.135193: step 23060, total loss = 0.28, predict loss = 0.08 (76.8 examples/sec; 0.052 sec/batch; 1h:50m:14s remains)
INFO - root - 2019-11-06 20:31:30.700723: step 23070, total loss = 0.19, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:47m:21s remains)
INFO - root - 2019-11-06 20:31:31.280206: step 23080, total loss = 0.19, predict loss = 0.05 (79.6 examples/sec; 0.050 sec/batch; 1h:46m:20s remains)
INFO - root - 2019-11-06 20:31:31.853072: step 23090, total loss = 0.16, predict loss = 0.04 (93.8 examples/sec; 0.043 sec/batch; 1h:30m:12s remains)
INFO - root - 2019-11-06 20:31:32.307209: step 23100, total loss = 0.34, predict loss = 0.10 (92.8 examples/sec; 0.043 sec/batch; 1h:31m:11s remains)
INFO - root - 2019-11-06 20:31:32.761589: step 23110, total loss = 0.29, predict loss = 0.07 (97.8 examples/sec; 0.041 sec/batch; 1h:26m:28s remains)
INFO - root - 2019-11-06 20:31:33.727086: step 23120, total loss = 0.28, predict loss = 0.07 (65.9 examples/sec; 0.061 sec/batch; 2h:08m:21s remains)
INFO - root - 2019-11-06 20:31:34.369389: step 23130, total loss = 0.50, predict loss = 0.15 (77.4 examples/sec; 0.052 sec/batch; 1h:49m:13s remains)
INFO - root - 2019-11-06 20:31:34.953248: step 23140, total loss = 0.20, predict loss = 0.05 (76.2 examples/sec; 0.053 sec/batch; 1h:51m:02s remains)
INFO - root - 2019-11-06 20:31:35.515608: step 23150, total loss = 0.28, predict loss = 0.07 (77.8 examples/sec; 0.051 sec/batch; 1h:48m:43s remains)
INFO - root - 2019-11-06 20:31:36.083809: step 23160, total loss = 0.21, predict loss = 0.05 (79.2 examples/sec; 0.051 sec/batch; 1h:46m:47s remains)
INFO - root - 2019-11-06 20:31:36.677707: step 23170, total loss = 0.16, predict loss = 0.04 (77.3 examples/sec; 0.052 sec/batch; 1h:49m:26s remains)
INFO - root - 2019-11-06 20:31:37.246614: step 23180, total loss = 0.27, predict loss = 0.07 (78.1 examples/sec; 0.051 sec/batch; 1h:48m:15s remains)
INFO - root - 2019-11-06 20:31:37.822410: step 23190, total loss = 0.25, predict loss = 0.06 (75.5 examples/sec; 0.053 sec/batch; 1h:51m:56s remains)
INFO - root - 2019-11-06 20:31:38.395145: step 23200, total loss = 0.22, predict loss = 0.06 (81.1 examples/sec; 0.049 sec/batch; 1h:44m:14s remains)
INFO - root - 2019-11-06 20:31:38.986961: step 23210, total loss = 0.44, predict loss = 0.10 (76.6 examples/sec; 0.052 sec/batch; 1h:50m:22s remains)
INFO - root - 2019-11-06 20:31:39.556342: step 23220, total loss = 0.28, predict loss = 0.07 (79.8 examples/sec; 0.050 sec/batch; 1h:45m:54s remains)
INFO - root - 2019-11-06 20:31:40.112884: step 23230, total loss = 0.53, predict loss = 0.15 (79.8 examples/sec; 0.050 sec/batch; 1h:45m:53s remains)
INFO - root - 2019-11-06 20:31:40.659230: step 23240, total loss = 0.39, predict loss = 0.10 (94.1 examples/sec; 0.042 sec/batch; 1h:29m:46s remains)
INFO - root - 2019-11-06 20:31:41.128248: step 23250, total loss = 0.25, predict loss = 0.06 (95.8 examples/sec; 0.042 sec/batch; 1h:28m:12s remains)
INFO - root - 2019-11-06 20:31:41.579236: step 23260, total loss = 0.19, predict loss = 0.05 (95.1 examples/sec; 0.042 sec/batch; 1h:28m:49s remains)
INFO - root - 2019-11-06 20:31:42.563668: step 23270, total loss = 0.38, predict loss = 0.09 (60.3 examples/sec; 0.066 sec/batch; 2h:20m:10s remains)
INFO - root - 2019-11-06 20:31:43.185457: step 23280, total loss = 0.23, predict loss = 0.06 (81.7 examples/sec; 0.049 sec/batch; 1h:43m:25s remains)
INFO - root - 2019-11-06 20:31:43.772389: step 23290, total loss = 0.31, predict loss = 0.08 (76.4 examples/sec; 0.052 sec/batch; 1h:50m:38s remains)
INFO - root - 2019-11-06 20:31:44.331130: step 23300, total loss = 0.28, predict loss = 0.07 (77.2 examples/sec; 0.052 sec/batch; 1h:49m:22s remains)
INFO - root - 2019-11-06 20:31:44.903839: step 23310, total loss = 0.43, predict loss = 0.10 (76.4 examples/sec; 0.052 sec/batch; 1h:50m:29s remains)
INFO - root - 2019-11-06 20:31:45.518149: step 23320, total loss = 0.18, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:46m:27s remains)
INFO - root - 2019-11-06 20:31:46.103122: step 23330, total loss = 0.21, predict loss = 0.06 (81.9 examples/sec; 0.049 sec/batch; 1h:43m:04s remains)
INFO - root - 2019-11-06 20:31:46.672159: step 23340, total loss = 0.27, predict loss = 0.07 (76.9 examples/sec; 0.052 sec/batch; 1h:49m:44s remains)
INFO - root - 2019-11-06 20:31:47.243980: step 23350, total loss = 0.54, predict loss = 0.14 (82.3 examples/sec; 0.049 sec/batch; 1h:42m:35s remains)
INFO - root - 2019-11-06 20:31:47.814322: step 23360, total loss = 0.53, predict loss = 0.15 (76.9 examples/sec; 0.052 sec/batch; 1h:49m:49s remains)
INFO - root - 2019-11-06 20:31:48.398100: step 23370, total loss = 0.35, predict loss = 0.10 (79.2 examples/sec; 0.050 sec/batch; 1h:46m:32s remains)
INFO - root - 2019-11-06 20:31:48.970067: step 23380, total loss = 0.25, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 1h:48m:52s remains)
INFO - root - 2019-11-06 20:31:49.483786: step 23390, total loss = 0.35, predict loss = 0.09 (104.8 examples/sec; 0.038 sec/batch; 1h:20m:30s remains)
INFO - root - 2019-11-06 20:31:49.924598: step 23400, total loss = 0.25, predict loss = 0.06 (107.9 examples/sec; 0.037 sec/batch; 1h:18m:14s remains)
INFO - root - 2019-11-06 20:31:50.404191: step 23410, total loss = 0.34, predict loss = 0.10 (95.7 examples/sec; 0.042 sec/batch; 1h:28m:08s remains)
INFO - root - 2019-11-06 20:31:51.492521: step 23420, total loss = 0.42, predict loss = 0.12 (55.5 examples/sec; 0.072 sec/batch; 2h:32m:07s remains)
INFO - root - 2019-11-06 20:31:52.137283: step 23430, total loss = 0.25, predict loss = 0.06 (76.6 examples/sec; 0.052 sec/batch; 1h:50m:09s remains)
INFO - root - 2019-11-06 20:31:52.707770: step 23440, total loss = 0.22, predict loss = 0.06 (78.0 examples/sec; 0.051 sec/batch; 1h:48m:12s remains)
INFO - root - 2019-11-06 20:31:53.300635: step 23450, total loss = 0.19, predict loss = 0.05 (76.3 examples/sec; 0.052 sec/batch; 1h:50m:34s remains)
INFO - root - 2019-11-06 20:31:53.867908: step 23460, total loss = 0.26, predict loss = 0.06 (79.5 examples/sec; 0.050 sec/batch; 1h:46m:09s remains)
INFO - root - 2019-11-06 20:31:54.434482: step 23470, total loss = 0.27, predict loss = 0.07 (75.4 examples/sec; 0.053 sec/batch; 1h:51m:54s remains)
INFO - root - 2019-11-06 20:31:55.003552: step 23480, total loss = 0.33, predict loss = 0.09 (81.6 examples/sec; 0.049 sec/batch; 1h:43m:24s remains)
INFO - root - 2019-11-06 20:31:55.590210: step 23490, total loss = 0.30, predict loss = 0.08 (77.8 examples/sec; 0.051 sec/batch; 1h:48m:25s remains)
INFO - root - 2019-11-06 20:31:56.162290: step 23500, total loss = 0.28, predict loss = 0.07 (73.0 examples/sec; 0.055 sec/batch; 1h:55m:32s remains)
INFO - root - 2019-11-06 20:31:56.734219: step 23510, total loss = 0.34, predict loss = 0.08 (79.4 examples/sec; 0.050 sec/batch; 1h:46m:15s remains)
INFO - root - 2019-11-06 20:31:57.305791: step 23520, total loss = 0.34, predict loss = 0.10 (76.8 examples/sec; 0.052 sec/batch; 1h:49m:43s remains)
INFO - root - 2019-11-06 20:31:57.901658: step 23530, total loss = 0.40, predict loss = 0.10 (73.7 examples/sec; 0.054 sec/batch; 1h:54m:23s remains)
INFO - root - 2019-11-06 20:31:58.398614: step 23540, total loss = 0.27, predict loss = 0.07 (101.1 examples/sec; 0.040 sec/batch; 1h:23m:23s remains)
INFO - root - 2019-11-06 20:31:58.843350: step 23550, total loss = 0.35, predict loss = 0.08 (99.4 examples/sec; 0.040 sec/batch; 1h:24m:49s remains)
INFO - root - 2019-11-06 20:31:59.299310: step 23560, total loss = 0.26, predict loss = 0.07 (93.9 examples/sec; 0.043 sec/batch; 1h:29m:44s remains)
INFO - root - 2019-11-06 20:32:00.391385: step 23570, total loss = 0.21, predict loss = 0.05 (61.1 examples/sec; 0.065 sec/batch; 2h:17m:57s remains)
INFO - root - 2019-11-06 20:32:00.987400: step 23580, total loss = 0.17, predict loss = 0.05 (78.3 examples/sec; 0.051 sec/batch; 1h:47m:37s remains)
INFO - root - 2019-11-06 20:32:01.567808: step 23590, total loss = 0.32, predict loss = 0.09 (76.6 examples/sec; 0.052 sec/batch; 1h:50m:02s remains)
INFO - root - 2019-11-06 20:32:02.130809: step 23600, total loss = 0.24, predict loss = 0.07 (80.1 examples/sec; 0.050 sec/batch; 1h:45m:15s remains)
INFO - root - 2019-11-06 20:32:02.721195: step 23610, total loss = 0.40, predict loss = 0.12 (78.0 examples/sec; 0.051 sec/batch; 1h:48m:05s remains)
INFO - root - 2019-11-06 20:32:03.299525: step 23620, total loss = 0.26, predict loss = 0.07 (79.6 examples/sec; 0.050 sec/batch; 1h:45m:47s remains)
INFO - root - 2019-11-06 20:32:03.868201: step 23630, total loss = 0.33, predict loss = 0.10 (79.4 examples/sec; 0.050 sec/batch; 1h:46m:07s remains)
INFO - root - 2019-11-06 20:32:04.440100: step 23640, total loss = 0.37, predict loss = 0.12 (82.3 examples/sec; 0.049 sec/batch; 1h:42m:22s remains)
INFO - root - 2019-11-06 20:32:05.025770: step 23650, total loss = 0.23, predict loss = 0.06 (79.6 examples/sec; 0.050 sec/batch; 1h:45m:51s remains)
INFO - root - 2019-11-06 20:32:05.601753: step 23660, total loss = 0.28, predict loss = 0.07 (78.4 examples/sec; 0.051 sec/batch; 1h:47m:23s remains)
INFO - root - 2019-11-06 20:32:06.181091: step 23670, total loss = 0.21, predict loss = 0.06 (80.8 examples/sec; 0.050 sec/batch; 1h:44m:13s remains)
INFO - root - 2019-11-06 20:32:06.751778: step 23680, total loss = 0.21, predict loss = 0.05 (80.1 examples/sec; 0.050 sec/batch; 1h:45m:05s remains)
INFO - root - 2019-11-06 20:32:07.231567: step 23690, total loss = 0.45, predict loss = 0.14 (97.4 examples/sec; 0.041 sec/batch; 1h:26m:29s remains)
INFO - root - 2019-11-06 20:32:07.682193: step 23700, total loss = 0.29, predict loss = 0.08 (96.7 examples/sec; 0.041 sec/batch; 1h:27m:03s remains)
INFO - root - 2019-11-06 20:32:08.597228: step 23710, total loss = 0.32, predict loss = 0.09 (79.9 examples/sec; 0.050 sec/batch; 1h:45m:21s remains)
INFO - root - 2019-11-06 20:32:09.204316: step 23720, total loss = 0.19, predict loss = 0.05 (72.0 examples/sec; 0.056 sec/batch; 1h:56m:51s remains)
INFO - root - 2019-11-06 20:32:09.850114: step 23730, total loss = 0.26, predict loss = 0.06 (76.6 examples/sec; 0.052 sec/batch; 1h:49m:50s remains)
INFO - root - 2019-11-06 20:32:10.426622: step 23740, total loss = 0.25, predict loss = 0.06 (80.3 examples/sec; 0.050 sec/batch; 1h:44m:48s remains)
INFO - root - 2019-11-06 20:32:11.006182: step 23750, total loss = 0.22, predict loss = 0.05 (76.0 examples/sec; 0.053 sec/batch; 1h:50m:48s remains)
INFO - root - 2019-11-06 20:32:11.574537: step 23760, total loss = 0.32, predict loss = 0.09 (75.2 examples/sec; 0.053 sec/batch; 1h:51m:57s remains)
INFO - root - 2019-11-06 20:32:12.172384: step 23770, total loss = 0.35, predict loss = 0.10 (79.1 examples/sec; 0.051 sec/batch; 1h:46m:20s remains)
INFO - root - 2019-11-06 20:32:12.726710: step 23780, total loss = 0.32, predict loss = 0.08 (78.6 examples/sec; 0.051 sec/batch; 1h:47m:00s remains)
INFO - root - 2019-11-06 20:32:13.299745: step 23790, total loss = 0.27, predict loss = 0.07 (77.4 examples/sec; 0.052 sec/batch; 1h:48m:42s remains)
INFO - root - 2019-11-06 20:32:13.878218: step 23800, total loss = 0.27, predict loss = 0.08 (78.1 examples/sec; 0.051 sec/batch; 1h:47m:43s remains)
INFO - root - 2019-11-06 20:32:14.466744: step 23810, total loss = 0.19, predict loss = 0.05 (75.6 examples/sec; 0.053 sec/batch; 1h:51m:19s remains)
INFO - root - 2019-11-06 20:32:15.067755: step 23820, total loss = 0.21, predict loss = 0.06 (54.9 examples/sec; 0.073 sec/batch; 2h:33m:16s remains)
INFO - root - 2019-11-06 20:32:15.659597: step 23830, total loss = 0.25, predict loss = 0.06 (88.4 examples/sec; 0.045 sec/batch; 1h:35m:07s remains)
INFO - root - 2019-11-06 20:32:16.117448: step 23840, total loss = 0.38, predict loss = 0.09 (94.6 examples/sec; 0.042 sec/batch; 1h:28m:51s remains)
INFO - root - 2019-11-06 20:32:16.596043: step 23850, total loss = 0.40, predict loss = 0.12 (100.0 examples/sec; 0.040 sec/batch; 1h:24m:03s remains)
INFO - root - 2019-11-06 20:32:17.521227: step 23860, total loss = 0.26, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:48m:56s remains)
INFO - root - 2019-11-06 20:32:18.173696: step 23870, total loss = 0.24, predict loss = 0.07 (66.7 examples/sec; 0.060 sec/batch; 2h:05m:59s remains)
INFO - root - 2019-11-06 20:32:18.789419: step 23880, total loss = 0.25, predict loss = 0.07 (77.3 examples/sec; 0.052 sec/batch; 1h:48m:48s remains)
INFO - root - 2019-11-06 20:32:19.408377: step 23890, total loss = 0.42, predict loss = 0.09 (75.0 examples/sec; 0.053 sec/batch; 1h:52m:09s remains)
INFO - root - 2019-11-06 20:32:19.974084: step 23900, total loss = 0.55, predict loss = 0.17 (77.5 examples/sec; 0.052 sec/batch; 1h:48m:28s remains)
INFO - root - 2019-11-06 20:32:20.550779: step 23910, total loss = 0.21, predict loss = 0.05 (80.5 examples/sec; 0.050 sec/batch; 1h:44m:25s remains)
INFO - root - 2019-11-06 20:32:21.132592: step 23920, total loss = 0.31, predict loss = 0.08 (78.5 examples/sec; 0.051 sec/batch; 1h:47m:05s remains)
INFO - root - 2019-11-06 20:32:21.730724: step 23930, total loss = 0.33, predict loss = 0.10 (80.8 examples/sec; 0.050 sec/batch; 1h:44m:01s remains)
INFO - root - 2019-11-06 20:32:22.303537: step 23940, total loss = 0.24, predict loss = 0.06 (82.8 examples/sec; 0.048 sec/batch; 1h:41m:30s remains)
INFO - root - 2019-11-06 20:32:22.882911: step 23950, total loss = 0.17, predict loss = 0.04 (80.8 examples/sec; 0.050 sec/batch; 1h:44m:01s remains)
INFO - root - 2019-11-06 20:32:23.459559: step 23960, total loss = 0.36, predict loss = 0.09 (74.9 examples/sec; 0.053 sec/batch; 1h:52m:07s remains)
INFO - root - 2019-11-06 20:32:24.038829: step 23970, total loss = 0.18, predict loss = 0.04 (84.6 examples/sec; 0.047 sec/batch; 1h:39m:19s remains)
INFO - root - 2019-11-06 20:32:24.576176: step 23980, total loss = 0.39, predict loss = 0.10 (96.7 examples/sec; 0.041 sec/batch; 1h:26m:54s remains)
INFO - root - 2019-11-06 20:32:25.041984: step 23990, total loss = 0.40, predict loss = 0.12 (96.5 examples/sec; 0.041 sec/batch; 1h:27m:02s remains)
INFO - root - 2019-11-06 20:32:25.498609: step 24000, total loss = 0.42, predict loss = 0.11 (95.5 examples/sec; 0.042 sec/batch; 1h:27m:55s remains)
INFO - root - 2019-11-06 20:32:26.497949: step 24010, total loss = 0.39, predict loss = 0.11 (67.5 examples/sec; 0.059 sec/batch; 2h:04m:29s remains)
INFO - root - 2019-11-06 20:32:27.169956: step 24020, total loss = 0.32, predict loss = 0.09 (71.8 examples/sec; 0.056 sec/batch; 1h:56m:59s remains)
INFO - root - 2019-11-06 20:32:27.769449: step 24030, total loss = 0.31, predict loss = 0.08 (84.1 examples/sec; 0.048 sec/batch; 1h:39m:48s remains)
INFO - root - 2019-11-06 20:32:28.348697: step 24040, total loss = 0.47, predict loss = 0.12 (81.2 examples/sec; 0.049 sec/batch; 1h:43m:21s remains)
INFO - root - 2019-11-06 20:32:28.939606: step 24050, total loss = 0.20, predict loss = 0.05 (76.8 examples/sec; 0.052 sec/batch; 1h:49m:16s remains)
INFO - root - 2019-11-06 20:32:29.514850: step 24060, total loss = 0.40, predict loss = 0.09 (77.1 examples/sec; 0.052 sec/batch; 1h:48m:56s remains)
INFO - root - 2019-11-06 20:32:30.097971: step 24070, total loss = 0.18, predict loss = 0.04 (78.0 examples/sec; 0.051 sec/batch; 1h:47m:41s remains)
INFO - root - 2019-11-06 20:32:30.670730: step 24080, total loss = 0.38, predict loss = 0.10 (77.8 examples/sec; 0.051 sec/batch; 1h:47m:55s remains)
INFO - root - 2019-11-06 20:32:31.254104: step 24090, total loss = 0.31, predict loss = 0.07 (83.7 examples/sec; 0.048 sec/batch; 1h:40m:16s remains)
INFO - root - 2019-11-06 20:32:31.827423: step 24100, total loss = 0.21, predict loss = 0.05 (80.2 examples/sec; 0.050 sec/batch; 1h:44m:40s remains)
INFO - root - 2019-11-06 20:32:32.395337: step 24110, total loss = 0.31, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 1h:47m:14s remains)
INFO - root - 2019-11-06 20:32:32.957863: step 24120, total loss = 0.31, predict loss = 0.09 (76.7 examples/sec; 0.052 sec/batch; 1h:49m:23s remains)
INFO - root - 2019-11-06 20:32:33.504757: step 24130, total loss = 0.29, predict loss = 0.07 (97.8 examples/sec; 0.041 sec/batch; 1h:25m:49s remains)
INFO - root - 2019-11-06 20:32:33.978259: step 24140, total loss = 0.15, predict loss = 0.04 (93.7 examples/sec; 0.043 sec/batch; 1h:29m:32s remains)
INFO - root - 2019-11-06 20:32:34.436628: step 24150, total loss = 0.31, predict loss = 0.08 (100.7 examples/sec; 0.040 sec/batch; 1h:23m:17s remains)
INFO - root - 2019-11-06 20:32:35.441593: step 24160, total loss = 0.56, predict loss = 0.17 (63.9 examples/sec; 0.063 sec/batch; 2h:11m:15s remains)
INFO - root - 2019-11-06 20:32:36.138667: step 24170, total loss = 0.20, predict loss = 0.04 (72.5 examples/sec; 0.055 sec/batch; 1h:55m:42s remains)
INFO - root - 2019-11-06 20:32:36.735256: step 24180, total loss = 0.22, predict loss = 0.06 (81.6 examples/sec; 0.049 sec/batch; 1h:42m:44s remains)
INFO - root - 2019-11-06 20:32:37.324496: step 24190, total loss = 0.16, predict loss = 0.04 (75.1 examples/sec; 0.053 sec/batch; 1h:51m:36s remains)
INFO - root - 2019-11-06 20:32:37.892451: step 24200, total loss = 0.26, predict loss = 0.07 (81.0 examples/sec; 0.049 sec/batch; 1h:43m:32s remains)
INFO - root - 2019-11-06 20:32:38.467194: step 24210, total loss = 0.33, predict loss = 0.08 (81.0 examples/sec; 0.049 sec/batch; 1h:43m:32s remains)
INFO - root - 2019-11-06 20:32:39.044315: step 24220, total loss = 0.45, predict loss = 0.12 (77.1 examples/sec; 0.052 sec/batch; 1h:48m:45s remains)
INFO - root - 2019-11-06 20:32:39.615172: step 24230, total loss = 0.22, predict loss = 0.05 (83.4 examples/sec; 0.048 sec/batch; 1h:40m:30s remains)
INFO - root - 2019-11-06 20:32:40.182667: step 24240, total loss = 0.28, predict loss = 0.06 (79.3 examples/sec; 0.050 sec/batch; 1h:45m:46s remains)
INFO - root - 2019-11-06 20:32:40.759453: step 24250, total loss = 0.33, predict loss = 0.08 (86.1 examples/sec; 0.046 sec/batch; 1h:37m:18s remains)
INFO - root - 2019-11-06 20:32:41.332119: step 24260, total loss = 0.23, predict loss = 0.06 (74.4 examples/sec; 0.054 sec/batch; 1h:52m:40s remains)
INFO - root - 2019-11-06 20:32:41.904152: step 24270, total loss = 0.31, predict loss = 0.09 (80.1 examples/sec; 0.050 sec/batch; 1h:44m:36s remains)
INFO - root - 2019-11-06 20:32:42.407647: step 24280, total loss = 0.36, predict loss = 0.09 (103.8 examples/sec; 0.039 sec/batch; 1h:20m:46s remains)
INFO - root - 2019-11-06 20:32:42.881097: step 24290, total loss = 0.25, predict loss = 0.06 (93.5 examples/sec; 0.043 sec/batch; 1h:29m:40s remains)
INFO - root - 2019-11-06 20:32:43.345761: step 24300, total loss = 0.44, predict loss = 0.15 (93.3 examples/sec; 0.043 sec/batch; 1h:29m:46s remains)
INFO - root - 2019-11-06 20:32:44.383803: step 24310, total loss = 0.26, predict loss = 0.07 (61.8 examples/sec; 0.065 sec/batch; 2h:15m:29s remains)
INFO - root - 2019-11-06 20:32:45.009096: step 24320, total loss = 0.50, predict loss = 0.17 (74.6 examples/sec; 0.054 sec/batch; 1h:52m:17s remains)
INFO - root - 2019-11-06 20:32:45.649758: step 24330, total loss = 0.33, predict loss = 0.09 (72.3 examples/sec; 0.055 sec/batch; 1h:55m:55s remains)
INFO - root - 2019-11-06 20:32:46.237449: step 24340, total loss = 0.32, predict loss = 0.08 (73.2 examples/sec; 0.055 sec/batch; 1h:54m:28s remains)
INFO - root - 2019-11-06 20:32:46.803749: step 24350, total loss = 0.29, predict loss = 0.08 (79.3 examples/sec; 0.050 sec/batch; 1h:45m:39s remains)
INFO - root - 2019-11-06 20:32:47.379618: step 24360, total loss = 0.31, predict loss = 0.08 (77.6 examples/sec; 0.052 sec/batch; 1h:47m:52s remains)
INFO - root - 2019-11-06 20:32:47.971877: step 24370, total loss = 0.32, predict loss = 0.09 (79.9 examples/sec; 0.050 sec/batch; 1h:44m:52s remains)
INFO - root - 2019-11-06 20:32:48.549617: step 24380, total loss = 0.28, predict loss = 0.08 (74.6 examples/sec; 0.054 sec/batch; 1h:52m:19s remains)
INFO - root - 2019-11-06 20:32:49.123930: step 24390, total loss = 0.31, predict loss = 0.07 (81.5 examples/sec; 0.049 sec/batch; 1h:42m:43s remains)
INFO - root - 2019-11-06 20:32:49.707049: step 24400, total loss = 0.43, predict loss = 0.13 (77.1 examples/sec; 0.052 sec/batch; 1h:48m:37s remains)
INFO - root - 2019-11-06 20:32:50.289504: step 24410, total loss = 0.29, predict loss = 0.08 (80.8 examples/sec; 0.049 sec/batch; 1h:43m:34s remains)
INFO - root - 2019-11-06 20:32:50.874483: step 24420, total loss = 0.35, predict loss = 0.10 (79.2 examples/sec; 0.051 sec/batch; 1h:45m:42s remains)
INFO - root - 2019-11-06 20:32:51.359804: step 24430, total loss = 0.42, predict loss = 0.11 (100.0 examples/sec; 0.040 sec/batch; 1h:23m:42s remains)
INFO - root - 2019-11-06 20:32:51.817830: step 24440, total loss = 0.23, predict loss = 0.06 (90.9 examples/sec; 0.044 sec/batch; 1h:32m:06s remains)
INFO - root - 2019-11-06 20:32:52.750388: step 24450, total loss = 0.17, predict loss = 0.04 (8.2 examples/sec; 0.489 sec/batch; 17h:03m:11s remains)
INFO - root - 2019-11-06 20:32:53.427911: step 24460, total loss = 0.23, predict loss = 0.05 (59.9 examples/sec; 0.067 sec/batch; 2h:19m:44s remains)
INFO - root - 2019-11-06 20:32:54.078162: step 24470, total loss = 0.39, predict loss = 0.11 (79.3 examples/sec; 0.050 sec/batch; 1h:45m:30s remains)
INFO - root - 2019-11-06 20:32:54.654799: step 24480, total loss = 0.32, predict loss = 0.08 (74.9 examples/sec; 0.053 sec/batch; 1h:51m:44s remains)
INFO - root - 2019-11-06 20:32:55.236998: step 24490, total loss = 0.33, predict loss = 0.08 (83.1 examples/sec; 0.048 sec/batch; 1h:40m:40s remains)
INFO - root - 2019-11-06 20:32:55.801645: step 24500, total loss = 0.27, predict loss = 0.07 (82.2 examples/sec; 0.049 sec/batch; 1h:41m:48s remains)
INFO - root - 2019-11-06 20:32:56.384714: step 24510, total loss = 0.24, predict loss = 0.07 (78.0 examples/sec; 0.051 sec/batch; 1h:47m:16s remains)
INFO - root - 2019-11-06 20:32:56.958596: step 24520, total loss = 0.22, predict loss = 0.06 (78.1 examples/sec; 0.051 sec/batch; 1h:47m:03s remains)
INFO - root - 2019-11-06 20:32:57.560061: step 24530, total loss = 0.22, predict loss = 0.05 (80.1 examples/sec; 0.050 sec/batch; 1h:44m:23s remains)
INFO - root - 2019-11-06 20:32:58.133628: step 24540, total loss = 0.36, predict loss = 0.09 (80.5 examples/sec; 0.050 sec/batch; 1h:43m:55s remains)
INFO - root - 2019-11-06 20:32:58.711848: step 24550, total loss = 0.28, predict loss = 0.06 (77.8 examples/sec; 0.051 sec/batch; 1h:47m:33s remains)
INFO - root - 2019-11-06 20:32:59.282877: step 24560, total loss = 0.33, predict loss = 0.09 (75.9 examples/sec; 0.053 sec/batch; 1h:50m:09s remains)
INFO - root - 2019-11-06 20:32:59.858258: step 24570, total loss = 0.25, predict loss = 0.08 (86.1 examples/sec; 0.046 sec/batch; 1h:37m:04s remains)
INFO - root - 2019-11-06 20:33:00.326268: step 24580, total loss = 0.47, predict loss = 0.14 (95.6 examples/sec; 0.042 sec/batch; 1h:27m:27s remains)
INFO - root - 2019-11-06 20:33:00.778470: step 24590, total loss = 0.28, predict loss = 0.08 (96.6 examples/sec; 0.041 sec/batch; 1h:26m:34s remains)
INFO - root - 2019-11-06 20:33:01.715293: step 24600, total loss = 0.21, predict loss = 0.05 (81.7 examples/sec; 0.049 sec/batch; 1h:42m:16s remains)
INFO - root - 2019-11-06 20:33:02.397931: step 24610, total loss = 0.44, predict loss = 0.12 (60.8 examples/sec; 0.066 sec/batch; 2h:17m:33s remains)
INFO - root - 2019-11-06 20:33:03.032691: step 24620, total loss = 0.23, predict loss = 0.06 (73.8 examples/sec; 0.054 sec/batch; 1h:53m:12s remains)
INFO - root - 2019-11-06 20:33:03.604304: step 24630, total loss = 0.31, predict loss = 0.07 (84.8 examples/sec; 0.047 sec/batch; 1h:38m:36s remains)
INFO - root - 2019-11-06 20:33:04.179319: step 24640, total loss = 0.25, predict loss = 0.06 (77.6 examples/sec; 0.052 sec/batch; 1h:47m:41s remains)
INFO - root - 2019-11-06 20:33:04.765285: step 24650, total loss = 0.35, predict loss = 0.09 (78.4 examples/sec; 0.051 sec/batch; 1h:46m:32s remains)
INFO - root - 2019-11-06 20:33:05.327324: step 24660, total loss = 0.30, predict loss = 0.08 (81.7 examples/sec; 0.049 sec/batch; 1h:42m:15s remains)
INFO - root - 2019-11-06 20:33:05.886633: step 24670, total loss = 0.23, predict loss = 0.05 (73.2 examples/sec; 0.055 sec/batch; 1h:54m:09s remains)
INFO - root - 2019-11-06 20:33:06.460210: step 24680, total loss = 0.32, predict loss = 0.09 (83.1 examples/sec; 0.048 sec/batch; 1h:40m:34s remains)
INFO - root - 2019-11-06 20:33:07.053648: step 24690, total loss = 0.32, predict loss = 0.09 (77.4 examples/sec; 0.052 sec/batch; 1h:47m:53s remains)
INFO - root - 2019-11-06 20:33:07.626587: step 24700, total loss = 0.32, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:45m:36s remains)
INFO - root - 2019-11-06 20:33:08.203440: step 24710, total loss = 0.20, predict loss = 0.05 (77.0 examples/sec; 0.052 sec/batch; 1h:48m:32s remains)
INFO - root - 2019-11-06 20:33:08.741001: step 24720, total loss = 0.38, predict loss = 0.12 (96.3 examples/sec; 0.042 sec/batch; 1h:26m:44s remains)
INFO - root - 2019-11-06 20:33:09.213603: step 24730, total loss = 0.42, predict loss = 0.11 (97.2 examples/sec; 0.041 sec/batch; 1h:25m:55s remains)
INFO - root - 2019-11-06 20:33:09.687773: step 24740, total loss = 0.33, predict loss = 0.09 (98.1 examples/sec; 0.041 sec/batch; 1h:25m:07s remains)
INFO - root - 2019-11-06 20:33:10.636138: step 24750, total loss = 0.31, predict loss = 0.07 (61.8 examples/sec; 0.065 sec/batch; 2h:15m:01s remains)
INFO - root - 2019-11-06 20:33:11.252853: step 24760, total loss = 0.21, predict loss = 0.06 (79.7 examples/sec; 0.050 sec/batch; 1h:44m:48s remains)
INFO - root - 2019-11-06 20:33:11.840793: step 24770, total loss = 0.20, predict loss = 0.05 (80.4 examples/sec; 0.050 sec/batch; 1h:43m:50s remains)
INFO - root - 2019-11-06 20:33:12.407305: step 24780, total loss = 0.33, predict loss = 0.08 (78.2 examples/sec; 0.051 sec/batch; 1h:46m:43s remains)
INFO - root - 2019-11-06 20:33:12.977646: step 24790, total loss = 0.24, predict loss = 0.07 (82.7 examples/sec; 0.048 sec/batch; 1h:40m:59s remains)
INFO - root - 2019-11-06 20:33:13.551020: step 24800, total loss = 0.35, predict loss = 0.10 (77.7 examples/sec; 0.051 sec/batch; 1h:47m:22s remains)
INFO - root - 2019-11-06 20:33:14.144796: step 24810, total loss = 0.36, predict loss = 0.09 (79.6 examples/sec; 0.050 sec/batch; 1h:44m:48s remains)
INFO - root - 2019-11-06 20:33:14.718763: step 24820, total loss = 0.41, predict loss = 0.08 (72.3 examples/sec; 0.055 sec/batch; 1h:55m:25s remains)
INFO - root - 2019-11-06 20:33:15.334406: step 24830, total loss = 0.24, predict loss = 0.07 (76.7 examples/sec; 0.052 sec/batch; 1h:48m:49s remains)
INFO - root - 2019-11-06 20:33:15.912181: step 24840, total loss = 0.26, predict loss = 0.06 (74.7 examples/sec; 0.054 sec/batch; 1h:51m:40s remains)
INFO - root - 2019-11-06 20:33:16.513575: step 24850, total loss = 0.19, predict loss = 0.05 (76.2 examples/sec; 0.053 sec/batch; 1h:49m:32s remains)
INFO - root - 2019-11-06 20:33:17.082249: step 24860, total loss = 0.27, predict loss = 0.06 (79.0 examples/sec; 0.051 sec/batch; 1h:45m:37s remains)
INFO - root - 2019-11-06 20:33:17.615595: step 24870, total loss = 0.31, predict loss = 0.08 (97.0 examples/sec; 0.041 sec/batch; 1h:26m:01s remains)
INFO - root - 2019-11-06 20:33:18.076886: step 24880, total loss = 0.46, predict loss = 0.13 (94.9 examples/sec; 0.042 sec/batch; 1h:27m:53s remains)
INFO - root - 2019-11-06 20:33:18.546459: step 24890, total loss = 0.24, predict loss = 0.06 (95.2 examples/sec; 0.042 sec/batch; 1h:27m:37s remains)
INFO - root - 2019-11-06 20:33:19.527483: step 24900, total loss = 0.25, predict loss = 0.07 (64.2 examples/sec; 0.062 sec/batch; 2h:09m:55s remains)
INFO - root - 2019-11-06 20:33:20.153196: step 24910, total loss = 0.40, predict loss = 0.08 (77.2 examples/sec; 0.052 sec/batch; 1h:48m:04s remains)
INFO - root - 2019-11-06 20:33:20.726399: step 24920, total loss = 0.22, predict loss = 0.06 (83.1 examples/sec; 0.048 sec/batch; 1h:40m:19s remains)
INFO - root - 2019-11-06 20:33:21.305612: step 24930, total loss = 0.25, predict loss = 0.06 (80.6 examples/sec; 0.050 sec/batch; 1h:43m:23s remains)
INFO - root - 2019-11-06 20:33:21.883190: step 24940, total loss = 0.29, predict loss = 0.08 (77.8 examples/sec; 0.051 sec/batch; 1h:47m:07s remains)
INFO - root - 2019-11-06 20:33:22.453471: step 24950, total loss = 0.33, predict loss = 0.09 (79.0 examples/sec; 0.051 sec/batch; 1h:45m:28s remains)
INFO - root - 2019-11-06 20:33:23.021450: step 24960, total loss = 0.43, predict loss = 0.14 (81.9 examples/sec; 0.049 sec/batch; 1h:41m:46s remains)
INFO - root - 2019-11-06 20:33:23.612802: step 24970, total loss = 0.21, predict loss = 0.05 (75.3 examples/sec; 0.053 sec/batch; 1h:50m:43s remains)
INFO - root - 2019-11-06 20:33:24.189057: step 24980, total loss = 0.34, predict loss = 0.11 (77.4 examples/sec; 0.052 sec/batch; 1h:47m:38s remains)
INFO - root - 2019-11-06 20:33:24.757520: step 24990, total loss = 0.34, predict loss = 0.09 (82.2 examples/sec; 0.049 sec/batch; 1h:41m:25s remains)
INFO - root - 2019-11-06 20:33:25.347808: step 25000, total loss = 0.47, predict loss = 0.15 (77.1 examples/sec; 0.052 sec/batch; 1h:48m:03s remains)
INFO - root - 2019-11-06 20:33:25.937206: step 25010, total loss = 0.19, predict loss = 0.04 (84.5 examples/sec; 0.047 sec/batch; 1h:38m:39s remains)
INFO - root - 2019-11-06 20:33:26.445941: step 25020, total loss = 0.21, predict loss = 0.06 (104.2 examples/sec; 0.038 sec/batch; 1h:19m:57s remains)
INFO - root - 2019-11-06 20:33:26.878756: step 25030, total loss = 0.36, predict loss = 0.08 (100.7 examples/sec; 0.040 sec/batch; 1h:22m:45s remains)
INFO - root - 2019-11-06 20:33:27.332110: step 25040, total loss = 0.34, predict loss = 0.09 (96.7 examples/sec; 0.041 sec/batch; 1h:26m:07s remains)
INFO - root - 2019-11-06 20:33:28.430066: step 25050, total loss = 0.27, predict loss = 0.06 (55.0 examples/sec; 0.073 sec/batch; 2h:31m:21s remains)
INFO - root - 2019-11-06 20:33:29.053034: step 25060, total loss = 0.45, predict loss = 0.13 (79.2 examples/sec; 0.051 sec/batch; 1h:45m:10s remains)
INFO - root - 2019-11-06 20:33:29.617887: step 25070, total loss = 0.25, predict loss = 0.06 (76.7 examples/sec; 0.052 sec/batch; 1h:48m:37s remains)
INFO - root - 2019-11-06 20:33:30.203566: step 25080, total loss = 0.30, predict loss = 0.07 (77.4 examples/sec; 0.052 sec/batch; 1h:47m:37s remains)
INFO - root - 2019-11-06 20:33:30.805040: step 25090, total loss = 0.46, predict loss = 0.13 (79.9 examples/sec; 0.050 sec/batch; 1h:44m:13s remains)
INFO - root - 2019-11-06 20:33:31.383450: step 25100, total loss = 0.31, predict loss = 0.08 (76.6 examples/sec; 0.052 sec/batch; 1h:48m:44s remains)
INFO - root - 2019-11-06 20:33:31.963228: step 25110, total loss = 0.28, predict loss = 0.08 (77.9 examples/sec; 0.051 sec/batch; 1h:46m:53s remains)
INFO - root - 2019-11-06 20:33:32.541142: step 25120, total loss = 0.29, predict loss = 0.08 (82.9 examples/sec; 0.048 sec/batch; 1h:40m:28s remains)
INFO - root - 2019-11-06 20:33:33.130355: step 25130, total loss = 0.30, predict loss = 0.09 (76.6 examples/sec; 0.052 sec/batch; 1h:48m:42s remains)
INFO - root - 2019-11-06 20:33:33.701642: step 25140, total loss = 0.29, predict loss = 0.08 (78.5 examples/sec; 0.051 sec/batch; 1h:45m:58s remains)
INFO - root - 2019-11-06 20:33:34.274687: step 25150, total loss = 0.25, predict loss = 0.07 (79.3 examples/sec; 0.050 sec/batch; 1h:44m:56s remains)
INFO - root - 2019-11-06 20:33:34.830365: step 25160, total loss = 0.22, predict loss = 0.07 (77.6 examples/sec; 0.052 sec/batch; 1h:47m:18s remains)
INFO - root - 2019-11-06 20:33:35.333804: step 25170, total loss = 0.20, predict loss = 0.05 (100.8 examples/sec; 0.040 sec/batch; 1h:22m:33s remains)
INFO - root - 2019-11-06 20:33:35.781720: step 25180, total loss = 0.31, predict loss = 0.08 (100.1 examples/sec; 0.040 sec/batch; 1h:23m:08s remains)
INFO - root - 2019-11-06 20:33:36.229377: step 25190, total loss = 0.27, predict loss = 0.07 (89.9 examples/sec; 0.045 sec/batch; 1h:32m:34s remains)
INFO - root - 2019-11-06 20:33:37.286101: step 25200, total loss = 0.27, predict loss = 0.06 (69.6 examples/sec; 0.057 sec/batch; 1h:59m:29s remains)
INFO - root - 2019-11-06 20:33:37.936326: step 25210, total loss = 0.29, predict loss = 0.08 (76.3 examples/sec; 0.052 sec/batch; 1h:48m:59s remains)
INFO - root - 2019-11-06 20:33:38.510711: step 25220, total loss = 0.35, predict loss = 0.09 (80.1 examples/sec; 0.050 sec/batch; 1h:43m:48s remains)
INFO - root - 2019-11-06 20:33:39.088502: step 25230, total loss = 0.25, predict loss = 0.06 (79.6 examples/sec; 0.050 sec/batch; 1h:44m:30s remains)
INFO - root - 2019-11-06 20:33:39.658748: step 25240, total loss = 0.28, predict loss = 0.08 (78.5 examples/sec; 0.051 sec/batch; 1h:45m:55s remains)
INFO - root - 2019-11-06 20:33:40.251522: step 25250, total loss = 0.33, predict loss = 0.09 (79.1 examples/sec; 0.051 sec/batch; 1h:45m:06s remains)
INFO - root - 2019-11-06 20:33:40.830140: step 25260, total loss = 0.25, predict loss = 0.07 (75.0 examples/sec; 0.053 sec/batch; 1h:50m:55s remains)
INFO - root - 2019-11-06 20:33:41.394834: step 25270, total loss = 0.22, predict loss = 0.05 (75.9 examples/sec; 0.053 sec/batch; 1h:49m:36s remains)
INFO - root - 2019-11-06 20:33:41.955335: step 25280, total loss = 0.18, predict loss = 0.04 (82.2 examples/sec; 0.049 sec/batch; 1h:41m:05s remains)
INFO - root - 2019-11-06 20:33:42.541412: step 25290, total loss = 0.22, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:47m:42s remains)
INFO - root - 2019-11-06 20:33:43.114984: step 25300, total loss = 0.18, predict loss = 0.05 (75.9 examples/sec; 0.053 sec/batch; 1h:49m:29s remains)
INFO - root - 2019-11-06 20:33:43.692276: step 25310, total loss = 0.33, predict loss = 0.08 (74.6 examples/sec; 0.054 sec/batch; 1h:51m:29s remains)
INFO - root - 2019-11-06 20:33:44.159769: step 25320, total loss = 0.28, predict loss = 0.07 (100.8 examples/sec; 0.040 sec/batch; 1h:22m:26s remains)
INFO - root - 2019-11-06 20:33:44.624845: step 25330, total loss = 0.22, predict loss = 0.06 (98.3 examples/sec; 0.041 sec/batch; 1h:24m:33s remains)
INFO - root - 2019-11-06 20:33:45.582193: step 25340, total loss = 0.42, predict loss = 0.12 (78.4 examples/sec; 0.051 sec/batch; 1h:46m:02s remains)
INFO - root - 2019-11-06 20:33:46.375871: step 25350, total loss = 0.24, predict loss = 0.06 (57.9 examples/sec; 0.069 sec/batch; 2h:23m:37s remains)
INFO - root - 2019-11-06 20:33:46.995293: step 25360, total loss = 0.26, predict loss = 0.06 (79.5 examples/sec; 0.050 sec/batch; 1h:44m:30s remains)
INFO - root - 2019-11-06 20:33:47.586943: step 25370, total loss = 0.32, predict loss = 0.08 (75.6 examples/sec; 0.053 sec/batch; 1h:49m:51s remains)
INFO - root - 2019-11-06 20:33:48.157202: step 25380, total loss = 0.23, predict loss = 0.06 (81.0 examples/sec; 0.049 sec/batch; 1h:42m:36s remains)
INFO - root - 2019-11-06 20:33:48.738022: step 25390, total loss = 0.19, predict loss = 0.05 (75.6 examples/sec; 0.053 sec/batch; 1h:49m:54s remains)
INFO - root - 2019-11-06 20:33:49.323730: step 25400, total loss = 0.42, predict loss = 0.12 (76.1 examples/sec; 0.053 sec/batch; 1h:49m:12s remains)
INFO - root - 2019-11-06 20:33:49.914178: step 25410, total loss = 0.33, predict loss = 0.10 (80.2 examples/sec; 0.050 sec/batch; 1h:43m:34s remains)
INFO - root - 2019-11-06 20:33:50.510151: step 25420, total loss = 0.18, predict loss = 0.04 (77.0 examples/sec; 0.052 sec/batch; 1h:47m:51s remains)
INFO - root - 2019-11-06 20:33:51.073114: step 25430, total loss = 0.43, predict loss = 0.12 (78.9 examples/sec; 0.051 sec/batch; 1h:45m:18s remains)
INFO - root - 2019-11-06 20:33:51.641029: step 25440, total loss = 0.17, predict loss = 0.05 (78.5 examples/sec; 0.051 sec/batch; 1h:45m:46s remains)
INFO - root - 2019-11-06 20:33:52.231713: step 25450, total loss = 0.26, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:47m:34s remains)
INFO - root - 2019-11-06 20:33:52.794670: step 25460, total loss = 0.28, predict loss = 0.08 (96.2 examples/sec; 0.042 sec/batch; 1h:26m:20s remains)
INFO - root - 2019-11-06 20:33:53.266406: step 25470, total loss = 0.37, predict loss = 0.12 (93.0 examples/sec; 0.043 sec/batch; 1h:29m:16s remains)
INFO - root - 2019-11-06 20:33:53.732232: step 25480, total loss = 0.24, predict loss = 0.06 (96.3 examples/sec; 0.042 sec/batch; 1h:26m:10s remains)
INFO - root - 2019-11-06 20:33:54.681629: step 25490, total loss = 0.24, predict loss = 0.07 (68.3 examples/sec; 0.059 sec/batch; 2h:01m:29s remains)
INFO - root - 2019-11-06 20:33:55.317475: step 25500, total loss = 0.49, predict loss = 0.18 (76.3 examples/sec; 0.052 sec/batch; 1h:48m:49s remains)
INFO - root - 2019-11-06 20:33:55.899669: step 25510, total loss = 0.27, predict loss = 0.06 (79.9 examples/sec; 0.050 sec/batch; 1h:43m:53s remains)
INFO - root - 2019-11-06 20:33:56.461736: step 25520, total loss = 0.26, predict loss = 0.07 (76.7 examples/sec; 0.052 sec/batch; 1h:48m:07s remains)
INFO - root - 2019-11-06 20:33:57.048143: step 25530, total loss = 0.16, predict loss = 0.04 (77.4 examples/sec; 0.052 sec/batch; 1h:47m:15s remains)
INFO - root - 2019-11-06 20:33:57.619365: step 25540, total loss = 0.23, predict loss = 0.06 (79.7 examples/sec; 0.050 sec/batch; 1h:44m:08s remains)
INFO - root - 2019-11-06 20:33:58.192425: step 25550, total loss = 0.31, predict loss = 0.08 (77.1 examples/sec; 0.052 sec/batch; 1h:47m:40s remains)
INFO - root - 2019-11-06 20:33:58.759451: step 25560, total loss = 0.25, predict loss = 0.07 (74.9 examples/sec; 0.053 sec/batch; 1h:50m:48s remains)
INFO - root - 2019-11-06 20:33:59.345463: step 25570, total loss = 0.19, predict loss = 0.05 (80.2 examples/sec; 0.050 sec/batch; 1h:43m:26s remains)
INFO - root - 2019-11-06 20:33:59.913766: step 25580, total loss = 0.34, predict loss = 0.10 (79.9 examples/sec; 0.050 sec/batch; 1h:43m:52s remains)
INFO - root - 2019-11-06 20:34:00.485041: step 25590, total loss = 0.29, predict loss = 0.07 (77.4 examples/sec; 0.052 sec/batch; 1h:47m:08s remains)
INFO - root - 2019-11-06 20:34:01.060887: step 25600, total loss = 0.27, predict loss = 0.07 (76.1 examples/sec; 0.053 sec/batch; 1h:49m:02s remains)
INFO - root - 2019-11-06 20:34:01.632507: step 25610, total loss = 0.54, predict loss = 0.14 (95.1 examples/sec; 0.042 sec/batch; 1h:27m:09s remains)
INFO - root - 2019-11-06 20:34:02.084076: step 25620, total loss = 0.27, predict loss = 0.07 (100.4 examples/sec; 0.040 sec/batch; 1h:22m:34s remains)
INFO - root - 2019-11-06 20:34:02.537719: step 25630, total loss = 0.23, predict loss = 0.06 (105.0 examples/sec; 0.038 sec/batch; 1h:18m:58s remains)
INFO - root - 2019-11-06 20:34:03.483334: step 25640, total loss = 0.35, predict loss = 0.10 (70.8 examples/sec; 0.057 sec/batch; 1h:57m:09s remains)
INFO - root - 2019-11-06 20:34:04.125824: step 25650, total loss = 0.20, predict loss = 0.05 (74.3 examples/sec; 0.054 sec/batch; 1h:51m:31s remains)
INFO - root - 2019-11-06 20:34:04.700037: step 25660, total loss = 0.31, predict loss = 0.08 (80.7 examples/sec; 0.050 sec/batch; 1h:42m:43s remains)
INFO - root - 2019-11-06 20:34:05.278163: step 25670, total loss = 0.19, predict loss = 0.05 (77.4 examples/sec; 0.052 sec/batch; 1h:47m:02s remains)
INFO - root - 2019-11-06 20:34:05.862531: step 25680, total loss = 0.28, predict loss = 0.07 (76.5 examples/sec; 0.052 sec/batch; 1h:48m:17s remains)
INFO - root - 2019-11-06 20:34:06.456274: step 25690, total loss = 0.38, predict loss = 0.10 (76.7 examples/sec; 0.052 sec/batch; 1h:48m:04s remains)
INFO - root - 2019-11-06 20:34:07.027321: step 25700, total loss = 0.29, predict loss = 0.08 (78.7 examples/sec; 0.051 sec/batch; 1h:45m:20s remains)
INFO - root - 2019-11-06 20:34:07.604680: step 25710, total loss = 0.34, predict loss = 0.09 (78.1 examples/sec; 0.051 sec/batch; 1h:46m:09s remains)
INFO - root - 2019-11-06 20:34:08.173959: step 25720, total loss = 0.33, predict loss = 0.09 (81.4 examples/sec; 0.049 sec/batch; 1h:41m:43s remains)
INFO - root - 2019-11-06 20:34:08.762010: step 25730, total loss = 0.21, predict loss = 0.06 (75.8 examples/sec; 0.053 sec/batch; 1h:49m:17s remains)
INFO - root - 2019-11-06 20:34:09.319596: step 25740, total loss = 0.36, predict loss = 0.10 (79.9 examples/sec; 0.050 sec/batch; 1h:43m:39s remains)
INFO - root - 2019-11-06 20:34:09.894267: step 25750, total loss = 0.33, predict loss = 0.09 (78.8 examples/sec; 0.051 sec/batch; 1h:45m:04s remains)
INFO - root - 2019-11-06 20:34:10.435737: step 25760, total loss = 0.26, predict loss = 0.07 (93.6 examples/sec; 0.043 sec/batch; 1h:28m:26s remains)
INFO - root - 2019-11-06 20:34:10.922770: step 25770, total loss = 0.24, predict loss = 0.06 (95.1 examples/sec; 0.042 sec/batch; 1h:27m:07s remains)
INFO - root - 2019-11-06 20:34:11.374176: step 25780, total loss = 0.44, predict loss = 0.15 (96.1 examples/sec; 0.042 sec/batch; 1h:26m:12s remains)
INFO - root - 2019-11-06 20:34:12.407302: step 25790, total loss = 0.19, predict loss = 0.04 (54.2 examples/sec; 0.074 sec/batch; 2h:32m:45s remains)
INFO - root - 2019-11-06 20:34:13.120889: step 25800, total loss = 0.21, predict loss = 0.05 (66.1 examples/sec; 0.061 sec/batch; 2h:05m:14s remains)
INFO - root - 2019-11-06 20:34:13.778880: step 25810, total loss = 0.22, predict loss = 0.05 (74.3 examples/sec; 0.054 sec/batch; 1h:51m:23s remains)
INFO - root - 2019-11-06 20:34:14.368376: step 25820, total loss = 0.22, predict loss = 0.05 (81.4 examples/sec; 0.049 sec/batch; 1h:41m:38s remains)
INFO - root - 2019-11-06 20:34:14.936569: step 25830, total loss = 0.31, predict loss = 0.08 (81.7 examples/sec; 0.049 sec/batch; 1h:41m:21s remains)
INFO - root - 2019-11-06 20:34:15.573623: step 25840, total loss = 0.28, predict loss = 0.07 (74.2 examples/sec; 0.054 sec/batch; 1h:51m:36s remains)
INFO - root - 2019-11-06 20:34:16.164470: step 25850, total loss = 0.25, predict loss = 0.06 (76.7 examples/sec; 0.052 sec/batch; 1h:47m:54s remains)
INFO - root - 2019-11-06 20:34:16.744501: step 25860, total loss = 0.28, predict loss = 0.09 (79.1 examples/sec; 0.051 sec/batch; 1h:44m:41s remains)
INFO - root - 2019-11-06 20:34:17.323112: step 25870, total loss = 0.21, predict loss = 0.06 (76.9 examples/sec; 0.052 sec/batch; 1h:47m:36s remains)
INFO - root - 2019-11-06 20:34:17.898265: step 25880, total loss = 0.26, predict loss = 0.07 (77.8 examples/sec; 0.051 sec/batch; 1h:46m:21s remains)
INFO - root - 2019-11-06 20:34:18.480190: step 25890, total loss = 0.19, predict loss = 0.04 (78.9 examples/sec; 0.051 sec/batch; 1h:44m:53s remains)
INFO - root - 2019-11-06 20:34:19.060112: step 25900, total loss = 0.32, predict loss = 0.08 (79.8 examples/sec; 0.050 sec/batch; 1h:43m:39s remains)
INFO - root - 2019-11-06 20:34:19.567691: step 25910, total loss = 0.44, predict loss = 0.12 (102.9 examples/sec; 0.039 sec/batch; 1h:20m:23s remains)
INFO - root - 2019-11-06 20:34:20.007297: step 25920, total loss = 0.31, predict loss = 0.08 (97.0 examples/sec; 0.041 sec/batch; 1h:25m:17s remains)
INFO - root - 2019-11-06 20:34:20.479045: step 25930, total loss = 0.34, predict loss = 0.09 (94.1 examples/sec; 0.043 sec/batch; 1h:27m:56s remains)
INFO - root - 2019-11-06 20:34:21.559853: step 25940, total loss = 0.22, predict loss = 0.06 (61.1 examples/sec; 0.065 sec/batch; 2h:15m:23s remains)
INFO - root - 2019-11-06 20:34:22.175534: step 25950, total loss = 0.44, predict loss = 0.14 (78.0 examples/sec; 0.051 sec/batch; 1h:45m:59s remains)
INFO - root - 2019-11-06 20:34:22.752574: step 25960, total loss = 0.17, predict loss = 0.04 (74.0 examples/sec; 0.054 sec/batch; 1h:51m:40s remains)
INFO - root - 2019-11-06 20:34:23.327821: step 25970, total loss = 0.31, predict loss = 0.07 (79.0 examples/sec; 0.051 sec/batch; 1h:44m:37s remains)
INFO - root - 2019-11-06 20:34:23.900189: step 25980, total loss = 0.27, predict loss = 0.06 (78.4 examples/sec; 0.051 sec/batch; 1h:45m:28s remains)
INFO - root - 2019-11-06 20:34:24.471827: step 25990, total loss = 0.18, predict loss = 0.05 (81.2 examples/sec; 0.049 sec/batch; 1h:41m:51s remains)
INFO - root - 2019-11-06 20:34:25.047090: step 26000, total loss = 0.45, predict loss = 0.13 (79.5 examples/sec; 0.050 sec/batch; 1h:43m:55s remains)
INFO - root - 2019-11-06 20:34:25.633273: step 26010, total loss = 0.27, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:46m:50s remains)
INFO - root - 2019-11-06 20:34:26.209844: step 26020, total loss = 0.21, predict loss = 0.05 (76.4 examples/sec; 0.052 sec/batch; 1h:48m:09s remains)
INFO - root - 2019-11-06 20:34:26.791128: step 26030, total loss = 0.41, predict loss = 0.11 (77.7 examples/sec; 0.052 sec/batch; 1h:46m:24s remains)
INFO - root - 2019-11-06 20:34:27.368038: step 26040, total loss = 0.24, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:47m:37s remains)
INFO - root - 2019-11-06 20:34:27.948369: step 26050, total loss = 0.27, predict loss = 0.06 (77.0 examples/sec; 0.052 sec/batch; 1h:47m:15s remains)
INFO - root - 2019-11-06 20:34:28.426537: step 26060, total loss = 0.28, predict loss = 0.07 (93.7 examples/sec; 0.043 sec/batch; 1h:28m:08s remains)
INFO - root - 2019-11-06 20:34:28.874967: step 26070, total loss = 0.25, predict loss = 0.06 (99.5 examples/sec; 0.040 sec/batch; 1h:23m:01s remains)
INFO - root - 2019-11-06 20:34:29.770223: step 26080, total loss = 0.22, predict loss = 0.05 (8.3 examples/sec; 0.484 sec/batch; 16h:40m:36s remains)
INFO - root - 2019-11-06 20:34:30.417611: step 26090, total loss = 0.36, predict loss = 0.09 (66.5 examples/sec; 0.060 sec/batch; 2h:04m:15s remains)
INFO - root - 2019-11-06 20:34:31.047522: step 26100, total loss = 0.18, predict loss = 0.04 (74.1 examples/sec; 0.054 sec/batch; 1h:51m:29s remains)
INFO - root - 2019-11-06 20:34:31.627618: step 26110, total loss = 0.46, predict loss = 0.11 (79.1 examples/sec; 0.051 sec/batch; 1h:44m:27s remains)
INFO - root - 2019-11-06 20:34:32.196933: step 26120, total loss = 0.41, predict loss = 0.12 (79.4 examples/sec; 0.050 sec/batch; 1h:44m:04s remains)
INFO - root - 2019-11-06 20:34:32.780930: step 26130, total loss = 0.19, predict loss = 0.04 (79.3 examples/sec; 0.050 sec/batch; 1h:44m:09s remains)
INFO - root - 2019-11-06 20:34:33.357720: step 26140, total loss = 0.32, predict loss = 0.09 (75.8 examples/sec; 0.053 sec/batch; 1h:48m:54s remains)
INFO - root - 2019-11-06 20:34:33.927956: step 26150, total loss = 0.20, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:44m:50s remains)
INFO - root - 2019-11-06 20:34:34.516466: step 26160, total loss = 0.20, predict loss = 0.05 (81.5 examples/sec; 0.049 sec/batch; 1h:41m:19s remains)
INFO - root - 2019-11-06 20:34:35.102175: step 26170, total loss = 0.32, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 1h:45m:25s remains)
INFO - root - 2019-11-06 20:34:35.682366: step 26180, total loss = 0.35, predict loss = 0.09 (76.2 examples/sec; 0.052 sec/batch; 1h:48m:17s remains)
INFO - root - 2019-11-06 20:34:36.260198: step 26190, total loss = 0.22, predict loss = 0.06 (75.5 examples/sec; 0.053 sec/batch; 1h:49m:21s remains)
INFO - root - 2019-11-06 20:34:36.828644: step 26200, total loss = 0.28, predict loss = 0.08 (85.1 examples/sec; 0.047 sec/batch; 1h:36m:58s remains)
INFO - root - 2019-11-06 20:34:37.312339: step 26210, total loss = 0.37, predict loss = 0.10 (96.5 examples/sec; 0.041 sec/batch; 1h:25m:32s remains)
INFO - root - 2019-11-06 20:34:37.769113: step 26220, total loss = 0.19, predict loss = 0.05 (92.9 examples/sec; 0.043 sec/batch; 1h:28m:51s remains)
INFO - root - 2019-11-06 20:34:38.680097: step 26230, total loss = 0.23, predict loss = 0.06 (79.4 examples/sec; 0.050 sec/batch; 1h:43m:52s remains)
INFO - root - 2019-11-06 20:34:39.396236: step 26240, total loss = 0.36, predict loss = 0.09 (62.8 examples/sec; 0.064 sec/batch; 2h:11m:24s remains)
INFO - root - 2019-11-06 20:34:40.026790: step 26250, total loss = 0.23, predict loss = 0.06 (76.0 examples/sec; 0.053 sec/batch; 1h:48m:32s remains)
INFO - root - 2019-11-06 20:34:40.591673: step 26260, total loss = 0.26, predict loss = 0.07 (78.9 examples/sec; 0.051 sec/batch; 1h:44m:29s remains)
INFO - root - 2019-11-06 20:34:41.159793: step 26270, total loss = 0.27, predict loss = 0.07 (80.0 examples/sec; 0.050 sec/batch; 1h:43m:05s remains)
INFO - root - 2019-11-06 20:34:41.733860: step 26280, total loss = 0.25, predict loss = 0.07 (83.6 examples/sec; 0.048 sec/batch; 1h:38m:38s remains)
INFO - root - 2019-11-06 20:34:42.316449: step 26290, total loss = 0.31, predict loss = 0.08 (82.2 examples/sec; 0.049 sec/batch; 1h:40m:16s remains)
INFO - root - 2019-11-06 20:34:42.882489: step 26300, total loss = 0.20, predict loss = 0.05 (81.1 examples/sec; 0.049 sec/batch; 1h:41m:42s remains)
INFO - root - 2019-11-06 20:34:43.456448: step 26310, total loss = 0.28, predict loss = 0.07 (75.7 examples/sec; 0.053 sec/batch; 1h:48m:59s remains)
INFO - root - 2019-11-06 20:34:44.035455: step 26320, total loss = 0.24, predict loss = 0.06 (80.0 examples/sec; 0.050 sec/batch; 1h:43m:07s remains)
INFO - root - 2019-11-06 20:34:44.620878: step 26330, total loss = 0.22, predict loss = 0.06 (77.8 examples/sec; 0.051 sec/batch; 1h:45m:57s remains)
INFO - root - 2019-11-06 20:34:45.218290: step 26340, total loss = 0.24, predict loss = 0.06 (70.4 examples/sec; 0.057 sec/batch; 1h:57m:04s remains)
INFO - root - 2019-11-06 20:34:45.769929: step 26350, total loss = 0.25, predict loss = 0.07 (94.8 examples/sec; 0.042 sec/batch; 1h:26m:59s remains)
INFO - root - 2019-11-06 20:34:46.229788: step 26360, total loss = 0.22, predict loss = 0.06 (92.3 examples/sec; 0.043 sec/batch; 1h:29m:21s remains)
INFO - root - 2019-11-06 20:34:46.723357: step 26370, total loss = 0.24, predict loss = 0.06 (86.2 examples/sec; 0.046 sec/batch; 1h:35m:38s remains)
INFO - root - 2019-11-06 20:34:47.668671: step 26380, total loss = 0.35, predict loss = 0.09 (68.2 examples/sec; 0.059 sec/batch; 2h:00m:48s remains)
INFO - root - 2019-11-06 20:34:48.349918: step 26390, total loss = 0.23, predict loss = 0.05 (71.6 examples/sec; 0.056 sec/batch; 1h:55m:06s remains)
INFO - root - 2019-11-06 20:34:48.943531: step 26400, total loss = 0.22, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:47m:21s remains)
INFO - root - 2019-11-06 20:34:49.533215: step 26410, total loss = 0.26, predict loss = 0.06 (80.4 examples/sec; 0.050 sec/batch; 1h:42m:30s remains)
INFO - root - 2019-11-06 20:34:50.104209: step 26420, total loss = 0.30, predict loss = 0.08 (84.7 examples/sec; 0.047 sec/batch; 1h:37m:14s remains)
INFO - root - 2019-11-06 20:34:50.679691: step 26430, total loss = 0.35, predict loss = 0.10 (74.7 examples/sec; 0.054 sec/batch; 1h:50m:15s remains)
INFO - root - 2019-11-06 20:34:51.247509: step 26440, total loss = 0.31, predict loss = 0.08 (77.7 examples/sec; 0.051 sec/batch; 1h:45m:58s remains)
INFO - root - 2019-11-06 20:34:51.831841: step 26450, total loss = 0.25, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:46m:21s remains)
INFO - root - 2019-11-06 20:34:52.399392: step 26460, total loss = 0.29, predict loss = 0.07 (80.8 examples/sec; 0.050 sec/batch; 1h:41m:57s remains)
INFO - root - 2019-11-06 20:34:52.974959: step 26470, total loss = 0.44, predict loss = 0.14 (79.1 examples/sec; 0.051 sec/batch; 1h:44m:05s remains)
INFO - root - 2019-11-06 20:34:53.537417: step 26480, total loss = 0.20, predict loss = 0.05 (81.6 examples/sec; 0.049 sec/batch; 1h:40m:56s remains)
INFO - root - 2019-11-06 20:34:54.130421: step 26490, total loss = 0.23, predict loss = 0.06 (76.7 examples/sec; 0.052 sec/batch; 1h:47m:23s remains)
INFO - root - 2019-11-06 20:34:54.660126: step 26500, total loss = 0.30, predict loss = 0.07 (93.7 examples/sec; 0.043 sec/batch; 1h:27m:54s remains)
INFO - root - 2019-11-06 20:34:55.106088: step 26510, total loss = 0.26, predict loss = 0.06 (96.7 examples/sec; 0.041 sec/batch; 1h:25m:07s remains)
INFO - root - 2019-11-06 20:34:55.567512: step 26520, total loss = 0.37, predict loss = 0.11 (94.4 examples/sec; 0.042 sec/batch; 1h:27m:11s remains)
INFO - root - 2019-11-06 20:34:56.598336: step 26530, total loss = 0.20, predict loss = 0.05 (55.6 examples/sec; 0.072 sec/batch; 2h:27m:56s remains)
INFO - root - 2019-11-06 20:34:57.260611: step 26540, total loss = 0.40, predict loss = 0.13 (74.9 examples/sec; 0.053 sec/batch; 1h:49m:54s remains)
INFO - root - 2019-11-06 20:34:57.826765: step 26550, total loss = 0.34, predict loss = 0.09 (75.2 examples/sec; 0.053 sec/batch; 1h:49m:30s remains)
INFO - root - 2019-11-06 20:34:58.409185: step 26560, total loss = 0.17, predict loss = 0.04 (78.8 examples/sec; 0.051 sec/batch; 1h:44m:28s remains)
INFO - root - 2019-11-06 20:34:59.005204: step 26570, total loss = 0.20, predict loss = 0.05 (84.0 examples/sec; 0.048 sec/batch; 1h:37m:59s remains)
INFO - root - 2019-11-06 20:34:59.575231: step 26580, total loss = 0.29, predict loss = 0.08 (76.9 examples/sec; 0.052 sec/batch; 1h:46m:56s remains)
INFO - root - 2019-11-06 20:35:00.150523: step 26590, total loss = 0.36, predict loss = 0.10 (77.3 examples/sec; 0.052 sec/batch; 1h:46m:22s remains)
INFO - root - 2019-11-06 20:35:00.729162: step 26600, total loss = 0.19, predict loss = 0.05 (75.7 examples/sec; 0.053 sec/batch; 1h:48m:39s remains)
INFO - root - 2019-11-06 20:35:01.337109: step 26610, total loss = 0.24, predict loss = 0.06 (78.5 examples/sec; 0.051 sec/batch; 1h:44m:44s remains)
INFO - root - 2019-11-06 20:35:01.917129: step 26620, total loss = 0.25, predict loss = 0.06 (80.0 examples/sec; 0.050 sec/batch; 1h:42m:51s remains)
INFO - root - 2019-11-06 20:35:02.490295: step 26630, total loss = 0.40, predict loss = 0.11 (77.7 examples/sec; 0.051 sec/batch; 1h:45m:49s remains)
INFO - root - 2019-11-06 20:35:03.055147: step 26640, total loss = 0.21, predict loss = 0.06 (81.2 examples/sec; 0.049 sec/batch; 1h:41m:18s remains)
INFO - root - 2019-11-06 20:35:03.581266: step 26650, total loss = 0.27, predict loss = 0.07 (104.8 examples/sec; 0.038 sec/batch; 1h:18m:29s remains)
INFO - root - 2019-11-06 20:35:04.040576: step 26660, total loss = 0.43, predict loss = 0.11 (96.2 examples/sec; 0.042 sec/batch; 1h:25m:29s remains)
INFO - root - 2019-11-06 20:35:04.503725: step 26670, total loss = 0.44, predict loss = 0.13 (86.7 examples/sec; 0.046 sec/batch; 1h:34m:52s remains)
INFO - root - 2019-11-06 20:35:05.626719: step 26680, total loss = 0.31, predict loss = 0.08 (49.0 examples/sec; 0.082 sec/batch; 2h:47m:37s remains)
INFO - root - 2019-11-06 20:35:06.340513: step 26690, total loss = 0.46, predict loss = 0.16 (64.6 examples/sec; 0.062 sec/batch; 2h:07m:15s remains)
INFO - root - 2019-11-06 20:35:06.950478: step 26700, total loss = 0.30, predict loss = 0.08 (80.6 examples/sec; 0.050 sec/batch; 1h:41m:57s remains)
INFO - root - 2019-11-06 20:35:07.528644: step 26710, total loss = 0.23, predict loss = 0.06 (81.6 examples/sec; 0.049 sec/batch; 1h:40m:44s remains)
INFO - root - 2019-11-06 20:35:08.106810: step 26720, total loss = 0.22, predict loss = 0.07 (76.7 examples/sec; 0.052 sec/batch; 1h:47m:08s remains)
INFO - root - 2019-11-06 20:35:08.690889: step 26730, total loss = 0.21, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 1h:44m:53s remains)
INFO - root - 2019-11-06 20:35:09.264061: step 26740, total loss = 0.30, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:43m:49s remains)
INFO - root - 2019-11-06 20:35:09.838876: step 26750, total loss = 0.31, predict loss = 0.10 (80.6 examples/sec; 0.050 sec/batch; 1h:41m:58s remains)
INFO - root - 2019-11-06 20:35:10.406658: step 26760, total loss = 0.23, predict loss = 0.06 (80.5 examples/sec; 0.050 sec/batch; 1h:42m:03s remains)
INFO - root - 2019-11-06 20:35:10.997707: step 26770, total loss = 0.19, predict loss = 0.04 (79.2 examples/sec; 0.050 sec/batch; 1h:43m:42s remains)
INFO - root - 2019-11-06 20:35:11.572417: step 26780, total loss = 0.37, predict loss = 0.10 (79.2 examples/sec; 0.051 sec/batch; 1h:43m:42s remains)
INFO - root - 2019-11-06 20:35:12.150616: step 26790, total loss = 0.20, predict loss = 0.05 (77.9 examples/sec; 0.051 sec/batch; 1h:45m:29s remains)
INFO - root - 2019-11-06 20:35:12.646391: step 26800, total loss = 0.25, predict loss = 0.07 (92.2 examples/sec; 0.043 sec/batch; 1h:29m:07s remains)
INFO - root - 2019-11-06 20:35:13.115480: step 26810, total loss = 0.49, predict loss = 0.13 (93.8 examples/sec; 0.043 sec/batch; 1h:27m:33s remains)
INFO - root - 2019-11-06 20:35:13.573966: step 26820, total loss = 0.35, predict loss = 0.09 (97.2 examples/sec; 0.041 sec/batch; 1h:24m:27s remains)
INFO - root - 2019-11-06 20:35:14.688771: step 26830, total loss = 0.30, predict loss = 0.09 (61.4 examples/sec; 0.065 sec/batch; 2h:13m:46s remains)
INFO - root - 2019-11-06 20:35:15.366788: step 26840, total loss = 0.56, predict loss = 0.18 (82.6 examples/sec; 0.048 sec/batch; 1h:39m:22s remains)
INFO - root - 2019-11-06 20:35:15.972634: step 26850, total loss = 0.24, predict loss = 0.05 (75.3 examples/sec; 0.053 sec/batch; 1h:48m:58s remains)
INFO - root - 2019-11-06 20:35:16.561689: step 26860, total loss = 0.16, predict loss = 0.04 (77.0 examples/sec; 0.052 sec/batch; 1h:46m:40s remains)
INFO - root - 2019-11-06 20:35:17.130241: step 26870, total loss = 0.26, predict loss = 0.06 (75.3 examples/sec; 0.053 sec/batch; 1h:48m:59s remains)
INFO - root - 2019-11-06 20:35:17.704815: step 26880, total loss = 0.16, predict loss = 0.04 (75.9 examples/sec; 0.053 sec/batch; 1h:48m:10s remains)
INFO - root - 2019-11-06 20:35:18.274221: step 26890, total loss = 0.22, predict loss = 0.05 (79.8 examples/sec; 0.050 sec/batch; 1h:42m:49s remains)
INFO - root - 2019-11-06 20:35:18.840470: step 26900, total loss = 0.39, predict loss = 0.11 (80.0 examples/sec; 0.050 sec/batch; 1h:42m:32s remains)
INFO - root - 2019-11-06 20:35:19.425480: step 26910, total loss = 0.35, predict loss = 0.09 (76.5 examples/sec; 0.052 sec/batch; 1h:47m:12s remains)
INFO - root - 2019-11-06 20:35:19.990232: step 26920, total loss = 0.16, predict loss = 0.04 (82.7 examples/sec; 0.048 sec/batch; 1h:39m:11s remains)
INFO - root - 2019-11-06 20:35:20.570251: step 26930, total loss = 0.34, predict loss = 0.09 (80.2 examples/sec; 0.050 sec/batch; 1h:42m:17s remains)
INFO - root - 2019-11-06 20:35:21.140492: step 26940, total loss = 0.32, predict loss = 0.09 (80.7 examples/sec; 0.050 sec/batch; 1h:41m:36s remains)
INFO - root - 2019-11-06 20:35:21.607069: step 26950, total loss = 0.32, predict loss = 0.08 (94.7 examples/sec; 0.042 sec/batch; 1h:26m:39s remains)
INFO - root - 2019-11-06 20:35:22.058924: step 26960, total loss = 0.28, predict loss = 0.06 (105.0 examples/sec; 0.038 sec/batch; 1h:18m:05s remains)
INFO - root - 2019-11-06 20:35:22.976990: step 26970, total loss = 0.27, predict loss = 0.08 (79.3 examples/sec; 0.050 sec/batch; 1h:43m:23s remains)
INFO - root - 2019-11-06 20:35:23.673882: step 26980, total loss = 0.25, predict loss = 0.06 (62.9 examples/sec; 0.064 sec/batch; 2h:10m:22s remains)
INFO - root - 2019-11-06 20:35:24.309119: step 26990, total loss = 0.38, predict loss = 0.10 (81.6 examples/sec; 0.049 sec/batch; 1h:40m:26s remains)
INFO - root - 2019-11-06 20:35:24.888225: step 27000, total loss = 0.26, predict loss = 0.07 (76.8 examples/sec; 0.052 sec/batch; 1h:46m:42s remains)
INFO - root - 2019-11-06 20:35:25.476640: step 27010, total loss = 0.24, predict loss = 0.07 (79.9 examples/sec; 0.050 sec/batch; 1h:42m:38s remains)
INFO - root - 2019-11-06 20:35:26.040973: step 27020, total loss = 0.23, predict loss = 0.06 (80.9 examples/sec; 0.049 sec/batch; 1h:41m:21s remains)
INFO - root - 2019-11-06 20:35:26.613434: step 27030, total loss = 0.27, predict loss = 0.07 (81.4 examples/sec; 0.049 sec/batch; 1h:40m:40s remains)
INFO - root - 2019-11-06 20:35:27.190009: step 27040, total loss = 0.36, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:43m:35s remains)
INFO - root - 2019-11-06 20:35:27.778858: step 27050, total loss = 0.22, predict loss = 0.05 (80.4 examples/sec; 0.050 sec/batch; 1h:41m:57s remains)
INFO - root - 2019-11-06 20:35:28.362168: step 27060, total loss = 0.20, predict loss = 0.04 (78.4 examples/sec; 0.051 sec/batch; 1h:44m:33s remains)
INFO - root - 2019-11-06 20:35:28.926269: step 27070, total loss = 0.33, predict loss = 0.10 (77.8 examples/sec; 0.051 sec/batch; 1h:45m:23s remains)
INFO - root - 2019-11-06 20:35:29.504527: step 27080, total loss = 0.36, predict loss = 0.10 (77.1 examples/sec; 0.052 sec/batch; 1h:46m:13s remains)
INFO - root - 2019-11-06 20:35:30.078078: step 27090, total loss = 0.23, predict loss = 0.06 (91.3 examples/sec; 0.044 sec/batch; 1h:29m:45s remains)
INFO - root - 2019-11-06 20:35:30.542492: step 27100, total loss = 0.31, predict loss = 0.09 (97.2 examples/sec; 0.041 sec/batch; 1h:24m:18s remains)
INFO - root - 2019-11-06 20:35:31.005785: step 27110, total loss = 0.24, predict loss = 0.06 (97.4 examples/sec; 0.041 sec/batch; 1h:24m:07s remains)
INFO - root - 2019-11-06 20:35:31.936644: step 27120, total loss = 0.44, predict loss = 0.14 (74.9 examples/sec; 0.053 sec/batch; 1h:49m:25s remains)
INFO - root - 2019-11-06 20:35:32.643809: step 27130, total loss = 0.30, predict loss = 0.08 (67.6 examples/sec; 0.059 sec/batch; 2h:01m:14s remains)
INFO - root - 2019-11-06 20:35:33.267410: step 27140, total loss = 0.28, predict loss = 0.07 (78.7 examples/sec; 0.051 sec/batch; 1h:44m:06s remains)
INFO - root - 2019-11-06 20:35:33.842710: step 27150, total loss = 0.25, predict loss = 0.06 (80.3 examples/sec; 0.050 sec/batch; 1h:42m:01s remains)
INFO - root - 2019-11-06 20:35:34.412741: step 27160, total loss = 0.23, predict loss = 0.05 (78.3 examples/sec; 0.051 sec/batch; 1h:44m:39s remains)
INFO - root - 2019-11-06 20:35:34.998002: step 27170, total loss = 0.33, predict loss = 0.09 (75.0 examples/sec; 0.053 sec/batch; 1h:49m:08s remains)
INFO - root - 2019-11-06 20:35:35.585529: step 27180, total loss = 0.27, predict loss = 0.07 (76.3 examples/sec; 0.052 sec/batch; 1h:47m:17s remains)
INFO - root - 2019-11-06 20:35:36.149796: step 27190, total loss = 0.36, predict loss = 0.12 (77.4 examples/sec; 0.052 sec/batch; 1h:45m:50s remains)
INFO - root - 2019-11-06 20:35:36.720604: step 27200, total loss = 0.35, predict loss = 0.10 (81.0 examples/sec; 0.049 sec/batch; 1h:41m:04s remains)
INFO - root - 2019-11-06 20:35:37.306783: step 27210, total loss = 0.22, predict loss = 0.05 (80.0 examples/sec; 0.050 sec/batch; 1h:42m:21s remains)
INFO - root - 2019-11-06 20:35:37.881038: step 27220, total loss = 0.29, predict loss = 0.08 (75.8 examples/sec; 0.053 sec/batch; 1h:47m:55s remains)
INFO - root - 2019-11-06 20:35:38.443996: step 27230, total loss = 0.39, predict loss = 0.10 (83.9 examples/sec; 0.048 sec/batch; 1h:37m:34s remains)
INFO - root - 2019-11-06 20:35:38.990810: step 27240, total loss = 0.23, predict loss = 0.06 (92.8 examples/sec; 0.043 sec/batch; 1h:28m:08s remains)
INFO - root - 2019-11-06 20:35:39.467798: step 27250, total loss = 0.25, predict loss = 0.07 (97.0 examples/sec; 0.041 sec/batch; 1h:24m:21s remains)
INFO - root - 2019-11-06 20:35:39.915768: step 27260, total loss = 0.21, predict loss = 0.06 (99.2 examples/sec; 0.040 sec/batch; 1h:22m:30s remains)
INFO - root - 2019-11-06 20:35:40.868490: step 27270, total loss = 0.20, predict loss = 0.04 (65.9 examples/sec; 0.061 sec/batch; 2h:04m:12s remains)
INFO - root - 2019-11-06 20:35:41.496646: step 27280, total loss = 0.30, predict loss = 0.07 (82.9 examples/sec; 0.048 sec/batch; 1h:38m:41s remains)
INFO - root - 2019-11-06 20:35:42.083127: step 27290, total loss = 0.35, predict loss = 0.11 (77.2 examples/sec; 0.052 sec/batch; 1h:45m:56s remains)
INFO - root - 2019-11-06 20:35:42.659507: step 27300, total loss = 0.32, predict loss = 0.08 (82.7 examples/sec; 0.048 sec/batch; 1h:38m:53s remains)
INFO - root - 2019-11-06 20:35:43.238331: step 27310, total loss = 0.26, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:46m:33s remains)
INFO - root - 2019-11-06 20:35:43.809513: step 27320, total loss = 0.26, predict loss = 0.07 (78.3 examples/sec; 0.051 sec/batch; 1h:44m:30s remains)
INFO - root - 2019-11-06 20:35:44.406103: step 27330, total loss = 0.26, predict loss = 0.07 (78.4 examples/sec; 0.051 sec/batch; 1h:44m:20s remains)
INFO - root - 2019-11-06 20:35:44.991140: step 27340, total loss = 0.25, predict loss = 0.07 (66.9 examples/sec; 0.060 sec/batch; 2h:02m:12s remains)
INFO - root - 2019-11-06 20:35:45.604182: step 27350, total loss = 0.22, predict loss = 0.06 (80.7 examples/sec; 0.050 sec/batch; 1h:41m:15s remains)
INFO - root - 2019-11-06 20:35:46.180125: step 27360, total loss = 0.24, predict loss = 0.06 (80.6 examples/sec; 0.050 sec/batch; 1h:41m:28s remains)
INFO - root - 2019-11-06 20:35:46.760387: step 27370, total loss = 0.34, predict loss = 0.11 (80.6 examples/sec; 0.050 sec/batch; 1h:41m:23s remains)
INFO - root - 2019-11-06 20:35:47.332540: step 27380, total loss = 0.23, predict loss = 0.06 (77.3 examples/sec; 0.052 sec/batch; 1h:45m:47s remains)
INFO - root - 2019-11-06 20:35:47.857452: step 27390, total loss = 0.34, predict loss = 0.09 (96.8 examples/sec; 0.041 sec/batch; 1h:24m:28s remains)
INFO - root - 2019-11-06 20:35:48.311456: step 27400, total loss = 0.30, predict loss = 0.09 (94.0 examples/sec; 0.043 sec/batch; 1h:26m:58s remains)
INFO - root - 2019-11-06 20:35:48.794198: step 27410, total loss = 0.26, predict loss = 0.06 (95.2 examples/sec; 0.042 sec/batch; 1h:25m:50s remains)
INFO - root - 2019-11-06 20:35:49.805992: step 27420, total loss = 0.20, predict loss = 0.05 (67.7 examples/sec; 0.059 sec/batch; 2h:00m:47s remains)
INFO - root - 2019-11-06 20:35:50.426366: step 27430, total loss = 0.38, predict loss = 0.11 (78.1 examples/sec; 0.051 sec/batch; 1h:44m:34s remains)
INFO - root - 2019-11-06 20:35:50.995887: step 27440, total loss = 0.29, predict loss = 0.09 (80.0 examples/sec; 0.050 sec/batch; 1h:42m:07s remains)
INFO - root - 2019-11-06 20:35:51.579075: step 27450, total loss = 0.31, predict loss = 0.08 (75.5 examples/sec; 0.053 sec/batch; 1h:48m:16s remains)
INFO - root - 2019-11-06 20:35:52.146826: step 27460, total loss = 0.23, predict loss = 0.06 (74.8 examples/sec; 0.054 sec/batch; 1h:49m:17s remains)
INFO - root - 2019-11-06 20:35:52.719269: step 27470, total loss = 0.23, predict loss = 0.06 (78.2 examples/sec; 0.051 sec/batch; 1h:44m:29s remains)
INFO - root - 2019-11-06 20:35:53.291581: step 27480, total loss = 0.30, predict loss = 0.07 (77.8 examples/sec; 0.051 sec/batch; 1h:44m:59s remains)
INFO - root - 2019-11-06 20:35:53.885800: step 27490, total loss = 0.26, predict loss = 0.07 (81.8 examples/sec; 0.049 sec/batch; 1h:39m:49s remains)
INFO - root - 2019-11-06 20:35:54.459004: step 27500, total loss = 0.32, predict loss = 0.09 (79.0 examples/sec; 0.051 sec/batch; 1h:43m:25s remains)
INFO - root - 2019-11-06 20:35:55.043846: step 27510, total loss = 0.48, predict loss = 0.11 (76.4 examples/sec; 0.052 sec/batch; 1h:46m:50s remains)
INFO - root - 2019-11-06 20:35:55.624420: step 27520, total loss = 0.21, predict loss = 0.06 (80.8 examples/sec; 0.050 sec/batch; 1h:41m:06s remains)
INFO - root - 2019-11-06 20:35:56.207898: step 27530, total loss = 0.21, predict loss = 0.05 (80.9 examples/sec; 0.049 sec/batch; 1h:40m:56s remains)
INFO - root - 2019-11-06 20:35:56.704425: step 27540, total loss = 0.24, predict loss = 0.06 (106.1 examples/sec; 0.038 sec/batch; 1h:16m:57s remains)
INFO - root - 2019-11-06 20:35:57.146256: step 27550, total loss = 0.35, predict loss = 0.11 (91.9 examples/sec; 0.044 sec/batch; 1h:28m:48s remains)
INFO - root - 2019-11-06 20:35:57.610917: step 27560, total loss = 0.45, predict loss = 0.13 (97.4 examples/sec; 0.041 sec/batch; 1h:23m:46s remains)
INFO - root - 2019-11-06 20:35:58.760607: step 27570, total loss = 0.27, predict loss = 0.07 (49.1 examples/sec; 0.081 sec/batch; 2h:46m:10s remains)
INFO - root - 2019-11-06 20:35:59.401116: step 27580, total loss = 0.22, predict loss = 0.05 (83.1 examples/sec; 0.048 sec/batch; 1h:38m:10s remains)
INFO - root - 2019-11-06 20:35:59.989438: step 27590, total loss = 0.20, predict loss = 0.05 (77.4 examples/sec; 0.052 sec/batch; 1h:45m:23s remains)
INFO - root - 2019-11-06 20:36:00.609033: step 27600, total loss = 0.23, predict loss = 0.05 (61.1 examples/sec; 0.065 sec/batch; 2h:13m:33s remains)
INFO - root - 2019-11-06 20:36:01.206891: step 27610, total loss = 0.26, predict loss = 0.07 (80.5 examples/sec; 0.050 sec/batch; 1h:41m:21s remains)
INFO - root - 2019-11-06 20:36:01.822696: step 27620, total loss = 0.39, predict loss = 0.10 (76.7 examples/sec; 0.052 sec/batch; 1h:46m:20s remains)
INFO - root - 2019-11-06 20:36:02.399323: step 27630, total loss = 0.27, predict loss = 0.07 (75.6 examples/sec; 0.053 sec/batch; 1h:47m:52s remains)
INFO - root - 2019-11-06 20:36:02.979169: step 27640, total loss = 0.34, predict loss = 0.10 (81.0 examples/sec; 0.049 sec/batch; 1h:40m:45s remains)
INFO - root - 2019-11-06 20:36:03.639561: step 27650, total loss = 0.20, predict loss = 0.05 (73.3 examples/sec; 0.055 sec/batch; 1h:51m:17s remains)
INFO - root - 2019-11-06 20:36:04.331872: step 27660, total loss = 0.16, predict loss = 0.04 (70.1 examples/sec; 0.057 sec/batch; 1h:56m:19s remains)
INFO - root - 2019-11-06 20:36:04.948191: step 27670, total loss = 0.28, predict loss = 0.08 (75.2 examples/sec; 0.053 sec/batch; 1h:48m:23s remains)
INFO - root - 2019-11-06 20:36:05.558679: step 27680, total loss = 0.31, predict loss = 0.07 (79.0 examples/sec; 0.051 sec/batch; 1h:43m:10s remains)
INFO - root - 2019-11-06 20:36:06.055244: step 27690, total loss = 0.59, predict loss = 0.18 (105.6 examples/sec; 0.038 sec/batch; 1h:17m:11s remains)
INFO - root - 2019-11-06 20:36:06.497754: step 27700, total loss = 0.24, predict loss = 0.06 (99.0 examples/sec; 0.040 sec/batch; 1h:22m:20s remains)
INFO - root - 2019-11-06 20:36:07.663178: step 27710, total loss = 0.35, predict loss = 0.10 (5.4 examples/sec; 0.738 sec/batch; 25h:04m:09s remains)
INFO - root - 2019-11-06 20:36:08.382131: step 27720, total loss = 0.21, predict loss = 0.04 (70.1 examples/sec; 0.057 sec/batch; 1h:56m:18s remains)
INFO - root - 2019-11-06 20:36:09.098518: step 27730, total loss = 0.26, predict loss = 0.06 (60.9 examples/sec; 0.066 sec/batch; 2h:13m:50s remains)
INFO - root - 2019-11-06 20:36:09.777140: step 27740, total loss = 0.41, predict loss = 0.11 (76.9 examples/sec; 0.052 sec/batch; 1h:46m:00s remains)
INFO - root - 2019-11-06 20:36:10.365729: step 27750, total loss = 0.28, predict loss = 0.08 (79.9 examples/sec; 0.050 sec/batch; 1h:41m:58s remains)
INFO - root - 2019-11-06 20:36:10.936887: step 27760, total loss = 0.32, predict loss = 0.09 (78.6 examples/sec; 0.051 sec/batch; 1h:43m:37s remains)
INFO - root - 2019-11-06 20:36:11.651534: step 27770, total loss = 0.28, predict loss = 0.08 (65.7 examples/sec; 0.061 sec/batch; 2h:04m:03s remains)
INFO - root - 2019-11-06 20:36:12.388620: step 27780, total loss = 0.29, predict loss = 0.08 (60.1 examples/sec; 0.067 sec/batch; 2h:15m:33s remains)
INFO - root - 2019-11-06 20:36:13.173105: step 27790, total loss = 0.29, predict loss = 0.08 (58.4 examples/sec; 0.068 sec/batch; 2h:19m:26s remains)
INFO - root - 2019-11-06 20:36:13.832302: step 27800, total loss = 0.24, predict loss = 0.06 (78.1 examples/sec; 0.051 sec/batch; 1h:44m:20s remains)
INFO - root - 2019-11-06 20:36:14.441396: step 27810, total loss = 0.25, predict loss = 0.06 (77.6 examples/sec; 0.052 sec/batch; 1h:44m:57s remains)
INFO - root - 2019-11-06 20:36:15.042170: step 27820, total loss = 0.30, predict loss = 0.08 (61.0 examples/sec; 0.066 sec/batch; 2h:13m:27s remains)
INFO - root - 2019-11-06 20:36:15.643183: step 27830, total loss = 0.34, predict loss = 0.09 (87.0 examples/sec; 0.046 sec/batch; 1h:33m:39s remains)
INFO - root - 2019-11-06 20:36:16.140447: step 27840, total loss = 0.24, predict loss = 0.07 (85.2 examples/sec; 0.047 sec/batch; 1h:35m:36s remains)
INFO - root - 2019-11-06 20:36:16.678895: step 27850, total loss = 0.23, predict loss = 0.06 (103.9 examples/sec; 0.039 sec/batch; 1h:18m:23s remains)
INFO - root - 2019-11-06 20:36:17.598907: step 27860, total loss = 0.18, predict loss = 0.04 (75.1 examples/sec; 0.053 sec/batch; 1h:48m:26s remains)
INFO - root - 2019-11-06 20:36:18.238738: step 27870, total loss = 0.40, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 1h:56m:39s remains)
INFO - root - 2019-11-06 20:36:18.865615: step 27880, total loss = 0.18, predict loss = 0.04 (71.4 examples/sec; 0.056 sec/batch; 1h:54m:04s remains)
INFO - root - 2019-11-06 20:36:19.504275: step 27890, total loss = 0.22, predict loss = 0.05 (58.1 examples/sec; 0.069 sec/batch; 2h:20m:05s remains)
INFO - root - 2019-11-06 20:36:20.200176: step 27900, total loss = 0.25, predict loss = 0.06 (80.8 examples/sec; 0.049 sec/batch; 1h:40m:41s remains)
INFO - root - 2019-11-06 20:36:20.769753: step 27910, total loss = 0.31, predict loss = 0.09 (81.3 examples/sec; 0.049 sec/batch; 1h:40m:04s remains)
INFO - root - 2019-11-06 20:36:21.349518: step 27920, total loss = 0.39, predict loss = 0.11 (75.9 examples/sec; 0.053 sec/batch; 1h:47m:15s remains)
INFO - root - 2019-11-06 20:36:21.958520: step 27930, total loss = 0.19, predict loss = 0.05 (76.2 examples/sec; 0.052 sec/batch; 1h:46m:47s remains)
INFO - root - 2019-11-06 20:36:22.542369: step 27940, total loss = 0.20, predict loss = 0.05 (71.5 examples/sec; 0.056 sec/batch; 1h:53m:52s remains)
INFO - root - 2019-11-06 20:36:23.120410: step 27950, total loss = 0.32, predict loss = 0.09 (76.5 examples/sec; 0.052 sec/batch; 1h:46m:23s remains)
INFO - root - 2019-11-06 20:36:23.713173: step 27960, total loss = 0.29, predict loss = 0.07 (72.5 examples/sec; 0.055 sec/batch; 1h:52m:14s remains)
INFO - root - 2019-11-06 20:36:24.315583: step 27970, total loss = 0.29, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:42m:07s remains)
INFO - root - 2019-11-06 20:36:24.961407: step 27980, total loss = 0.26, predict loss = 0.07 (93.4 examples/sec; 0.043 sec/batch; 1h:27m:07s remains)
INFO - root - 2019-11-06 20:36:25.416094: step 27990, total loss = 0.22, predict loss = 0.06 (99.8 examples/sec; 0.040 sec/batch; 1h:21m:28s remains)
INFO - root - 2019-11-06 20:36:25.864948: step 28000, total loss = 0.17, predict loss = 0.05 (92.6 examples/sec; 0.043 sec/batch; 1h:27m:49s remains)
INFO - root - 2019-11-06 20:36:26.883269: step 28010, total loss = 0.36, predict loss = 0.08 (74.7 examples/sec; 0.054 sec/batch; 1h:48m:51s remains)
INFO - root - 2019-11-06 20:36:27.657811: step 28020, total loss = 0.47, predict loss = 0.16 (58.6 examples/sec; 0.068 sec/batch; 2h:18m:41s remains)
INFO - root - 2019-11-06 20:36:28.277672: step 28030, total loss = 0.31, predict loss = 0.09 (79.9 examples/sec; 0.050 sec/batch; 1h:41m:44s remains)
INFO - root - 2019-11-06 20:36:28.856596: step 28040, total loss = 0.28, predict loss = 0.07 (79.6 examples/sec; 0.050 sec/batch; 1h:42m:06s remains)
INFO - root - 2019-11-06 20:36:29.471434: step 28050, total loss = 0.25, predict loss = 0.07 (80.0 examples/sec; 0.050 sec/batch; 1h:41m:40s remains)
INFO - root - 2019-11-06 20:36:30.078254: step 28060, total loss = 0.33, predict loss = 0.12 (80.8 examples/sec; 0.049 sec/batch; 1h:40m:35s remains)
INFO - root - 2019-11-06 20:36:30.665366: step 28070, total loss = 0.46, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 1h:57m:23s remains)
INFO - root - 2019-11-06 20:36:31.296322: step 28080, total loss = 0.19, predict loss = 0.04 (78.5 examples/sec; 0.051 sec/batch; 1h:43m:35s remains)
INFO - root - 2019-11-06 20:36:31.913082: step 28090, total loss = 0.41, predict loss = 0.12 (73.4 examples/sec; 0.055 sec/batch; 1h:50m:46s remains)
INFO - root - 2019-11-06 20:36:32.501689: step 28100, total loss = 0.27, predict loss = 0.07 (78.2 examples/sec; 0.051 sec/batch; 1h:43m:55s remains)
INFO - root - 2019-11-06 20:36:33.122752: step 28110, total loss = 0.24, predict loss = 0.06 (72.9 examples/sec; 0.055 sec/batch; 1h:51m:27s remains)
INFO - root - 2019-11-06 20:36:33.720232: step 28120, total loss = 0.43, predict loss = 0.11 (75.2 examples/sec; 0.053 sec/batch; 1h:48m:00s remains)
INFO - root - 2019-11-06 20:36:34.285056: step 28130, total loss = 0.42, predict loss = 0.11 (92.5 examples/sec; 0.043 sec/batch; 1h:27m:48s remains)
INFO - root - 2019-11-06 20:36:34.740576: step 28140, total loss = 0.41, predict loss = 0.11 (98.3 examples/sec; 0.041 sec/batch; 1h:22m:36s remains)
INFO - root - 2019-11-06 20:36:35.203993: step 28150, total loss = 0.23, predict loss = 0.05 (97.0 examples/sec; 0.041 sec/batch; 1h:23m:44s remains)
INFO - root - 2019-11-06 20:36:36.253754: step 28160, total loss = 0.24, predict loss = 0.06 (65.7 examples/sec; 0.061 sec/batch; 2h:03m:38s remains)
INFO - root - 2019-11-06 20:36:36.887494: step 28170, total loss = 0.20, predict loss = 0.05 (66.5 examples/sec; 0.060 sec/batch; 2h:02m:04s remains)
INFO - root - 2019-11-06 20:36:37.533499: step 28180, total loss = 0.16, predict loss = 0.04 (63.8 examples/sec; 0.063 sec/batch; 2h:07m:19s remains)
INFO - root - 2019-11-06 20:36:38.140127: step 28190, total loss = 0.19, predict loss = 0.04 (77.4 examples/sec; 0.052 sec/batch; 1h:44m:58s remains)
INFO - root - 2019-11-06 20:36:38.728224: step 28200, total loss = 0.24, predict loss = 0.06 (79.0 examples/sec; 0.051 sec/batch; 1h:42m:50s remains)
INFO - root - 2019-11-06 20:36:39.314513: step 28210, total loss = 0.39, predict loss = 0.11 (76.1 examples/sec; 0.053 sec/batch; 1h:46m:39s remains)
INFO - root - 2019-11-06 20:36:39.903656: step 28220, total loss = 0.20, predict loss = 0.05 (76.1 examples/sec; 0.053 sec/batch; 1h:46m:44s remains)
INFO - root - 2019-11-06 20:36:40.483821: step 28230, total loss = 0.25, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:41m:47s remains)
INFO - root - 2019-11-06 20:36:41.073494: step 28240, total loss = 0.19, predict loss = 0.04 (72.8 examples/sec; 0.055 sec/batch; 1h:51m:31s remains)
INFO - root - 2019-11-06 20:36:41.670153: step 28250, total loss = 0.26, predict loss = 0.07 (74.0 examples/sec; 0.054 sec/batch; 1h:49m:41s remains)
INFO - root - 2019-11-06 20:36:42.248488: step 28260, total loss = 0.24, predict loss = 0.06 (81.7 examples/sec; 0.049 sec/batch; 1h:39m:17s remains)
INFO - root - 2019-11-06 20:36:42.854992: step 28270, total loss = 0.35, predict loss = 0.10 (52.9 examples/sec; 0.076 sec/batch; 2h:33m:24s remains)
INFO - root - 2019-11-06 20:36:43.386702: step 28280, total loss = 0.30, predict loss = 0.07 (104.0 examples/sec; 0.038 sec/batch; 1h:18m:00s remains)
INFO - root - 2019-11-06 20:36:43.869765: step 28290, total loss = 0.36, predict loss = 0.09 (89.1 examples/sec; 0.045 sec/batch; 1h:31m:02s remains)
INFO - root - 2019-11-06 20:36:44.324949: step 28300, total loss = 0.34, predict loss = 0.10 (94.6 examples/sec; 0.042 sec/batch; 1h:25m:45s remains)
INFO - root - 2019-11-06 20:36:45.346018: step 28310, total loss = 0.23, predict loss = 0.05 (77.1 examples/sec; 0.052 sec/batch; 1h:45m:11s remains)
INFO - root - 2019-11-06 20:36:45.951865: step 28320, total loss = 0.26, predict loss = 0.07 (76.7 examples/sec; 0.052 sec/batch; 1h:45m:45s remains)
INFO - root - 2019-11-06 20:36:46.600080: step 28330, total loss = 0.32, predict loss = 0.08 (78.2 examples/sec; 0.051 sec/batch; 1h:43m:44s remains)
INFO - root - 2019-11-06 20:36:47.183146: step 28340, total loss = 0.26, predict loss = 0.07 (72.5 examples/sec; 0.055 sec/batch; 1h:51m:54s remains)
INFO - root - 2019-11-06 20:36:47.760969: step 28350, total loss = 0.47, predict loss = 0.12 (79.8 examples/sec; 0.050 sec/batch; 1h:41m:34s remains)
INFO - root - 2019-11-06 20:36:48.336797: step 28360, total loss = 0.23, predict loss = 0.06 (79.4 examples/sec; 0.050 sec/batch; 1h:42m:07s remains)
INFO - root - 2019-11-06 20:36:48.964923: step 28370, total loss = 0.34, predict loss = 0.09 (79.3 examples/sec; 0.050 sec/batch; 1h:42m:13s remains)
INFO - root - 2019-11-06 20:36:49.552771: step 28380, total loss = 0.23, predict loss = 0.05 (79.5 examples/sec; 0.050 sec/batch; 1h:41m:55s remains)
INFO - root - 2019-11-06 20:36:50.114303: step 28390, total loss = 0.21, predict loss = 0.05 (78.2 examples/sec; 0.051 sec/batch; 1h:43m:38s remains)
INFO - root - 2019-11-06 20:36:50.679600: step 28400, total loss = 0.28, predict loss = 0.10 (77.0 examples/sec; 0.052 sec/batch; 1h:45m:19s remains)
INFO - root - 2019-11-06 20:36:51.314996: step 28410, total loss = 0.17, predict loss = 0.05 (75.4 examples/sec; 0.053 sec/batch; 1h:47m:28s remains)
INFO - root - 2019-11-06 20:36:51.887604: step 28420, total loss = 0.35, predict loss = 0.11 (80.2 examples/sec; 0.050 sec/batch; 1h:41m:00s remains)
INFO - root - 2019-11-06 20:36:52.372241: step 28430, total loss = 0.22, predict loss = 0.06 (97.2 examples/sec; 0.041 sec/batch; 1h:23m:22s remains)
INFO - root - 2019-11-06 20:36:52.819760: step 28440, total loss = 0.28, predict loss = 0.07 (92.2 examples/sec; 0.043 sec/batch; 1h:27m:51s remains)
INFO - root - 2019-11-06 20:36:53.296647: step 28450, total loss = 0.17, predict loss = 0.05 (98.4 examples/sec; 0.041 sec/batch; 1h:22m:18s remains)
INFO - root - 2019-11-06 20:36:54.376441: step 28460, total loss = 0.33, predict loss = 0.08 (61.6 examples/sec; 0.065 sec/batch; 2h:11m:36s remains)
INFO - root - 2019-11-06 20:36:55.005028: step 28470, total loss = 0.25, predict loss = 0.05 (72.1 examples/sec; 0.055 sec/batch; 1h:52m:18s remains)
INFO - root - 2019-11-06 20:36:55.592169: step 28480, total loss = 0.28, predict loss = 0.07 (81.1 examples/sec; 0.049 sec/batch; 1h:39m:52s remains)
INFO - root - 2019-11-06 20:36:56.169767: step 28490, total loss = 0.22, predict loss = 0.05 (80.7 examples/sec; 0.050 sec/batch; 1h:40m:19s remains)
INFO - root - 2019-11-06 20:36:56.755249: step 28500, total loss = 0.35, predict loss = 0.09 (78.6 examples/sec; 0.051 sec/batch; 1h:42m:59s remains)
INFO - root - 2019-11-06 20:36:57.346262: step 28510, total loss = 0.24, predict loss = 0.07 (76.9 examples/sec; 0.052 sec/batch; 1h:45m:18s remains)
INFO - root - 2019-11-06 20:36:57.915327: step 28520, total loss = 0.20, predict loss = 0.05 (75.7 examples/sec; 0.053 sec/batch; 1h:46m:56s remains)
INFO - root - 2019-11-06 20:36:58.510482: step 28530, total loss = 0.35, predict loss = 0.09 (76.1 examples/sec; 0.053 sec/batch; 1h:46m:28s remains)
INFO - root - 2019-11-06 20:36:59.084913: step 28540, total loss = 0.21, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:43m:52s remains)
INFO - root - 2019-11-06 20:36:59.670568: step 28550, total loss = 0.18, predict loss = 0.04 (76.5 examples/sec; 0.052 sec/batch; 1h:45m:49s remains)
INFO - root - 2019-11-06 20:37:00.239239: step 28560, total loss = 0.25, predict loss = 0.07 (82.6 examples/sec; 0.048 sec/batch; 1h:38m:04s remains)
INFO - root - 2019-11-06 20:37:00.822053: step 28570, total loss = 0.24, predict loss = 0.07 (81.7 examples/sec; 0.049 sec/batch; 1h:39m:01s remains)
INFO - root - 2019-11-06 20:37:01.289583: step 28580, total loss = 0.30, predict loss = 0.08 (94.5 examples/sec; 0.042 sec/batch; 1h:25m:37s remains)
INFO - root - 2019-11-06 20:37:01.738505: step 28590, total loss = 0.16, predict loss = 0.04 (97.8 examples/sec; 0.041 sec/batch; 1h:22m:46s remains)
INFO - root - 2019-11-06 20:37:02.647255: step 28600, total loss = 0.29, predict loss = 0.07 (75.7 examples/sec; 0.053 sec/batch; 1h:46m:53s remains)
INFO - root - 2019-11-06 20:37:03.260321: step 28610, total loss = 0.31, predict loss = 0.09 (73.9 examples/sec; 0.054 sec/batch; 1h:49m:27s remains)
INFO - root - 2019-11-06 20:37:03.851072: step 28620, total loss = 0.33, predict loss = 0.08 (83.2 examples/sec; 0.048 sec/batch; 1h:37m:16s remains)
INFO - root - 2019-11-06 20:37:04.410703: step 28630, total loss = 0.34, predict loss = 0.09 (77.0 examples/sec; 0.052 sec/batch; 1h:45m:07s remains)
INFO - root - 2019-11-06 20:37:04.995814: step 28640, total loss = 0.36, predict loss = 0.12 (79.8 examples/sec; 0.050 sec/batch; 1h:41m:20s remains)
INFO - root - 2019-11-06 20:37:05.580345: step 28650, total loss = 0.28, predict loss = 0.07 (79.8 examples/sec; 0.050 sec/batch; 1h:41m:25s remains)
INFO - root - 2019-11-06 20:37:06.146298: step 28660, total loss = 0.21, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:43m:43s remains)
INFO - root - 2019-11-06 20:37:06.717284: step 28670, total loss = 0.27, predict loss = 0.07 (75.8 examples/sec; 0.053 sec/batch; 1h:46m:45s remains)
INFO - root - 2019-11-06 20:37:07.295156: step 28680, total loss = 0.18, predict loss = 0.05 (79.4 examples/sec; 0.050 sec/batch; 1h:41m:52s remains)
INFO - root - 2019-11-06 20:37:07.891669: step 28690, total loss = 0.27, predict loss = 0.06 (79.4 examples/sec; 0.050 sec/batch; 1h:41m:52s remains)
INFO - root - 2019-11-06 20:37:08.472325: step 28700, total loss = 0.33, predict loss = 0.08 (80.3 examples/sec; 0.050 sec/batch; 1h:40m:41s remains)
INFO - root - 2019-11-06 20:37:09.057467: step 28710, total loss = 0.23, predict loss = 0.06 (81.5 examples/sec; 0.049 sec/batch; 1h:39m:16s remains)
INFO - root - 2019-11-06 20:37:09.621314: step 28720, total loss = 0.17, predict loss = 0.05 (83.4 examples/sec; 0.048 sec/batch; 1h:36m:54s remains)
INFO - root - 2019-11-06 20:37:10.096305: step 28730, total loss = 0.28, predict loss = 0.08 (100.1 examples/sec; 0.040 sec/batch; 1h:20m:46s remains)
INFO - root - 2019-11-06 20:37:10.539234: step 28740, total loss = 0.32, predict loss = 0.08 (95.3 examples/sec; 0.042 sec/batch; 1h:24m:49s remains)
INFO - root - 2019-11-06 20:37:11.478008: step 28750, total loss = 0.19, predict loss = 0.04 (73.7 examples/sec; 0.054 sec/batch; 1h:49m:40s remains)
INFO - root - 2019-11-06 20:37:12.192743: step 28760, total loss = 0.19, predict loss = 0.04 (65.3 examples/sec; 0.061 sec/batch; 2h:03m:46s remains)
INFO - root - 2019-11-06 20:37:12.850585: step 28770, total loss = 0.39, predict loss = 0.11 (65.6 examples/sec; 0.061 sec/batch; 2h:03m:12s remains)
INFO - root - 2019-11-06 20:37:13.454645: step 28780, total loss = 0.23, predict loss = 0.06 (76.4 examples/sec; 0.052 sec/batch; 1h:45m:43s remains)
INFO - root - 2019-11-06 20:37:14.028201: step 28790, total loss = 0.64, predict loss = 0.18 (75.3 examples/sec; 0.053 sec/batch; 1h:47m:20s remains)
INFO - root - 2019-11-06 20:37:14.598892: step 28800, total loss = 0.28, predict loss = 0.07 (80.9 examples/sec; 0.049 sec/batch; 1h:39m:53s remains)
INFO - root - 2019-11-06 20:37:15.186702: step 28810, total loss = 0.33, predict loss = 0.09 (77.4 examples/sec; 0.052 sec/batch; 1h:44m:26s remains)
INFO - root - 2019-11-06 20:37:15.757848: step 28820, total loss = 0.25, predict loss = 0.06 (80.7 examples/sec; 0.050 sec/batch; 1h:40m:06s remains)
INFO - root - 2019-11-06 20:37:16.335547: step 28830, total loss = 0.29, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 1h:43m:07s remains)
INFO - root - 2019-11-06 20:37:16.922573: step 28840, total loss = 0.22, predict loss = 0.05 (75.8 examples/sec; 0.053 sec/batch; 1h:46m:35s remains)
INFO - root - 2019-11-06 20:37:17.523461: step 28850, total loss = 0.30, predict loss = 0.08 (81.2 examples/sec; 0.049 sec/batch; 1h:39m:27s remains)
INFO - root - 2019-11-06 20:37:18.095046: step 28860, total loss = 0.55, predict loss = 0.15 (81.8 examples/sec; 0.049 sec/batch; 1h:38m:45s remains)
INFO - root - 2019-11-06 20:37:18.649871: step 28870, total loss = 0.34, predict loss = 0.08 (95.7 examples/sec; 0.042 sec/batch; 1h:24m:23s remains)
INFO - root - 2019-11-06 20:37:19.096790: step 28880, total loss = 0.22, predict loss = 0.05 (96.4 examples/sec; 0.042 sec/batch; 1h:23m:47s remains)
INFO - root - 2019-11-06 20:37:19.579631: step 28890, total loss = 0.37, predict loss = 0.10 (95.4 examples/sec; 0.042 sec/batch; 1h:24m:39s remains)
INFO - root - 2019-11-06 20:37:20.579477: step 28900, total loss = 0.27, predict loss = 0.07 (60.7 examples/sec; 0.066 sec/batch; 2h:13m:02s remains)
INFO - root - 2019-11-06 20:37:21.190111: step 28910, total loss = 0.21, predict loss = 0.05 (80.3 examples/sec; 0.050 sec/batch; 1h:40m:34s remains)
INFO - root - 2019-11-06 20:37:21.750051: step 28920, total loss = 0.14, predict loss = 0.03 (79.4 examples/sec; 0.050 sec/batch; 1h:41m:37s remains)
INFO - root - 2019-11-06 20:37:22.343679: step 28930, total loss = 0.27, predict loss = 0.06 (74.5 examples/sec; 0.054 sec/batch; 1h:48m:17s remains)
INFO - root - 2019-11-06 20:37:22.916808: step 28940, total loss = 0.23, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:44m:17s remains)
INFO - root - 2019-11-06 20:37:23.492541: step 28950, total loss = 0.30, predict loss = 0.07 (79.1 examples/sec; 0.051 sec/batch; 1h:42m:03s remains)
INFO - root - 2019-11-06 20:37:24.066177: step 28960, total loss = 0.32, predict loss = 0.08 (78.1 examples/sec; 0.051 sec/batch; 1h:43m:17s remains)
INFO - root - 2019-11-06 20:37:24.645986: step 28970, total loss = 0.30, predict loss = 0.09 (78.1 examples/sec; 0.051 sec/batch; 1h:43m:16s remains)
INFO - root - 2019-11-06 20:37:25.228487: step 28980, total loss = 0.25, predict loss = 0.06 (78.4 examples/sec; 0.051 sec/batch; 1h:42m:55s remains)
INFO - root - 2019-11-06 20:37:25.802284: step 28990, total loss = 0.18, predict loss = 0.05 (77.9 examples/sec; 0.051 sec/batch; 1h:43m:32s remains)
INFO - root - 2019-11-06 20:37:26.375828: step 29000, total loss = 0.28, predict loss = 0.07 (82.8 examples/sec; 0.048 sec/batch; 1h:37m:25s remains)
INFO - root - 2019-11-06 20:37:26.963290: step 29010, total loss = 0.23, predict loss = 0.06 (81.1 examples/sec; 0.049 sec/batch; 1h:39m:24s remains)
INFO - root - 2019-11-06 20:37:27.493720: step 29020, total loss = 0.33, predict loss = 0.09 (95.2 examples/sec; 0.042 sec/batch; 1h:24m:41s remains)
INFO - root - 2019-11-06 20:37:27.945612: step 29030, total loss = 0.30, predict loss = 0.08 (102.9 examples/sec; 0.039 sec/batch; 1h:18m:23s remains)
INFO - root - 2019-11-06 20:37:28.394219: step 29040, total loss = 0.23, predict loss = 0.06 (93.0 examples/sec; 0.043 sec/batch; 1h:26m:40s remains)
INFO - root - 2019-11-06 20:37:29.470223: step 29050, total loss = 0.32, predict loss = 0.09 (61.8 examples/sec; 0.065 sec/batch; 2h:10m:32s remains)
INFO - root - 2019-11-06 20:37:30.091031: step 29060, total loss = 0.29, predict loss = 0.09 (79.5 examples/sec; 0.050 sec/batch; 1h:41m:26s remains)
INFO - root - 2019-11-06 20:37:30.666115: step 29070, total loss = 0.26, predict loss = 0.06 (75.1 examples/sec; 0.053 sec/batch; 1h:47m:24s remains)
INFO - root - 2019-11-06 20:37:31.241640: step 29080, total loss = 0.26, predict loss = 0.07 (77.7 examples/sec; 0.051 sec/batch; 1h:43m:44s remains)
INFO - root - 2019-11-06 20:37:31.828403: step 29090, total loss = 0.27, predict loss = 0.07 (77.6 examples/sec; 0.052 sec/batch; 1h:43m:49s remains)
INFO - root - 2019-11-06 20:37:32.412605: step 29100, total loss = 0.22, predict loss = 0.06 (72.9 examples/sec; 0.055 sec/batch; 1h:50m:30s remains)
INFO - root - 2019-11-06 20:37:32.990478: step 29110, total loss = 0.24, predict loss = 0.07 (78.3 examples/sec; 0.051 sec/batch; 1h:42m:59s remains)
INFO - root - 2019-11-06 20:37:33.562386: step 29120, total loss = 0.26, predict loss = 0.07 (82.6 examples/sec; 0.048 sec/batch; 1h:37m:30s remains)
INFO - root - 2019-11-06 20:37:34.149972: step 29130, total loss = 0.33, predict loss = 0.09 (75.1 examples/sec; 0.053 sec/batch; 1h:47m:22s remains)
INFO - root - 2019-11-06 20:37:34.727702: step 29140, total loss = 0.24, predict loss = 0.06 (84.0 examples/sec; 0.048 sec/batch; 1h:35m:57s remains)
INFO - root - 2019-11-06 20:37:35.310423: step 29150, total loss = 0.35, predict loss = 0.10 (75.0 examples/sec; 0.053 sec/batch; 1h:47m:21s remains)
INFO - root - 2019-11-06 20:37:35.879409: step 29160, total loss = 0.18, predict loss = 0.05 (75.1 examples/sec; 0.053 sec/batch; 1h:47m:16s remains)
INFO - root - 2019-11-06 20:37:36.409068: step 29170, total loss = 0.38, predict loss = 0.09 (96.5 examples/sec; 0.041 sec/batch; 1h:23m:26s remains)
INFO - root - 2019-11-06 20:37:36.878475: step 29180, total loss = 0.22, predict loss = 0.06 (96.0 examples/sec; 0.042 sec/batch; 1h:23m:55s remains)
INFO - root - 2019-11-06 20:37:37.332862: step 29190, total loss = 0.29, predict loss = 0.08 (95.4 examples/sec; 0.042 sec/batch; 1h:24m:23s remains)
INFO - root - 2019-11-06 20:37:38.398918: step 29200, total loss = 0.39, predict loss = 0.10 (62.3 examples/sec; 0.064 sec/batch; 2h:09m:16s remains)
INFO - root - 2019-11-06 20:37:39.078584: step 29210, total loss = 0.19, predict loss = 0.04 (73.4 examples/sec; 0.055 sec/batch; 1h:49m:45s remains)
INFO - root - 2019-11-06 20:37:39.665229: step 29220, total loss = 0.20, predict loss = 0.05 (76.4 examples/sec; 0.052 sec/batch; 1h:45m:20s remains)
INFO - root - 2019-11-06 20:37:40.225793: step 29230, total loss = 0.24, predict loss = 0.06 (80.7 examples/sec; 0.050 sec/batch; 1h:39m:45s remains)
INFO - root - 2019-11-06 20:37:40.802956: step 29240, total loss = 0.24, predict loss = 0.06 (80.8 examples/sec; 0.050 sec/batch; 1h:39m:39s remains)
INFO - root - 2019-11-06 20:37:41.386775: step 29250, total loss = 0.16, predict loss = 0.05 (77.5 examples/sec; 0.052 sec/batch; 1h:43m:48s remains)
INFO - root - 2019-11-06 20:37:41.983003: step 29260, total loss = 0.25, predict loss = 0.06 (71.7 examples/sec; 0.056 sec/batch; 1h:52m:12s remains)
INFO - root - 2019-11-06 20:37:42.569881: step 29270, total loss = 0.24, predict loss = 0.06 (76.4 examples/sec; 0.052 sec/batch; 1h:45m:22s remains)
INFO - root - 2019-11-06 20:37:43.187560: step 29280, total loss = 0.30, predict loss = 0.08 (76.1 examples/sec; 0.053 sec/batch; 1h:45m:46s remains)
INFO - root - 2019-11-06 20:37:43.789863: step 29290, total loss = 0.27, predict loss = 0.08 (79.4 examples/sec; 0.050 sec/batch; 1h:41m:24s remains)
INFO - root - 2019-11-06 20:37:44.357316: step 29300, total loss = 0.24, predict loss = 0.06 (73.1 examples/sec; 0.055 sec/batch; 1h:50m:02s remains)
INFO - root - 2019-11-06 20:37:44.952702: step 29310, total loss = 0.25, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:40m:45s remains)
INFO - root - 2019-11-06 20:37:45.435562: step 29320, total loss = 0.27, predict loss = 0.07 (99.7 examples/sec; 0.040 sec/batch; 1h:20m:41s remains)
INFO - root - 2019-11-06 20:37:45.918622: step 29330, total loss = 0.30, predict loss = 0.10 (98.4 examples/sec; 0.041 sec/batch; 1h:21m:43s remains)
INFO - root - 2019-11-06 20:37:46.819394: step 29340, total loss = 0.24, predict loss = 0.06 (8.2 examples/sec; 0.485 sec/batch; 16h:15m:43s remains)
INFO - root - 2019-11-06 20:37:47.464101: step 29350, total loss = 0.40, predict loss = 0.11 (63.7 examples/sec; 0.063 sec/batch; 2h:06m:14s remains)
INFO - root - 2019-11-06 20:37:48.080597: step 29360, total loss = 0.18, predict loss = 0.05 (80.2 examples/sec; 0.050 sec/batch; 1h:40m:13s remains)
INFO - root - 2019-11-06 20:37:48.665142: step 29370, total loss = 0.30, predict loss = 0.08 (82.1 examples/sec; 0.049 sec/batch; 1h:37m:54s remains)
INFO - root - 2019-11-06 20:37:49.242170: step 29380, total loss = 0.24, predict loss = 0.06 (83.3 examples/sec; 0.048 sec/batch; 1h:36m:35s remains)
INFO - root - 2019-11-06 20:37:49.816667: step 29390, total loss = 0.27, predict loss = 0.07 (75.0 examples/sec; 0.053 sec/batch; 1h:47m:14s remains)
INFO - root - 2019-11-06 20:37:50.388655: step 29400, total loss = 0.25, predict loss = 0.06 (76.9 examples/sec; 0.052 sec/batch; 1h:44m:33s remains)
INFO - root - 2019-11-06 20:37:50.978638: step 29410, total loss = 0.31, predict loss = 0.09 (76.4 examples/sec; 0.052 sec/batch; 1h:45m:13s remains)
INFO - root - 2019-11-06 20:37:51.550728: step 29420, total loss = 0.35, predict loss = 0.09 (78.9 examples/sec; 0.051 sec/batch; 1h:41m:54s remains)
INFO - root - 2019-11-06 20:37:52.134647: step 29430, total loss = 0.26, predict loss = 0.06 (79.2 examples/sec; 0.051 sec/batch; 1h:41m:30s remains)
INFO - root - 2019-11-06 20:37:52.750730: step 29440, total loss = 0.33, predict loss = 0.08 (75.6 examples/sec; 0.053 sec/batch; 1h:46m:15s remains)
INFO - root - 2019-11-06 20:37:53.354484: step 29450, total loss = 0.26, predict loss = 0.07 (80.6 examples/sec; 0.050 sec/batch; 1h:39m:42s remains)
INFO - root - 2019-11-06 20:37:53.927411: step 29460, total loss = 0.16, predict loss = 0.04 (87.5 examples/sec; 0.046 sec/batch; 1h:31m:49s remains)
INFO - root - 2019-11-06 20:37:54.386334: step 29470, total loss = 0.24, predict loss = 0.06 (103.6 examples/sec; 0.039 sec/batch; 1h:17m:33s remains)
INFO - root - 2019-11-06 20:37:54.842906: step 29480, total loss = 0.24, predict loss = 0.06 (100.8 examples/sec; 0.040 sec/batch; 1h:19m:44s remains)
INFO - root - 2019-11-06 20:37:55.781716: step 29490, total loss = 0.43, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 1h:47m:06s remains)
INFO - root - 2019-11-06 20:37:56.471865: step 29500, total loss = 0.16, predict loss = 0.04 (62.4 examples/sec; 0.064 sec/batch; 2h:08m:48s remains)
INFO - root - 2019-11-06 20:37:57.089309: step 29510, total loss = 0.31, predict loss = 0.08 (81.0 examples/sec; 0.049 sec/batch; 1h:39m:12s remains)
INFO - root - 2019-11-06 20:37:57.672881: step 29520, total loss = 0.37, predict loss = 0.10 (82.9 examples/sec; 0.048 sec/batch; 1h:36m:50s remains)
INFO - root - 2019-11-06 20:37:58.273645: step 29530, total loss = 0.21, predict loss = 0.06 (74.9 examples/sec; 0.053 sec/batch; 1h:47m:14s remains)
INFO - root - 2019-11-06 20:37:58.852858: step 29540, total loss = 0.27, predict loss = 0.06 (80.4 examples/sec; 0.050 sec/batch; 1h:39m:52s remains)
INFO - root - 2019-11-06 20:37:59.416181: step 29550, total loss = 0.25, predict loss = 0.07 (83.7 examples/sec; 0.048 sec/batch; 1h:35m:58s remains)
INFO - root - 2019-11-06 20:37:59.990576: step 29560, total loss = 0.32, predict loss = 0.08 (77.3 examples/sec; 0.052 sec/batch; 1h:43m:51s remains)
INFO - root - 2019-11-06 20:38:00.595553: step 29570, total loss = 0.44, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 1h:45m:45s remains)
INFO - root - 2019-11-06 20:38:01.165394: step 29580, total loss = 0.34, predict loss = 0.09 (81.1 examples/sec; 0.049 sec/batch; 1h:38m:57s remains)
INFO - root - 2019-11-06 20:38:01.741299: step 29590, total loss = 0.25, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:43m:42s remains)
INFO - root - 2019-11-06 20:38:02.334885: step 29600, total loss = 0.22, predict loss = 0.06 (76.7 examples/sec; 0.052 sec/batch; 1h:44m:37s remains)
INFO - root - 2019-11-06 20:38:02.907279: step 29610, total loss = 0.31, predict loss = 0.08 (95.6 examples/sec; 0.042 sec/batch; 1h:23m:55s remains)
INFO - root - 2019-11-06 20:38:03.353310: step 29620, total loss = 0.25, predict loss = 0.07 (94.2 examples/sec; 0.042 sec/batch; 1h:25m:09s remains)
INFO - root - 2019-11-06 20:38:03.808141: step 29630, total loss = 0.36, predict loss = 0.10 (91.9 examples/sec; 0.044 sec/batch; 1h:27m:17s remains)
INFO - root - 2019-11-06 20:38:04.757196: step 29640, total loss = 0.20, predict loss = 0.05 (67.3 examples/sec; 0.059 sec/batch; 1h:59m:13s remains)
INFO - root - 2019-11-06 20:38:05.477762: step 29650, total loss = 0.20, predict loss = 0.05 (67.0 examples/sec; 0.060 sec/batch; 1h:59m:47s remains)
INFO - root - 2019-11-06 20:38:06.099692: step 29660, total loss = 0.44, predict loss = 0.12 (74.2 examples/sec; 0.054 sec/batch; 1h:48m:08s remains)
INFO - root - 2019-11-06 20:38:06.669341: step 29670, total loss = 0.41, predict loss = 0.12 (79.8 examples/sec; 0.050 sec/batch; 1h:40m:34s remains)
INFO - root - 2019-11-06 20:38:07.231930: step 29680, total loss = 0.20, predict loss = 0.05 (84.1 examples/sec; 0.048 sec/batch; 1h:35m:24s remains)
INFO - root - 2019-11-06 20:38:07.819778: step 29690, total loss = 0.26, predict loss = 0.07 (81.6 examples/sec; 0.049 sec/batch; 1h:38m:15s remains)
INFO - root - 2019-11-06 20:38:08.387201: step 29700, total loss = 0.25, predict loss = 0.07 (79.4 examples/sec; 0.050 sec/batch; 1h:41m:03s remains)
INFO - root - 2019-11-06 20:38:08.962236: step 29710, total loss = 0.41, predict loss = 0.12 (79.3 examples/sec; 0.050 sec/batch; 1h:41m:10s remains)
INFO - root - 2019-11-06 20:38:09.537156: step 29720, total loss = 0.35, predict loss = 0.09 (81.0 examples/sec; 0.049 sec/batch; 1h:39m:01s remains)
INFO - root - 2019-11-06 20:38:10.117655: step 29730, total loss = 0.31, predict loss = 0.10 (76.0 examples/sec; 0.053 sec/batch; 1h:45m:30s remains)
INFO - root - 2019-11-06 20:38:10.699526: step 29740, total loss = 0.28, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 1h:42m:20s remains)
INFO - root - 2019-11-06 20:38:11.286984: step 29750, total loss = 0.26, predict loss = 0.07 (75.6 examples/sec; 0.053 sec/batch; 1h:46m:00s remains)
INFO - root - 2019-11-06 20:38:11.823842: step 29760, total loss = 0.29, predict loss = 0.07 (97.4 examples/sec; 0.041 sec/batch; 1h:22m:18s remains)
INFO - root - 2019-11-06 20:38:12.292429: step 29770, total loss = 0.31, predict loss = 0.07 (97.2 examples/sec; 0.041 sec/batch; 1h:22m:25s remains)
INFO - root - 2019-11-06 20:38:12.753923: step 29780, total loss = 0.36, predict loss = 0.08 (94.0 examples/sec; 0.043 sec/batch; 1h:25m:17s remains)
INFO - root - 2019-11-06 20:38:13.817163: step 29790, total loss = 0.23, predict loss = 0.06 (52.9 examples/sec; 0.076 sec/batch; 2h:31m:35s remains)
INFO - root - 2019-11-06 20:38:14.469387: step 29800, total loss = 0.23, predict loss = 0.05 (73.2 examples/sec; 0.055 sec/batch; 1h:49m:28s remains)
INFO - root - 2019-11-06 20:38:15.068117: step 29810, total loss = 0.19, predict loss = 0.05 (82.5 examples/sec; 0.048 sec/batch; 1h:37m:06s remains)
INFO - root - 2019-11-06 20:38:15.653697: step 29820, total loss = 0.21, predict loss = 0.05 (77.6 examples/sec; 0.052 sec/batch; 1h:43m:17s remains)
INFO - root - 2019-11-06 20:38:16.223905: step 29830, total loss = 0.19, predict loss = 0.05 (72.5 examples/sec; 0.055 sec/batch; 1h:50m:27s remains)
INFO - root - 2019-11-06 20:38:16.798907: step 29840, total loss = 0.20, predict loss = 0.05 (78.9 examples/sec; 0.051 sec/batch; 1h:41m:32s remains)
INFO - root - 2019-11-06 20:38:17.393351: step 29850, total loss = 0.30, predict loss = 0.08 (76.9 examples/sec; 0.052 sec/batch; 1h:44m:12s remains)
INFO - root - 2019-11-06 20:38:17.979659: step 29860, total loss = 0.30, predict loss = 0.08 (75.8 examples/sec; 0.053 sec/batch; 1h:45m:41s remains)
INFO - root - 2019-11-06 20:38:18.569950: step 29870, total loss = 0.32, predict loss = 0.08 (78.2 examples/sec; 0.051 sec/batch; 1h:42m:24s remains)
INFO - root - 2019-11-06 20:38:19.149935: step 29880, total loss = 0.19, predict loss = 0.05 (81.7 examples/sec; 0.049 sec/batch; 1h:38m:00s remains)
INFO - root - 2019-11-06 20:38:19.752612: step 29890, total loss = 0.22, predict loss = 0.06 (78.1 examples/sec; 0.051 sec/batch; 1h:42m:32s remains)
INFO - root - 2019-11-06 20:38:20.327073: step 29900, total loss = 0.27, predict loss = 0.08 (76.3 examples/sec; 0.052 sec/batch; 1h:44m:52s remains)
INFO - root - 2019-11-06 20:38:20.845982: step 29910, total loss = 0.22, predict loss = 0.05 (105.1 examples/sec; 0.038 sec/batch; 1h:16m:09s remains)
INFO - root - 2019-11-06 20:38:21.322957: step 29920, total loss = 0.19, predict loss = 0.05 (93.4 examples/sec; 0.043 sec/batch; 1h:25m:40s remains)
INFO - root - 2019-11-06 20:38:21.804519: step 29930, total loss = 0.25, predict loss = 0.07 (89.8 examples/sec; 0.045 sec/batch; 1h:29m:08s remains)
INFO - root - 2019-11-06 20:38:22.851842: step 29940, total loss = 0.21, predict loss = 0.05 (56.1 examples/sec; 0.071 sec/batch; 2h:22m:42s remains)
INFO - root - 2019-11-06 20:38:23.462526: step 29950, total loss = 0.35, predict loss = 0.09 (82.5 examples/sec; 0.048 sec/batch; 1h:36m:57s remains)
INFO - root - 2019-11-06 20:38:24.041791: step 29960, total loss = 0.27, predict loss = 0.08 (74.9 examples/sec; 0.053 sec/batch; 1h:46m:52s remains)
INFO - root - 2019-11-06 20:38:24.630471: step 29970, total loss = 0.23, predict loss = 0.07 (83.3 examples/sec; 0.048 sec/batch; 1h:36m:05s remains)
INFO - root - 2019-11-06 20:38:25.200266: step 29980, total loss = 0.18, predict loss = 0.05 (79.6 examples/sec; 0.050 sec/batch; 1h:40m:31s remains)
INFO - root - 2019-11-06 20:38:25.778657: step 29990, total loss = 0.30, predict loss = 0.09 (75.3 examples/sec; 0.053 sec/batch; 1h:46m:11s remains)
INFO - root - 2019-11-06 20:38:26.351959: step 30000, total loss = 0.20, predict loss = 0.05 (80.7 examples/sec; 0.050 sec/batch; 1h:39m:04s remains)
INFO - root - 2019-11-06 20:38:27.556906: step 30010, total loss = 0.36, predict loss = 0.12 (77.1 examples/sec; 0.052 sec/batch; 1h:43m:48s remains)
INFO - root - 2019-11-06 20:38:28.128194: step 30020, total loss = 0.30, predict loss = 0.08 (80.2 examples/sec; 0.050 sec/batch; 1h:39m:46s remains)
INFO - root - 2019-11-06 20:38:28.706812: step 30030, total loss = 0.30, predict loss = 0.07 (75.6 examples/sec; 0.053 sec/batch; 1h:45m:46s remains)
INFO - root - 2019-11-06 20:38:29.281174: step 30040, total loss = 0.36, predict loss = 0.09 (77.2 examples/sec; 0.052 sec/batch; 1h:43m:38s remains)
INFO - root - 2019-11-06 20:38:29.883427: step 30050, total loss = 0.16, predict loss = 0.04 (74.1 examples/sec; 0.054 sec/batch; 1h:47m:58s remains)
INFO - root - 2019-11-06 20:38:30.384356: step 30060, total loss = 0.38, predict loss = 0.12 (93.5 examples/sec; 0.043 sec/batch; 1h:25m:33s remains)
INFO - root - 2019-11-06 20:38:30.831625: step 30070, total loss = 0.24, predict loss = 0.06 (97.4 examples/sec; 0.041 sec/batch; 1h:22m:06s remains)
INFO - root - 2019-11-06 20:38:31.275246: step 30080, total loss = 0.19, predict loss = 0.05 (93.0 examples/sec; 0.043 sec/batch; 1h:25m:56s remains)
INFO - root - 2019-11-06 20:38:32.463222: step 30090, total loss = 0.35, predict loss = 0.10 (53.6 examples/sec; 0.075 sec/batch; 2h:29m:01s remains)
INFO - root - 2019-11-06 20:38:33.077516: step 30100, total loss = 0.19, predict loss = 0.05 (77.3 examples/sec; 0.052 sec/batch; 1h:43m:20s remains)
INFO - root - 2019-11-06 20:38:33.657989: step 30110, total loss = 0.28, predict loss = 0.07 (79.1 examples/sec; 0.051 sec/batch; 1h:40m:58s remains)
INFO - root - 2019-11-06 20:38:34.238854: step 30120, total loss = 0.27, predict loss = 0.08 (73.5 examples/sec; 0.054 sec/batch; 1h:48m:43s remains)
INFO - root - 2019-11-06 20:38:34.822113: step 30130, total loss = 0.22, predict loss = 0.05 (83.8 examples/sec; 0.048 sec/batch; 1h:35m:23s remains)
INFO - root - 2019-11-06 20:38:35.410112: step 30140, total loss = 0.42, predict loss = 0.12 (79.8 examples/sec; 0.050 sec/batch; 1h:40m:06s remains)
INFO - root - 2019-11-06 20:38:35.979243: step 30150, total loss = 0.29, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:40m:15s remains)
INFO - root - 2019-11-06 20:38:36.554487: step 30160, total loss = 0.35, predict loss = 0.09 (80.2 examples/sec; 0.050 sec/batch; 1h:39m:38s remains)
INFO - root - 2019-11-06 20:38:37.144672: step 30170, total loss = 0.52, predict loss = 0.15 (80.7 examples/sec; 0.050 sec/batch; 1h:39m:00s remains)
INFO - root - 2019-11-06 20:38:37.730794: step 30180, total loss = 0.23, predict loss = 0.06 (75.0 examples/sec; 0.053 sec/batch; 1h:46m:33s remains)
INFO - root - 2019-11-06 20:38:38.299874: step 30190, total loss = 0.28, predict loss = 0.06 (80.8 examples/sec; 0.050 sec/batch; 1h:38m:53s remains)
INFO - root - 2019-11-06 20:38:38.883281: step 30200, total loss = 0.34, predict loss = 0.10 (74.8 examples/sec; 0.053 sec/batch; 1h:46m:42s remains)
INFO - root - 2019-11-06 20:38:39.364642: step 30210, total loss = 0.28, predict loss = 0.07 (103.8 examples/sec; 0.039 sec/batch; 1h:16m:54s remains)
INFO - root - 2019-11-06 20:38:39.817231: step 30220, total loss = 0.25, predict loss = 0.05 (96.6 examples/sec; 0.041 sec/batch; 1h:22m:40s remains)
INFO - root - 2019-11-06 20:38:40.724499: step 30230, total loss = 0.21, predict loss = 0.05 (80.9 examples/sec; 0.049 sec/batch; 1h:38m:43s remains)
INFO - root - 2019-11-06 20:38:41.439605: step 30240, total loss = 0.23, predict loss = 0.06 (59.9 examples/sec; 0.067 sec/batch; 2h:13m:13s remains)
INFO - root - 2019-11-06 20:38:42.100555: step 30250, total loss = 0.18, predict loss = 0.04 (74.8 examples/sec; 0.053 sec/batch; 1h:46m:40s remains)
INFO - root - 2019-11-06 20:38:42.683022: step 30260, total loss = 0.17, predict loss = 0.04 (73.4 examples/sec; 0.055 sec/batch; 1h:48m:49s remains)
INFO - root - 2019-11-06 20:38:43.284733: step 30270, total loss = 0.29, predict loss = 0.07 (81.3 examples/sec; 0.049 sec/batch; 1h:38m:11s remains)
INFO - root - 2019-11-06 20:38:43.855040: step 30280, total loss = 0.38, predict loss = 0.10 (80.8 examples/sec; 0.049 sec/batch; 1h:38m:45s remains)
INFO - root - 2019-11-06 20:38:44.465788: step 30290, total loss = 0.40, predict loss = 0.12 (77.8 examples/sec; 0.051 sec/batch; 1h:42m:35s remains)
INFO - root - 2019-11-06 20:38:45.039466: step 30300, total loss = 0.17, predict loss = 0.04 (81.7 examples/sec; 0.049 sec/batch; 1h:37m:40s remains)
INFO - root - 2019-11-06 20:38:45.613692: step 30310, total loss = 0.29, predict loss = 0.10 (77.0 examples/sec; 0.052 sec/batch; 1h:43m:39s remains)
INFO - root - 2019-11-06 20:38:46.195557: step 30320, total loss = 0.18, predict loss = 0.04 (78.4 examples/sec; 0.051 sec/batch; 1h:41m:48s remains)
INFO - root - 2019-11-06 20:38:46.801203: step 30330, total loss = 0.23, predict loss = 0.06 (76.2 examples/sec; 0.053 sec/batch; 1h:44m:43s remains)
INFO - root - 2019-11-06 20:38:47.376417: step 30340, total loss = 0.22, predict loss = 0.07 (77.6 examples/sec; 0.052 sec/batch; 1h:42m:47s remains)
INFO - root - 2019-11-06 20:38:47.940072: step 30350, total loss = 0.32, predict loss = 0.07 (88.3 examples/sec; 0.045 sec/batch; 1h:30m:20s remains)
INFO - root - 2019-11-06 20:38:48.404149: step 30360, total loss = 0.22, predict loss = 0.05 (93.4 examples/sec; 0.043 sec/batch; 1h:25m:23s remains)
INFO - root - 2019-11-06 20:38:48.879335: step 30370, total loss = 0.31, predict loss = 0.09 (101.6 examples/sec; 0.039 sec/batch; 1h:18m:31s remains)
INFO - root - 2019-11-06 20:38:49.833405: step 30380, total loss = 0.21, predict loss = 0.06 (68.0 examples/sec; 0.059 sec/batch; 1h:57m:19s remains)
INFO - root - 2019-11-06 20:38:50.488501: step 30390, total loss = 0.20, predict loss = 0.05 (76.3 examples/sec; 0.052 sec/batch; 1h:44m:32s remains)
INFO - root - 2019-11-06 20:38:51.075769: step 30400, total loss = 0.40, predict loss = 0.11 (79.8 examples/sec; 0.050 sec/batch; 1h:39m:56s remains)
INFO - root - 2019-11-06 20:38:51.669805: step 30410, total loss = 0.29, predict loss = 0.07 (74.8 examples/sec; 0.054 sec/batch; 1h:46m:38s remains)
INFO - root - 2019-11-06 20:38:52.247986: step 30420, total loss = 0.42, predict loss = 0.12 (75.3 examples/sec; 0.053 sec/batch; 1h:45m:48s remains)
INFO - root - 2019-11-06 20:38:52.833139: step 30430, total loss = 0.31, predict loss = 0.08 (72.2 examples/sec; 0.055 sec/batch; 1h:50m:21s remains)
INFO - root - 2019-11-06 20:38:53.431125: step 30440, total loss = 0.21, predict loss = 0.05 (70.0 examples/sec; 0.057 sec/batch; 1h:53m:53s remains)
INFO - root - 2019-11-06 20:38:54.038599: step 30450, total loss = 0.19, predict loss = 0.05 (69.7 examples/sec; 0.057 sec/batch; 1h:54m:21s remains)
INFO - root - 2019-11-06 20:38:54.628628: step 30460, total loss = 0.37, predict loss = 0.10 (74.4 examples/sec; 0.054 sec/batch; 1h:47m:10s remains)
INFO - root - 2019-11-06 20:38:55.240964: step 30470, total loss = 0.29, predict loss = 0.07 (78.0 examples/sec; 0.051 sec/batch; 1h:42m:09s remains)
INFO - root - 2019-11-06 20:38:55.831442: step 30480, total loss = 0.36, predict loss = 0.11 (77.5 examples/sec; 0.052 sec/batch; 1h:42m:47s remains)
INFO - root - 2019-11-06 20:38:56.442704: step 30490, total loss = 0.24, predict loss = 0.06 (76.7 examples/sec; 0.052 sec/batch; 1h:43m:54s remains)
INFO - root - 2019-11-06 20:38:57.025153: step 30500, total loss = 0.25, predict loss = 0.06 (96.6 examples/sec; 0.041 sec/batch; 1h:22m:26s remains)
INFO - root - 2019-11-06 20:38:57.471888: step 30510, total loss = 0.33, predict loss = 0.09 (104.3 examples/sec; 0.038 sec/batch; 1h:16m:21s remains)
INFO - root - 2019-11-06 20:38:57.911259: step 30520, total loss = 0.25, predict loss = 0.07 (106.4 examples/sec; 0.038 sec/batch; 1h:14m:50s remains)
INFO - root - 2019-11-06 20:38:58.911999: step 30530, total loss = 0.34, predict loss = 0.08 (58.5 examples/sec; 0.068 sec/batch; 2h:16m:08s remains)
INFO - root - 2019-11-06 20:38:59.537129: step 30540, total loss = 0.17, predict loss = 0.04 (80.0 examples/sec; 0.050 sec/batch; 1h:39m:32s remains)
INFO - root - 2019-11-06 20:39:00.118193: step 30550, total loss = 0.19, predict loss = 0.05 (74.2 examples/sec; 0.054 sec/batch; 1h:47m:23s remains)
INFO - root - 2019-11-06 20:39:00.701873: step 30560, total loss = 0.37, predict loss = 0.10 (78.1 examples/sec; 0.051 sec/batch; 1h:42m:01s remains)
INFO - root - 2019-11-06 20:39:01.296922: step 30570, total loss = 0.26, predict loss = 0.07 (74.8 examples/sec; 0.054 sec/batch; 1h:46m:29s remains)
INFO - root - 2019-11-06 20:39:01.882562: step 30580, total loss = 0.24, predict loss = 0.07 (75.6 examples/sec; 0.053 sec/batch; 1h:45m:18s remains)
INFO - root - 2019-11-06 20:39:02.464320: step 30590, total loss = 0.25, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 1h:41m:13s remains)
INFO - root - 2019-11-06 20:39:03.046190: step 30600, total loss = 0.29, predict loss = 0.07 (79.4 examples/sec; 0.050 sec/batch; 1h:40m:12s remains)
INFO - root - 2019-11-06 20:39:03.638333: step 30610, total loss = 0.30, predict loss = 0.08 (79.5 examples/sec; 0.050 sec/batch; 1h:40m:09s remains)
INFO - root - 2019-11-06 20:39:04.209999: step 30620, total loss = 0.35, predict loss = 0.08 (79.8 examples/sec; 0.050 sec/batch; 1h:39m:42s remains)
INFO - root - 2019-11-06 20:39:04.773104: step 30630, total loss = 0.24, predict loss = 0.07 (80.8 examples/sec; 0.050 sec/batch; 1h:38m:29s remains)
INFO - root - 2019-11-06 20:39:05.351022: step 30640, total loss = 0.18, predict loss = 0.05 (82.4 examples/sec; 0.049 sec/batch; 1h:36m:31s remains)
INFO - root - 2019-11-06 20:39:05.889029: step 30650, total loss = 0.62, predict loss = 0.20 (92.7 examples/sec; 0.043 sec/batch; 1h:25m:50s remains)
INFO - root - 2019-11-06 20:39:06.330359: step 30660, total loss = 0.23, predict loss = 0.06 (96.4 examples/sec; 0.041 sec/batch; 1h:22m:31s remains)
INFO - root - 2019-11-06 20:39:06.783724: step 30670, total loss = 0.27, predict loss = 0.07 (100.6 examples/sec; 0.040 sec/batch; 1h:19m:04s remains)
INFO - root - 2019-11-06 20:39:07.842830: step 30680, total loss = 0.30, predict loss = 0.07 (55.7 examples/sec; 0.072 sec/batch; 2h:22m:43s remains)
INFO - root - 2019-11-06 20:39:08.508929: step 30690, total loss = 0.37, predict loss = 0.11 (79.5 examples/sec; 0.050 sec/batch; 1h:39m:59s remains)
INFO - root - 2019-11-06 20:39:09.092469: step 30700, total loss = 0.26, predict loss = 0.07 (77.9 examples/sec; 0.051 sec/batch; 1h:42m:07s remains)
INFO - root - 2019-11-06 20:39:09.667768: step 30710, total loss = 0.25, predict loss = 0.07 (77.9 examples/sec; 0.051 sec/batch; 1h:42m:06s remains)
INFO - root - 2019-11-06 20:39:10.242945: step 30720, total loss = 0.28, predict loss = 0.08 (76.3 examples/sec; 0.052 sec/batch; 1h:44m:14s remains)
INFO - root - 2019-11-06 20:39:10.839878: step 30730, total loss = 0.18, predict loss = 0.04 (76.9 examples/sec; 0.052 sec/batch; 1h:43m:24s remains)
INFO - root - 2019-11-06 20:39:11.426374: step 30740, total loss = 0.35, predict loss = 0.10 (80.8 examples/sec; 0.050 sec/batch; 1h:38m:24s remains)
INFO - root - 2019-11-06 20:39:11.998729: step 30750, total loss = 0.28, predict loss = 0.07 (79.8 examples/sec; 0.050 sec/batch; 1h:39m:36s remains)
INFO - root - 2019-11-06 20:39:12.588339: step 30760, total loss = 0.22, predict loss = 0.05 (78.5 examples/sec; 0.051 sec/batch; 1h:41m:16s remains)
INFO - root - 2019-11-06 20:39:13.224221: step 30770, total loss = 0.27, predict loss = 0.09 (80.5 examples/sec; 0.050 sec/batch; 1h:38m:47s remains)
INFO - root - 2019-11-06 20:39:13.803081: step 30780, total loss = 0.25, predict loss = 0.07 (77.6 examples/sec; 0.052 sec/batch; 1h:42m:29s remains)
INFO - root - 2019-11-06 20:39:14.378435: step 30790, total loss = 0.32, predict loss = 0.09 (77.0 examples/sec; 0.052 sec/batch; 1h:43m:12s remains)
INFO - root - 2019-11-06 20:39:14.892259: step 30800, total loss = 0.19, predict loss = 0.05 (104.5 examples/sec; 0.038 sec/batch; 1h:16m:00s remains)
INFO - root - 2019-11-06 20:39:15.375012: step 30810, total loss = 0.18, predict loss = 0.04 (97.2 examples/sec; 0.041 sec/batch; 1h:21m:43s remains)
INFO - root - 2019-11-06 20:39:15.818356: step 30820, total loss = 0.34, predict loss = 0.09 (100.2 examples/sec; 0.040 sec/batch; 1h:19m:17s remains)
INFO - root - 2019-11-06 20:39:16.907776: step 30830, total loss = 0.32, predict loss = 0.09 (51.9 examples/sec; 0.077 sec/batch; 2h:33m:08s remains)
INFO - root - 2019-11-06 20:39:17.539872: step 30840, total loss = 0.26, predict loss = 0.08 (77.9 examples/sec; 0.051 sec/batch; 1h:42m:00s remains)
INFO - root - 2019-11-06 20:39:18.149226: step 30850, total loss = 0.28, predict loss = 0.07 (82.3 examples/sec; 0.049 sec/batch; 1h:36m:28s remains)
INFO - root - 2019-11-06 20:39:18.726643: step 30860, total loss = 0.20, predict loss = 0.05 (77.9 examples/sec; 0.051 sec/batch; 1h:42m:00s remains)
INFO - root - 2019-11-06 20:39:19.301574: step 30870, total loss = 0.22, predict loss = 0.05 (82.3 examples/sec; 0.049 sec/batch; 1h:36m:32s remains)
INFO - root - 2019-11-06 20:39:19.866566: step 30880, total loss = 0.29, predict loss = 0.07 (80.5 examples/sec; 0.050 sec/batch; 1h:38m:36s remains)
INFO - root - 2019-11-06 20:39:20.454207: step 30890, total loss = 0.19, predict loss = 0.05 (80.1 examples/sec; 0.050 sec/batch; 1h:39m:07s remains)
INFO - root - 2019-11-06 20:39:21.040092: step 30900, total loss = 0.23, predict loss = 0.06 (82.0 examples/sec; 0.049 sec/batch; 1h:36m:46s remains)
INFO - root - 2019-11-06 20:39:21.616628: step 30910, total loss = 0.29, predict loss = 0.08 (76.0 examples/sec; 0.053 sec/batch; 1h:44m:25s remains)
INFO - root - 2019-11-06 20:39:22.192823: step 30920, total loss = 0.36, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 1h:48m:47s remains)
INFO - root - 2019-11-06 20:39:22.782438: step 30930, total loss = 0.26, predict loss = 0.06 (79.0 examples/sec; 0.051 sec/batch; 1h:40m:27s remains)
INFO - root - 2019-11-06 20:39:23.347027: step 30940, total loss = 0.35, predict loss = 0.08 (79.5 examples/sec; 0.050 sec/batch; 1h:39m:48s remains)
INFO - root - 2019-11-06 20:39:23.836748: step 30950, total loss = 0.24, predict loss = 0.06 (98.0 examples/sec; 0.041 sec/batch; 1h:20m:59s remains)
INFO - root - 2019-11-06 20:39:24.290474: step 30960, total loss = 0.32, predict loss = 0.09 (89.7 examples/sec; 0.045 sec/batch; 1h:28m:29s remains)
INFO - root - 2019-11-06 20:39:25.234901: step 30970, total loss = 0.41, predict loss = 0.10 (7.8 examples/sec; 0.510 sec/batch; 16h:52m:28s remains)
INFO - root - 2019-11-06 20:39:25.952308: step 30980, total loss = 0.28, predict loss = 0.08 (57.3 examples/sec; 0.070 sec/batch; 2h:18m:22s remains)
INFO - root - 2019-11-06 20:39:26.576057: step 30990, total loss = 0.19, predict loss = 0.05 (77.8 examples/sec; 0.051 sec/batch; 1h:41m:55s remains)
INFO - root - 2019-11-06 20:39:27.141034: step 31000, total loss = 0.37, predict loss = 0.11 (76.5 examples/sec; 0.052 sec/batch; 1h:43m:38s remains)
INFO - root - 2019-11-06 20:39:27.735400: step 31010, total loss = 0.26, predict loss = 0.07 (78.1 examples/sec; 0.051 sec/batch; 1h:41m:38s remains)
INFO - root - 2019-11-06 20:39:28.318182: step 31020, total loss = 0.25, predict loss = 0.06 (81.9 examples/sec; 0.049 sec/batch; 1h:36m:48s remains)
INFO - root - 2019-11-06 20:39:28.893484: step 31030, total loss = 0.25, predict loss = 0.07 (78.3 examples/sec; 0.051 sec/batch; 1h:41m:17s remains)
INFO - root - 2019-11-06 20:39:29.479554: step 31040, total loss = 0.31, predict loss = 0.08 (75.5 examples/sec; 0.053 sec/batch; 1h:45m:01s remains)
INFO - root - 2019-11-06 20:39:30.068251: step 31050, total loss = 0.32, predict loss = 0.10 (77.4 examples/sec; 0.052 sec/batch; 1h:42m:29s remains)
INFO - root - 2019-11-06 20:39:30.642925: step 31060, total loss = 0.20, predict loss = 0.05 (81.0 examples/sec; 0.049 sec/batch; 1h:37m:55s remains)
INFO - root - 2019-11-06 20:39:31.225139: step 31070, total loss = 0.32, predict loss = 0.09 (82.9 examples/sec; 0.048 sec/batch; 1h:35m:39s remains)
INFO - root - 2019-11-06 20:39:31.798959: step 31080, total loss = 0.23, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:40m:13s remains)
INFO - root - 2019-11-06 20:39:32.391849: step 31090, total loss = 0.21, predict loss = 0.05 (89.3 examples/sec; 0.045 sec/batch; 1h:28m:44s remains)
INFO - root - 2019-11-06 20:39:32.865603: step 31100, total loss = 0.35, predict loss = 0.09 (92.8 examples/sec; 0.043 sec/batch; 1h:25m:22s remains)
INFO - root - 2019-11-06 20:39:33.333567: step 31110, total loss = 0.44, predict loss = 0.12 (92.2 examples/sec; 0.043 sec/batch; 1h:25m:59s remains)
INFO - root - 2019-11-06 20:39:34.245176: step 31120, total loss = 0.39, predict loss = 0.13 (77.0 examples/sec; 0.052 sec/batch; 1h:42m:53s remains)
INFO - root - 2019-11-06 20:39:34.887998: step 31130, total loss = 0.38, predict loss = 0.14 (71.0 examples/sec; 0.056 sec/batch; 1h:51m:40s remains)
INFO - root - 2019-11-06 20:39:35.492793: step 31140, total loss = 0.26, predict loss = 0.07 (78.3 examples/sec; 0.051 sec/batch; 1h:41m:13s remains)
INFO - root - 2019-11-06 20:39:36.076449: step 31150, total loss = 0.24, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:43m:10s remains)
INFO - root - 2019-11-06 20:39:36.653649: step 31160, total loss = 0.35, predict loss = 0.10 (79.2 examples/sec; 0.051 sec/batch; 1h:40m:04s remains)
INFO - root - 2019-11-06 20:39:37.236934: step 31170, total loss = 0.26, predict loss = 0.07 (79.2 examples/sec; 0.050 sec/batch; 1h:40m:00s remains)
INFO - root - 2019-11-06 20:39:37.832574: step 31180, total loss = 0.29, predict loss = 0.08 (75.8 examples/sec; 0.053 sec/batch; 1h:44m:26s remains)
INFO - root - 2019-11-06 20:39:38.407746: step 31190, total loss = 0.22, predict loss = 0.05 (80.9 examples/sec; 0.049 sec/batch; 1h:37m:57s remains)
INFO - root - 2019-11-06 20:39:38.982964: step 31200, total loss = 0.32, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 1h:41m:14s remains)
INFO - root - 2019-11-06 20:39:39.585991: step 31210, total loss = 0.24, predict loss = 0.06 (77.1 examples/sec; 0.052 sec/batch; 1h:42m:44s remains)
INFO - root - 2019-11-06 20:39:40.169155: step 31220, total loss = 0.23, predict loss = 0.06 (79.2 examples/sec; 0.050 sec/batch; 1h:39m:55s remains)
INFO - root - 2019-11-06 20:39:40.742306: step 31230, total loss = 0.19, predict loss = 0.04 (74.5 examples/sec; 0.054 sec/batch; 1h:46m:17s remains)
INFO - root - 2019-11-06 20:39:41.299750: step 31240, total loss = 0.27, predict loss = 0.06 (95.0 examples/sec; 0.042 sec/batch; 1h:23m:18s remains)
INFO - root - 2019-11-06 20:39:41.775575: step 31250, total loss = 0.29, predict loss = 0.08 (100.2 examples/sec; 0.040 sec/batch; 1h:19m:00s remains)
INFO - root - 2019-11-06 20:39:42.219345: step 31260, total loss = 0.31, predict loss = 0.09 (105.7 examples/sec; 0.038 sec/batch; 1h:14m:52s remains)
INFO - root - 2019-11-06 20:39:43.347178: step 31270, total loss = 0.25, predict loss = 0.06 (63.3 examples/sec; 0.063 sec/batch; 2h:05m:00s remains)
INFO - root - 2019-11-06 20:39:44.056366: step 31280, total loss = 0.23, predict loss = 0.05 (62.7 examples/sec; 0.064 sec/batch; 2h:06m:11s remains)
INFO - root - 2019-11-06 20:39:44.747456: step 31290, total loss = 0.31, predict loss = 0.08 (67.2 examples/sec; 0.060 sec/batch; 1h:57m:47s remains)
INFO - root - 2019-11-06 20:39:45.333414: step 31300, total loss = 0.15, predict loss = 0.04 (77.5 examples/sec; 0.052 sec/batch; 1h:42m:05s remains)
INFO - root - 2019-11-06 20:39:45.972158: step 31310, total loss = 0.21, predict loss = 0.06 (65.0 examples/sec; 0.062 sec/batch; 2h:01m:44s remains)
INFO - root - 2019-11-06 20:39:46.555072: step 31320, total loss = 0.23, predict loss = 0.06 (73.6 examples/sec; 0.054 sec/batch; 1h:47m:31s remains)
INFO - root - 2019-11-06 20:39:47.182102: step 31330, total loss = 0.21, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 1h:41m:03s remains)
INFO - root - 2019-11-06 20:39:47.757723: step 31340, total loss = 0.28, predict loss = 0.07 (73.1 examples/sec; 0.055 sec/batch; 1h:48m:10s remains)
INFO - root - 2019-11-06 20:39:48.343038: step 31350, total loss = 0.20, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:42m:24s remains)
INFO - root - 2019-11-06 20:39:48.960843: step 31360, total loss = 0.34, predict loss = 0.08 (66.6 examples/sec; 0.060 sec/batch; 1h:58m:45s remains)
INFO - root - 2019-11-06 20:39:49.573038: step 31370, total loss = 0.25, predict loss = 0.07 (72.3 examples/sec; 0.055 sec/batch; 1h:49m:21s remains)
INFO - root - 2019-11-06 20:39:50.153707: step 31380, total loss = 0.20, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:42m:25s remains)
INFO - root - 2019-11-06 20:39:50.683586: step 31390, total loss = 0.22, predict loss = 0.06 (95.7 examples/sec; 0.042 sec/batch; 1h:22m:38s remains)
INFO - root - 2019-11-06 20:39:51.138692: step 31400, total loss = 0.24, predict loss = 0.07 (96.4 examples/sec; 0.041 sec/batch; 1h:21m:59s remains)
INFO - root - 2019-11-06 20:39:51.620547: step 31410, total loss = 0.19, predict loss = 0.05 (93.5 examples/sec; 0.043 sec/batch; 1h:24m:31s remains)
INFO - root - 2019-11-06 20:39:52.598643: step 31420, total loss = 0.23, predict loss = 0.06 (60.4 examples/sec; 0.066 sec/batch; 2h:10m:55s remains)
INFO - root - 2019-11-06 20:39:53.218211: step 31430, total loss = 0.14, predict loss = 0.03 (77.2 examples/sec; 0.052 sec/batch; 1h:42m:24s remains)
INFO - root - 2019-11-06 20:39:53.790356: step 31440, total loss = 0.26, predict loss = 0.07 (76.9 examples/sec; 0.052 sec/batch; 1h:42m:46s remains)
INFO - root - 2019-11-06 20:39:54.394479: step 31450, total loss = 0.20, predict loss = 0.05 (78.5 examples/sec; 0.051 sec/batch; 1h:40m:39s remains)
INFO - root - 2019-11-06 20:39:54.965901: step 31460, total loss = 0.31, predict loss = 0.08 (80.5 examples/sec; 0.050 sec/batch; 1h:38m:12s remains)
INFO - root - 2019-11-06 20:39:55.571213: step 31470, total loss = 0.24, predict loss = 0.07 (56.1 examples/sec; 0.071 sec/batch; 2h:20m:45s remains)
INFO - root - 2019-11-06 20:39:56.220242: step 31480, total loss = 0.23, predict loss = 0.05 (71.2 examples/sec; 0.056 sec/batch; 1h:50m:59s remains)
INFO - root - 2019-11-06 20:39:56.810475: step 31490, total loss = 0.26, predict loss = 0.06 (80.1 examples/sec; 0.050 sec/batch; 1h:38m:40s remains)
INFO - root - 2019-11-06 20:39:57.387523: step 31500, total loss = 0.21, predict loss = 0.05 (75.6 examples/sec; 0.053 sec/batch; 1h:44m:32s remains)
INFO - root - 2019-11-06 20:39:57.979441: step 31510, total loss = 0.38, predict loss = 0.12 (80.0 examples/sec; 0.050 sec/batch; 1h:38m:40s remains)
INFO - root - 2019-11-06 20:39:58.559628: step 31520, total loss = 0.17, predict loss = 0.05 (75.8 examples/sec; 0.053 sec/batch; 1h:44m:08s remains)
INFO - root - 2019-11-06 20:39:59.156494: step 31530, total loss = 0.24, predict loss = 0.06 (80.2 examples/sec; 0.050 sec/batch; 1h:38m:30s remains)
INFO - root - 2019-11-06 20:39:59.671391: step 31540, total loss = 0.27, predict loss = 0.07 (101.6 examples/sec; 0.039 sec/batch; 1h:17m:46s remains)
INFO - root - 2019-11-06 20:40:00.121522: step 31550, total loss = 0.38, predict loss = 0.10 (90.0 examples/sec; 0.044 sec/batch; 1h:27m:43s remains)
INFO - root - 2019-11-06 20:40:00.566351: step 31560, total loss = 0.22, predict loss = 0.06 (100.4 examples/sec; 0.040 sec/batch; 1h:18m:37s remains)
INFO - root - 2019-11-06 20:40:01.639025: step 31570, total loss = 0.19, predict loss = 0.05 (58.9 examples/sec; 0.068 sec/batch; 2h:14m:02s remains)
INFO - root - 2019-11-06 20:40:02.369502: step 31580, total loss = 0.29, predict loss = 0.08 (64.4 examples/sec; 0.062 sec/batch; 2h:02m:37s remains)
INFO - root - 2019-11-06 20:40:02.964815: step 31590, total loss = 0.23, predict loss = 0.05 (65.7 examples/sec; 0.061 sec/batch; 2h:00m:04s remains)
INFO - root - 2019-11-06 20:40:03.665887: step 31600, total loss = 0.43, predict loss = 0.11 (61.7 examples/sec; 0.065 sec/batch; 2h:08m:00s remains)
INFO - root - 2019-11-06 20:40:04.334026: step 31610, total loss = 0.46, predict loss = 0.13 (58.8 examples/sec; 0.068 sec/batch; 2h:14m:17s remains)
INFO - root - 2019-11-06 20:40:05.050541: step 31620, total loss = 0.18, predict loss = 0.05 (57.4 examples/sec; 0.070 sec/batch; 2h:17m:22s remains)
INFO - root - 2019-11-06 20:40:05.824076: step 31630, total loss = 0.21, predict loss = 0.06 (48.2 examples/sec; 0.083 sec/batch; 2h:43m:34s remains)
INFO - root - 2019-11-06 20:40:06.526298: step 31640, total loss = 0.23, predict loss = 0.06 (79.3 examples/sec; 0.050 sec/batch; 1h:39m:32s remains)
INFO - root - 2019-11-06 20:40:07.117629: step 31650, total loss = 0.22, predict loss = 0.06 (78.5 examples/sec; 0.051 sec/batch; 1h:40m:33s remains)
INFO - root - 2019-11-06 20:40:07.706256: step 31660, total loss = 0.21, predict loss = 0.05 (71.8 examples/sec; 0.056 sec/batch; 1h:49m:55s remains)
INFO - root - 2019-11-06 20:40:08.286715: step 31670, total loss = 0.33, predict loss = 0.09 (74.9 examples/sec; 0.053 sec/batch; 1h:45m:22s remains)
INFO - root - 2019-11-06 20:40:08.869616: step 31680, total loss = 0.22, predict loss = 0.05 (76.3 examples/sec; 0.052 sec/batch; 1h:43m:25s remains)
INFO - root - 2019-11-06 20:40:09.376654: step 31690, total loss = 0.31, predict loss = 0.09 (99.2 examples/sec; 0.040 sec/batch; 1h:19m:28s remains)
INFO - root - 2019-11-06 20:40:09.822381: step 31700, total loss = 0.38, predict loss = 0.09 (92.8 examples/sec; 0.043 sec/batch; 1h:25m:01s remains)
INFO - root - 2019-11-06 20:40:10.287777: step 31710, total loss = 0.34, predict loss = 0.08 (94.2 examples/sec; 0.042 sec/batch; 1h:23m:45s remains)
INFO - root - 2019-11-06 20:40:11.517879: step 31720, total loss = 0.40, predict loss = 0.12 (64.6 examples/sec; 0.062 sec/batch; 2h:02m:05s remains)
INFO - root - 2019-11-06 20:40:12.216597: step 31730, total loss = 0.30, predict loss = 0.08 (72.7 examples/sec; 0.055 sec/batch; 1h:48m:31s remains)
INFO - root - 2019-11-06 20:40:12.826617: step 31740, total loss = 0.31, predict loss = 0.08 (75.7 examples/sec; 0.053 sec/batch; 1h:44m:09s remains)
INFO - root - 2019-11-06 20:40:13.396846: step 31750, total loss = 0.18, predict loss = 0.05 (79.6 examples/sec; 0.050 sec/batch; 1h:39m:00s remains)
INFO - root - 2019-11-06 20:40:13.977050: step 31760, total loss = 0.36, predict loss = 0.10 (82.1 examples/sec; 0.049 sec/batch; 1h:35m:59s remains)
INFO - root - 2019-11-06 20:40:14.579303: step 31770, total loss = 0.22, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 1h:40m:20s remains)
INFO - root - 2019-11-06 20:40:15.166436: step 31780, total loss = 0.25, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:42m:34s remains)
INFO - root - 2019-11-06 20:40:15.764355: step 31790, total loss = 0.22, predict loss = 0.06 (67.4 examples/sec; 0.059 sec/batch; 1h:56m:50s remains)
INFO - root - 2019-11-06 20:40:16.450425: step 31800, total loss = 0.29, predict loss = 0.07 (77.4 examples/sec; 0.052 sec/batch; 1h:41m:51s remains)
INFO - root - 2019-11-06 20:40:17.077382: step 31810, total loss = 0.35, predict loss = 0.09 (73.7 examples/sec; 0.054 sec/batch; 1h:46m:54s remains)
INFO - root - 2019-11-06 20:40:17.649384: step 31820, total loss = 0.28, predict loss = 0.07 (79.9 examples/sec; 0.050 sec/batch; 1h:38m:36s remains)
INFO - root - 2019-11-06 20:40:18.224177: step 31830, total loss = 0.31, predict loss = 0.08 (78.9 examples/sec; 0.051 sec/batch; 1h:39m:52s remains)
INFO - root - 2019-11-06 20:40:18.701566: step 31840, total loss = 0.41, predict loss = 0.13 (96.8 examples/sec; 0.041 sec/batch; 1h:21m:23s remains)
INFO - root - 2019-11-06 20:40:19.185785: step 31850, total loss = 0.23, predict loss = 0.06 (90.1 examples/sec; 0.044 sec/batch; 1h:27m:24s remains)
INFO - root - 2019-11-06 20:40:20.180659: step 31860, total loss = 0.23, predict loss = 0.05 (76.4 examples/sec; 0.052 sec/batch; 1h:43m:05s remains)
INFO - root - 2019-11-06 20:40:20.895399: step 31870, total loss = 0.37, predict loss = 0.09 (52.9 examples/sec; 0.076 sec/batch; 2h:28m:48s remains)
INFO - root - 2019-11-06 20:40:21.525576: step 31880, total loss = 0.24, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:42m:35s remains)
INFO - root - 2019-11-06 20:40:22.124107: step 31890, total loss = 0.29, predict loss = 0.08 (81.2 examples/sec; 0.049 sec/batch; 1h:37m:00s remains)
INFO - root - 2019-11-06 20:40:22.703177: step 31900, total loss = 0.27, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:39m:47s remains)
INFO - root - 2019-11-06 20:40:23.283379: step 31910, total loss = 0.24, predict loss = 0.06 (73.2 examples/sec; 0.055 sec/batch; 1h:47m:34s remains)
INFO - root - 2019-11-06 20:40:23.907612: step 31920, total loss = 0.28, predict loss = 0.08 (64.6 examples/sec; 0.062 sec/batch; 2h:01m:56s remains)
INFO - root - 2019-11-06 20:40:24.522430: step 31930, total loss = 0.27, predict loss = 0.08 (83.1 examples/sec; 0.048 sec/batch; 1h:34m:40s remains)
INFO - root - 2019-11-06 20:40:25.107478: step 31940, total loss = 0.24, predict loss = 0.07 (78.6 examples/sec; 0.051 sec/batch; 1h:40m:11s remains)
INFO - root - 2019-11-06 20:40:25.712138: step 31950, total loss = 0.21, predict loss = 0.05 (79.5 examples/sec; 0.050 sec/batch; 1h:38m:56s remains)
INFO - root - 2019-11-06 20:40:26.298016: step 31960, total loss = 0.26, predict loss = 0.06 (74.4 examples/sec; 0.054 sec/batch; 1h:45m:46s remains)
INFO - root - 2019-11-06 20:40:26.942207: step 31970, total loss = 0.32, predict loss = 0.09 (78.7 examples/sec; 0.051 sec/batch; 1h:39m:55s remains)
INFO - root - 2019-11-06 20:40:27.501286: step 31980, total loss = 0.35, predict loss = 0.08 (91.3 examples/sec; 0.044 sec/batch; 1h:26m:09s remains)
INFO - root - 2019-11-06 20:40:27.969146: step 31990, total loss = 0.24, predict loss = 0.07 (107.3 examples/sec; 0.037 sec/batch; 1h:13m:19s remains)
INFO - root - 2019-11-06 20:40:28.413810: step 32000, total loss = 0.37, predict loss = 0.09 (104.3 examples/sec; 0.038 sec/batch; 1h:15m:25s remains)
INFO - root - 2019-11-06 20:40:29.377334: step 32010, total loss = 0.14, predict loss = 0.03 (73.7 examples/sec; 0.054 sec/batch; 1h:46m:45s remains)
INFO - root - 2019-11-06 20:40:29.998543: step 32020, total loss = 0.22, predict loss = 0.06 (74.3 examples/sec; 0.054 sec/batch; 1h:45m:49s remains)
INFO - root - 2019-11-06 20:40:30.663721: step 32030, total loss = 0.23, predict loss = 0.06 (67.3 examples/sec; 0.059 sec/batch; 1h:56m:54s remains)
INFO - root - 2019-11-06 20:40:31.267181: step 32040, total loss = 0.30, predict loss = 0.08 (66.9 examples/sec; 0.060 sec/batch; 1h:57m:33s remains)
INFO - root - 2019-11-06 20:40:31.896812: step 32050, total loss = 0.22, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 1h:39m:59s remains)
INFO - root - 2019-11-06 20:40:32.479215: step 32060, total loss = 0.19, predict loss = 0.05 (70.0 examples/sec; 0.057 sec/batch; 1h:52m:16s remains)
INFO - root - 2019-11-06 20:40:33.194140: step 32070, total loss = 0.23, predict loss = 0.07 (70.3 examples/sec; 0.057 sec/batch; 1h:51m:49s remains)
INFO - root - 2019-11-06 20:40:33.796489: step 32080, total loss = 0.33, predict loss = 0.09 (81.3 examples/sec; 0.049 sec/batch; 1h:36m:39s remains)
INFO - root - 2019-11-06 20:40:34.387780: step 32090, total loss = 0.20, predict loss = 0.05 (74.3 examples/sec; 0.054 sec/batch; 1h:45m:48s remains)
INFO - root - 2019-11-06 20:40:34.960440: step 32100, total loss = 0.25, predict loss = 0.06 (80.9 examples/sec; 0.049 sec/batch; 1h:37m:06s remains)
INFO - root - 2019-11-06 20:40:35.534660: step 32110, total loss = 0.45, predict loss = 0.11 (82.2 examples/sec; 0.049 sec/batch; 1h:35m:37s remains)
INFO - root - 2019-11-06 20:40:36.195953: step 32120, total loss = 0.25, predict loss = 0.07 (78.2 examples/sec; 0.051 sec/batch; 1h:40m:31s remains)
INFO - root - 2019-11-06 20:40:36.761928: step 32130, total loss = 0.30, predict loss = 0.07 (92.9 examples/sec; 0.043 sec/batch; 1h:24m:35s remains)
INFO - root - 2019-11-06 20:40:37.200060: step 32140, total loss = 0.39, predict loss = 0.11 (98.6 examples/sec; 0.041 sec/batch; 1h:19m:40s remains)
INFO - root - 2019-11-06 20:40:37.642216: step 32150, total loss = 0.23, predict loss = 0.06 (102.1 examples/sec; 0.039 sec/batch; 1h:16m:58s remains)
INFO - root - 2019-11-06 20:40:38.704764: step 32160, total loss = 0.30, predict loss = 0.08 (60.7 examples/sec; 0.066 sec/batch; 2h:09m:26s remains)
INFO - root - 2019-11-06 20:40:39.448504: step 32170, total loss = 0.24, predict loss = 0.05 (60.9 examples/sec; 0.066 sec/batch; 2h:08m:59s remains)
INFO - root - 2019-11-06 20:40:40.194625: step 32180, total loss = 0.27, predict loss = 0.08 (58.1 examples/sec; 0.069 sec/batch; 2h:15m:05s remains)
INFO - root - 2019-11-06 20:40:40.905849: step 32190, total loss = 0.26, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 1h:40m:19s remains)
INFO - root - 2019-11-06 20:40:41.489278: step 32200, total loss = 0.24, predict loss = 0.07 (79.1 examples/sec; 0.051 sec/batch; 1h:39m:14s remains)
INFO - root - 2019-11-06 20:40:42.136220: step 32210, total loss = 0.26, predict loss = 0.07 (78.6 examples/sec; 0.051 sec/batch; 1h:39m:53s remains)
INFO - root - 2019-11-06 20:40:42.696135: step 32220, total loss = 0.31, predict loss = 0.08 (73.6 examples/sec; 0.054 sec/batch; 1h:46m:37s remains)
INFO - root - 2019-11-06 20:40:43.271287: step 32230, total loss = 0.23, predict loss = 0.06 (78.0 examples/sec; 0.051 sec/batch; 1h:40m:37s remains)
INFO - root - 2019-11-06 20:40:43.826686: step 32240, total loss = 0.29, predict loss = 0.07 (81.3 examples/sec; 0.049 sec/batch; 1h:36m:36s remains)
INFO - root - 2019-11-06 20:40:44.417195: step 32250, total loss = 0.15, predict loss = 0.03 (77.2 examples/sec; 0.052 sec/batch; 1h:41m:43s remains)
INFO - root - 2019-11-06 20:40:44.998704: step 32260, total loss = 0.33, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 1h:40m:24s remains)
INFO - root - 2019-11-06 20:40:45.568427: step 32270, total loss = 0.21, predict loss = 0.05 (80.3 examples/sec; 0.050 sec/batch; 1h:37m:41s remains)
INFO - root - 2019-11-06 20:40:46.098723: step 32280, total loss = 0.49, predict loss = 0.17 (94.6 examples/sec; 0.042 sec/batch; 1h:22m:58s remains)
INFO - root - 2019-11-06 20:40:46.565954: step 32290, total loss = 0.21, predict loss = 0.06 (96.9 examples/sec; 0.041 sec/batch; 1h:21m:00s remains)
INFO - root - 2019-11-06 20:40:47.035929: step 32300, total loss = 0.18, predict loss = 0.04 (94.0 examples/sec; 0.043 sec/batch; 1h:23m:26s remains)
INFO - root - 2019-11-06 20:40:48.094094: step 32310, total loss = 0.29, predict loss = 0.07 (50.3 examples/sec; 0.080 sec/batch; 2h:36m:04s remains)
INFO - root - 2019-11-06 20:40:48.739338: step 32320, total loss = 0.28, predict loss = 0.08 (79.4 examples/sec; 0.050 sec/batch; 1h:38m:49s remains)
INFO - root - 2019-11-06 20:40:49.336732: step 32330, total loss = 0.23, predict loss = 0.06 (78.8 examples/sec; 0.051 sec/batch; 1h:39m:33s remains)
INFO - root - 2019-11-06 20:40:49.917954: step 32340, total loss = 0.34, predict loss = 0.07 (77.0 examples/sec; 0.052 sec/batch; 1h:41m:50s remains)
INFO - root - 2019-11-06 20:40:50.505218: step 32350, total loss = 0.23, predict loss = 0.07 (82.4 examples/sec; 0.049 sec/batch; 1h:35m:11s remains)
INFO - root - 2019-11-06 20:40:51.081665: step 32360, total loss = 0.30, predict loss = 0.08 (78.7 examples/sec; 0.051 sec/batch; 1h:39m:38s remains)
INFO - root - 2019-11-06 20:40:51.666368: step 32370, total loss = 0.29, predict loss = 0.09 (78.1 examples/sec; 0.051 sec/batch; 1h:40m:26s remains)
INFO - root - 2019-11-06 20:40:52.241074: step 32380, total loss = 0.22, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:39m:04s remains)
INFO - root - 2019-11-06 20:40:52.828181: step 32390, total loss = 0.24, predict loss = 0.06 (74.8 examples/sec; 0.054 sec/batch; 1h:44m:53s remains)
INFO - root - 2019-11-06 20:40:53.406031: step 32400, total loss = 0.28, predict loss = 0.07 (80.7 examples/sec; 0.050 sec/batch; 1h:37m:08s remains)
INFO - root - 2019-11-06 20:40:53.986898: step 32410, total loss = 0.26, predict loss = 0.07 (84.7 examples/sec; 0.047 sec/batch; 1h:32m:31s remains)
INFO - root - 2019-11-06 20:40:54.549416: step 32420, total loss = 0.21, predict loss = 0.05 (77.8 examples/sec; 0.051 sec/batch; 1h:40m:46s remains)
INFO - root - 2019-11-06 20:40:55.046717: step 32430, total loss = 0.32, predict loss = 0.09 (97.0 examples/sec; 0.041 sec/batch; 1h:20m:49s remains)
INFO - root - 2019-11-06 20:40:55.495491: step 32440, total loss = 0.26, predict loss = 0.07 (96.8 examples/sec; 0.041 sec/batch; 1h:20m:59s remains)
INFO - root - 2019-11-06 20:40:55.976669: step 32450, total loss = 0.22, predict loss = 0.06 (93.4 examples/sec; 0.043 sec/batch; 1h:23m:53s remains)
INFO - root - 2019-11-06 20:40:57.122628: step 32460, total loss = 0.24, predict loss = 0.06 (67.8 examples/sec; 0.059 sec/batch; 1h:55m:35s remains)
INFO - root - 2019-11-06 20:40:57.783769: step 32470, total loss = 0.14, predict loss = 0.04 (70.1 examples/sec; 0.057 sec/batch; 1h:51m:47s remains)
INFO - root - 2019-11-06 20:40:58.381259: step 32480, total loss = 0.20, predict loss = 0.05 (77.4 examples/sec; 0.052 sec/batch; 1h:41m:14s remains)
INFO - root - 2019-11-06 20:40:58.969175: step 32490, total loss = 0.25, predict loss = 0.07 (77.8 examples/sec; 0.051 sec/batch; 1h:40m:45s remains)
INFO - root - 2019-11-06 20:40:59.550160: step 32500, total loss = 0.25, predict loss = 0.08 (78.5 examples/sec; 0.051 sec/batch; 1h:39m:49s remains)
INFO - root - 2019-11-06 20:41:00.134166: step 32510, total loss = 0.33, predict loss = 0.10 (76.7 examples/sec; 0.052 sec/batch; 1h:42m:03s remains)
INFO - root - 2019-11-06 20:41:00.708773: step 32520, total loss = 0.18, predict loss = 0.04 (77.4 examples/sec; 0.052 sec/batch; 1h:41m:14s remains)
INFO - root - 2019-11-06 20:41:01.307255: step 32530, total loss = 0.37, predict loss = 0.09 (77.5 examples/sec; 0.052 sec/batch; 1h:41m:06s remains)
INFO - root - 2019-11-06 20:41:01.877237: step 32540, total loss = 0.23, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 1h:41m:05s remains)
INFO - root - 2019-11-06 20:41:02.475649: step 32550, total loss = 0.44, predict loss = 0.14 (76.9 examples/sec; 0.052 sec/batch; 1h:41m:51s remains)
INFO - root - 2019-11-06 20:41:03.054095: step 32560, total loss = 0.26, predict loss = 0.06 (80.0 examples/sec; 0.050 sec/batch; 1h:37m:53s remains)
INFO - root - 2019-11-06 20:41:03.629846: step 32570, total loss = 0.31, predict loss = 0.08 (76.7 examples/sec; 0.052 sec/batch; 1h:42m:07s remains)
INFO - root - 2019-11-06 20:41:04.110649: step 32580, total loss = 0.15, predict loss = 0.04 (98.9 examples/sec; 0.040 sec/batch; 1h:19m:07s remains)
INFO - root - 2019-11-06 20:41:04.556957: step 32590, total loss = 0.41, predict loss = 0.12 (96.0 examples/sec; 0.042 sec/batch; 1h:21m:30s remains)
INFO - root - 2019-11-06 20:41:05.459753: step 32600, total loss = 0.27, predict loss = 0.07 (8.3 examples/sec; 0.484 sec/batch; 15h:46m:05s remains)
INFO - root - 2019-11-06 20:41:06.114459: step 32610, total loss = 0.37, predict loss = 0.11 (65.7 examples/sec; 0.061 sec/batch; 1h:59m:11s remains)
INFO - root - 2019-11-06 20:41:06.727711: step 32620, total loss = 0.28, predict loss = 0.07 (76.3 examples/sec; 0.052 sec/batch; 1h:42m:33s remains)
INFO - root - 2019-11-06 20:41:07.296280: step 32630, total loss = 0.30, predict loss = 0.08 (77.6 examples/sec; 0.052 sec/batch; 1h:40m:47s remains)
INFO - root - 2019-11-06 20:41:07.869540: step 32640, total loss = 0.21, predict loss = 0.05 (76.1 examples/sec; 0.053 sec/batch; 1h:42m:52s remains)
INFO - root - 2019-11-06 20:41:08.466699: step 32650, total loss = 0.27, predict loss = 0.07 (80.0 examples/sec; 0.050 sec/batch; 1h:37m:50s remains)
INFO - root - 2019-11-06 20:41:09.046999: step 32660, total loss = 0.17, predict loss = 0.04 (76.8 examples/sec; 0.052 sec/batch; 1h:41m:51s remains)
INFO - root - 2019-11-06 20:41:09.632586: step 32670, total loss = 0.26, predict loss = 0.07 (76.0 examples/sec; 0.053 sec/batch; 1h:42m:53s remains)
INFO - root - 2019-11-06 20:41:10.275477: step 32680, total loss = 0.30, predict loss = 0.08 (62.8 examples/sec; 0.064 sec/batch; 2h:04m:36s remains)
INFO - root - 2019-11-06 20:41:10.973752: step 32690, total loss = 0.18, predict loss = 0.05 (64.9 examples/sec; 0.062 sec/batch; 2h:00m:28s remains)
INFO - root - 2019-11-06 20:41:11.583009: step 32700, total loss = 0.23, predict loss = 0.07 (75.8 examples/sec; 0.053 sec/batch; 1h:43m:13s remains)
INFO - root - 2019-11-06 20:41:12.173846: step 32710, total loss = 0.25, predict loss = 0.06 (79.6 examples/sec; 0.050 sec/batch; 1h:38m:11s remains)
INFO - root - 2019-11-06 20:41:12.746366: step 32720, total loss = 0.24, predict loss = 0.07 (85.0 examples/sec; 0.047 sec/batch; 1h:32m:01s remains)
INFO - root - 2019-11-06 20:41:13.227768: step 32730, total loss = 0.23, predict loss = 0.07 (97.3 examples/sec; 0.041 sec/batch; 1h:20m:22s remains)
INFO - root - 2019-11-06 20:41:13.679746: step 32740, total loss = 0.25, predict loss = 0.07 (93.5 examples/sec; 0.043 sec/batch; 1h:23m:38s remains)
INFO - root - 2019-11-06 20:41:14.599641: step 32750, total loss = 0.38, predict loss = 0.11 (83.5 examples/sec; 0.048 sec/batch; 1h:33m:38s remains)
INFO - root - 2019-11-06 20:41:15.267051: step 32760, total loss = 0.34, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 1h:52m:31s remains)
INFO - root - 2019-11-06 20:41:15.964144: step 32770, total loss = 0.27, predict loss = 0.07 (67.4 examples/sec; 0.059 sec/batch; 1h:55m:55s remains)
INFO - root - 2019-11-06 20:41:16.560263: step 32780, total loss = 0.19, predict loss = 0.05 (82.2 examples/sec; 0.049 sec/batch; 1h:35m:02s remains)
INFO - root - 2019-11-06 20:41:17.142570: step 32790, total loss = 0.26, predict loss = 0.07 (81.5 examples/sec; 0.049 sec/batch; 1h:35m:53s remains)
INFO - root - 2019-11-06 20:41:17.717716: step 32800, total loss = 0.18, predict loss = 0.05 (83.0 examples/sec; 0.048 sec/batch; 1h:34m:06s remains)
INFO - root - 2019-11-06 20:41:18.309161: step 32810, total loss = 0.23, predict loss = 0.06 (80.4 examples/sec; 0.050 sec/batch; 1h:37m:12s remains)
INFO - root - 2019-11-06 20:41:18.886360: step 32820, total loss = 0.15, predict loss = 0.04 (76.6 examples/sec; 0.052 sec/batch; 1h:41m:58s remains)
INFO - root - 2019-11-06 20:41:19.468293: step 32830, total loss = 0.35, predict loss = 0.09 (82.0 examples/sec; 0.049 sec/batch; 1h:35m:15s remains)
INFO - root - 2019-11-06 20:41:20.041634: step 32840, total loss = 0.24, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 1h:40m:44s remains)
INFO - root - 2019-11-06 20:41:20.635882: step 32850, total loss = 0.25, predict loss = 0.07 (82.2 examples/sec; 0.049 sec/batch; 1h:34m:57s remains)
INFO - root - 2019-11-06 20:41:21.200561: step 32860, total loss = 0.23, predict loss = 0.06 (74.4 examples/sec; 0.054 sec/batch; 1h:44m:54s remains)
INFO - root - 2019-11-06 20:41:21.754193: step 32870, total loss = 0.21, predict loss = 0.05 (93.2 examples/sec; 0.043 sec/batch; 1h:23m:46s remains)
INFO - root - 2019-11-06 20:41:22.211597: step 32880, total loss = 0.35, predict loss = 0.10 (96.5 examples/sec; 0.041 sec/batch; 1h:20m:53s remains)
INFO - root - 2019-11-06 20:41:22.689532: step 32890, total loss = 0.23, predict loss = 0.06 (90.1 examples/sec; 0.044 sec/batch; 1h:26m:40s remains)
INFO - root - 2019-11-06 20:41:23.647299: step 32900, total loss = 0.20, predict loss = 0.05 (69.4 examples/sec; 0.058 sec/batch; 1h:52m:33s remains)
INFO - root - 2019-11-06 20:41:24.293335: step 32910, total loss = 0.35, predict loss = 0.09 (73.6 examples/sec; 0.054 sec/batch; 1h:46m:02s remains)
INFO - root - 2019-11-06 20:41:24.880749: step 32920, total loss = 0.16, predict loss = 0.04 (78.5 examples/sec; 0.051 sec/batch; 1h:39m:29s remains)
INFO - root - 2019-11-06 20:41:25.475638: step 32930, total loss = 0.25, predict loss = 0.06 (80.9 examples/sec; 0.049 sec/batch; 1h:36m:31s remains)
INFO - root - 2019-11-06 20:41:26.054125: step 32940, total loss = 0.27, predict loss = 0.07 (81.6 examples/sec; 0.049 sec/batch; 1h:35m:41s remains)
INFO - root - 2019-11-06 20:41:26.633578: step 32950, total loss = 0.23, predict loss = 0.06 (73.1 examples/sec; 0.055 sec/batch; 1h:46m:48s remains)
INFO - root - 2019-11-06 20:41:27.202037: step 32960, total loss = 0.30, predict loss = 0.07 (79.0 examples/sec; 0.051 sec/batch; 1h:38m:42s remains)
INFO - root - 2019-11-06 20:41:27.790732: step 32970, total loss = 0.32, predict loss = 0.08 (75.6 examples/sec; 0.053 sec/batch; 1h:43m:12s remains)
INFO - root - 2019-11-06 20:41:28.383564: step 32980, total loss = 0.28, predict loss = 0.07 (77.6 examples/sec; 0.052 sec/batch; 1h:40m:33s remains)
INFO - root - 2019-11-06 20:41:28.960055: step 32990, total loss = 0.21, predict loss = 0.06 (78.5 examples/sec; 0.051 sec/batch; 1h:39m:21s remains)
INFO - root - 2019-11-06 20:41:29.534057: step 33000, total loss = 0.31, predict loss = 0.09 (80.1 examples/sec; 0.050 sec/batch; 1h:37m:24s remains)
INFO - root - 2019-11-06 20:41:30.131480: step 33010, total loss = 0.29, predict loss = 0.08 (77.8 examples/sec; 0.051 sec/batch; 1h:40m:14s remains)
INFO - root - 2019-11-06 20:41:30.671915: step 33020, total loss = 0.26, predict loss = 0.07 (96.7 examples/sec; 0.041 sec/batch; 1h:20m:36s remains)
INFO - root - 2019-11-06 20:41:31.125682: step 33030, total loss = 0.14, predict loss = 0.04 (98.1 examples/sec; 0.041 sec/batch; 1h:19m:29s remains)
INFO - root - 2019-11-06 20:41:31.564648: step 33040, total loss = 0.32, predict loss = 0.08 (99.6 examples/sec; 0.040 sec/batch; 1h:18m:18s remains)
INFO - root - 2019-11-06 20:41:32.591189: step 33050, total loss = 0.37, predict loss = 0.10 (51.9 examples/sec; 0.077 sec/batch; 2h:30m:19s remains)
INFO - root - 2019-11-06 20:41:33.226867: step 33060, total loss = 0.36, predict loss = 0.10 (78.3 examples/sec; 0.051 sec/batch; 1h:39m:31s remains)
INFO - root - 2019-11-06 20:41:33.816595: step 33070, total loss = 0.33, predict loss = 0.10 (74.6 examples/sec; 0.054 sec/batch; 1h:44m:27s remains)
INFO - root - 2019-11-06 20:41:34.391411: step 33080, total loss = 0.20, predict loss = 0.05 (79.2 examples/sec; 0.051 sec/batch; 1h:38m:28s remains)
INFO - root - 2019-11-06 20:41:34.982908: step 33090, total loss = 0.22, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:38m:14s remains)
INFO - root - 2019-11-06 20:41:35.560842: step 33100, total loss = 0.25, predict loss = 0.06 (80.8 examples/sec; 0.049 sec/batch; 1h:36m:24s remains)
INFO - root - 2019-11-06 20:41:36.143527: step 33110, total loss = 0.38, predict loss = 0.10 (80.6 examples/sec; 0.050 sec/batch; 1h:36m:40s remains)
INFO - root - 2019-11-06 20:41:36.738931: step 33120, total loss = 0.24, predict loss = 0.06 (76.9 examples/sec; 0.052 sec/batch; 1h:41m:21s remains)
INFO - root - 2019-11-06 20:41:37.325457: step 33130, total loss = 0.23, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:40m:42s remains)
INFO - root - 2019-11-06 20:41:37.907354: step 33140, total loss = 0.38, predict loss = 0.10 (78.3 examples/sec; 0.051 sec/batch; 1h:39m:30s remains)
INFO - root - 2019-11-06 20:41:38.485477: step 33150, total loss = 0.29, predict loss = 0.07 (78.6 examples/sec; 0.051 sec/batch; 1h:39m:09s remains)
INFO - root - 2019-11-06 20:41:39.063430: step 33160, total loss = 0.25, predict loss = 0.06 (78.0 examples/sec; 0.051 sec/batch; 1h:39m:49s remains)
INFO - root - 2019-11-06 20:41:39.589801: step 33170, total loss = 0.30, predict loss = 0.07 (105.3 examples/sec; 0.038 sec/batch; 1h:13m:58s remains)
INFO - root - 2019-11-06 20:41:40.039812: step 33180, total loss = 0.20, predict loss = 0.06 (90.6 examples/sec; 0.044 sec/batch; 1h:26m:00s remains)
INFO - root - 2019-11-06 20:41:40.521898: step 33190, total loss = 0.24, predict loss = 0.05 (104.5 examples/sec; 0.038 sec/batch; 1h:14m:30s remains)
INFO - root - 2019-11-06 20:41:41.576830: step 33200, total loss = 0.26, predict loss = 0.07 (59.7 examples/sec; 0.067 sec/batch; 2h:10m:21s remains)
INFO - root - 2019-11-06 20:41:42.282059: step 33210, total loss = 0.30, predict loss = 0.10 (75.5 examples/sec; 0.053 sec/batch; 1h:43m:04s remains)
INFO - root - 2019-11-06 20:41:42.874864: step 33220, total loss = 0.19, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:40m:54s remains)
INFO - root - 2019-11-06 20:41:43.453640: step 33230, total loss = 0.23, predict loss = 0.06 (73.0 examples/sec; 0.055 sec/batch; 1h:46m:41s remains)
INFO - root - 2019-11-06 20:41:44.029212: step 33240, total loss = 0.20, predict loss = 0.05 (81.9 examples/sec; 0.049 sec/batch; 1h:35m:05s remains)
INFO - root - 2019-11-06 20:41:44.623625: step 33250, total loss = 0.21, predict loss = 0.06 (78.0 examples/sec; 0.051 sec/batch; 1h:39m:45s remains)
INFO - root - 2019-11-06 20:41:45.216686: step 33260, total loss = 0.24, predict loss = 0.07 (74.7 examples/sec; 0.054 sec/batch; 1h:44m:07s remains)
INFO - root - 2019-11-06 20:41:45.825787: step 33270, total loss = 0.32, predict loss = 0.09 (76.1 examples/sec; 0.053 sec/batch; 1h:42m:11s remains)
INFO - root - 2019-11-06 20:41:46.412483: step 33280, total loss = 0.21, predict loss = 0.05 (77.3 examples/sec; 0.052 sec/batch; 1h:40m:42s remains)
INFO - root - 2019-11-06 20:41:47.028477: step 33290, total loss = 0.17, predict loss = 0.04 (74.6 examples/sec; 0.054 sec/batch; 1h:44m:13s remains)
INFO - root - 2019-11-06 20:41:47.759343: step 33300, total loss = 0.22, predict loss = 0.06 (65.9 examples/sec; 0.061 sec/batch; 1h:58m:00s remains)
INFO - root - 2019-11-06 20:41:48.338224: step 33310, total loss = 0.23, predict loss = 0.06 (79.2 examples/sec; 0.050 sec/batch; 1h:38m:09s remains)
INFO - root - 2019-11-06 20:41:48.822306: step 33320, total loss = 0.24, predict loss = 0.07 (105.9 examples/sec; 0.038 sec/batch; 1h:13m:26s remains)
INFO - root - 2019-11-06 20:41:49.306195: step 33330, total loss = 0.18, predict loss = 0.04 (87.8 examples/sec; 0.046 sec/batch; 1h:28m:37s remains)
INFO - root - 2019-11-06 20:41:49.768868: step 33340, total loss = 0.30, predict loss = 0.08 (101.6 examples/sec; 0.039 sec/batch; 1h:16m:33s remains)
INFO - root - 2019-11-06 20:41:50.917208: step 33350, total loss = 0.22, predict loss = 0.06 (52.3 examples/sec; 0.077 sec/batch; 2h:28m:49s remains)
INFO - root - 2019-11-06 20:41:51.617070: step 33360, total loss = 0.27, predict loss = 0.07 (71.1 examples/sec; 0.056 sec/batch; 1h:49m:21s remains)
INFO - root - 2019-11-06 20:41:52.206487: step 33370, total loss = 0.24, predict loss = 0.06 (76.7 examples/sec; 0.052 sec/batch; 1h:41m:23s remains)
INFO - root - 2019-11-06 20:41:52.787175: step 33380, total loss = 0.34, predict loss = 0.09 (75.8 examples/sec; 0.053 sec/batch; 1h:42m:32s remains)
INFO - root - 2019-11-06 20:41:53.397111: step 33390, total loss = 0.24, predict loss = 0.06 (60.6 examples/sec; 0.066 sec/batch; 2h:08m:18s remains)
INFO - root - 2019-11-06 20:41:54.001678: step 33400, total loss = 0.22, predict loss = 0.05 (77.9 examples/sec; 0.051 sec/batch; 1h:39m:49s remains)
INFO - root - 2019-11-06 20:41:54.598148: step 33410, total loss = 0.24, predict loss = 0.06 (75.3 examples/sec; 0.053 sec/batch; 1h:43m:13s remains)
INFO - root - 2019-11-06 20:41:55.188831: step 33420, total loss = 0.24, predict loss = 0.07 (76.6 examples/sec; 0.052 sec/batch; 1h:41m:26s remains)
INFO - root - 2019-11-06 20:41:55.760881: step 33430, total loss = 0.47, predict loss = 0.13 (77.6 examples/sec; 0.052 sec/batch; 1h:40m:11s remains)
INFO - root - 2019-11-06 20:41:56.345162: step 33440, total loss = 0.14, predict loss = 0.03 (76.9 examples/sec; 0.052 sec/batch; 1h:41m:02s remains)
INFO - root - 2019-11-06 20:41:56.940087: step 33450, total loss = 0.18, predict loss = 0.04 (72.3 examples/sec; 0.055 sec/batch; 1h:47m:28s remains)
INFO - root - 2019-11-06 20:41:57.520567: step 33460, total loss = 0.26, predict loss = 0.06 (67.9 examples/sec; 0.059 sec/batch; 1h:54m:28s remains)
INFO - root - 2019-11-06 20:41:58.066429: step 33470, total loss = 0.48, predict loss = 0.14 (101.1 examples/sec; 0.040 sec/batch; 1h:16m:49s remains)
INFO - root - 2019-11-06 20:41:58.511555: step 33480, total loss = 0.29, predict loss = 0.08 (94.2 examples/sec; 0.042 sec/batch; 1h:22m:29s remains)
INFO - root - 2019-11-06 20:41:59.454567: step 33490, total loss = 0.22, predict loss = 0.05 (77.9 examples/sec; 0.051 sec/batch; 1h:39m:39s remains)
INFO - root - 2019-11-06 20:42:00.119388: step 33500, total loss = 0.24, predict loss = 0.05 (59.2 examples/sec; 0.068 sec/batch; 2h:11m:11s remains)
INFO - root - 2019-11-06 20:42:00.732273: step 33510, total loss = 0.38, predict loss = 0.10 (78.3 examples/sec; 0.051 sec/batch; 1h:39m:14s remains)
INFO - root - 2019-11-06 20:42:01.296405: step 33520, total loss = 0.25, predict loss = 0.06 (79.7 examples/sec; 0.050 sec/batch; 1h:37m:29s remains)
INFO - root - 2019-11-06 20:42:01.901589: step 33530, total loss = 0.28, predict loss = 0.07 (72.8 examples/sec; 0.055 sec/batch; 1h:46m:39s remains)
INFO - root - 2019-11-06 20:42:02.486035: step 33540, total loss = 0.19, predict loss = 0.05 (83.3 examples/sec; 0.048 sec/batch; 1h:33m:12s remains)
INFO - root - 2019-11-06 20:42:03.064930: step 33550, total loss = 0.35, predict loss = 0.09 (77.8 examples/sec; 0.051 sec/batch; 1h:39m:46s remains)
INFO - root - 2019-11-06 20:42:03.654591: step 33560, total loss = 0.26, predict loss = 0.07 (77.2 examples/sec; 0.052 sec/batch; 1h:40m:29s remains)
INFO - root - 2019-11-06 20:42:04.252314: step 33570, total loss = 0.25, predict loss = 0.07 (82.3 examples/sec; 0.049 sec/batch; 1h:34m:20s remains)
INFO - root - 2019-11-06 20:42:04.837091: step 33580, total loss = 0.21, predict loss = 0.05 (80.6 examples/sec; 0.050 sec/batch; 1h:36m:15s remains)
INFO - root - 2019-11-06 20:42:05.437381: step 33590, total loss = 0.27, predict loss = 0.07 (68.2 examples/sec; 0.059 sec/batch; 1h:53m:46s remains)
INFO - root - 2019-11-06 20:42:06.059711: step 33600, total loss = 0.24, predict loss = 0.07 (79.2 examples/sec; 0.050 sec/batch; 1h:37m:58s remains)
INFO - root - 2019-11-06 20:42:06.704519: step 33610, total loss = 0.23, predict loss = 0.06 (91.2 examples/sec; 0.044 sec/batch; 1h:25m:05s remains)
INFO - root - 2019-11-06 20:42:07.151029: step 33620, total loss = 0.24, predict loss = 0.06 (102.5 examples/sec; 0.039 sec/batch; 1h:15m:41s remains)
INFO - root - 2019-11-06 20:42:07.603496: step 33630, total loss = 0.33, predict loss = 0.09 (92.1 examples/sec; 0.043 sec/batch; 1h:24m:13s remains)
INFO - root - 2019-11-06 20:42:08.563057: step 33640, total loss = 0.22, predict loss = 0.05 (75.6 examples/sec; 0.053 sec/batch; 1h:42m:39s remains)
INFO - root - 2019-11-06 20:42:09.292005: step 33650, total loss = 0.26, predict loss = 0.07 (61.7 examples/sec; 0.065 sec/batch; 2h:05m:45s remains)
INFO - root - 2019-11-06 20:42:09.906058: step 33660, total loss = 0.27, predict loss = 0.07 (73.4 examples/sec; 0.054 sec/batch; 1h:45m:38s remains)
INFO - root - 2019-11-06 20:42:10.553657: step 33670, total loss = 0.22, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:39m:26s remains)
INFO - root - 2019-11-06 20:42:11.130627: step 33680, total loss = 0.19, predict loss = 0.04 (83.3 examples/sec; 0.048 sec/batch; 1h:33m:06s remains)
INFO - root - 2019-11-06 20:42:11.727153: step 33690, total loss = 0.21, predict loss = 0.06 (82.5 examples/sec; 0.048 sec/batch; 1h:33m:57s remains)
INFO - root - 2019-11-06 20:42:12.291009: step 33700, total loss = 0.29, predict loss = 0.08 (79.6 examples/sec; 0.050 sec/batch; 1h:37m:23s remains)
INFO - root - 2019-11-06 20:42:12.862448: step 33710, total loss = 0.18, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:40m:27s remains)
INFO - root - 2019-11-06 20:42:13.426606: step 33720, total loss = 0.36, predict loss = 0.10 (78.1 examples/sec; 0.051 sec/batch; 1h:39m:16s remains)
INFO - root - 2019-11-06 20:42:14.015431: step 33730, total loss = 0.22, predict loss = 0.05 (76.3 examples/sec; 0.052 sec/batch; 1h:41m:38s remains)
INFO - root - 2019-11-06 20:42:14.586730: step 33740, total loss = 0.21, predict loss = 0.05 (76.7 examples/sec; 0.052 sec/batch; 1h:41m:05s remains)
INFO - root - 2019-11-06 20:42:15.159287: step 33750, total loss = 0.20, predict loss = 0.05 (81.6 examples/sec; 0.049 sec/batch; 1h:34m:57s remains)
INFO - root - 2019-11-06 20:42:15.692215: step 33760, total loss = 0.21, predict loss = 0.05 (95.3 examples/sec; 0.042 sec/batch; 1h:21m:20s remains)
INFO - root - 2019-11-06 20:42:16.165387: step 33770, total loss = 0.32, predict loss = 0.09 (96.7 examples/sec; 0.041 sec/batch; 1h:20m:07s remains)
INFO - root - 2019-11-06 20:42:16.605309: step 33780, total loss = 0.21, predict loss = 0.06 (101.9 examples/sec; 0.039 sec/batch; 1h:16m:01s remains)
INFO - root - 2019-11-06 20:42:17.578941: step 33790, total loss = 0.38, predict loss = 0.10 (56.3 examples/sec; 0.071 sec/batch; 2h:17m:40s remains)
INFO - root - 2019-11-06 20:42:18.279097: step 33800, total loss = 0.27, predict loss = 0.07 (72.9 examples/sec; 0.055 sec/batch; 1h:46m:17s remains)
INFO - root - 2019-11-06 20:42:18.913868: step 33810, total loss = 0.23, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:40m:00s remains)
INFO - root - 2019-11-06 20:42:19.484957: step 33820, total loss = 0.20, predict loss = 0.06 (75.8 examples/sec; 0.053 sec/batch; 1h:42m:08s remains)
INFO - root - 2019-11-06 20:42:20.053081: step 33830, total loss = 0.18, predict loss = 0.05 (77.8 examples/sec; 0.051 sec/batch; 1h:39m:29s remains)
INFO - root - 2019-11-06 20:42:20.623243: step 33840, total loss = 0.26, predict loss = 0.08 (83.1 examples/sec; 0.048 sec/batch; 1h:33m:12s remains)
INFO - root - 2019-11-06 20:42:21.212609: step 33850, total loss = 0.33, predict loss = 0.08 (78.7 examples/sec; 0.051 sec/batch; 1h:38m:20s remains)
INFO - root - 2019-11-06 20:42:21.800557: step 33860, total loss = 0.25, predict loss = 0.07 (79.2 examples/sec; 0.051 sec/batch; 1h:37m:45s remains)
INFO - root - 2019-11-06 20:42:22.385378: step 33870, total loss = 0.21, predict loss = 0.05 (75.8 examples/sec; 0.053 sec/batch; 1h:42m:05s remains)
INFO - root - 2019-11-06 20:42:22.954977: step 33880, total loss = 0.30, predict loss = 0.10 (78.9 examples/sec; 0.051 sec/batch; 1h:38m:09s remains)
INFO - root - 2019-11-06 20:42:23.540796: step 33890, total loss = 0.20, predict loss = 0.05 (75.9 examples/sec; 0.053 sec/batch; 1h:41m:55s remains)
INFO - root - 2019-11-06 20:42:24.120106: step 33900, total loss = 0.23, predict loss = 0.07 (78.1 examples/sec; 0.051 sec/batch; 1h:39m:04s remains)
INFO - root - 2019-11-06 20:42:24.632744: step 33910, total loss = 0.28, predict loss = 0.08 (96.8 examples/sec; 0.041 sec/batch; 1h:19m:55s remains)
INFO - root - 2019-11-06 20:42:25.078074: step 33920, total loss = 0.27, predict loss = 0.07 (96.1 examples/sec; 0.042 sec/batch; 1h:20m:33s remains)
INFO - root - 2019-11-06 20:42:25.560527: step 33930, total loss = 0.23, predict loss = 0.05 (100.6 examples/sec; 0.040 sec/batch; 1h:16m:57s remains)
INFO - root - 2019-11-06 20:42:26.596123: step 33940, total loss = 0.18, predict loss = 0.05 (52.7 examples/sec; 0.076 sec/batch; 2h:26m:46s remains)
INFO - root - 2019-11-06 20:42:27.237902: step 33950, total loss = 0.23, predict loss = 0.06 (73.9 examples/sec; 0.054 sec/batch; 1h:44m:43s remains)
INFO - root - 2019-11-06 20:42:27.815462: step 33960, total loss = 0.20, predict loss = 0.05 (79.4 examples/sec; 0.050 sec/batch; 1h:37m:26s remains)
INFO - root - 2019-11-06 20:42:28.390718: step 33970, total loss = 0.17, predict loss = 0.04 (83.5 examples/sec; 0.048 sec/batch; 1h:32m:35s remains)
INFO - root - 2019-11-06 20:42:28.963877: step 33980, total loss = 0.28, predict loss = 0.07 (79.2 examples/sec; 0.051 sec/batch; 1h:37m:40s remains)
INFO - root - 2019-11-06 20:42:29.539428: step 33990, total loss = 0.16, predict loss = 0.05 (74.8 examples/sec; 0.053 sec/batch; 1h:43m:25s remains)
INFO - root - 2019-11-06 20:42:30.124681: step 34000, total loss = 0.27, predict loss = 0.08 (77.3 examples/sec; 0.052 sec/batch; 1h:40m:03s remains)
INFO - root - 2019-11-06 20:42:30.723491: step 34010, total loss = 0.21, predict loss = 0.06 (75.1 examples/sec; 0.053 sec/batch; 1h:43m:01s remains)
INFO - root - 2019-11-06 20:42:31.302041: step 34020, total loss = 0.27, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 1h:38m:43s remains)
INFO - root - 2019-11-06 20:42:31.884462: step 34030, total loss = 0.19, predict loss = 0.04 (71.5 examples/sec; 0.056 sec/batch; 1h:48m:09s remains)
INFO - root - 2019-11-06 20:42:32.461388: step 34040, total loss = 0.43, predict loss = 0.14 (80.9 examples/sec; 0.049 sec/batch; 1h:35m:37s remains)
INFO - root - 2019-11-06 20:42:33.060563: step 34050, total loss = 0.28, predict loss = 0.06 (77.9 examples/sec; 0.051 sec/batch; 1h:39m:13s remains)
INFO - root - 2019-11-06 20:42:33.564993: step 34060, total loss = 0.22, predict loss = 0.05 (100.6 examples/sec; 0.040 sec/batch; 1h:16m:51s remains)
INFO - root - 2019-11-06 20:42:34.024586: step 34070, total loss = 0.25, predict loss = 0.07 (100.3 examples/sec; 0.040 sec/batch; 1h:17m:05s remains)
INFO - root - 2019-11-06 20:42:34.471856: step 34080, total loss = 0.23, predict loss = 0.06 (92.5 examples/sec; 0.043 sec/batch; 1h:23m:31s remains)
INFO - root - 2019-11-06 20:42:35.639345: step 34090, total loss = 0.26, predict loss = 0.08 (47.1 examples/sec; 0.085 sec/batch; 2h:44m:11s remains)
INFO - root - 2019-11-06 20:42:36.298390: step 34100, total loss = 0.25, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:36m:49s remains)
INFO - root - 2019-11-06 20:42:36.873084: step 34110, total loss = 0.29, predict loss = 0.08 (76.8 examples/sec; 0.052 sec/batch; 1h:40m:33s remains)
INFO - root - 2019-11-06 20:42:37.438660: step 34120, total loss = 0.26, predict loss = 0.06 (82.9 examples/sec; 0.048 sec/batch; 1h:33m:09s remains)
INFO - root - 2019-11-06 20:42:38.031101: step 34130, total loss = 0.24, predict loss = 0.06 (79.7 examples/sec; 0.050 sec/batch; 1h:36m:54s remains)
INFO - root - 2019-11-06 20:42:38.652035: step 34140, total loss = 0.20, predict loss = 0.05 (67.6 examples/sec; 0.059 sec/batch; 1h:54m:19s remains)
INFO - root - 2019-11-06 20:42:39.254880: step 34150, total loss = 0.18, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:40m:01s remains)
INFO - root - 2019-11-06 20:42:39.827041: step 34160, total loss = 0.25, predict loss = 0.07 (79.2 examples/sec; 0.050 sec/batch; 1h:37m:27s remains)
INFO - root - 2019-11-06 20:42:40.452229: step 34170, total loss = 0.21, predict loss = 0.04 (78.9 examples/sec; 0.051 sec/batch; 1h:37m:50s remains)
INFO - root - 2019-11-06 20:42:41.046964: step 34180, total loss = 0.20, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 1h:38m:36s remains)
INFO - root - 2019-11-06 20:42:41.612478: step 34190, total loss = 0.28, predict loss = 0.08 (77.0 examples/sec; 0.052 sec/batch; 1h:40m:16s remains)
INFO - root - 2019-11-06 20:42:42.261611: step 34200, total loss = 0.19, predict loss = 0.04 (78.5 examples/sec; 0.051 sec/batch; 1h:38m:21s remains)
INFO - root - 2019-11-06 20:42:42.755986: step 34210, total loss = 0.23, predict loss = 0.07 (100.8 examples/sec; 0.040 sec/batch; 1h:16m:36s remains)
INFO - root - 2019-11-06 20:42:43.218329: step 34220, total loss = 0.25, predict loss = 0.07 (100.4 examples/sec; 0.040 sec/batch; 1h:16m:54s remains)
INFO - root - 2019-11-06 20:42:44.125953: step 34230, total loss = 0.20, predict loss = 0.05 (7.9 examples/sec; 0.505 sec/batch; 16h:13m:48s remains)
INFO - root - 2019-11-06 20:42:44.842134: step 34240, total loss = 0.23, predict loss = 0.05 (62.2 examples/sec; 0.064 sec/batch; 2h:04m:02s remains)
INFO - root - 2019-11-06 20:42:45.580637: step 34250, total loss = 0.17, predict loss = 0.04 (58.6 examples/sec; 0.068 sec/batch; 2h:11m:44s remains)
INFO - root - 2019-11-06 20:42:46.362176: step 34260, total loss = 0.20, predict loss = 0.06 (65.0 examples/sec; 0.062 sec/batch; 1h:58m:47s remains)
INFO - root - 2019-11-06 20:42:46.972852: step 34270, total loss = 0.29, predict loss = 0.08 (72.6 examples/sec; 0.055 sec/batch; 1h:46m:19s remains)
INFO - root - 2019-11-06 20:42:47.574186: step 34280, total loss = 0.24, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:39m:54s remains)
INFO - root - 2019-11-06 20:42:48.168887: step 34290, total loss = 0.39, predict loss = 0.12 (78.7 examples/sec; 0.051 sec/batch; 1h:38m:04s remains)
INFO - root - 2019-11-06 20:42:48.755842: step 34300, total loss = 0.27, predict loss = 0.07 (77.7 examples/sec; 0.051 sec/batch; 1h:39m:13s remains)
INFO - root - 2019-11-06 20:42:49.319870: step 34310, total loss = 0.25, predict loss = 0.07 (79.6 examples/sec; 0.050 sec/batch; 1h:36m:56s remains)
INFO - root - 2019-11-06 20:42:49.895002: step 34320, total loss = 0.33, predict loss = 0.11 (78.9 examples/sec; 0.051 sec/batch; 1h:37m:44s remains)
INFO - root - 2019-11-06 20:42:50.470997: step 34330, total loss = 0.27, predict loss = 0.08 (79.9 examples/sec; 0.050 sec/batch; 1h:36m:28s remains)
INFO - root - 2019-11-06 20:42:51.040185: step 34340, total loss = 0.22, predict loss = 0.06 (77.3 examples/sec; 0.052 sec/batch; 1h:39m:41s remains)
INFO - root - 2019-11-06 20:42:51.618888: step 34350, total loss = 0.22, predict loss = 0.05 (86.8 examples/sec; 0.046 sec/batch; 1h:28m:47s remains)
INFO - root - 2019-11-06 20:42:52.083869: step 34360, total loss = 0.20, predict loss = 0.05 (94.5 examples/sec; 0.042 sec/batch; 1h:21m:33s remains)
INFO - root - 2019-11-06 20:42:52.586906: step 34370, total loss = 0.33, predict loss = 0.09 (99.0 examples/sec; 0.040 sec/batch; 1h:17m:53s remains)
INFO - root - 2019-11-06 20:42:53.566698: step 34380, total loss = 0.28, predict loss = 0.07 (85.2 examples/sec; 0.047 sec/batch; 1h:30m:30s remains)
INFO - root - 2019-11-06 20:42:54.259886: step 34390, total loss = 0.26, predict loss = 0.07 (61.9 examples/sec; 0.065 sec/batch; 2h:04m:34s remains)
INFO - root - 2019-11-06 20:42:54.849618: step 34400, total loss = 0.21, predict loss = 0.05 (76.5 examples/sec; 0.052 sec/batch; 1h:40m:47s remains)
INFO - root - 2019-11-06 20:42:55.434045: step 34410, total loss = 0.19, predict loss = 0.05 (84.0 examples/sec; 0.048 sec/batch; 1h:31m:46s remains)
INFO - root - 2019-11-06 20:42:56.009792: step 34420, total loss = 0.16, predict loss = 0.04 (81.7 examples/sec; 0.049 sec/batch; 1h:34m:15s remains)
INFO - root - 2019-11-06 20:42:56.665108: step 34430, total loss = 0.21, predict loss = 0.05 (62.4 examples/sec; 0.064 sec/batch; 2h:03m:29s remains)
INFO - root - 2019-11-06 20:42:57.261690: step 34440, total loss = 0.19, predict loss = 0.05 (77.3 examples/sec; 0.052 sec/batch; 1h:39m:38s remains)
INFO - root - 2019-11-06 20:42:57.850909: step 34450, total loss = 0.24, predict loss = 0.06 (78.5 examples/sec; 0.051 sec/batch; 1h:38m:07s remains)
INFO - root - 2019-11-06 20:42:58.471209: step 34460, total loss = 0.29, predict loss = 0.08 (64.0 examples/sec; 0.062 sec/batch; 2h:00m:16s remains)
INFO - root - 2019-11-06 20:42:59.088683: step 34470, total loss = 0.35, predict loss = 0.09 (77.0 examples/sec; 0.052 sec/batch; 1h:39m:58s remains)
INFO - root - 2019-11-06 20:42:59.723135: step 34480, total loss = 0.28, predict loss = 0.09 (77.0 examples/sec; 0.052 sec/batch; 1h:40m:01s remains)
INFO - root - 2019-11-06 20:43:00.312589: step 34490, total loss = 0.27, predict loss = 0.07 (77.2 examples/sec; 0.052 sec/batch; 1h:39m:42s remains)
INFO - root - 2019-11-06 20:43:00.868865: step 34500, total loss = 0.19, predict loss = 0.05 (96.2 examples/sec; 0.042 sec/batch; 1h:20m:04s remains)
INFO - root - 2019-11-06 20:43:01.325925: step 34510, total loss = 0.39, predict loss = 0.12 (93.4 examples/sec; 0.043 sec/batch; 1h:22m:26s remains)
INFO - root - 2019-11-06 20:43:01.790750: step 34520, total loss = 0.39, predict loss = 0.11 (89.9 examples/sec; 0.044 sec/batch; 1h:25m:37s remains)
INFO - root - 2019-11-06 20:43:02.740830: step 34530, total loss = 0.27, predict loss = 0.07 (63.2 examples/sec; 0.063 sec/batch; 2h:01m:44s remains)
INFO - root - 2019-11-06 20:43:03.416266: step 34540, total loss = 0.25, predict loss = 0.08 (70.8 examples/sec; 0.057 sec/batch; 1h:48m:43s remains)
INFO - root - 2019-11-06 20:43:04.010135: step 34550, total loss = 0.26, predict loss = 0.08 (77.6 examples/sec; 0.052 sec/batch; 1h:39m:07s remains)
INFO - root - 2019-11-06 20:43:04.579980: step 34560, total loss = 0.26, predict loss = 0.07 (82.1 examples/sec; 0.049 sec/batch; 1h:33m:44s remains)
INFO - root - 2019-11-06 20:43:05.169062: step 34570, total loss = 0.22, predict loss = 0.06 (78.0 examples/sec; 0.051 sec/batch; 1h:38m:41s remains)
INFO - root - 2019-11-06 20:43:05.751414: step 34580, total loss = 0.24, predict loss = 0.06 (80.4 examples/sec; 0.050 sec/batch; 1h:35m:44s remains)
INFO - root - 2019-11-06 20:43:06.332590: step 34590, total loss = 0.23, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:37m:28s remains)
INFO - root - 2019-11-06 20:43:06.912648: step 34600, total loss = 0.31, predict loss = 0.08 (77.9 examples/sec; 0.051 sec/batch; 1h:38m:46s remains)
INFO - root - 2019-11-06 20:43:07.515085: step 34610, total loss = 0.30, predict loss = 0.08 (75.3 examples/sec; 0.053 sec/batch; 1h:42m:13s remains)
INFO - root - 2019-11-06 20:43:08.090909: step 34620, total loss = 0.41, predict loss = 0.13 (80.3 examples/sec; 0.050 sec/batch; 1h:35m:44s remains)
INFO - root - 2019-11-06 20:43:08.668289: step 34630, total loss = 0.37, predict loss = 0.09 (77.3 examples/sec; 0.052 sec/batch; 1h:39m:27s remains)
INFO - root - 2019-11-06 20:43:09.245521: step 34640, total loss = 0.22, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:39m:35s remains)
INFO - root - 2019-11-06 20:43:09.804229: step 34650, total loss = 0.32, predict loss = 0.08 (94.3 examples/sec; 0.042 sec/batch; 1h:21m:31s remains)
INFO - root - 2019-11-06 20:43:10.261563: step 34660, total loss = 0.19, predict loss = 0.05 (96.7 examples/sec; 0.041 sec/batch; 1h:19m:30s remains)
INFO - root - 2019-11-06 20:43:10.722558: step 34670, total loss = 0.25, predict loss = 0.07 (92.4 examples/sec; 0.043 sec/batch; 1h:23m:13s remains)
INFO - root - 2019-11-06 20:43:11.721427: step 34680, total loss = 0.23, predict loss = 0.06 (55.2 examples/sec; 0.073 sec/batch; 2h:19m:23s remains)
INFO - root - 2019-11-06 20:43:12.431113: step 34690, total loss = 0.31, predict loss = 0.09 (64.6 examples/sec; 0.062 sec/batch; 1h:59m:05s remains)
INFO - root - 2019-11-06 20:43:13.026349: step 34700, total loss = 0.28, predict loss = 0.07 (78.7 examples/sec; 0.051 sec/batch; 1h:37m:41s remains)
INFO - root - 2019-11-06 20:43:13.607170: step 34710, total loss = 0.28, predict loss = 0.07 (78.3 examples/sec; 0.051 sec/batch; 1h:38m:11s remains)
INFO - root - 2019-11-06 20:43:14.188304: step 34720, total loss = 0.22, predict loss = 0.06 (80.4 examples/sec; 0.050 sec/batch; 1h:35m:32s remains)
INFO - root - 2019-11-06 20:43:14.782404: step 34730, total loss = 0.15, predict loss = 0.04 (75.5 examples/sec; 0.053 sec/batch; 1h:41m:50s remains)
INFO - root - 2019-11-06 20:43:15.351610: step 34740, total loss = 0.25, predict loss = 0.06 (79.0 examples/sec; 0.051 sec/batch; 1h:37m:18s remains)
INFO - root - 2019-11-06 20:43:15.933524: step 34750, total loss = 0.38, predict loss = 0.11 (74.2 examples/sec; 0.054 sec/batch; 1h:43m:33s remains)
INFO - root - 2019-11-06 20:43:16.539384: step 34760, total loss = 0.24, predict loss = 0.06 (82.0 examples/sec; 0.049 sec/batch; 1h:33m:44s remains)
INFO - root - 2019-11-06 20:43:17.148357: step 34770, total loss = 0.30, predict loss = 0.08 (75.0 examples/sec; 0.053 sec/batch; 1h:42m:25s remains)
INFO - root - 2019-11-06 20:43:17.713654: step 34780, total loss = 0.18, predict loss = 0.05 (82.4 examples/sec; 0.049 sec/batch; 1h:33m:13s remains)
INFO - root - 2019-11-06 20:43:18.289163: step 34790, total loss = 0.35, predict loss = 0.10 (76.1 examples/sec; 0.053 sec/batch; 1h:40m:58s remains)
INFO - root - 2019-11-06 20:43:18.806145: step 34800, total loss = 0.24, predict loss = 0.06 (104.0 examples/sec; 0.038 sec/batch; 1h:13m:50s remains)
INFO - root - 2019-11-06 20:43:19.288781: step 34810, total loss = 0.36, predict loss = 0.10 (94.8 examples/sec; 0.042 sec/batch; 1h:20m:59s remains)
INFO - root - 2019-11-06 20:43:19.744612: step 34820, total loss = 0.19, predict loss = 0.05 (98.6 examples/sec; 0.041 sec/batch; 1h:17m:51s remains)
INFO - root - 2019-11-06 20:43:20.853249: step 34830, total loss = 0.29, predict loss = 0.07 (54.4 examples/sec; 0.074 sec/batch; 2h:21m:15s remains)
INFO - root - 2019-11-06 20:43:21.489602: step 34840, total loss = 0.27, predict loss = 0.08 (75.0 examples/sec; 0.053 sec/batch; 1h:42m:23s remains)
INFO - root - 2019-11-06 20:43:22.076132: step 34850, total loss = 0.18, predict loss = 0.04 (81.7 examples/sec; 0.049 sec/batch; 1h:33m:56s remains)
INFO - root - 2019-11-06 20:43:22.658875: step 34860, total loss = 0.26, predict loss = 0.07 (74.7 examples/sec; 0.054 sec/batch; 1h:42m:43s remains)
INFO - root - 2019-11-06 20:43:23.326927: step 34870, total loss = 0.20, predict loss = 0.05 (71.3 examples/sec; 0.056 sec/batch; 1h:47m:39s remains)
INFO - root - 2019-11-06 20:43:23.962679: step 34880, total loss = 0.24, predict loss = 0.07 (63.7 examples/sec; 0.063 sec/batch; 2h:00m:32s remains)
INFO - root - 2019-11-06 20:43:24.636383: step 34890, total loss = 0.23, predict loss = 0.06 (62.1 examples/sec; 0.064 sec/batch; 2h:03m:29s remains)
INFO - root - 2019-11-06 20:43:25.289854: step 34900, total loss = 0.24, predict loss = 0.06 (75.1 examples/sec; 0.053 sec/batch; 1h:42m:10s remains)
INFO - root - 2019-11-06 20:43:25.879336: step 34910, total loss = 0.25, predict loss = 0.07 (79.9 examples/sec; 0.050 sec/batch; 1h:36m:02s remains)
INFO - root - 2019-11-06 20:43:26.449621: step 34920, total loss = 0.21, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:37m:00s remains)
INFO - root - 2019-11-06 20:43:27.059637: step 34930, total loss = 0.34, predict loss = 0.10 (74.2 examples/sec; 0.054 sec/batch; 1h:43m:24s remains)
INFO - root - 2019-11-06 20:43:27.667911: step 34940, total loss = 0.22, predict loss = 0.06 (82.8 examples/sec; 0.048 sec/batch; 1h:32m:37s remains)
INFO - root - 2019-11-06 20:43:28.175773: step 34950, total loss = 0.36, predict loss = 0.10 (92.5 examples/sec; 0.043 sec/batch; 1h:22m:57s remains)
INFO - root - 2019-11-06 20:43:28.656245: step 34960, total loss = 0.26, predict loss = 0.07 (97.9 examples/sec; 0.041 sec/batch; 1h:18m:22s remains)
INFO - root - 2019-11-06 20:43:29.139204: step 34970, total loss = 0.24, predict loss = 0.06 (85.8 examples/sec; 0.047 sec/batch; 1h:29m:24s remains)
INFO - root - 2019-11-06 20:43:30.298502: step 34980, total loss = 0.26, predict loss = 0.06 (65.4 examples/sec; 0.061 sec/batch; 1h:57m:15s remains)
INFO - root - 2019-11-06 20:43:31.054583: step 34990, total loss = 0.40, predict loss = 0.11 (62.3 examples/sec; 0.064 sec/batch; 2h:02m:59s remains)
INFO - root - 2019-11-06 20:43:31.759111: step 35000, total loss = 0.30, predict loss = 0.07 (71.2 examples/sec; 0.056 sec/batch; 1h:47m:40s remains)
INFO - root - 2019-11-06 20:43:32.439419: step 35010, total loss = 0.19, predict loss = 0.05 (71.1 examples/sec; 0.056 sec/batch; 1h:47m:53s remains)
INFO - root - 2019-11-06 20:43:33.155997: step 35020, total loss = 0.27, predict loss = 0.06 (59.6 examples/sec; 0.067 sec/batch; 2h:08m:41s remains)
INFO - root - 2019-11-06 20:43:33.879582: step 35030, total loss = 0.35, predict loss = 0.09 (66.0 examples/sec; 0.061 sec/batch; 1h:56m:11s remains)
INFO - root - 2019-11-06 20:43:34.552683: step 35040, total loss = 0.21, predict loss = 0.06 (62.3 examples/sec; 0.064 sec/batch; 2h:03m:00s remains)
INFO - root - 2019-11-06 20:43:35.260700: step 35050, total loss = 0.34, predict loss = 0.10 (75.3 examples/sec; 0.053 sec/batch; 1h:41m:42s remains)
INFO - root - 2019-11-06 20:43:35.860164: step 35060, total loss = 0.20, predict loss = 0.04 (67.6 examples/sec; 0.059 sec/batch; 1h:53m:19s remains)
INFO - root - 2019-11-06 20:43:36.484361: step 35070, total loss = 0.31, predict loss = 0.08 (63.4 examples/sec; 0.063 sec/batch; 2h:00m:53s remains)
INFO - root - 2019-11-06 20:43:37.193369: step 35080, total loss = 0.25, predict loss = 0.07 (61.5 examples/sec; 0.065 sec/batch; 2h:04m:28s remains)
INFO - root - 2019-11-06 20:43:37.935452: step 35090, total loss = 0.22, predict loss = 0.06 (63.9 examples/sec; 0.063 sec/batch; 1h:59m:55s remains)
INFO - root - 2019-11-06 20:43:38.484053: step 35100, total loss = 0.40, predict loss = 0.10 (88.7 examples/sec; 0.045 sec/batch; 1h:26m:19s remains)
INFO - root - 2019-11-06 20:43:38.963449: step 35110, total loss = 0.22, predict loss = 0.06 (88.1 examples/sec; 0.045 sec/batch; 1h:26m:57s remains)
INFO - root - 2019-11-06 20:43:39.902218: step 35120, total loss = 0.38, predict loss = 0.12 (82.4 examples/sec; 0.049 sec/batch; 1h:32m:54s remains)
INFO - root - 2019-11-06 20:43:40.555839: step 35130, total loss = 0.18, predict loss = 0.04 (75.4 examples/sec; 0.053 sec/batch; 1h:41m:33s remains)
INFO - root - 2019-11-06 20:43:41.137147: step 35140, total loss = 0.16, predict loss = 0.04 (80.4 examples/sec; 0.050 sec/batch; 1h:35m:13s remains)
INFO - root - 2019-11-06 20:43:41.720699: step 35150, total loss = 0.20, predict loss = 0.05 (75.9 examples/sec; 0.053 sec/batch; 1h:40m:55s remains)
INFO - root - 2019-11-06 20:43:42.289475: step 35160, total loss = 0.24, predict loss = 0.06 (77.8 examples/sec; 0.051 sec/batch; 1h:38m:24s remains)
INFO - root - 2019-11-06 20:43:42.906866: step 35170, total loss = 0.21, predict loss = 0.05 (80.5 examples/sec; 0.050 sec/batch; 1h:35m:05s remains)
INFO - root - 2019-11-06 20:43:43.490885: step 35180, total loss = 0.28, predict loss = 0.08 (72.7 examples/sec; 0.055 sec/batch; 1h:45m:16s remains)
INFO - root - 2019-11-06 20:43:44.093987: step 35190, total loss = 0.15, predict loss = 0.04 (76.7 examples/sec; 0.052 sec/batch; 1h:39m:45s remains)
INFO - root - 2019-11-06 20:43:44.736076: step 35200, total loss = 0.26, predict loss = 0.06 (68.5 examples/sec; 0.058 sec/batch; 1h:51m:43s remains)
INFO - root - 2019-11-06 20:43:45.460697: step 35210, total loss = 0.27, predict loss = 0.09 (62.4 examples/sec; 0.064 sec/batch; 2h:02m:32s remains)
INFO - root - 2019-11-06 20:43:46.092148: step 35220, total loss = 0.31, predict loss = 0.08 (68.9 examples/sec; 0.058 sec/batch; 1h:51m:01s remains)
INFO - root - 2019-11-06 20:43:46.941425: step 35230, total loss = 0.22, predict loss = 0.06 (51.1 examples/sec; 0.078 sec/batch; 2h:29m:35s remains)
INFO - root - 2019-11-06 20:43:47.582472: step 35240, total loss = 0.24, predict loss = 0.06 (82.7 examples/sec; 0.048 sec/batch; 1h:32m:31s remains)
INFO - root - 2019-11-06 20:43:48.090659: step 35250, total loss = 0.23, predict loss = 0.06 (105.8 examples/sec; 0.038 sec/batch; 1h:12m:18s remains)
INFO - root - 2019-11-06 20:43:48.534053: step 35260, total loss = 0.29, predict loss = 0.08 (99.0 examples/sec; 0.040 sec/batch; 1h:17m:17s remains)
INFO - root - 2019-11-06 20:43:49.467025: step 35270, total loss = 0.23, predict loss = 0.06 (73.8 examples/sec; 0.054 sec/batch; 1h:43m:34s remains)
INFO - root - 2019-11-06 20:43:50.147698: step 35280, total loss = 0.21, predict loss = 0.05 (62.0 examples/sec; 0.065 sec/batch; 2h:03m:25s remains)
INFO - root - 2019-11-06 20:43:50.860958: step 35290, total loss = 0.43, predict loss = 0.14 (72.6 examples/sec; 0.055 sec/batch; 1h:45m:17s remains)
INFO - root - 2019-11-06 20:43:51.480186: step 35300, total loss = 0.26, predict loss = 0.06 (79.7 examples/sec; 0.050 sec/batch; 1h:35m:58s remains)
INFO - root - 2019-11-06 20:43:52.074828: step 35310, total loss = 0.18, predict loss = 0.04 (74.6 examples/sec; 0.054 sec/batch; 1h:42m:30s remains)
INFO - root - 2019-11-06 20:43:52.653795: step 35320, total loss = 0.22, predict loss = 0.06 (77.9 examples/sec; 0.051 sec/batch; 1h:38m:05s remains)
INFO - root - 2019-11-06 20:43:53.243569: step 35330, total loss = 0.19, predict loss = 0.05 (80.7 examples/sec; 0.050 sec/batch; 1h:34m:42s remains)
INFO - root - 2019-11-06 20:43:53.825250: step 35340, total loss = 0.43, predict loss = 0.13 (81.4 examples/sec; 0.049 sec/batch; 1h:33m:57s remains)
INFO - root - 2019-11-06 20:43:54.399847: step 35350, total loss = 0.23, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:38m:02s remains)
INFO - root - 2019-11-06 20:43:54.994158: step 35360, total loss = 0.17, predict loss = 0.04 (79.7 examples/sec; 0.050 sec/batch; 1h:35m:52s remains)
INFO - root - 2019-11-06 20:43:55.594515: step 35370, total loss = 0.34, predict loss = 0.08 (81.9 examples/sec; 0.049 sec/batch; 1h:33m:20s remains)
INFO - root - 2019-11-06 20:43:56.211543: step 35380, total loss = 0.24, predict loss = 0.06 (82.5 examples/sec; 0.049 sec/batch; 1h:32m:39s remains)
INFO - root - 2019-11-06 20:43:56.767048: step 35390, total loss = 0.21, predict loss = 0.06 (90.8 examples/sec; 0.044 sec/batch; 1h:24m:06s remains)
INFO - root - 2019-11-06 20:43:57.298473: step 35400, total loss = 0.23, predict loss = 0.06 (86.9 examples/sec; 0.046 sec/batch; 1h:27m:52s remains)
INFO - root - 2019-11-06 20:43:57.814411: step 35410, total loss = 0.23, predict loss = 0.06 (83.2 examples/sec; 0.048 sec/batch; 1h:31m:46s remains)
INFO - root - 2019-11-06 20:43:58.898029: step 35420, total loss = 0.25, predict loss = 0.06 (72.8 examples/sec; 0.055 sec/batch; 1h:44m:57s remains)
INFO - root - 2019-11-06 20:43:59.609677: step 35430, total loss = 0.24, predict loss = 0.06 (60.5 examples/sec; 0.066 sec/batch; 2h:06m:20s remains)
INFO - root - 2019-11-06 20:44:00.356341: step 35440, total loss = 0.17, predict loss = 0.04 (58.8 examples/sec; 0.068 sec/batch; 2h:09m:54s remains)
INFO - root - 2019-11-06 20:44:01.112664: step 35450, total loss = 0.20, predict loss = 0.05 (67.9 examples/sec; 0.059 sec/batch; 1h:52m:32s remains)
INFO - root - 2019-11-06 20:44:01.695643: step 35460, total loss = 0.32, predict loss = 0.09 (76.1 examples/sec; 0.053 sec/batch; 1h:40m:21s remains)
INFO - root - 2019-11-06 20:44:02.259931: step 35470, total loss = 0.31, predict loss = 0.07 (81.4 examples/sec; 0.049 sec/batch; 1h:33m:50s remains)
INFO - root - 2019-11-06 20:44:02.827999: step 35480, total loss = 0.32, predict loss = 0.08 (79.3 examples/sec; 0.050 sec/batch; 1h:36m:13s remains)
INFO - root - 2019-11-06 20:44:03.484636: step 35490, total loss = 0.30, predict loss = 0.08 (77.5 examples/sec; 0.052 sec/batch; 1h:38m:26s remains)
INFO - root - 2019-11-06 20:44:04.058012: step 35500, total loss = 0.28, predict loss = 0.06 (80.2 examples/sec; 0.050 sec/batch; 1h:35m:09s remains)
INFO - root - 2019-11-06 20:44:04.637670: step 35510, total loss = 0.24, predict loss = 0.06 (80.6 examples/sec; 0.050 sec/batch; 1h:34m:42s remains)
INFO - root - 2019-11-06 20:44:05.252013: step 35520, total loss = 0.29, predict loss = 0.08 (77.2 examples/sec; 0.052 sec/batch; 1h:38m:49s remains)
INFO - root - 2019-11-06 20:44:05.858031: step 35530, total loss = 0.37, predict loss = 0.10 (76.7 examples/sec; 0.052 sec/batch; 1h:39m:26s remains)
INFO - root - 2019-11-06 20:44:06.376124: step 35540, total loss = 0.17, predict loss = 0.04 (95.9 examples/sec; 0.042 sec/batch; 1h:19m:32s remains)
INFO - root - 2019-11-06 20:44:06.821889: step 35550, total loss = 0.22, predict loss = 0.06 (96.5 examples/sec; 0.041 sec/batch; 1h:19m:01s remains)
INFO - root - 2019-11-06 20:44:07.293945: step 35560, total loss = 0.24, predict loss = 0.06 (81.7 examples/sec; 0.049 sec/batch; 1h:33m:22s remains)
INFO - root - 2019-11-06 20:44:08.586352: step 35570, total loss = 0.26, predict loss = 0.07 (55.7 examples/sec; 0.072 sec/batch; 2h:16m:51s remains)
INFO - root - 2019-11-06 20:44:09.207362: step 35580, total loss = 0.26, predict loss = 0.07 (76.8 examples/sec; 0.052 sec/batch; 1h:39m:23s remains)
INFO - root - 2019-11-06 20:44:09.779601: step 35590, total loss = 0.30, predict loss = 0.07 (83.3 examples/sec; 0.048 sec/batch; 1h:31m:35s remains)
INFO - root - 2019-11-06 20:44:10.371659: step 35600, total loss = 0.21, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:36m:27s remains)
INFO - root - 2019-11-06 20:44:10.958060: step 35610, total loss = 0.35, predict loss = 0.10 (77.1 examples/sec; 0.052 sec/batch; 1h:38m:55s remains)
INFO - root - 2019-11-06 20:44:11.531970: step 35620, total loss = 0.35, predict loss = 0.09 (77.2 examples/sec; 0.052 sec/batch; 1h:38m:42s remains)
INFO - root - 2019-11-06 20:44:12.117244: step 35630, total loss = 0.26, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:35m:34s remains)
INFO - root - 2019-11-06 20:44:12.691872: step 35640, total loss = 0.20, predict loss = 0.04 (76.9 examples/sec; 0.052 sec/batch; 1h:39m:11s remains)
INFO - root - 2019-11-06 20:44:13.278886: step 35650, total loss = 0.19, predict loss = 0.04 (80.8 examples/sec; 0.049 sec/batch; 1h:34m:17s remains)
INFO - root - 2019-11-06 20:44:13.841648: step 35660, total loss = 0.25, predict loss = 0.06 (82.6 examples/sec; 0.048 sec/batch; 1h:32m:14s remains)
INFO - root - 2019-11-06 20:44:14.413318: step 35670, total loss = 0.23, predict loss = 0.05 (83.2 examples/sec; 0.048 sec/batch; 1h:31m:33s remains)
INFO - root - 2019-11-06 20:44:14.988233: step 35680, total loss = 0.21, predict loss = 0.06 (78.1 examples/sec; 0.051 sec/batch; 1h:37m:36s remains)
INFO - root - 2019-11-06 20:44:15.507554: step 35690, total loss = 0.42, predict loss = 0.12 (99.9 examples/sec; 0.040 sec/batch; 1h:16m:18s remains)
INFO - root - 2019-11-06 20:44:15.982538: step 35700, total loss = 0.29, predict loss = 0.08 (103.1 examples/sec; 0.039 sec/batch; 1h:13m:56s remains)
INFO - root - 2019-11-06 20:44:16.491550: step 35710, total loss = 0.22, predict loss = 0.05 (85.9 examples/sec; 0.047 sec/batch; 1h:28m:40s remains)
INFO - root - 2019-11-06 20:44:17.671426: step 35720, total loss = 0.23, predict loss = 0.06 (70.3 examples/sec; 0.057 sec/batch; 1h:48m:24s remains)
INFO - root - 2019-11-06 20:44:18.324180: step 35730, total loss = 0.21, predict loss = 0.04 (63.5 examples/sec; 0.063 sec/batch; 1h:59m:54s remains)
INFO - root - 2019-11-06 20:44:19.007529: step 35740, total loss = 0.32, predict loss = 0.09 (74.0 examples/sec; 0.054 sec/batch; 1h:42m:53s remains)
INFO - root - 2019-11-06 20:44:19.657306: step 35750, total loss = 0.30, predict loss = 0.07 (62.6 examples/sec; 0.064 sec/batch; 2h:01m:41s remains)
INFO - root - 2019-11-06 20:44:20.294712: step 35760, total loss = 0.18, predict loss = 0.04 (69.2 examples/sec; 0.058 sec/batch; 1h:50m:00s remains)
INFO - root - 2019-11-06 20:44:20.919421: step 35770, total loss = 0.19, predict loss = 0.05 (77.0 examples/sec; 0.052 sec/batch; 1h:38m:50s remains)
INFO - root - 2019-11-06 20:44:21.510880: step 35780, total loss = 0.33, predict loss = 0.08 (70.9 examples/sec; 0.056 sec/batch; 1h:47m:23s remains)
INFO - root - 2019-11-06 20:44:22.173736: step 35790, total loss = 0.20, predict loss = 0.04 (77.7 examples/sec; 0.051 sec/batch; 1h:37m:55s remains)
INFO - root - 2019-11-06 20:44:22.760121: step 35800, total loss = 0.18, predict loss = 0.04 (72.0 examples/sec; 0.056 sec/batch; 1h:45m:43s remains)
INFO - root - 2019-11-06 20:44:23.353448: step 35810, total loss = 0.18, predict loss = 0.05 (77.3 examples/sec; 0.052 sec/batch; 1h:38m:28s remains)
INFO - root - 2019-11-06 20:44:23.942272: step 35820, total loss = 0.29, predict loss = 0.07 (80.9 examples/sec; 0.049 sec/batch; 1h:34m:07s remains)
INFO - root - 2019-11-06 20:44:24.522279: step 35830, total loss = 0.26, predict loss = 0.07 (76.3 examples/sec; 0.052 sec/batch; 1h:39m:41s remains)
INFO - root - 2019-11-06 20:44:25.004570: step 35840, total loss = 0.27, predict loss = 0.07 (100.5 examples/sec; 0.040 sec/batch; 1h:15m:42s remains)
INFO - root - 2019-11-06 20:44:25.499318: step 35850, total loss = 0.40, predict loss = 0.12 (102.9 examples/sec; 0.039 sec/batch; 1h:13m:56s remains)
INFO - root - 2019-11-06 20:44:26.401270: step 35860, total loss = 0.22, predict loss = 0.05 (8.2 examples/sec; 0.486 sec/batch; 15h:23m:53s remains)
INFO - root - 2019-11-06 20:44:27.068796: step 35870, total loss = 0.21, predict loss = 0.06 (67.2 examples/sec; 0.060 sec/batch; 1h:53m:15s remains)
INFO - root - 2019-11-06 20:44:27.707948: step 35880, total loss = 0.29, predict loss = 0.07 (71.1 examples/sec; 0.056 sec/batch; 1h:47m:00s remains)
INFO - root - 2019-11-06 20:44:28.325638: step 35890, total loss = 0.21, predict loss = 0.05 (81.3 examples/sec; 0.049 sec/batch; 1h:33m:34s remains)
INFO - root - 2019-11-06 20:44:28.894965: step 35900, total loss = 0.18, predict loss = 0.04 (77.3 examples/sec; 0.052 sec/batch; 1h:38m:25s remains)
INFO - root - 2019-11-06 20:44:29.460855: step 35910, total loss = 0.47, predict loss = 0.13 (81.7 examples/sec; 0.049 sec/batch; 1h:33m:04s remains)
INFO - root - 2019-11-06 20:44:30.044627: step 35920, total loss = 0.31, predict loss = 0.09 (77.8 examples/sec; 0.051 sec/batch; 1h:37m:46s remains)
INFO - root - 2019-11-06 20:44:30.683176: step 35930, total loss = 0.21, predict loss = 0.05 (74.3 examples/sec; 0.054 sec/batch; 1h:42m:24s remains)
INFO - root - 2019-11-06 20:44:31.273856: step 35940, total loss = 0.27, predict loss = 0.08 (76.3 examples/sec; 0.052 sec/batch; 1h:39m:42s remains)
INFO - root - 2019-11-06 20:44:31.837788: step 35950, total loss = 0.22, predict loss = 0.06 (77.9 examples/sec; 0.051 sec/batch; 1h:37m:37s remains)
INFO - root - 2019-11-06 20:44:32.419388: step 35960, total loss = 0.28, predict loss = 0.08 (79.2 examples/sec; 0.051 sec/batch; 1h:36m:01s remains)
INFO - root - 2019-11-06 20:44:33.015898: step 35970, total loss = 0.18, predict loss = 0.04 (76.6 examples/sec; 0.052 sec/batch; 1h:39m:14s remains)
INFO - root - 2019-11-06 20:44:33.577113: step 35980, total loss = 0.45, predict loss = 0.10 (86.5 examples/sec; 0.046 sec/batch; 1h:27m:51s remains)
INFO - root - 2019-11-06 20:44:34.048049: step 35990, total loss = 0.27, predict loss = 0.07 (92.5 examples/sec; 0.043 sec/batch; 1h:22m:07s remains)
INFO - root - 2019-11-06 20:44:34.508525: step 36000, total loss = 0.29, predict loss = 0.08 (91.5 examples/sec; 0.044 sec/batch; 1h:23m:05s remains)
INFO - root - 2019-11-06 20:44:35.467414: step 36010, total loss = 0.30, predict loss = 0.08 (76.1 examples/sec; 0.053 sec/batch; 1h:39m:52s remains)
INFO - root - 2019-11-06 20:44:36.128473: step 36020, total loss = 0.20, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:36m:06s remains)
INFO - root - 2019-11-06 20:44:36.715101: step 36030, total loss = 0.21, predict loss = 0.05 (76.0 examples/sec; 0.053 sec/batch; 1h:40m:02s remains)
INFO - root - 2019-11-06 20:44:37.280099: step 36040, total loss = 0.31, predict loss = 0.09 (79.0 examples/sec; 0.051 sec/batch; 1h:36m:13s remains)
INFO - root - 2019-11-06 20:44:37.867695: step 36050, total loss = 0.15, predict loss = 0.04 (78.8 examples/sec; 0.051 sec/batch; 1h:36m:26s remains)
INFO - root - 2019-11-06 20:44:38.427072: step 36060, total loss = 0.32, predict loss = 0.08 (80.2 examples/sec; 0.050 sec/batch; 1h:34m:44s remains)
INFO - root - 2019-11-06 20:44:39.001470: step 36070, total loss = 0.20, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:35m:45s remains)
INFO - root - 2019-11-06 20:44:39.586787: step 36080, total loss = 0.26, predict loss = 0.07 (77.3 examples/sec; 0.052 sec/batch; 1h:38m:11s remains)
INFO - root - 2019-11-06 20:44:40.181017: step 36090, total loss = 0.26, predict loss = 0.07 (78.9 examples/sec; 0.051 sec/batch; 1h:36m:18s remains)
INFO - root - 2019-11-06 20:44:40.765082: step 36100, total loss = 0.19, predict loss = 0.05 (76.9 examples/sec; 0.052 sec/batch; 1h:38m:42s remains)
INFO - root - 2019-11-06 20:44:41.379911: step 36110, total loss = 0.18, predict loss = 0.04 (78.6 examples/sec; 0.051 sec/batch; 1h:36m:35s remains)
INFO - root - 2019-11-06 20:44:41.997350: step 36120, total loss = 0.15, predict loss = 0.04 (71.8 examples/sec; 0.056 sec/batch; 1h:45m:45s remains)
INFO - root - 2019-11-06 20:44:42.669749: step 36130, total loss = 0.23, predict loss = 0.05 (87.2 examples/sec; 0.046 sec/batch; 1h:27m:06s remains)
INFO - root - 2019-11-06 20:44:43.143295: step 36140, total loss = 0.21, predict loss = 0.05 (97.4 examples/sec; 0.041 sec/batch; 1h:17m:53s remains)
INFO - root - 2019-11-06 20:44:43.584744: step 36150, total loss = 0.34, predict loss = 0.12 (99.8 examples/sec; 0.040 sec/batch; 1h:16m:05s remains)
INFO - root - 2019-11-06 20:44:44.531720: step 36160, total loss = 0.32, predict loss = 0.09 (76.5 examples/sec; 0.052 sec/batch; 1h:39m:13s remains)
INFO - root - 2019-11-06 20:44:45.269187: step 36170, total loss = 0.21, predict loss = 0.05 (59.9 examples/sec; 0.067 sec/batch; 2h:06m:44s remains)
INFO - root - 2019-11-06 20:44:45.872140: step 36180, total loss = 0.23, predict loss = 0.06 (77.8 examples/sec; 0.051 sec/batch; 1h:37m:31s remains)
INFO - root - 2019-11-06 20:44:46.460858: step 36190, total loss = 0.24, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:36m:07s remains)
INFO - root - 2019-11-06 20:44:47.031055: step 36200, total loss = 0.25, predict loss = 0.06 (81.9 examples/sec; 0.049 sec/batch; 1h:32m:37s remains)
INFO - root - 2019-11-06 20:44:47.628278: step 36210, total loss = 0.18, predict loss = 0.05 (79.2 examples/sec; 0.051 sec/batch; 1h:35m:49s remains)
INFO - root - 2019-11-06 20:44:48.208047: step 36220, total loss = 0.21, predict loss = 0.06 (76.6 examples/sec; 0.052 sec/batch; 1h:39m:03s remains)
INFO - root - 2019-11-06 20:44:48.783825: step 36230, total loss = 0.25, predict loss = 0.07 (79.2 examples/sec; 0.051 sec/batch; 1h:35m:45s remains)
INFO - root - 2019-11-06 20:44:49.374665: step 36240, total loss = 0.23, predict loss = 0.05 (79.9 examples/sec; 0.050 sec/batch; 1h:34m:55s remains)
INFO - root - 2019-11-06 20:44:49.972182: step 36250, total loss = 0.19, predict loss = 0.05 (73.2 examples/sec; 0.055 sec/batch; 1h:43m:36s remains)
INFO - root - 2019-11-06 20:44:50.551782: step 36260, total loss = 0.21, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:37m:10s remains)
INFO - root - 2019-11-06 20:44:51.121382: step 36270, total loss = 0.23, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:35m:02s remains)
INFO - root - 2019-11-06 20:44:51.639854: step 36280, total loss = 0.30, predict loss = 0.07 (98.0 examples/sec; 0.041 sec/batch; 1h:17m:19s remains)
INFO - root - 2019-11-06 20:44:52.100769: step 36290, total loss = 0.22, predict loss = 0.07 (103.6 examples/sec; 0.039 sec/batch; 1h:13m:08s remains)
INFO - root - 2019-11-06 20:44:52.561652: step 36300, total loss = 0.21, predict loss = 0.06 (96.9 examples/sec; 0.041 sec/batch; 1h:18m:12s remains)
INFO - root - 2019-11-06 20:44:53.544660: step 36310, total loss = 0.31, predict loss = 0.09 (64.1 examples/sec; 0.062 sec/batch; 1h:58m:16s remains)
INFO - root - 2019-11-06 20:44:54.170197: step 36320, total loss = 0.18, predict loss = 0.04 (76.8 examples/sec; 0.052 sec/batch; 1h:38m:41s remains)
INFO - root - 2019-11-06 20:44:54.772902: step 36330, total loss = 0.24, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:37m:56s remains)
INFO - root - 2019-11-06 20:44:55.345397: step 36340, total loss = 0.25, predict loss = 0.06 (81.6 examples/sec; 0.049 sec/batch; 1h:32m:52s remains)
INFO - root - 2019-11-06 20:44:55.924364: step 36350, total loss = 0.28, predict loss = 0.07 (78.7 examples/sec; 0.051 sec/batch; 1h:36m:18s remains)
INFO - root - 2019-11-06 20:44:56.507766: step 36360, total loss = 0.41, predict loss = 0.12 (79.4 examples/sec; 0.050 sec/batch; 1h:35m:25s remains)
INFO - root - 2019-11-06 20:44:57.106547: step 36370, total loss = 0.21, predict loss = 0.05 (79.2 examples/sec; 0.051 sec/batch; 1h:35m:38s remains)
INFO - root - 2019-11-06 20:44:57.682235: step 36380, total loss = 0.19, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:38m:07s remains)
INFO - root - 2019-11-06 20:44:58.269169: step 36390, total loss = 0.24, predict loss = 0.06 (79.5 examples/sec; 0.050 sec/batch; 1h:35m:12s remains)
INFO - root - 2019-11-06 20:44:58.849615: step 36400, total loss = 0.16, predict loss = 0.04 (74.3 examples/sec; 0.054 sec/batch; 1h:41m:53s remains)
INFO - root - 2019-11-06 20:44:59.451151: step 36410, total loss = 0.27, predict loss = 0.06 (81.2 examples/sec; 0.049 sec/batch; 1h:33m:12s remains)
INFO - root - 2019-11-06 20:45:00.030757: step 36420, total loss = 0.20, predict loss = 0.05 (77.3 examples/sec; 0.052 sec/batch; 1h:37m:54s remains)
INFO - root - 2019-11-06 20:45:00.577573: step 36430, total loss = 0.28, predict loss = 0.07 (98.9 examples/sec; 0.040 sec/batch; 1h:16m:32s remains)
INFO - root - 2019-11-06 20:45:01.033163: step 36440, total loss = 0.26, predict loss = 0.09 (96.8 examples/sec; 0.041 sec/batch; 1h:18m:11s remains)
INFO - root - 2019-11-06 20:45:01.499349: step 36450, total loss = 0.29, predict loss = 0.08 (101.7 examples/sec; 0.039 sec/batch; 1h:14m:25s remains)
INFO - root - 2019-11-06 20:45:02.508778: step 36460, total loss = 0.40, predict loss = 0.12 (75.6 examples/sec; 0.053 sec/batch; 1h:40m:08s remains)
INFO - root - 2019-11-06 20:45:03.095426: step 36470, total loss = 0.26, predict loss = 0.06 (74.9 examples/sec; 0.053 sec/batch; 1h:40m:59s remains)
INFO - root - 2019-11-06 20:45:03.677703: step 36480, total loss = 0.24, predict loss = 0.06 (78.5 examples/sec; 0.051 sec/batch; 1h:36m:23s remains)
INFO - root - 2019-11-06 20:45:04.280700: step 36490, total loss = 0.19, predict loss = 0.05 (79.8 examples/sec; 0.050 sec/batch; 1h:34m:50s remains)
INFO - root - 2019-11-06 20:45:04.874025: step 36500, total loss = 0.20, predict loss = 0.06 (74.6 examples/sec; 0.054 sec/batch; 1h:41m:29s remains)
INFO - root - 2019-11-06 20:45:05.445330: step 36510, total loss = 0.34, predict loss = 0.09 (83.3 examples/sec; 0.048 sec/batch; 1h:30m:46s remains)
INFO - root - 2019-11-06 20:45:06.072413: step 36520, total loss = 0.30, predict loss = 0.09 (82.4 examples/sec; 0.049 sec/batch; 1h:31m:50s remains)
INFO - root - 2019-11-06 20:45:06.712221: step 36530, total loss = 0.21, predict loss = 0.05 (64.0 examples/sec; 0.063 sec/batch; 1h:58m:14s remains)
INFO - root - 2019-11-06 20:45:07.306875: step 36540, total loss = 0.18, predict loss = 0.04 (79.4 examples/sec; 0.050 sec/batch; 1h:35m:13s remains)
INFO - root - 2019-11-06 20:45:07.866108: step 36550, total loss = 0.27, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 1h:36m:35s remains)
INFO - root - 2019-11-06 20:45:08.445494: step 36560, total loss = 0.26, predict loss = 0.07 (82.4 examples/sec; 0.049 sec/batch; 1h:31m:44s remains)
INFO - root - 2019-11-06 20:45:09.040891: step 36570, total loss = 0.20, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:37m:55s remains)
INFO - root - 2019-11-06 20:45:09.536215: step 36580, total loss = 0.30, predict loss = 0.08 (100.1 examples/sec; 0.040 sec/batch; 1h:15m:33s remains)
INFO - root - 2019-11-06 20:45:09.988606: step 36590, total loss = 0.22, predict loss = 0.05 (94.0 examples/sec; 0.043 sec/batch; 1h:20m:27s remains)
INFO - root - 2019-11-06 20:45:10.453258: step 36600, total loss = 0.34, predict loss = 0.10 (91.7 examples/sec; 0.044 sec/batch; 1h:22m:24s remains)
INFO - root - 2019-11-06 20:45:11.584310: step 36610, total loss = 0.21, predict loss = 0.05 (61.4 examples/sec; 0.065 sec/batch; 2h:03m:11s remains)
INFO - root - 2019-11-06 20:45:12.212545: step 36620, total loss = 0.23, predict loss = 0.06 (80.5 examples/sec; 0.050 sec/batch; 1h:33m:53s remains)
INFO - root - 2019-11-06 20:45:12.792940: step 36630, total loss = 0.16, predict loss = 0.04 (78.4 examples/sec; 0.051 sec/batch; 1h:36m:25s remains)
INFO - root - 2019-11-06 20:45:13.353990: step 36640, total loss = 0.25, predict loss = 0.06 (80.3 examples/sec; 0.050 sec/batch; 1h:34m:09s remains)
INFO - root - 2019-11-06 20:45:13.942788: step 36650, total loss = 0.23, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:35m:33s remains)
INFO - root - 2019-11-06 20:45:14.507010: step 36660, total loss = 0.35, predict loss = 0.09 (79.7 examples/sec; 0.050 sec/batch; 1h:34m:45s remains)
INFO - root - 2019-11-06 20:45:15.083156: step 36670, total loss = 0.22, predict loss = 0.06 (75.5 examples/sec; 0.053 sec/batch; 1h:40m:07s remains)
INFO - root - 2019-11-06 20:45:15.652577: step 36680, total loss = 0.27, predict loss = 0.08 (80.7 examples/sec; 0.050 sec/batch; 1h:33m:34s remains)
INFO - root - 2019-11-06 20:45:16.290970: step 36690, total loss = 0.25, predict loss = 0.06 (76.0 examples/sec; 0.053 sec/batch; 1h:39m:20s remains)
INFO - root - 2019-11-06 20:45:16.874956: step 36700, total loss = 0.32, predict loss = 0.09 (76.9 examples/sec; 0.052 sec/batch; 1h:38m:09s remains)
INFO - root - 2019-11-06 20:45:17.486972: step 36710, total loss = 0.27, predict loss = 0.07 (62.0 examples/sec; 0.065 sec/batch; 2h:01m:54s remains)
INFO - root - 2019-11-06 20:45:18.077272: step 36720, total loss = 0.24, predict loss = 0.07 (78.4 examples/sec; 0.051 sec/batch; 1h:36m:19s remains)
INFO - root - 2019-11-06 20:45:18.555744: step 36730, total loss = 0.23, predict loss = 0.06 (99.2 examples/sec; 0.040 sec/batch; 1h:16m:07s remains)
INFO - root - 2019-11-06 20:45:19.002373: step 36740, total loss = 0.36, predict loss = 0.10 (94.6 examples/sec; 0.042 sec/batch; 1h:19m:47s remains)
INFO - root - 2019-11-06 20:45:19.938219: step 36750, total loss = 0.26, predict loss = 0.06 (81.9 examples/sec; 0.049 sec/batch; 1h:32m:12s remains)
INFO - root - 2019-11-06 20:45:20.596711: step 36760, total loss = 0.31, predict loss = 0.09 (61.2 examples/sec; 0.065 sec/batch; 2h:03m:15s remains)
INFO - root - 2019-11-06 20:45:21.219066: step 36770, total loss = 0.23, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:35m:48s remains)
INFO - root - 2019-11-06 20:45:21.798890: step 36780, total loss = 0.44, predict loss = 0.13 (79.8 examples/sec; 0.050 sec/batch; 1h:34m:33s remains)
INFO - root - 2019-11-06 20:45:22.375759: step 36790, total loss = 0.25, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:35m:26s remains)
INFO - root - 2019-11-06 20:45:22.949714: step 36800, total loss = 0.19, predict loss = 0.06 (79.2 examples/sec; 0.051 sec/batch; 1h:35m:18s remains)
INFO - root - 2019-11-06 20:45:23.556360: step 36810, total loss = 0.31, predict loss = 0.09 (80.1 examples/sec; 0.050 sec/batch; 1h:34m:14s remains)
INFO - root - 2019-11-06 20:45:24.140783: step 36820, total loss = 0.28, predict loss = 0.08 (75.2 examples/sec; 0.053 sec/batch; 1h:40m:21s remains)
INFO - root - 2019-11-06 20:45:24.722876: step 36830, total loss = 0.36, predict loss = 0.09 (79.3 examples/sec; 0.050 sec/batch; 1h:35m:04s remains)
INFO - root - 2019-11-06 20:45:25.297746: step 36840, total loss = 0.26, predict loss = 0.06 (82.4 examples/sec; 0.049 sec/batch; 1h:31m:31s remains)
INFO - root - 2019-11-06 20:45:25.882319: step 36850, total loss = 0.18, predict loss = 0.05 (77.7 examples/sec; 0.051 sec/batch; 1h:37m:04s remains)
INFO - root - 2019-11-06 20:45:26.457983: step 36860, total loss = 0.27, predict loss = 0.06 (72.9 examples/sec; 0.055 sec/batch; 1h:43m:29s remains)
INFO - root - 2019-11-06 20:45:27.008397: step 36870, total loss = 0.21, predict loss = 0.06 (91.9 examples/sec; 0.044 sec/batch; 1h:22m:05s remains)
INFO - root - 2019-11-06 20:45:27.474048: step 36880, total loss = 0.25, predict loss = 0.06 (97.9 examples/sec; 0.041 sec/batch; 1h:17m:03s remains)
INFO - root - 2019-11-06 20:45:27.951908: step 36890, total loss = 0.22, predict loss = 0.06 (99.4 examples/sec; 0.040 sec/batch; 1h:15m:51s remains)
INFO - root - 2019-11-06 20:45:28.876783: step 36900, total loss = 0.18, predict loss = 0.05 (75.8 examples/sec; 0.053 sec/batch; 1h:39m:25s remains)
INFO - root - 2019-11-06 20:45:29.603232: step 36910, total loss = 0.34, predict loss = 0.10 (64.0 examples/sec; 0.063 sec/batch; 1h:57m:48s remains)
INFO - root - 2019-11-06 20:45:30.213602: step 36920, total loss = 0.40, predict loss = 0.12 (80.0 examples/sec; 0.050 sec/batch; 1h:34m:14s remains)
INFO - root - 2019-11-06 20:45:30.854048: step 36930, total loss = 0.33, predict loss = 0.09 (82.9 examples/sec; 0.048 sec/batch; 1h:30m:57s remains)
INFO - root - 2019-11-06 20:45:31.424154: step 36940, total loss = 0.49, predict loss = 0.14 (79.8 examples/sec; 0.050 sec/batch; 1h:34m:26s remains)
INFO - root - 2019-11-06 20:45:31.990222: step 36950, total loss = 0.20, predict loss = 0.06 (76.0 examples/sec; 0.053 sec/batch; 1h:39m:10s remains)
INFO - root - 2019-11-06 20:45:32.581342: step 36960, total loss = 0.25, predict loss = 0.07 (76.1 examples/sec; 0.053 sec/batch; 1h:38m:58s remains)
INFO - root - 2019-11-06 20:45:33.182702: step 36970, total loss = 0.20, predict loss = 0.05 (74.5 examples/sec; 0.054 sec/batch; 1h:41m:04s remains)
INFO - root - 2019-11-06 20:45:33.764623: step 36980, total loss = 0.21, predict loss = 0.06 (79.9 examples/sec; 0.050 sec/batch; 1h:34m:21s remains)
INFO - root - 2019-11-06 20:45:34.349363: step 36990, total loss = 0.22, predict loss = 0.05 (79.7 examples/sec; 0.050 sec/batch; 1h:34m:31s remains)
INFO - root - 2019-11-06 20:45:34.914075: step 37000, total loss = 0.27, predict loss = 0.07 (80.0 examples/sec; 0.050 sec/batch; 1h:34m:08s remains)
INFO - root - 2019-11-06 20:45:35.509253: step 37010, total loss = 0.19, predict loss = 0.05 (80.8 examples/sec; 0.049 sec/batch; 1h:33m:11s remains)
INFO - root - 2019-11-06 20:45:36.062254: step 37020, total loss = 0.35, predict loss = 0.10 (91.9 examples/sec; 0.044 sec/batch; 1h:22m:00s remains)
INFO - root - 2019-11-06 20:45:36.522346: step 37030, total loss = 0.21, predict loss = 0.05 (94.6 examples/sec; 0.042 sec/batch; 1h:19m:35s remains)
INFO - root - 2019-11-06 20:45:36.972166: step 37040, total loss = 0.22, predict loss = 0.05 (99.7 examples/sec; 0.040 sec/batch; 1h:15m:32s remains)
INFO - root - 2019-11-06 20:45:37.981430: step 37050, total loss = 0.32, predict loss = 0.08 (53.4 examples/sec; 0.075 sec/batch; 2h:20m:54s remains)
INFO - root - 2019-11-06 20:45:38.609670: step 37060, total loss = 0.24, predict loss = 0.06 (75.8 examples/sec; 0.053 sec/batch; 1h:39m:17s remains)
INFO - root - 2019-11-06 20:45:39.201508: step 37070, total loss = 0.19, predict loss = 0.05 (76.9 examples/sec; 0.052 sec/batch; 1h:37m:54s remains)
INFO - root - 2019-11-06 20:45:39.789211: step 37080, total loss = 0.24, predict loss = 0.07 (76.8 examples/sec; 0.052 sec/batch; 1h:37m:59s remains)
INFO - root - 2019-11-06 20:45:40.388206: step 37090, total loss = 0.17, predict loss = 0.04 (74.6 examples/sec; 0.054 sec/batch; 1h:40m:50s remains)
INFO - root - 2019-11-06 20:45:40.956388: step 37100, total loss = 0.28, predict loss = 0.08 (80.5 examples/sec; 0.050 sec/batch; 1h:33m:27s remains)
INFO - root - 2019-11-06 20:45:41.537147: step 37110, total loss = 0.35, predict loss = 0.08 (79.7 examples/sec; 0.050 sec/batch; 1h:34m:25s remains)
INFO - root - 2019-11-06 20:45:42.107271: step 37120, total loss = 0.25, predict loss = 0.07 (82.5 examples/sec; 0.049 sec/batch; 1h:31m:16s remains)
INFO - root - 2019-11-06 20:45:42.692367: step 37130, total loss = 0.29, predict loss = 0.08 (80.5 examples/sec; 0.050 sec/batch; 1h:33m:29s remains)
INFO - root - 2019-11-06 20:45:43.273554: step 37140, total loss = 0.30, predict loss = 0.08 (72.5 examples/sec; 0.055 sec/batch; 1h:43m:45s remains)
INFO - root - 2019-11-06 20:45:43.845858: step 37150, total loss = 0.18, predict loss = 0.04 (78.2 examples/sec; 0.051 sec/batch; 1h:36m:14s remains)
INFO - root - 2019-11-06 20:45:44.418677: step 37160, total loss = 0.35, predict loss = 0.10 (81.0 examples/sec; 0.049 sec/batch; 1h:32m:52s remains)
INFO - root - 2019-11-06 20:45:44.968638: step 37170, total loss = 0.27, predict loss = 0.07 (92.5 examples/sec; 0.043 sec/batch; 1h:21m:16s remains)
INFO - root - 2019-11-06 20:45:45.407407: step 37180, total loss = 0.18, predict loss = 0.05 (104.7 examples/sec; 0.038 sec/batch; 1h:11m:52s remains)
INFO - root - 2019-11-06 20:45:45.861696: step 37190, total loss = 0.28, predict loss = 0.08 (99.6 examples/sec; 0.040 sec/batch; 1h:15m:29s remains)
INFO - root - 2019-11-06 20:45:46.891265: step 37200, total loss = 0.27, predict loss = 0.07 (65.7 examples/sec; 0.061 sec/batch; 1h:54m:26s remains)
INFO - root - 2019-11-06 20:45:47.543243: step 37210, total loss = 0.25, predict loss = 0.07 (79.5 examples/sec; 0.050 sec/batch; 1h:34m:32s remains)
INFO - root - 2019-11-06 20:45:48.121924: step 37220, total loss = 0.30, predict loss = 0.08 (79.7 examples/sec; 0.050 sec/batch; 1h:34m:20s remains)
INFO - root - 2019-11-06 20:45:48.694424: step 37230, total loss = 0.26, predict loss = 0.07 (77.7 examples/sec; 0.052 sec/batch; 1h:36m:47s remains)
INFO - root - 2019-11-06 20:45:49.282726: step 37240, total loss = 0.25, predict loss = 0.06 (75.3 examples/sec; 0.053 sec/batch; 1h:39m:50s remains)
INFO - root - 2019-11-06 20:45:49.872842: step 37250, total loss = 0.25, predict loss = 0.07 (76.9 examples/sec; 0.052 sec/batch; 1h:37m:41s remains)
INFO - root - 2019-11-06 20:45:50.447774: step 37260, total loss = 0.22, predict loss = 0.06 (74.8 examples/sec; 0.053 sec/batch; 1h:40m:30s remains)
INFO - root - 2019-11-06 20:45:51.032764: step 37270, total loss = 0.24, predict loss = 0.05 (76.2 examples/sec; 0.052 sec/batch; 1h:38m:35s remains)
INFO - root - 2019-11-06 20:45:51.612993: step 37280, total loss = 0.24, predict loss = 0.07 (75.1 examples/sec; 0.053 sec/batch; 1h:40m:03s remains)
INFO - root - 2019-11-06 20:45:52.212091: step 37290, total loss = 0.15, predict loss = 0.03 (73.2 examples/sec; 0.055 sec/batch; 1h:42m:40s remains)
INFO - root - 2019-11-06 20:45:52.797203: step 37300, total loss = 0.23, predict loss = 0.05 (76.2 examples/sec; 0.052 sec/batch; 1h:38m:35s remains)
INFO - root - 2019-11-06 20:45:53.376941: step 37310, total loss = 0.24, predict loss = 0.06 (76.1 examples/sec; 0.053 sec/batch; 1h:38m:41s remains)
INFO - root - 2019-11-06 20:45:53.880046: step 37320, total loss = 0.46, predict loss = 0.11 (101.8 examples/sec; 0.039 sec/batch; 1h:13m:46s remains)
INFO - root - 2019-11-06 20:45:54.353267: step 37330, total loss = 0.20, predict loss = 0.05 (103.2 examples/sec; 0.039 sec/batch; 1h:12m:47s remains)
INFO - root - 2019-11-06 20:45:54.825122: step 37340, total loss = 0.41, predict loss = 0.13 (94.7 examples/sec; 0.042 sec/batch; 1h:19m:19s remains)
INFO - root - 2019-11-06 20:45:55.889695: step 37350, total loss = 0.16, predict loss = 0.04 (63.1 examples/sec; 0.063 sec/batch; 1h:59m:04s remains)
INFO - root - 2019-11-06 20:45:56.506897: step 37360, total loss = 0.44, predict loss = 0.14 (80.5 examples/sec; 0.050 sec/batch; 1h:33m:16s remains)
INFO - root - 2019-11-06 20:45:57.101098: step 37370, total loss = 0.17, predict loss = 0.04 (81.4 examples/sec; 0.049 sec/batch; 1h:32m:14s remains)
INFO - root - 2019-11-06 20:45:57.665850: step 37380, total loss = 0.32, predict loss = 0.10 (79.3 examples/sec; 0.050 sec/batch; 1h:34m:39s remains)
INFO - root - 2019-11-06 20:45:58.220414: step 37390, total loss = 0.35, predict loss = 0.10 (83.1 examples/sec; 0.048 sec/batch; 1h:30m:22s remains)
INFO - root - 2019-11-06 20:45:58.805232: step 37400, total loss = 0.28, predict loss = 0.07 (72.6 examples/sec; 0.055 sec/batch; 1h:43m:24s remains)
INFO - root - 2019-11-06 20:45:59.393582: step 37410, total loss = 0.20, predict loss = 0.05 (79.2 examples/sec; 0.051 sec/batch; 1h:34m:48s remains)
INFO - root - 2019-11-06 20:45:59.987620: step 37420, total loss = 0.23, predict loss = 0.06 (76.9 examples/sec; 0.052 sec/batch; 1h:37m:38s remains)
INFO - root - 2019-11-06 20:46:00.637374: step 37430, total loss = 0.21, predict loss = 0.05 (66.0 examples/sec; 0.061 sec/batch; 1h:53m:42s remains)
INFO - root - 2019-11-06 20:46:01.213965: step 37440, total loss = 0.18, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:34m:50s remains)
INFO - root - 2019-11-06 20:46:01.813734: step 37450, total loss = 0.20, predict loss = 0.05 (74.2 examples/sec; 0.054 sec/batch; 1h:41m:05s remains)
INFO - root - 2019-11-06 20:46:02.396452: step 37460, total loss = 0.56, predict loss = 0.19 (77.1 examples/sec; 0.052 sec/batch; 1h:37m:15s remains)
INFO - root - 2019-11-06 20:46:02.885602: step 37470, total loss = 0.30, predict loss = 0.07 (96.3 examples/sec; 0.042 sec/batch; 1h:17m:54s remains)
INFO - root - 2019-11-06 20:46:03.325438: step 37480, total loss = 0.28, predict loss = 0.07 (98.2 examples/sec; 0.041 sec/batch; 1h:16m:24s remains)
INFO - root - 2019-11-06 20:46:04.254208: step 37490, total loss = 0.26, predict loss = 0.07 (7.9 examples/sec; 0.505 sec/batch; 15h:47m:42s remains)
INFO - root - 2019-11-06 20:46:05.070990: step 37500, total loss = 0.31, predict loss = 0.09 (46.3 examples/sec; 0.086 sec/batch; 2h:42m:00s remains)
INFO - root - 2019-11-06 20:46:05.728896: step 37510, total loss = 0.24, predict loss = 0.07 (79.2 examples/sec; 0.050 sec/batch; 1h:34m:38s remains)
INFO - root - 2019-11-06 20:46:06.301450: step 37520, total loss = 0.21, predict loss = 0.05 (79.2 examples/sec; 0.050 sec/batch; 1h:34m:37s remains)
INFO - root - 2019-11-06 20:46:06.893091: step 37530, total loss = 0.24, predict loss = 0.07 (78.8 examples/sec; 0.051 sec/batch; 1h:35m:06s remains)
INFO - root - 2019-11-06 20:46:07.474007: step 37540, total loss = 0.21, predict loss = 0.05 (78.5 examples/sec; 0.051 sec/batch; 1h:35m:33s remains)
INFO - root - 2019-11-06 20:46:08.046911: step 37550, total loss = 0.29, predict loss = 0.08 (77.6 examples/sec; 0.052 sec/batch; 1h:36m:39s remains)
INFO - root - 2019-11-06 20:46:08.619897: step 37560, total loss = 0.22, predict loss = 0.05 (81.0 examples/sec; 0.049 sec/batch; 1h:32m:34s remains)
INFO - root - 2019-11-06 20:46:09.202908: step 37570, total loss = 0.27, predict loss = 0.07 (78.8 examples/sec; 0.051 sec/batch; 1h:35m:06s remains)
INFO - root - 2019-11-06 20:46:09.782420: step 37580, total loss = 0.21, predict loss = 0.05 (78.5 examples/sec; 0.051 sec/batch; 1h:35m:29s remains)
INFO - root - 2019-11-06 20:46:10.346692: step 37590, total loss = 0.22, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:34m:58s remains)
INFO - root - 2019-11-06 20:46:10.925476: step 37600, total loss = 0.27, predict loss = 0.07 (72.7 examples/sec; 0.055 sec/batch; 1h:43m:04s remains)
INFO - root - 2019-11-06 20:46:11.510837: step 37610, total loss = 0.19, predict loss = 0.05 (89.2 examples/sec; 0.045 sec/batch; 1h:23m:59s remains)
INFO - root - 2019-11-06 20:46:11.976919: step 37620, total loss = 0.37, predict loss = 0.11 (94.7 examples/sec; 0.042 sec/batch; 1h:19m:07s remains)
INFO - root - 2019-11-06 20:46:12.428284: step 37630, total loss = 0.22, predict loss = 0.06 (99.7 examples/sec; 0.040 sec/batch; 1h:15m:08s remains)
INFO - root - 2019-11-06 20:46:13.347394: step 37640, total loss = 0.25, predict loss = 0.07 (80.6 examples/sec; 0.050 sec/batch; 1h:32m:56s remains)
INFO - root - 2019-11-06 20:46:14.078782: step 37650, total loss = 0.25, predict loss = 0.06 (54.8 examples/sec; 0.073 sec/batch; 2h:16m:40s remains)
INFO - root - 2019-11-06 20:46:14.711567: step 37660, total loss = 0.37, predict loss = 0.11 (79.9 examples/sec; 0.050 sec/batch; 1h:33m:40s remains)
INFO - root - 2019-11-06 20:46:15.289168: step 37670, total loss = 0.19, predict loss = 0.05 (76.9 examples/sec; 0.052 sec/batch; 1h:37m:23s remains)
INFO - root - 2019-11-06 20:46:15.874687: step 37680, total loss = 0.20, predict loss = 0.05 (77.7 examples/sec; 0.051 sec/batch; 1h:36m:19s remains)
INFO - root - 2019-11-06 20:46:16.471064: step 37690, total loss = 0.26, predict loss = 0.06 (78.1 examples/sec; 0.051 sec/batch; 1h:35m:51s remains)
INFO - root - 2019-11-06 20:46:17.053728: step 37700, total loss = 0.27, predict loss = 0.07 (80.3 examples/sec; 0.050 sec/batch; 1h:33m:14s remains)
INFO - root - 2019-11-06 20:46:17.634790: step 37710, total loss = 0.16, predict loss = 0.04 (78.7 examples/sec; 0.051 sec/batch; 1h:35m:06s remains)
INFO - root - 2019-11-06 20:46:18.208978: step 37720, total loss = 0.19, predict loss = 0.05 (79.7 examples/sec; 0.050 sec/batch; 1h:33m:55s remains)
INFO - root - 2019-11-06 20:46:18.801774: step 37730, total loss = 0.31, predict loss = 0.08 (77.5 examples/sec; 0.052 sec/batch; 1h:36m:33s remains)
INFO - root - 2019-11-06 20:46:19.390756: step 37740, total loss = 0.23, predict loss = 0.06 (79.6 examples/sec; 0.050 sec/batch; 1h:34m:01s remains)
INFO - root - 2019-11-06 20:46:19.970068: step 37750, total loss = 0.20, predict loss = 0.05 (79.8 examples/sec; 0.050 sec/batch; 1h:33m:46s remains)
INFO - root - 2019-11-06 20:46:20.524697: step 37760, total loss = 0.25, predict loss = 0.07 (91.7 examples/sec; 0.044 sec/batch; 1h:21m:34s remains)
INFO - root - 2019-11-06 20:46:20.994322: step 37770, total loss = 0.35, predict loss = 0.10 (100.3 examples/sec; 0.040 sec/batch; 1h:14m:37s remains)
INFO - root - 2019-11-06 20:46:21.443844: step 37780, total loss = 0.24, predict loss = 0.06 (98.0 examples/sec; 0.041 sec/batch; 1h:16m:20s remains)
INFO - root - 2019-11-06 20:46:22.373720: step 37790, total loss = 0.25, predict loss = 0.06 (76.1 examples/sec; 0.053 sec/batch; 1h:38m:15s remains)
INFO - root - 2019-11-06 20:46:23.034498: step 37800, total loss = 0.30, predict loss = 0.08 (77.8 examples/sec; 0.051 sec/batch; 1h:36m:11s remains)
INFO - root - 2019-11-06 20:46:23.634574: step 37810, total loss = 0.21, predict loss = 0.05 (79.8 examples/sec; 0.050 sec/batch; 1h:33m:47s remains)
INFO - root - 2019-11-06 20:46:24.199113: step 37820, total loss = 0.32, predict loss = 0.09 (78.8 examples/sec; 0.051 sec/batch; 1h:34m:55s remains)
INFO - root - 2019-11-06 20:46:24.780982: step 37830, total loss = 0.28, predict loss = 0.08 (79.0 examples/sec; 0.051 sec/batch; 1h:34m:37s remains)
INFO - root - 2019-11-06 20:46:25.359972: step 37840, total loss = 0.29, predict loss = 0.07 (75.6 examples/sec; 0.053 sec/batch; 1h:38m:58s remains)
INFO - root - 2019-11-06 20:46:25.947742: step 37850, total loss = 0.25, predict loss = 0.07 (78.3 examples/sec; 0.051 sec/batch; 1h:35m:31s remains)
INFO - root - 2019-11-06 20:46:26.522677: step 37860, total loss = 0.25, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:33m:42s remains)
INFO - root - 2019-11-06 20:46:27.097940: step 37870, total loss = 0.24, predict loss = 0.06 (79.4 examples/sec; 0.050 sec/batch; 1h:34m:07s remains)
INFO - root - 2019-11-06 20:46:27.684083: step 37880, total loss = 0.22, predict loss = 0.06 (80.4 examples/sec; 0.050 sec/batch; 1h:32m:59s remains)
INFO - root - 2019-11-06 20:46:28.286619: step 37890, total loss = 0.37, predict loss = 0.11 (74.3 examples/sec; 0.054 sec/batch; 1h:40m:33s remains)
INFO - root - 2019-11-06 20:46:28.858673: step 37900, total loss = 0.24, predict loss = 0.07 (81.2 examples/sec; 0.049 sec/batch; 1h:31m:59s remains)
INFO - root - 2019-11-06 20:46:29.380902: step 37910, total loss = 0.37, predict loss = 0.11 (99.3 examples/sec; 0.040 sec/batch; 1h:15m:16s remains)
INFO - root - 2019-11-06 20:46:29.843820: step 37920, total loss = 0.22, predict loss = 0.06 (94.3 examples/sec; 0.042 sec/batch; 1h:19m:12s remains)
INFO - root - 2019-11-06 20:46:30.326545: step 37930, total loss = 0.34, predict loss = 0.09 (91.2 examples/sec; 0.044 sec/batch; 1h:21m:57s remains)
INFO - root - 2019-11-06 20:46:31.366262: step 37940, total loss = 0.30, predict loss = 0.08 (59.8 examples/sec; 0.067 sec/batch; 2h:05m:01s remains)
INFO - root - 2019-11-06 20:46:31.994236: step 37950, total loss = 0.21, predict loss = 0.06 (80.4 examples/sec; 0.050 sec/batch; 1h:32m:51s remains)
INFO - root - 2019-11-06 20:46:32.572545: step 37960, total loss = 0.24, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 1h:34m:59s remains)
INFO - root - 2019-11-06 20:46:33.172118: step 37970, total loss = 0.18, predict loss = 0.05 (77.5 examples/sec; 0.052 sec/batch; 1h:36m:22s remains)
INFO - root - 2019-11-06 20:46:33.750773: step 37980, total loss = 0.20, predict loss = 0.04 (82.7 examples/sec; 0.048 sec/batch; 1h:30m:17s remains)
INFO - root - 2019-11-06 20:46:34.328656: step 37990, total loss = 0.24, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:37m:12s remains)
INFO - root - 2019-11-06 20:46:34.913966: step 38000, total loss = 0.18, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:34m:11s remains)
INFO - root - 2019-11-06 20:46:35.507616: step 38010, total loss = 0.24, predict loss = 0.06 (80.9 examples/sec; 0.049 sec/batch; 1h:32m:17s remains)
INFO - root - 2019-11-06 20:46:36.085225: step 38020, total loss = 0.22, predict loss = 0.06 (75.5 examples/sec; 0.053 sec/batch; 1h:38m:54s remains)
INFO - root - 2019-11-06 20:46:36.660524: step 38030, total loss = 0.23, predict loss = 0.06 (79.3 examples/sec; 0.050 sec/batch; 1h:34m:10s remains)
INFO - root - 2019-11-06 20:46:37.243760: step 38040, total loss = 0.34, predict loss = 0.09 (76.4 examples/sec; 0.052 sec/batch; 1h:37m:40s remains)
INFO - root - 2019-11-06 20:46:37.840554: step 38050, total loss = 0.28, predict loss = 0.08 (77.7 examples/sec; 0.051 sec/batch; 1h:36m:01s remains)
INFO - root - 2019-11-06 20:46:38.353748: step 38060, total loss = 0.31, predict loss = 0.08 (103.1 examples/sec; 0.039 sec/batch; 1h:12m:21s remains)
INFO - root - 2019-11-06 20:46:38.807650: step 38070, total loss = 0.32, predict loss = 0.08 (93.7 examples/sec; 0.043 sec/batch; 1h:19m:39s remains)
INFO - root - 2019-11-06 20:46:39.254642: step 38080, total loss = 0.21, predict loss = 0.05 (98.0 examples/sec; 0.041 sec/batch; 1h:16m:08s remains)
INFO - root - 2019-11-06 20:46:40.353370: step 38090, total loss = 0.34, predict loss = 0.09 (51.8 examples/sec; 0.077 sec/batch; 2h:24m:01s remains)
INFO - root - 2019-11-06 20:46:40.996704: step 38100, total loss = 0.28, predict loss = 0.08 (80.5 examples/sec; 0.050 sec/batch; 1h:32m:37s remains)
INFO - root - 2019-11-06 20:46:41.574631: step 38110, total loss = 0.25, predict loss = 0.07 (77.6 examples/sec; 0.052 sec/batch; 1h:36m:08s remains)
INFO - root - 2019-11-06 20:46:42.153277: step 38120, total loss = 0.38, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 1h:36m:09s remains)
INFO - root - 2019-11-06 20:46:42.756788: step 38130, total loss = 0.19, predict loss = 0.05 (74.0 examples/sec; 0.054 sec/batch; 1h:40m:49s remains)
INFO - root - 2019-11-06 20:46:43.341041: step 38140, total loss = 0.19, predict loss = 0.05 (79.7 examples/sec; 0.050 sec/batch; 1h:33m:36s remains)
INFO - root - 2019-11-06 20:46:43.911856: step 38150, total loss = 0.22, predict loss = 0.06 (78.8 examples/sec; 0.051 sec/batch; 1h:34m:34s remains)
INFO - root - 2019-11-06 20:46:44.493246: step 38160, total loss = 0.16, predict loss = 0.04 (77.3 examples/sec; 0.052 sec/batch; 1h:36m:23s remains)
INFO - root - 2019-11-06 20:46:45.090498: step 38170, total loss = 0.23, predict loss = 0.08 (78.5 examples/sec; 0.051 sec/batch; 1h:35m:00s remains)
INFO - root - 2019-11-06 20:46:45.666342: step 38180, total loss = 0.28, predict loss = 0.07 (78.7 examples/sec; 0.051 sec/batch; 1h:34m:43s remains)
INFO - root - 2019-11-06 20:46:46.257690: step 38190, total loss = 0.16, predict loss = 0.04 (76.7 examples/sec; 0.052 sec/batch; 1h:37m:11s remains)
INFO - root - 2019-11-06 20:46:46.838806: step 38200, total loss = 0.26, predict loss = 0.07 (74.9 examples/sec; 0.053 sec/batch; 1h:39m:30s remains)
INFO - root - 2019-11-06 20:46:47.345549: step 38210, total loss = 0.21, predict loss = 0.06 (100.9 examples/sec; 0.040 sec/batch; 1h:13m:52s remains)
INFO - root - 2019-11-06 20:46:47.794409: step 38220, total loss = 0.15, predict loss = 0.04 (97.2 examples/sec; 0.041 sec/batch; 1h:16m:41s remains)
INFO - root - 2019-11-06 20:46:48.253427: step 38230, total loss = 0.29, predict loss = 0.07 (100.4 examples/sec; 0.040 sec/batch; 1h:14m:13s remains)
INFO - root - 2019-11-06 20:46:49.388741: step 38240, total loss = 0.26, predict loss = 0.07 (64.9 examples/sec; 0.062 sec/batch; 1h:54m:43s remains)
INFO - root - 2019-11-06 20:46:50.071473: step 38250, total loss = 0.14, predict loss = 0.03 (74.9 examples/sec; 0.053 sec/batch; 1h:39m:27s remains)
INFO - root - 2019-11-06 20:46:50.680054: step 38260, total loss = 0.27, predict loss = 0.08 (77.4 examples/sec; 0.052 sec/batch; 1h:36m:16s remains)
INFO - root - 2019-11-06 20:46:51.246632: step 38270, total loss = 0.22, predict loss = 0.07 (82.5 examples/sec; 0.049 sec/batch; 1h:30m:19s remains)
INFO - root - 2019-11-06 20:46:51.814688: step 38280, total loss = 0.25, predict loss = 0.07 (78.2 examples/sec; 0.051 sec/batch; 1h:35m:12s remains)
INFO - root - 2019-11-06 20:46:52.405271: step 38290, total loss = 0.26, predict loss = 0.07 (83.9 examples/sec; 0.048 sec/batch; 1h:28m:44s remains)
INFO - root - 2019-11-06 20:46:52.978222: step 38300, total loss = 0.39, predict loss = 0.12 (78.6 examples/sec; 0.051 sec/batch; 1h:34m:44s remains)
INFO - root - 2019-11-06 20:46:53.553821: step 38310, total loss = 0.26, predict loss = 0.07 (81.3 examples/sec; 0.049 sec/batch; 1h:31m:33s remains)
INFO - root - 2019-11-06 20:46:54.120692: step 38320, total loss = 0.29, predict loss = 0.08 (79.6 examples/sec; 0.050 sec/batch; 1h:33m:31s remains)
INFO - root - 2019-11-06 20:46:54.724386: step 38330, total loss = 0.20, predict loss = 0.06 (75.7 examples/sec; 0.053 sec/batch; 1h:38m:18s remains)
INFO - root - 2019-11-06 20:46:55.299472: step 38340, total loss = 0.19, predict loss = 0.04 (79.1 examples/sec; 0.051 sec/batch; 1h:34m:03s remains)
INFO - root - 2019-11-06 20:46:55.876055: step 38350, total loss = 0.20, predict loss = 0.05 (77.4 examples/sec; 0.052 sec/batch; 1h:36m:07s remains)
INFO - root - 2019-11-06 20:46:56.355394: step 38360, total loss = 0.26, predict loss = 0.07 (99.3 examples/sec; 0.040 sec/batch; 1h:14m:56s remains)
INFO - root - 2019-11-06 20:46:56.825848: step 38370, total loss = 0.18, predict loss = 0.04 (95.6 examples/sec; 0.042 sec/batch; 1h:17m:48s remains)
INFO - root - 2019-11-06 20:46:57.733071: step 38380, total loss = 0.20, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:34m:26s remains)
INFO - root - 2019-11-06 20:46:58.458996: step 38390, total loss = 0.28, predict loss = 0.07 (59.1 examples/sec; 0.068 sec/batch; 2h:05m:58s remains)
INFO - root - 2019-11-06 20:46:59.084894: step 38400, total loss = 0.15, predict loss = 0.04 (78.0 examples/sec; 0.051 sec/batch; 1h:35m:21s remains)
INFO - root - 2019-11-06 20:46:59.676555: step 38410, total loss = 0.24, predict loss = 0.06 (81.8 examples/sec; 0.049 sec/batch; 1h:30m:55s remains)
INFO - root - 2019-11-06 20:47:00.256500: step 38420, total loss = 0.15, predict loss = 0.04 (81.5 examples/sec; 0.049 sec/batch; 1h:31m:15s remains)
INFO - root - 2019-11-06 20:47:00.877144: step 38430, total loss = 0.21, predict loss = 0.05 (84.3 examples/sec; 0.047 sec/batch; 1h:28m:12s remains)
INFO - root - 2019-11-06 20:47:01.442763: step 38440, total loss = 0.26, predict loss = 0.07 (78.5 examples/sec; 0.051 sec/batch; 1h:34m:46s remains)
INFO - root - 2019-11-06 20:47:02.031266: step 38450, total loss = 0.26, predict loss = 0.08 (76.1 examples/sec; 0.053 sec/batch; 1h:37m:41s remains)
INFO - root - 2019-11-06 20:47:02.611281: step 38460, total loss = 0.21, predict loss = 0.06 (79.5 examples/sec; 0.050 sec/batch; 1h:33m:32s remains)
INFO - root - 2019-11-06 20:47:03.187266: step 38470, total loss = 0.18, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:35m:10s remains)
INFO - root - 2019-11-06 20:47:03.762366: step 38480, total loss = 0.20, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 1h:34m:38s remains)
INFO - root - 2019-11-06 20:47:04.364806: step 38490, total loss = 0.18, predict loss = 0.05 (75.2 examples/sec; 0.053 sec/batch; 1h:38m:50s remains)
INFO - root - 2019-11-06 20:47:04.936462: step 38500, total loss = 0.36, predict loss = 0.12 (89.0 examples/sec; 0.045 sec/batch; 1h:23m:32s remains)
INFO - root - 2019-11-06 20:47:05.395492: step 38510, total loss = 0.16, predict loss = 0.04 (97.3 examples/sec; 0.041 sec/batch; 1h:16m:22s remains)
INFO - root - 2019-11-06 20:47:05.853945: step 38520, total loss = 0.39, predict loss = 0.12 (95.5 examples/sec; 0.042 sec/batch; 1h:17m:47s remains)
INFO - root - 2019-11-06 20:47:06.812427: step 38530, total loss = 0.23, predict loss = 0.07 (65.5 examples/sec; 0.061 sec/batch; 1h:53m:27s remains)
INFO - root - 2019-11-06 20:47:07.503993: step 38540, total loss = 0.31, predict loss = 0.08 (66.6 examples/sec; 0.060 sec/batch; 1h:51m:38s remains)
INFO - root - 2019-11-06 20:47:08.166280: step 38550, total loss = 0.43, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 1h:41m:44s remains)
INFO - root - 2019-11-06 20:47:08.763425: step 38560, total loss = 0.28, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:33m:54s remains)
INFO - root - 2019-11-06 20:47:09.345403: step 38570, total loss = 0.19, predict loss = 0.04 (81.0 examples/sec; 0.049 sec/batch; 1h:31m:43s remains)
INFO - root - 2019-11-06 20:47:09.909303: step 38580, total loss = 0.28, predict loss = 0.07 (80.6 examples/sec; 0.050 sec/batch; 1h:32m:10s remains)
INFO - root - 2019-11-06 20:47:10.481029: step 38590, total loss = 0.26, predict loss = 0.07 (79.3 examples/sec; 0.050 sec/batch; 1h:33m:38s remains)
INFO - root - 2019-11-06 20:47:11.056903: step 38600, total loss = 0.16, predict loss = 0.04 (80.4 examples/sec; 0.050 sec/batch; 1h:32m:24s remains)
INFO - root - 2019-11-06 20:47:11.646000: step 38610, total loss = 0.18, predict loss = 0.05 (78.6 examples/sec; 0.051 sec/batch; 1h:34m:26s remains)
INFO - root - 2019-11-06 20:47:12.221322: step 38620, total loss = 0.22, predict loss = 0.05 (80.8 examples/sec; 0.049 sec/batch; 1h:31m:51s remains)
INFO - root - 2019-11-06 20:47:12.801465: step 38630, total loss = 0.25, predict loss = 0.09 (78.8 examples/sec; 0.051 sec/batch; 1h:34m:13s remains)
INFO - root - 2019-11-06 20:47:13.375610: step 38640, total loss = 0.44, predict loss = 0.12 (76.5 examples/sec; 0.052 sec/batch; 1h:37m:04s remains)
INFO - root - 2019-11-06 20:47:13.941706: step 38650, total loss = 0.33, predict loss = 0.09 (97.0 examples/sec; 0.041 sec/batch; 1h:16m:33s remains)
INFO - root - 2019-11-06 20:47:14.386220: step 38660, total loss = 0.25, predict loss = 0.07 (97.7 examples/sec; 0.041 sec/batch; 1h:16m:00s remains)
INFO - root - 2019-11-06 20:47:14.831464: step 38670, total loss = 0.16, predict loss = 0.04 (92.0 examples/sec; 0.043 sec/batch; 1h:20m:38s remains)
INFO - root - 2019-11-06 20:47:15.797774: step 38680, total loss = 0.20, predict loss = 0.05 (59.0 examples/sec; 0.068 sec/batch; 2h:05m:43s remains)
INFO - root - 2019-11-06 20:47:16.452350: step 38690, total loss = 0.32, predict loss = 0.09 (76.4 examples/sec; 0.052 sec/batch; 1h:37m:08s remains)
INFO - root - 2019-11-06 20:47:17.043829: step 38700, total loss = 0.18, predict loss = 0.05 (74.4 examples/sec; 0.054 sec/batch; 1h:39m:42s remains)
INFO - root - 2019-11-06 20:47:17.621918: step 38710, total loss = 0.23, predict loss = 0.06 (77.8 examples/sec; 0.051 sec/batch; 1h:35m:20s remains)
INFO - root - 2019-11-06 20:47:18.200478: step 38720, total loss = 0.26, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:33m:02s remains)
INFO - root - 2019-11-06 20:47:18.788989: step 38730, total loss = 0.30, predict loss = 0.08 (76.8 examples/sec; 0.052 sec/batch; 1h:36m:32s remains)
INFO - root - 2019-11-06 20:47:19.371387: step 38740, total loss = 0.28, predict loss = 0.07 (77.3 examples/sec; 0.052 sec/batch; 1h:35m:59s remains)
INFO - root - 2019-11-06 20:47:19.960291: step 38750, total loss = 0.19, predict loss = 0.05 (79.9 examples/sec; 0.050 sec/batch; 1h:32m:50s remains)
INFO - root - 2019-11-06 20:47:20.534714: step 38760, total loss = 0.23, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:33m:42s remains)
INFO - root - 2019-11-06 20:47:21.133609: step 38770, total loss = 0.25, predict loss = 0.06 (78.4 examples/sec; 0.051 sec/batch; 1h:34m:36s remains)
INFO - root - 2019-11-06 20:47:21.723057: step 38780, total loss = 0.31, predict loss = 0.09 (75.4 examples/sec; 0.053 sec/batch; 1h:38m:19s remains)
INFO - root - 2019-11-06 20:47:22.305113: step 38790, total loss = 0.24, predict loss = 0.06 (77.7 examples/sec; 0.051 sec/batch; 1h:35m:24s remains)
INFO - root - 2019-11-06 20:47:22.831353: step 38800, total loss = 0.33, predict loss = 0.10 (89.6 examples/sec; 0.045 sec/batch; 1h:22m:46s remains)
INFO - root - 2019-11-06 20:47:23.302185: step 38810, total loss = 0.18, predict loss = 0.05 (101.6 examples/sec; 0.039 sec/batch; 1h:12m:56s remains)
INFO - root - 2019-11-06 20:47:23.753067: step 38820, total loss = 0.23, predict loss = 0.06 (97.1 examples/sec; 0.041 sec/batch; 1h:16m:17s remains)
INFO - root - 2019-11-06 20:47:24.774929: step 38830, total loss = 0.34, predict loss = 0.12 (64.0 examples/sec; 0.063 sec/batch; 1h:55m:50s remains)
INFO - root - 2019-11-06 20:47:25.440771: step 38840, total loss = 0.18, predict loss = 0.04 (76.9 examples/sec; 0.052 sec/batch; 1h:36m:20s remains)
INFO - root - 2019-11-06 20:47:26.065665: step 38850, total loss = 0.20, predict loss = 0.05 (73.3 examples/sec; 0.055 sec/batch; 1h:41m:03s remains)
INFO - root - 2019-11-06 20:47:26.641907: step 38860, total loss = 0.22, predict loss = 0.05 (79.0 examples/sec; 0.051 sec/batch; 1h:33m:48s remains)
INFO - root - 2019-11-06 20:47:27.227565: step 38870, total loss = 0.28, predict loss = 0.07 (73.5 examples/sec; 0.054 sec/batch; 1h:40m:51s remains)
INFO - root - 2019-11-06 20:47:27.804983: step 38880, total loss = 0.30, predict loss = 0.09 (74.8 examples/sec; 0.054 sec/batch; 1h:39m:05s remains)
INFO - root - 2019-11-06 20:47:28.393194: step 38890, total loss = 0.21, predict loss = 0.06 (82.1 examples/sec; 0.049 sec/batch; 1h:30m:12s remains)
INFO - root - 2019-11-06 20:47:28.960322: step 38900, total loss = 0.17, predict loss = 0.05 (79.2 examples/sec; 0.050 sec/batch; 1h:33m:30s remains)
INFO - root - 2019-11-06 20:47:29.537225: step 38910, total loss = 0.19, predict loss = 0.05 (75.4 examples/sec; 0.053 sec/batch; 1h:38m:13s remains)
INFO - root - 2019-11-06 20:47:30.116380: step 38920, total loss = 0.25, predict loss = 0.06 (78.7 examples/sec; 0.051 sec/batch; 1h:34m:03s remains)
INFO - root - 2019-11-06 20:47:30.760322: step 38930, total loss = 0.24, predict loss = 0.06 (79.9 examples/sec; 0.050 sec/batch; 1h:32m:43s remains)
INFO - root - 2019-11-06 20:47:31.341584: step 38940, total loss = 0.19, predict loss = 0.05 (77.5 examples/sec; 0.052 sec/batch; 1h:35m:34s remains)
INFO - root - 2019-11-06 20:47:31.842122: step 38950, total loss = 0.21, predict loss = 0.05 (93.6 examples/sec; 0.043 sec/batch; 1h:19m:03s remains)
INFO - root - 2019-11-06 20:47:32.307764: step 38960, total loss = 0.27, predict loss = 0.07 (88.5 examples/sec; 0.045 sec/batch; 1h:23m:38s remains)
INFO - root - 2019-11-06 20:47:32.796477: step 38970, total loss = 0.18, predict loss = 0.05 (87.2 examples/sec; 0.046 sec/batch; 1h:24m:54s remains)
INFO - root - 2019-11-06 20:47:33.869339: step 38980, total loss = 0.19, predict loss = 0.05 (60.8 examples/sec; 0.066 sec/batch; 2h:01m:38s remains)
INFO - root - 2019-11-06 20:47:34.503467: step 38990, total loss = 0.33, predict loss = 0.11 (77.3 examples/sec; 0.052 sec/batch; 1h:35m:45s remains)
INFO - root - 2019-11-06 20:47:35.085920: step 39000, total loss = 0.21, predict loss = 0.05 (77.6 examples/sec; 0.052 sec/batch; 1h:35m:18s remains)
INFO - root - 2019-11-06 20:47:35.677450: step 39010, total loss = 0.23, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:33m:49s remains)
INFO - root - 2019-11-06 20:47:36.261623: step 39020, total loss = 0.23, predict loss = 0.07 (81.2 examples/sec; 0.049 sec/batch; 1h:31m:05s remains)
INFO - root - 2019-11-06 20:47:36.834329: step 39030, total loss = 0.29, predict loss = 0.08 (79.2 examples/sec; 0.050 sec/batch; 1h:33m:22s remains)
INFO - root - 2019-11-06 20:47:37.409738: step 39040, total loss = 0.21, predict loss = 0.05 (80.7 examples/sec; 0.050 sec/batch; 1h:31m:43s remains)
INFO - root - 2019-11-06 20:47:38.005545: step 39050, total loss = 0.26, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:32m:47s remains)
INFO - root - 2019-11-06 20:47:38.587111: step 39060, total loss = 0.22, predict loss = 0.06 (78.7 examples/sec; 0.051 sec/batch; 1h:33m:59s remains)
INFO - root - 2019-11-06 20:47:39.162470: step 39070, total loss = 0.38, predict loss = 0.10 (81.7 examples/sec; 0.049 sec/batch; 1h:30m:33s remains)
INFO - root - 2019-11-06 20:47:39.739557: step 39080, total loss = 0.25, predict loss = 0.06 (76.9 examples/sec; 0.052 sec/batch; 1h:36m:09s remains)
INFO - root - 2019-11-06 20:47:40.320625: step 39090, total loss = 0.22, predict loss = 0.05 (79.0 examples/sec; 0.051 sec/batch; 1h:33m:33s remains)
INFO - root - 2019-11-06 20:47:40.798697: step 39100, total loss = 0.33, predict loss = 0.09 (100.7 examples/sec; 0.040 sec/batch; 1h:13m:25s remains)
INFO - root - 2019-11-06 20:47:41.237961: step 39110, total loss = 0.17, predict loss = 0.04 (92.9 examples/sec; 0.043 sec/batch; 1h:19m:34s remains)
INFO - root - 2019-11-06 20:47:42.141362: step 39120, total loss = 0.28, predict loss = 0.08 (8.3 examples/sec; 0.483 sec/batch; 14h:51m:56s remains)
INFO - root - 2019-11-06 20:47:42.829644: step 39130, total loss = 0.22, predict loss = 0.06 (63.0 examples/sec; 0.064 sec/batch; 1h:57m:23s remains)
INFO - root - 2019-11-06 20:47:43.437311: step 39140, total loss = 0.23, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:32m:34s remains)
INFO - root - 2019-11-06 20:47:44.023775: step 39150, total loss = 0.20, predict loss = 0.05 (80.2 examples/sec; 0.050 sec/batch; 1h:32m:09s remains)
INFO - root - 2019-11-06 20:47:44.589277: step 39160, total loss = 0.26, predict loss = 0.07 (81.3 examples/sec; 0.049 sec/batch; 1h:30m:51s remains)
INFO - root - 2019-11-06 20:47:45.188310: step 39170, total loss = 0.20, predict loss = 0.05 (77.8 examples/sec; 0.051 sec/batch; 1h:34m:56s remains)
INFO - root - 2019-11-06 20:47:45.767895: step 39180, total loss = 0.28, predict loss = 0.07 (78.6 examples/sec; 0.051 sec/batch; 1h:33m:56s remains)
INFO - root - 2019-11-06 20:47:46.334326: step 39190, total loss = 0.24, predict loss = 0.06 (81.1 examples/sec; 0.049 sec/batch; 1h:31m:02s remains)
INFO - root - 2019-11-06 20:47:46.896865: step 39200, total loss = 0.23, predict loss = 0.06 (80.4 examples/sec; 0.050 sec/batch; 1h:31m:55s remains)
INFO - root - 2019-11-06 20:47:47.498296: step 39210, total loss = 0.35, predict loss = 0.10 (72.9 examples/sec; 0.055 sec/batch; 1h:41m:21s remains)
INFO - root - 2019-11-06 20:47:48.072146: step 39220, total loss = 0.25, predict loss = 0.07 (78.4 examples/sec; 0.051 sec/batch; 1h:34m:09s remains)
INFO - root - 2019-11-06 20:47:48.643885: step 39230, total loss = 0.19, predict loss = 0.05 (81.8 examples/sec; 0.049 sec/batch; 1h:30m:17s remains)
INFO - root - 2019-11-06 20:47:49.225052: step 39240, total loss = 0.22, predict loss = 0.05 (87.0 examples/sec; 0.046 sec/batch; 1h:24m:50s remains)
INFO - root - 2019-11-06 20:47:49.711348: step 39250, total loss = 0.23, predict loss = 0.06 (90.1 examples/sec; 0.044 sec/batch; 1h:21m:57s remains)
INFO - root - 2019-11-06 20:47:50.169164: step 39260, total loss = 0.24, predict loss = 0.06 (100.0 examples/sec; 0.040 sec/batch; 1h:13m:48s remains)
INFO - root - 2019-11-06 20:47:51.057936: step 39270, total loss = 0.17, predict loss = 0.04 (77.6 examples/sec; 0.052 sec/batch; 1h:35m:07s remains)
INFO - root - 2019-11-06 20:47:51.773041: step 39280, total loss = 0.18, predict loss = 0.05 (61.7 examples/sec; 0.065 sec/batch; 1h:59m:35s remains)
INFO - root - 2019-11-06 20:47:52.463310: step 39290, total loss = 0.18, predict loss = 0.05 (72.1 examples/sec; 0.055 sec/batch; 1h:42m:18s remains)
INFO - root - 2019-11-06 20:47:53.071265: step 39300, total loss = 0.32, predict loss = 0.10 (80.8 examples/sec; 0.049 sec/batch; 1h:31m:19s remains)
INFO - root - 2019-11-06 20:47:53.631656: step 39310, total loss = 0.21, predict loss = 0.06 (81.6 examples/sec; 0.049 sec/batch; 1h:30m:23s remains)
INFO - root - 2019-11-06 20:47:54.205592: step 39320, total loss = 0.30, predict loss = 0.08 (79.8 examples/sec; 0.050 sec/batch; 1h:32m:31s remains)
INFO - root - 2019-11-06 20:47:54.797249: step 39330, total loss = 0.20, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:34m:35s remains)
INFO - root - 2019-11-06 20:47:55.385321: step 39340, total loss = 0.22, predict loss = 0.06 (73.0 examples/sec; 0.055 sec/batch; 1h:41m:05s remains)
INFO - root - 2019-11-06 20:47:55.963602: step 39350, total loss = 0.20, predict loss = 0.05 (82.6 examples/sec; 0.048 sec/batch; 1h:29m:16s remains)
INFO - root - 2019-11-06 20:47:56.541599: step 39360, total loss = 0.19, predict loss = 0.05 (82.7 examples/sec; 0.048 sec/batch; 1h:29m:13s remains)
INFO - root - 2019-11-06 20:47:57.137367: step 39370, total loss = 0.22, predict loss = 0.05 (77.4 examples/sec; 0.052 sec/batch; 1h:35m:15s remains)
INFO - root - 2019-11-06 20:47:57.721019: step 39380, total loss = 0.21, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:35m:34s remains)
INFO - root - 2019-11-06 20:47:58.272213: step 39390, total loss = 0.31, predict loss = 0.09 (90.3 examples/sec; 0.044 sec/batch; 1h:21m:39s remains)
INFO - root - 2019-11-06 20:47:58.731160: step 39400, total loss = 0.27, predict loss = 0.07 (92.9 examples/sec; 0.043 sec/batch; 1h:19m:22s remains)
INFO - root - 2019-11-06 20:47:59.206672: step 39410, total loss = 0.45, predict loss = 0.13 (93.3 examples/sec; 0.043 sec/batch; 1h:18m:59s remains)
INFO - root - 2019-11-06 20:48:00.154646: step 39420, total loss = 0.24, predict loss = 0.06 (57.1 examples/sec; 0.070 sec/batch; 2h:09m:02s remains)
INFO - root - 2019-11-06 20:48:00.900242: step 39430, total loss = 0.22, predict loss = 0.06 (69.5 examples/sec; 0.058 sec/batch; 1h:46m:06s remains)
INFO - root - 2019-11-06 20:48:01.555554: step 39440, total loss = 0.15, predict loss = 0.04 (69.5 examples/sec; 0.058 sec/batch; 1h:45m:59s remains)
INFO - root - 2019-11-06 20:48:02.225252: step 39450, total loss = 0.20, predict loss = 0.06 (70.9 examples/sec; 0.056 sec/batch; 1h:44m:01s remains)
INFO - root - 2019-11-06 20:48:02.816617: step 39460, total loss = 0.14, predict loss = 0.04 (78.6 examples/sec; 0.051 sec/batch; 1h:33m:42s remains)
INFO - root - 2019-11-06 20:48:03.392031: step 39470, total loss = 0.31, predict loss = 0.08 (80.9 examples/sec; 0.049 sec/batch; 1h:31m:04s remains)
INFO - root - 2019-11-06 20:48:03.967312: step 39480, total loss = 0.27, predict loss = 0.07 (82.0 examples/sec; 0.049 sec/batch; 1h:29m:53s remains)
INFO - root - 2019-11-06 20:48:04.556121: step 39490, total loss = 0.23, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:35m:28s remains)
INFO - root - 2019-11-06 20:48:05.140492: step 39500, total loss = 0.19, predict loss = 0.04 (79.1 examples/sec; 0.051 sec/batch; 1h:33m:07s remains)
INFO - root - 2019-11-06 20:48:05.709702: step 39510, total loss = 0.17, predict loss = 0.04 (81.5 examples/sec; 0.049 sec/batch; 1h:30m:21s remains)
INFO - root - 2019-11-06 20:48:06.283688: step 39520, total loss = 0.23, predict loss = 0.05 (76.8 examples/sec; 0.052 sec/batch; 1h:35m:56s remains)
INFO - root - 2019-11-06 20:48:06.889925: step 39530, total loss = 0.25, predict loss = 0.06 (80.9 examples/sec; 0.049 sec/batch; 1h:31m:01s remains)
INFO - root - 2019-11-06 20:48:07.428773: step 39540, total loss = 0.33, predict loss = 0.08 (97.1 examples/sec; 0.041 sec/batch; 1h:15m:49s remains)
INFO - root - 2019-11-06 20:48:07.888052: step 39550, total loss = 0.21, predict loss = 0.05 (104.7 examples/sec; 0.038 sec/batch; 1h:10m:19s remains)
INFO - root - 2019-11-06 20:48:08.341386: step 39560, total loss = 0.28, predict loss = 0.07 (94.7 examples/sec; 0.042 sec/batch; 1h:17m:45s remains)
INFO - root - 2019-11-06 20:48:09.343272: step 39570, total loss = 0.20, predict loss = 0.05 (61.8 examples/sec; 0.065 sec/batch; 1h:59m:03s remains)
INFO - root - 2019-11-06 20:48:09.990015: step 39580, total loss = 0.21, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:35m:22s remains)
INFO - root - 2019-11-06 20:48:10.570726: step 39590, total loss = 0.28, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 1h:33m:59s remains)
INFO - root - 2019-11-06 20:48:11.161024: step 39600, total loss = 0.27, predict loss = 0.09 (80.3 examples/sec; 0.050 sec/batch; 1h:31m:40s remains)
INFO - root - 2019-11-06 20:48:11.762900: step 39610, total loss = 0.22, predict loss = 0.06 (77.3 examples/sec; 0.052 sec/batch; 1h:35m:13s remains)
INFO - root - 2019-11-06 20:48:12.350485: step 39620, total loss = 0.22, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:35m:17s remains)
INFO - root - 2019-11-06 20:48:12.931972: step 39630, total loss = 0.20, predict loss = 0.05 (80.1 examples/sec; 0.050 sec/batch; 1h:31m:50s remains)
INFO - root - 2019-11-06 20:48:13.494885: step 39640, total loss = 0.26, predict loss = 0.07 (80.9 examples/sec; 0.049 sec/batch; 1h:30m:55s remains)
INFO - root - 2019-11-06 20:48:14.085061: step 39650, total loss = 0.25, predict loss = 0.06 (76.6 examples/sec; 0.052 sec/batch; 1h:36m:01s remains)
INFO - root - 2019-11-06 20:48:14.672594: step 39660, total loss = 0.24, predict loss = 0.05 (80.8 examples/sec; 0.050 sec/batch; 1h:31m:03s remains)
INFO - root - 2019-11-06 20:48:15.238181: step 39670, total loss = 0.15, predict loss = 0.04 (82.0 examples/sec; 0.049 sec/batch; 1h:29m:40s remains)
INFO - root - 2019-11-06 20:48:15.804870: step 39680, total loss = 0.18, predict loss = 0.05 (77.9 examples/sec; 0.051 sec/batch; 1h:34m:26s remains)
INFO - root - 2019-11-06 20:48:16.343724: step 39690, total loss = 0.26, predict loss = 0.07 (102.3 examples/sec; 0.039 sec/batch; 1h:11m:51s remains)
INFO - root - 2019-11-06 20:48:16.798219: step 39700, total loss = 0.31, predict loss = 0.08 (95.3 examples/sec; 0.042 sec/batch; 1h:17m:09s remains)
INFO - root - 2019-11-06 20:48:17.243358: step 39710, total loss = 0.28, predict loss = 0.08 (92.6 examples/sec; 0.043 sec/batch; 1h:19m:21s remains)
INFO - root - 2019-11-06 20:48:18.298791: step 39720, total loss = 0.16, predict loss = 0.04 (62.3 examples/sec; 0.064 sec/batch; 1h:58m:05s remains)
INFO - root - 2019-11-06 20:48:18.996274: step 39730, total loss = 0.22, predict loss = 0.05 (72.3 examples/sec; 0.055 sec/batch; 1h:41m:37s remains)
INFO - root - 2019-11-06 20:48:19.615663: step 39740, total loss = 0.32, predict loss = 0.08 (74.4 examples/sec; 0.054 sec/batch; 1h:38m:51s remains)
INFO - root - 2019-11-06 20:48:20.202922: step 39750, total loss = 0.22, predict loss = 0.05 (79.5 examples/sec; 0.050 sec/batch; 1h:32m:28s remains)
INFO - root - 2019-11-06 20:48:20.777855: step 39760, total loss = 0.19, predict loss = 0.05 (72.0 examples/sec; 0.056 sec/batch; 1h:42m:07s remains)
INFO - root - 2019-11-06 20:48:21.370691: step 39770, total loss = 0.38, predict loss = 0.10 (72.8 examples/sec; 0.055 sec/batch; 1h:41m:00s remains)
INFO - root - 2019-11-06 20:48:21.942326: step 39780, total loss = 0.22, predict loss = 0.05 (76.4 examples/sec; 0.052 sec/batch; 1h:36m:08s remains)
INFO - root - 2019-11-06 20:48:22.516477: step 39790, total loss = 0.23, predict loss = 0.05 (79.4 examples/sec; 0.050 sec/batch; 1h:32m:35s remains)
INFO - root - 2019-11-06 20:48:23.090187: step 39800, total loss = 0.32, predict loss = 0.08 (81.2 examples/sec; 0.049 sec/batch; 1h:30m:26s remains)
INFO - root - 2019-11-06 20:48:23.673009: step 39810, total loss = 0.26, predict loss = 0.08 (76.4 examples/sec; 0.052 sec/batch; 1h:36m:07s remains)
INFO - root - 2019-11-06 20:48:24.245070: step 39820, total loss = 0.22, predict loss = 0.05 (80.0 examples/sec; 0.050 sec/batch; 1h:31m:46s remains)
INFO - root - 2019-11-06 20:48:24.824847: step 39830, total loss = 0.24, predict loss = 0.06 (81.0 examples/sec; 0.049 sec/batch; 1h:30m:40s remains)
INFO - root - 2019-11-06 20:48:25.314280: step 39840, total loss = 0.34, predict loss = 0.10 (101.5 examples/sec; 0.039 sec/batch; 1h:12m:20s remains)
INFO - root - 2019-11-06 20:48:25.772679: step 39850, total loss = 0.27, predict loss = 0.09 (96.3 examples/sec; 0.042 sec/batch; 1h:16m:16s remains)
INFO - root - 2019-11-06 20:48:26.227081: step 39860, total loss = 0.22, predict loss = 0.06 (97.5 examples/sec; 0.041 sec/batch; 1h:15m:17s remains)
INFO - root - 2019-11-06 20:48:27.307672: step 39870, total loss = 0.20, predict loss = 0.05 (61.8 examples/sec; 0.065 sec/batch; 1h:58m:51s remains)
INFO - root - 2019-11-06 20:48:27.922551: step 39880, total loss = 0.18, predict loss = 0.05 (81.5 examples/sec; 0.049 sec/batch; 1h:30m:04s remains)
INFO - root - 2019-11-06 20:48:28.509796: step 39890, total loss = 0.19, predict loss = 0.05 (77.1 examples/sec; 0.052 sec/batch; 1h:35m:12s remains)
INFO - root - 2019-11-06 20:48:29.076951: step 39900, total loss = 0.25, predict loss = 0.07 (75.5 examples/sec; 0.053 sec/batch; 1h:37m:14s remains)
INFO - root - 2019-11-06 20:48:29.669118: step 39910, total loss = 0.31, predict loss = 0.09 (74.8 examples/sec; 0.053 sec/batch; 1h:38m:06s remains)
INFO - root - 2019-11-06 20:48:30.256214: step 39920, total loss = 0.24, predict loss = 0.06 (76.7 examples/sec; 0.052 sec/batch; 1h:35m:40s remains)
INFO - root - 2019-11-06 20:48:30.899969: step 39930, total loss = 0.15, predict loss = 0.03 (79.8 examples/sec; 0.050 sec/batch; 1h:31m:59s remains)
INFO - root - 2019-11-06 20:48:31.467419: step 39940, total loss = 0.19, predict loss = 0.05 (80.5 examples/sec; 0.050 sec/batch; 1h:31m:09s remains)
INFO - root - 2019-11-06 20:48:32.045652: step 39950, total loss = 0.20, predict loss = 0.05 (81.2 examples/sec; 0.049 sec/batch; 1h:30m:23s remains)
INFO - root - 2019-11-06 20:48:32.614566: step 39960, total loss = 0.27, predict loss = 0.08 (80.4 examples/sec; 0.050 sec/batch; 1h:31m:16s remains)
INFO - root - 2019-11-06 20:48:33.204005: step 39970, total loss = 0.23, predict loss = 0.06 (80.5 examples/sec; 0.050 sec/batch; 1h:31m:06s remains)
INFO - root - 2019-11-06 20:48:33.787266: step 39980, total loss = 0.22, predict loss = 0.06 (77.6 examples/sec; 0.052 sec/batch; 1h:34m:34s remains)
INFO - root - 2019-11-06 20:48:34.269403: step 39990, total loss = 0.18, predict loss = 0.04 (93.1 examples/sec; 0.043 sec/batch; 1h:18m:44s remains)
INFO - root - 2019-11-06 20:48:34.726029: step 40000, total loss = 0.24, predict loss = 0.06 (98.4 examples/sec; 0.041 sec/batch; 1h:14m:30s remains)
INFO - root - 2019-11-06 20:48:35.659931: step 40010, total loss = 0.22, predict loss = 0.06 (77.3 examples/sec; 0.052 sec/batch; 1h:34m:48s remains)
INFO - root - 2019-11-06 20:48:36.299191: step 40020, total loss = 0.25, predict loss = 0.06 (65.6 examples/sec; 0.061 sec/batch; 1h:51m:47s remains)
INFO - root - 2019-11-06 20:48:36.901762: step 40030, total loss = 0.26, predict loss = 0.08 (80.1 examples/sec; 0.050 sec/batch; 1h:31m:29s remains)
INFO - root - 2019-11-06 20:48:37.490356: step 40040, total loss = 0.19, predict loss = 0.05 (76.0 examples/sec; 0.053 sec/batch; 1h:36m:26s remains)
INFO - root - 2019-11-06 20:48:38.078357: step 40050, total loss = 0.20, predict loss = 0.05 (81.6 examples/sec; 0.049 sec/batch; 1h:29m:49s remains)
INFO - root - 2019-11-06 20:48:38.663418: step 40060, total loss = 0.25, predict loss = 0.07 (79.1 examples/sec; 0.051 sec/batch; 1h:32m:39s remains)
INFO - root - 2019-11-06 20:48:39.234822: step 40070, total loss = 0.29, predict loss = 0.08 (80.5 examples/sec; 0.050 sec/batch; 1h:31m:00s remains)
INFO - root - 2019-11-06 20:48:39.807728: step 40080, total loss = 0.21, predict loss = 0.05 (81.0 examples/sec; 0.049 sec/batch; 1h:30m:30s remains)
INFO - root - 2019-11-06 20:48:40.411794: step 40090, total loss = 0.34, predict loss = 0.09 (77.5 examples/sec; 0.052 sec/batch; 1h:34m:35s remains)
INFO - root - 2019-11-06 20:48:40.984894: step 40100, total loss = 0.19, predict loss = 0.05 (76.5 examples/sec; 0.052 sec/batch; 1h:35m:50s remains)
INFO - root - 2019-11-06 20:48:41.560546: step 40110, total loss = 0.35, predict loss = 0.11 (78.1 examples/sec; 0.051 sec/batch; 1h:33m:51s remains)
INFO - root - 2019-11-06 20:48:42.136404: step 40120, total loss = 0.24, predict loss = 0.06 (81.8 examples/sec; 0.049 sec/batch; 1h:29m:31s remains)
INFO - root - 2019-11-06 20:48:42.708455: step 40130, total loss = 0.38, predict loss = 0.09 (89.9 examples/sec; 0.045 sec/batch; 1h:21m:29s remains)
INFO - root - 2019-11-06 20:48:43.161654: step 40140, total loss = 0.34, predict loss = 0.11 (95.4 examples/sec; 0.042 sec/batch; 1h:16m:45s remains)
INFO - root - 2019-11-06 20:48:43.617235: step 40150, total loss = 0.24, predict loss = 0.06 (104.0 examples/sec; 0.038 sec/batch; 1h:10m:23s remains)
INFO - root - 2019-11-06 20:48:44.540460: step 40160, total loss = 0.24, predict loss = 0.05 (77.5 examples/sec; 0.052 sec/batch; 1h:34m:26s remains)
INFO - root - 2019-11-06 20:48:45.212569: step 40170, total loss = 0.19, predict loss = 0.04 (66.5 examples/sec; 0.060 sec/batch; 1h:50m:09s remains)
INFO - root - 2019-11-06 20:48:45.809317: step 40180, total loss = 0.21, predict loss = 0.05 (77.7 examples/sec; 0.051 sec/batch; 1h:34m:14s remains)
INFO - root - 2019-11-06 20:48:46.387035: step 40190, total loss = 0.27, predict loss = 0.07 (76.8 examples/sec; 0.052 sec/batch; 1h:35m:21s remains)
INFO - root - 2019-11-06 20:48:46.969615: step 40200, total loss = 0.29, predict loss = 0.08 (76.9 examples/sec; 0.052 sec/batch; 1h:35m:09s remains)
INFO - root - 2019-11-06 20:48:47.558298: step 40210, total loss = 0.22, predict loss = 0.06 (77.7 examples/sec; 0.052 sec/batch; 1h:34m:15s remains)
INFO - root - 2019-11-06 20:48:48.131390: step 40220, total loss = 0.29, predict loss = 0.08 (77.3 examples/sec; 0.052 sec/batch; 1h:34m:44s remains)
INFO - root - 2019-11-06 20:48:48.712134: step 40230, total loss = 0.15, predict loss = 0.03 (78.5 examples/sec; 0.051 sec/batch; 1h:33m:14s remains)
INFO - root - 2019-11-06 20:48:49.288703: step 40240, total loss = 0.18, predict loss = 0.04 (80.1 examples/sec; 0.050 sec/batch; 1h:31m:20s remains)
INFO - root - 2019-11-06 20:48:49.877389: step 40250, total loss = 0.29, predict loss = 0.09 (78.9 examples/sec; 0.051 sec/batch; 1h:32m:46s remains)
INFO - root - 2019-11-06 20:48:50.467386: step 40260, total loss = 0.18, predict loss = 0.05 (76.4 examples/sec; 0.052 sec/batch; 1h:35m:45s remains)
INFO - root - 2019-11-06 20:48:51.043945: step 40270, total loss = 0.14, predict loss = 0.03 (81.4 examples/sec; 0.049 sec/batch; 1h:29m:51s remains)
INFO - root - 2019-11-06 20:48:51.599420: step 40280, total loss = 0.19, predict loss = 0.05 (89.3 examples/sec; 0.045 sec/batch; 1h:21m:55s remains)
INFO - root - 2019-11-06 20:48:52.080891: step 40290, total loss = 0.23, predict loss = 0.05 (101.9 examples/sec; 0.039 sec/batch; 1h:11m:45s remains)
INFO - root - 2019-11-06 20:48:52.527741: step 40300, total loss = 0.18, predict loss = 0.05 (95.1 examples/sec; 0.042 sec/batch; 1h:16m:53s remains)
INFO - root - 2019-11-06 20:48:53.514377: step 40310, total loss = 0.30, predict loss = 0.08 (58.8 examples/sec; 0.068 sec/batch; 2h:04m:25s remains)
INFO - root - 2019-11-06 20:48:54.148491: step 40320, total loss = 0.26, predict loss = 0.07 (76.0 examples/sec; 0.053 sec/batch; 1h:36m:12s remains)
INFO - root - 2019-11-06 20:48:54.743446: step 40330, total loss = 0.23, predict loss = 0.05 (82.5 examples/sec; 0.049 sec/batch; 1h:28m:40s remains)
INFO - root - 2019-11-06 20:48:55.327933: step 40340, total loss = 0.17, predict loss = 0.05 (78.2 examples/sec; 0.051 sec/batch; 1h:33m:27s remains)
INFO - root - 2019-11-06 20:48:55.903928: step 40350, total loss = 0.26, predict loss = 0.07 (80.4 examples/sec; 0.050 sec/batch; 1h:30m:57s remains)
INFO - root - 2019-11-06 20:48:56.480170: step 40360, total loss = 0.23, predict loss = 0.06 (76.2 examples/sec; 0.053 sec/batch; 1h:35m:57s remains)
INFO - root - 2019-11-06 20:48:57.079843: step 40370, total loss = 0.23, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 1h:33m:23s remains)
INFO - root - 2019-11-06 20:48:57.650879: step 40380, total loss = 0.25, predict loss = 0.07 (79.2 examples/sec; 0.051 sec/batch; 1h:32m:17s remains)
INFO - root - 2019-11-06 20:48:58.236751: step 40390, total loss = 0.26, predict loss = 0.08 (78.7 examples/sec; 0.051 sec/batch; 1h:32m:50s remains)
INFO - root - 2019-11-06 20:48:58.812448: step 40400, total loss = 0.21, predict loss = 0.05 (78.7 examples/sec; 0.051 sec/batch; 1h:32m:50s remains)
INFO - root - 2019-11-06 20:48:59.395051: step 40410, total loss = 0.27, predict loss = 0.07 (79.9 examples/sec; 0.050 sec/batch; 1h:31m:25s remains)
INFO - root - 2019-11-06 20:48:59.970702: step 40420, total loss = 0.31, predict loss = 0.10 (78.9 examples/sec; 0.051 sec/batch; 1h:32m:34s remains)
INFO - root - 2019-11-06 20:49:00.539139: step 40430, total loss = 0.22, predict loss = 0.06 (74.2 examples/sec; 0.054 sec/batch; 1h:38m:22s remains)
INFO - root - 2019-11-06 20:49:01.002486: step 40440, total loss = 0.21, predict loss = 0.06 (99.5 examples/sec; 0.040 sec/batch; 1h:13m:25s remains)
INFO - root - 2019-11-06 20:49:01.489561: step 40450, total loss = 0.23, predict loss = 0.06 (87.5 examples/sec; 0.046 sec/batch; 1h:23m:29s remains)
INFO - root - 2019-11-06 20:49:02.525149: step 40460, total loss = 0.19, predict loss = 0.05 (61.3 examples/sec; 0.065 sec/batch; 1h:59m:02s remains)
INFO - root - 2019-11-06 20:49:03.198527: step 40470, total loss = 0.19, predict loss = 0.04 (68.2 examples/sec; 0.059 sec/batch; 1h:47m:00s remains)
INFO - root - 2019-11-06 20:49:03.798771: step 40480, total loss = 0.31, predict loss = 0.08 (81.3 examples/sec; 0.049 sec/batch; 1h:29m:45s remains)
INFO - root - 2019-11-06 20:49:04.388676: step 40490, total loss = 0.26, predict loss = 0.07 (77.3 examples/sec; 0.052 sec/batch; 1h:34m:24s remains)
INFO - root - 2019-11-06 20:49:04.958639: step 40500, total loss = 0.29, predict loss = 0.08 (77.7 examples/sec; 0.052 sec/batch; 1h:34m:00s remains)
INFO - root - 2019-11-06 20:49:05.542691: step 40510, total loss = 0.18, predict loss = 0.05 (74.2 examples/sec; 0.054 sec/batch; 1h:38m:25s remains)
INFO - root - 2019-11-06 20:49:06.126942: step 40520, total loss = 0.26, predict loss = 0.07 (79.9 examples/sec; 0.050 sec/batch; 1h:31m:22s remains)
INFO - root - 2019-11-06 20:49:06.705508: step 40530, total loss = 0.22, predict loss = 0.05 (73.4 examples/sec; 0.055 sec/batch; 1h:39m:28s remains)
INFO - root - 2019-11-06 20:49:07.291706: step 40540, total loss = 0.19, predict loss = 0.04 (77.2 examples/sec; 0.052 sec/batch; 1h:34m:30s remains)
INFO - root - 2019-11-06 20:49:07.874752: step 40550, total loss = 0.28, predict loss = 0.08 (76.9 examples/sec; 0.052 sec/batch; 1h:34m:52s remains)
INFO - root - 2019-11-06 20:49:08.454752: step 40560, total loss = 0.27, predict loss = 0.07 (82.0 examples/sec; 0.049 sec/batch; 1h:28m:57s remains)
INFO - root - 2019-11-06 20:49:09.041055: step 40570, total loss = 0.23, predict loss = 0.05 (75.4 examples/sec; 0.053 sec/batch; 1h:36m:45s remains)
INFO - root - 2019-11-06 20:49:09.540462: step 40580, total loss = 0.39, predict loss = 0.10 (98.5 examples/sec; 0.041 sec/batch; 1h:14m:03s remains)
INFO - root - 2019-11-06 20:49:09.988135: step 40590, total loss = 0.37, predict loss = 0.10 (92.7 examples/sec; 0.043 sec/batch; 1h:18m:41s remains)
INFO - root - 2019-11-06 20:49:10.447451: step 40600, total loss = 0.23, predict loss = 0.07 (94.7 examples/sec; 0.042 sec/batch; 1h:16m:59s remains)
INFO - root - 2019-11-06 20:49:11.635146: step 40610, total loss = 0.29, predict loss = 0.08 (48.9 examples/sec; 0.082 sec/batch; 2h:28m:59s remains)
INFO - root - 2019-11-06 20:49:12.271107: step 40620, total loss = 0.30, predict loss = 0.08 (83.0 examples/sec; 0.048 sec/batch; 1h:27m:48s remains)
INFO - root - 2019-11-06 20:49:12.864837: step 40630, total loss = 0.32, predict loss = 0.09 (77.4 examples/sec; 0.052 sec/batch; 1h:34m:15s remains)
INFO - root - 2019-11-06 20:49:13.447827: step 40640, total loss = 0.18, predict loss = 0.04 (72.9 examples/sec; 0.055 sec/batch; 1h:40m:00s remains)
INFO - root - 2019-11-06 20:49:14.038120: step 40650, total loss = 0.20, predict loss = 0.05 (78.4 examples/sec; 0.051 sec/batch; 1h:33m:02s remains)
INFO - root - 2019-11-06 20:49:14.617094: step 40660, total loss = 0.19, predict loss = 0.05 (75.1 examples/sec; 0.053 sec/batch; 1h:37m:01s remains)
INFO - root - 2019-11-06 20:49:15.197925: step 40670, total loss = 0.20, predict loss = 0.05 (78.9 examples/sec; 0.051 sec/batch; 1h:32m:21s remains)
INFO - root - 2019-11-06 20:49:15.778427: step 40680, total loss = 0.25, predict loss = 0.06 (76.3 examples/sec; 0.052 sec/batch; 1h:35m:34s remains)
INFO - root - 2019-11-06 20:49:16.379505: step 40690, total loss = 0.41, predict loss = 0.11 (81.6 examples/sec; 0.049 sec/batch; 1h:29m:21s remains)
INFO - root - 2019-11-06 20:49:16.957040: step 40700, total loss = 0.24, predict loss = 0.06 (81.1 examples/sec; 0.049 sec/batch; 1h:29m:48s remains)
INFO - root - 2019-11-06 20:49:17.528679: step 40710, total loss = 0.30, predict loss = 0.07 (79.8 examples/sec; 0.050 sec/batch; 1h:31m:16s remains)
INFO - root - 2019-11-06 20:49:18.099936: step 40720, total loss = 0.37, predict loss = 0.10 (76.1 examples/sec; 0.053 sec/batch; 1h:35m:41s remains)
INFO - root - 2019-11-06 20:49:18.604245: step 40730, total loss = 0.31, predict loss = 0.08 (93.5 examples/sec; 0.043 sec/batch; 1h:17m:56s remains)
INFO - root - 2019-11-06 20:49:19.072196: step 40740, total loss = 0.24, predict loss = 0.06 (98.6 examples/sec; 0.041 sec/batch; 1h:13m:53s remains)
INFO - root - 2019-11-06 20:49:19.974763: step 40750, total loss = 0.27, predict loss = 0.08 (8.3 examples/sec; 0.484 sec/batch; 14h:40m:32s remains)
INFO - root - 2019-11-06 20:49:20.628140: step 40760, total loss = 0.15, predict loss = 0.04 (67.5 examples/sec; 0.059 sec/batch; 1h:47m:50s remains)
INFO - root - 2019-11-06 20:49:21.274005: step 40770, total loss = 0.29, predict loss = 0.08 (76.7 examples/sec; 0.052 sec/batch; 1h:34m:54s remains)
INFO - root - 2019-11-06 20:49:21.847261: step 40780, total loss = 0.26, predict loss = 0.07 (82.1 examples/sec; 0.049 sec/batch; 1h:28m:39s remains)
INFO - root - 2019-11-06 20:49:22.421881: step 40790, total loss = 0.25, predict loss = 0.06 (78.2 examples/sec; 0.051 sec/batch; 1h:33m:03s remains)
INFO - root - 2019-11-06 20:49:22.996921: step 40800, total loss = 0.25, predict loss = 0.06 (81.9 examples/sec; 0.049 sec/batch; 1h:28m:55s remains)
INFO - root - 2019-11-06 20:49:23.579240: step 40810, total loss = 0.23, predict loss = 0.06 (77.1 examples/sec; 0.052 sec/batch; 1h:34m:21s remains)
INFO - root - 2019-11-06 20:49:24.154094: step 40820, total loss = 0.23, predict loss = 0.05 (75.7 examples/sec; 0.053 sec/batch; 1h:36m:09s remains)
INFO - root - 2019-11-06 20:49:24.740613: step 40830, total loss = 0.16, predict loss = 0.04 (74.3 examples/sec; 0.054 sec/batch; 1h:37m:58s remains)
INFO - root - 2019-11-06 20:49:25.322560: step 40840, total loss = 0.29, predict loss = 0.07 (80.2 examples/sec; 0.050 sec/batch; 1h:30m:43s remains)
INFO - root - 2019-11-06 20:49:25.921912: step 40850, total loss = 0.23, predict loss = 0.06 (79.0 examples/sec; 0.051 sec/batch; 1h:32m:03s remains)
INFO - root - 2019-11-06 20:49:26.490751: step 40860, total loss = 0.18, predict loss = 0.04 (76.9 examples/sec; 0.052 sec/batch; 1h:34m:37s remains)
INFO - root - 2019-11-06 20:49:27.058413: step 40870, total loss = 0.24, predict loss = 0.06 (85.5 examples/sec; 0.047 sec/batch; 1h:25m:05s remains)
INFO - root - 2019-11-06 20:49:27.521951: step 40880, total loss = 0.31, predict loss = 0.08 (98.9 examples/sec; 0.040 sec/batch; 1h:13m:33s remains)
INFO - root - 2019-11-06 20:49:28.019078: step 40890, total loss = 0.41, predict loss = 0.12 (91.9 examples/sec; 0.044 sec/batch; 1h:19m:09s remains)
INFO - root - 2019-11-06 20:49:28.928756: step 40900, total loss = 0.24, predict loss = 0.07 (76.7 examples/sec; 0.052 sec/batch; 1h:34m:51s remains)
INFO - root - 2019-11-06 20:49:29.623778: step 40910, total loss = 0.24, predict loss = 0.06 (67.2 examples/sec; 0.060 sec/batch; 1h:48m:11s remains)
INFO - root - 2019-11-06 20:49:30.241984: step 40920, total loss = 0.37, predict loss = 0.10 (80.3 examples/sec; 0.050 sec/batch; 1h:30m:33s remains)
INFO - root - 2019-11-06 20:49:30.875887: step 40930, total loss = 0.16, predict loss = 0.04 (80.0 examples/sec; 0.050 sec/batch; 1h:30m:53s remains)
INFO - root - 2019-11-06 20:49:31.460514: step 40940, total loss = 0.20, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:32m:19s remains)
INFO - root - 2019-11-06 20:49:32.038950: step 40950, total loss = 0.28, predict loss = 0.08 (74.5 examples/sec; 0.054 sec/batch; 1h:37m:33s remains)
INFO - root - 2019-11-06 20:49:32.624030: step 40960, total loss = 0.20, predict loss = 0.05 (77.1 examples/sec; 0.052 sec/batch; 1h:34m:17s remains)
INFO - root - 2019-11-06 20:49:33.217135: step 40970, total loss = 0.28, predict loss = 0.07 (76.4 examples/sec; 0.052 sec/batch; 1h:35m:11s remains)
INFO - root - 2019-11-06 20:49:33.801259: step 40980, total loss = 0.26, predict loss = 0.08 (74.6 examples/sec; 0.054 sec/batch; 1h:37m:25s remains)
INFO - root - 2019-11-06 20:49:34.386277: step 40990, total loss = 0.15, predict loss = 0.03 (79.9 examples/sec; 0.050 sec/batch; 1h:30m:57s remains)
INFO - root - 2019-11-06 20:49:34.954294: step 41000, total loss = 0.32, predict loss = 0.09 (77.5 examples/sec; 0.052 sec/batch; 1h:33m:47s remains)
INFO - root - 2019-11-06 20:49:35.551645: step 41010, total loss = 0.19, predict loss = 0.04 (75.9 examples/sec; 0.053 sec/batch; 1h:35m:45s remains)
INFO - root - 2019-11-06 20:49:36.106623: step 41020, total loss = 0.18, predict loss = 0.05 (92.8 examples/sec; 0.043 sec/batch; 1h:18m:18s remains)
INFO - root - 2019-11-06 20:49:36.555966: step 41030, total loss = 0.20, predict loss = 0.05 (97.3 examples/sec; 0.041 sec/batch; 1h:14m:37s remains)
INFO - root - 2019-11-06 20:49:37.009962: step 41040, total loss = 0.29, predict loss = 0.08 (101.5 examples/sec; 0.039 sec/batch; 1h:11m:32s remains)
INFO - root - 2019-11-06 20:49:37.979408: step 41050, total loss = 0.27, predict loss = 0.08 (77.3 examples/sec; 0.052 sec/batch; 1h:33m:56s remains)
INFO - root - 2019-11-06 20:49:38.701984: step 41060, total loss = 0.24, predict loss = 0.06 (62.8 examples/sec; 0.064 sec/batch; 1h:55m:40s remains)
INFO - root - 2019-11-06 20:49:39.311441: step 41070, total loss = 0.24, predict loss = 0.07 (77.9 examples/sec; 0.051 sec/batch; 1h:33m:11s remains)
INFO - root - 2019-11-06 20:49:39.884294: step 41080, total loss = 0.17, predict loss = 0.04 (78.4 examples/sec; 0.051 sec/batch; 1h:32m:35s remains)
INFO - root - 2019-11-06 20:49:40.471401: step 41090, total loss = 0.25, predict loss = 0.06 (80.3 examples/sec; 0.050 sec/batch; 1h:30m:25s remains)
INFO - root - 2019-11-06 20:49:41.036742: step 41100, total loss = 0.26, predict loss = 0.07 (83.0 examples/sec; 0.048 sec/batch; 1h:27m:25s remains)
INFO - root - 2019-11-06 20:49:41.609802: step 41110, total loss = 0.30, predict loss = 0.08 (78.9 examples/sec; 0.051 sec/batch; 1h:32m:01s remains)
INFO - root - 2019-11-06 20:49:42.173709: step 41120, total loss = 0.26, predict loss = 0.06 (81.4 examples/sec; 0.049 sec/batch; 1h:29m:11s remains)
INFO - root - 2019-11-06 20:49:42.757837: step 41130, total loss = 0.19, predict loss = 0.05 (81.3 examples/sec; 0.049 sec/batch; 1h:29m:19s remains)
INFO - root - 2019-11-06 20:49:43.351316: step 41140, total loss = 0.24, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:31m:41s remains)
INFO - root - 2019-11-06 20:49:43.939119: step 41150, total loss = 0.27, predict loss = 0.07 (77.0 examples/sec; 0.052 sec/batch; 1h:34m:16s remains)
INFO - root - 2019-11-06 20:49:44.525739: step 41160, total loss = 0.33, predict loss = 0.10 (76.8 examples/sec; 0.052 sec/batch; 1h:34m:31s remains)
INFO - root - 2019-11-06 20:49:45.072405: step 41170, total loss = 0.17, predict loss = 0.04 (96.2 examples/sec; 0.042 sec/batch; 1h:15m:23s remains)
INFO - root - 2019-11-06 20:49:45.522868: step 41180, total loss = 0.23, predict loss = 0.06 (97.7 examples/sec; 0.041 sec/batch; 1h:14m:14s remains)
INFO - root - 2019-11-06 20:49:45.981854: step 41190, total loss = 0.31, predict loss = 0.10 (97.8 examples/sec; 0.041 sec/batch; 1h:14m:10s remains)
INFO - root - 2019-11-06 20:49:46.971849: step 41200, total loss = 0.19, predict loss = 0.05 (56.2 examples/sec; 0.071 sec/batch; 2h:09m:02s remains)
INFO - root - 2019-11-06 20:49:47.648848: step 41210, total loss = 0.33, predict loss = 0.09 (73.6 examples/sec; 0.054 sec/batch; 1h:38m:35s remains)
INFO - root - 2019-11-06 20:49:48.240621: step 41220, total loss = 0.16, predict loss = 0.04 (79.5 examples/sec; 0.050 sec/batch; 1h:31m:14s remains)
INFO - root - 2019-11-06 20:49:48.815311: step 41230, total loss = 0.47, predict loss = 0.15 (71.4 examples/sec; 0.056 sec/batch; 1h:41m:37s remains)
INFO - root - 2019-11-06 20:49:49.405166: step 41240, total loss = 0.28, predict loss = 0.08 (76.3 examples/sec; 0.052 sec/batch; 1h:34m:59s remains)
INFO - root - 2019-11-06 20:49:49.990273: step 41250, total loss = 0.20, predict loss = 0.05 (74.8 examples/sec; 0.054 sec/batch; 1h:36m:58s remains)
INFO - root - 2019-11-06 20:49:50.556878: step 41260, total loss = 0.25, predict loss = 0.06 (82.9 examples/sec; 0.048 sec/batch; 1h:27m:29s remains)
INFO - root - 2019-11-06 20:49:51.137816: step 41270, total loss = 0.20, predict loss = 0.05 (75.3 examples/sec; 0.053 sec/batch; 1h:36m:15s remains)
INFO - root - 2019-11-06 20:49:51.723260: step 41280, total loss = 0.38, predict loss = 0.10 (76.8 examples/sec; 0.052 sec/batch; 1h:34m:24s remains)
INFO - root - 2019-11-06 20:49:52.314246: step 41290, total loss = 0.22, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:33m:49s remains)
INFO - root - 2019-11-06 20:49:52.898454: step 41300, total loss = 0.18, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:32m:54s remains)
INFO - root - 2019-11-06 20:49:53.473436: step 41310, total loss = 0.20, predict loss = 0.06 (80.7 examples/sec; 0.050 sec/batch; 1h:29m:47s remains)
INFO - root - 2019-11-06 20:49:53.983334: step 41320, total loss = 0.24, predict loss = 0.06 (105.1 examples/sec; 0.038 sec/batch; 1h:08m:55s remains)
INFO - root - 2019-11-06 20:49:54.456747: step 41330, total loss = 0.32, predict loss = 0.08 (99.3 examples/sec; 0.040 sec/batch; 1h:12m:55s remains)
INFO - root - 2019-11-06 20:49:54.907600: step 41340, total loss = 0.15, predict loss = 0.04 (97.3 examples/sec; 0.041 sec/batch; 1h:14m:25s remains)
INFO - root - 2019-11-06 20:49:55.996336: step 41350, total loss = 0.28, predict loss = 0.07 (57.6 examples/sec; 0.069 sec/batch; 2h:05m:40s remains)
INFO - root - 2019-11-06 20:49:56.635156: step 41360, total loss = 0.19, predict loss = 0.05 (79.8 examples/sec; 0.050 sec/batch; 1h:30m:43s remains)
INFO - root - 2019-11-06 20:49:57.233548: step 41370, total loss = 0.16, predict loss = 0.05 (77.7 examples/sec; 0.051 sec/batch; 1h:33m:11s remains)
INFO - root - 2019-11-06 20:49:57.814657: step 41380, total loss = 0.38, predict loss = 0.11 (78.0 examples/sec; 0.051 sec/batch; 1h:32m:47s remains)
INFO - root - 2019-11-06 20:49:58.400801: step 41390, total loss = 0.27, predict loss = 0.07 (70.5 examples/sec; 0.057 sec/batch; 1h:42m:46s remains)
INFO - root - 2019-11-06 20:49:58.985042: step 41400, total loss = 0.15, predict loss = 0.04 (78.5 examples/sec; 0.051 sec/batch; 1h:32m:15s remains)
INFO - root - 2019-11-06 20:49:59.575765: step 41410, total loss = 0.20, predict loss = 0.05 (81.2 examples/sec; 0.049 sec/batch; 1h:29m:11s remains)
INFO - root - 2019-11-06 20:50:00.154315: step 41420, total loss = 0.41, predict loss = 0.11 (74.5 examples/sec; 0.054 sec/batch; 1h:37m:12s remains)
INFO - root - 2019-11-06 20:50:00.777967: step 41430, total loss = 0.14, predict loss = 0.03 (75.6 examples/sec; 0.053 sec/batch; 1h:35m:40s remains)
INFO - root - 2019-11-06 20:50:01.367241: step 41440, total loss = 0.21, predict loss = 0.06 (76.3 examples/sec; 0.052 sec/batch; 1h:34m:53s remains)
INFO - root - 2019-11-06 20:50:01.973468: step 41450, total loss = 0.28, predict loss = 0.07 (79.6 examples/sec; 0.050 sec/batch; 1h:30m:51s remains)
INFO - root - 2019-11-06 20:50:02.543437: step 41460, total loss = 0.18, predict loss = 0.05 (77.4 examples/sec; 0.052 sec/batch; 1h:33m:27s remains)
INFO - root - 2019-11-06 20:50:03.036509: step 41470, total loss = 0.27, predict loss = 0.07 (102.8 examples/sec; 0.039 sec/batch; 1h:10m:23s remains)
INFO - root - 2019-11-06 20:50:03.485759: step 41480, total loss = 0.22, predict loss = 0.05 (95.5 examples/sec; 0.042 sec/batch; 1h:15m:46s remains)
INFO - root - 2019-11-06 20:50:03.958365: step 41490, total loss = 0.22, predict loss = 0.05 (95.2 examples/sec; 0.042 sec/batch; 1h:15m:57s remains)
INFO - root - 2019-11-06 20:50:05.145202: step 41500, total loss = 0.27, predict loss = 0.08 (50.7 examples/sec; 0.079 sec/batch; 2h:22m:46s remains)
INFO - root - 2019-11-06 20:50:05.794911: step 41510, total loss = 0.18, predict loss = 0.05 (76.6 examples/sec; 0.052 sec/batch; 1h:34m:28s remains)
INFO - root - 2019-11-06 20:50:06.372944: step 41520, total loss = 0.25, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:31m:29s remains)
INFO - root - 2019-11-06 20:50:06.970417: step 41530, total loss = 0.30, predict loss = 0.09 (78.4 examples/sec; 0.051 sec/batch; 1h:32m:16s remains)
INFO - root - 2019-11-06 20:50:07.545022: step 41540, total loss = 0.23, predict loss = 0.06 (77.6 examples/sec; 0.052 sec/batch; 1h:33m:11s remains)
INFO - root - 2019-11-06 20:50:08.134241: step 41550, total loss = 0.28, predict loss = 0.07 (77.2 examples/sec; 0.052 sec/batch; 1h:33m:36s remains)
INFO - root - 2019-11-06 20:50:08.719533: step 41560, total loss = 0.20, predict loss = 0.05 (80.2 examples/sec; 0.050 sec/batch; 1h:30m:06s remains)
INFO - root - 2019-11-06 20:50:09.328076: step 41570, total loss = 0.21, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:31m:26s remains)
INFO - root - 2019-11-06 20:50:09.906065: step 41580, total loss = 0.20, predict loss = 0.05 (80.3 examples/sec; 0.050 sec/batch; 1h:30m:00s remains)
INFO - root - 2019-11-06 20:50:10.480314: step 41590, total loss = 0.16, predict loss = 0.04 (79.0 examples/sec; 0.051 sec/batch; 1h:31m:31s remains)
INFO - root - 2019-11-06 20:50:11.047977: step 41600, total loss = 0.21, predict loss = 0.05 (78.2 examples/sec; 0.051 sec/batch; 1h:32m:22s remains)
INFO - root - 2019-11-06 20:50:11.631426: step 41610, total loss = 0.21, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:33m:33s remains)
INFO - root - 2019-11-06 20:50:12.109492: step 41620, total loss = 0.17, predict loss = 0.05 (95.7 examples/sec; 0.042 sec/batch; 1h:15m:30s remains)
INFO - root - 2019-11-06 20:50:12.556137: step 41630, total loss = 0.36, predict loss = 0.11 (93.7 examples/sec; 0.043 sec/batch; 1h:17m:05s remains)
INFO - root - 2019-11-06 20:50:13.458975: step 41640, total loss = 0.19, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 1h:32m:16s remains)
INFO - root - 2019-11-06 20:50:14.101820: step 41650, total loss = 0.27, predict loss = 0.08 (66.8 examples/sec; 0.060 sec/batch; 1h:48m:11s remains)
INFO - root - 2019-11-06 20:50:14.685707: step 41660, total loss = 0.20, predict loss = 0.05 (83.8 examples/sec; 0.048 sec/batch; 1h:26m:13s remains)
INFO - root - 2019-11-06 20:50:15.263587: step 41670, total loss = 0.19, predict loss = 0.06 (80.8 examples/sec; 0.050 sec/batch; 1h:29m:24s remains)
INFO - root - 2019-11-06 20:50:15.842488: step 41680, total loss = 0.21, predict loss = 0.05 (79.6 examples/sec; 0.050 sec/batch; 1h:30m:44s remains)
INFO - root - 2019-11-06 20:50:16.433573: step 41690, total loss = 0.15, predict loss = 0.04 (81.6 examples/sec; 0.049 sec/batch; 1h:28m:31s remains)
INFO - root - 2019-11-06 20:50:17.003546: step 41700, total loss = 0.24, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 1h:31m:54s remains)
INFO - root - 2019-11-06 20:50:17.580789: step 41710, total loss = 0.27, predict loss = 0.07 (75.2 examples/sec; 0.053 sec/batch; 1h:36m:03s remains)
INFO - root - 2019-11-06 20:50:18.167720: step 41720, total loss = 0.31, predict loss = 0.09 (80.4 examples/sec; 0.050 sec/batch; 1h:29m:46s remains)
INFO - root - 2019-11-06 20:50:18.761807: step 41730, total loss = 0.29, predict loss = 0.07 (76.3 examples/sec; 0.052 sec/batch; 1h:34m:34s remains)
INFO - root - 2019-11-06 20:50:19.336189: step 41740, total loss = 0.24, predict loss = 0.07 (76.2 examples/sec; 0.053 sec/batch; 1h:34m:46s remains)
INFO - root - 2019-11-06 20:50:19.908650: step 41750, total loss = 0.22, predict loss = 0.06 (76.0 examples/sec; 0.053 sec/batch; 1h:34m:57s remains)
INFO - root - 2019-11-06 20:50:20.556049: step 41760, total loss = 0.15, predict loss = 0.04 (84.4 examples/sec; 0.047 sec/batch; 1h:25m:29s remains)
INFO - root - 2019-11-06 20:50:21.061365: step 41770, total loss = 0.20, predict loss = 0.05 (95.5 examples/sec; 0.042 sec/batch; 1h:15m:34s remains)
INFO - root - 2019-11-06 20:50:21.536370: step 41780, total loss = 0.22, predict loss = 0.06 (93.9 examples/sec; 0.043 sec/batch; 1h:16m:51s remains)
INFO - root - 2019-11-06 20:50:22.568830: step 41790, total loss = 0.38, predict loss = 0.11 (74.2 examples/sec; 0.054 sec/batch; 1h:37m:15s remains)
INFO - root - 2019-11-06 20:50:23.259620: step 41800, total loss = 0.30, predict loss = 0.08 (64.9 examples/sec; 0.062 sec/batch; 1h:51m:12s remains)
INFO - root - 2019-11-06 20:50:23.998823: step 41810, total loss = 0.39, predict loss = 0.11 (65.0 examples/sec; 0.062 sec/batch; 1h:50m:56s remains)
INFO - root - 2019-11-06 20:50:24.725357: step 41820, total loss = 0.31, predict loss = 0.08 (57.9 examples/sec; 0.069 sec/batch; 2h:04m:31s remains)
INFO - root - 2019-11-06 20:50:25.430806: step 41830, total loss = 0.23, predict loss = 0.06 (61.7 examples/sec; 0.065 sec/batch; 1h:56m:56s remains)
INFO - root - 2019-11-06 20:50:26.138897: step 41840, total loss = 0.22, predict loss = 0.06 (66.2 examples/sec; 0.060 sec/batch; 1h:48m:51s remains)
INFO - root - 2019-11-06 20:50:26.887223: step 41850, total loss = 0.26, predict loss = 0.07 (64.5 examples/sec; 0.062 sec/batch; 1h:51m:41s remains)
INFO - root - 2019-11-06 20:50:27.584410: step 41860, total loss = 0.32, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 1h:48m:23s remains)
INFO - root - 2019-11-06 20:50:28.280806: step 41870, total loss = 0.22, predict loss = 0.06 (64.8 examples/sec; 0.062 sec/batch; 1h:51m:19s remains)
INFO - root - 2019-11-06 20:50:28.992054: step 41880, total loss = 0.25, predict loss = 0.07 (60.6 examples/sec; 0.066 sec/batch; 1h:59m:01s remains)
INFO - root - 2019-11-06 20:50:29.732117: step 41890, total loss = 0.18, predict loss = 0.04 (63.0 examples/sec; 0.063 sec/batch; 1h:54m:19s remains)
INFO - root - 2019-11-06 20:50:30.476653: step 41900, total loss = 0.21, predict loss = 0.05 (51.3 examples/sec; 0.078 sec/batch; 2h:20m:21s remains)
INFO - root - 2019-11-06 20:50:31.119954: step 41910, total loss = 0.50, predict loss = 0.14 (74.7 examples/sec; 0.054 sec/batch; 1h:36m:24s remains)
INFO - root - 2019-11-06 20:50:31.601241: step 41920, total loss = 0.37, predict loss = 0.10 (105.1 examples/sec; 0.038 sec/batch; 1h:08m:34s remains)
INFO - root - 2019-11-06 20:50:32.075523: step 41930, total loss = 0.26, predict loss = 0.06 (103.4 examples/sec; 0.039 sec/batch; 1h:09m:39s remains)
INFO - root - 2019-11-06 20:50:33.058777: step 41940, total loss = 0.26, predict loss = 0.07 (66.0 examples/sec; 0.061 sec/batch; 1h:49m:08s remains)
INFO - root - 2019-11-06 20:50:33.695948: step 41950, total loss = 0.17, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:32m:16s remains)
INFO - root - 2019-11-06 20:50:34.272701: step 41960, total loss = 0.31, predict loss = 0.09 (83.9 examples/sec; 0.048 sec/batch; 1h:25m:50s remains)
INFO - root - 2019-11-06 20:50:34.860986: step 41970, total loss = 0.22, predict loss = 0.06 (80.3 examples/sec; 0.050 sec/batch; 1h:29m:38s remains)
INFO - root - 2019-11-06 20:50:35.436208: step 41980, total loss = 0.22, predict loss = 0.05 (77.3 examples/sec; 0.052 sec/batch; 1h:33m:10s remains)
INFO - root - 2019-11-06 20:50:36.006081: step 41990, total loss = 0.23, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:30m:23s remains)
INFO - root - 2019-11-06 20:50:36.587899: step 42000, total loss = 0.18, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:33m:12s remains)
INFO - root - 2019-11-06 20:50:37.179893: step 42010, total loss = 0.22, predict loss = 0.06 (85.8 examples/sec; 0.047 sec/batch; 1h:23m:56s remains)
INFO - root - 2019-11-06 20:50:37.745376: step 42020, total loss = 0.18, predict loss = 0.04 (79.7 examples/sec; 0.050 sec/batch; 1h:30m:21s remains)
INFO - root - 2019-11-06 20:50:38.314294: step 42030, total loss = 0.21, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:31m:20s remains)
INFO - root - 2019-11-06 20:50:38.882036: step 42040, total loss = 0.23, predict loss = 0.06 (72.4 examples/sec; 0.055 sec/batch; 1h:39m:20s remains)
INFO - root - 2019-11-06 20:50:39.481614: step 42050, total loss = 0.30, predict loss = 0.09 (80.5 examples/sec; 0.050 sec/batch; 1h:29m:21s remains)
INFO - root - 2019-11-06 20:50:40.010370: step 42060, total loss = 0.26, predict loss = 0.07 (98.1 examples/sec; 0.041 sec/batch; 1h:13m:23s remains)
INFO - root - 2019-11-06 20:50:40.456625: step 42070, total loss = 0.25, predict loss = 0.07 (102.6 examples/sec; 0.039 sec/batch; 1h:10m:07s remains)
INFO - root - 2019-11-06 20:50:40.904891: step 42080, total loss = 0.22, predict loss = 0.05 (99.9 examples/sec; 0.040 sec/batch; 1h:12m:01s remains)
INFO - root - 2019-11-06 20:50:41.877219: step 42090, total loss = 0.25, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 1h:32m:50s remains)
INFO - root - 2019-11-06 20:50:42.531027: step 42100, total loss = 0.16, predict loss = 0.04 (69.0 examples/sec; 0.058 sec/batch; 1h:44m:16s remains)
INFO - root - 2019-11-06 20:50:43.116804: step 42110, total loss = 0.13, predict loss = 0.04 (79.8 examples/sec; 0.050 sec/batch; 1h:30m:11s remains)
INFO - root - 2019-11-06 20:50:43.682869: step 42120, total loss = 0.25, predict loss = 0.06 (77.3 examples/sec; 0.052 sec/batch; 1h:33m:05s remains)
INFO - root - 2019-11-06 20:50:44.279287: step 42130, total loss = 0.18, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:31m:17s remains)
INFO - root - 2019-11-06 20:50:44.848072: step 42140, total loss = 0.21, predict loss = 0.05 (80.1 examples/sec; 0.050 sec/batch; 1h:29m:43s remains)
INFO - root - 2019-11-06 20:50:45.437307: step 42150, total loss = 0.22, predict loss = 0.05 (72.6 examples/sec; 0.055 sec/batch; 1h:38m:59s remains)
INFO - root - 2019-11-06 20:50:46.024719: step 42160, total loss = 0.22, predict loss = 0.06 (81.7 examples/sec; 0.049 sec/batch; 1h:27m:58s remains)
INFO - root - 2019-11-06 20:50:46.627774: step 42170, total loss = 0.25, predict loss = 0.07 (77.9 examples/sec; 0.051 sec/batch; 1h:32m:19s remains)
INFO - root - 2019-11-06 20:50:47.203780: step 42180, total loss = 0.29, predict loss = 0.09 (74.6 examples/sec; 0.054 sec/batch; 1h:36m:19s remains)
INFO - root - 2019-11-06 20:50:47.784024: step 42190, total loss = 0.36, predict loss = 0.10 (75.7 examples/sec; 0.053 sec/batch; 1h:34m:53s remains)
INFO - root - 2019-11-06 20:50:48.362363: step 42200, total loss = 0.23, predict loss = 0.07 (77.2 examples/sec; 0.052 sec/batch; 1h:33m:08s remains)
INFO - root - 2019-11-06 20:50:48.885960: step 42210, total loss = 0.31, predict loss = 0.10 (104.8 examples/sec; 0.038 sec/batch; 1h:08m:34s remains)
INFO - root - 2019-11-06 20:50:49.334586: step 42220, total loss = 0.22, predict loss = 0.06 (91.5 examples/sec; 0.044 sec/batch; 1h:18m:31s remains)
INFO - root - 2019-11-06 20:50:49.790832: step 42230, total loss = 0.25, predict loss = 0.08 (94.8 examples/sec; 0.042 sec/batch; 1h:15m:49s remains)
INFO - root - 2019-11-06 20:50:50.850976: step 42240, total loss = 0.15, predict loss = 0.04 (66.0 examples/sec; 0.061 sec/batch; 1h:48m:54s remains)
INFO - root - 2019-11-06 20:50:51.499253: step 42250, total loss = 0.29, predict loss = 0.08 (79.7 examples/sec; 0.050 sec/batch; 1h:30m:05s remains)
INFO - root - 2019-11-06 20:50:52.072852: step 42260, total loss = 0.21, predict loss = 0.05 (80.4 examples/sec; 0.050 sec/batch; 1h:29m:19s remains)
INFO - root - 2019-11-06 20:50:52.651882: step 42270, total loss = 0.23, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 1h:31m:43s remains)
INFO - root - 2019-11-06 20:50:53.230091: step 42280, total loss = 0.15, predict loss = 0.03 (76.8 examples/sec; 0.052 sec/batch; 1h:33m:27s remains)
INFO - root - 2019-11-06 20:50:53.820271: step 42290, total loss = 0.21, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 1h:31m:21s remains)
INFO - root - 2019-11-06 20:50:54.394831: step 42300, total loss = 0.19, predict loss = 0.05 (76.6 examples/sec; 0.052 sec/batch; 1h:33m:45s remains)
INFO - root - 2019-11-06 20:50:54.962338: step 42310, total loss = 0.21, predict loss = 0.05 (75.8 examples/sec; 0.053 sec/batch; 1h:34m:46s remains)
INFO - root - 2019-11-06 20:50:55.535703: step 42320, total loss = 0.18, predict loss = 0.05 (75.6 examples/sec; 0.053 sec/batch; 1h:35m:00s remains)
INFO - root - 2019-11-06 20:50:56.124064: step 42330, total loss = 0.22, predict loss = 0.06 (81.6 examples/sec; 0.049 sec/batch; 1h:27m:55s remains)
INFO - root - 2019-11-06 20:50:56.698466: step 42340, total loss = 0.21, predict loss = 0.05 (70.5 examples/sec; 0.057 sec/batch; 1h:41m:44s remains)
INFO - root - 2019-11-06 20:50:57.272615: step 42350, total loss = 0.20, predict loss = 0.05 (76.3 examples/sec; 0.052 sec/batch; 1h:34m:05s remains)
INFO - root - 2019-11-06 20:50:57.749164: step 42360, total loss = 0.34, predict loss = 0.08 (104.4 examples/sec; 0.038 sec/batch; 1h:08m:43s remains)
INFO - root - 2019-11-06 20:50:58.215159: step 42370, total loss = 0.31, predict loss = 0.08 (102.2 examples/sec; 0.039 sec/batch; 1h:10m:10s remains)
INFO - root - 2019-11-06 20:50:59.127618: step 42380, total loss = 0.18, predict loss = 0.05 (8.0 examples/sec; 0.501 sec/batch; 14h:58m:57s remains)
INFO - root - 2019-11-06 20:50:59.801853: step 42390, total loss = 0.21, predict loss = 0.06 (65.2 examples/sec; 0.061 sec/batch; 1h:50m:03s remains)
INFO - root - 2019-11-06 20:51:00.443996: step 42400, total loss = 0.18, predict loss = 0.04 (77.5 examples/sec; 0.052 sec/batch; 1h:32m:32s remains)
INFO - root - 2019-11-06 20:51:01.050710: step 42410, total loss = 0.21, predict loss = 0.04 (78.6 examples/sec; 0.051 sec/batch; 1h:31m:15s remains)
INFO - root - 2019-11-06 20:51:01.620894: step 42420, total loss = 0.29, predict loss = 0.08 (77.1 examples/sec; 0.052 sec/batch; 1h:33m:02s remains)
INFO - root - 2019-11-06 20:51:02.193570: step 42430, total loss = 0.29, predict loss = 0.08 (76.5 examples/sec; 0.052 sec/batch; 1h:33m:44s remains)
INFO - root - 2019-11-06 20:51:02.791477: step 42440, total loss = 0.29, predict loss = 0.08 (72.5 examples/sec; 0.055 sec/batch; 1h:38m:50s remains)
INFO - root - 2019-11-06 20:51:03.382346: step 42450, total loss = 0.19, predict loss = 0.05 (79.4 examples/sec; 0.050 sec/batch; 1h:30m:17s remains)
INFO - root - 2019-11-06 20:51:03.955901: step 42460, total loss = 0.24, predict loss = 0.06 (75.3 examples/sec; 0.053 sec/batch; 1h:35m:10s remains)
INFO - root - 2019-11-06 20:51:04.534150: step 42470, total loss = 0.17, predict loss = 0.04 (77.4 examples/sec; 0.052 sec/batch; 1h:32m:40s remains)
INFO - root - 2019-11-06 20:51:05.100742: step 42480, total loss = 0.23, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:32m:39s remains)
INFO - root - 2019-11-06 20:51:05.682189: step 42490, total loss = 0.25, predict loss = 0.07 (82.3 examples/sec; 0.049 sec/batch; 1h:27m:05s remains)
INFO - root - 2019-11-06 20:51:06.262819: step 42500, total loss = 0.26, predict loss = 0.08 (83.0 examples/sec; 0.048 sec/batch; 1h:26m:21s remains)
INFO - root - 2019-11-06 20:51:06.720836: step 42510, total loss = 0.22, predict loss = 0.05 (101.3 examples/sec; 0.039 sec/batch; 1h:10m:45s remains)
INFO - root - 2019-11-06 20:51:07.170058: step 42520, total loss = 0.34, predict loss = 0.09 (94.0 examples/sec; 0.043 sec/batch; 1h:16m:12s remains)
INFO - root - 2019-11-06 20:51:08.096496: step 42530, total loss = 0.24, predict loss = 0.06 (80.2 examples/sec; 0.050 sec/batch; 1h:29m:19s remains)
INFO - root - 2019-11-06 20:51:08.803181: step 42540, total loss = 0.19, predict loss = 0.05 (53.6 examples/sec; 0.075 sec/batch; 2h:13m:44s remains)
INFO - root - 2019-11-06 20:51:09.425468: step 42550, total loss = 0.19, predict loss = 0.04 (77.5 examples/sec; 0.052 sec/batch; 1h:32m:23s remains)
INFO - root - 2019-11-06 20:51:09.992292: step 42560, total loss = 0.25, predict loss = 0.06 (75.2 examples/sec; 0.053 sec/batch; 1h:35m:16s remains)
INFO - root - 2019-11-06 20:51:10.576288: step 42570, total loss = 0.20, predict loss = 0.04 (77.2 examples/sec; 0.052 sec/batch; 1h:32m:42s remains)
INFO - root - 2019-11-06 20:51:11.144113: step 42580, total loss = 0.21, predict loss = 0.06 (78.7 examples/sec; 0.051 sec/batch; 1h:30m:57s remains)
INFO - root - 2019-11-06 20:51:11.724363: step 42590, total loss = 0.22, predict loss = 0.06 (76.6 examples/sec; 0.052 sec/batch; 1h:33m:28s remains)
INFO - root - 2019-11-06 20:51:12.289840: step 42600, total loss = 0.18, predict loss = 0.05 (79.2 examples/sec; 0.051 sec/batch; 1h:30m:25s remains)
INFO - root - 2019-11-06 20:51:12.881854: step 42610, total loss = 0.22, predict loss = 0.06 (76.4 examples/sec; 0.052 sec/batch; 1h:33m:42s remains)
INFO - root - 2019-11-06 20:51:13.459894: step 42620, total loss = 0.19, predict loss = 0.05 (82.2 examples/sec; 0.049 sec/batch; 1h:27m:06s remains)
INFO - root - 2019-11-06 20:51:14.035022: step 42630, total loss = 0.18, predict loss = 0.04 (79.0 examples/sec; 0.051 sec/batch; 1h:30m:34s remains)
INFO - root - 2019-11-06 20:51:14.607499: step 42640, total loss = 0.25, predict loss = 0.06 (80.7 examples/sec; 0.050 sec/batch; 1h:28m:38s remains)
INFO - root - 2019-11-06 20:51:15.173870: step 42650, total loss = 0.19, predict loss = 0.06 (94.5 examples/sec; 0.042 sec/batch; 1h:15m:42s remains)
INFO - root - 2019-11-06 20:51:15.634559: step 42660, total loss = 0.31, predict loss = 0.09 (91.2 examples/sec; 0.044 sec/batch; 1h:18m:25s remains)
INFO - root - 2019-11-06 20:51:16.099971: step 42670, total loss = 0.22, predict loss = 0.06 (95.6 examples/sec; 0.042 sec/batch; 1h:14m:48s remains)
INFO - root - 2019-11-06 20:51:17.047381: step 42680, total loss = 0.24, predict loss = 0.07 (67.0 examples/sec; 0.060 sec/batch; 1h:46m:44s remains)
INFO - root - 2019-11-06 20:51:17.724182: step 42690, total loss = 0.25, predict loss = 0.06 (70.7 examples/sec; 0.057 sec/batch; 1h:41m:10s remains)
INFO - root - 2019-11-06 20:51:18.311216: step 42700, total loss = 0.23, predict loss = 0.07 (81.1 examples/sec; 0.049 sec/batch; 1h:28m:12s remains)
INFO - root - 2019-11-06 20:51:18.889858: step 42710, total loss = 0.15, predict loss = 0.04 (79.3 examples/sec; 0.050 sec/batch; 1h:30m:14s remains)
INFO - root - 2019-11-06 20:51:19.464966: step 42720, total loss = 0.21, predict loss = 0.05 (82.6 examples/sec; 0.048 sec/batch; 1h:26m:36s remains)
INFO - root - 2019-11-06 20:51:20.055791: step 42730, total loss = 0.20, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:31m:32s remains)
INFO - root - 2019-11-06 20:51:20.624498: step 42740, total loss = 0.15, predict loss = 0.04 (75.9 examples/sec; 0.053 sec/batch; 1h:34m:14s remains)
INFO - root - 2019-11-06 20:51:21.208723: step 42750, total loss = 0.28, predict loss = 0.07 (70.6 examples/sec; 0.057 sec/batch; 1h:41m:15s remains)
INFO - root - 2019-11-06 20:51:21.788329: step 42760, total loss = 0.34, predict loss = 0.09 (80.2 examples/sec; 0.050 sec/batch; 1h:29m:09s remains)
INFO - root - 2019-11-06 20:51:22.379773: step 42770, total loss = 0.25, predict loss = 0.05 (77.8 examples/sec; 0.051 sec/batch; 1h:31m:54s remains)
INFO - root - 2019-11-06 20:51:22.963167: step 42780, total loss = 0.32, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 1h:31m:24s remains)
INFO - root - 2019-11-06 20:51:23.534213: step 42790, total loss = 0.18, predict loss = 0.04 (79.0 examples/sec; 0.051 sec/batch; 1h:30m:26s remains)
INFO - root - 2019-11-06 20:51:24.059360: step 42800, total loss = 0.28, predict loss = 0.07 (96.4 examples/sec; 0.041 sec/batch; 1h:14m:07s remains)
INFO - root - 2019-11-06 20:51:24.533409: step 42810, total loss = 0.18, predict loss = 0.05 (96.7 examples/sec; 0.041 sec/batch; 1h:13m:53s remains)
INFO - root - 2019-11-06 20:51:24.980280: step 42820, total loss = 0.24, predict loss = 0.06 (101.3 examples/sec; 0.039 sec/batch; 1h:10m:31s remains)
INFO - root - 2019-11-06 20:51:25.965833: step 42830, total loss = 0.27, predict loss = 0.07 (56.6 examples/sec; 0.071 sec/batch; 2h:06m:09s remains)
INFO - root - 2019-11-06 20:51:26.609064: step 42840, total loss = 0.23, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:30m:21s remains)
INFO - root - 2019-11-06 20:51:27.187938: step 42850, total loss = 0.22, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:30m:15s remains)
INFO - root - 2019-11-06 20:51:27.767874: step 42860, total loss = 0.37, predict loss = 0.10 (81.7 examples/sec; 0.049 sec/batch; 1h:27m:26s remains)
INFO - root - 2019-11-06 20:51:28.358503: step 42870, total loss = 0.18, predict loss = 0.05 (72.8 examples/sec; 0.055 sec/batch; 1h:38m:02s remains)
INFO - root - 2019-11-06 20:51:28.935673: step 42880, total loss = 0.24, predict loss = 0.06 (80.1 examples/sec; 0.050 sec/batch; 1h:29m:08s remains)
INFO - root - 2019-11-06 20:51:29.524470: step 42890, total loss = 0.23, predict loss = 0.07 (83.3 examples/sec; 0.048 sec/batch; 1h:25m:41s remains)
INFO - root - 2019-11-06 20:51:30.083304: step 42900, total loss = 0.20, predict loss = 0.05 (79.4 examples/sec; 0.050 sec/batch; 1h:29m:54s remains)
INFO - root - 2019-11-06 20:51:30.680486: step 42910, total loss = 0.23, predict loss = 0.06 (81.4 examples/sec; 0.049 sec/batch; 1h:27m:44s remains)
INFO - root - 2019-11-06 20:51:31.261033: step 42920, total loss = 0.21, predict loss = 0.06 (78.8 examples/sec; 0.051 sec/batch; 1h:30m:36s remains)
INFO - root - 2019-11-06 20:51:31.861012: step 42930, total loss = 0.27, predict loss = 0.08 (79.2 examples/sec; 0.051 sec/batch; 1h:30m:07s remains)
INFO - root - 2019-11-06 20:51:32.440479: step 42940, total loss = 0.30, predict loss = 0.08 (78.5 examples/sec; 0.051 sec/batch; 1h:30m:58s remains)
INFO - root - 2019-11-06 20:51:32.954625: step 42950, total loss = 0.18, predict loss = 0.04 (103.5 examples/sec; 0.039 sec/batch; 1h:08m:58s remains)
INFO - root - 2019-11-06 20:51:33.392047: step 42960, total loss = 0.23, predict loss = 0.06 (103.9 examples/sec; 0.038 sec/batch; 1h:08m:40s remains)
INFO - root - 2019-11-06 20:51:33.862650: step 42970, total loss = 0.31, predict loss = 0.08 (93.0 examples/sec; 0.043 sec/batch; 1h:16m:44s remains)
INFO - root - 2019-11-06 20:51:35.005279: step 42980, total loss = 0.12, predict loss = 0.03 (47.4 examples/sec; 0.084 sec/batch; 2h:30m:24s remains)
INFO - root - 2019-11-06 20:51:35.655558: step 42990, total loss = 0.24, predict loss = 0.06 (79.4 examples/sec; 0.050 sec/batch; 1h:29m:49s remains)
INFO - root - 2019-11-06 20:51:36.225832: step 43000, total loss = 0.20, predict loss = 0.05 (81.5 examples/sec; 0.049 sec/batch; 1h:27m:29s remains)
INFO - root - 2019-11-06 20:51:36.826894: step 43010, total loss = 0.25, predict loss = 0.07 (76.3 examples/sec; 0.052 sec/batch; 1h:33m:31s remains)
INFO - root - 2019-11-06 20:51:37.391711: step 43020, total loss = 0.32, predict loss = 0.09 (80.6 examples/sec; 0.050 sec/batch; 1h:28m:27s remains)
INFO - root - 2019-11-06 20:51:37.965425: step 43030, total loss = 0.24, predict loss = 0.06 (84.1 examples/sec; 0.048 sec/batch; 1h:24m:47s remains)
INFO - root - 2019-11-06 20:51:38.542742: step 43040, total loss = 0.25, predict loss = 0.07 (77.0 examples/sec; 0.052 sec/batch; 1h:32m:37s remains)
INFO - root - 2019-11-06 20:51:39.135645: step 43050, total loss = 0.28, predict loss = 0.09 (78.9 examples/sec; 0.051 sec/batch; 1h:30m:24s remains)
INFO - root - 2019-11-06 20:51:39.716700: step 43060, total loss = 0.20, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:30m:30s remains)
INFO - root - 2019-11-06 20:51:40.304790: step 43070, total loss = 0.27, predict loss = 0.07 (81.5 examples/sec; 0.049 sec/batch; 1h:27m:29s remains)
INFO - root - 2019-11-06 20:51:40.888811: step 43080, total loss = 0.14, predict loss = 0.03 (77.3 examples/sec; 0.052 sec/batch; 1h:32m:15s remains)
INFO - root - 2019-11-06 20:51:41.471203: step 43090, total loss = 0.17, predict loss = 0.04 (80.4 examples/sec; 0.050 sec/batch; 1h:28m:41s remains)
INFO - root - 2019-11-06 20:51:41.963410: step 43100, total loss = 0.24, predict loss = 0.07 (103.7 examples/sec; 0.039 sec/batch; 1h:08m:42s remains)
INFO - root - 2019-11-06 20:51:42.422035: step 43110, total loss = 0.24, predict loss = 0.06 (95.4 examples/sec; 0.042 sec/batch; 1h:14m:41s remains)
INFO - root - 2019-11-06 20:51:42.879585: step 43120, total loss = 0.25, predict loss = 0.06 (90.4 examples/sec; 0.044 sec/batch; 1h:18m:49s remains)
INFO - root - 2019-11-06 20:51:44.086470: step 43130, total loss = 0.18, predict loss = 0.05 (50.6 examples/sec; 0.079 sec/batch; 2h:20m:46s remains)
INFO - root - 2019-11-06 20:51:44.700025: step 43140, total loss = 0.28, predict loss = 0.07 (73.2 examples/sec; 0.055 sec/batch; 1h:37m:18s remains)
INFO - root - 2019-11-06 20:51:45.277671: step 43150, total loss = 0.18, predict loss = 0.05 (75.2 examples/sec; 0.053 sec/batch; 1h:34m:46s remains)
INFO - root - 2019-11-06 20:51:45.851793: step 43160, total loss = 0.30, predict loss = 0.09 (84.6 examples/sec; 0.047 sec/batch; 1h:24m:10s remains)
INFO - root - 2019-11-06 20:51:46.444227: step 43170, total loss = 0.22, predict loss = 0.05 (76.0 examples/sec; 0.053 sec/batch; 1h:33m:45s remains)
INFO - root - 2019-11-06 20:51:47.011246: step 43180, total loss = 0.32, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 1h:30m:56s remains)
INFO - root - 2019-11-06 20:51:47.596636: step 43190, total loss = 0.24, predict loss = 0.08 (78.8 examples/sec; 0.051 sec/batch; 1h:30m:19s remains)
INFO - root - 2019-11-06 20:51:48.180945: step 43200, total loss = 0.30, predict loss = 0.08 (82.7 examples/sec; 0.048 sec/batch; 1h:26m:08s remains)
INFO - root - 2019-11-06 20:51:48.771344: step 43210, total loss = 0.23, predict loss = 0.06 (82.6 examples/sec; 0.048 sec/batch; 1h:26m:10s remains)
INFO - root - 2019-11-06 20:51:49.339247: step 43220, total loss = 0.17, predict loss = 0.05 (79.6 examples/sec; 0.050 sec/batch; 1h:29m:25s remains)
INFO - root - 2019-11-06 20:51:49.922058: step 43230, total loss = 0.20, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:32m:13s remains)
INFO - root - 2019-11-06 20:51:50.506416: step 43240, total loss = 0.24, predict loss = 0.06 (79.3 examples/sec; 0.050 sec/batch; 1h:29m:47s remains)
INFO - root - 2019-11-06 20:51:50.991436: step 43250, total loss = 0.37, predict loss = 0.09 (98.6 examples/sec; 0.041 sec/batch; 1h:12m:11s remains)
INFO - root - 2019-11-06 20:51:51.452754: step 43260, total loss = 0.29, predict loss = 0.07 (99.9 examples/sec; 0.040 sec/batch; 1h:11m:14s remains)
INFO - root - 2019-11-06 20:51:52.364821: step 43270, total loss = 0.19, predict loss = 0.05 (79.0 examples/sec; 0.051 sec/batch; 1h:30m:01s remains)
INFO - root - 2019-11-06 20:51:53.030541: step 43280, total loss = 0.21, predict loss = 0.06 (65.7 examples/sec; 0.061 sec/batch; 1h:48m:13s remains)
INFO - root - 2019-11-06 20:51:53.689147: step 43290, total loss = 0.21, predict loss = 0.05 (78.5 examples/sec; 0.051 sec/batch; 1h:30m:36s remains)
INFO - root - 2019-11-06 20:51:54.262463: step 43300, total loss = 0.20, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:30m:18s remains)
INFO - root - 2019-11-06 20:51:54.834525: step 43310, total loss = 0.23, predict loss = 0.06 (78.0 examples/sec; 0.051 sec/batch; 1h:31m:08s remains)
INFO - root - 2019-11-06 20:51:55.406855: step 43320, total loss = 0.23, predict loss = 0.05 (76.4 examples/sec; 0.052 sec/batch; 1h:33m:07s remains)
INFO - root - 2019-11-06 20:51:56.001526: step 43330, total loss = 0.26, predict loss = 0.06 (76.7 examples/sec; 0.052 sec/batch; 1h:32m:39s remains)
INFO - root - 2019-11-06 20:51:56.572829: step 43340, total loss = 0.20, predict loss = 0.04 (78.6 examples/sec; 0.051 sec/batch; 1h:30m:27s remains)
INFO - root - 2019-11-06 20:51:57.167528: step 43350, total loss = 0.19, predict loss = 0.04 (80.4 examples/sec; 0.050 sec/batch; 1h:28m:25s remains)
INFO - root - 2019-11-06 20:51:57.741428: step 43360, total loss = 0.22, predict loss = 0.06 (82.0 examples/sec; 0.049 sec/batch; 1h:26m:45s remains)
INFO - root - 2019-11-06 20:51:58.328116: step 43370, total loss = 0.31, predict loss = 0.09 (77.3 examples/sec; 0.052 sec/batch; 1h:31m:55s remains)
INFO - root - 2019-11-06 20:51:58.915150: step 43380, total loss = 0.15, predict loss = 0.04 (74.0 examples/sec; 0.054 sec/batch; 1h:36m:04s remains)
INFO - root - 2019-11-06 20:51:59.477917: step 43390, total loss = 0.15, predict loss = 0.04 (90.9 examples/sec; 0.044 sec/batch; 1h:18m:11s remains)
INFO - root - 2019-11-06 20:51:59.935468: step 43400, total loss = 0.36, predict loss = 0.10 (91.4 examples/sec; 0.044 sec/batch; 1h:17m:45s remains)
INFO - root - 2019-11-06 20:52:00.404785: step 43410, total loss = 0.22, predict loss = 0.06 (94.5 examples/sec; 0.042 sec/batch; 1h:15m:13s remains)
INFO - root - 2019-11-06 20:52:01.319014: step 43420, total loss = 0.42, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 1h:36m:06s remains)
INFO - root - 2019-11-06 20:52:02.079644: step 43430, total loss = 0.29, predict loss = 0.09 (57.5 examples/sec; 0.070 sec/batch; 2h:03m:38s remains)
INFO - root - 2019-11-06 20:52:02.718796: step 43440, total loss = 0.18, predict loss = 0.05 (75.4 examples/sec; 0.053 sec/batch; 1h:34m:14s remains)
INFO - root - 2019-11-06 20:52:03.323383: step 43450, total loss = 0.34, predict loss = 0.09 (81.3 examples/sec; 0.049 sec/batch; 1h:27m:24s remains)
INFO - root - 2019-11-06 20:52:03.905901: step 43460, total loss = 0.23, predict loss = 0.07 (79.6 examples/sec; 0.050 sec/batch; 1h:29m:13s remains)
INFO - root - 2019-11-06 20:52:04.486714: step 43470, total loss = 0.27, predict loss = 0.07 (76.6 examples/sec; 0.052 sec/batch; 1h:32m:44s remains)
INFO - root - 2019-11-06 20:52:05.054515: step 43480, total loss = 0.17, predict loss = 0.05 (78.3 examples/sec; 0.051 sec/batch; 1h:30m:44s remains)
INFO - root - 2019-11-06 20:52:05.652317: step 43490, total loss = 0.25, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:29m:45s remains)
INFO - root - 2019-11-06 20:52:06.228111: step 43500, total loss = 0.26, predict loss = 0.07 (75.8 examples/sec; 0.053 sec/batch; 1h:33m:41s remains)
INFO - root - 2019-11-06 20:52:06.807835: step 43510, total loss = 0.35, predict loss = 0.10 (70.8 examples/sec; 0.057 sec/batch; 1h:40m:20s remains)
INFO - root - 2019-11-06 20:52:07.390641: step 43520, total loss = 0.17, predict loss = 0.04 (73.1 examples/sec; 0.055 sec/batch; 1h:37m:05s remains)
INFO - root - 2019-11-06 20:52:07.986040: step 43530, total loss = 0.43, predict loss = 0.13 (80.3 examples/sec; 0.050 sec/batch; 1h:28m:20s remains)
INFO - root - 2019-11-06 20:52:08.527290: step 43540, total loss = 0.16, predict loss = 0.04 (96.7 examples/sec; 0.041 sec/batch; 1h:13m:22s remains)
INFO - root - 2019-11-06 20:52:08.976066: step 43550, total loss = 0.25, predict loss = 0.07 (102.1 examples/sec; 0.039 sec/batch; 1h:09m:28s remains)
INFO - root - 2019-11-06 20:52:09.421485: step 43560, total loss = 0.23, predict loss = 0.06 (100.7 examples/sec; 0.040 sec/batch; 1h:10m:27s remains)
INFO - root - 2019-11-06 20:52:10.422045: step 43570, total loss = 0.24, predict loss = 0.06 (54.3 examples/sec; 0.074 sec/batch; 2h:10m:46s remains)
INFO - root - 2019-11-06 20:52:11.059703: step 43580, total loss = 0.27, predict loss = 0.07 (76.9 examples/sec; 0.052 sec/batch; 1h:32m:16s remains)
INFO - root - 2019-11-06 20:52:11.636914: step 43590, total loss = 0.37, predict loss = 0.10 (77.8 examples/sec; 0.051 sec/batch; 1h:31m:09s remains)
INFO - root - 2019-11-06 20:52:12.217828: step 43600, total loss = 0.22, predict loss = 0.05 (82.2 examples/sec; 0.049 sec/batch; 1h:26m:19s remains)
INFO - root - 2019-11-06 20:52:12.819729: step 43610, total loss = 0.20, predict loss = 0.05 (76.6 examples/sec; 0.052 sec/batch; 1h:32m:38s remains)
INFO - root - 2019-11-06 20:52:13.393487: step 43620, total loss = 0.23, predict loss = 0.06 (79.0 examples/sec; 0.051 sec/batch; 1h:29m:49s remains)
INFO - root - 2019-11-06 20:52:13.973421: step 43630, total loss = 0.25, predict loss = 0.06 (79.6 examples/sec; 0.050 sec/batch; 1h:29m:05s remains)
INFO - root - 2019-11-06 20:52:14.533563: step 43640, total loss = 0.25, predict loss = 0.07 (79.9 examples/sec; 0.050 sec/batch; 1h:28m:46s remains)
INFO - root - 2019-11-06 20:52:15.126575: step 43650, total loss = 0.20, predict loss = 0.05 (79.5 examples/sec; 0.050 sec/batch; 1h:29m:13s remains)
INFO - root - 2019-11-06 20:52:15.701181: step 43660, total loss = 0.35, predict loss = 0.10 (83.2 examples/sec; 0.048 sec/batch; 1h:25m:12s remains)
INFO - root - 2019-11-06 20:52:16.264822: step 43670, total loss = 0.22, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:32m:18s remains)
INFO - root - 2019-11-06 20:52:16.826098: step 43680, total loss = 0.21, predict loss = 0.05 (80.5 examples/sec; 0.050 sec/batch; 1h:27m:59s remains)
INFO - root - 2019-11-06 20:52:17.366331: step 43690, total loss = 0.27, predict loss = 0.07 (94.6 examples/sec; 0.042 sec/batch; 1h:14m:53s remains)
INFO - root - 2019-11-06 20:52:17.838944: step 43700, total loss = 0.24, predict loss = 0.06 (92.0 examples/sec; 0.043 sec/batch; 1h:17m:03s remains)
INFO - root - 2019-11-06 20:52:18.298605: step 43710, total loss = 0.19, predict loss = 0.04 (105.8 examples/sec; 0.038 sec/batch; 1h:06m:56s remains)
INFO - root - 2019-11-06 20:52:19.269211: step 43720, total loss = 0.24, predict loss = 0.05 (72.6 examples/sec; 0.055 sec/batch; 1h:37m:35s remains)
INFO - root - 2019-11-06 20:52:19.951886: step 43730, total loss = 0.27, predict loss = 0.08 (73.7 examples/sec; 0.054 sec/batch; 1h:36m:05s remains)
INFO - root - 2019-11-06 20:52:20.547083: step 43740, total loss = 0.36, predict loss = 0.12 (74.2 examples/sec; 0.054 sec/batch; 1h:35m:31s remains)
INFO - root - 2019-11-06 20:52:21.113790: step 43750, total loss = 0.18, predict loss = 0.04 (77.3 examples/sec; 0.052 sec/batch; 1h:31m:36s remains)
INFO - root - 2019-11-06 20:52:21.692212: step 43760, total loss = 0.19, predict loss = 0.05 (77.5 examples/sec; 0.052 sec/batch; 1h:31m:21s remains)
INFO - root - 2019-11-06 20:52:22.282431: step 43770, total loss = 0.20, predict loss = 0.05 (79.6 examples/sec; 0.050 sec/batch; 1h:28m:59s remains)
INFO - root - 2019-11-06 20:52:22.868600: step 43780, total loss = 0.26, predict loss = 0.07 (80.4 examples/sec; 0.050 sec/batch; 1h:28m:01s remains)
INFO - root - 2019-11-06 20:52:23.441296: step 43790, total loss = 0.22, predict loss = 0.06 (80.1 examples/sec; 0.050 sec/batch; 1h:28m:22s remains)
INFO - root - 2019-11-06 20:52:24.037439: step 43800, total loss = 0.17, predict loss = 0.04 (77.0 examples/sec; 0.052 sec/batch; 1h:31m:57s remains)
INFO - root - 2019-11-06 20:52:24.634190: step 43810, total loss = 0.22, predict loss = 0.06 (75.8 examples/sec; 0.053 sec/batch; 1h:33m:26s remains)
INFO - root - 2019-11-06 20:52:25.217546: step 43820, total loss = 0.32, predict loss = 0.09 (74.2 examples/sec; 0.054 sec/batch; 1h:35m:22s remains)
INFO - root - 2019-11-06 20:52:25.809276: step 43830, total loss = 0.25, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:28m:44s remains)
INFO - root - 2019-11-06 20:52:26.319175: step 43840, total loss = 0.30, predict loss = 0.08 (103.0 examples/sec; 0.039 sec/batch; 1h:08m:43s remains)
INFO - root - 2019-11-06 20:52:26.796557: step 43850, total loss = 0.30, predict loss = 0.08 (97.4 examples/sec; 0.041 sec/batch; 1h:12m:40s remains)
INFO - root - 2019-11-06 20:52:27.257608: step 43860, total loss = 0.24, predict loss = 0.06 (91.2 examples/sec; 0.044 sec/batch; 1h:17m:36s remains)
INFO - root - 2019-11-06 20:52:28.360427: step 43870, total loss = 0.18, predict loss = 0.05 (56.6 examples/sec; 0.071 sec/batch; 2h:05m:05s remains)
INFO - root - 2019-11-06 20:52:28.998419: step 43880, total loss = 0.20, predict loss = 0.05 (78.5 examples/sec; 0.051 sec/batch; 1h:30m:07s remains)
INFO - root - 2019-11-06 20:52:29.592608: step 43890, total loss = 0.19, predict loss = 0.05 (78.6 examples/sec; 0.051 sec/batch; 1h:30m:00s remains)
INFO - root - 2019-11-06 20:52:30.170791: step 43900, total loss = 0.15, predict loss = 0.03 (78.5 examples/sec; 0.051 sec/batch; 1h:30m:08s remains)
INFO - root - 2019-11-06 20:52:30.786828: step 43910, total loss = 0.21, predict loss = 0.05 (80.8 examples/sec; 0.050 sec/batch; 1h:27m:33s remains)
INFO - root - 2019-11-06 20:52:31.361837: step 43920, total loss = 0.26, predict loss = 0.07 (75.0 examples/sec; 0.053 sec/batch; 1h:34m:15s remains)
INFO - root - 2019-11-06 20:52:31.972867: step 43930, total loss = 0.24, predict loss = 0.07 (77.7 examples/sec; 0.051 sec/batch; 1h:31m:00s remains)
INFO - root - 2019-11-06 20:52:32.547972: step 43940, total loss = 0.39, predict loss = 0.10 (80.3 examples/sec; 0.050 sec/batch; 1h:28m:02s remains)
INFO - root - 2019-11-06 20:52:33.115488: step 43950, total loss = 0.18, predict loss = 0.04 (74.5 examples/sec; 0.054 sec/batch; 1h:34m:51s remains)
INFO - root - 2019-11-06 20:52:33.688182: step 43960, total loss = 0.20, predict loss = 0.05 (78.7 examples/sec; 0.051 sec/batch; 1h:29m:48s remains)
INFO - root - 2019-11-06 20:52:34.281836: step 43970, total loss = 0.27, predict loss = 0.08 (75.8 examples/sec; 0.053 sec/batch; 1h:33m:18s remains)
INFO - root - 2019-11-06 20:52:34.862861: step 43980, total loss = 0.18, predict loss = 0.05 (76.7 examples/sec; 0.052 sec/batch; 1h:32m:05s remains)
INFO - root - 2019-11-06 20:52:35.350662: step 43990, total loss = 0.20, predict loss = 0.06 (100.1 examples/sec; 0.040 sec/batch; 1h:10m:34s remains)
INFO - root - 2019-11-06 20:52:35.811977: step 44000, total loss = 0.22, predict loss = 0.05 (100.0 examples/sec; 0.040 sec/batch; 1h:10m:39s remains)
INFO - root - 2019-11-06 20:52:36.732395: step 44010, total loss = 0.32, predict loss = 0.10 (8.4 examples/sec; 0.477 sec/batch; 14h:03m:04s remains)
INFO - root - 2019-11-06 20:52:37.390314: step 44020, total loss = 0.22, predict loss = 0.05 (63.0 examples/sec; 0.064 sec/batch; 1h:52m:11s remains)
INFO - root - 2019-11-06 20:52:38.002488: step 44030, total loss = 0.23, predict loss = 0.07 (77.5 examples/sec; 0.052 sec/batch; 1h:31m:06s remains)
INFO - root - 2019-11-06 20:52:38.592106: step 44040, total loss = 0.32, predict loss = 0.10 (76.7 examples/sec; 0.052 sec/batch; 1h:32m:08s remains)
INFO - root - 2019-11-06 20:52:39.177907: step 44050, total loss = 0.26, predict loss = 0.07 (78.3 examples/sec; 0.051 sec/batch; 1h:30m:15s remains)
INFO - root - 2019-11-06 20:52:39.761851: step 44060, total loss = 0.20, predict loss = 0.05 (80.5 examples/sec; 0.050 sec/batch; 1h:27m:45s remains)
INFO - root - 2019-11-06 20:52:40.336499: step 44070, total loss = 0.23, predict loss = 0.05 (74.5 examples/sec; 0.054 sec/batch; 1h:34m:43s remains)
INFO - root - 2019-11-06 20:52:40.910979: step 44080, total loss = 0.17, predict loss = 0.05 (80.7 examples/sec; 0.050 sec/batch; 1h:27m:32s remains)
INFO - root - 2019-11-06 20:52:41.498018: step 44090, total loss = 0.35, predict loss = 0.11 (79.2 examples/sec; 0.051 sec/batch; 1h:29m:09s remains)
INFO - root - 2019-11-06 20:52:42.071800: step 44100, total loss = 0.18, predict loss = 0.04 (77.0 examples/sec; 0.052 sec/batch; 1h:31m:44s remains)
INFO - root - 2019-11-06 20:52:42.646805: step 44110, total loss = 0.19, predict loss = 0.05 (81.5 examples/sec; 0.049 sec/batch; 1h:26m:40s remains)
INFO - root - 2019-11-06 20:52:43.226723: step 44120, total loss = 0.20, predict loss = 0.05 (77.3 examples/sec; 0.052 sec/batch; 1h:31m:18s remains)
INFO - root - 2019-11-06 20:52:43.809257: step 44130, total loss = 0.29, predict loss = 0.09 (81.7 examples/sec; 0.049 sec/batch; 1h:26m:21s remains)
INFO - root - 2019-11-06 20:52:44.266524: step 44140, total loss = 0.26, predict loss = 0.07 (98.3 examples/sec; 0.041 sec/batch; 1h:11m:48s remains)
INFO - root - 2019-11-06 20:52:44.707325: step 44150, total loss = 0.19, predict loss = 0.05 (97.7 examples/sec; 0.041 sec/batch; 1h:12m:14s remains)
INFO - root - 2019-11-06 20:52:45.637098: step 44160, total loss = 0.21, predict loss = 0.06 (81.7 examples/sec; 0.049 sec/batch; 1h:26m:19s remains)
INFO - root - 2019-11-06 20:52:46.401851: step 44170, total loss = 0.17, predict loss = 0.04 (53.8 examples/sec; 0.074 sec/batch; 2h:11m:07s remains)
INFO - root - 2019-11-06 20:52:47.019150: step 44180, total loss = 0.20, predict loss = 0.05 (83.1 examples/sec; 0.048 sec/batch; 1h:24m:54s remains)
INFO - root - 2019-11-06 20:52:47.597813: step 44190, total loss = 0.25, predict loss = 0.06 (81.6 examples/sec; 0.049 sec/batch; 1h:26m:28s remains)
INFO - root - 2019-11-06 20:52:48.171762: step 44200, total loss = 0.24, predict loss = 0.06 (81.1 examples/sec; 0.049 sec/batch; 1h:27m:00s remains)
INFO - root - 2019-11-06 20:52:48.761477: step 44210, total loss = 0.31, predict loss = 0.09 (77.2 examples/sec; 0.052 sec/batch; 1h:31m:23s remains)
INFO - root - 2019-11-06 20:52:49.334671: step 44220, total loss = 0.25, predict loss = 0.07 (80.8 examples/sec; 0.050 sec/batch; 1h:27m:18s remains)
INFO - root - 2019-11-06 20:52:49.903021: step 44230, total loss = 0.24, predict loss = 0.08 (78.4 examples/sec; 0.051 sec/batch; 1h:29m:58s remains)
INFO - root - 2019-11-06 20:52:50.472852: step 44240, total loss = 0.27, predict loss = 0.09 (73.5 examples/sec; 0.054 sec/batch; 1h:35m:56s remains)
INFO - root - 2019-11-06 20:52:51.059321: step 44250, total loss = 0.17, predict loss = 0.04 (79.7 examples/sec; 0.050 sec/batch; 1h:28m:28s remains)
INFO - root - 2019-11-06 20:52:51.636199: step 44260, total loss = 0.28, predict loss = 0.07 (76.6 examples/sec; 0.052 sec/batch; 1h:31m:58s remains)
INFO - root - 2019-11-06 20:52:52.221591: step 44270, total loss = 0.23, predict loss = 0.06 (80.4 examples/sec; 0.050 sec/batch; 1h:27m:38s remains)
INFO - root - 2019-11-06 20:52:52.776428: step 44280, total loss = 0.28, predict loss = 0.08 (93.4 examples/sec; 0.043 sec/batch; 1h:15m:25s remains)
INFO - root - 2019-11-06 20:52:53.253687: step 44290, total loss = 0.17, predict loss = 0.04 (98.4 examples/sec; 0.041 sec/batch; 1h:11m:35s remains)
INFO - root - 2019-11-06 20:52:53.705193: step 44300, total loss = 0.23, predict loss = 0.06 (97.8 examples/sec; 0.041 sec/batch; 1h:12m:04s remains)
INFO - root - 2019-11-06 20:52:54.637774: step 44310, total loss = 0.20, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:31m:18s remains)
INFO - root - 2019-11-06 20:52:55.289739: step 44320, total loss = 0.19, predict loss = 0.05 (72.4 examples/sec; 0.055 sec/batch; 1h:37m:15s remains)
INFO - root - 2019-11-06 20:52:55.937868: step 44330, total loss = 0.32, predict loss = 0.08 (71.8 examples/sec; 0.056 sec/batch; 1h:38m:04s remains)
INFO - root - 2019-11-06 20:52:56.535802: step 44340, total loss = 0.29, predict loss = 0.07 (76.4 examples/sec; 0.052 sec/batch; 1h:32m:10s remains)
INFO - root - 2019-11-06 20:52:57.109085: step 44350, total loss = 0.22, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:28m:16s remains)
INFO - root - 2019-11-06 20:52:57.685025: step 44360, total loss = 0.16, predict loss = 0.04 (82.1 examples/sec; 0.049 sec/batch; 1h:25m:48s remains)
INFO - root - 2019-11-06 20:52:58.275629: step 44370, total loss = 0.19, predict loss = 0.05 (73.1 examples/sec; 0.055 sec/batch; 1h:36m:19s remains)
INFO - root - 2019-11-06 20:52:58.863616: step 44380, total loss = 0.17, predict loss = 0.04 (81.7 examples/sec; 0.049 sec/batch; 1h:26m:13s remains)
INFO - root - 2019-11-06 20:52:59.441277: step 44390, total loss = 0.29, predict loss = 0.08 (78.9 examples/sec; 0.051 sec/batch; 1h:29m:12s remains)
INFO - root - 2019-11-06 20:53:00.008364: step 44400, total loss = 0.24, predict loss = 0.07 (78.0 examples/sec; 0.051 sec/batch; 1h:30m:18s remains)
INFO - root - 2019-11-06 20:53:00.626515: step 44410, total loss = 0.15, predict loss = 0.04 (81.0 examples/sec; 0.049 sec/batch; 1h:26m:51s remains)
INFO - root - 2019-11-06 20:53:01.215712: step 44420, total loss = 0.17, predict loss = 0.04 (79.8 examples/sec; 0.050 sec/batch; 1h:28m:13s remains)
INFO - root - 2019-11-06 20:53:01.754815: step 44430, total loss = 0.25, predict loss = 0.07 (93.4 examples/sec; 0.043 sec/batch; 1h:15m:19s remains)
INFO - root - 2019-11-06 20:53:02.213424: step 44440, total loss = 0.25, predict loss = 0.07 (98.1 examples/sec; 0.041 sec/batch; 1h:11m:45s remains)
INFO - root - 2019-11-06 20:53:02.702106: step 44450, total loss = 0.28, predict loss = 0.07 (89.2 examples/sec; 0.045 sec/batch; 1h:18m:52s remains)
INFO - root - 2019-11-06 20:53:03.744093: step 44460, total loss = 0.14, predict loss = 0.04 (52.6 examples/sec; 0.076 sec/batch; 2h:13m:42s remains)
INFO - root - 2019-11-06 20:53:04.387788: step 44470, total loss = 0.23, predict loss = 0.06 (74.7 examples/sec; 0.054 sec/batch; 1h:34m:13s remains)
INFO - root - 2019-11-06 20:53:04.962465: step 44480, total loss = 0.20, predict loss = 0.05 (78.5 examples/sec; 0.051 sec/batch; 1h:29m:37s remains)
INFO - root - 2019-11-06 20:53:05.554134: step 44490, total loss = 0.25, predict loss = 0.06 (77.0 examples/sec; 0.052 sec/batch; 1h:31m:22s remains)
INFO - root - 2019-11-06 20:53:06.136932: step 44500, total loss = 0.14, predict loss = 0.04 (79.1 examples/sec; 0.051 sec/batch; 1h:28m:56s remains)
INFO - root - 2019-11-06 20:53:06.722954: step 44510, total loss = 0.27, predict loss = 0.07 (75.3 examples/sec; 0.053 sec/batch; 1h:33m:26s remains)
INFO - root - 2019-11-06 20:53:07.286727: step 44520, total loss = 0.28, predict loss = 0.07 (77.9 examples/sec; 0.051 sec/batch; 1h:30m:15s remains)
INFO - root - 2019-11-06 20:53:07.870833: step 44530, total loss = 0.20, predict loss = 0.05 (81.6 examples/sec; 0.049 sec/batch; 1h:26m:12s remains)
INFO - root - 2019-11-06 20:53:08.447835: step 44540, total loss = 0.20, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:30m:11s remains)
INFO - root - 2019-11-06 20:53:09.020388: step 44550, total loss = 0.28, predict loss = 0.08 (81.4 examples/sec; 0.049 sec/batch; 1h:26m:21s remains)
INFO - root - 2019-11-06 20:53:09.600659: step 44560, total loss = 0.27, predict loss = 0.06 (83.3 examples/sec; 0.048 sec/batch; 1h:24m:25s remains)
INFO - root - 2019-11-06 20:53:10.183897: step 44570, total loss = 0.20, predict loss = 0.05 (76.8 examples/sec; 0.052 sec/batch; 1h:31m:28s remains)
INFO - root - 2019-11-06 20:53:10.701561: step 44580, total loss = 0.15, predict loss = 0.04 (102.1 examples/sec; 0.039 sec/batch; 1h:08m:50s remains)
INFO - root - 2019-11-06 20:53:11.144254: step 44590, total loss = 0.20, predict loss = 0.05 (96.1 examples/sec; 0.042 sec/batch; 1h:13m:06s remains)
INFO - root - 2019-11-06 20:53:11.603509: step 44600, total loss = 0.30, predict loss = 0.08 (91.5 examples/sec; 0.044 sec/batch; 1h:16m:46s remains)
INFO - root - 2019-11-06 20:53:12.658917: step 44610, total loss = 0.27, predict loss = 0.07 (65.7 examples/sec; 0.061 sec/batch; 1h:47m:01s remains)
INFO - root - 2019-11-06 20:53:13.328814: step 44620, total loss = 0.25, predict loss = 0.06 (64.2 examples/sec; 0.062 sec/batch; 1h:49m:29s remains)
INFO - root - 2019-11-06 20:53:13.928107: step 44630, total loss = 0.23, predict loss = 0.06 (75.8 examples/sec; 0.053 sec/batch; 1h:32m:38s remains)
INFO - root - 2019-11-06 20:53:14.501963: step 44640, total loss = 0.24, predict loss = 0.06 (77.8 examples/sec; 0.051 sec/batch; 1h:30m:19s remains)
INFO - root - 2019-11-06 20:53:15.101933: step 44650, total loss = 0.30, predict loss = 0.08 (79.4 examples/sec; 0.050 sec/batch; 1h:28m:27s remains)
INFO - root - 2019-11-06 20:53:15.676997: step 44660, total loss = 0.28, predict loss = 0.07 (80.7 examples/sec; 0.050 sec/batch; 1h:27m:01s remains)
INFO - root - 2019-11-06 20:53:16.250773: step 44670, total loss = 0.16, predict loss = 0.04 (78.2 examples/sec; 0.051 sec/batch; 1h:29m:47s remains)
INFO - root - 2019-11-06 20:53:16.822810: step 44680, total loss = 0.13, predict loss = 0.03 (79.8 examples/sec; 0.050 sec/batch; 1h:27m:56s remains)
INFO - root - 2019-11-06 20:53:17.414805: step 44690, total loss = 0.39, predict loss = 0.11 (79.7 examples/sec; 0.050 sec/batch; 1h:28m:07s remains)
INFO - root - 2019-11-06 20:53:18.007872: step 44700, total loss = 0.25, predict loss = 0.05 (82.7 examples/sec; 0.048 sec/batch; 1h:24m:53s remains)
INFO - root - 2019-11-06 20:53:18.602567: step 44710, total loss = 0.25, predict loss = 0.07 (72.2 examples/sec; 0.055 sec/batch; 1h:37m:12s remains)
INFO - root - 2019-11-06 20:53:19.176250: step 44720, total loss = 0.27, predict loss = 0.08 (78.9 examples/sec; 0.051 sec/batch; 1h:28m:55s remains)
INFO - root - 2019-11-06 20:53:19.673417: step 44730, total loss = 0.23, predict loss = 0.06 (102.8 examples/sec; 0.039 sec/batch; 1h:08m:17s remains)
INFO - root - 2019-11-06 20:53:20.129191: step 44740, total loss = 0.17, predict loss = 0.04 (97.0 examples/sec; 0.041 sec/batch; 1h:12m:22s remains)
INFO - root - 2019-11-06 20:53:20.579261: step 44750, total loss = 0.25, predict loss = 0.07 (92.9 examples/sec; 0.043 sec/batch; 1h:15m:32s remains)
INFO - root - 2019-11-06 20:53:21.688888: step 44760, total loss = 0.27, predict loss = 0.08 (61.9 examples/sec; 0.065 sec/batch; 1h:53m:16s remains)
INFO - root - 2019-11-06 20:53:22.404345: step 44770, total loss = 0.26, predict loss = 0.07 (67.2 examples/sec; 0.059 sec/batch; 1h:44m:19s remains)
INFO - root - 2019-11-06 20:53:23.006433: step 44780, total loss = 0.22, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:28m:26s remains)
INFO - root - 2019-11-06 20:53:23.579018: step 44790, total loss = 0.29, predict loss = 0.08 (79.2 examples/sec; 0.051 sec/batch; 1h:28m:36s remains)
INFO - root - 2019-11-06 20:53:24.151952: step 44800, total loss = 0.17, predict loss = 0.04 (79.8 examples/sec; 0.050 sec/batch; 1h:27m:54s remains)
INFO - root - 2019-11-06 20:53:24.735813: step 44810, total loss = 0.31, predict loss = 0.09 (80.7 examples/sec; 0.050 sec/batch; 1h:26m:51s remains)
INFO - root - 2019-11-06 20:53:25.309105: step 44820, total loss = 0.26, predict loss = 0.06 (77.6 examples/sec; 0.052 sec/batch; 1h:30m:22s remains)
INFO - root - 2019-11-06 20:53:25.898165: step 44830, total loss = 0.21, predict loss = 0.05 (76.2 examples/sec; 0.053 sec/batch; 1h:32m:02s remains)
INFO - root - 2019-11-06 20:53:26.478392: step 44840, total loss = 0.22, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 1h:30m:29s remains)
INFO - root - 2019-11-06 20:53:27.081513: step 44850, total loss = 0.27, predict loss = 0.07 (74.3 examples/sec; 0.054 sec/batch; 1h:34m:20s remains)
INFO - root - 2019-11-06 20:53:27.657678: step 44860, total loss = 0.35, predict loss = 0.10 (75.3 examples/sec; 0.053 sec/batch; 1h:33m:03s remains)
INFO - root - 2019-11-06 20:53:28.233064: step 44870, total loss = 0.23, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:30m:33s remains)
INFO - root - 2019-11-06 20:53:28.713456: step 44880, total loss = 0.19, predict loss = 0.04 (91.6 examples/sec; 0.044 sec/batch; 1h:16m:30s remains)
INFO - root - 2019-11-06 20:53:29.194492: step 44890, total loss = 0.25, predict loss = 0.06 (89.4 examples/sec; 0.045 sec/batch; 1h:18m:23s remains)
INFO - root - 2019-11-06 20:53:30.098502: step 44900, total loss = 0.17, predict loss = 0.04 (78.9 examples/sec; 0.051 sec/batch; 1h:28m:45s remains)
INFO - root - 2019-11-06 20:53:30.837571: step 44910, total loss = 0.21, predict loss = 0.05 (58.7 examples/sec; 0.068 sec/batch; 1h:59m:17s remains)
INFO - root - 2019-11-06 20:53:31.497814: step 44920, total loss = 0.22, predict loss = 0.06 (72.1 examples/sec; 0.055 sec/batch; 1h:37m:07s remains)
INFO - root - 2019-11-06 20:53:32.106197: step 44930, total loss = 0.20, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:29m:40s remains)
INFO - root - 2019-11-06 20:53:32.677276: step 44940, total loss = 0.24, predict loss = 0.06 (75.3 examples/sec; 0.053 sec/batch; 1h:32m:57s remains)
INFO - root - 2019-11-06 20:53:33.247236: step 44950, total loss = 0.21, predict loss = 0.06 (81.8 examples/sec; 0.049 sec/batch; 1h:25m:37s remains)
INFO - root - 2019-11-06 20:53:33.808401: step 44960, total loss = 0.26, predict loss = 0.07 (79.0 examples/sec; 0.051 sec/batch; 1h:28m:36s remains)
INFO - root - 2019-11-06 20:53:34.407575: step 44970, total loss = 0.21, predict loss = 0.06 (80.3 examples/sec; 0.050 sec/batch; 1h:27m:11s remains)
INFO - root - 2019-11-06 20:53:34.981071: step 44980, total loss = 0.32, predict loss = 0.09 (77.2 examples/sec; 0.052 sec/batch; 1h:30m:40s remains)
INFO - root - 2019-11-06 20:53:35.551901: step 44990, total loss = 0.27, predict loss = 0.07 (82.0 examples/sec; 0.049 sec/batch; 1h:25m:23s remains)
INFO - root - 2019-11-06 20:53:36.130416: step 45000, total loss = 0.25, predict loss = 0.07 (79.6 examples/sec; 0.050 sec/batch; 1h:27m:59s remains)
INFO - root - 2019-11-06 20:53:37.334023: step 45010, total loss = 0.18, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:28m:16s remains)
INFO - root - 2019-11-06 20:53:37.912035: step 45020, total loss = 0.21, predict loss = 0.05 (90.1 examples/sec; 0.044 sec/batch; 1h:17m:39s remains)
INFO - root - 2019-11-06 20:53:38.373920: step 45030, total loss = 0.26, predict loss = 0.07 (93.9 examples/sec; 0.043 sec/batch; 1h:14m:29s remains)
INFO - root - 2019-11-06 20:53:38.827347: step 45040, total loss = 0.30, predict loss = 0.09 (98.0 examples/sec; 0.041 sec/batch; 1h:11m:24s remains)
INFO - root - 2019-11-06 20:53:39.769079: step 45050, total loss = 0.31, predict loss = 0.09 (77.5 examples/sec; 0.052 sec/batch; 1h:30m:15s remains)
INFO - root - 2019-11-06 20:53:40.480130: step 45060, total loss = 0.31, predict loss = 0.08 (66.8 examples/sec; 0.060 sec/batch; 1h:44m:40s remains)
INFO - root - 2019-11-06 20:53:41.079501: step 45070, total loss = 0.27, predict loss = 0.07 (77.8 examples/sec; 0.051 sec/batch; 1h:29m:57s remains)
INFO - root - 2019-11-06 20:53:41.659142: step 45080, total loss = 0.18, predict loss = 0.04 (78.4 examples/sec; 0.051 sec/batch; 1h:29m:10s remains)
INFO - root - 2019-11-06 20:53:42.254888: step 45090, total loss = 0.20, predict loss = 0.05 (82.0 examples/sec; 0.049 sec/batch; 1h:25m:17s remains)
INFO - root - 2019-11-06 20:53:42.814265: step 45100, total loss = 0.22, predict loss = 0.07 (84.9 examples/sec; 0.047 sec/batch; 1h:22m:20s remains)
INFO - root - 2019-11-06 20:53:43.391581: step 45110, total loss = 0.16, predict loss = 0.04 (77.6 examples/sec; 0.052 sec/batch; 1h:30m:04s remains)
INFO - root - 2019-11-06 20:53:43.966375: step 45120, total loss = 0.29, predict loss = 0.08 (79.7 examples/sec; 0.050 sec/batch; 1h:27m:44s remains)
INFO - root - 2019-11-06 20:53:44.549936: step 45130, total loss = 0.16, predict loss = 0.04 (75.5 examples/sec; 0.053 sec/batch; 1h:32m:35s remains)
INFO - root - 2019-11-06 20:53:45.132742: step 45140, total loss = 0.22, predict loss = 0.04 (78.4 examples/sec; 0.051 sec/batch; 1h:29m:12s remains)
INFO - root - 2019-11-06 20:53:45.715019: step 45150, total loss = 0.26, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:28m:22s remains)
INFO - root - 2019-11-06 20:53:46.302228: step 45160, total loss = 0.29, predict loss = 0.08 (77.3 examples/sec; 0.052 sec/batch; 1h:30m:25s remains)
INFO - root - 2019-11-06 20:53:46.881350: step 45170, total loss = 0.33, predict loss = 0.09 (91.8 examples/sec; 0.044 sec/batch; 1h:16m:09s remains)
INFO - root - 2019-11-06 20:53:47.337518: step 45180, total loss = 0.28, predict loss = 0.07 (97.7 examples/sec; 0.041 sec/batch; 1h:11m:32s remains)
INFO - root - 2019-11-06 20:53:47.789151: step 45190, total loss = 0.19, predict loss = 0.05 (95.2 examples/sec; 0.042 sec/batch; 1h:13m:23s remains)
INFO - root - 2019-11-06 20:53:48.774819: step 45200, total loss = 0.19, predict loss = 0.05 (62.3 examples/sec; 0.064 sec/batch; 1h:52m:06s remains)
INFO - root - 2019-11-06 20:53:49.417617: step 45210, total loss = 0.25, predict loss = 0.07 (74.3 examples/sec; 0.054 sec/batch; 1h:34m:04s remains)
INFO - root - 2019-11-06 20:53:50.004055: step 45220, total loss = 0.22, predict loss = 0.06 (80.6 examples/sec; 0.050 sec/batch; 1h:26m:41s remains)
INFO - root - 2019-11-06 20:53:50.576980: step 45230, total loss = 0.21, predict loss = 0.05 (76.5 examples/sec; 0.052 sec/batch; 1h:31m:19s remains)
INFO - root - 2019-11-06 20:53:51.154833: step 45240, total loss = 0.23, predict loss = 0.08 (79.3 examples/sec; 0.050 sec/batch; 1h:28m:05s remains)
INFO - root - 2019-11-06 20:53:51.765196: step 45250, total loss = 0.21, predict loss = 0.06 (77.1 examples/sec; 0.052 sec/batch; 1h:30m:31s remains)
INFO - root - 2019-11-06 20:53:52.340741: step 45260, total loss = 0.25, predict loss = 0.07 (80.9 examples/sec; 0.049 sec/batch; 1h:26m:18s remains)
INFO - root - 2019-11-06 20:53:52.925135: step 45270, total loss = 0.21, predict loss = 0.06 (76.0 examples/sec; 0.053 sec/batch; 1h:31m:52s remains)
INFO - root - 2019-11-06 20:53:53.502718: step 45280, total loss = 0.19, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:28m:17s remains)
INFO - root - 2019-11-06 20:53:54.110237: step 45290, total loss = 0.14, predict loss = 0.04 (76.4 examples/sec; 0.052 sec/batch; 1h:31m:22s remains)
INFO - root - 2019-11-06 20:53:54.695779: step 45300, total loss = 0.30, predict loss = 0.08 (78.6 examples/sec; 0.051 sec/batch; 1h:28m:51s remains)
INFO - root - 2019-11-06 20:53:55.273301: step 45310, total loss = 0.22, predict loss = 0.05 (78.4 examples/sec; 0.051 sec/batch; 1h:29m:00s remains)
INFO - root - 2019-11-06 20:53:55.795818: step 45320, total loss = 0.20, predict loss = 0.05 (92.0 examples/sec; 0.043 sec/batch; 1h:15m:53s remains)
INFO - root - 2019-11-06 20:53:56.284811: step 45330, total loss = 0.29, predict loss = 0.07 (95.5 examples/sec; 0.042 sec/batch; 1h:13m:04s remains)
INFO - root - 2019-11-06 20:53:56.733076: step 45340, total loss = 0.24, predict loss = 0.06 (96.7 examples/sec; 0.041 sec/batch; 1h:12m:10s remains)
INFO - root - 2019-11-06 20:53:57.699702: step 45350, total loss = 0.18, predict loss = 0.05 (75.2 examples/sec; 0.053 sec/batch; 1h:32m:44s remains)
INFO - root - 2019-11-06 20:53:58.295402: step 45360, total loss = 0.31, predict loss = 0.07 (81.1 examples/sec; 0.049 sec/batch; 1h:26m:04s remains)
INFO - root - 2019-11-06 20:53:58.885382: step 45370, total loss = 0.31, predict loss = 0.08 (77.3 examples/sec; 0.052 sec/batch; 1h:30m:15s remains)
INFO - root - 2019-11-06 20:53:59.459067: step 45380, total loss = 0.18, predict loss = 0.04 (74.1 examples/sec; 0.054 sec/batch; 1h:34m:03s remains)
INFO - root - 2019-11-06 20:54:00.044606: step 45390, total loss = 0.25, predict loss = 0.06 (80.8 examples/sec; 0.050 sec/batch; 1h:26m:21s remains)
INFO - root - 2019-11-06 20:54:00.656086: step 45400, total loss = 0.30, predict loss = 0.07 (74.4 examples/sec; 0.054 sec/batch; 1h:33m:41s remains)
INFO - root - 2019-11-06 20:54:01.244631: step 45410, total loss = 0.19, predict loss = 0.05 (74.9 examples/sec; 0.053 sec/batch; 1h:33m:04s remains)
INFO - root - 2019-11-06 20:54:01.821587: step 45420, total loss = 0.28, predict loss = 0.07 (74.3 examples/sec; 0.054 sec/batch; 1h:33m:51s remains)
INFO - root - 2019-11-06 20:54:02.392949: step 45430, total loss = 0.19, predict loss = 0.05 (73.9 examples/sec; 0.054 sec/batch; 1h:34m:20s remains)
INFO - root - 2019-11-06 20:54:02.965312: step 45440, total loss = 0.30, predict loss = 0.08 (76.5 examples/sec; 0.052 sec/batch; 1h:31m:09s remains)
INFO - root - 2019-11-06 20:54:03.563926: step 45450, total loss = 0.20, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:30m:01s remains)
INFO - root - 2019-11-06 20:54:04.137317: step 45460, total loss = 0.41, predict loss = 0.13 (79.5 examples/sec; 0.050 sec/batch; 1h:27m:41s remains)
INFO - root - 2019-11-06 20:54:04.647247: step 45470, total loss = 0.37, predict loss = 0.10 (103.1 examples/sec; 0.039 sec/batch; 1h:07m:34s remains)
INFO - root - 2019-11-06 20:54:05.086033: step 45480, total loss = 0.28, predict loss = 0.08 (96.9 examples/sec; 0.041 sec/batch; 1h:11m:52s remains)
INFO - root - 2019-11-06 20:54:05.562094: step 45490, total loss = 0.14, predict loss = 0.04 (100.3 examples/sec; 0.040 sec/batch; 1h:09m:29s remains)
INFO - root - 2019-11-06 20:54:06.671243: step 45500, total loss = 0.19, predict loss = 0.05 (51.8 examples/sec; 0.077 sec/batch; 2h:14m:26s remains)
INFO - root - 2019-11-06 20:54:07.313244: step 45510, total loss = 0.21, predict loss = 0.05 (75.6 examples/sec; 0.053 sec/batch; 1h:32m:08s remains)
INFO - root - 2019-11-06 20:54:07.894149: step 45520, total loss = 0.23, predict loss = 0.06 (72.8 examples/sec; 0.055 sec/batch; 1h:35m:40s remains)
INFO - root - 2019-11-06 20:54:08.499642: step 45530, total loss = 0.23, predict loss = 0.06 (77.1 examples/sec; 0.052 sec/batch; 1h:30m:18s remains)
INFO - root - 2019-11-06 20:54:09.080154: step 45540, total loss = 0.21, predict loss = 0.05 (77.7 examples/sec; 0.051 sec/batch; 1h:29m:38s remains)
INFO - root - 2019-11-06 20:54:09.656917: step 45550, total loss = 0.21, predict loss = 0.06 (85.2 examples/sec; 0.047 sec/batch; 1h:21m:44s remains)
INFO - root - 2019-11-06 20:54:10.225820: step 45560, total loss = 0.20, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:28m:02s remains)
INFO - root - 2019-11-06 20:54:10.826745: step 45570, total loss = 0.25, predict loss = 0.07 (77.6 examples/sec; 0.052 sec/batch; 1h:29m:44s remains)
INFO - root - 2019-11-06 20:54:11.398565: step 45580, total loss = 0.23, predict loss = 0.06 (80.3 examples/sec; 0.050 sec/batch; 1h:26m:38s remains)
INFO - root - 2019-11-06 20:54:11.986532: step 45590, total loss = 0.31, predict loss = 0.08 (75.1 examples/sec; 0.053 sec/batch; 1h:32m:41s remains)
INFO - root - 2019-11-06 20:54:12.560204: step 45600, total loss = 0.29, predict loss = 0.07 (82.5 examples/sec; 0.049 sec/batch; 1h:24m:24s remains)
INFO - root - 2019-11-06 20:54:13.150537: step 45610, total loss = 0.19, predict loss = 0.05 (81.6 examples/sec; 0.049 sec/batch; 1h:25m:18s remains)
INFO - root - 2019-11-06 20:54:13.633304: step 45620, total loss = 0.24, predict loss = 0.06 (100.5 examples/sec; 0.040 sec/batch; 1h:09m:13s remains)
INFO - root - 2019-11-06 20:54:14.075091: step 45630, total loss = 0.20, predict loss = 0.05 (103.1 examples/sec; 0.039 sec/batch; 1h:07m:29s remains)
INFO - root - 2019-11-06 20:54:14.971037: step 45640, total loss = 0.25, predict loss = 0.08 (8.5 examples/sec; 0.471 sec/batch; 13h:39m:49s remains)
INFO - root - 2019-11-06 20:54:15.668657: step 45650, total loss = 0.34, predict loss = 0.09 (59.4 examples/sec; 0.067 sec/batch; 1h:57m:01s remains)
INFO - root - 2019-11-06 20:54:16.301152: step 45660, total loss = 0.17, predict loss = 0.05 (74.8 examples/sec; 0.053 sec/batch; 1h:32m:59s remains)
INFO - root - 2019-11-06 20:54:16.873227: step 45670, total loss = 0.17, predict loss = 0.04 (79.0 examples/sec; 0.051 sec/batch; 1h:28m:02s remains)
INFO - root - 2019-11-06 20:54:17.450075: step 45680, total loss = 0.18, predict loss = 0.05 (77.5 examples/sec; 0.052 sec/batch; 1h:29m:45s remains)
INFO - root - 2019-11-06 20:54:18.041191: step 45690, total loss = 0.27, predict loss = 0.08 (83.1 examples/sec; 0.048 sec/batch; 1h:23m:43s remains)
INFO - root - 2019-11-06 20:54:18.608811: step 45700, total loss = 0.31, predict loss = 0.09 (73.8 examples/sec; 0.054 sec/batch; 1h:34m:14s remains)
INFO - root - 2019-11-06 20:54:19.181063: step 45710, total loss = 0.23, predict loss = 0.06 (80.7 examples/sec; 0.050 sec/batch; 1h:26m:11s remains)
INFO - root - 2019-11-06 20:54:19.757338: step 45720, total loss = 0.26, predict loss = 0.08 (80.3 examples/sec; 0.050 sec/batch; 1h:26m:35s remains)
INFO - root - 2019-11-06 20:54:20.349002: step 45730, total loss = 0.25, predict loss = 0.06 (79.2 examples/sec; 0.050 sec/batch; 1h:27m:44s remains)
INFO - root - 2019-11-06 20:54:20.917759: step 45740, total loss = 0.22, predict loss = 0.06 (80.5 examples/sec; 0.050 sec/batch; 1h:26m:21s remains)
INFO - root - 2019-11-06 20:54:21.504838: step 45750, total loss = 0.20, predict loss = 0.05 (74.2 examples/sec; 0.054 sec/batch; 1h:33m:37s remains)
INFO - root - 2019-11-06 20:54:22.077153: step 45760, total loss = 0.38, predict loss = 0.13 (89.6 examples/sec; 0.045 sec/batch; 1h:17m:32s remains)
INFO - root - 2019-11-06 20:54:22.558408: step 45770, total loss = 0.16, predict loss = 0.04 (96.6 examples/sec; 0.041 sec/batch; 1h:11m:54s remains)
INFO - root - 2019-11-06 20:54:23.012452: step 45780, total loss = 0.25, predict loss = 0.06 (92.5 examples/sec; 0.043 sec/batch; 1h:15m:04s remains)
INFO - root - 2019-11-06 20:54:23.924387: step 45790, total loss = 0.22, predict loss = 0.06 (80.4 examples/sec; 0.050 sec/batch; 1h:26m:22s remains)
INFO - root - 2019-11-06 20:54:24.595256: step 45800, total loss = 0.16, predict loss = 0.04 (63.6 examples/sec; 0.063 sec/batch; 1h:49m:17s remains)
INFO - root - 2019-11-06 20:54:25.223727: step 45810, total loss = 0.18, predict loss = 0.04 (79.5 examples/sec; 0.050 sec/batch; 1h:27m:22s remains)
INFO - root - 2019-11-06 20:54:25.793684: step 45820, total loss = 0.18, predict loss = 0.04 (78.8 examples/sec; 0.051 sec/batch; 1h:28m:07s remains)
INFO - root - 2019-11-06 20:54:26.378005: step 45830, total loss = 0.20, predict loss = 0.05 (80.3 examples/sec; 0.050 sec/batch; 1h:26m:29s remains)
INFO - root - 2019-11-06 20:54:26.956207: step 45840, total loss = 0.33, predict loss = 0.08 (79.4 examples/sec; 0.050 sec/batch; 1h:27m:27s remains)
INFO - root - 2019-11-06 20:54:27.547723: step 45850, total loss = 0.32, predict loss = 0.09 (78.5 examples/sec; 0.051 sec/batch; 1h:28m:25s remains)
INFO - root - 2019-11-06 20:54:28.127775: step 45860, total loss = 0.19, predict loss = 0.05 (80.5 examples/sec; 0.050 sec/batch; 1h:26m:12s remains)
INFO - root - 2019-11-06 20:54:28.721496: step 45870, total loss = 0.30, predict loss = 0.08 (73.9 examples/sec; 0.054 sec/batch; 1h:33m:52s remains)
INFO - root - 2019-11-06 20:54:29.296505: step 45880, total loss = 0.24, predict loss = 0.07 (79.2 examples/sec; 0.051 sec/batch; 1h:27m:41s remains)
INFO - root - 2019-11-06 20:54:29.893201: step 45890, total loss = 0.17, predict loss = 0.04 (77.1 examples/sec; 0.052 sec/batch; 1h:30m:01s remains)
INFO - root - 2019-11-06 20:54:30.480269: step 45900, total loss = 0.23, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 1h:29m:34s remains)
INFO - root - 2019-11-06 20:54:31.054686: step 45910, total loss = 0.24, predict loss = 0.07 (89.6 examples/sec; 0.045 sec/batch; 1h:17m:29s remains)
INFO - root - 2019-11-06 20:54:31.512380: step 45920, total loss = 0.22, predict loss = 0.06 (97.1 examples/sec; 0.041 sec/batch; 1h:11m:28s remains)
INFO - root - 2019-11-06 20:54:31.982969: step 45930, total loss = 0.25, predict loss = 0.07 (100.3 examples/sec; 0.040 sec/batch; 1h:09m:12s remains)
INFO - root - 2019-11-06 20:54:32.925732: step 45940, total loss = 0.19, predict loss = 0.05 (65.5 examples/sec; 0.061 sec/batch; 1h:45m:56s remains)
INFO - root - 2019-11-06 20:54:33.575595: step 45950, total loss = 0.14, predict loss = 0.04 (69.0 examples/sec; 0.058 sec/batch; 1h:40m:35s remains)
INFO - root - 2019-11-06 20:54:34.171017: step 45960, total loss = 0.30, predict loss = 0.08 (77.8 examples/sec; 0.051 sec/batch; 1h:29m:11s remains)
INFO - root - 2019-11-06 20:54:34.768562: step 45970, total loss = 0.17, predict loss = 0.05 (81.3 examples/sec; 0.049 sec/batch; 1h:25m:20s remains)
INFO - root - 2019-11-06 20:54:35.346602: step 45980, total loss = 0.27, predict loss = 0.07 (79.6 examples/sec; 0.050 sec/batch; 1h:27m:06s remains)
INFO - root - 2019-11-06 20:54:35.918124: step 45990, total loss = 0.15, predict loss = 0.04 (76.1 examples/sec; 0.053 sec/batch; 1h:31m:08s remains)
INFO - root - 2019-11-06 20:54:36.488768: step 46000, total loss = 0.20, predict loss = 0.06 (81.1 examples/sec; 0.049 sec/batch; 1h:25m:32s remains)
INFO - root - 2019-11-06 20:54:37.087739: step 46010, total loss = 0.23, predict loss = 0.06 (76.1 examples/sec; 0.053 sec/batch; 1h:31m:03s remains)
INFO - root - 2019-11-06 20:54:37.674011: step 46020, total loss = 0.12, predict loss = 0.03 (81.8 examples/sec; 0.049 sec/batch; 1h:24m:42s remains)
INFO - root - 2019-11-06 20:54:38.261547: step 46030, total loss = 0.20, predict loss = 0.04 (73.3 examples/sec; 0.055 sec/batch; 1h:34m:34s remains)
INFO - root - 2019-11-06 20:54:38.838593: step 46040, total loss = 0.24, predict loss = 0.06 (80.2 examples/sec; 0.050 sec/batch; 1h:26m:25s remains)
INFO - root - 2019-11-06 20:54:39.442731: step 46050, total loss = 0.27, predict loss = 0.08 (76.5 examples/sec; 0.052 sec/batch; 1h:30m:36s remains)
INFO - root - 2019-11-06 20:54:39.987280: step 46060, total loss = 0.21, predict loss = 0.05 (90.7 examples/sec; 0.044 sec/batch; 1h:16m:26s remains)
INFO - root - 2019-11-06 20:54:40.448629: step 46070, total loss = 0.29, predict loss = 0.08 (96.1 examples/sec; 0.042 sec/batch; 1h:12m:06s remains)
INFO - root - 2019-11-06 20:54:40.894280: step 46080, total loss = 0.30, predict loss = 0.07 (100.5 examples/sec; 0.040 sec/batch; 1h:08m:54s remains)
INFO - root - 2019-11-06 20:54:41.920005: step 46090, total loss = 0.24, predict loss = 0.06 (45.7 examples/sec; 0.087 sec/batch; 2h:31m:31s remains)
INFO - root - 2019-11-06 20:54:42.587573: step 46100, total loss = 0.19, predict loss = 0.05 (70.5 examples/sec; 0.057 sec/batch; 1h:38m:15s remains)
INFO - root - 2019-11-06 20:54:43.192706: step 46110, total loss = 0.21, predict loss = 0.06 (75.7 examples/sec; 0.053 sec/batch; 1h:31m:28s remains)
INFO - root - 2019-11-06 20:54:43.773592: step 46120, total loss = 0.21, predict loss = 0.05 (76.9 examples/sec; 0.052 sec/batch; 1h:30m:06s remains)
INFO - root - 2019-11-06 20:54:44.376748: step 46130, total loss = 0.20, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 1h:29m:17s remains)
INFO - root - 2019-11-06 20:54:44.943557: step 46140, total loss = 0.21, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:26m:47s remains)
INFO - root - 2019-11-06 20:54:45.523202: step 46150, total loss = 0.20, predict loss = 0.05 (70.3 examples/sec; 0.057 sec/batch; 1h:38m:25s remains)
INFO - root - 2019-11-06 20:54:46.098444: step 46160, total loss = 0.25, predict loss = 0.07 (74.9 examples/sec; 0.053 sec/batch; 1h:32m:25s remains)
INFO - root - 2019-11-06 20:54:46.690363: step 46170, total loss = 0.23, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 1h:28m:27s remains)
INFO - root - 2019-11-06 20:54:47.277588: step 46180, total loss = 0.24, predict loss = 0.07 (77.8 examples/sec; 0.051 sec/batch; 1h:28m:55s remains)
INFO - root - 2019-11-06 20:54:47.865382: step 46190, total loss = 0.16, predict loss = 0.04 (74.9 examples/sec; 0.053 sec/batch; 1h:32m:21s remains)
INFO - root - 2019-11-06 20:54:48.433099: step 46200, total loss = 0.25, predict loss = 0.07 (79.5 examples/sec; 0.050 sec/batch; 1h:27m:00s remains)
INFO - root - 2019-11-06 20:54:48.966542: step 46210, total loss = 0.15, predict loss = 0.03 (100.3 examples/sec; 0.040 sec/batch; 1h:08m:59s remains)
INFO - root - 2019-11-06 20:54:49.413329: step 46220, total loss = 0.24, predict loss = 0.07 (90.7 examples/sec; 0.044 sec/batch; 1h:16m:16s remains)
INFO - root - 2019-11-06 20:54:49.863376: step 46230, total loss = 0.24, predict loss = 0.06 (98.8 examples/sec; 0.040 sec/batch; 1h:10m:02s remains)
INFO - root - 2019-11-06 20:54:50.938114: step 46240, total loss = 0.18, predict loss = 0.04 (58.7 examples/sec; 0.068 sec/batch; 1h:57m:45s remains)
INFO - root - 2019-11-06 20:54:51.581679: step 46250, total loss = 0.23, predict loss = 0.06 (78.2 examples/sec; 0.051 sec/batch; 1h:28m:25s remains)
INFO - root - 2019-11-06 20:54:52.161290: step 46260, total loss = 0.28, predict loss = 0.07 (74.5 examples/sec; 0.054 sec/batch; 1h:32m:46s remains)
INFO - root - 2019-11-06 20:54:52.750516: step 46270, total loss = 0.25, predict loss = 0.07 (74.4 examples/sec; 0.054 sec/batch; 1h:32m:57s remains)
INFO - root - 2019-11-06 20:54:53.336701: step 46280, total loss = 0.20, predict loss = 0.05 (74.7 examples/sec; 0.054 sec/batch; 1h:32m:31s remains)
INFO - root - 2019-11-06 20:54:53.918912: step 46290, total loss = 0.33, predict loss = 0.09 (73.2 examples/sec; 0.055 sec/batch; 1h:34m:26s remains)
INFO - root - 2019-11-06 20:54:54.496781: step 46300, total loss = 0.20, predict loss = 0.05 (81.7 examples/sec; 0.049 sec/batch; 1h:24m:39s remains)
INFO - root - 2019-11-06 20:54:55.075493: step 46310, total loss = 0.25, predict loss = 0.07 (76.5 examples/sec; 0.052 sec/batch; 1h:30m:24s remains)
INFO - root - 2019-11-06 20:54:55.654971: step 46320, total loss = 0.22, predict loss = 0.05 (77.8 examples/sec; 0.051 sec/batch; 1h:28m:47s remains)
INFO - root - 2019-11-06 20:54:56.252439: step 46330, total loss = 0.23, predict loss = 0.05 (76.0 examples/sec; 0.053 sec/batch; 1h:30m:59s remains)
INFO - root - 2019-11-06 20:54:56.825180: step 46340, total loss = 0.20, predict loss = 0.05 (82.5 examples/sec; 0.048 sec/batch; 1h:23m:45s remains)
INFO - root - 2019-11-06 20:54:57.398552: step 46350, total loss = 0.26, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:26m:43s remains)
INFO - root - 2019-11-06 20:54:57.895775: step 46360, total loss = 0.18, predict loss = 0.05 (100.0 examples/sec; 0.040 sec/batch; 1h:09m:03s remains)
INFO - root - 2019-11-06 20:54:58.381838: step 46370, total loss = 0.18, predict loss = 0.05 (98.8 examples/sec; 0.040 sec/batch; 1h:09m:54s remains)
INFO - root - 2019-11-06 20:54:58.828110: step 46380, total loss = 0.24, predict loss = 0.07 (97.4 examples/sec; 0.041 sec/batch; 1h:10m:55s remains)
INFO - root - 2019-11-06 20:54:59.991642: step 46390, total loss = 0.18, predict loss = 0.04 (46.3 examples/sec; 0.086 sec/batch; 2h:29m:16s remains)
INFO - root - 2019-11-06 20:55:00.687059: step 46400, total loss = 0.17, predict loss = 0.04 (72.8 examples/sec; 0.055 sec/batch; 1h:34m:53s remains)
INFO - root - 2019-11-06 20:55:01.318191: step 46410, total loss = 0.24, predict loss = 0.06 (75.6 examples/sec; 0.053 sec/batch; 1h:31m:21s remains)
INFO - root - 2019-11-06 20:55:01.903715: step 46420, total loss = 0.42, predict loss = 0.11 (71.0 examples/sec; 0.056 sec/batch; 1h:37m:16s remains)
INFO - root - 2019-11-06 20:55:02.468152: step 46430, total loss = 0.24, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:27m:31s remains)
INFO - root - 2019-11-06 20:55:03.035454: step 46440, total loss = 0.17, predict loss = 0.05 (76.0 examples/sec; 0.053 sec/batch; 1h:30m:53s remains)
INFO - root - 2019-11-06 20:55:03.627876: step 46450, total loss = 0.25, predict loss = 0.07 (79.3 examples/sec; 0.050 sec/batch; 1h:27m:00s remains)
INFO - root - 2019-11-06 20:55:04.204931: step 46460, total loss = 0.22, predict loss = 0.05 (83.1 examples/sec; 0.048 sec/batch; 1h:23m:02s remains)
INFO - root - 2019-11-06 20:55:04.786966: step 46470, total loss = 0.20, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:27m:32s remains)
INFO - root - 2019-11-06 20:55:05.361545: step 46480, total loss = 0.19, predict loss = 0.05 (75.4 examples/sec; 0.053 sec/batch; 1h:31m:28s remains)
INFO - root - 2019-11-06 20:55:05.950097: step 46490, total loss = 0.21, predict loss = 0.05 (80.3 examples/sec; 0.050 sec/batch; 1h:25m:56s remains)
INFO - root - 2019-11-06 20:55:06.525736: step 46500, total loss = 0.24, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:27m:16s remains)
INFO - root - 2019-11-06 20:55:06.990573: step 46510, total loss = 0.17, predict loss = 0.05 (98.3 examples/sec; 0.041 sec/batch; 1h:10m:12s remains)
INFO - root - 2019-11-06 20:55:07.439662: step 46520, total loss = 0.21, predict loss = 0.05 (93.3 examples/sec; 0.043 sec/batch; 1h:13m:56s remains)
INFO - root - 2019-11-06 20:55:08.375493: step 46530, total loss = 0.18, predict loss = 0.04 (77.4 examples/sec; 0.052 sec/batch; 1h:29m:03s remains)
INFO - root - 2019-11-06 20:55:09.017935: step 46540, total loss = 0.24, predict loss = 0.07 (65.6 examples/sec; 0.061 sec/batch; 1h:45m:10s remains)
INFO - root - 2019-11-06 20:55:09.736628: step 46550, total loss = 0.18, predict loss = 0.05 (63.7 examples/sec; 0.063 sec/batch; 1h:48m:16s remains)
INFO - root - 2019-11-06 20:55:10.369885: step 46560, total loss = 0.22, predict loss = 0.06 (70.8 examples/sec; 0.057 sec/batch; 1h:37m:25s remains)
INFO - root - 2019-11-06 20:55:10.973765: step 46570, total loss = 0.19, predict loss = 0.04 (79.0 examples/sec; 0.051 sec/batch; 1h:27m:14s remains)
INFO - root - 2019-11-06 20:55:11.543932: step 46580, total loss = 0.26, predict loss = 0.07 (73.5 examples/sec; 0.054 sec/batch; 1h:33m:51s remains)
INFO - root - 2019-11-06 20:55:12.131067: step 46590, total loss = 0.20, predict loss = 0.05 (76.5 examples/sec; 0.052 sec/batch; 1h:30m:05s remains)
INFO - root - 2019-11-06 20:55:12.700433: step 46600, total loss = 0.19, predict loss = 0.05 (83.1 examples/sec; 0.048 sec/batch; 1h:22m:58s remains)
INFO - root - 2019-11-06 20:55:13.293310: step 46610, total loss = 0.16, predict loss = 0.04 (76.9 examples/sec; 0.052 sec/batch; 1h:29m:38s remains)
INFO - root - 2019-11-06 20:55:13.868493: step 46620, total loss = 0.14, predict loss = 0.03 (75.7 examples/sec; 0.053 sec/batch; 1h:31m:00s remains)
INFO - root - 2019-11-06 20:55:14.435888: step 46630, total loss = 0.21, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:28m:21s remains)
INFO - root - 2019-11-06 20:55:15.022721: step 46640, total loss = 0.30, predict loss = 0.08 (76.7 examples/sec; 0.052 sec/batch; 1h:29m:52s remains)
INFO - root - 2019-11-06 20:55:15.604995: step 46650, total loss = 0.24, predict loss = 0.06 (85.2 examples/sec; 0.047 sec/batch; 1h:20m:49s remains)
INFO - root - 2019-11-06 20:55:16.066540: step 46660, total loss = 0.12, predict loss = 0.03 (92.1 examples/sec; 0.043 sec/batch; 1h:14m:48s remains)
INFO - root - 2019-11-06 20:55:16.510581: step 46670, total loss = 0.26, predict loss = 0.07 (100.9 examples/sec; 0.040 sec/batch; 1h:08m:17s remains)
INFO - root - 2019-11-06 20:55:17.416967: step 46680, total loss = 0.17, predict loss = 0.04 (77.9 examples/sec; 0.051 sec/batch; 1h:28m:28s remains)
INFO - root - 2019-11-06 20:55:18.088865: step 46690, total loss = 0.25, predict loss = 0.07 (66.6 examples/sec; 0.060 sec/batch; 1h:43m:27s remains)
INFO - root - 2019-11-06 20:55:18.749451: step 46700, total loss = 0.21, predict loss = 0.05 (63.7 examples/sec; 0.063 sec/batch; 1h:48m:10s remains)
INFO - root - 2019-11-06 20:55:19.363636: step 46710, total loss = 0.21, predict loss = 0.05 (76.3 examples/sec; 0.052 sec/batch; 1h:30m:17s remains)
INFO - root - 2019-11-06 20:55:19.936506: step 46720, total loss = 0.19, predict loss = 0.04 (74.3 examples/sec; 0.054 sec/batch; 1h:32m:42s remains)
INFO - root - 2019-11-06 20:55:20.521786: step 46730, total loss = 0.20, predict loss = 0.05 (79.5 examples/sec; 0.050 sec/batch; 1h:26m:38s remains)
INFO - root - 2019-11-06 20:55:21.084840: step 46740, total loss = 0.31, predict loss = 0.07 (82.8 examples/sec; 0.048 sec/batch; 1h:23m:10s remains)
INFO - root - 2019-11-06 20:55:21.667379: step 46750, total loss = 0.16, predict loss = 0.04 (76.5 examples/sec; 0.052 sec/batch; 1h:29m:59s remains)
INFO - root - 2019-11-06 20:55:22.225663: step 46760, total loss = 0.19, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:26m:45s remains)
INFO - root - 2019-11-06 20:55:22.828352: step 46770, total loss = 0.25, predict loss = 0.07 (81.7 examples/sec; 0.049 sec/batch; 1h:24m:12s remains)
INFO - root - 2019-11-06 20:55:23.402390: step 46780, total loss = 0.20, predict loss = 0.05 (78.7 examples/sec; 0.051 sec/batch; 1h:27m:28s remains)
INFO - root - 2019-11-06 20:55:23.968894: step 46790, total loss = 0.18, predict loss = 0.05 (79.8 examples/sec; 0.050 sec/batch; 1h:26m:14s remains)
INFO - root - 2019-11-06 20:55:24.520179: step 46800, total loss = 0.20, predict loss = 0.06 (97.4 examples/sec; 0.041 sec/batch; 1h:10m:37s remains)
INFO - root - 2019-11-06 20:55:24.989724: step 46810, total loss = 0.16, predict loss = 0.04 (99.6 examples/sec; 0.040 sec/batch; 1h:09m:04s remains)
INFO - root - 2019-11-06 20:55:25.424823: step 46820, total loss = 0.16, predict loss = 0.04 (100.7 examples/sec; 0.040 sec/batch; 1h:08m:20s remains)
INFO - root - 2019-11-06 20:55:26.400142: step 46830, total loss = 0.24, predict loss = 0.06 (54.4 examples/sec; 0.074 sec/batch; 2h:06m:28s remains)
INFO - root - 2019-11-06 20:55:27.071992: step 46840, total loss = 0.19, predict loss = 0.05 (76.4 examples/sec; 0.052 sec/batch; 1h:30m:00s remains)
INFO - root - 2019-11-06 20:55:27.694340: step 46850, total loss = 0.22, predict loss = 0.06 (74.8 examples/sec; 0.054 sec/batch; 1h:31m:59s remains)
INFO - root - 2019-11-06 20:55:28.264835: step 46860, total loss = 0.24, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:27m:11s remains)
INFO - root - 2019-11-06 20:55:28.845555: step 46870, total loss = 0.17, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:26m:56s remains)
INFO - root - 2019-11-06 20:55:29.424707: step 46880, total loss = 0.24, predict loss = 0.06 (77.6 examples/sec; 0.052 sec/batch; 1h:28m:36s remains)
INFO - root - 2019-11-06 20:55:30.009768: step 46890, total loss = 0.24, predict loss = 0.06 (77.1 examples/sec; 0.052 sec/batch; 1h:29m:12s remains)
INFO - root - 2019-11-06 20:55:30.610532: step 46900, total loss = 0.22, predict loss = 0.06 (76.2 examples/sec; 0.053 sec/batch; 1h:30m:14s remains)
INFO - root - 2019-11-06 20:55:31.217062: step 46910, total loss = 0.20, predict loss = 0.04 (70.5 examples/sec; 0.057 sec/batch; 1h:37m:28s remains)
INFO - root - 2019-11-06 20:55:31.794118: step 46920, total loss = 0.25, predict loss = 0.07 (78.4 examples/sec; 0.051 sec/batch; 1h:27m:42s remains)
INFO - root - 2019-11-06 20:55:32.392330: step 46930, total loss = 0.15, predict loss = 0.04 (79.8 examples/sec; 0.050 sec/batch; 1h:26m:04s remains)
INFO - root - 2019-11-06 20:55:32.953993: step 46940, total loss = 0.19, predict loss = 0.05 (75.8 examples/sec; 0.053 sec/batch; 1h:30m:36s remains)
INFO - root - 2019-11-06 20:55:33.479554: step 46950, total loss = 0.16, predict loss = 0.04 (99.1 examples/sec; 0.040 sec/batch; 1h:09m:19s remains)
INFO - root - 2019-11-06 20:55:33.931847: step 46960, total loss = 0.19, predict loss = 0.05 (91.0 examples/sec; 0.044 sec/batch; 1h:15m:29s remains)
INFO - root - 2019-11-06 20:55:34.409501: step 46970, total loss = 0.30, predict loss = 0.07 (93.6 examples/sec; 0.043 sec/batch; 1h:13m:23s remains)
INFO - root - 2019-11-06 20:55:35.435910: step 46980, total loss = 0.20, predict loss = 0.05 (46.9 examples/sec; 0.085 sec/batch; 2h:26m:25s remains)
INFO - root - 2019-11-06 20:55:36.080602: step 46990, total loss = 0.18, predict loss = 0.04 (74.4 examples/sec; 0.054 sec/batch; 1h:32m:20s remains)
INFO - root - 2019-11-06 20:55:36.660720: step 47000, total loss = 0.30, predict loss = 0.08 (79.9 examples/sec; 0.050 sec/batch; 1h:25m:57s remains)
INFO - root - 2019-11-06 20:55:37.251164: step 47010, total loss = 0.20, predict loss = 0.05 (74.5 examples/sec; 0.054 sec/batch; 1h:32m:09s remains)
INFO - root - 2019-11-06 20:55:37.819844: step 47020, total loss = 0.46, predict loss = 0.12 (78.2 examples/sec; 0.051 sec/batch; 1h:27m:44s remains)
INFO - root - 2019-11-06 20:55:38.406391: step 47030, total loss = 0.34, predict loss = 0.09 (75.7 examples/sec; 0.053 sec/batch; 1h:30m:43s remains)
INFO - root - 2019-11-06 20:55:38.978098: step 47040, total loss = 0.27, predict loss = 0.07 (78.7 examples/sec; 0.051 sec/batch; 1h:27m:13s remains)
INFO - root - 2019-11-06 20:55:39.565762: step 47050, total loss = 0.18, predict loss = 0.04 (82.9 examples/sec; 0.048 sec/batch; 1h:22m:49s remains)
INFO - root - 2019-11-06 20:55:40.136491: step 47060, total loss = 0.24, predict loss = 0.07 (78.0 examples/sec; 0.051 sec/batch; 1h:28m:01s remains)
INFO - root - 2019-11-06 20:55:40.714275: step 47070, total loss = 0.35, predict loss = 0.10 (79.8 examples/sec; 0.050 sec/batch; 1h:25m:59s remains)
INFO - root - 2019-11-06 20:55:41.278802: step 47080, total loss = 0.16, predict loss = 0.04 (75.3 examples/sec; 0.053 sec/batch; 1h:31m:07s remains)
INFO - root - 2019-11-06 20:55:41.866532: step 47090, total loss = 0.19, predict loss = 0.05 (79.2 examples/sec; 0.051 sec/batch; 1h:26m:40s remains)
INFO - root - 2019-11-06 20:55:42.378173: step 47100, total loss = 0.21, predict loss = 0.05 (102.5 examples/sec; 0.039 sec/batch; 1h:06m:54s remains)
INFO - root - 2019-11-06 20:55:42.827171: step 47110, total loss = 0.34, predict loss = 0.10 (99.5 examples/sec; 0.040 sec/batch; 1h:08m:54s remains)
INFO - root - 2019-11-06 20:55:43.286632: step 47120, total loss = 0.26, predict loss = 0.07 (96.6 examples/sec; 0.041 sec/batch; 1h:11m:01s remains)
INFO - root - 2019-11-06 20:55:44.432379: step 47130, total loss = 0.33, predict loss = 0.09 (55.0 examples/sec; 0.073 sec/batch; 2h:04m:37s remains)
INFO - root - 2019-11-06 20:55:45.094957: step 47140, total loss = 0.24, predict loss = 0.07 (80.0 examples/sec; 0.050 sec/batch; 1h:25m:43s remains)
INFO - root - 2019-11-06 20:55:45.682904: step 47150, total loss = 0.17, predict loss = 0.04 (77.2 examples/sec; 0.052 sec/batch; 1h:28m:51s remains)
INFO - root - 2019-11-06 20:55:46.252641: step 47160, total loss = 0.24, predict loss = 0.07 (77.7 examples/sec; 0.051 sec/batch; 1h:28m:14s remains)
INFO - root - 2019-11-06 20:55:46.857957: step 47170, total loss = 0.26, predict loss = 0.07 (71.6 examples/sec; 0.056 sec/batch; 1h:35m:46s remains)
INFO - root - 2019-11-06 20:55:47.426228: step 47180, total loss = 0.32, predict loss = 0.09 (79.3 examples/sec; 0.050 sec/batch; 1h:26m:24s remains)
INFO - root - 2019-11-06 20:55:47.993015: step 47190, total loss = 0.14, predict loss = 0.04 (79.2 examples/sec; 0.050 sec/batch; 1h:26m:29s remains)
INFO - root - 2019-11-06 20:55:48.585762: step 47200, total loss = 0.21, predict loss = 0.05 (73.7 examples/sec; 0.054 sec/batch; 1h:32m:56s remains)
INFO - root - 2019-11-06 20:55:49.190960: step 47210, total loss = 0.21, predict loss = 0.05 (75.5 examples/sec; 0.053 sec/batch; 1h:30m:45s remains)
INFO - root - 2019-11-06 20:55:49.783073: step 47220, total loss = 0.27, predict loss = 0.08 (82.1 examples/sec; 0.049 sec/batch; 1h:23m:26s remains)
INFO - root - 2019-11-06 20:55:50.363487: step 47230, total loss = 0.28, predict loss = 0.07 (77.1 examples/sec; 0.052 sec/batch; 1h:28m:50s remains)
INFO - root - 2019-11-06 20:55:50.932940: step 47240, total loss = 0.34, predict loss = 0.09 (79.3 examples/sec; 0.050 sec/batch; 1h:26m:21s remains)
INFO - root - 2019-11-06 20:55:51.422852: step 47250, total loss = 0.21, predict loss = 0.05 (100.6 examples/sec; 0.040 sec/batch; 1h:08m:06s remains)
INFO - root - 2019-11-06 20:55:51.861490: step 47260, total loss = 0.19, predict loss = 0.05 (97.9 examples/sec; 0.041 sec/batch; 1h:09m:58s remains)
INFO - root - 2019-11-06 20:55:52.755380: step 47270, total loss = 0.20, predict loss = 0.05 (8.2 examples/sec; 0.486 sec/batch; 13h:52m:54s remains)
INFO - root - 2019-11-06 20:55:53.431237: step 47280, total loss = 0.26, predict loss = 0.07 (62.5 examples/sec; 0.064 sec/batch; 1h:49m:35s remains)
INFO - root - 2019-11-06 20:55:54.085767: step 47290, total loss = 0.24, predict loss = 0.06 (75.7 examples/sec; 0.053 sec/batch; 1h:30m:24s remains)
INFO - root - 2019-11-06 20:55:54.670965: step 47300, total loss = 0.20, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:26m:33s remains)
INFO - root - 2019-11-06 20:55:55.259150: step 47310, total loss = 0.27, predict loss = 0.08 (73.2 examples/sec; 0.055 sec/batch; 1h:33m:30s remains)
INFO - root - 2019-11-06 20:55:55.838833: step 47320, total loss = 0.32, predict loss = 0.08 (79.2 examples/sec; 0.051 sec/batch; 1h:26m:26s remains)
INFO - root - 2019-11-06 20:55:56.427827: step 47330, total loss = 0.20, predict loss = 0.06 (79.4 examples/sec; 0.050 sec/batch; 1h:26m:13s remains)
INFO - root - 2019-11-06 20:55:56.998874: step 47340, total loss = 0.20, predict loss = 0.05 (81.2 examples/sec; 0.049 sec/batch; 1h:24m:18s remains)
INFO - root - 2019-11-06 20:55:57.577338: step 47350, total loss = 0.26, predict loss = 0.07 (78.6 examples/sec; 0.051 sec/batch; 1h:27m:06s remains)
INFO - root - 2019-11-06 20:55:58.150625: step 47360, total loss = 0.25, predict loss = 0.06 (77.6 examples/sec; 0.052 sec/batch; 1h:28m:12s remains)
INFO - root - 2019-11-06 20:55:58.744930: step 47370, total loss = 0.23, predict loss = 0.06 (75.9 examples/sec; 0.053 sec/batch; 1h:30m:08s remains)
INFO - root - 2019-11-06 20:55:59.324448: step 47380, total loss = 0.18, predict loss = 0.04 (77.1 examples/sec; 0.052 sec/batch; 1h:28m:46s remains)
INFO - root - 2019-11-06 20:55:59.897617: step 47390, total loss = 0.17, predict loss = 0.04 (90.1 examples/sec; 0.044 sec/batch; 1h:15m:54s remains)
INFO - root - 2019-11-06 20:56:00.359940: step 47400, total loss = 0.23, predict loss = 0.06 (98.6 examples/sec; 0.041 sec/batch; 1h:09m:21s remains)
INFO - root - 2019-11-06 20:56:00.853477: step 47410, total loss = 0.14, predict loss = 0.03 (96.6 examples/sec; 0.041 sec/batch; 1h:10m:48s remains)
INFO - root - 2019-11-06 20:56:01.765382: step 47420, total loss = 0.23, predict loss = 0.06 (76.9 examples/sec; 0.052 sec/batch; 1h:28m:56s remains)
INFO - root - 2019-11-06 20:56:02.489724: step 47430, total loss = 0.20, predict loss = 0.05 (53.5 examples/sec; 0.075 sec/batch; 2h:07m:55s remains)
INFO - root - 2019-11-06 20:56:03.124515: step 47440, total loss = 0.22, predict loss = 0.06 (79.0 examples/sec; 0.051 sec/batch; 1h:26m:34s remains)
INFO - root - 2019-11-06 20:56:03.711094: step 47450, total loss = 0.20, predict loss = 0.05 (77.6 examples/sec; 0.052 sec/batch; 1h:28m:07s remains)
INFO - root - 2019-11-06 20:56:04.272631: step 47460, total loss = 0.18, predict loss = 0.05 (83.8 examples/sec; 0.048 sec/batch; 1h:21m:35s remains)
INFO - root - 2019-11-06 20:56:04.851198: step 47470, total loss = 0.18, predict loss = 0.05 (77.5 examples/sec; 0.052 sec/batch; 1h:28m:11s remains)
INFO - root - 2019-11-06 20:56:05.429949: step 47480, total loss = 0.23, predict loss = 0.06 (76.1 examples/sec; 0.053 sec/batch; 1h:29m:51s remains)
INFO - root - 2019-11-06 20:56:06.028685: step 47490, total loss = 0.18, predict loss = 0.04 (74.3 examples/sec; 0.054 sec/batch; 1h:32m:01s remains)
INFO - root - 2019-11-06 20:56:06.607840: step 47500, total loss = 0.27, predict loss = 0.07 (83.2 examples/sec; 0.048 sec/batch; 1h:22m:07s remains)
INFO - root - 2019-11-06 20:56:07.167808: step 47510, total loss = 0.18, predict loss = 0.05 (80.4 examples/sec; 0.050 sec/batch; 1h:25m:01s remains)
INFO - root - 2019-11-06 20:56:07.747143: step 47520, total loss = 0.18, predict loss = 0.04 (75.5 examples/sec; 0.053 sec/batch; 1h:30m:29s remains)
INFO - root - 2019-11-06 20:56:08.344232: step 47530, total loss = 0.18, predict loss = 0.04 (78.2 examples/sec; 0.051 sec/batch; 1h:27m:23s remains)
INFO - root - 2019-11-06 20:56:08.899274: step 47540, total loss = 0.23, predict loss = 0.06 (94.1 examples/sec; 0.043 sec/batch; 1h:12m:37s remains)
INFO - root - 2019-11-06 20:56:09.352503: step 47550, total loss = 0.26, predict loss = 0.07 (100.9 examples/sec; 0.040 sec/batch; 1h:07m:42s remains)
INFO - root - 2019-11-06 20:56:09.818114: step 47560, total loss = 0.21, predict loss = 0.05 (96.0 examples/sec; 0.042 sec/batch; 1h:11m:08s remains)
INFO - root - 2019-11-06 20:56:10.780627: step 47570, total loss = 0.33, predict loss = 0.09 (70.8 examples/sec; 0.056 sec/batch; 1h:36m:25s remains)
INFO - root - 2019-11-06 20:56:11.425801: step 47580, total loss = 0.29, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:25m:43s remains)
INFO - root - 2019-11-06 20:56:12.004585: step 47590, total loss = 0.27, predict loss = 0.07 (77.2 examples/sec; 0.052 sec/batch; 1h:28m:24s remains)
INFO - root - 2019-11-06 20:56:12.582488: step 47600, total loss = 0.17, predict loss = 0.04 (79.0 examples/sec; 0.051 sec/batch; 1h:26m:22s remains)
INFO - root - 2019-11-06 20:56:13.168864: step 47610, total loss = 0.13, predict loss = 0.03 (78.6 examples/sec; 0.051 sec/batch; 1h:26m:53s remains)
INFO - root - 2019-11-06 20:56:13.750812: step 47620, total loss = 0.22, predict loss = 0.05 (77.1 examples/sec; 0.052 sec/batch; 1h:28m:33s remains)
INFO - root - 2019-11-06 20:56:14.315480: step 47630, total loss = 0.28, predict loss = 0.07 (73.8 examples/sec; 0.054 sec/batch; 1h:32m:31s remains)
INFO - root - 2019-11-06 20:56:14.893516: step 47640, total loss = 0.21, predict loss = 0.05 (75.2 examples/sec; 0.053 sec/batch; 1h:30m:42s remains)
INFO - root - 2019-11-06 20:56:15.488079: step 47650, total loss = 0.17, predict loss = 0.04 (78.7 examples/sec; 0.051 sec/batch; 1h:26m:39s remains)
INFO - root - 2019-11-06 20:56:16.074472: step 47660, total loss = 0.34, predict loss = 0.09 (77.1 examples/sec; 0.052 sec/batch; 1h:28m:31s remains)
INFO - root - 2019-11-06 20:56:16.653767: step 47670, total loss = 0.21, predict loss = 0.06 (80.3 examples/sec; 0.050 sec/batch; 1h:24m:57s remains)
INFO - root - 2019-11-06 20:56:17.224818: step 47680, total loss = 0.21, predict loss = 0.05 (84.2 examples/sec; 0.048 sec/batch; 1h:21m:03s remains)
INFO - root - 2019-11-06 20:56:17.780214: step 47690, total loss = 0.21, predict loss = 0.06 (94.1 examples/sec; 0.042 sec/batch; 1h:12m:27s remains)
INFO - root - 2019-11-06 20:56:18.223151: step 47700, total loss = 0.22, predict loss = 0.05 (103.7 examples/sec; 0.039 sec/batch; 1h:05m:47s remains)
INFO - root - 2019-11-06 20:56:18.670647: step 47710, total loss = 0.23, predict loss = 0.05 (96.9 examples/sec; 0.041 sec/batch; 1h:10m:22s remains)
INFO - root - 2019-11-06 20:56:19.707265: step 47720, total loss = 0.21, predict loss = 0.05 (59.1 examples/sec; 0.068 sec/batch; 1h:55m:22s remains)
INFO - root - 2019-11-06 20:56:20.395542: step 47730, total loss = 0.24, predict loss = 0.06 (66.2 examples/sec; 0.060 sec/batch; 1h:42m:58s remains)
INFO - root - 2019-11-06 20:56:21.043020: step 47740, total loss = 0.19, predict loss = 0.05 (69.0 examples/sec; 0.058 sec/batch; 1h:38m:49s remains)
INFO - root - 2019-11-06 20:56:21.639118: step 47750, total loss = 0.18, predict loss = 0.05 (81.1 examples/sec; 0.049 sec/batch; 1h:24m:06s remains)
INFO - root - 2019-11-06 20:56:22.200676: step 47760, total loss = 0.22, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:28m:02s remains)
INFO - root - 2019-11-06 20:56:22.785838: step 47770, total loss = 0.27, predict loss = 0.08 (81.4 examples/sec; 0.049 sec/batch; 1h:23m:41s remains)
INFO - root - 2019-11-06 20:56:23.365900: step 47780, total loss = 0.22, predict loss = 0.06 (76.0 examples/sec; 0.053 sec/batch; 1h:29m:36s remains)
INFO - root - 2019-11-06 20:56:23.950049: step 47790, total loss = 0.25, predict loss = 0.08 (77.5 examples/sec; 0.052 sec/batch; 1h:27m:53s remains)
INFO - root - 2019-11-06 20:56:24.526893: step 47800, total loss = 0.16, predict loss = 0.04 (82.0 examples/sec; 0.049 sec/batch; 1h:23m:07s remains)
INFO - root - 2019-11-06 20:56:25.108881: step 47810, total loss = 0.28, predict loss = 0.08 (79.8 examples/sec; 0.050 sec/batch; 1h:25m:20s remains)
INFO - root - 2019-11-06 20:56:25.681296: step 47820, total loss = 0.25, predict loss = 0.07 (76.3 examples/sec; 0.052 sec/batch; 1h:29m:14s remains)
INFO - root - 2019-11-06 20:56:26.269068: step 47830, total loss = 0.19, predict loss = 0.05 (77.8 examples/sec; 0.051 sec/batch; 1h:27m:32s remains)
INFO - root - 2019-11-06 20:56:26.799505: step 47840, total loss = 0.26, predict loss = 0.07 (101.1 examples/sec; 0.040 sec/batch; 1h:07m:22s remains)
INFO - root - 2019-11-06 20:56:27.281348: step 47850, total loss = 0.21, predict loss = 0.05 (94.6 examples/sec; 0.042 sec/batch; 1h:11m:57s remains)
INFO - root - 2019-11-06 20:56:27.723310: step 47860, total loss = 0.16, predict loss = 0.04 (91.1 examples/sec; 0.044 sec/batch; 1h:14m:47s remains)
INFO - root - 2019-11-06 20:56:28.764621: step 47870, total loss = 0.24, predict loss = 0.06 (61.1 examples/sec; 0.065 sec/batch; 1h:51m:21s remains)
INFO - root - 2019-11-06 20:56:29.401487: step 47880, total loss = 0.31, predict loss = 0.09 (78.0 examples/sec; 0.051 sec/batch; 1h:27m:15s remains)
INFO - root - 2019-11-06 20:56:29.990495: step 47890, total loss = 0.28, predict loss = 0.07 (79.1 examples/sec; 0.051 sec/batch; 1h:26m:04s remains)
INFO - root - 2019-11-06 20:56:30.591786: step 47900, total loss = 0.15, predict loss = 0.04 (81.5 examples/sec; 0.049 sec/batch; 1h:23m:28s remains)
INFO - root - 2019-11-06 20:56:31.164212: step 47910, total loss = 0.19, predict loss = 0.05 (80.2 examples/sec; 0.050 sec/batch; 1h:24m:53s remains)
INFO - root - 2019-11-06 20:56:31.735817: step 47920, total loss = 0.24, predict loss = 0.06 (80.5 examples/sec; 0.050 sec/batch; 1h:24m:33s remains)
INFO - root - 2019-11-06 20:56:32.326685: step 47930, total loss = 0.21, predict loss = 0.05 (78.4 examples/sec; 0.051 sec/batch; 1h:26m:46s remains)
INFO - root - 2019-11-06 20:56:32.899861: step 47940, total loss = 0.18, predict loss = 0.05 (80.9 examples/sec; 0.049 sec/batch; 1h:24m:03s remains)
INFO - root - 2019-11-06 20:56:33.487256: step 47950, total loss = 0.20, predict loss = 0.05 (77.7 examples/sec; 0.051 sec/batch; 1h:27m:32s remains)
INFO - root - 2019-11-06 20:56:34.069602: step 47960, total loss = 0.16, predict loss = 0.04 (78.3 examples/sec; 0.051 sec/batch; 1h:26m:52s remains)
INFO - root - 2019-11-06 20:56:34.666318: step 47970, total loss = 0.15, predict loss = 0.03 (77.2 examples/sec; 0.052 sec/batch; 1h:28m:04s remains)
INFO - root - 2019-11-06 20:56:35.242788: step 47980, total loss = 0.22, predict loss = 0.05 (78.4 examples/sec; 0.051 sec/batch; 1h:26m:46s remains)
INFO - root - 2019-11-06 20:56:35.740321: step 47990, total loss = 0.19, predict loss = 0.05 (99.7 examples/sec; 0.040 sec/batch; 1h:08m:11s remains)
INFO - root - 2019-11-06 20:56:36.188599: step 48000, total loss = 0.19, predict loss = 0.05 (103.9 examples/sec; 0.038 sec/batch; 1h:05m:25s remains)
INFO - root - 2019-11-06 20:56:36.655836: step 48010, total loss = 0.30, predict loss = 0.08 (97.3 examples/sec; 0.041 sec/batch; 1h:09m:54s remains)
INFO - root - 2019-11-06 20:56:37.768957: step 48020, total loss = 0.15, predict loss = 0.04 (57.9 examples/sec; 0.069 sec/batch; 1h:57m:26s remains)
INFO - root - 2019-11-06 20:56:38.398653: step 48030, total loss = 0.32, predict loss = 0.09 (77.1 examples/sec; 0.052 sec/batch; 1h:28m:11s remains)
INFO - root - 2019-11-06 20:56:38.978994: step 48040, total loss = 0.18, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:27m:03s remains)
INFO - root - 2019-11-06 20:56:39.577285: step 48050, total loss = 0.20, predict loss = 0.05 (77.4 examples/sec; 0.052 sec/batch; 1h:27m:48s remains)
INFO - root - 2019-11-06 20:56:40.155199: step 48060, total loss = 0.24, predict loss = 0.06 (75.3 examples/sec; 0.053 sec/batch; 1h:30m:11s remains)
INFO - root - 2019-11-06 20:56:40.735425: step 48070, total loss = 0.23, predict loss = 0.06 (76.5 examples/sec; 0.052 sec/batch; 1h:28m:51s remains)
INFO - root - 2019-11-06 20:56:41.300245: step 48080, total loss = 0.20, predict loss = 0.06 (81.7 examples/sec; 0.049 sec/batch; 1h:23m:11s remains)
INFO - root - 2019-11-06 20:56:41.897540: step 48090, total loss = 0.26, predict loss = 0.07 (74.8 examples/sec; 0.053 sec/batch; 1h:30m:51s remains)
INFO - root - 2019-11-06 20:56:42.475675: step 48100, total loss = 0.27, predict loss = 0.06 (74.4 examples/sec; 0.054 sec/batch; 1h:31m:20s remains)
INFO - root - 2019-11-06 20:56:43.050782: step 48110, total loss = 0.21, predict loss = 0.06 (76.3 examples/sec; 0.052 sec/batch; 1h:29m:01s remains)
INFO - root - 2019-11-06 20:56:43.629495: step 48120, total loss = 0.16, predict loss = 0.04 (80.7 examples/sec; 0.050 sec/batch; 1h:24m:07s remains)
INFO - root - 2019-11-06 20:56:44.210873: step 48130, total loss = 0.21, predict loss = 0.06 (79.7 examples/sec; 0.050 sec/batch; 1h:25m:12s remains)
INFO - root - 2019-11-06 20:56:44.677231: step 48140, total loss = 0.25, predict loss = 0.06 (102.5 examples/sec; 0.039 sec/batch; 1h:06m:16s remains)
INFO - root - 2019-11-06 20:56:45.126145: step 48150, total loss = 0.30, predict loss = 0.09 (93.3 examples/sec; 0.043 sec/batch; 1h:12m:45s remains)
INFO - root - 2019-11-06 20:56:46.031585: step 48160, total loss = 0.25, predict loss = 0.07 (76.6 examples/sec; 0.052 sec/batch; 1h:28m:39s remains)
INFO - root - 2019-11-06 20:56:46.745909: step 48170, total loss = 0.16, predict loss = 0.04 (58.4 examples/sec; 0.068 sec/batch; 1h:56m:09s remains)
INFO - root - 2019-11-06 20:56:47.348955: step 48180, total loss = 0.27, predict loss = 0.08 (80.2 examples/sec; 0.050 sec/batch; 1h:24m:36s remains)
INFO - root - 2019-11-06 20:56:47.916003: step 48190, total loss = 0.24, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:25m:50s remains)
INFO - root - 2019-11-06 20:56:48.492857: step 48200, total loss = 0.22, predict loss = 0.05 (81.0 examples/sec; 0.049 sec/batch; 1h:23m:48s remains)
INFO - root - 2019-11-06 20:56:49.072995: step 48210, total loss = 0.25, predict loss = 0.07 (80.2 examples/sec; 0.050 sec/batch; 1h:24m:37s remains)
INFO - root - 2019-11-06 20:56:49.641910: step 48220, total loss = 0.19, predict loss = 0.04 (78.0 examples/sec; 0.051 sec/batch; 1h:26m:59s remains)
INFO - root - 2019-11-06 20:56:50.221383: step 48230, total loss = 0.17, predict loss = 0.04 (76.3 examples/sec; 0.052 sec/batch; 1h:28m:52s remains)
INFO - root - 2019-11-06 20:56:50.795416: step 48240, total loss = 0.25, predict loss = 0.06 (76.1 examples/sec; 0.053 sec/batch; 1h:29m:07s remains)
INFO - root - 2019-11-06 20:56:51.393490: step 48250, total loss = 0.30, predict loss = 0.09 (78.1 examples/sec; 0.051 sec/batch; 1h:26m:54s remains)
INFO - root - 2019-11-06 20:56:51.962792: step 48260, total loss = 0.20, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:25m:31s remains)
INFO - root - 2019-11-06 20:56:52.534938: step 48270, total loss = 0.25, predict loss = 0.08 (76.9 examples/sec; 0.052 sec/batch; 1h:28m:12s remains)
INFO - root - 2019-11-06 20:56:53.105721: step 48280, total loss = 0.25, predict loss = 0.06 (91.3 examples/sec; 0.044 sec/batch; 1h:14m:14s remains)
INFO - root - 2019-11-06 20:56:53.578423: step 48290, total loss = 0.23, predict loss = 0.06 (97.3 examples/sec; 0.041 sec/batch; 1h:09m:40s remains)
INFO - root - 2019-11-06 20:56:54.025285: step 48300, total loss = 0.16, predict loss = 0.04 (101.5 examples/sec; 0.039 sec/batch; 1h:06m:47s remains)
INFO - root - 2019-11-06 20:56:54.941521: step 48310, total loss = 0.19, predict loss = 0.05 (73.2 examples/sec; 0.055 sec/batch; 1h:32m:37s remains)
INFO - root - 2019-11-06 20:56:55.591618: step 48320, total loss = 0.15, predict loss = 0.03 (68.1 examples/sec; 0.059 sec/batch; 1h:39m:34s remains)
INFO - root - 2019-11-06 20:56:56.213796: step 48330, total loss = 0.15, predict loss = 0.03 (78.0 examples/sec; 0.051 sec/batch; 1h:26m:54s remains)
INFO - root - 2019-11-06 20:56:56.792900: step 48340, total loss = 0.18, predict loss = 0.04 (77.3 examples/sec; 0.052 sec/batch; 1h:27m:43s remains)
INFO - root - 2019-11-06 20:56:57.371927: step 48350, total loss = 0.28, predict loss = 0.07 (77.7 examples/sec; 0.051 sec/batch; 1h:27m:12s remains)
INFO - root - 2019-11-06 20:56:57.952845: step 48360, total loss = 0.31, predict loss = 0.08 (79.8 examples/sec; 0.050 sec/batch; 1h:24m:56s remains)
INFO - root - 2019-11-06 20:56:58.549924: step 48370, total loss = 0.19, predict loss = 0.05 (76.4 examples/sec; 0.052 sec/batch; 1h:28m:37s remains)
INFO - root - 2019-11-06 20:56:59.119584: step 48380, total loss = 0.21, predict loss = 0.06 (81.0 examples/sec; 0.049 sec/batch; 1h:23m:38s remains)
INFO - root - 2019-11-06 20:56:59.697572: step 48390, total loss = 0.28, predict loss = 0.06 (72.4 examples/sec; 0.055 sec/batch; 1h:33m:31s remains)
INFO - root - 2019-11-06 20:57:00.284793: step 48400, total loss = 0.16, predict loss = 0.04 (79.4 examples/sec; 0.050 sec/batch; 1h:25m:17s remains)
INFO - root - 2019-11-06 20:57:00.896998: step 48410, total loss = 0.18, predict loss = 0.05 (82.2 examples/sec; 0.049 sec/batch; 1h:22m:24s remains)
INFO - root - 2019-11-06 20:57:01.466354: step 48420, total loss = 0.28, predict loss = 0.08 (75.7 examples/sec; 0.053 sec/batch; 1h:29m:25s remains)
INFO - root - 2019-11-06 20:57:02.011851: step 48430, total loss = 0.25, predict loss = 0.06 (98.2 examples/sec; 0.041 sec/batch; 1h:08m:57s remains)
INFO - root - 2019-11-06 20:57:02.474968: step 48440, total loss = 0.18, predict loss = 0.05 (95.0 examples/sec; 0.042 sec/batch; 1h:11m:15s remains)
INFO - root - 2019-11-06 20:57:02.960721: step 48450, total loss = 0.21, predict loss = 0.06 (97.8 examples/sec; 0.041 sec/batch; 1h:09m:14s remains)
INFO - root - 2019-11-06 20:57:03.915130: step 48460, total loss = 0.22, predict loss = 0.06 (72.6 examples/sec; 0.055 sec/batch; 1h:33m:13s remains)
INFO - root - 2019-11-06 20:57:04.555447: step 48470, total loss = 0.24, predict loss = 0.07 (70.2 examples/sec; 0.057 sec/batch; 1h:36m:23s remains)
INFO - root - 2019-11-06 20:57:05.141804: step 48480, total loss = 0.23, predict loss = 0.06 (76.5 examples/sec; 0.052 sec/batch; 1h:28m:26s remains)
INFO - root - 2019-11-06 20:57:05.742739: step 48490, total loss = 0.25, predict loss = 0.07 (73.4 examples/sec; 0.055 sec/batch; 1h:32m:14s remains)
INFO - root - 2019-11-06 20:57:06.323411: step 48500, total loss = 0.21, predict loss = 0.05 (76.0 examples/sec; 0.053 sec/batch; 1h:28m:59s remains)
INFO - root - 2019-11-06 20:57:06.899146: step 48510, total loss = 0.22, predict loss = 0.05 (74.7 examples/sec; 0.054 sec/batch; 1h:30m:32s remains)
INFO - root - 2019-11-06 20:57:07.479260: step 48520, total loss = 0.22, predict loss = 0.06 (77.1 examples/sec; 0.052 sec/batch; 1h:27m:42s remains)
INFO - root - 2019-11-06 20:57:08.080688: step 48530, total loss = 0.18, predict loss = 0.04 (77.3 examples/sec; 0.052 sec/batch; 1h:27m:27s remains)
INFO - root - 2019-11-06 20:57:08.659699: step 48540, total loss = 0.19, predict loss = 0.05 (77.4 examples/sec; 0.052 sec/batch; 1h:27m:26s remains)
INFO - root - 2019-11-06 20:57:09.230421: step 48550, total loss = 0.26, predict loss = 0.07 (81.8 examples/sec; 0.049 sec/batch; 1h:22m:43s remains)
INFO - root - 2019-11-06 20:57:09.809146: step 48560, total loss = 0.27, predict loss = 0.07 (77.0 examples/sec; 0.052 sec/batch; 1h:27m:51s remains)
INFO - root - 2019-11-06 20:57:10.389790: step 48570, total loss = 0.22, predict loss = 0.07 (78.5 examples/sec; 0.051 sec/batch; 1h:26m:06s remains)
INFO - root - 2019-11-06 20:57:10.926087: step 48580, total loss = 0.26, predict loss = 0.08 (95.1 examples/sec; 0.042 sec/batch; 1h:11m:05s remains)
INFO - root - 2019-11-06 20:57:11.381004: step 48590, total loss = 0.23, predict loss = 0.05 (95.1 examples/sec; 0.042 sec/batch; 1h:11m:03s remains)
INFO - root - 2019-11-06 20:57:11.835441: step 48600, total loss = 0.24, predict loss = 0.06 (100.9 examples/sec; 0.040 sec/batch; 1h:06m:57s remains)
INFO - root - 2019-11-06 20:57:12.861601: step 48610, total loss = 0.15, predict loss = 0.04 (64.9 examples/sec; 0.062 sec/batch; 1h:44m:06s remains)
INFO - root - 2019-11-06 20:57:13.503289: step 48620, total loss = 0.16, predict loss = 0.03 (72.4 examples/sec; 0.055 sec/batch; 1h:33m:22s remains)
INFO - root - 2019-11-06 20:57:14.101424: step 48630, total loss = 0.18, predict loss = 0.04 (73.4 examples/sec; 0.055 sec/batch; 1h:32m:07s remains)
INFO - root - 2019-11-06 20:57:14.673259: step 48640, total loss = 0.21, predict loss = 0.05 (79.0 examples/sec; 0.051 sec/batch; 1h:25m:35s remains)
INFO - root - 2019-11-06 20:57:15.264195: step 48650, total loss = 0.19, predict loss = 0.05 (78.7 examples/sec; 0.051 sec/batch; 1h:25m:52s remains)
INFO - root - 2019-11-06 20:57:15.844129: step 48660, total loss = 0.22, predict loss = 0.06 (79.6 examples/sec; 0.050 sec/batch; 1h:24m:55s remains)
INFO - root - 2019-11-06 20:57:16.416985: step 48670, total loss = 0.17, predict loss = 0.04 (85.8 examples/sec; 0.047 sec/batch; 1h:18m:43s remains)
INFO - root - 2019-11-06 20:57:16.988697: step 48680, total loss = 0.19, predict loss = 0.05 (75.6 examples/sec; 0.053 sec/batch; 1h:29m:17s remains)
INFO - root - 2019-11-06 20:57:17.576846: step 48690, total loss = 0.26, predict loss = 0.07 (74.8 examples/sec; 0.053 sec/batch; 1h:30m:15s remains)
INFO - root - 2019-11-06 20:57:18.155346: step 48700, total loss = 0.25, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:24m:42s remains)
INFO - root - 2019-11-06 20:57:18.720423: step 48710, total loss = 0.27, predict loss = 0.06 (78.1 examples/sec; 0.051 sec/batch; 1h:26m:30s remains)
INFO - root - 2019-11-06 20:57:19.301186: step 48720, total loss = 0.25, predict loss = 0.07 (79.2 examples/sec; 0.050 sec/batch; 1h:25m:14s remains)
INFO - root - 2019-11-06 20:57:19.822762: step 48730, total loss = 0.24, predict loss = 0.06 (98.2 examples/sec; 0.041 sec/batch; 1h:08m:45s remains)
INFO - root - 2019-11-06 20:57:20.285026: step 48740, total loss = 0.23, predict loss = 0.05 (91.6 examples/sec; 0.044 sec/batch; 1h:13m:44s remains)
INFO - root - 2019-11-06 20:57:20.741406: step 48750, total loss = 0.48, predict loss = 0.13 (101.5 examples/sec; 0.039 sec/batch; 1h:06m:31s remains)
INFO - root - 2019-11-06 20:57:21.860874: step 48760, total loss = 0.15, predict loss = 0.04 (56.0 examples/sec; 0.071 sec/batch; 2h:00m:30s remains)
INFO - root - 2019-11-06 20:57:22.484065: step 48770, total loss = 0.24, predict loss = 0.07 (78.2 examples/sec; 0.051 sec/batch; 1h:26m:18s remains)
INFO - root - 2019-11-06 20:57:23.059743: step 48780, total loss = 0.19, predict loss = 0.05 (75.3 examples/sec; 0.053 sec/batch; 1h:29m:39s remains)
INFO - root - 2019-11-06 20:57:23.639175: step 48790, total loss = 0.20, predict loss = 0.05 (81.5 examples/sec; 0.049 sec/batch; 1h:22m:47s remains)
INFO - root - 2019-11-06 20:57:24.209213: step 48800, total loss = 0.28, predict loss = 0.07 (75.3 examples/sec; 0.053 sec/batch; 1h:29m:38s remains)
INFO - root - 2019-11-06 20:57:24.797500: step 48810, total loss = 0.19, predict loss = 0.05 (75.5 examples/sec; 0.053 sec/batch; 1h:29m:18s remains)
INFO - root - 2019-11-06 20:57:25.377511: step 48820, total loss = 0.21, predict loss = 0.05 (81.6 examples/sec; 0.049 sec/batch; 1h:22m:42s remains)
INFO - root - 2019-11-06 20:57:25.944706: step 48830, total loss = 0.16, predict loss = 0.04 (76.5 examples/sec; 0.052 sec/batch; 1h:28m:10s remains)
INFO - root - 2019-11-06 20:57:26.514807: step 48840, total loss = 0.14, predict loss = 0.03 (78.3 examples/sec; 0.051 sec/batch; 1h:26m:07s remains)
INFO - root - 2019-11-06 20:57:27.105005: step 48850, total loss = 0.21, predict loss = 0.05 (75.0 examples/sec; 0.053 sec/batch; 1h:29m:54s remains)
INFO - root - 2019-11-06 20:57:27.673088: step 48860, total loss = 0.31, predict loss = 0.08 (79.0 examples/sec; 0.051 sec/batch; 1h:25m:20s remains)
INFO - root - 2019-11-06 20:57:28.256511: step 48870, total loss = 0.16, predict loss = 0.04 (77.1 examples/sec; 0.052 sec/batch; 1h:27m:28s remains)
INFO - root - 2019-11-06 20:57:28.745608: step 48880, total loss = 0.24, predict loss = 0.06 (107.0 examples/sec; 0.037 sec/batch; 1h:03m:00s remains)
INFO - root - 2019-11-06 20:57:29.224487: step 48890, total loss = 0.15, predict loss = 0.04 (91.2 examples/sec; 0.044 sec/batch; 1h:13m:54s remains)
INFO - root - 2019-11-06 20:57:30.119354: step 48900, total loss = 0.24, predict loss = 0.06 (8.3 examples/sec; 0.484 sec/batch; 13h:36m:01s remains)
INFO - root - 2019-11-06 20:57:30.766489: step 48910, total loss = 0.18, predict loss = 0.05 (65.5 examples/sec; 0.061 sec/batch; 1h:42m:50s remains)
INFO - root - 2019-11-06 20:57:31.394208: step 48920, total loss = 0.20, predict loss = 0.05 (75.3 examples/sec; 0.053 sec/batch; 1h:29m:26s remains)
INFO - root - 2019-11-06 20:57:31.994801: step 48930, total loss = 0.18, predict loss = 0.04 (77.3 examples/sec; 0.052 sec/batch; 1h:27m:10s remains)
INFO - root - 2019-11-06 20:57:32.554898: step 48940, total loss = 0.24, predict loss = 0.05 (82.3 examples/sec; 0.049 sec/batch; 1h:21m:51s remains)
INFO - root - 2019-11-06 20:57:33.126931: step 48950, total loss = 0.19, predict loss = 0.04 (75.4 examples/sec; 0.053 sec/batch; 1h:29m:23s remains)
INFO - root - 2019-11-06 20:57:33.704940: step 48960, total loss = 0.20, predict loss = 0.05 (75.3 examples/sec; 0.053 sec/batch; 1h:29m:30s remains)
INFO - root - 2019-11-06 20:57:34.296488: step 48970, total loss = 0.16, predict loss = 0.04 (79.7 examples/sec; 0.050 sec/batch; 1h:24m:32s remains)
INFO - root - 2019-11-06 20:57:34.877597: step 48980, total loss = 0.15, predict loss = 0.04 (75.4 examples/sec; 0.053 sec/batch; 1h:29m:16s remains)
INFO - root - 2019-11-06 20:57:35.473392: step 48990, total loss = 0.18, predict loss = 0.05 (76.7 examples/sec; 0.052 sec/batch; 1h:27m:46s remains)
INFO - root - 2019-11-06 20:57:36.045785: step 49000, total loss = 0.42, predict loss = 0.12 (73.2 examples/sec; 0.055 sec/batch; 1h:32m:00s remains)
INFO - root - 2019-11-06 20:57:36.642403: step 49010, total loss = 0.21, predict loss = 0.05 (80.4 examples/sec; 0.050 sec/batch; 1h:23m:43s remains)
INFO - root - 2019-11-06 20:57:37.213638: step 49020, total loss = 0.23, predict loss = 0.06 (86.0 examples/sec; 0.047 sec/batch; 1h:18m:18s remains)
INFO - root - 2019-11-06 20:57:37.679068: step 49030, total loss = 0.21, predict loss = 0.06 (93.4 examples/sec; 0.043 sec/batch; 1h:12m:02s remains)
INFO - root - 2019-11-06 20:57:38.136390: step 49040, total loss = 0.25, predict loss = 0.07 (92.2 examples/sec; 0.043 sec/batch; 1h:12m:58s remains)
INFO - root - 2019-11-06 20:57:39.053499: step 49050, total loss = 0.24, predict loss = 0.06 (76.2 examples/sec; 0.053 sec/batch; 1h:28m:19s remains)
INFO - root - 2019-11-06 20:57:39.713938: step 49060, total loss = 0.17, predict loss = 0.05 (64.5 examples/sec; 0.062 sec/batch; 1h:44m:18s remains)
INFO - root - 2019-11-06 20:57:40.335129: step 49070, total loss = 0.21, predict loss = 0.06 (75.1 examples/sec; 0.053 sec/batch; 1h:29m:39s remains)
INFO - root - 2019-11-06 20:57:40.917373: step 49080, total loss = 0.31, predict loss = 0.08 (77.9 examples/sec; 0.051 sec/batch; 1h:26m:20s remains)
INFO - root - 2019-11-06 20:57:41.509489: step 49090, total loss = 0.21, predict loss = 0.05 (73.9 examples/sec; 0.054 sec/batch; 1h:31m:03s remains)
INFO - root - 2019-11-06 20:57:42.096336: step 49100, total loss = 0.22, predict loss = 0.05 (77.3 examples/sec; 0.052 sec/batch; 1h:27m:02s remains)
INFO - root - 2019-11-06 20:57:42.675171: step 49110, total loss = 0.37, predict loss = 0.11 (80.0 examples/sec; 0.050 sec/batch; 1h:24m:05s remains)
INFO - root - 2019-11-06 20:57:43.247616: step 49120, total loss = 0.18, predict loss = 0.05 (78.7 examples/sec; 0.051 sec/batch; 1h:25m:24s remains)
INFO - root - 2019-11-06 20:57:43.834013: step 49130, total loss = 0.16, predict loss = 0.04 (76.3 examples/sec; 0.052 sec/batch; 1h:28m:05s remains)
INFO - root - 2019-11-06 20:57:44.407948: step 49140, total loss = 0.19, predict loss = 0.05 (78.7 examples/sec; 0.051 sec/batch; 1h:25m:24s remains)
INFO - root - 2019-11-06 20:57:44.975087: step 49150, total loss = 0.24, predict loss = 0.07 (79.5 examples/sec; 0.050 sec/batch; 1h:24m:36s remains)
INFO - root - 2019-11-06 20:57:45.554752: step 49160, total loss = 0.24, predict loss = 0.07 (76.9 examples/sec; 0.052 sec/batch; 1h:27m:28s remains)
INFO - root - 2019-11-06 20:57:46.130319: step 49170, total loss = 0.22, predict loss = 0.06 (92.6 examples/sec; 0.043 sec/batch; 1h:12m:35s remains)
INFO - root - 2019-11-06 20:57:46.589009: step 49180, total loss = 0.17, predict loss = 0.04 (93.0 examples/sec; 0.043 sec/batch; 1h:12m:14s remains)
INFO - root - 2019-11-06 20:57:47.058566: step 49190, total loss = 0.26, predict loss = 0.07 (87.8 examples/sec; 0.046 sec/batch; 1h:16m:33s remains)
INFO - root - 2019-11-06 20:57:48.011792: step 49200, total loss = 0.21, predict loss = 0.05 (69.7 examples/sec; 0.057 sec/batch; 1h:36m:23s remains)
INFO - root - 2019-11-06 20:57:48.724242: step 49210, total loss = 0.20, predict loss = 0.05 (64.9 examples/sec; 0.062 sec/batch; 1h:43m:29s remains)
INFO - root - 2019-11-06 20:57:49.313237: step 49220, total loss = 0.18, predict loss = 0.04 (77.1 examples/sec; 0.052 sec/batch; 1h:27m:06s remains)
INFO - root - 2019-11-06 20:57:49.873425: step 49230, total loss = 0.24, predict loss = 0.06 (75.4 examples/sec; 0.053 sec/batch; 1h:29m:04s remains)
INFO - root - 2019-11-06 20:57:50.442774: step 49240, total loss = 0.21, predict loss = 0.05 (75.9 examples/sec; 0.053 sec/batch; 1h:28m:30s remains)
INFO - root - 2019-11-06 20:57:51.042732: step 49250, total loss = 0.18, predict loss = 0.05 (80.9 examples/sec; 0.049 sec/batch; 1h:22m:58s remains)
INFO - root - 2019-11-06 20:57:51.623617: step 49260, total loss = 0.23, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 1h:26m:42s remains)
INFO - root - 2019-11-06 20:57:52.199939: step 49270, total loss = 0.24, predict loss = 0.08 (78.7 examples/sec; 0.051 sec/batch; 1h:25m:18s remains)
INFO - root - 2019-11-06 20:57:52.780970: step 49280, total loss = 0.21, predict loss = 0.05 (80.3 examples/sec; 0.050 sec/batch; 1h:23m:37s remains)
INFO - root - 2019-11-06 20:57:53.372996: step 49290, total loss = 0.21, predict loss = 0.06 (81.6 examples/sec; 0.049 sec/batch; 1h:22m:15s remains)
INFO - root - 2019-11-06 20:57:53.958228: step 49300, total loss = 0.16, predict loss = 0.04 (77.4 examples/sec; 0.052 sec/batch; 1h:26m:45s remains)
INFO - root - 2019-11-06 20:57:54.535590: step 49310, total loss = 0.22, predict loss = 0.05 (76.3 examples/sec; 0.052 sec/batch; 1h:27m:57s remains)
INFO - root - 2019-11-06 20:57:55.084670: step 49320, total loss = 0.15, predict loss = 0.04 (94.4 examples/sec; 0.042 sec/batch; 1h:11m:05s remains)
INFO - root - 2019-11-06 20:57:55.555864: step 49330, total loss = 0.19, predict loss = 0.05 (100.6 examples/sec; 0.040 sec/batch; 1h:06m:41s remains)
INFO - root - 2019-11-06 20:57:56.006228: step 49340, total loss = 0.28, predict loss = 0.07 (96.4 examples/sec; 0.041 sec/batch; 1h:09m:36s remains)
INFO - root - 2019-11-06 20:57:57.026388: step 49350, total loss = 0.26, predict loss = 0.06 (55.5 examples/sec; 0.072 sec/batch; 2h:00m:52s remains)
INFO - root - 2019-11-06 20:57:57.662059: step 49360, total loss = 0.27, predict loss = 0.08 (79.3 examples/sec; 0.050 sec/batch; 1h:24m:38s remains)
INFO - root - 2019-11-06 20:57:58.256279: step 49370, total loss = 0.32, predict loss = 0.09 (79.2 examples/sec; 0.050 sec/batch; 1h:24m:41s remains)
INFO - root - 2019-11-06 20:57:58.835719: step 49380, total loss = 0.15, predict loss = 0.03 (77.2 examples/sec; 0.052 sec/batch; 1h:26m:53s remains)
INFO - root - 2019-11-06 20:57:59.410645: step 49390, total loss = 0.18, predict loss = 0.04 (80.7 examples/sec; 0.050 sec/batch; 1h:23m:07s remains)
INFO - root - 2019-11-06 20:57:59.999481: step 49400, total loss = 0.23, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 1h:26m:50s remains)
INFO - root - 2019-11-06 20:58:00.624138: step 49410, total loss = 0.26, predict loss = 0.06 (80.1 examples/sec; 0.050 sec/batch; 1h:23m:42s remains)
INFO - root - 2019-11-06 20:58:01.190084: step 49420, total loss = 0.16, predict loss = 0.04 (82.8 examples/sec; 0.048 sec/batch; 1h:21m:00s remains)
INFO - root - 2019-11-06 20:58:01.756864: step 49430, total loss = 0.17, predict loss = 0.05 (80.6 examples/sec; 0.050 sec/batch; 1h:23m:11s remains)
INFO - root - 2019-11-06 20:58:02.332936: step 49440, total loss = 0.22, predict loss = 0.06 (77.6 examples/sec; 0.052 sec/batch; 1h:26m:22s remains)
INFO - root - 2019-11-06 20:58:02.910716: step 49450, total loss = 0.20, predict loss = 0.05 (78.4 examples/sec; 0.051 sec/batch; 1h:25m:30s remains)
INFO - root - 2019-11-06 20:58:03.489464: step 49460, total loss = 0.21, predict loss = 0.06 (76.4 examples/sec; 0.052 sec/batch; 1h:27m:42s remains)
INFO - root - 2019-11-06 20:58:03.998630: step 49470, total loss = 0.18, predict loss = 0.05 (101.6 examples/sec; 0.039 sec/batch; 1h:05m:56s remains)
INFO - root - 2019-11-06 20:58:04.443442: step 49480, total loss = 0.20, predict loss = 0.05 (96.0 examples/sec; 0.042 sec/batch; 1h:09m:50s remains)
INFO - root - 2019-11-06 20:58:04.915915: step 49490, total loss = 0.22, predict loss = 0.06 (97.3 examples/sec; 0.041 sec/batch; 1h:08m:50s remains)
INFO - root - 2019-11-06 20:58:05.962017: step 49500, total loss = 0.14, predict loss = 0.03 (53.4 examples/sec; 0.075 sec/batch; 2h:05m:25s remains)
INFO - root - 2019-11-06 20:58:06.583219: step 49510, total loss = 0.24, predict loss = 0.07 (79.5 examples/sec; 0.050 sec/batch; 1h:24m:17s remains)
INFO - root - 2019-11-06 20:58:07.167615: step 49520, total loss = 0.18, predict loss = 0.04 (79.7 examples/sec; 0.050 sec/batch; 1h:24m:03s remains)
INFO - root - 2019-11-06 20:58:07.762604: step 49530, total loss = 0.20, predict loss = 0.05 (77.8 examples/sec; 0.051 sec/batch; 1h:26m:05s remains)
INFO - root - 2019-11-06 20:58:08.343644: step 49540, total loss = 0.21, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 1h:26m:22s remains)
INFO - root - 2019-11-06 20:58:08.918688: step 49550, total loss = 0.24, predict loss = 0.06 (74.9 examples/sec; 0.053 sec/batch; 1h:29m:20s remains)
INFO - root - 2019-11-06 20:58:09.501617: step 49560, total loss = 0.24, predict loss = 0.06 (73.1 examples/sec; 0.055 sec/batch; 1h:31m:38s remains)
INFO - root - 2019-11-06 20:58:10.095368: step 49570, total loss = 0.29, predict loss = 0.09 (78.8 examples/sec; 0.051 sec/batch; 1h:24m:56s remains)
INFO - root - 2019-11-06 20:58:10.662808: step 49580, total loss = 0.15, predict loss = 0.04 (78.8 examples/sec; 0.051 sec/batch; 1h:24m:55s remains)
INFO - root - 2019-11-06 20:58:11.239639: step 49590, total loss = 0.26, predict loss = 0.06 (77.7 examples/sec; 0.051 sec/batch; 1h:26m:07s remains)
INFO - root - 2019-11-06 20:58:11.802863: step 49600, total loss = 0.26, predict loss = 0.07 (83.2 examples/sec; 0.048 sec/batch; 1h:20m:24s remains)
INFO - root - 2019-11-06 20:58:12.396384: step 49610, total loss = 0.20, predict loss = 0.05 (77.1 examples/sec; 0.052 sec/batch; 1h:26m:49s remains)
INFO - root - 2019-11-06 20:58:12.889671: step 49620, total loss = 0.18, predict loss = 0.05 (99.9 examples/sec; 0.040 sec/batch; 1h:06m:59s remains)
INFO - root - 2019-11-06 20:58:13.347515: step 49630, total loss = 0.30, predict loss = 0.08 (92.2 examples/sec; 0.043 sec/batch; 1h:12m:35s remains)
INFO - root - 2019-11-06 20:58:13.800286: step 49640, total loss = 0.25, predict loss = 0.07 (95.1 examples/sec; 0.042 sec/batch; 1h:10m:23s remains)
INFO - root - 2019-11-06 20:58:14.949113: step 49650, total loss = 0.21, predict loss = 0.05 (60.7 examples/sec; 0.066 sec/batch; 1h:50m:09s remains)
INFO - root - 2019-11-06 20:58:15.612478: step 49660, total loss = 0.22, predict loss = 0.06 (72.5 examples/sec; 0.055 sec/batch; 1h:32m:19s remains)
INFO - root - 2019-11-06 20:58:16.185033: step 49670, total loss = 0.26, predict loss = 0.07 (77.0 examples/sec; 0.052 sec/batch; 1h:26m:49s remains)
INFO - root - 2019-11-06 20:58:16.756353: step 49680, total loss = 0.24, predict loss = 0.07 (79.4 examples/sec; 0.050 sec/batch; 1h:24m:16s remains)
INFO - root - 2019-11-06 20:58:17.342811: step 49690, total loss = 0.26, predict loss = 0.06 (82.3 examples/sec; 0.049 sec/batch; 1h:21m:12s remains)
INFO - root - 2019-11-06 20:58:17.915621: step 49700, total loss = 0.21, predict loss = 0.05 (75.3 examples/sec; 0.053 sec/batch; 1h:28m:44s remains)
INFO - root - 2019-11-06 20:58:18.499660: step 49710, total loss = 0.27, predict loss = 0.07 (79.2 examples/sec; 0.050 sec/batch; 1h:24m:22s remains)
INFO - root - 2019-11-06 20:58:19.078757: step 49720, total loss = 0.24, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:24m:44s remains)
INFO - root - 2019-11-06 20:58:19.669067: step 49730, total loss = 0.17, predict loss = 0.04 (76.7 examples/sec; 0.052 sec/batch; 1h:27m:08s remains)
INFO - root - 2019-11-06 20:58:20.247713: step 49740, total loss = 0.22, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:24m:30s remains)
INFO - root - 2019-11-06 20:58:20.828776: step 49750, total loss = 0.16, predict loss = 0.04 (78.8 examples/sec; 0.051 sec/batch; 1h:24m:49s remains)
INFO - root - 2019-11-06 20:58:21.415237: step 49760, total loss = 0.13, predict loss = 0.03 (78.4 examples/sec; 0.051 sec/batch; 1h:25m:11s remains)
INFO - root - 2019-11-06 20:58:21.913900: step 49770, total loss = 0.30, predict loss = 0.08 (97.2 examples/sec; 0.041 sec/batch; 1h:08m:43s remains)
INFO - root - 2019-11-06 20:58:22.361253: step 49780, total loss = 0.15, predict loss = 0.04 (95.6 examples/sec; 0.042 sec/batch; 1h:09m:54s remains)
INFO - root - 2019-11-06 20:58:23.257336: step 49790, total loss = 0.22, predict loss = 0.06 (81.2 examples/sec; 0.049 sec/batch; 1h:22m:18s remains)
INFO - root - 2019-11-06 20:58:23.922302: step 49800, total loss = 0.16, predict loss = 0.04 (63.8 examples/sec; 0.063 sec/batch; 1h:44m:39s remains)
INFO - root - 2019-11-06 20:58:24.555441: step 49810, total loss = 0.16, predict loss = 0.04 (76.7 examples/sec; 0.052 sec/batch; 1h:27m:08s remains)
INFO - root - 2019-11-06 20:58:25.141094: step 49820, total loss = 0.20, predict loss = 0.05 (71.7 examples/sec; 0.056 sec/batch; 1h:33m:05s remains)
INFO - root - 2019-11-06 20:58:25.719792: step 49830, total loss = 0.19, predict loss = 0.05 (80.0 examples/sec; 0.050 sec/batch; 1h:23m:31s remains)
INFO - root - 2019-11-06 20:58:26.297796: step 49840, total loss = 0.23, predict loss = 0.07 (77.6 examples/sec; 0.052 sec/batch; 1h:26m:03s remains)
INFO - root - 2019-11-06 20:58:26.885853: step 49850, total loss = 0.29, predict loss = 0.09 (78.3 examples/sec; 0.051 sec/batch; 1h:25m:14s remains)
INFO - root - 2019-11-06 20:58:27.453770: step 49860, total loss = 0.22, predict loss = 0.05 (78.5 examples/sec; 0.051 sec/batch; 1h:25m:05s remains)
INFO - root - 2019-11-06 20:58:28.031970: step 49870, total loss = 0.20, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:26m:58s remains)
INFO - root - 2019-11-06 20:58:28.604340: step 49880, total loss = 0.21, predict loss = 0.05 (75.9 examples/sec; 0.053 sec/batch; 1h:27m:52s remains)
INFO - root - 2019-11-06 20:58:29.193462: step 49890, total loss = 0.19, predict loss = 0.05 (78.9 examples/sec; 0.051 sec/batch; 1h:24m:35s remains)
INFO - root - 2019-11-06 20:58:29.777572: step 49900, total loss = 0.30, predict loss = 0.08 (77.7 examples/sec; 0.051 sec/batch; 1h:25m:51s remains)
INFO - root - 2019-11-06 20:58:30.346969: step 49910, total loss = 0.19, predict loss = 0.05 (88.1 examples/sec; 0.045 sec/batch; 1h:15m:46s remains)
INFO - root - 2019-11-06 20:58:30.828281: step 49920, total loss = 0.20, predict loss = 0.05 (97.1 examples/sec; 0.041 sec/batch; 1h:08m:42s remains)
INFO - root - 2019-11-06 20:58:31.294942: step 49930, total loss = 0.22, predict loss = 0.05 (95.1 examples/sec; 0.042 sec/batch; 1h:10m:07s remains)
INFO - root - 2019-11-06 20:58:32.230423: step 49940, total loss = 0.25, predict loss = 0.06 (75.1 examples/sec; 0.053 sec/batch; 1h:28m:46s remains)
INFO - root - 2019-11-06 20:58:32.864345: step 49950, total loss = 0.20, predict loss = 0.05 (71.4 examples/sec; 0.056 sec/batch; 1h:33m:27s remains)
INFO - root - 2019-11-06 20:58:33.450509: step 49960, total loss = 0.27, predict loss = 0.08 (73.0 examples/sec; 0.055 sec/batch; 1h:31m:21s remains)
INFO - root - 2019-11-06 20:58:34.050952: step 49970, total loss = 0.23, predict loss = 0.07 (75.5 examples/sec; 0.053 sec/batch; 1h:28m:18s remains)
INFO - root - 2019-11-06 20:58:34.624370: step 49980, total loss = 0.20, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:25m:31s remains)
INFO - root - 2019-11-06 20:58:35.212727: step 49990, total loss = 0.20, predict loss = 0.05 (82.1 examples/sec; 0.049 sec/batch; 1h:21m:12s remains)
INFO - root - 2019-11-06 20:58:35.780252: step 50000, total loss = 0.20, predict loss = 0.05 (79.9 examples/sec; 0.050 sec/batch; 1h:23m:27s remains)
INFO - root - 2019-11-06 20:58:36.385002: step 50010, total loss = 0.13, predict loss = 0.03 (78.6 examples/sec; 0.051 sec/batch; 1h:24m:47s remains)
INFO - root - 2019-11-06 20:58:36.960470: step 50020, total loss = 0.18, predict loss = 0.04 (79.0 examples/sec; 0.051 sec/batch; 1h:24m:19s remains)
INFO - root - 2019-11-06 20:58:37.538888: step 50030, total loss = 0.21, predict loss = 0.06 (70.6 examples/sec; 0.057 sec/batch; 1h:34m:26s remains)
INFO - root - 2019-11-06 20:58:38.113325: step 50040, total loss = 0.16, predict loss = 0.04 (81.9 examples/sec; 0.049 sec/batch; 1h:21m:21s remains)
INFO - root - 2019-11-06 20:58:38.707251: step 50050, total loss = 0.24, predict loss = 0.06 (70.3 examples/sec; 0.057 sec/batch; 1h:34m:47s remains)
INFO - root - 2019-11-06 20:58:39.252079: step 50060, total loss = 0.25, predict loss = 0.07 (94.7 examples/sec; 0.042 sec/batch; 1h:10m:20s remains)
INFO - root - 2019-11-06 20:58:39.714773: step 50070, total loss = 0.23, predict loss = 0.06 (92.7 examples/sec; 0.043 sec/batch; 1h:11m:50s remains)
INFO - root - 2019-11-06 20:58:40.162907: step 50080, total loss = 0.31, predict loss = 0.09 (95.9 examples/sec; 0.042 sec/batch; 1h:09m:29s remains)
INFO - root - 2019-11-06 20:58:41.170162: step 50090, total loss = 0.20, predict loss = 0.05 (58.7 examples/sec; 0.068 sec/batch; 1h:53m:32s remains)
INFO - root - 2019-11-06 20:58:41.798031: step 50100, total loss = 0.21, predict loss = 0.06 (75.4 examples/sec; 0.053 sec/batch; 1h:28m:19s remains)
INFO - root - 2019-11-06 20:58:42.380091: step 50110, total loss = 0.15, predict loss = 0.04 (80.6 examples/sec; 0.050 sec/batch; 1h:22m:38s remains)
INFO - root - 2019-11-06 20:58:42.951225: step 50120, total loss = 0.16, predict loss = 0.04 (75.6 examples/sec; 0.053 sec/batch; 1h:28m:02s remains)
INFO - root - 2019-11-06 20:58:43.547154: step 50130, total loss = 0.19, predict loss = 0.05 (74.7 examples/sec; 0.054 sec/batch; 1h:29m:08s remains)
INFO - root - 2019-11-06 20:58:44.131279: step 50140, total loss = 0.26, predict loss = 0.07 (78.0 examples/sec; 0.051 sec/batch; 1h:25m:23s remains)
INFO - root - 2019-11-06 20:58:44.712491: step 50150, total loss = 0.29, predict loss = 0.08 (79.8 examples/sec; 0.050 sec/batch; 1h:23m:22s remains)
INFO - root - 2019-11-06 20:58:45.303180: step 50160, total loss = 0.27, predict loss = 0.07 (80.0 examples/sec; 0.050 sec/batch; 1h:23m:10s remains)
INFO - root - 2019-11-06 20:58:45.901371: step 50170, total loss = 0.23, predict loss = 0.05 (77.6 examples/sec; 0.052 sec/batch; 1h:25m:46s remains)
INFO - root - 2019-11-06 20:58:46.471755: step 50180, total loss = 0.23, predict loss = 0.05 (78.9 examples/sec; 0.051 sec/batch; 1h:24m:17s remains)
INFO - root - 2019-11-06 20:58:47.048650: step 50190, total loss = 0.18, predict loss = 0.05 (81.2 examples/sec; 0.049 sec/batch; 1h:21m:56s remains)
INFO - root - 2019-11-06 20:58:47.630093: step 50200, total loss = 0.21, predict loss = 0.05 (77.0 examples/sec; 0.052 sec/batch; 1h:26m:27s remains)
INFO - root - 2019-11-06 20:58:48.174236: step 50210, total loss = 0.19, predict loss = 0.05 (93.9 examples/sec; 0.043 sec/batch; 1h:10m:49s remains)
INFO - root - 2019-11-06 20:58:48.617955: step 50220, total loss = 0.26, predict loss = 0.07 (92.1 examples/sec; 0.043 sec/batch; 1h:12m:11s remains)
INFO - root - 2019-11-06 20:58:49.070485: step 50230, total loss = 0.18, predict loss = 0.05 (102.5 examples/sec; 0.039 sec/batch; 1h:04m:54s remains)
INFO - root - 2019-11-06 20:58:50.132550: step 50240, total loss = 0.21, predict loss = 0.05 (55.4 examples/sec; 0.072 sec/batch; 1h:59m:57s remains)
INFO - root - 2019-11-06 20:58:50.785782: step 50250, total loss = 0.18, predict loss = 0.04 (74.2 examples/sec; 0.054 sec/batch; 1h:29m:39s remains)
INFO - root - 2019-11-06 20:58:51.359076: step 50260, total loss = 0.27, predict loss = 0.07 (84.8 examples/sec; 0.047 sec/batch; 1h:18m:23s remains)
INFO - root - 2019-11-06 20:58:51.965859: step 50270, total loss = 0.15, predict loss = 0.04 (72.7 examples/sec; 0.055 sec/batch; 1h:31m:30s remains)
INFO - root - 2019-11-06 20:58:52.539733: step 50280, total loss = 0.24, predict loss = 0.06 (82.2 examples/sec; 0.049 sec/batch; 1h:20m:53s remains)
INFO - root - 2019-11-06 20:58:53.121813: step 50290, total loss = 0.22, predict loss = 0.06 (81.9 examples/sec; 0.049 sec/batch; 1h:21m:10s remains)
INFO - root - 2019-11-06 20:58:53.691409: step 50300, total loss = 0.30, predict loss = 0.08 (75.0 examples/sec; 0.053 sec/batch; 1h:28m:36s remains)
INFO - root - 2019-11-06 20:58:54.259398: step 50310, total loss = 0.17, predict loss = 0.04 (83.2 examples/sec; 0.048 sec/batch; 1h:19m:55s remains)
INFO - root - 2019-11-06 20:58:54.831480: step 50320, total loss = 0.18, predict loss = 0.05 (77.0 examples/sec; 0.052 sec/batch; 1h:26m:18s remains)
INFO - root - 2019-11-06 20:58:55.424267: step 50330, total loss = 0.39, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 1h:29m:55s remains)
INFO - root - 2019-11-06 20:58:55.993152: step 50340, total loss = 0.21, predict loss = 0.05 (77.9 examples/sec; 0.051 sec/batch; 1h:25m:20s remains)
INFO - root - 2019-11-06 20:58:56.567320: step 50350, total loss = 0.16, predict loss = 0.04 (72.8 examples/sec; 0.055 sec/batch; 1h:31m:11s remains)
INFO - root - 2019-11-06 20:58:57.068793: step 50360, total loss = 0.21, predict loss = 0.06 (104.3 examples/sec; 0.038 sec/batch; 1h:03m:40s remains)
INFO - root - 2019-11-06 20:58:57.538716: step 50370, total loss = 0.24, predict loss = 0.07 (103.2 examples/sec; 0.039 sec/batch; 1h:04m:22s remains)
INFO - root - 2019-11-06 20:58:57.998325: step 50380, total loss = 0.17, predict loss = 0.04 (95.7 examples/sec; 0.042 sec/batch; 1h:09m:23s remains)
INFO - root - 2019-11-06 20:58:59.085105: step 50390, total loss = 0.18, predict loss = 0.05 (56.6 examples/sec; 0.071 sec/batch; 1h:57m:16s remains)
INFO - root - 2019-11-06 20:58:59.700126: step 50400, total loss = 0.22, predict loss = 0.06 (80.4 examples/sec; 0.050 sec/batch; 1h:22m:35s remains)
INFO - root - 2019-11-06 20:59:00.309173: step 50410, total loss = 0.19, predict loss = 0.05 (77.6 examples/sec; 0.052 sec/batch; 1h:25m:33s remains)
INFO - root - 2019-11-06 20:59:00.897112: step 50420, total loss = 0.23, predict loss = 0.08 (75.5 examples/sec; 0.053 sec/batch; 1h:27m:56s remains)
INFO - root - 2019-11-06 20:59:01.482643: step 50430, total loss = 0.21, predict loss = 0.05 (80.6 examples/sec; 0.050 sec/batch; 1h:22m:23s remains)
INFO - root - 2019-11-06 20:59:02.060166: step 50440, total loss = 0.16, predict loss = 0.04 (77.3 examples/sec; 0.052 sec/batch; 1h:25m:53s remains)
INFO - root - 2019-11-06 20:59:02.659307: step 50450, total loss = 0.16, predict loss = 0.04 (77.0 examples/sec; 0.052 sec/batch; 1h:26m:08s remains)
INFO - root - 2019-11-06 20:59:03.238319: step 50460, total loss = 0.21, predict loss = 0.06 (73.2 examples/sec; 0.055 sec/batch; 1h:30m:36s remains)
INFO - root - 2019-11-06 20:59:03.820565: step 50470, total loss = 0.28, predict loss = 0.07 (82.7 examples/sec; 0.048 sec/batch; 1h:20m:16s remains)
INFO - root - 2019-11-06 20:59:04.402523: step 50480, total loss = 0.18, predict loss = 0.05 (77.9 examples/sec; 0.051 sec/batch; 1h:25m:06s remains)
INFO - root - 2019-11-06 20:59:04.994440: step 50490, total loss = 0.24, predict loss = 0.06 (76.0 examples/sec; 0.053 sec/batch; 1h:27m:16s remains)
INFO - root - 2019-11-06 20:59:05.573877: step 50500, total loss = 0.23, predict loss = 0.05 (79.7 examples/sec; 0.050 sec/batch; 1h:23m:11s remains)
INFO - root - 2019-11-06 20:59:06.058277: step 50510, total loss = 0.28, predict loss = 0.08 (103.7 examples/sec; 0.039 sec/batch; 1h:03m:56s remains)
INFO - root - 2019-11-06 20:59:06.505401: step 50520, total loss = 0.23, predict loss = 0.06 (100.6 examples/sec; 0.040 sec/batch; 1h:05m:56s remains)
INFO - root - 2019-11-06 20:59:07.435646: step 50530, total loss = 0.26, predict loss = 0.08 (8.2 examples/sec; 0.489 sec/batch; 13h:30m:36s remains)
INFO - root - 2019-11-06 20:59:08.065764: step 50540, total loss = 0.19, predict loss = 0.05 (59.6 examples/sec; 0.067 sec/batch; 1h:51m:19s remains)
INFO - root - 2019-11-06 20:59:08.681195: step 50550, total loss = 0.23, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:23m:48s remains)
INFO - root - 2019-11-06 20:59:09.277865: step 50560, total loss = 0.20, predict loss = 0.05 (73.4 examples/sec; 0.055 sec/batch; 1h:30m:21s remains)
INFO - root - 2019-11-06 20:59:09.861684: step 50570, total loss = 0.29, predict loss = 0.07 (77.5 examples/sec; 0.052 sec/batch; 1h:25m:28s remains)
INFO - root - 2019-11-06 20:59:10.440313: step 50580, total loss = 0.25, predict loss = 0.07 (83.0 examples/sec; 0.048 sec/batch; 1h:19m:51s remains)
INFO - root - 2019-11-06 20:59:11.022168: step 50590, total loss = 0.15, predict loss = 0.04 (73.9 examples/sec; 0.054 sec/batch; 1h:29m:43s remains)
INFO - root - 2019-11-06 20:59:11.597462: step 50600, total loss = 0.18, predict loss = 0.05 (75.5 examples/sec; 0.053 sec/batch; 1h:27m:48s remains)
INFO - root - 2019-11-06 20:59:12.200205: step 50610, total loss = 0.21, predict loss = 0.06 (75.1 examples/sec; 0.053 sec/batch; 1h:28m:15s remains)
INFO - root - 2019-11-06 20:59:12.775858: step 50620, total loss = 0.20, predict loss = 0.05 (78.7 examples/sec; 0.051 sec/batch; 1h:24m:11s remains)
INFO - root - 2019-11-06 20:59:13.356540: step 50630, total loss = 0.22, predict loss = 0.06 (82.5 examples/sec; 0.048 sec/batch; 1h:20m:17s remains)
INFO - root - 2019-11-06 20:59:13.936402: step 50640, total loss = 0.19, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:23m:34s remains)
INFO - root - 2019-11-06 20:59:14.525050: step 50650, total loss = 0.17, predict loss = 0.05 (84.5 examples/sec; 0.047 sec/batch; 1h:18m:20s remains)
INFO - root - 2019-11-06 20:59:14.987808: step 50660, total loss = 0.25, predict loss = 0.07 (98.5 examples/sec; 0.041 sec/batch; 1h:07m:13s remains)
INFO - root - 2019-11-06 20:59:15.427692: step 50670, total loss = 0.28, predict loss = 0.08 (97.2 examples/sec; 0.041 sec/batch; 1h:08m:06s remains)
INFO - root - 2019-11-06 20:59:16.329452: step 50680, total loss = 0.21, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:24m:52s remains)
INFO - root - 2019-11-06 20:59:17.028895: step 50690, total loss = 0.25, predict loss = 0.07 (64.4 examples/sec; 0.062 sec/batch; 1h:42m:52s remains)
INFO - root - 2019-11-06 20:59:17.652826: step 50700, total loss = 0.24, predict loss = 0.06 (83.1 examples/sec; 0.048 sec/batch; 1h:19m:42s remains)
INFO - root - 2019-11-06 20:59:18.225861: step 50710, total loss = 0.25, predict loss = 0.06 (79.5 examples/sec; 0.050 sec/batch; 1h:23m:16s remains)
INFO - root - 2019-11-06 20:59:18.797729: step 50720, total loss = 0.19, predict loss = 0.05 (78.2 examples/sec; 0.051 sec/batch; 1h:24m:39s remains)
INFO - root - 2019-11-06 20:59:19.389219: step 50730, total loss = 0.29, predict loss = 0.08 (79.6 examples/sec; 0.050 sec/batch; 1h:23m:09s remains)
INFO - root - 2019-11-06 20:59:19.974551: step 50740, total loss = 0.17, predict loss = 0.04 (77.7 examples/sec; 0.051 sec/batch; 1h:25m:08s remains)
INFO - root - 2019-11-06 20:59:20.546867: step 50750, total loss = 0.18, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:23m:26s remains)
INFO - root - 2019-11-06 20:59:21.130743: step 50760, total loss = 0.21, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:23m:36s remains)
INFO - root - 2019-11-06 20:59:21.726389: step 50770, total loss = 0.30, predict loss = 0.10 (76.6 examples/sec; 0.052 sec/batch; 1h:26m:20s remains)
INFO - root - 2019-11-06 20:59:22.280347: step 50780, total loss = 0.22, predict loss = 0.05 (79.0 examples/sec; 0.051 sec/batch; 1h:23m:45s remains)
INFO - root - 2019-11-06 20:59:22.865664: step 50790, total loss = 0.17, predict loss = 0.04 (72.4 examples/sec; 0.055 sec/batch; 1h:31m:18s remains)
INFO - root - 2019-11-06 20:59:23.426436: step 50800, total loss = 0.25, predict loss = 0.07 (90.1 examples/sec; 0.044 sec/batch; 1h:13m:23s remains)
INFO - root - 2019-11-06 20:59:23.895018: step 50810, total loss = 0.22, predict loss = 0.05 (100.0 examples/sec; 0.040 sec/batch; 1h:06m:08s remains)
INFO - root - 2019-11-06 20:59:24.332289: step 50820, total loss = 0.28, predict loss = 0.07 (101.5 examples/sec; 0.039 sec/batch; 1h:05m:07s remains)
INFO - root - 2019-11-06 20:59:25.269029: step 50830, total loss = 0.21, predict loss = 0.05 (78.9 examples/sec; 0.051 sec/batch; 1h:23m:48s remains)
INFO - root - 2019-11-06 20:59:25.919117: step 50840, total loss = 0.21, predict loss = 0.05 (71.1 examples/sec; 0.056 sec/batch; 1h:32m:56s remains)
INFO - root - 2019-11-06 20:59:26.587072: step 50850, total loss = 0.26, predict loss = 0.08 (69.0 examples/sec; 0.058 sec/batch; 1h:35m:46s remains)
INFO - root - 2019-11-06 20:59:27.184963: step 50860, total loss = 0.19, predict loss = 0.06 (78.1 examples/sec; 0.051 sec/batch; 1h:24m:38s remains)
INFO - root - 2019-11-06 20:59:27.763031: step 50870, total loss = 0.21, predict loss = 0.06 (78.1 examples/sec; 0.051 sec/batch; 1h:24m:37s remains)
INFO - root - 2019-11-06 20:59:28.339569: step 50880, total loss = 0.21, predict loss = 0.06 (79.2 examples/sec; 0.050 sec/batch; 1h:23m:25s remains)
INFO - root - 2019-11-06 20:59:28.924965: step 50890, total loss = 0.19, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:23m:34s remains)
INFO - root - 2019-11-06 20:59:29.497602: step 50900, total loss = 0.14, predict loss = 0.03 (81.9 examples/sec; 0.049 sec/batch; 1h:20m:40s remains)
INFO - root - 2019-11-06 20:59:30.073913: step 50910, total loss = 0.26, predict loss = 0.07 (81.0 examples/sec; 0.049 sec/batch; 1h:21m:33s remains)
INFO - root - 2019-11-06 20:59:30.689688: step 50920, total loss = 0.22, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:23m:50s remains)
INFO - root - 2019-11-06 20:59:31.288714: step 50930, total loss = 0.19, predict loss = 0.05 (77.8 examples/sec; 0.051 sec/batch; 1h:24m:55s remains)
INFO - root - 2019-11-06 20:59:31.864804: step 50940, total loss = 0.19, predict loss = 0.05 (74.4 examples/sec; 0.054 sec/batch; 1h:28m:49s remains)
INFO - root - 2019-11-06 20:59:32.406557: step 50950, total loss = 0.29, predict loss = 0.08 (93.3 examples/sec; 0.043 sec/batch; 1h:10m:48s remains)
INFO - root - 2019-11-06 20:59:32.856735: step 50960, total loss = 0.22, predict loss = 0.05 (104.2 examples/sec; 0.038 sec/batch; 1h:03m:20s remains)
INFO - root - 2019-11-06 20:59:33.346913: step 50970, total loss = 0.24, predict loss = 0.07 (94.6 examples/sec; 0.042 sec/batch; 1h:09m:45s remains)
INFO - root - 2019-11-06 20:59:34.321102: step 50980, total loss = 0.20, predict loss = 0.05 (61.8 examples/sec; 0.065 sec/batch; 1h:46m:45s remains)
INFO - root - 2019-11-06 20:59:34.937604: step 50990, total loss = 0.24, predict loss = 0.06 (77.0 examples/sec; 0.052 sec/batch; 1h:25m:41s remains)
INFO - root - 2019-11-06 20:59:35.526347: step 51000, total loss = 0.23, predict loss = 0.06 (82.3 examples/sec; 0.049 sec/batch; 1h:20m:14s remains)
INFO - root - 2019-11-06 20:59:36.125352: step 51010, total loss = 0.26, predict loss = 0.08 (81.5 examples/sec; 0.049 sec/batch; 1h:20m:59s remains)
INFO - root - 2019-11-06 20:59:36.710106: step 51020, total loss = 0.27, predict loss = 0.08 (81.3 examples/sec; 0.049 sec/batch; 1h:21m:11s remains)
INFO - root - 2019-11-06 20:59:37.289039: step 51030, total loss = 0.27, predict loss = 0.08 (81.3 examples/sec; 0.049 sec/batch; 1h:21m:08s remains)
INFO - root - 2019-11-06 20:59:37.861283: step 51040, total loss = 0.19, predict loss = 0.04 (74.7 examples/sec; 0.054 sec/batch; 1h:28m:22s remains)
INFO - root - 2019-11-06 20:59:38.450765: step 51050, total loss = 0.13, predict loss = 0.03 (76.3 examples/sec; 0.052 sec/batch; 1h:26m:24s remains)
INFO - root - 2019-11-06 20:59:39.039489: step 51060, total loss = 0.29, predict loss = 0.08 (78.7 examples/sec; 0.051 sec/batch; 1h:23m:51s remains)
INFO - root - 2019-11-06 20:59:39.626884: step 51070, total loss = 0.19, predict loss = 0.06 (78.7 examples/sec; 0.051 sec/batch; 1h:23m:49s remains)
INFO - root - 2019-11-06 20:59:40.209696: step 51080, total loss = 0.17, predict loss = 0.04 (76.9 examples/sec; 0.052 sec/batch; 1h:25m:45s remains)
INFO - root - 2019-11-06 20:59:40.797939: step 51090, total loss = 0.16, predict loss = 0.04 (76.7 examples/sec; 0.052 sec/batch; 1h:25m:59s remains)
INFO - root - 2019-11-06 20:59:41.324551: step 51100, total loss = 0.22, predict loss = 0.06 (100.9 examples/sec; 0.040 sec/batch; 1h:05m:19s remains)
INFO - root - 2019-11-06 20:59:41.792310: step 51110, total loss = 0.30, predict loss = 0.08 (92.1 examples/sec; 0.043 sec/batch; 1h:11m:34s remains)
INFO - root - 2019-11-06 20:59:42.258451: step 51120, total loss = 0.16, predict loss = 0.04 (94.1 examples/sec; 0.043 sec/batch; 1h:10m:05s remains)
INFO - root - 2019-11-06 20:59:43.317305: step 51130, total loss = 0.26, predict loss = 0.06 (62.8 examples/sec; 0.064 sec/batch; 1h:44m:53s remains)
INFO - root - 2019-11-06 20:59:43.933985: step 51140, total loss = 0.15, predict loss = 0.03 (80.0 examples/sec; 0.050 sec/batch; 1h:22m:22s remains)
INFO - root - 2019-11-06 20:59:44.513469: step 51150, total loss = 0.15, predict loss = 0.04 (78.9 examples/sec; 0.051 sec/batch; 1h:23m:29s remains)
INFO - root - 2019-11-06 20:59:45.090372: step 51160, total loss = 0.20, predict loss = 0.05 (74.1 examples/sec; 0.054 sec/batch; 1h:28m:52s remains)
INFO - root - 2019-11-06 20:59:45.687017: step 51170, total loss = 0.25, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 1h:24m:10s remains)
INFO - root - 2019-11-06 20:59:46.255410: step 51180, total loss = 0.24, predict loss = 0.06 (78.4 examples/sec; 0.051 sec/batch; 1h:24m:01s remains)
INFO - root - 2019-11-06 20:59:46.826102: step 51190, total loss = 0.43, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 1h:24m:51s remains)
INFO - root - 2019-11-06 20:59:47.395720: step 51200, total loss = 0.28, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 1h:23m:18s remains)
INFO - root - 2019-11-06 20:59:47.982610: step 51210, total loss = 0.27, predict loss = 0.10 (76.8 examples/sec; 0.052 sec/batch; 1h:25m:44s remains)
INFO - root - 2019-11-06 20:59:48.555235: step 51220, total loss = 0.13, predict loss = 0.03 (80.2 examples/sec; 0.050 sec/batch; 1h:22m:06s remains)
INFO - root - 2019-11-06 20:59:49.131033: step 51230, total loss = 0.24, predict loss = 0.07 (77.3 examples/sec; 0.052 sec/batch; 1h:25m:07s remains)
INFO - root - 2019-11-06 20:59:49.713770: step 51240, total loss = 0.19, predict loss = 0.05 (76.6 examples/sec; 0.052 sec/batch; 1h:25m:57s remains)
INFO - root - 2019-11-06 20:59:50.223018: step 51250, total loss = 0.30, predict loss = 0.07 (104.4 examples/sec; 0.038 sec/batch; 1h:03m:04s remains)
INFO - root - 2019-11-06 20:59:50.665587: step 51260, total loss = 0.22, predict loss = 0.06 (96.8 examples/sec; 0.041 sec/batch; 1h:07m:59s remains)
INFO - root - 2019-11-06 20:59:51.122368: step 51270, total loss = 0.25, predict loss = 0.07 (94.5 examples/sec; 0.042 sec/batch; 1h:09m:38s remains)
INFO - root - 2019-11-06 20:59:52.211384: step 51280, total loss = 0.20, predict loss = 0.05 (66.3 examples/sec; 0.060 sec/batch; 1h:39m:17s remains)
INFO - root - 2019-11-06 20:59:52.876419: step 51290, total loss = 0.23, predict loss = 0.07 (70.5 examples/sec; 0.057 sec/batch; 1h:33m:18s remains)
INFO - root - 2019-11-06 20:59:53.462302: step 51300, total loss = 0.19, predict loss = 0.05 (75.7 examples/sec; 0.053 sec/batch; 1h:26m:52s remains)
INFO - root - 2019-11-06 20:59:54.039096: step 51310, total loss = 0.17, predict loss = 0.04 (79.2 examples/sec; 0.050 sec/batch; 1h:23m:01s remains)
INFO - root - 2019-11-06 20:59:54.618436: step 51320, total loss = 0.24, predict loss = 0.05 (78.3 examples/sec; 0.051 sec/batch; 1h:24m:02s remains)
INFO - root - 2019-11-06 20:59:55.203137: step 51330, total loss = 0.26, predict loss = 0.07 (78.7 examples/sec; 0.051 sec/batch; 1h:23m:35s remains)
INFO - root - 2019-11-06 20:59:55.785666: step 51340, total loss = 0.14, predict loss = 0.03 (80.3 examples/sec; 0.050 sec/batch; 1h:21m:53s remains)
INFO - root - 2019-11-06 20:59:56.354266: step 51350, total loss = 0.26, predict loss = 0.06 (80.6 examples/sec; 0.050 sec/batch; 1h:21m:34s remains)
INFO - root - 2019-11-06 20:59:56.936699: step 51360, total loss = 0.25, predict loss = 0.07 (75.3 examples/sec; 0.053 sec/batch; 1h:27m:22s remains)
INFO - root - 2019-11-06 20:59:57.546537: step 51370, total loss = 0.19, predict loss = 0.05 (82.0 examples/sec; 0.049 sec/batch; 1h:20m:12s remains)
INFO - root - 2019-11-06 20:59:58.132967: step 51380, total loss = 0.18, predict loss = 0.05 (74.9 examples/sec; 0.053 sec/batch; 1h:27m:45s remains)
INFO - root - 2019-11-06 20:59:58.716181: step 51390, total loss = 0.24, predict loss = 0.06 (76.5 examples/sec; 0.052 sec/batch; 1h:25m:58s remains)
INFO - root - 2019-11-06 20:59:59.180175: step 51400, total loss = 0.19, predict loss = 0.05 (96.6 examples/sec; 0.041 sec/batch; 1h:08m:03s remains)
INFO - root - 2019-11-06 20:59:59.660549: step 51410, total loss = 0.30, predict loss = 0.08 (97.7 examples/sec; 0.041 sec/batch; 1h:07m:16s remains)
INFO - root - 2019-11-06 21:00:00.617974: step 51420, total loss = 0.17, predict loss = 0.04 (76.2 examples/sec; 0.052 sec/batch; 1h:26m:13s remains)
INFO - root - 2019-11-06 21:00:01.280363: step 51430, total loss = 0.14, predict loss = 0.03 (63.8 examples/sec; 0.063 sec/batch; 1h:42m:58s remains)
INFO - root - 2019-11-06 21:00:01.890109: step 51440, total loss = 0.25, predict loss = 0.06 (79.6 examples/sec; 0.050 sec/batch; 1h:22m:32s remains)
INFO - root - 2019-11-06 21:00:02.480701: step 51450, total loss = 0.21, predict loss = 0.05 (73.4 examples/sec; 0.054 sec/batch; 1h:29m:29s remains)
INFO - root - 2019-11-06 21:00:03.061537: step 51460, total loss = 0.25, predict loss = 0.07 (77.4 examples/sec; 0.052 sec/batch; 1h:24m:52s remains)
INFO - root - 2019-11-06 21:00:03.633011: step 51470, total loss = 0.17, predict loss = 0.05 (75.7 examples/sec; 0.053 sec/batch; 1h:26m:47s remains)
INFO - root - 2019-11-06 21:00:04.224486: step 51480, total loss = 0.23, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:23m:02s remains)
INFO - root - 2019-11-06 21:00:04.809756: step 51490, total loss = 0.21, predict loss = 0.05 (76.2 examples/sec; 0.053 sec/batch; 1h:26m:12s remains)
INFO - root - 2019-11-06 21:00:05.398303: step 51500, total loss = 0.24, predict loss = 0.06 (82.9 examples/sec; 0.048 sec/batch; 1h:19m:14s remains)
INFO - root - 2019-11-06 21:00:05.984197: step 51510, total loss = 0.28, predict loss = 0.08 (81.5 examples/sec; 0.049 sec/batch; 1h:20m:34s remains)
INFO - root - 2019-11-06 21:00:06.570320: step 51520, total loss = 0.15, predict loss = 0.04 (79.7 examples/sec; 0.050 sec/batch; 1h:22m:22s remains)
INFO - root - 2019-11-06 21:00:07.165901: step 51530, total loss = 0.18, predict loss = 0.05 (74.0 examples/sec; 0.054 sec/batch; 1h:28m:42s remains)
INFO - root - 2019-11-06 21:00:07.737646: step 51540, total loss = 0.16, predict loss = 0.04 (85.7 examples/sec; 0.047 sec/batch; 1h:16m:35s remains)
INFO - root - 2019-11-06 21:00:08.192360: step 51550, total loss = 0.26, predict loss = 0.08 (94.4 examples/sec; 0.042 sec/batch; 1h:09m:32s remains)
INFO - root - 2019-11-06 21:00:08.650657: step 51560, total loss = 0.27, predict loss = 0.07 (94.6 examples/sec; 0.042 sec/batch; 1h:09m:20s remains)
INFO - root - 2019-11-06 21:00:09.611614: step 51570, total loss = 0.27, predict loss = 0.08 (71.2 examples/sec; 0.056 sec/batch; 1h:32m:09s remains)
INFO - root - 2019-11-06 21:00:10.277807: step 51580, total loss = 0.25, predict loss = 0.06 (74.9 examples/sec; 0.053 sec/batch; 1h:27m:36s remains)
INFO - root - 2019-11-06 21:00:10.881819: step 51590, total loss = 0.17, predict loss = 0.04 (76.2 examples/sec; 0.052 sec/batch; 1h:26m:04s remains)
INFO - root - 2019-11-06 21:00:11.459896: step 51600, total loss = 0.29, predict loss = 0.07 (79.2 examples/sec; 0.050 sec/batch; 1h:22m:47s remains)
INFO - root - 2019-11-06 21:00:12.057017: step 51610, total loss = 0.21, predict loss = 0.05 (75.4 examples/sec; 0.053 sec/batch; 1h:26m:59s remains)
INFO - root - 2019-11-06 21:00:12.644455: step 51620, total loss = 0.26, predict loss = 0.06 (78.7 examples/sec; 0.051 sec/batch; 1h:23m:19s remains)
INFO - root - 2019-11-06 21:00:13.228491: step 51630, total loss = 0.20, predict loss = 0.06 (75.6 examples/sec; 0.053 sec/batch; 1h:26m:47s remains)
INFO - root - 2019-11-06 21:00:13.798543: step 51640, total loss = 0.14, predict loss = 0.03 (79.6 examples/sec; 0.050 sec/batch; 1h:22m:25s remains)
INFO - root - 2019-11-06 21:00:14.383000: step 51650, total loss = 0.29, predict loss = 0.08 (76.5 examples/sec; 0.052 sec/batch; 1h:25m:43s remains)
INFO - root - 2019-11-06 21:00:14.953522: step 51660, total loss = 0.30, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 1h:23m:48s remains)
INFO - root - 2019-11-06 21:00:15.534149: step 51670, total loss = 0.25, predict loss = 0.06 (73.6 examples/sec; 0.054 sec/batch; 1h:29m:01s remains)
INFO - root - 2019-11-06 21:00:16.117856: step 51680, total loss = 0.17, predict loss = 0.04 (74.3 examples/sec; 0.054 sec/batch; 1h:28m:09s remains)
INFO - root - 2019-11-06 21:00:16.680627: step 51690, total loss = 0.20, predict loss = 0.05 (94.7 examples/sec; 0.042 sec/batch; 1h:09m:12s remains)
INFO - root - 2019-11-06 21:00:17.129973: step 51700, total loss = 0.30, predict loss = 0.09 (100.4 examples/sec; 0.040 sec/batch; 1h:05m:17s remains)
INFO - root - 2019-11-06 21:00:17.575567: step 51710, total loss = 0.29, predict loss = 0.08 (98.0 examples/sec; 0.041 sec/batch; 1h:06m:50s remains)
INFO - root - 2019-11-06 21:00:18.569668: step 51720, total loss = 0.24, predict loss = 0.06 (56.3 examples/sec; 0.071 sec/batch; 1h:56m:25s remains)
INFO - root - 2019-11-06 21:00:19.313331: step 51730, total loss = 0.17, predict loss = 0.04 (61.8 examples/sec; 0.065 sec/batch; 1h:46m:02s remains)
INFO - root - 2019-11-06 21:00:19.938692: step 51740, total loss = 0.23, predict loss = 0.06 (79.6 examples/sec; 0.050 sec/batch; 1h:22m:16s remains)
INFO - root - 2019-11-06 21:00:20.531275: step 51750, total loss = 0.24, predict loss = 0.06 (73.6 examples/sec; 0.054 sec/batch; 1h:28m:56s remains)
INFO - root - 2019-11-06 21:00:21.106082: step 51760, total loss = 0.23, predict loss = 0.06 (79.2 examples/sec; 0.050 sec/batch; 1h:22m:40s remains)
INFO - root - 2019-11-06 21:00:21.702345: step 51770, total loss = 0.20, predict loss = 0.05 (80.4 examples/sec; 0.050 sec/batch; 1h:21m:24s remains)
INFO - root - 2019-11-06 21:00:22.286085: step 51780, total loss = 0.28, predict loss = 0.08 (76.3 examples/sec; 0.052 sec/batch; 1h:25m:51s remains)
INFO - root - 2019-11-06 21:00:22.858778: step 51790, total loss = 0.16, predict loss = 0.04 (77.7 examples/sec; 0.051 sec/batch; 1h:24m:15s remains)
INFO - root - 2019-11-06 21:00:23.431926: step 51800, total loss = 0.25, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 1h:24m:25s remains)
INFO - root - 2019-11-06 21:00:24.028755: step 51810, total loss = 0.21, predict loss = 0.06 (76.3 examples/sec; 0.052 sec/batch; 1h:25m:46s remains)
INFO - root - 2019-11-06 21:00:24.604652: step 51820, total loss = 0.27, predict loss = 0.07 (77.3 examples/sec; 0.052 sec/batch; 1h:24m:41s remains)
INFO - root - 2019-11-06 21:00:25.177916: step 51830, total loss = 0.18, predict loss = 0.04 (76.0 examples/sec; 0.053 sec/batch; 1h:26m:03s remains)
INFO - root - 2019-11-06 21:00:25.713408: step 51840, total loss = 0.27, predict loss = 0.07 (94.9 examples/sec; 0.042 sec/batch; 1h:08m:57s remains)
INFO - root - 2019-11-06 21:00:26.189567: step 51850, total loss = 0.37, predict loss = 0.11 (94.0 examples/sec; 0.043 sec/batch; 1h:09m:38s remains)
INFO - root - 2019-11-06 21:00:26.648817: step 51860, total loss = 0.25, predict loss = 0.06 (92.9 examples/sec; 0.043 sec/batch; 1h:10m:23s remains)
INFO - root - 2019-11-06 21:00:27.666065: step 51870, total loss = 0.25, predict loss = 0.06 (62.7 examples/sec; 0.064 sec/batch; 1h:44m:22s remains)
INFO - root - 2019-11-06 21:00:28.298155: step 51880, total loss = 0.27, predict loss = 0.07 (74.4 examples/sec; 0.054 sec/batch; 1h:27m:56s remains)
INFO - root - 2019-11-06 21:00:28.893642: step 51890, total loss = 0.31, predict loss = 0.08 (77.1 examples/sec; 0.052 sec/batch; 1h:24m:50s remains)
INFO - root - 2019-11-06 21:00:29.466781: step 51900, total loss = 0.26, predict loss = 0.07 (81.1 examples/sec; 0.049 sec/batch; 1h:20m:38s remains)
INFO - root - 2019-11-06 21:00:30.046632: step 51910, total loss = 0.25, predict loss = 0.07 (76.6 examples/sec; 0.052 sec/batch; 1h:25m:22s remains)
INFO - root - 2019-11-06 21:00:30.650326: step 51920, total loss = 0.24, predict loss = 0.07 (76.7 examples/sec; 0.052 sec/batch; 1h:25m:11s remains)
INFO - root - 2019-11-06 21:00:31.259973: step 51930, total loss = 0.14, predict loss = 0.04 (72.0 examples/sec; 0.056 sec/batch; 1h:30m:50s remains)
INFO - root - 2019-11-06 21:00:31.840369: step 51940, total loss = 0.19, predict loss = 0.05 (79.0 examples/sec; 0.051 sec/batch; 1h:22m:42s remains)
INFO - root - 2019-11-06 21:00:32.406386: step 51950, total loss = 0.16, predict loss = 0.04 (80.8 examples/sec; 0.049 sec/batch; 1h:20m:52s remains)
INFO - root - 2019-11-06 21:00:32.998373: step 51960, total loss = 0.14, predict loss = 0.04 (78.4 examples/sec; 0.051 sec/batch; 1h:23m:24s remains)
INFO - root - 2019-11-06 21:00:33.580736: step 51970, total loss = 0.14, predict loss = 0.04 (80.8 examples/sec; 0.049 sec/batch; 1h:20m:50s remains)
INFO - root - 2019-11-06 21:00:34.159772: step 51980, total loss = 0.22, predict loss = 0.06 (81.6 examples/sec; 0.049 sec/batch; 1h:20m:04s remains)
INFO - root - 2019-11-06 21:00:34.663836: step 51990, total loss = 0.35, predict loss = 0.10 (102.5 examples/sec; 0.039 sec/batch; 1h:03m:43s remains)
INFO - root - 2019-11-06 21:00:35.109510: step 52000, total loss = 0.26, predict loss = 0.06 (97.1 examples/sec; 0.041 sec/batch; 1h:07m:18s remains)
INFO - root - 2019-11-06 21:00:35.587368: step 52010, total loss = 0.19, predict loss = 0.05 (86.9 examples/sec; 0.046 sec/batch; 1h:15m:11s remains)
INFO - root - 2019-11-06 21:00:36.646777: step 52020, total loss = 0.25, predict loss = 0.06 (64.7 examples/sec; 0.062 sec/batch; 1h:40m:53s remains)
INFO - root - 2019-11-06 21:00:37.255835: step 52030, total loss = 0.23, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:22m:49s remains)
INFO - root - 2019-11-06 21:00:37.835302: step 52040, total loss = 0.19, predict loss = 0.05 (79.7 examples/sec; 0.050 sec/batch; 1h:21m:54s remains)
INFO - root - 2019-11-06 21:00:38.416959: step 52050, total loss = 0.21, predict loss = 0.05 (79.0 examples/sec; 0.051 sec/batch; 1h:22m:36s remains)
INFO - root - 2019-11-06 21:00:38.993642: step 52060, total loss = 0.26, predict loss = 0.07 (79.4 examples/sec; 0.050 sec/batch; 1h:22m:13s remains)
INFO - root - 2019-11-06 21:00:39.573944: step 52070, total loss = 0.23, predict loss = 0.06 (76.3 examples/sec; 0.052 sec/batch; 1h:25m:31s remains)
INFO - root - 2019-11-06 21:00:40.146154: step 52080, total loss = 0.31, predict loss = 0.09 (72.4 examples/sec; 0.055 sec/batch; 1h:30m:09s remains)
INFO - root - 2019-11-06 21:00:40.729146: step 52090, total loss = 0.22, predict loss = 0.05 (82.8 examples/sec; 0.048 sec/batch; 1h:18m:48s remains)
INFO - root - 2019-11-06 21:00:41.306351: step 52100, total loss = 0.24, predict loss = 0.07 (78.1 examples/sec; 0.051 sec/batch; 1h:23m:34s remains)
INFO - root - 2019-11-06 21:00:41.879869: step 52110, total loss = 0.14, predict loss = 0.03 (73.5 examples/sec; 0.054 sec/batch; 1h:28m:49s remains)
INFO - root - 2019-11-06 21:00:42.466571: step 52120, total loss = 0.36, predict loss = 0.10 (81.4 examples/sec; 0.049 sec/batch; 1h:20m:10s remains)
INFO - root - 2019-11-06 21:00:43.066622: step 52130, total loss = 0.18, predict loss = 0.05 (83.4 examples/sec; 0.048 sec/batch; 1h:18m:13s remains)
INFO - root - 2019-11-06 21:00:43.549528: step 52140, total loss = 0.26, predict loss = 0.07 (101.9 examples/sec; 0.039 sec/batch; 1h:04m:01s remains)
INFO - root - 2019-11-06 21:00:43.999061: step 52150, total loss = 0.31, predict loss = 0.08 (99.8 examples/sec; 0.040 sec/batch; 1h:05m:23s remains)
INFO - root - 2019-11-06 21:00:44.915246: step 52160, total loss = 0.23, predict loss = 0.06 (8.0 examples/sec; 0.500 sec/batch; 13h:35m:08s remains)
INFO - root - 2019-11-06 21:00:45.616870: step 52170, total loss = 0.16, predict loss = 0.04 (58.9 examples/sec; 0.068 sec/batch; 1h:50m:47s remains)
INFO - root - 2019-11-06 21:00:46.232457: step 52180, total loss = 0.22, predict loss = 0.05 (80.8 examples/sec; 0.050 sec/batch; 1h:20m:45s remains)
INFO - root - 2019-11-06 21:00:46.806824: step 52190, total loss = 0.18, predict loss = 0.04 (79.6 examples/sec; 0.050 sec/batch; 1h:21m:55s remains)
INFO - root - 2019-11-06 21:00:47.383643: step 52200, total loss = 0.16, predict loss = 0.04 (77.0 examples/sec; 0.052 sec/batch; 1h:24m:42s remains)
INFO - root - 2019-11-06 21:00:47.973215: step 52210, total loss = 0.38, predict loss = 0.12 (81.0 examples/sec; 0.049 sec/batch; 1h:20m:30s remains)
INFO - root - 2019-11-06 21:00:48.549663: step 52220, total loss = 0.15, predict loss = 0.04 (77.8 examples/sec; 0.051 sec/batch; 1h:23m:46s remains)
INFO - root - 2019-11-06 21:00:49.113267: step 52230, total loss = 0.27, predict loss = 0.08 (77.1 examples/sec; 0.052 sec/batch; 1h:24m:32s remains)
INFO - root - 2019-11-06 21:00:49.684386: step 52240, total loss = 0.16, predict loss = 0.04 (78.1 examples/sec; 0.051 sec/batch; 1h:23m:24s remains)
INFO - root - 2019-11-06 21:00:50.279187: step 52250, total loss = 0.21, predict loss = 0.06 (72.8 examples/sec; 0.055 sec/batch; 1h:29m:33s remains)
INFO - root - 2019-11-06 21:00:50.854984: step 52260, total loss = 0.29, predict loss = 0.09 (80.0 examples/sec; 0.050 sec/batch; 1h:21m:28s remains)
INFO - root - 2019-11-06 21:00:51.425529: step 52270, total loss = 0.18, predict loss = 0.06 (75.9 examples/sec; 0.053 sec/batch; 1h:25m:52s remains)
INFO - root - 2019-11-06 21:00:51.990363: step 52280, total loss = 0.23, predict loss = 0.06 (85.8 examples/sec; 0.047 sec/batch; 1h:15m:56s remains)
INFO - root - 2019-11-06 21:00:52.472780: step 52290, total loss = 0.15, predict loss = 0.04 (97.7 examples/sec; 0.041 sec/batch; 1h:06m:40s remains)
INFO - root - 2019-11-06 21:00:52.933142: step 52300, total loss = 0.24, predict loss = 0.06 (91.1 examples/sec; 0.044 sec/batch; 1h:11m:29s remains)
INFO - root - 2019-11-06 21:00:53.853960: step 52310, total loss = 0.30, predict loss = 0.08 (74.8 examples/sec; 0.053 sec/batch; 1h:27m:03s remains)
INFO - root - 2019-11-06 21:00:54.501930: step 52320, total loss = 0.11, predict loss = 0.03 (67.5 examples/sec; 0.059 sec/batch; 1h:36m:30s remains)
INFO - root - 2019-11-06 21:00:55.128334: step 52330, total loss = 0.19, predict loss = 0.05 (78.7 examples/sec; 0.051 sec/batch; 1h:22m:45s remains)
INFO - root - 2019-11-06 21:00:55.712801: step 52340, total loss = 0.22, predict loss = 0.06 (71.0 examples/sec; 0.056 sec/batch; 1h:31m:42s remains)
INFO - root - 2019-11-06 21:00:56.287430: step 52350, total loss = 0.16, predict loss = 0.04 (76.1 examples/sec; 0.053 sec/batch; 1h:25m:31s remains)
INFO - root - 2019-11-06 21:00:56.869981: step 52360, total loss = 0.24, predict loss = 0.07 (79.4 examples/sec; 0.050 sec/batch; 1h:22m:00s remains)
INFO - root - 2019-11-06 21:00:57.463807: step 52370, total loss = 0.17, predict loss = 0.04 (79.0 examples/sec; 0.051 sec/batch; 1h:22m:22s remains)
INFO - root - 2019-11-06 21:00:58.053644: step 52380, total loss = 0.21, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:22m:15s remains)
INFO - root - 2019-11-06 21:00:58.623592: step 52390, total loss = 0.19, predict loss = 0.04 (80.0 examples/sec; 0.050 sec/batch; 1h:21m:21s remains)
INFO - root - 2019-11-06 21:00:59.200172: step 52400, total loss = 0.23, predict loss = 0.06 (77.1 examples/sec; 0.052 sec/batch; 1h:24m:21s remains)
INFO - root - 2019-11-06 21:00:59.800738: step 52410, total loss = 0.14, predict loss = 0.03 (75.5 examples/sec; 0.053 sec/batch; 1h:26m:13s remains)
INFO - root - 2019-11-06 21:01:00.396226: step 52420, total loss = 0.19, predict loss = 0.04 (69.0 examples/sec; 0.058 sec/batch; 1h:34m:12s remains)
INFO - root - 2019-11-06 21:01:00.973149: step 52430, total loss = 0.27, predict loss = 0.07 (94.8 examples/sec; 0.042 sec/batch; 1h:08m:34s remains)
INFO - root - 2019-11-06 21:01:01.434695: step 52440, total loss = 0.28, predict loss = 0.08 (96.9 examples/sec; 0.041 sec/batch; 1h:07m:07s remains)
INFO - root - 2019-11-06 21:01:01.903010: step 52450, total loss = 0.18, predict loss = 0.05 (101.3 examples/sec; 0.039 sec/batch; 1h:04m:11s remains)
INFO - root - 2019-11-06 21:01:02.841758: step 52460, total loss = 0.18, predict loss = 0.05 (69.6 examples/sec; 0.057 sec/batch; 1h:33m:24s remains)
INFO - root - 2019-11-06 21:01:03.534549: step 52470, total loss = 0.33, predict loss = 0.10 (72.1 examples/sec; 0.055 sec/batch; 1h:30m:08s remains)
INFO - root - 2019-11-06 21:01:04.155996: step 52480, total loss = 0.19, predict loss = 0.05 (77.7 examples/sec; 0.052 sec/batch; 1h:23m:43s remains)
INFO - root - 2019-11-06 21:01:04.761835: step 52490, total loss = 0.31, predict loss = 0.08 (82.6 examples/sec; 0.048 sec/batch; 1h:18m:41s remains)
INFO - root - 2019-11-06 21:01:05.333533: step 52500, total loss = 0.20, predict loss = 0.06 (77.8 examples/sec; 0.051 sec/batch; 1h:23m:30s remains)
INFO - root - 2019-11-06 21:01:05.910839: step 52510, total loss = 0.19, predict loss = 0.05 (76.2 examples/sec; 0.052 sec/batch; 1h:25m:17s remains)
INFO - root - 2019-11-06 21:01:06.508124: step 52520, total loss = 0.25, predict loss = 0.07 (79.5 examples/sec; 0.050 sec/batch; 1h:21m:45s remains)
INFO - root - 2019-11-06 21:01:07.099256: step 52530, total loss = 0.19, predict loss = 0.04 (79.7 examples/sec; 0.050 sec/batch; 1h:21m:34s remains)
INFO - root - 2019-11-06 21:01:07.683007: step 52540, total loss = 0.26, predict loss = 0.07 (74.7 examples/sec; 0.054 sec/batch; 1h:26m:55s remains)
INFO - root - 2019-11-06 21:01:08.256996: step 52550, total loss = 0.18, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:23m:11s remains)
INFO - root - 2019-11-06 21:01:08.843781: step 52560, total loss = 0.20, predict loss = 0.06 (81.6 examples/sec; 0.049 sec/batch; 1h:19m:35s remains)
INFO - root - 2019-11-06 21:01:09.446306: step 52570, total loss = 0.24, predict loss = 0.06 (76.2 examples/sec; 0.052 sec/batch; 1h:25m:13s remains)
INFO - root - 2019-11-06 21:01:09.989818: step 52580, total loss = 0.21, predict loss = 0.06 (97.2 examples/sec; 0.041 sec/batch; 1h:06m:50s remains)
INFO - root - 2019-11-06 21:01:10.443033: step 52590, total loss = 0.20, predict loss = 0.05 (90.5 examples/sec; 0.044 sec/batch; 1h:11m:47s remains)
INFO - root - 2019-11-06 21:01:10.900975: step 52600, total loss = 0.19, predict loss = 0.05 (96.3 examples/sec; 0.042 sec/batch; 1h:07m:27s remains)
INFO - root - 2019-11-06 21:01:11.957948: step 52610, total loss = 0.22, predict loss = 0.07 (51.2 examples/sec; 0.078 sec/batch; 2h:06m:48s remains)
INFO - root - 2019-11-06 21:01:12.611376: step 52620, total loss = 0.15, predict loss = 0.04 (73.3 examples/sec; 0.055 sec/batch; 1h:28m:30s remains)
INFO - root - 2019-11-06 21:01:13.180239: step 52630, total loss = 0.19, predict loss = 0.05 (78.3 examples/sec; 0.051 sec/batch; 1h:22m:51s remains)
INFO - root - 2019-11-06 21:01:13.758229: step 52640, total loss = 0.19, predict loss = 0.05 (75.3 examples/sec; 0.053 sec/batch; 1h:26m:11s remains)
INFO - root - 2019-11-06 21:01:14.349896: step 52650, total loss = 0.26, predict loss = 0.07 (75.0 examples/sec; 0.053 sec/batch; 1h:26m:30s remains)
INFO - root - 2019-11-06 21:01:14.927382: step 52660, total loss = 0.23, predict loss = 0.06 (78.8 examples/sec; 0.051 sec/batch; 1h:22m:22s remains)
INFO - root - 2019-11-06 21:01:15.504072: step 52670, total loss = 0.20, predict loss = 0.06 (81.5 examples/sec; 0.049 sec/batch; 1h:19m:37s remains)
INFO - root - 2019-11-06 21:01:16.087683: step 52680, total loss = 0.21, predict loss = 0.06 (78.7 examples/sec; 0.051 sec/batch; 1h:22m:28s remains)
INFO - root - 2019-11-06 21:01:16.667287: step 52690, total loss = 0.23, predict loss = 0.06 (79.6 examples/sec; 0.050 sec/batch; 1h:21m:27s remains)
INFO - root - 2019-11-06 21:01:17.236501: step 52700, total loss = 0.24, predict loss = 0.06 (76.0 examples/sec; 0.053 sec/batch; 1h:25m:20s remains)
INFO - root - 2019-11-06 21:01:17.825599: step 52710, total loss = 0.22, predict loss = 0.06 (75.3 examples/sec; 0.053 sec/batch; 1h:26m:11s remains)
INFO - root - 2019-11-06 21:01:18.410311: step 52720, total loss = 0.15, predict loss = 0.04 (78.3 examples/sec; 0.051 sec/batch; 1h:22m:48s remains)
INFO - root - 2019-11-06 21:01:18.945177: step 52730, total loss = 0.28, predict loss = 0.08 (100.6 examples/sec; 0.040 sec/batch; 1h:04m:27s remains)
INFO - root - 2019-11-06 21:01:19.391817: step 52740, total loss = 0.26, predict loss = 0.07 (91.9 examples/sec; 0.044 sec/batch; 1h:10m:35s remains)
INFO - root - 2019-11-06 21:01:19.845859: step 52750, total loss = 0.23, predict loss = 0.06 (96.0 examples/sec; 0.042 sec/batch; 1h:07m:30s remains)
INFO - root - 2019-11-06 21:01:20.903930: step 52760, total loss = 0.17, predict loss = 0.04 (63.1 examples/sec; 0.063 sec/batch; 1h:42m:40s remains)
INFO - root - 2019-11-06 21:01:21.571394: step 52770, total loss = 0.20, predict loss = 0.05 (71.8 examples/sec; 0.056 sec/batch; 1h:30m:19s remains)
INFO - root - 2019-11-06 21:01:22.173736: step 52780, total loss = 0.18, predict loss = 0.04 (78.4 examples/sec; 0.051 sec/batch; 1h:22m:38s remains)
INFO - root - 2019-11-06 21:01:22.755352: step 52790, total loss = 0.23, predict loss = 0.06 (75.8 examples/sec; 0.053 sec/batch; 1h:25m:26s remains)
INFO - root - 2019-11-06 21:01:23.333006: step 52800, total loss = 0.15, predict loss = 0.04 (76.7 examples/sec; 0.052 sec/batch; 1h:24m:29s remains)
INFO - root - 2019-11-06 21:01:23.918039: step 52810, total loss = 0.32, predict loss = 0.09 (80.9 examples/sec; 0.049 sec/batch; 1h:20m:06s remains)
INFO - root - 2019-11-06 21:01:24.492692: step 52820, total loss = 0.25, predict loss = 0.07 (82.5 examples/sec; 0.049 sec/batch; 1h:18m:33s remains)
INFO - root - 2019-11-06 21:01:25.074865: step 52830, total loss = 0.23, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:24m:20s remains)
INFO - root - 2019-11-06 21:01:25.646330: step 52840, total loss = 0.14, predict loss = 0.03 (74.9 examples/sec; 0.053 sec/batch; 1h:26m:26s remains)
INFO - root - 2019-11-06 21:01:26.240931: step 52850, total loss = 0.20, predict loss = 0.05 (71.2 examples/sec; 0.056 sec/batch; 1h:30m:59s remains)
INFO - root - 2019-11-06 21:01:26.814141: step 52860, total loss = 0.25, predict loss = 0.06 (77.8 examples/sec; 0.051 sec/batch; 1h:23m:11s remains)
INFO - root - 2019-11-06 21:01:27.383502: step 52870, total loss = 0.26, predict loss = 0.06 (80.0 examples/sec; 0.050 sec/batch; 1h:20m:54s remains)
INFO - root - 2019-11-06 21:01:27.875990: step 52880, total loss = 0.29, predict loss = 0.07 (103.5 examples/sec; 0.039 sec/batch; 1h:02m:32s remains)
INFO - root - 2019-11-06 21:01:28.340516: step 52890, total loss = 0.14, predict loss = 0.03 (102.0 examples/sec; 0.039 sec/batch; 1h:03m:28s remains)
INFO - root - 2019-11-06 21:01:28.788638: step 52900, total loss = 0.16, predict loss = 0.04 (95.0 examples/sec; 0.042 sec/batch; 1h:08m:09s remains)
INFO - root - 2019-11-06 21:01:29.881094: step 52910, total loss = 0.18, predict loss = 0.04 (58.3 examples/sec; 0.069 sec/batch; 1h:51m:01s remains)
INFO - root - 2019-11-06 21:01:30.538237: step 52920, total loss = 0.18, predict loss = 0.05 (64.0 examples/sec; 0.062 sec/batch; 1h:41m:02s remains)
INFO - root - 2019-11-06 21:01:31.128207: step 52930, total loss = 0.23, predict loss = 0.06 (79.8 examples/sec; 0.050 sec/batch; 1h:21m:04s remains)
INFO - root - 2019-11-06 21:01:31.704228: step 52940, total loss = 0.15, predict loss = 0.04 (78.5 examples/sec; 0.051 sec/batch; 1h:22m:23s remains)
INFO - root - 2019-11-06 21:01:32.279923: step 52950, total loss = 0.29, predict loss = 0.07 (80.8 examples/sec; 0.049 sec/batch; 1h:20m:03s remains)
INFO - root - 2019-11-06 21:01:32.856304: step 52960, total loss = 0.23, predict loss = 0.07 (80.6 examples/sec; 0.050 sec/batch; 1h:20m:14s remains)
INFO - root - 2019-11-06 21:01:33.448579: step 52970, total loss = 0.17, predict loss = 0.04 (76.6 examples/sec; 0.052 sec/batch; 1h:24m:28s remains)
INFO - root - 2019-11-06 21:01:34.025067: step 52980, total loss = 0.26, predict loss = 0.06 (79.3 examples/sec; 0.050 sec/batch; 1h:21m:31s remains)
INFO - root - 2019-11-06 21:01:34.605495: step 52990, total loss = 0.29, predict loss = 0.08 (75.3 examples/sec; 0.053 sec/batch; 1h:25m:54s remains)
INFO - root - 2019-11-06 21:01:35.187842: step 53000, total loss = 0.32, predict loss = 0.10 (77.1 examples/sec; 0.052 sec/batch; 1h:23m:49s remains)
INFO - root - 2019-11-06 21:01:35.776015: step 53010, total loss = 0.30, predict loss = 0.09 (83.0 examples/sec; 0.048 sec/batch; 1h:17m:56s remains)
INFO - root - 2019-11-06 21:01:36.345405: step 53020, total loss = 0.15, predict loss = 0.04 (81.1 examples/sec; 0.049 sec/batch; 1h:19m:45s remains)
INFO - root - 2019-11-06 21:01:36.820672: step 53030, total loss = 0.23, predict loss = 0.06 (97.7 examples/sec; 0.041 sec/batch; 1h:06m:10s remains)
INFO - root - 2019-11-06 21:01:37.281090: step 53040, total loss = 0.24, predict loss = 0.06 (97.6 examples/sec; 0.041 sec/batch; 1h:06m:12s remains)
INFO - root - 2019-11-06 21:01:38.208834: step 53050, total loss = 0.21, predict loss = 0.05 (79.5 examples/sec; 0.050 sec/batch; 1h:21m:19s remains)
INFO - root - 2019-11-06 21:01:38.888211: step 53060, total loss = 0.21, predict loss = 0.05 (65.9 examples/sec; 0.061 sec/batch; 1h:38m:06s remains)
INFO - root - 2019-11-06 21:01:39.505601: step 53070, total loss = 0.22, predict loss = 0.05 (77.4 examples/sec; 0.052 sec/batch; 1h:23m:27s remains)
INFO - root - 2019-11-06 21:01:40.095039: step 53080, total loss = 0.26, predict loss = 0.07 (77.5 examples/sec; 0.052 sec/batch; 1h:23m:19s remains)
INFO - root - 2019-11-06 21:01:40.678437: step 53090, total loss = 0.21, predict loss = 0.05 (79.9 examples/sec; 0.050 sec/batch; 1h:20m:49s remains)
INFO - root - 2019-11-06 21:01:41.248490: step 53100, total loss = 0.24, predict loss = 0.07 (80.4 examples/sec; 0.050 sec/batch; 1h:20m:21s remains)
INFO - root - 2019-11-06 21:01:41.827348: step 53110, total loss = 0.19, predict loss = 0.05 (81.0 examples/sec; 0.049 sec/batch; 1h:19m:41s remains)
INFO - root - 2019-11-06 21:01:42.406123: step 53120, total loss = 0.22, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:23m:39s remains)
INFO - root - 2019-11-06 21:01:43.003133: step 53130, total loss = 0.15, predict loss = 0.04 (77.1 examples/sec; 0.052 sec/batch; 1h:23m:46s remains)
INFO - root - 2019-11-06 21:01:43.584170: step 53140, total loss = 0.20, predict loss = 0.05 (79.6 examples/sec; 0.050 sec/batch; 1h:21m:04s remains)
INFO - root - 2019-11-06 21:01:44.153008: step 53150, total loss = 0.21, predict loss = 0.05 (79.9 examples/sec; 0.050 sec/batch; 1h:20m:46s remains)
INFO - root - 2019-11-06 21:01:44.737539: step 53160, total loss = 0.14, predict loss = 0.04 (78.2 examples/sec; 0.051 sec/batch; 1h:22m:33s remains)
INFO - root - 2019-11-06 21:01:45.331285: step 53170, total loss = 0.23, predict loss = 0.06 (86.6 examples/sec; 0.046 sec/batch; 1h:14m:34s remains)
INFO - root - 2019-11-06 21:01:45.787417: step 53180, total loss = 0.21, predict loss = 0.06 (97.1 examples/sec; 0.041 sec/batch; 1h:06m:26s remains)
INFO - root - 2019-11-06 21:01:46.255380: step 53190, total loss = 0.19, predict loss = 0.05 (93.5 examples/sec; 0.043 sec/batch; 1h:09m:00s remains)
INFO - root - 2019-11-06 21:01:47.195327: step 53200, total loss = 0.19, predict loss = 0.05 (70.2 examples/sec; 0.057 sec/batch; 1h:31m:55s remains)
INFO - root - 2019-11-06 21:01:47.943204: step 53210, total loss = 0.18, predict loss = 0.05 (67.9 examples/sec; 0.059 sec/batch; 1h:35m:01s remains)
INFO - root - 2019-11-06 21:01:48.562076: step 53220, total loss = 0.20, predict loss = 0.05 (78.5 examples/sec; 0.051 sec/batch; 1h:22m:08s remains)
INFO - root - 2019-11-06 21:01:49.135761: step 53230, total loss = 0.16, predict loss = 0.04 (79.5 examples/sec; 0.050 sec/batch; 1h:21m:06s remains)
INFO - root - 2019-11-06 21:01:49.722997: step 53240, total loss = 0.18, predict loss = 0.04 (78.1 examples/sec; 0.051 sec/batch; 1h:22m:37s remains)
INFO - root - 2019-11-06 21:01:50.314851: step 53250, total loss = 0.28, predict loss = 0.07 (80.3 examples/sec; 0.050 sec/batch; 1h:20m:16s remains)
INFO - root - 2019-11-06 21:01:50.895428: step 53260, total loss = 0.17, predict loss = 0.04 (76.0 examples/sec; 0.053 sec/batch; 1h:24m:53s remains)
INFO - root - 2019-11-06 21:01:51.476385: step 53270, total loss = 0.17, predict loss = 0.05 (79.2 examples/sec; 0.051 sec/batch; 1h:21m:26s remains)
INFO - root - 2019-11-06 21:01:52.059169: step 53280, total loss = 0.23, predict loss = 0.06 (79.5 examples/sec; 0.050 sec/batch; 1h:21m:05s remains)
INFO - root - 2019-11-06 21:01:52.647492: step 53290, total loss = 0.17, predict loss = 0.04 (80.8 examples/sec; 0.049 sec/batch; 1h:19m:46s remains)
INFO - root - 2019-11-06 21:01:53.220121: step 53300, total loss = 0.16, predict loss = 0.04 (83.2 examples/sec; 0.048 sec/batch; 1h:17m:27s remains)
INFO - root - 2019-11-06 21:01:53.794446: step 53310, total loss = 0.21, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:23m:56s remains)
INFO - root - 2019-11-06 21:01:54.342626: step 53320, total loss = 0.39, predict loss = 0.12 (93.8 examples/sec; 0.043 sec/batch; 1h:08m:41s remains)
INFO - root - 2019-11-06 21:01:54.815806: step 53330, total loss = 0.18, predict loss = 0.04 (95.8 examples/sec; 0.042 sec/batch; 1h:07m:14s remains)
INFO - root - 2019-11-06 21:01:55.267698: step 53340, total loss = 0.20, predict loss = 0.05 (95.7 examples/sec; 0.042 sec/batch; 1h:07m:19s remains)
INFO - root - 2019-11-06 21:01:56.269923: step 53350, total loss = 0.16, predict loss = 0.04 (60.7 examples/sec; 0.066 sec/batch; 1h:46m:09s remains)
INFO - root - 2019-11-06 21:01:56.982388: step 53360, total loss = 0.16, predict loss = 0.04 (65.4 examples/sec; 0.061 sec/batch; 1h:38m:27s remains)
INFO - root - 2019-11-06 21:01:57.655333: step 53370, total loss = 0.17, predict loss = 0.05 (73.8 examples/sec; 0.054 sec/batch; 1h:27m:20s remains)
INFO - root - 2019-11-06 21:01:58.231901: step 53380, total loss = 0.23, predict loss = 0.06 (79.5 examples/sec; 0.050 sec/batch; 1h:20m:58s remains)
INFO - root - 2019-11-06 21:01:58.795636: step 53390, total loss = 0.19, predict loss = 0.05 (81.0 examples/sec; 0.049 sec/batch; 1h:19m:30s remains)
INFO - root - 2019-11-06 21:01:59.372087: step 53400, total loss = 0.21, predict loss = 0.05 (78.2 examples/sec; 0.051 sec/batch; 1h:22m:21s remains)
INFO - root - 2019-11-06 21:01:59.967632: step 53410, total loss = 0.32, predict loss = 0.08 (80.1 examples/sec; 0.050 sec/batch; 1h:20m:22s remains)
INFO - root - 2019-11-06 21:02:00.573342: step 53420, total loss = 0.19, predict loss = 0.04 (71.1 examples/sec; 0.056 sec/batch; 1h:30m:32s remains)
INFO - root - 2019-11-06 21:02:01.160514: step 53430, total loss = 0.26, predict loss = 0.07 (79.3 examples/sec; 0.050 sec/batch; 1h:21m:08s remains)
INFO - root - 2019-11-06 21:02:01.734917: step 53440, total loss = 0.16, predict loss = 0.04 (80.5 examples/sec; 0.050 sec/batch; 1h:19m:56s remains)
INFO - root - 2019-11-06 21:02:02.323366: step 53450, total loss = 0.24, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:21m:21s remains)
INFO - root - 2019-11-06 21:02:02.897395: step 53460, total loss = 0.27, predict loss = 0.07 (77.2 examples/sec; 0.052 sec/batch; 1h:23m:22s remains)
INFO - root - 2019-11-06 21:02:03.422587: step 53470, total loss = 0.19, predict loss = 0.05 (90.5 examples/sec; 0.044 sec/batch; 1h:11m:06s remains)
INFO - root - 2019-11-06 21:02:03.884874: step 53480, total loss = 0.19, predict loss = 0.05 (97.8 examples/sec; 0.041 sec/batch; 1h:05m:46s remains)
INFO - root - 2019-11-06 21:02:04.350692: step 53490, total loss = 0.15, predict loss = 0.04 (94.1 examples/sec; 0.043 sec/batch; 1h:08m:24s remains)
INFO - root - 2019-11-06 21:02:05.370786: step 53500, total loss = 0.17, predict loss = 0.04 (56.2 examples/sec; 0.071 sec/batch; 1h:54m:31s remains)
INFO - root - 2019-11-06 21:02:06.013846: step 53510, total loss = 0.18, predict loss = 0.05 (75.7 examples/sec; 0.053 sec/batch; 1h:24m:56s remains)
INFO - root - 2019-11-06 21:02:06.587065: step 53520, total loss = 0.22, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 1h:21m:17s remains)
INFO - root - 2019-11-06 21:02:07.174669: step 53530, total loss = 0.21, predict loss = 0.06 (78.0 examples/sec; 0.051 sec/batch; 1h:22m:26s remains)
INFO - root - 2019-11-06 21:02:07.758410: step 53540, total loss = 0.18, predict loss = 0.05 (77.5 examples/sec; 0.052 sec/batch; 1h:22m:58s remains)
INFO - root - 2019-11-06 21:02:08.331089: step 53550, total loss = 0.22, predict loss = 0.06 (76.3 examples/sec; 0.052 sec/batch; 1h:24m:16s remains)
INFO - root - 2019-11-06 21:02:08.900618: step 53560, total loss = 0.17, predict loss = 0.04 (83.0 examples/sec; 0.048 sec/batch; 1h:17m:30s remains)
INFO - root - 2019-11-06 21:02:09.499566: step 53570, total loss = 0.20, predict loss = 0.05 (76.1 examples/sec; 0.053 sec/batch; 1h:24m:31s remains)
INFO - root - 2019-11-06 21:02:10.078160: step 53580, total loss = 0.25, predict loss = 0.08 (83.0 examples/sec; 0.048 sec/batch; 1h:17m:26s remains)
INFO - root - 2019-11-06 21:02:10.647296: step 53590, total loss = 0.19, predict loss = 0.05 (78.9 examples/sec; 0.051 sec/batch; 1h:21m:27s remains)
INFO - root - 2019-11-06 21:02:11.230783: step 53600, total loss = 0.22, predict loss = 0.06 (81.6 examples/sec; 0.049 sec/batch; 1h:18m:42s remains)
INFO - root - 2019-11-06 21:02:11.843126: step 53610, total loss = 0.24, predict loss = 0.07 (75.9 examples/sec; 0.053 sec/batch; 1h:24m:39s remains)
INFO - root - 2019-11-06 21:02:12.353145: step 53620, total loss = 0.20, predict loss = 0.06 (103.1 examples/sec; 0.039 sec/batch; 1h:02m:20s remains)
INFO - root - 2019-11-06 21:02:12.790814: step 53630, total loss = 0.22, predict loss = 0.05 (99.6 examples/sec; 0.040 sec/batch; 1h:04m:30s remains)
INFO - root - 2019-11-06 21:02:13.250286: step 53640, total loss = 0.18, predict loss = 0.05 (95.7 examples/sec; 0.042 sec/batch; 1h:07m:07s remains)
INFO - root - 2019-11-06 21:02:14.463901: step 53650, total loss = 0.17, predict loss = 0.04 (44.2 examples/sec; 0.090 sec/batch; 2h:25m:12s remains)
INFO - root - 2019-11-06 21:02:15.117440: step 53660, total loss = 0.22, predict loss = 0.06 (72.0 examples/sec; 0.056 sec/batch; 1h:29m:10s remains)
INFO - root - 2019-11-06 21:02:15.680325: step 53670, total loss = 0.21, predict loss = 0.06 (81.0 examples/sec; 0.049 sec/batch; 1h:19m:18s remains)
INFO - root - 2019-11-06 21:02:16.259061: step 53680, total loss = 0.15, predict loss = 0.04 (77.9 examples/sec; 0.051 sec/batch; 1h:22m:26s remains)
INFO - root - 2019-11-06 21:02:16.858375: step 53690, total loss = 0.25, predict loss = 0.07 (73.4 examples/sec; 0.054 sec/batch; 1h:27m:28s remains)
INFO - root - 2019-11-06 21:02:17.433418: step 53700, total loss = 0.19, predict loss = 0.05 (77.7 examples/sec; 0.051 sec/batch; 1h:22m:38s remains)
INFO - root - 2019-11-06 21:02:18.003631: step 53710, total loss = 0.23, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:22m:57s remains)
INFO - root - 2019-11-06 21:02:18.576878: step 53720, total loss = 0.16, predict loss = 0.04 (81.2 examples/sec; 0.049 sec/batch; 1h:19m:04s remains)
INFO - root - 2019-11-06 21:02:19.169675: step 53730, total loss = 0.21, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:21m:23s remains)
INFO - root - 2019-11-06 21:02:19.740654: step 53740, total loss = 0.18, predict loss = 0.05 (79.4 examples/sec; 0.050 sec/batch; 1h:20m:52s remains)
INFO - root - 2019-11-06 21:02:20.304801: step 53750, total loss = 0.18, predict loss = 0.05 (81.7 examples/sec; 0.049 sec/batch; 1h:18m:30s remains)
INFO - root - 2019-11-06 21:02:20.885348: step 53760, total loss = 0.21, predict loss = 0.06 (78.4 examples/sec; 0.051 sec/batch; 1h:21m:52s remains)
INFO - root - 2019-11-06 21:02:21.387430: step 53770, total loss = 0.31, predict loss = 0.08 (97.0 examples/sec; 0.041 sec/batch; 1h:06m:08s remains)
INFO - root - 2019-11-06 21:02:21.833409: step 53780, total loss = 0.23, predict loss = 0.06 (99.8 examples/sec; 0.040 sec/batch; 1h:04m:15s remains)
INFO - root - 2019-11-06 21:02:22.720291: step 53790, total loss = 0.19, predict loss = 0.05 (8.3 examples/sec; 0.483 sec/batch; 12h:55m:01s remains)
INFO - root - 2019-11-06 21:02:23.372251: step 53800, total loss = 0.22, predict loss = 0.05 (67.3 examples/sec; 0.059 sec/batch; 1h:35m:18s remains)
INFO - root - 2019-11-06 21:02:24.017576: step 53810, total loss = 0.16, predict loss = 0.04 (78.4 examples/sec; 0.051 sec/batch; 1h:21m:45s remains)
INFO - root - 2019-11-06 21:02:24.609489: step 53820, total loss = 0.17, predict loss = 0.04 (79.0 examples/sec; 0.051 sec/batch; 1h:21m:10s remains)
INFO - root - 2019-11-06 21:02:25.184427: step 53830, total loss = 0.18, predict loss = 0.04 (77.4 examples/sec; 0.052 sec/batch; 1h:22m:51s remains)
INFO - root - 2019-11-06 21:02:25.760772: step 53840, total loss = 0.27, predict loss = 0.07 (82.1 examples/sec; 0.049 sec/batch; 1h:18m:06s remains)
INFO - root - 2019-11-06 21:02:26.359860: step 53850, total loss = 0.19, predict loss = 0.05 (76.5 examples/sec; 0.052 sec/batch; 1h:23m:49s remains)
INFO - root - 2019-11-06 21:02:26.929446: step 53860, total loss = 0.16, predict loss = 0.04 (78.3 examples/sec; 0.051 sec/batch; 1h:21m:53s remains)
INFO - root - 2019-11-06 21:02:27.508462: step 53870, total loss = 0.14, predict loss = 0.04 (77.1 examples/sec; 0.052 sec/batch; 1h:23m:07s remains)
INFO - root - 2019-11-06 21:02:28.078004: step 53880, total loss = 0.20, predict loss = 0.05 (83.6 examples/sec; 0.048 sec/batch; 1h:16m:40s remains)
INFO - root - 2019-11-06 21:02:28.675733: step 53890, total loss = 0.22, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 1h:21m:10s remains)
INFO - root - 2019-11-06 21:02:29.247041: step 53900, total loss = 0.15, predict loss = 0.04 (78.1 examples/sec; 0.051 sec/batch; 1h:21m:58s remains)
INFO - root - 2019-11-06 21:02:29.825256: step 53910, total loss = 0.25, predict loss = 0.07 (86.9 examples/sec; 0.046 sec/batch; 1h:13m:40s remains)
INFO - root - 2019-11-06 21:02:30.298648: step 53920, total loss = 0.25, predict loss = 0.06 (91.9 examples/sec; 0.044 sec/batch; 1h:09m:43s remains)
INFO - root - 2019-11-06 21:02:30.791269: step 53930, total loss = 0.23, predict loss = 0.06 (100.7 examples/sec; 0.040 sec/batch; 1h:03m:37s remains)
INFO - root - 2019-11-06 21:02:31.714146: step 53940, total loss = 0.30, predict loss = 0.08 (77.4 examples/sec; 0.052 sec/batch; 1h:22m:43s remains)
INFO - root - 2019-11-06 21:02:32.425630: step 53950, total loss = 0.24, predict loss = 0.07 (58.8 examples/sec; 0.068 sec/batch; 1h:48m:53s remains)
INFO - root - 2019-11-06 21:02:33.060316: step 53960, total loss = 0.19, predict loss = 0.05 (76.5 examples/sec; 0.052 sec/batch; 1h:23m:43s remains)
INFO - root - 2019-11-06 21:02:33.657275: step 53970, total loss = 0.14, predict loss = 0.04 (77.4 examples/sec; 0.052 sec/batch; 1h:22m:40s remains)
INFO - root - 2019-11-06 21:02:34.224458: step 53980, total loss = 0.24, predict loss = 0.06 (77.9 examples/sec; 0.051 sec/batch; 1h:22m:08s remains)
INFO - root - 2019-11-06 21:02:34.794718: step 53990, total loss = 0.22, predict loss = 0.06 (77.4 examples/sec; 0.052 sec/batch; 1h:22m:42s remains)
INFO - root - 2019-11-06 21:02:35.369350: step 54000, total loss = 0.31, predict loss = 0.08 (81.4 examples/sec; 0.049 sec/batch; 1h:18m:35s remains)
INFO - root - 2019-11-06 21:02:35.967549: step 54010, total loss = 0.22, predict loss = 0.06 (78.7 examples/sec; 0.051 sec/batch; 1h:21m:16s remains)
INFO - root - 2019-11-06 21:02:36.552062: step 54020, total loss = 0.19, predict loss = 0.05 (71.3 examples/sec; 0.056 sec/batch; 1h:29m:44s remains)
INFO - root - 2019-11-06 21:02:37.117639: step 54030, total loss = 0.22, predict loss = 0.06 (75.8 examples/sec; 0.053 sec/batch; 1h:24m:24s remains)
INFO - root - 2019-11-06 21:02:37.699181: step 54040, total loss = 0.18, predict loss = 0.05 (82.9 examples/sec; 0.048 sec/batch; 1h:17m:11s remains)
INFO - root - 2019-11-06 21:02:38.312397: step 54050, total loss = 0.21, predict loss = 0.06 (76.8 examples/sec; 0.052 sec/batch; 1h:23m:14s remains)
INFO - root - 2019-11-06 21:02:38.877688: step 54060, total loss = 0.25, predict loss = 0.07 (93.7 examples/sec; 0.043 sec/batch; 1h:08m:14s remains)
INFO - root - 2019-11-06 21:02:39.332508: step 54070, total loss = 0.27, predict loss = 0.08 (94.7 examples/sec; 0.042 sec/batch; 1h:07m:33s remains)
INFO - root - 2019-11-06 21:02:39.787784: step 54080, total loss = 0.19, predict loss = 0.04 (100.2 examples/sec; 0.040 sec/batch; 1h:03m:50s remains)
INFO - root - 2019-11-06 21:02:40.788166: step 54090, total loss = 0.26, predict loss = 0.06 (68.1 examples/sec; 0.059 sec/batch; 1h:33m:49s remains)
INFO - root - 2019-11-06 21:02:41.480979: step 54100, total loss = 0.14, predict loss = 0.03 (68.5 examples/sec; 0.058 sec/batch; 1h:33m:17s remains)
INFO - root - 2019-11-06 21:02:42.082861: step 54110, total loss = 0.17, predict loss = 0.05 (78.5 examples/sec; 0.051 sec/batch; 1h:21m:26s remains)
INFO - root - 2019-11-06 21:02:42.653329: step 54120, total loss = 0.16, predict loss = 0.04 (77.2 examples/sec; 0.052 sec/batch; 1h:22m:49s remains)
INFO - root - 2019-11-06 21:02:43.247289: step 54130, total loss = 0.15, predict loss = 0.03 (80.3 examples/sec; 0.050 sec/batch; 1h:19m:37s remains)
INFO - root - 2019-11-06 21:02:43.825979: step 54140, total loss = 0.15, predict loss = 0.04 (79.1 examples/sec; 0.051 sec/batch; 1h:20m:49s remains)
INFO - root - 2019-11-06 21:02:44.386627: step 54150, total loss = 0.22, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 1h:22m:28s remains)
INFO - root - 2019-11-06 21:02:44.959107: step 54160, total loss = 0.22, predict loss = 0.06 (82.5 examples/sec; 0.049 sec/batch; 1h:17m:28s remains)
INFO - root - 2019-11-06 21:02:45.553449: step 54170, total loss = 0.21, predict loss = 0.06 (84.2 examples/sec; 0.048 sec/batch; 1h:15m:52s remains)
INFO - root - 2019-11-06 21:02:46.138404: step 54180, total loss = 0.28, predict loss = 0.08 (87.4 examples/sec; 0.046 sec/batch; 1h:13m:05s remains)
INFO - root - 2019-11-06 21:02:46.708731: step 54190, total loss = 0.15, predict loss = 0.04 (79.3 examples/sec; 0.050 sec/batch; 1h:20m:33s remains)
INFO - root - 2019-11-06 21:02:47.288142: step 54200, total loss = 0.21, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:21m:44s remains)
INFO - root - 2019-11-06 21:02:47.843705: step 54210, total loss = 0.16, predict loss = 0.04 (97.5 examples/sec; 0.041 sec/batch; 1h:05m:31s remains)
INFO - root - 2019-11-06 21:02:48.301144: step 54220, total loss = 0.30, predict loss = 0.08 (95.0 examples/sec; 0.042 sec/batch; 1h:07m:13s remains)
INFO - root - 2019-11-06 21:02:48.750029: step 54230, total loss = 0.19, predict loss = 0.05 (93.8 examples/sec; 0.043 sec/batch; 1h:08m:02s remains)
INFO - root - 2019-11-06 21:02:49.716775: step 54240, total loss = 0.29, predict loss = 0.07 (70.8 examples/sec; 0.056 sec/batch; 1h:30m:06s remains)
INFO - root - 2019-11-06 21:02:50.356384: step 54250, total loss = 0.25, predict loss = 0.07 (70.9 examples/sec; 0.056 sec/batch; 1h:30m:01s remains)
INFO - root - 2019-11-06 21:02:50.944071: step 54260, total loss = 0.16, predict loss = 0.04 (77.5 examples/sec; 0.052 sec/batch; 1h:22m:18s remains)
INFO - root - 2019-11-06 21:02:51.526358: step 54270, total loss = 0.25, predict loss = 0.06 (80.1 examples/sec; 0.050 sec/batch; 1h:19m:40s remains)
INFO - root - 2019-11-06 21:02:52.100456: step 54280, total loss = 0.20, predict loss = 0.05 (80.0 examples/sec; 0.050 sec/batch; 1h:19m:47s remains)
INFO - root - 2019-11-06 21:02:52.692057: step 54290, total loss = 0.15, predict loss = 0.04 (76.4 examples/sec; 0.052 sec/batch; 1h:23m:33s remains)
INFO - root - 2019-11-06 21:02:53.261403: step 54300, total loss = 0.25, predict loss = 0.07 (83.1 examples/sec; 0.048 sec/batch; 1h:16m:46s remains)
INFO - root - 2019-11-06 21:02:53.827814: step 54310, total loss = 0.19, predict loss = 0.05 (75.5 examples/sec; 0.053 sec/batch; 1h:24m:27s remains)
INFO - root - 2019-11-06 21:02:54.394410: step 54320, total loss = 0.25, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 1h:21m:11s remains)
INFO - root - 2019-11-06 21:02:54.980981: step 54330, total loss = 0.25, predict loss = 0.07 (83.1 examples/sec; 0.048 sec/batch; 1h:16m:47s remains)
INFO - root - 2019-11-06 21:02:55.564051: step 54340, total loss = 0.26, predict loss = 0.07 (79.4 examples/sec; 0.050 sec/batch; 1h:20m:21s remains)
INFO - root - 2019-11-06 21:02:56.147492: step 54350, total loss = 0.18, predict loss = 0.05 (77.5 examples/sec; 0.052 sec/batch; 1h:22m:14s remains)
INFO - root - 2019-11-06 21:02:56.657699: step 54360, total loss = 0.21, predict loss = 0.05 (104.5 examples/sec; 0.038 sec/batch; 1h:01m:01s remains)
INFO - root - 2019-11-06 21:02:57.122584: step 54370, total loss = 0.18, predict loss = 0.04 (93.5 examples/sec; 0.043 sec/batch; 1h:08m:10s remains)
INFO - root - 2019-11-06 21:02:57.578487: step 54380, total loss = 0.28, predict loss = 0.08 (94.5 examples/sec; 0.042 sec/batch; 1h:07m:28s remains)
INFO - root - 2019-11-06 21:02:58.701057: step 54390, total loss = 0.17, predict loss = 0.04 (49.9 examples/sec; 0.080 sec/batch; 2h:07m:47s remains)
INFO - root - 2019-11-06 21:02:59.343771: step 54400, total loss = 0.20, predict loss = 0.05 (76.4 examples/sec; 0.052 sec/batch; 1h:23m:25s remains)
INFO - root - 2019-11-06 21:02:59.940006: step 54410, total loss = 0.28, predict loss = 0.07 (78.8 examples/sec; 0.051 sec/batch; 1h:20m:51s remains)
INFO - root - 2019-11-06 21:03:00.540911: step 54420, total loss = 0.21, predict loss = 0.05 (68.6 examples/sec; 0.058 sec/batch; 1h:32m:55s remains)
INFO - root - 2019-11-06 21:03:01.128807: step 54430, total loss = 0.19, predict loss = 0.05 (81.2 examples/sec; 0.049 sec/batch; 1h:18m:27s remains)
INFO - root - 2019-11-06 21:03:01.709259: step 54440, total loss = 0.24, predict loss = 0.07 (80.1 examples/sec; 0.050 sec/batch; 1h:19m:33s remains)
INFO - root - 2019-11-06 21:03:02.307382: step 54450, total loss = 0.23, predict loss = 0.06 (79.3 examples/sec; 0.050 sec/batch; 1h:20m:17s remains)
INFO - root - 2019-11-06 21:03:02.884746: step 54460, total loss = 0.15, predict loss = 0.04 (76.3 examples/sec; 0.052 sec/batch; 1h:23m:26s remains)
INFO - root - 2019-11-06 21:03:03.470514: step 54470, total loss = 0.19, predict loss = 0.05 (79.1 examples/sec; 0.051 sec/batch; 1h:20m:28s remains)
INFO - root - 2019-11-06 21:03:04.054029: step 54480, total loss = 0.17, predict loss = 0.04 (79.3 examples/sec; 0.050 sec/batch; 1h:20m:19s remains)
INFO - root - 2019-11-06 21:03:04.645883: step 54490, total loss = 0.19, predict loss = 0.05 (82.8 examples/sec; 0.048 sec/batch; 1h:16m:56s remains)
INFO - root - 2019-11-06 21:03:05.235284: step 54500, total loss = 0.27, predict loss = 0.08 (74.7 examples/sec; 0.054 sec/batch; 1h:25m:14s remains)
INFO - root - 2019-11-06 21:03:05.726727: step 54510, total loss = 0.28, predict loss = 0.08 (105.0 examples/sec; 0.038 sec/batch; 1h:00m:37s remains)
INFO - root - 2019-11-06 21:03:06.191640: step 54520, total loss = 0.31, predict loss = 0.09 (93.8 examples/sec; 0.043 sec/batch; 1h:07m:52s remains)
INFO - root - 2019-11-06 21:03:06.683249: step 54530, total loss = 0.23, predict loss = 0.06 (95.0 examples/sec; 0.042 sec/batch; 1h:07m:00s remains)
INFO - root - 2019-11-06 21:03:07.763291: step 54540, total loss = 0.18, predict loss = 0.04 (58.4 examples/sec; 0.068 sec/batch; 1h:48m:56s remains)
INFO - root - 2019-11-06 21:03:08.390524: step 54550, total loss = 0.21, predict loss = 0.06 (75.2 examples/sec; 0.053 sec/batch; 1h:24m:38s remains)
INFO - root - 2019-11-06 21:03:08.971677: step 54560, total loss = 0.24, predict loss = 0.07 (79.8 examples/sec; 0.050 sec/batch; 1h:19m:41s remains)
INFO - root - 2019-11-06 21:03:09.550718: step 54570, total loss = 0.18, predict loss = 0.04 (80.4 examples/sec; 0.050 sec/batch; 1h:19m:10s remains)
INFO - root - 2019-11-06 21:03:10.128737: step 54580, total loss = 0.17, predict loss = 0.04 (80.2 examples/sec; 0.050 sec/batch; 1h:19m:16s remains)
INFO - root - 2019-11-06 21:03:10.699187: step 54590, total loss = 0.26, predict loss = 0.06 (77.6 examples/sec; 0.052 sec/batch; 1h:21m:57s remains)
INFO - root - 2019-11-06 21:03:11.282986: step 54600, total loss = 0.26, predict loss = 0.07 (77.5 examples/sec; 0.052 sec/batch; 1h:22m:00s remains)
INFO - root - 2019-11-06 21:03:11.876741: step 54610, total loss = 0.16, predict loss = 0.04 (74.9 examples/sec; 0.053 sec/batch; 1h:24m:57s remains)
INFO - root - 2019-11-06 21:03:12.435844: step 54620, total loss = 0.20, predict loss = 0.05 (79.7 examples/sec; 0.050 sec/batch; 1h:19m:48s remains)
INFO - root - 2019-11-06 21:03:13.016750: step 54630, total loss = 0.20, predict loss = 0.05 (75.6 examples/sec; 0.053 sec/batch; 1h:24m:03s remains)
INFO - root - 2019-11-06 21:03:13.596393: step 54640, total loss = 0.21, predict loss = 0.05 (74.0 examples/sec; 0.054 sec/batch; 1h:25m:52s remains)
INFO - root - 2019-11-06 21:03:14.192070: step 54650, total loss = 0.18, predict loss = 0.04 (82.5 examples/sec; 0.048 sec/batch; 1h:17m:04s remains)
INFO - root - 2019-11-06 21:03:14.655035: step 54660, total loss = 0.20, predict loss = 0.05 (98.2 examples/sec; 0.041 sec/batch; 1h:04m:43s remains)
INFO - root - 2019-11-06 21:03:15.098375: step 54670, total loss = 0.26, predict loss = 0.06 (96.5 examples/sec; 0.041 sec/batch; 1h:05m:50s remains)
INFO - root - 2019-11-06 21:03:15.983485: step 54680, total loss = 0.23, predict loss = 0.07 (75.4 examples/sec; 0.053 sec/batch; 1h:24m:14s remains)
INFO - root - 2019-11-06 21:03:16.654466: step 54690, total loss = 0.14, predict loss = 0.04 (65.2 examples/sec; 0.061 sec/batch; 1h:37m:30s remains)
INFO - root - 2019-11-06 21:03:17.274656: step 54700, total loss = 0.21, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:21m:22s remains)
INFO - root - 2019-11-06 21:03:17.849090: step 54710, total loss = 0.24, predict loss = 0.07 (77.3 examples/sec; 0.052 sec/batch; 1h:22m:08s remains)
INFO - root - 2019-11-06 21:03:18.428369: step 54720, total loss = 0.20, predict loss = 0.06 (68.4 examples/sec; 0.058 sec/batch; 1h:32m:49s remains)
INFO - root - 2019-11-06 21:03:19.035767: step 54730, total loss = 0.29, predict loss = 0.08 (77.4 examples/sec; 0.052 sec/batch; 1h:22m:06s remains)
INFO - root - 2019-11-06 21:03:19.612318: step 54740, total loss = 0.21, predict loss = 0.05 (73.1 examples/sec; 0.055 sec/batch; 1h:26m:54s remains)
INFO - root - 2019-11-06 21:03:20.202526: step 54750, total loss = 0.16, predict loss = 0.04 (80.4 examples/sec; 0.050 sec/batch; 1h:18m:57s remains)
INFO - root - 2019-11-06 21:03:20.781618: step 54760, total loss = 0.17, predict loss = 0.04 (77.1 examples/sec; 0.052 sec/batch; 1h:22m:22s remains)
INFO - root - 2019-11-06 21:03:21.376126: step 54770, total loss = 0.17, predict loss = 0.04 (76.3 examples/sec; 0.052 sec/batch; 1h:23m:14s remains)
INFO - root - 2019-11-06 21:03:21.949524: step 54780, total loss = 0.33, predict loss = 0.09 (75.0 examples/sec; 0.053 sec/batch; 1h:24m:41s remains)
INFO - root - 2019-11-06 21:03:22.528327: step 54790, total loss = 0.28, predict loss = 0.08 (78.9 examples/sec; 0.051 sec/batch; 1h:20m:27s remains)
INFO - root - 2019-11-06 21:03:23.088054: step 54800, total loss = 0.21, predict loss = 0.06 (90.8 examples/sec; 0.044 sec/batch; 1h:09m:53s remains)
INFO - root - 2019-11-06 21:03:23.570796: step 54810, total loss = 0.28, predict loss = 0.09 (89.8 examples/sec; 0.045 sec/batch; 1h:10m:41s remains)
INFO - root - 2019-11-06 21:03:24.027371: step 54820, total loss = 0.26, predict loss = 0.06 (92.5 examples/sec; 0.043 sec/batch; 1h:08m:35s remains)
INFO - root - 2019-11-06 21:03:24.955027: step 54830, total loss = 0.19, predict loss = 0.04 (79.6 examples/sec; 0.050 sec/batch; 1h:19m:42s remains)
INFO - root - 2019-11-06 21:03:25.672396: step 54840, total loss = 0.21, predict loss = 0.05 (60.8 examples/sec; 0.066 sec/batch; 1h:44m:20s remains)
INFO - root - 2019-11-06 21:03:26.328638: step 54850, total loss = 0.14, predict loss = 0.04 (72.6 examples/sec; 0.055 sec/batch; 1h:27m:21s remains)
INFO - root - 2019-11-06 21:03:26.916987: step 54860, total loss = 0.23, predict loss = 0.07 (78.9 examples/sec; 0.051 sec/batch; 1h:20m:22s remains)
INFO - root - 2019-11-06 21:03:27.488833: step 54870, total loss = 0.25, predict loss = 0.06 (79.0 examples/sec; 0.051 sec/batch; 1h:20m:15s remains)
INFO - root - 2019-11-06 21:03:28.068312: step 54880, total loss = 0.19, predict loss = 0.04 (70.6 examples/sec; 0.057 sec/batch; 1h:29m:51s remains)
INFO - root - 2019-11-06 21:03:28.667430: step 54890, total loss = 0.20, predict loss = 0.05 (77.6 examples/sec; 0.052 sec/batch; 1h:21m:43s remains)
INFO - root - 2019-11-06 21:03:29.245721: step 54900, total loss = 0.18, predict loss = 0.04 (74.2 examples/sec; 0.054 sec/batch; 1h:25m:25s remains)
INFO - root - 2019-11-06 21:03:29.819974: step 54910, total loss = 0.28, predict loss = 0.08 (78.2 examples/sec; 0.051 sec/batch; 1h:21m:02s remains)
INFO - root - 2019-11-06 21:03:30.384518: step 54920, total loss = 0.26, predict loss = 0.08 (74.8 examples/sec; 0.053 sec/batch; 1h:24m:43s remains)
INFO - root - 2019-11-06 21:03:31.008018: step 54930, total loss = 0.37, predict loss = 0.10 (84.0 examples/sec; 0.048 sec/batch; 1h:15m:24s remains)
INFO - root - 2019-11-06 21:03:31.584324: step 54940, total loss = 0.23, predict loss = 0.06 (79.7 examples/sec; 0.050 sec/batch; 1h:19m:32s remains)
INFO - root - 2019-11-06 21:03:32.124214: step 54950, total loss = 0.18, predict loss = 0.05 (93.8 examples/sec; 0.043 sec/batch; 1h:07m:33s remains)
INFO - root - 2019-11-06 21:03:32.588331: step 54960, total loss = 0.23, predict loss = 0.06 (92.6 examples/sec; 0.043 sec/batch; 1h:08m:25s remains)
INFO - root - 2019-11-06 21:03:33.061822: step 54970, total loss = 0.19, predict loss = 0.05 (100.5 examples/sec; 0.040 sec/batch; 1h:03m:00s remains)
INFO - root - 2019-11-06 21:03:34.045052: step 54980, total loss = 0.25, predict loss = 0.07 (59.9 examples/sec; 0.067 sec/batch; 1h:45m:47s remains)
INFO - root - 2019-11-06 21:03:34.697191: step 54990, total loss = 0.16, predict loss = 0.05 (80.1 examples/sec; 0.050 sec/batch; 1h:19m:02s remains)
INFO - root - 2019-11-06 21:03:35.292596: step 55000, total loss = 0.18, predict loss = 0.04 (73.7 examples/sec; 0.054 sec/batch; 1h:25m:56s remains)
INFO - root - 2019-11-06 21:03:35.884892: step 55010, total loss = 0.20, predict loss = 0.05 (79.0 examples/sec; 0.051 sec/batch; 1h:20m:08s remains)
INFO - root - 2019-11-06 21:03:36.471487: step 55020, total loss = 0.17, predict loss = 0.04 (75.3 examples/sec; 0.053 sec/batch; 1h:24m:07s remains)
INFO - root - 2019-11-06 21:03:37.052382: step 55030, total loss = 0.27, predict loss = 0.07 (78.5 examples/sec; 0.051 sec/batch; 1h:20m:41s remains)
INFO - root - 2019-11-06 21:03:37.628866: step 55040, total loss = 0.14, predict loss = 0.03 (77.4 examples/sec; 0.052 sec/batch; 1h:21m:45s remains)
INFO - root - 2019-11-06 21:03:38.220226: step 55050, total loss = 0.18, predict loss = 0.04 (81.0 examples/sec; 0.049 sec/batch; 1h:18m:10s remains)
INFO - root - 2019-11-06 21:03:38.794968: step 55060, total loss = 0.21, predict loss = 0.05 (83.1 examples/sec; 0.048 sec/batch; 1h:16m:08s remains)
INFO - root - 2019-11-06 21:03:39.368544: step 55070, total loss = 0.20, predict loss = 0.05 (81.3 examples/sec; 0.049 sec/batch; 1h:17m:49s remains)
INFO - root - 2019-11-06 21:03:39.939033: step 55080, total loss = 0.22, predict loss = 0.06 (81.1 examples/sec; 0.049 sec/batch; 1h:18m:02s remains)
INFO - root - 2019-11-06 21:03:40.517954: step 55090, total loss = 0.20, predict loss = 0.06 (80.7 examples/sec; 0.050 sec/batch; 1h:18m:26s remains)
INFO - root - 2019-11-06 21:03:41.035319: step 55100, total loss = 0.21, predict loss = 0.06 (96.8 examples/sec; 0.041 sec/batch; 1h:05m:21s remains)
INFO - root - 2019-11-06 21:03:41.496490: step 55110, total loss = 0.14, predict loss = 0.03 (94.6 examples/sec; 0.042 sec/batch; 1h:06m:52s remains)
INFO - root - 2019-11-06 21:03:41.953462: step 55120, total loss = 0.29, predict loss = 0.08 (96.5 examples/sec; 0.041 sec/batch; 1h:05m:34s remains)
INFO - root - 2019-11-06 21:03:43.017948: step 55130, total loss = 0.14, predict loss = 0.03 (58.6 examples/sec; 0.068 sec/batch; 1h:47m:58s remains)
INFO - root - 2019-11-06 21:03:43.695691: step 55140, total loss = 0.27, predict loss = 0.08 (69.2 examples/sec; 0.058 sec/batch; 1h:31m:21s remains)
INFO - root - 2019-11-06 21:03:44.285466: step 55150, total loss = 0.21, predict loss = 0.06 (78.1 examples/sec; 0.051 sec/batch; 1h:20m:57s remains)
INFO - root - 2019-11-06 21:03:44.852977: step 55160, total loss = 0.24, predict loss = 0.07 (78.0 examples/sec; 0.051 sec/batch; 1h:21m:04s remains)
INFO - root - 2019-11-06 21:03:45.446416: step 55170, total loss = 0.17, predict loss = 0.04 (76.1 examples/sec; 0.053 sec/batch; 1h:23m:06s remains)
INFO - root - 2019-11-06 21:03:46.029193: step 55180, total loss = 0.21, predict loss = 0.06 (69.7 examples/sec; 0.057 sec/batch; 1h:30m:43s remains)
INFO - root - 2019-11-06 21:03:46.611235: step 55190, total loss = 0.19, predict loss = 0.05 (77.1 examples/sec; 0.052 sec/batch; 1h:21m:57s remains)
INFO - root - 2019-11-06 21:03:47.177039: step 55200, total loss = 0.18, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:21m:04s remains)
INFO - root - 2019-11-06 21:03:47.771833: step 55210, total loss = 0.25, predict loss = 0.08 (79.2 examples/sec; 0.051 sec/batch; 1h:19m:47s remains)
INFO - root - 2019-11-06 21:03:48.345390: step 55220, total loss = 0.15, predict loss = 0.04 (77.3 examples/sec; 0.052 sec/batch; 1h:21m:41s remains)
INFO - root - 2019-11-06 21:03:48.921312: step 55230, total loss = 0.16, predict loss = 0.04 (77.6 examples/sec; 0.052 sec/batch; 1h:21m:25s remains)
INFO - root - 2019-11-06 21:03:49.488931: step 55240, total loss = 0.22, predict loss = 0.06 (82.1 examples/sec; 0.049 sec/batch; 1h:16m:58s remains)
INFO - root - 2019-11-06 21:03:50.002434: step 55250, total loss = 0.20, predict loss = 0.05 (103.1 examples/sec; 0.039 sec/batch; 1h:01m:16s remains)
INFO - root - 2019-11-06 21:03:50.459869: step 55260, total loss = 0.17, predict loss = 0.05 (92.4 examples/sec; 0.043 sec/batch; 1h:08m:21s remains)
INFO - root - 2019-11-06 21:03:50.910680: step 55270, total loss = 0.24, predict loss = 0.06 (101.4 examples/sec; 0.039 sec/batch; 1h:02m:16s remains)
INFO - root - 2019-11-06 21:03:51.965930: step 55280, total loss = 0.18, predict loss = 0.04 (70.1 examples/sec; 0.057 sec/batch; 1h:30m:07s remains)
INFO - root - 2019-11-06 21:03:52.624141: step 55290, total loss = 0.13, predict loss = 0.03 (77.4 examples/sec; 0.052 sec/batch; 1h:21m:33s remains)
INFO - root - 2019-11-06 21:03:53.202244: step 55300, total loss = 0.20, predict loss = 0.05 (76.1 examples/sec; 0.053 sec/batch; 1h:22m:58s remains)
INFO - root - 2019-11-06 21:03:53.781191: step 55310, total loss = 0.32, predict loss = 0.09 (81.3 examples/sec; 0.049 sec/batch; 1h:17m:39s remains)
INFO - root - 2019-11-06 21:03:54.353997: step 55320, total loss = 0.21, predict loss = 0.05 (76.8 examples/sec; 0.052 sec/batch; 1h:22m:11s remains)
INFO - root - 2019-11-06 21:03:54.954266: step 55330, total loss = 0.20, predict loss = 0.05 (77.6 examples/sec; 0.052 sec/batch; 1h:21m:17s remains)
INFO - root - 2019-11-06 21:03:55.522703: step 55340, total loss = 0.16, predict loss = 0.04 (80.0 examples/sec; 0.050 sec/batch; 1h:18m:53s remains)
INFO - root - 2019-11-06 21:03:56.103283: step 55350, total loss = 0.26, predict loss = 0.07 (77.6 examples/sec; 0.052 sec/batch; 1h:21m:18s remains)
INFO - root - 2019-11-06 21:03:56.693138: step 55360, total loss = 0.18, predict loss = 0.05 (76.4 examples/sec; 0.052 sec/batch; 1h:22m:36s remains)
INFO - root - 2019-11-06 21:03:57.293414: step 55370, total loss = 0.20, predict loss = 0.05 (78.0 examples/sec; 0.051 sec/batch; 1h:20m:53s remains)
INFO - root - 2019-11-06 21:03:57.872143: step 55380, total loss = 0.26, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:19m:09s remains)
INFO - root - 2019-11-06 21:03:58.452069: step 55390, total loss = 0.18, predict loss = 0.04 (77.3 examples/sec; 0.052 sec/batch; 1h:21m:35s remains)
INFO - root - 2019-11-06 21:03:58.934118: step 55400, total loss = 0.16, predict loss = 0.04 (99.0 examples/sec; 0.040 sec/batch; 1h:03m:42s remains)
INFO - root - 2019-11-06 21:03:59.427560: step 55410, total loss = 0.20, predict loss = 0.05 (92.9 examples/sec; 0.043 sec/batch; 1h:07m:54s remains)
INFO - root - 2019-11-06 21:04:00.354411: step 55420, total loss = 0.20, predict loss = 0.05 (7.9 examples/sec; 0.504 sec/batch; 13h:15m:00s remains)
INFO - root - 2019-11-06 21:04:01.029072: step 55430, total loss = 0.13, predict loss = 0.03 (60.7 examples/sec; 0.066 sec/batch; 1h:43m:52s remains)
INFO - root - 2019-11-06 21:04:01.635700: step 55440, total loss = 0.27, predict loss = 0.06 (82.2 examples/sec; 0.049 sec/batch; 1h:16m:40s remains)
INFO - root - 2019-11-06 21:04:02.222865: step 55450, total loss = 0.21, predict loss = 0.05 (76.3 examples/sec; 0.052 sec/batch; 1h:22m:33s remains)
INFO - root - 2019-11-06 21:04:02.798782: step 55460, total loss = 0.18, predict loss = 0.05 (82.1 examples/sec; 0.049 sec/batch; 1h:16m:44s remains)
INFO - root - 2019-11-06 21:04:03.379779: step 55470, total loss = 0.21, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:20m:42s remains)
INFO - root - 2019-11-06 21:04:03.964389: step 55480, total loss = 0.24, predict loss = 0.06 (80.9 examples/sec; 0.049 sec/batch; 1h:17m:52s remains)
INFO - root - 2019-11-06 21:04:04.554685: step 55490, total loss = 0.19, predict loss = 0.05 (81.0 examples/sec; 0.049 sec/batch; 1h:17m:49s remains)
INFO - root - 2019-11-06 21:04:05.141482: step 55500, total loss = 0.22, predict loss = 0.06 (79.9 examples/sec; 0.050 sec/batch; 1h:18m:51s remains)
INFO - root - 2019-11-06 21:04:05.725874: step 55510, total loss = 0.14, predict loss = 0.03 (76.7 examples/sec; 0.052 sec/batch; 1h:22m:09s remains)
INFO - root - 2019-11-06 21:04:06.306518: step 55520, total loss = 0.25, predict loss = 0.07 (81.1 examples/sec; 0.049 sec/batch; 1h:17m:37s remains)
INFO - root - 2019-11-06 21:04:06.902902: step 55530, total loss = 0.22, predict loss = 0.05 (77.0 examples/sec; 0.052 sec/batch; 1h:21m:46s remains)
INFO - root - 2019-11-06 21:04:07.477119: step 55540, total loss = 0.24, predict loss = 0.07 (87.9 examples/sec; 0.045 sec/batch; 1h:11m:36s remains)
INFO - root - 2019-11-06 21:04:07.933073: step 55550, total loss = 0.17, predict loss = 0.05 (100.1 examples/sec; 0.040 sec/batch; 1h:02m:52s remains)
INFO - root - 2019-11-06 21:04:08.385893: step 55560, total loss = 0.28, predict loss = 0.07 (97.4 examples/sec; 0.041 sec/batch; 1h:04m:39s remains)
INFO - root - 2019-11-06 21:04:09.323621: step 55570, total loss = 0.19, predict loss = 0.05 (76.6 examples/sec; 0.052 sec/batch; 1h:22m:09s remains)
INFO - root - 2019-11-06 21:04:09.954760: step 55580, total loss = 0.21, predict loss = 0.05 (68.0 examples/sec; 0.059 sec/batch; 1h:32m:34s remains)
INFO - root - 2019-11-06 21:04:10.545576: step 55590, total loss = 0.19, predict loss = 0.04 (74.8 examples/sec; 0.053 sec/batch; 1h:24m:07s remains)
INFO - root - 2019-11-06 21:04:11.134625: step 55600, total loss = 0.15, predict loss = 0.03 (74.3 examples/sec; 0.054 sec/batch; 1h:24m:40s remains)
INFO - root - 2019-11-06 21:04:11.732032: step 55610, total loss = 0.18, predict loss = 0.04 (74.5 examples/sec; 0.054 sec/batch; 1h:24m:27s remains)
INFO - root - 2019-11-06 21:04:12.319415: step 55620, total loss = 0.20, predict loss = 0.06 (76.5 examples/sec; 0.052 sec/batch; 1h:22m:12s remains)
INFO - root - 2019-11-06 21:04:12.921446: step 55630, total loss = 0.18, predict loss = 0.04 (72.2 examples/sec; 0.055 sec/batch; 1h:27m:04s remains)
INFO - root - 2019-11-06 21:04:13.492627: step 55640, total loss = 0.18, predict loss = 0.04 (77.1 examples/sec; 0.052 sec/batch; 1h:21m:36s remains)
INFO - root - 2019-11-06 21:04:14.101713: step 55650, total loss = 0.18, predict loss = 0.04 (77.2 examples/sec; 0.052 sec/batch; 1h:21m:30s remains)
INFO - root - 2019-11-06 21:04:14.685375: step 55660, total loss = 0.20, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:21m:27s remains)
INFO - root - 2019-11-06 21:04:15.266879: step 55670, total loss = 0.17, predict loss = 0.04 (79.9 examples/sec; 0.050 sec/batch; 1h:18m:39s remains)
INFO - root - 2019-11-06 21:04:15.837688: step 55680, total loss = 0.23, predict loss = 0.05 (79.0 examples/sec; 0.051 sec/batch; 1h:19m:34s remains)
INFO - root - 2019-11-06 21:04:16.412016: step 55690, total loss = 0.23, predict loss = 0.06 (91.7 examples/sec; 0.044 sec/batch; 1h:08m:34s remains)
INFO - root - 2019-11-06 21:04:16.869506: step 55700, total loss = 0.24, predict loss = 0.07 (98.5 examples/sec; 0.041 sec/batch; 1h:03m:49s remains)
INFO - root - 2019-11-06 21:04:17.325140: step 55710, total loss = 0.23, predict loss = 0.07 (89.6 examples/sec; 0.045 sec/batch; 1h:10m:10s remains)
INFO - root - 2019-11-06 21:04:18.295877: step 55720, total loss = 0.29, predict loss = 0.08 (54.5 examples/sec; 0.073 sec/batch; 1h:55m:14s remains)
INFO - root - 2019-11-06 21:04:18.974141: step 55730, total loss = 0.26, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 1h:18m:53s remains)
INFO - root - 2019-11-06 21:04:19.542944: step 55740, total loss = 0.24, predict loss = 0.06 (80.3 examples/sec; 0.050 sec/batch; 1h:18m:13s remains)
INFO - root - 2019-11-06 21:04:20.126909: step 55750, total loss = 0.21, predict loss = 0.06 (75.1 examples/sec; 0.053 sec/batch; 1h:23m:39s remains)
INFO - root - 2019-11-06 21:04:20.695811: step 55760, total loss = 0.18, predict loss = 0.04 (75.2 examples/sec; 0.053 sec/batch; 1h:23m:34s remains)
INFO - root - 2019-11-06 21:04:21.282929: step 55770, total loss = 0.24, predict loss = 0.07 (81.2 examples/sec; 0.049 sec/batch; 1h:17m:22s remains)
INFO - root - 2019-11-06 21:04:21.858318: step 55780, total loss = 0.16, predict loss = 0.04 (80.7 examples/sec; 0.050 sec/batch; 1h:17m:50s remains)
INFO - root - 2019-11-06 21:04:22.438281: step 55790, total loss = 0.15, predict loss = 0.04 (82.3 examples/sec; 0.049 sec/batch; 1h:16m:18s remains)
INFO - root - 2019-11-06 21:04:23.024427: step 55800, total loss = 0.24, predict loss = 0.06 (76.9 examples/sec; 0.052 sec/batch; 1h:21m:39s remains)
INFO - root - 2019-11-06 21:04:23.617370: step 55810, total loss = 0.26, predict loss = 0.08 (78.7 examples/sec; 0.051 sec/batch; 1h:19m:48s remains)
INFO - root - 2019-11-06 21:04:24.208158: step 55820, total loss = 0.17, predict loss = 0.04 (77.9 examples/sec; 0.051 sec/batch; 1h:20m:35s remains)
INFO - root - 2019-11-06 21:04:24.789948: step 55830, total loss = 0.19, predict loss = 0.05 (80.1 examples/sec; 0.050 sec/batch; 1h:18m:20s remains)
INFO - root - 2019-11-06 21:04:25.338885: step 55840, total loss = 0.23, predict loss = 0.06 (97.4 examples/sec; 0.041 sec/batch; 1h:04m:25s remains)
INFO - root - 2019-11-06 21:04:25.804855: step 55850, total loss = 0.19, predict loss = 0.05 (106.7 examples/sec; 0.038 sec/batch; 0h:58m:50s remains)
INFO - root - 2019-11-06 21:04:26.235840: step 55860, total loss = 0.24, predict loss = 0.08 (105.2 examples/sec; 0.038 sec/batch; 0h:59m:38s remains)
INFO - root - 2019-11-06 21:04:27.298334: step 55870, total loss = 0.15, predict loss = 0.04 (41.2 examples/sec; 0.097 sec/batch; 2h:32m:23s remains)
INFO - root - 2019-11-06 21:04:27.967711: step 55880, total loss = 0.20, predict loss = 0.05 (76.0 examples/sec; 0.053 sec/batch; 1h:22m:36s remains)
INFO - root - 2019-11-06 21:04:28.577862: step 55890, total loss = 0.25, predict loss = 0.07 (77.0 examples/sec; 0.052 sec/batch; 1h:21m:26s remains)
INFO - root - 2019-11-06 21:04:29.152662: step 55900, total loss = 0.19, predict loss = 0.04 (73.6 examples/sec; 0.054 sec/batch; 1h:25m:13s remains)
INFO - root - 2019-11-06 21:04:29.739027: step 55910, total loss = 0.23, predict loss = 0.06 (75.2 examples/sec; 0.053 sec/batch; 1h:23m:25s remains)
INFO - root - 2019-11-06 21:04:30.306341: step 55920, total loss = 0.17, predict loss = 0.04 (77.7 examples/sec; 0.051 sec/batch; 1h:20m:43s remains)
INFO - root - 2019-11-06 21:04:30.930904: step 55930, total loss = 0.28, predict loss = 0.08 (73.9 examples/sec; 0.054 sec/batch; 1h:24m:53s remains)
INFO - root - 2019-11-06 21:04:31.517444: step 55940, total loss = 0.23, predict loss = 0.06 (73.6 examples/sec; 0.054 sec/batch; 1h:25m:11s remains)
INFO - root - 2019-11-06 21:04:32.096719: step 55950, total loss = 0.19, predict loss = 0.05 (73.3 examples/sec; 0.055 sec/batch; 1h:25m:29s remains)
INFO - root - 2019-11-06 21:04:32.670388: step 55960, total loss = 0.29, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 1h:20m:12s remains)
INFO - root - 2019-11-06 21:04:33.272853: step 55970, total loss = 0.24, predict loss = 0.06 (76.0 examples/sec; 0.053 sec/batch; 1h:22m:29s remains)
INFO - root - 2019-11-06 21:04:33.851179: step 55980, total loss = 0.15, predict loss = 0.04 (72.0 examples/sec; 0.056 sec/batch; 1h:27m:04s remains)
INFO - root - 2019-11-06 21:04:34.373590: step 55990, total loss = 0.15, predict loss = 0.04 (100.1 examples/sec; 0.040 sec/batch; 1h:02m:37s remains)
INFO - root - 2019-11-06 21:04:34.809415: step 56000, total loss = 0.18, predict loss = 0.04 (102.3 examples/sec; 0.039 sec/batch; 1h:01m:15s remains)
INFO - root - 2019-11-06 21:04:35.277032: step 56010, total loss = 0.25, predict loss = 0.07 (94.6 examples/sec; 0.042 sec/batch; 1h:06m:14s remains)
INFO - root - 2019-11-06 21:04:36.333219: step 56020, total loss = 0.17, predict loss = 0.04 (52.3 examples/sec; 0.076 sec/batch; 1h:59m:42s remains)
INFO - root - 2019-11-06 21:04:36.969930: step 56030, total loss = 0.17, predict loss = 0.04 (76.9 examples/sec; 0.052 sec/batch; 1h:21m:30s remains)
INFO - root - 2019-11-06 21:04:37.555049: step 56040, total loss = 0.22, predict loss = 0.06 (72.9 examples/sec; 0.055 sec/batch; 1h:25m:56s remains)
INFO - root - 2019-11-06 21:04:38.151083: step 56050, total loss = 0.22, predict loss = 0.06 (68.9 examples/sec; 0.058 sec/batch; 1h:30m:54s remains)
INFO - root - 2019-11-06 21:04:38.723669: step 56060, total loss = 0.16, predict loss = 0.04 (75.4 examples/sec; 0.053 sec/batch; 1h:23m:04s remains)
INFO - root - 2019-11-06 21:04:39.298446: step 56070, total loss = 0.19, predict loss = 0.05 (75.1 examples/sec; 0.053 sec/batch; 1h:23m:23s remains)
INFO - root - 2019-11-06 21:04:39.876600: step 56080, total loss = 0.21, predict loss = 0.05 (80.2 examples/sec; 0.050 sec/batch; 1h:18m:06s remains)
INFO - root - 2019-11-06 21:04:40.465354: step 56090, total loss = 0.17, predict loss = 0.04 (78.3 examples/sec; 0.051 sec/batch; 1h:19m:56s remains)
INFO - root - 2019-11-06 21:04:41.035254: step 56100, total loss = 0.17, predict loss = 0.04 (81.5 examples/sec; 0.049 sec/batch; 1h:16m:48s remains)
INFO - root - 2019-11-06 21:04:41.621481: step 56110, total loss = 0.15, predict loss = 0.04 (78.3 examples/sec; 0.051 sec/batch; 1h:19m:55s remains)
INFO - root - 2019-11-06 21:04:42.198199: step 56120, total loss = 0.24, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 1h:19m:35s remains)
INFO - root - 2019-11-06 21:04:42.777461: step 56130, total loss = 0.17, predict loss = 0.04 (77.7 examples/sec; 0.051 sec/batch; 1h:20m:32s remains)
INFO - root - 2019-11-06 21:04:43.271101: step 56140, total loss = 0.28, predict loss = 0.08 (102.0 examples/sec; 0.039 sec/batch; 1h:01m:21s remains)
INFO - root - 2019-11-06 21:04:43.717153: step 56150, total loss = 0.24, predict loss = 0.06 (90.4 examples/sec; 0.044 sec/batch; 1h:09m:12s remains)
INFO - root - 2019-11-06 21:04:44.186359: step 56160, total loss = 0.18, predict loss = 0.04 (92.2 examples/sec; 0.043 sec/batch; 1h:07m:53s remains)
INFO - root - 2019-11-06 21:04:45.304794: step 56170, total loss = 0.20, predict loss = 0.05 (64.2 examples/sec; 0.062 sec/batch; 1h:37m:29s remains)
INFO - root - 2019-11-06 21:04:45.915590: step 56180, total loss = 0.15, predict loss = 0.04 (75.5 examples/sec; 0.053 sec/batch; 1h:22m:51s remains)
INFO - root - 2019-11-06 21:04:46.489909: step 56190, total loss = 0.26, predict loss = 0.07 (77.2 examples/sec; 0.052 sec/batch; 1h:21m:01s remains)
INFO - root - 2019-11-06 21:04:47.058363: step 56200, total loss = 0.24, predict loss = 0.06 (80.6 examples/sec; 0.050 sec/batch; 1h:17m:35s remains)
INFO - root - 2019-11-06 21:04:47.650798: step 56210, total loss = 0.19, predict loss = 0.05 (75.4 examples/sec; 0.053 sec/batch; 1h:22m:53s remains)
INFO - root - 2019-11-06 21:04:48.225353: step 56220, total loss = 0.24, predict loss = 0.06 (79.9 examples/sec; 0.050 sec/batch; 1h:18m:16s remains)
INFO - root - 2019-11-06 21:04:48.799879: step 56230, total loss = 0.17, predict loss = 0.04 (78.6 examples/sec; 0.051 sec/batch; 1h:19m:29s remains)
INFO - root - 2019-11-06 21:04:49.389685: step 56240, total loss = 0.19, predict loss = 0.04 (79.7 examples/sec; 0.050 sec/batch; 1h:18m:25s remains)
INFO - root - 2019-11-06 21:04:49.992412: step 56250, total loss = 0.16, predict loss = 0.04 (82.3 examples/sec; 0.049 sec/batch; 1h:15m:58s remains)
INFO - root - 2019-11-06 21:04:50.575436: step 56260, total loss = 0.26, predict loss = 0.06 (70.3 examples/sec; 0.057 sec/batch; 1h:28m:50s remains)
INFO - root - 2019-11-06 21:04:51.159063: step 56270, total loss = 0.19, predict loss = 0.05 (79.2 examples/sec; 0.051 sec/batch; 1h:18m:55s remains)
INFO - root - 2019-11-06 21:04:51.743586: step 56280, total loss = 0.23, predict loss = 0.05 (77.0 examples/sec; 0.052 sec/batch; 1h:21m:09s remains)
INFO - root - 2019-11-06 21:04:52.230165: step 56290, total loss = 0.29, predict loss = 0.07 (96.0 examples/sec; 0.042 sec/batch; 1h:05m:05s remains)
INFO - root - 2019-11-06 21:04:52.683337: step 56300, total loss = 0.22, predict loss = 0.05 (98.7 examples/sec; 0.041 sec/batch; 1h:03m:18s remains)
INFO - root - 2019-11-06 21:04:53.594174: step 56310, total loss = 0.18, predict loss = 0.04 (79.7 examples/sec; 0.050 sec/batch; 1h:18m:24s remains)
INFO - root - 2019-11-06 21:04:54.241985: step 56320, total loss = 0.18, predict loss = 0.04 (68.1 examples/sec; 0.059 sec/batch; 1h:31m:39s remains)
INFO - root - 2019-11-06 21:04:54.876322: step 56330, total loss = 0.25, predict loss = 0.05 (78.6 examples/sec; 0.051 sec/batch; 1h:19m:27s remains)
INFO - root - 2019-11-06 21:04:55.466976: step 56340, total loss = 0.29, predict loss = 0.08 (77.7 examples/sec; 0.052 sec/batch; 1h:20m:24s remains)
INFO - root - 2019-11-06 21:04:56.044880: step 56350, total loss = 0.18, predict loss = 0.05 (78.6 examples/sec; 0.051 sec/batch; 1h:19m:28s remains)
INFO - root - 2019-11-06 21:04:56.617038: step 56360, total loss = 0.23, predict loss = 0.06 (80.6 examples/sec; 0.050 sec/batch; 1h:17m:25s remains)
INFO - root - 2019-11-06 21:04:57.206565: step 56370, total loss = 0.12, predict loss = 0.03 (76.7 examples/sec; 0.052 sec/batch; 1h:21m:23s remains)
INFO - root - 2019-11-06 21:04:57.775635: step 56380, total loss = 0.19, predict loss = 0.04 (75.5 examples/sec; 0.053 sec/batch; 1h:22m:39s remains)
INFO - root - 2019-11-06 21:04:58.363765: step 56390, total loss = 0.22, predict loss = 0.06 (79.4 examples/sec; 0.050 sec/batch; 1h:18m:37s remains)
INFO - root - 2019-11-06 21:04:58.949685: step 56400, total loss = 0.27, predict loss = 0.07 (81.8 examples/sec; 0.049 sec/batch; 1h:16m:18s remains)
INFO - root - 2019-11-06 21:04:59.540910: step 56410, total loss = 0.19, predict loss = 0.04 (81.2 examples/sec; 0.049 sec/batch; 1h:16m:51s remains)
INFO - root - 2019-11-06 21:05:00.115415: step 56420, total loss = 0.16, predict loss = 0.04 (79.9 examples/sec; 0.050 sec/batch; 1h:18m:07s remains)
INFO - root - 2019-11-06 21:05:00.702975: step 56430, total loss = 0.15, predict loss = 0.04 (92.1 examples/sec; 0.043 sec/batch; 1h:07m:42s remains)
INFO - root - 2019-11-06 21:05:01.173320: step 56440, total loss = 0.20, predict loss = 0.05 (96.0 examples/sec; 0.042 sec/batch; 1h:04m:57s remains)
INFO - root - 2019-11-06 21:05:01.654361: step 56450, total loss = 0.22, predict loss = 0.06 (96.7 examples/sec; 0.041 sec/batch; 1h:04m:30s remains)
INFO - root - 2019-11-06 21:05:02.571749: step 56460, total loss = 0.30, predict loss = 0.09 (76.5 examples/sec; 0.052 sec/batch; 1h:21m:31s remains)
INFO - root - 2019-11-06 21:05:03.248569: step 56470, total loss = 0.35, predict loss = 0.10 (69.2 examples/sec; 0.058 sec/batch; 1h:30m:07s remains)
INFO - root - 2019-11-06 21:05:03.856063: step 56480, total loss = 0.20, predict loss = 0.05 (74.0 examples/sec; 0.054 sec/batch; 1h:24m:17s remains)
INFO - root - 2019-11-06 21:05:04.457062: step 56490, total loss = 0.16, predict loss = 0.04 (79.1 examples/sec; 0.051 sec/batch; 1h:18m:50s remains)
INFO - root - 2019-11-06 21:05:05.027217: step 56500, total loss = 0.17, predict loss = 0.04 (79.9 examples/sec; 0.050 sec/batch; 1h:18m:01s remains)
INFO - root - 2019-11-06 21:05:05.606957: step 56510, total loss = 0.26, predict loss = 0.07 (78.9 examples/sec; 0.051 sec/batch; 1h:19m:00s remains)
INFO - root - 2019-11-06 21:05:06.189552: step 56520, total loss = 0.26, predict loss = 0.08 (81.5 examples/sec; 0.049 sec/batch; 1h:16m:29s remains)
INFO - root - 2019-11-06 21:05:06.785913: step 56530, total loss = 0.19, predict loss = 0.05 (74.3 examples/sec; 0.054 sec/batch; 1h:23m:53s remains)
INFO - root - 2019-11-06 21:05:07.358870: step 56540, total loss = 0.22, predict loss = 0.05 (82.2 examples/sec; 0.049 sec/batch; 1h:15m:50s remains)
INFO - root - 2019-11-06 21:05:07.931828: step 56550, total loss = 0.16, predict loss = 0.04 (80.4 examples/sec; 0.050 sec/batch; 1h:17m:28s remains)
INFO - root - 2019-11-06 21:05:08.507568: step 56560, total loss = 0.27, predict loss = 0.07 (77.7 examples/sec; 0.051 sec/batch; 1h:20m:07s remains)
INFO - root - 2019-11-06 21:05:09.089639: step 56570, total loss = 0.23, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:19m:45s remains)
INFO - root - 2019-11-06 21:05:09.636754: step 56580, total loss = 0.32, predict loss = 0.07 (92.8 examples/sec; 0.043 sec/batch; 1h:07m:08s remains)
INFO - root - 2019-11-06 21:05:10.103227: step 56590, total loss = 0.33, predict loss = 0.09 (89.0 examples/sec; 0.045 sec/batch; 1h:09m:59s remains)
INFO - root - 2019-11-06 21:05:10.564073: step 56600, total loss = 0.23, predict loss = 0.05 (92.9 examples/sec; 0.043 sec/batch; 1h:07m:02s remains)
INFO - root - 2019-11-06 21:05:11.547440: step 56610, total loss = 0.17, predict loss = 0.05 (65.0 examples/sec; 0.062 sec/batch; 1h:35m:44s remains)
INFO - root - 2019-11-06 21:05:12.158275: step 56620, total loss = 0.17, predict loss = 0.04 (79.2 examples/sec; 0.051 sec/batch; 1h:18m:37s remains)
INFO - root - 2019-11-06 21:05:12.736326: step 56630, total loss = 0.23, predict loss = 0.06 (75.6 examples/sec; 0.053 sec/batch; 1h:22m:20s remains)
INFO - root - 2019-11-06 21:05:13.313185: step 56640, total loss = 0.15, predict loss = 0.04 (73.9 examples/sec; 0.054 sec/batch; 1h:24m:11s remains)
INFO - root - 2019-11-06 21:05:13.901280: step 56650, total loss = 0.14, predict loss = 0.04 (83.4 examples/sec; 0.048 sec/batch; 1h:14m:37s remains)
INFO - root - 2019-11-06 21:05:14.479084: step 56660, total loss = 0.31, predict loss = 0.09 (76.4 examples/sec; 0.052 sec/batch; 1h:21m:24s remains)
INFO - root - 2019-11-06 21:05:15.056645: step 56670, total loss = 0.18, predict loss = 0.04 (73.6 examples/sec; 0.054 sec/batch; 1h:24m:30s remains)
INFO - root - 2019-11-06 21:05:15.627750: step 56680, total loss = 0.32, predict loss = 0.10 (81.8 examples/sec; 0.049 sec/batch; 1h:16m:02s remains)
INFO - root - 2019-11-06 21:05:16.228712: step 56690, total loss = 0.21, predict loss = 0.05 (75.4 examples/sec; 0.053 sec/batch; 1h:22m:29s remains)
INFO - root - 2019-11-06 21:05:16.804151: step 56700, total loss = 0.15, predict loss = 0.04 (78.4 examples/sec; 0.051 sec/batch; 1h:19m:19s remains)
INFO - root - 2019-11-06 21:05:17.377720: step 56710, total loss = 0.24, predict loss = 0.07 (74.6 examples/sec; 0.054 sec/batch; 1h:23m:25s remains)
INFO - root - 2019-11-06 21:05:17.951280: step 56720, total loss = 0.19, predict loss = 0.06 (83.2 examples/sec; 0.048 sec/batch; 1h:14m:42s remains)
INFO - root - 2019-11-06 21:05:18.486865: step 56730, total loss = 0.25, predict loss = 0.07 (96.8 examples/sec; 0.041 sec/batch; 1h:04m:12s remains)
INFO - root - 2019-11-06 21:05:18.955267: step 56740, total loss = 0.18, predict loss = 0.05 (91.8 examples/sec; 0.044 sec/batch; 1h:07m:42s remains)
INFO - root - 2019-11-06 21:05:19.421194: step 56750, total loss = 0.29, predict loss = 0.08 (92.5 examples/sec; 0.043 sec/batch; 1h:07m:12s remains)
INFO - root - 2019-11-06 21:05:20.452831: step 56760, total loss = 0.21, predict loss = 0.05 (64.6 examples/sec; 0.062 sec/batch; 1h:36m:16s remains)
INFO - root - 2019-11-06 21:05:21.134602: step 56770, total loss = 0.22, predict loss = 0.07 (71.4 examples/sec; 0.056 sec/batch; 1h:27m:03s remains)
INFO - root - 2019-11-06 21:05:21.744451: step 56780, total loss = 0.21, predict loss = 0.06 (78.8 examples/sec; 0.051 sec/batch; 1h:18m:52s remains)
INFO - root - 2019-11-06 21:05:22.315987: step 56790, total loss = 0.26, predict loss = 0.07 (79.6 examples/sec; 0.050 sec/batch; 1h:18m:01s remains)
INFO - root - 2019-11-06 21:05:22.879626: step 56800, total loss = 0.41, predict loss = 0.13 (81.3 examples/sec; 0.049 sec/batch; 1h:16m:24s remains)
INFO - root - 2019-11-06 21:05:23.479074: step 56810, total loss = 0.15, predict loss = 0.04 (78.3 examples/sec; 0.051 sec/batch; 1h:19m:17s remains)
INFO - root - 2019-11-06 21:05:24.051409: step 56820, total loss = 0.26, predict loss = 0.07 (81.9 examples/sec; 0.049 sec/batch; 1h:15m:52s remains)
INFO - root - 2019-11-06 21:05:24.623138: step 56830, total loss = 0.25, predict loss = 0.07 (73.3 examples/sec; 0.055 sec/batch; 1h:24m:43s remains)
INFO - root - 2019-11-06 21:05:25.199795: step 56840, total loss = 0.47, predict loss = 0.15 (80.4 examples/sec; 0.050 sec/batch; 1h:17m:15s remains)
INFO - root - 2019-11-06 21:05:25.792497: step 56850, total loss = 0.18, predict loss = 0.04 (79.6 examples/sec; 0.050 sec/batch; 1h:17m:59s remains)
INFO - root - 2019-11-06 21:05:26.367648: step 56860, total loss = 0.25, predict loss = 0.07 (81.5 examples/sec; 0.049 sec/batch; 1h:16m:11s remains)
INFO - root - 2019-11-06 21:05:26.940322: step 56870, total loss = 0.24, predict loss = 0.06 (81.3 examples/sec; 0.049 sec/batch; 1h:16m:19s remains)
INFO - root - 2019-11-06 21:05:27.450062: step 56880, total loss = 0.23, predict loss = 0.06 (95.3 examples/sec; 0.042 sec/batch; 1h:05m:09s remains)
INFO - root - 2019-11-06 21:05:27.920870: step 56890, total loss = 0.24, predict loss = 0.06 (97.2 examples/sec; 0.041 sec/batch; 1h:03m:49s remains)
INFO - root - 2019-11-06 21:05:28.372776: step 56900, total loss = 0.16, predict loss = 0.04 (98.1 examples/sec; 0.041 sec/batch; 1h:03m:16s remains)
INFO - root - 2019-11-06 21:05:29.521857: step 56910, total loss = 0.17, predict loss = 0.04 (45.5 examples/sec; 0.088 sec/batch; 2h:16m:19s remains)
INFO - root - 2019-11-06 21:05:30.160831: step 56920, total loss = 0.19, predict loss = 0.05 (74.5 examples/sec; 0.054 sec/batch; 1h:23m:14s remains)
INFO - root - 2019-11-06 21:05:30.775496: step 56930, total loss = 0.19, predict loss = 0.05 (79.0 examples/sec; 0.051 sec/batch; 1h:18m:32s remains)
INFO - root - 2019-11-06 21:05:31.348905: step 56940, total loss = 0.17, predict loss = 0.04 (71.9 examples/sec; 0.056 sec/batch; 1h:26m:19s remains)
INFO - root - 2019-11-06 21:05:31.926888: step 56950, total loss = 0.18, predict loss = 0.05 (75.5 examples/sec; 0.053 sec/batch; 1h:22m:07s remains)
INFO - root - 2019-11-06 21:05:32.518768: step 56960, total loss = 0.30, predict loss = 0.08 (74.2 examples/sec; 0.054 sec/batch; 1h:23m:35s remains)
INFO - root - 2019-11-06 21:05:33.112511: step 56970, total loss = 0.27, predict loss = 0.07 (78.8 examples/sec; 0.051 sec/batch; 1h:18m:42s remains)
INFO - root - 2019-11-06 21:05:33.687871: step 56980, total loss = 0.19, predict loss = 0.05 (77.0 examples/sec; 0.052 sec/batch; 1h:20m:30s remains)
INFO - root - 2019-11-06 21:05:34.266221: step 56990, total loss = 0.17, predict loss = 0.04 (73.3 examples/sec; 0.055 sec/batch; 1h:24m:35s remains)
INFO - root - 2019-11-06 21:05:34.833531: step 57000, total loss = 0.20, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 1h:18m:51s remains)
INFO - root - 2019-11-06 21:05:35.425936: step 57010, total loss = 0.21, predict loss = 0.06 (77.0 examples/sec; 0.052 sec/batch; 1h:20m:28s remains)
INFO - root - 2019-11-06 21:05:36.010658: step 57020, total loss = 0.18, predict loss = 0.04 (83.3 examples/sec; 0.048 sec/batch; 1h:14m:26s remains)
INFO - root - 2019-11-06 21:05:36.483083: step 57030, total loss = 0.18, predict loss = 0.04 (103.7 examples/sec; 0.039 sec/batch; 0h:59m:44s remains)
INFO - root - 2019-11-06 21:05:36.913821: step 57040, total loss = 0.22, predict loss = 0.06 (93.5 examples/sec; 0.043 sec/batch; 1h:06m:15s remains)
INFO - root - 2019-11-06 21:05:37.855145: step 57050, total loss = 0.29, predict loss = 0.08 (8.0 examples/sec; 0.500 sec/batch; 12h:55m:01s remains)
INFO - root - 2019-11-06 21:05:38.534298: step 57060, total loss = 0.20, predict loss = 0.04 (56.7 examples/sec; 0.071 sec/batch; 1h:49m:14s remains)
INFO - root - 2019-11-06 21:05:39.174046: step 57070, total loss = 0.33, predict loss = 0.09 (72.9 examples/sec; 0.055 sec/batch; 1h:25m:00s remains)
INFO - root - 2019-11-06 21:05:39.756295: step 57080, total loss = 0.19, predict loss = 0.05 (80.2 examples/sec; 0.050 sec/batch; 1h:17m:15s remains)
INFO - root - 2019-11-06 21:05:40.354281: step 57090, total loss = 0.17, predict loss = 0.04 (78.4 examples/sec; 0.051 sec/batch; 1h:19m:02s remains)
INFO - root - 2019-11-06 21:05:40.941237: step 57100, total loss = 0.19, predict loss = 0.05 (74.6 examples/sec; 0.054 sec/batch; 1h:23m:00s remains)
INFO - root - 2019-11-06 21:05:41.524762: step 57110, total loss = 0.25, predict loss = 0.08 (76.6 examples/sec; 0.052 sec/batch; 1h:20m:51s remains)
INFO - root - 2019-11-06 21:05:42.105981: step 57120, total loss = 0.19, predict loss = 0.05 (76.9 examples/sec; 0.052 sec/batch; 1h:20m:29s remains)
INFO - root - 2019-11-06 21:05:42.694172: step 57130, total loss = 0.18, predict loss = 0.05 (79.5 examples/sec; 0.050 sec/batch; 1h:17m:54s remains)
INFO - root - 2019-11-06 21:05:43.270548: step 57140, total loss = 0.18, predict loss = 0.04 (75.8 examples/sec; 0.053 sec/batch; 1h:21m:40s remains)
INFO - root - 2019-11-06 21:05:43.851179: step 57150, total loss = 0.16, predict loss = 0.04 (77.6 examples/sec; 0.052 sec/batch; 1h:19m:45s remains)
INFO - root - 2019-11-06 21:05:44.440768: step 57160, total loss = 0.34, predict loss = 0.10 (81.7 examples/sec; 0.049 sec/batch; 1h:15m:44s remains)
INFO - root - 2019-11-06 21:05:45.032229: step 57170, total loss = 0.15, predict loss = 0.03 (83.4 examples/sec; 0.048 sec/batch; 1h:14m:12s remains)
INFO - root - 2019-11-06 21:05:45.499562: step 57180, total loss = 0.33, predict loss = 0.09 (96.6 examples/sec; 0.041 sec/batch; 1h:04m:04s remains)
INFO - root - 2019-11-06 21:05:45.954214: step 57190, total loss = 0.23, predict loss = 0.06 (92.3 examples/sec; 0.043 sec/batch; 1h:07m:03s remains)
INFO - root - 2019-11-06 21:05:46.876987: step 57200, total loss = 0.18, predict loss = 0.05 (75.0 examples/sec; 0.053 sec/batch; 1h:22m:31s remains)
INFO - root - 2019-11-06 21:05:47.562224: step 57210, total loss = 0.29, predict loss = 0.08 (61.6 examples/sec; 0.065 sec/batch; 1h:40m:28s remains)
INFO - root - 2019-11-06 21:05:48.164294: step 57220, total loss = 0.19, predict loss = 0.05 (75.4 examples/sec; 0.053 sec/batch; 1h:22m:00s remains)
INFO - root - 2019-11-06 21:05:48.736892: step 57230, total loss = 0.22, predict loss = 0.06 (79.4 examples/sec; 0.050 sec/batch; 1h:17m:53s remains)
INFO - root - 2019-11-06 21:05:49.312734: step 57240, total loss = 0.18, predict loss = 0.04 (76.8 examples/sec; 0.052 sec/batch; 1h:20m:33s remains)
INFO - root - 2019-11-06 21:05:49.921583: step 57250, total loss = 0.16, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:19m:07s remains)
INFO - root - 2019-11-06 21:05:50.508550: step 57260, total loss = 0.16, predict loss = 0.04 (79.2 examples/sec; 0.050 sec/batch; 1h:18m:03s remains)
INFO - root - 2019-11-06 21:05:51.094825: step 57270, total loss = 0.19, predict loss = 0.05 (75.2 examples/sec; 0.053 sec/batch; 1h:22m:13s remains)
INFO - root - 2019-11-06 21:05:51.676189: step 57280, total loss = 0.20, predict loss = 0.05 (79.5 examples/sec; 0.050 sec/batch; 1h:17m:43s remains)
INFO - root - 2019-11-06 21:05:52.274996: step 57290, total loss = 0.33, predict loss = 0.10 (73.4 examples/sec; 0.055 sec/batch; 1h:24m:14s remains)
INFO - root - 2019-11-06 21:05:52.848329: step 57300, total loss = 0.18, predict loss = 0.05 (76.5 examples/sec; 0.052 sec/batch; 1h:20m:44s remains)
INFO - root - 2019-11-06 21:05:53.428357: step 57310, total loss = 0.20, predict loss = 0.05 (77.5 examples/sec; 0.052 sec/batch; 1h:19m:45s remains)
INFO - root - 2019-11-06 21:05:54.000087: step 57320, total loss = 0.22, predict loss = 0.06 (94.0 examples/sec; 0.043 sec/batch; 1h:05m:42s remains)
INFO - root - 2019-11-06 21:05:54.482310: step 57330, total loss = 0.16, predict loss = 0.04 (96.1 examples/sec; 0.042 sec/batch; 1h:04m:15s remains)
INFO - root - 2019-11-06 21:05:54.941773: step 57340, total loss = 0.15, predict loss = 0.04 (96.9 examples/sec; 0.041 sec/batch; 1h:03m:44s remains)
INFO - root - 2019-11-06 21:05:55.922898: step 57350, total loss = 0.28, predict loss = 0.08 (65.7 examples/sec; 0.061 sec/batch; 1h:34m:04s remains)
INFO - root - 2019-11-06 21:05:56.622450: step 57360, total loss = 0.21, predict loss = 0.05 (71.0 examples/sec; 0.056 sec/batch; 1h:27m:02s remains)
INFO - root - 2019-11-06 21:05:57.322321: step 57370, total loss = 0.28, predict loss = 0.07 (75.2 examples/sec; 0.053 sec/batch; 1h:22m:08s remains)
INFO - root - 2019-11-06 21:05:57.946105: step 57380, total loss = 0.24, predict loss = 0.06 (70.9 examples/sec; 0.056 sec/batch; 1h:27m:05s remains)
INFO - root - 2019-11-06 21:05:58.537803: step 57390, total loss = 0.16, predict loss = 0.04 (78.1 examples/sec; 0.051 sec/batch; 1h:19m:05s remains)
INFO - root - 2019-11-06 21:05:59.114265: step 57400, total loss = 0.18, predict loss = 0.04 (80.4 examples/sec; 0.050 sec/batch; 1h:16m:46s remains)
INFO - root - 2019-11-06 21:05:59.698722: step 57410, total loss = 0.24, predict loss = 0.06 (75.0 examples/sec; 0.053 sec/batch; 1h:22m:17s remains)
INFO - root - 2019-11-06 21:06:00.381896: step 57420, total loss = 0.25, predict loss = 0.07 (63.1 examples/sec; 0.063 sec/batch; 1h:37m:45s remains)
INFO - root - 2019-11-06 21:06:01.059358: step 57430, total loss = 0.25, predict loss = 0.07 (73.6 examples/sec; 0.054 sec/batch; 1h:23m:52s remains)
INFO - root - 2019-11-06 21:06:01.681661: step 57440, total loss = 0.20, predict loss = 0.06 (62.1 examples/sec; 0.064 sec/batch; 1h:39m:17s remains)
INFO - root - 2019-11-06 21:06:02.417297: step 57450, total loss = 0.20, predict loss = 0.05 (69.3 examples/sec; 0.058 sec/batch; 1h:29m:04s remains)
INFO - root - 2019-11-06 21:06:03.189635: step 57460, total loss = 0.13, predict loss = 0.03 (54.9 examples/sec; 0.073 sec/batch; 1h:52m:17s remains)
INFO - root - 2019-11-06 21:06:03.969100: step 57470, total loss = 0.30, predict loss = 0.09 (75.6 examples/sec; 0.053 sec/batch; 1h:21m:38s remains)
INFO - root - 2019-11-06 21:06:04.430201: step 57480, total loss = 0.28, predict loss = 0.07 (107.6 examples/sec; 0.037 sec/batch; 0h:57m:20s remains)
INFO - root - 2019-11-06 21:06:04.914215: step 57490, total loss = 0.40, predict loss = 0.13 (99.7 examples/sec; 0.040 sec/batch; 1h:01m:51s remains)
INFO - root - 2019-11-06 21:06:05.893563: step 57500, total loss = 0.24, predict loss = 0.06 (53.3 examples/sec; 0.075 sec/batch; 1h:55m:41s remains)
INFO - root - 2019-11-06 21:06:06.552643: step 57510, total loss = 0.17, predict loss = 0.05 (77.7 examples/sec; 0.051 sec/batch; 1h:19m:20s remains)
INFO - root - 2019-11-06 21:06:07.180188: step 57520, total loss = 0.16, predict loss = 0.04 (56.0 examples/sec; 0.071 sec/batch; 1h:50m:00s remains)
INFO - root - 2019-11-06 21:06:07.910833: step 57530, total loss = 0.20, predict loss = 0.05 (77.2 examples/sec; 0.052 sec/batch; 1h:19m:50s remains)
INFO - root - 2019-11-06 21:06:08.500229: step 57540, total loss = 0.20, predict loss = 0.05 (78.8 examples/sec; 0.051 sec/batch; 1h:18m:15s remains)
INFO - root - 2019-11-06 21:06:09.073768: step 57550, total loss = 0.12, predict loss = 0.03 (74.8 examples/sec; 0.053 sec/batch; 1h:22m:23s remains)
INFO - root - 2019-11-06 21:06:09.656341: step 57560, total loss = 0.20, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:17m:41s remains)
INFO - root - 2019-11-06 21:06:10.341355: step 57570, total loss = 0.19, predict loss = 0.05 (76.5 examples/sec; 0.052 sec/batch; 1h:20m:35s remains)
INFO - root - 2019-11-06 21:06:11.005002: step 57580, total loss = 0.16, predict loss = 0.04 (73.5 examples/sec; 0.054 sec/batch; 1h:23m:46s remains)
INFO - root - 2019-11-06 21:06:11.595424: step 57590, total loss = 0.13, predict loss = 0.03 (79.2 examples/sec; 0.050 sec/batch; 1h:17m:46s remains)
INFO - root - 2019-11-06 21:06:12.199369: step 57600, total loss = 0.28, predict loss = 0.09 (66.5 examples/sec; 0.060 sec/batch; 1h:32m:38s remains)
INFO - root - 2019-11-06 21:06:12.928664: step 57610, total loss = 0.17, predict loss = 0.04 (56.2 examples/sec; 0.071 sec/batch; 1h:49m:32s remains)
INFO - root - 2019-11-06 21:06:13.539252: step 57620, total loss = 0.30, predict loss = 0.10 (102.9 examples/sec; 0.039 sec/batch; 0h:59m:52s remains)
INFO - root - 2019-11-06 21:06:13.983102: step 57630, total loss = 0.17, predict loss = 0.04 (96.5 examples/sec; 0.041 sec/batch; 1h:03m:47s remains)
INFO - root - 2019-11-06 21:06:14.417823: step 57640, total loss = 0.25, predict loss = 0.07 (103.2 examples/sec; 0.039 sec/batch; 0h:59m:39s remains)
INFO - root - 2019-11-06 21:06:15.485054: step 57650, total loss = 0.14, predict loss = 0.04 (63.8 examples/sec; 0.063 sec/batch; 1h:36m:28s remains)
INFO - root - 2019-11-06 21:06:16.148341: step 57660, total loss = 0.19, predict loss = 0.04 (75.4 examples/sec; 0.053 sec/batch; 1h:21m:36s remains)
INFO - root - 2019-11-06 21:06:16.738696: step 57670, total loss = 0.27, predict loss = 0.07 (73.5 examples/sec; 0.054 sec/batch; 1h:23m:41s remains)
INFO - root - 2019-11-06 21:06:17.326950: step 57680, total loss = 0.13, predict loss = 0.03 (77.9 examples/sec; 0.051 sec/batch; 1h:18m:57s remains)
INFO - root - 2019-11-06 21:06:17.910881: step 57690, total loss = 0.18, predict loss = 0.05 (82.3 examples/sec; 0.049 sec/batch; 1h:14m:45s remains)
INFO - root - 2019-11-06 21:06:18.501553: step 57700, total loss = 0.19, predict loss = 0.05 (80.4 examples/sec; 0.050 sec/batch; 1h:16m:31s remains)
INFO - root - 2019-11-06 21:06:19.100453: step 57710, total loss = 0.21, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:18m:46s remains)
INFO - root - 2019-11-06 21:06:19.679096: step 57720, total loss = 0.24, predict loss = 0.07 (77.1 examples/sec; 0.052 sec/batch; 1h:19m:46s remains)
INFO - root - 2019-11-06 21:06:20.279001: step 57730, total loss = 0.25, predict loss = 0.07 (81.2 examples/sec; 0.049 sec/batch; 1h:15m:42s remains)
INFO - root - 2019-11-06 21:06:21.000828: step 57740, total loss = 0.17, predict loss = 0.04 (73.8 examples/sec; 0.054 sec/batch; 1h:23m:18s remains)
INFO - root - 2019-11-06 21:06:21.646051: step 57750, total loss = 0.19, predict loss = 0.05 (73.2 examples/sec; 0.055 sec/batch; 1h:23m:57s remains)
INFO - root - 2019-11-06 21:06:22.326703: step 57760, total loss = 0.22, predict loss = 0.06 (75.1 examples/sec; 0.053 sec/batch; 1h:21m:53s remains)
INFO - root - 2019-11-06 21:06:22.857092: step 57770, total loss = 0.18, predict loss = 0.05 (82.8 examples/sec; 0.048 sec/batch; 1h:14m:12s remains)
INFO - root - 2019-11-06 21:06:23.332449: step 57780, total loss = 0.25, predict loss = 0.07 (100.2 examples/sec; 0.040 sec/batch; 1h:01m:21s remains)
INFO - root - 2019-11-06 21:06:23.782347: step 57790, total loss = 0.21, predict loss = 0.06 (101.6 examples/sec; 0.039 sec/batch; 1h:00m:30s remains)
INFO - root - 2019-11-06 21:06:24.873826: step 57800, total loss = 0.23, predict loss = 0.06 (68.7 examples/sec; 0.058 sec/batch; 1h:29m:28s remains)
INFO - root - 2019-11-06 21:06:25.551817: step 57810, total loss = 0.17, predict loss = 0.05 (75.5 examples/sec; 0.053 sec/batch; 1h:21m:23s remains)
INFO - root - 2019-11-06 21:06:26.146732: step 57820, total loss = 0.17, predict loss = 0.04 (76.6 examples/sec; 0.052 sec/batch; 1h:20m:12s remains)
INFO - root - 2019-11-06 21:06:26.760748: step 57830, total loss = 0.12, predict loss = 0.03 (74.9 examples/sec; 0.053 sec/batch; 1h:21m:59s remains)
INFO - root - 2019-11-06 21:06:27.399122: step 57840, total loss = 0.20, predict loss = 0.06 (72.4 examples/sec; 0.055 sec/batch; 1h:24m:48s remains)
INFO - root - 2019-11-06 21:06:28.038752: step 57850, total loss = 0.21, predict loss = 0.06 (75.8 examples/sec; 0.053 sec/batch; 1h:21m:02s remains)
INFO - root - 2019-11-06 21:06:28.659290: step 57860, total loss = 0.17, predict loss = 0.04 (73.3 examples/sec; 0.055 sec/batch; 1h:23m:44s remains)
INFO - root - 2019-11-06 21:06:29.235997: step 57870, total loss = 0.21, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 1h:18m:36s remains)
INFO - root - 2019-11-06 21:06:29.814133: step 57880, total loss = 0.21, predict loss = 0.05 (77.6 examples/sec; 0.052 sec/batch; 1h:19m:11s remains)
INFO - root - 2019-11-06 21:06:30.411144: step 57890, total loss = 0.31, predict loss = 0.10 (75.1 examples/sec; 0.053 sec/batch; 1h:21m:43s remains)
INFO - root - 2019-11-06 21:06:30.995471: step 57900, total loss = 0.22, predict loss = 0.06 (78.5 examples/sec; 0.051 sec/batch; 1h:18m:11s remains)
INFO - root - 2019-11-06 21:06:31.578441: step 57910, total loss = 0.16, predict loss = 0.04 (75.7 examples/sec; 0.053 sec/batch; 1h:21m:07s remains)
INFO - root - 2019-11-06 21:06:32.055855: step 57920, total loss = 0.18, predict loss = 0.05 (97.7 examples/sec; 0.041 sec/batch; 1h:02m:51s remains)
INFO - root - 2019-11-06 21:06:32.523032: step 57930, total loss = 0.14, predict loss = 0.03 (95.4 examples/sec; 0.042 sec/batch; 1h:04m:18s remains)
INFO - root - 2019-11-06 21:06:33.427297: step 57940, total loss = 0.16, predict loss = 0.04 (67.0 examples/sec; 0.060 sec/batch; 1h:31m:35s remains)
INFO - root - 2019-11-06 21:06:34.172581: step 57950, total loss = 0.24, predict loss = 0.06 (59.8 examples/sec; 0.067 sec/batch; 1h:42m:32s remains)
INFO - root - 2019-11-06 21:06:34.813757: step 57960, total loss = 0.30, predict loss = 0.08 (76.4 examples/sec; 0.052 sec/batch; 1h:20m:20s remains)
INFO - root - 2019-11-06 21:06:35.406177: step 57970, total loss = 0.17, predict loss = 0.04 (78.6 examples/sec; 0.051 sec/batch; 1h:18m:04s remains)
INFO - root - 2019-11-06 21:06:35.978262: step 57980, total loss = 0.17, predict loss = 0.04 (78.5 examples/sec; 0.051 sec/batch; 1h:18m:10s remains)
INFO - root - 2019-11-06 21:06:36.559683: step 57990, total loss = 0.19, predict loss = 0.05 (78.5 examples/sec; 0.051 sec/batch; 1h:18m:06s remains)
INFO - root - 2019-11-06 21:06:37.148437: step 58000, total loss = 0.23, predict loss = 0.06 (76.9 examples/sec; 0.052 sec/batch; 1h:19m:46s remains)
INFO - root - 2019-11-06 21:06:37.749565: step 58010, total loss = 0.15, predict loss = 0.04 (76.2 examples/sec; 0.052 sec/batch; 1h:20m:29s remains)
INFO - root - 2019-11-06 21:06:38.337897: step 58020, total loss = 0.19, predict loss = 0.05 (78.6 examples/sec; 0.051 sec/batch; 1h:18m:01s remains)
INFO - root - 2019-11-06 21:06:38.927124: step 58030, total loss = 0.17, predict loss = 0.04 (75.7 examples/sec; 0.053 sec/batch; 1h:21m:02s remains)
INFO - root - 2019-11-06 21:06:39.529199: step 58040, total loss = 0.20, predict loss = 0.06 (80.6 examples/sec; 0.050 sec/batch; 1h:16m:04s remains)
INFO - root - 2019-11-06 21:06:40.172988: step 58050, total loss = 0.20, predict loss = 0.06 (79.9 examples/sec; 0.050 sec/batch; 1h:16m:41s remains)
INFO - root - 2019-11-06 21:06:40.744591: step 58060, total loss = 0.20, predict loss = 0.05 (82.7 examples/sec; 0.048 sec/batch; 1h:14m:08s remains)
INFO - root - 2019-11-06 21:06:41.208888: step 58070, total loss = 0.20, predict loss = 0.06 (95.7 examples/sec; 0.042 sec/batch; 1h:04m:01s remains)
INFO - root - 2019-11-06 21:06:41.660472: step 58080, total loss = 0.34, predict loss = 0.09 (92.2 examples/sec; 0.043 sec/batch; 1h:06m:27s remains)
INFO - root - 2019-11-06 21:06:42.632292: step 58090, total loss = 0.17, predict loss = 0.04 (71.2 examples/sec; 0.056 sec/batch; 1h:26m:06s remains)
INFO - root - 2019-11-06 21:06:43.228428: step 58100, total loss = 0.16, predict loss = 0.04 (76.1 examples/sec; 0.053 sec/batch; 1h:20m:27s remains)
INFO - root - 2019-11-06 21:06:43.821878: step 58110, total loss = 0.19, predict loss = 0.05 (79.3 examples/sec; 0.050 sec/batch; 1h:17m:17s remains)
INFO - root - 2019-11-06 21:06:44.413843: step 58120, total loss = 0.16, predict loss = 0.04 (77.2 examples/sec; 0.052 sec/batch; 1h:19m:21s remains)
INFO - root - 2019-11-06 21:06:45.017869: step 58130, total loss = 0.20, predict loss = 0.05 (81.4 examples/sec; 0.049 sec/batch; 1h:15m:13s remains)
INFO - root - 2019-11-06 21:06:45.614097: step 58140, total loss = 0.22, predict loss = 0.06 (80.7 examples/sec; 0.050 sec/batch; 1h:15m:53s remains)
INFO - root - 2019-11-06 21:06:46.196327: step 58150, total loss = 0.18, predict loss = 0.05 (82.1 examples/sec; 0.049 sec/batch; 1h:14m:34s remains)
