INFO - bisenet-v2 - Running command 'main'
INFO - bisenet-v2 - Started run with ID "5"
INFO - root - nvidia-ml-py is not installed, automatically select gpu is disabled!
WARNING:tensorflow:From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - tensorflow - From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:53: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:53: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:65: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:65: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
INFO - root - preproces -- augment
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:73: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:73: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:74: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:74: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:131: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:131: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:164: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:164: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e431d5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e431d5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e431d5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e431d5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e519b978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e519b978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e519b978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e519b978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e49989e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e49989e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e49989e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e49989e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e4998ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e4998ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e4998ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e4998ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e4998ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e4998ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e4998ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e4998ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e4dbe828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e4dbe828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e4dbe828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96e4dbe828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4998860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f96e4998860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f96e4998860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f96e4998860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f96e4998860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e527bf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e527bf28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e527bf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e527bf28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51b52e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51b52e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51b52e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51b52e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4998630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4998630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4998630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4998630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e5311390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e5311390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e5311390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e5311390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51ba6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51ba6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51ba6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51ba6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51b5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51b5048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51b5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51b5048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51b0ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51b0ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51b0ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51b0ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51b50f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51b50f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51b50f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51b50f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51ba2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51ba2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51ba2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51ba2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e5130400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e5130400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e5130400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e5130400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51b50f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51b50f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51b50f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51b50f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4f70c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4f70c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4f70c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4f70c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51e4358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51e4358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51e4358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51e4358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e505d518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e505d518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e505d518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e505d518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51b5f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51b5f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51b5f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51b5f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4f70f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4f70f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4f70f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4f70f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4ef1518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4ef1518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4ef1518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4ef1518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4de56d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4de56d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4de56d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4de56d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4f3af28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4f3af28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4f3af28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4f3af28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4d71588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4d71588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4d71588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4d71588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4daaef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4daaef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4daaef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4daaef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4d71ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4d71ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4d71ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4d71ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4d11198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4d11198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4d11198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4d11198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4f70c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4f70c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4f70c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4f70c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51aaa58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51aaa58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51aaa58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51aaa58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4c9dcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4c9dcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4c9dcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4c9dcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4d11198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4d11198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4d11198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4d11198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4b85198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4b85198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4b85198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4b85198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4f70c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4f70c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4f70c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4f70c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4b85518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4b85518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4b85518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4b85518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4cf4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4cf4710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4cf4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4cf4710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4b85128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4b85128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4b85128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e4b85128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4cf4828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4cf4828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4cf4828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4cf4828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51baba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51baba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51baba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51baba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4d8fb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4d8fb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4d8fb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4d8fb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e53119e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e53119e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e53119e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e53119e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4ce1208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4ce1208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4ce1208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4ce1208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e491cda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e491cda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e491cda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e491cda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e482c978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e482c978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e482c978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e482c978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e48dddd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e48dddd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e48dddd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e48dddd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4c84c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4c84c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4c84c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4c84c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e476c240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e476c240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e476c240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e476c240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e48dd668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e48dd668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e48dd668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e48dd668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e476cb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e476cb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e476cb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e476cb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e48f4588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e48f4588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e48f4588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e48f4588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e46a4f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e46a4f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e46a4f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e46a4f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4b85ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4b85ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4b85ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4b85ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e47cffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e47cffd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e47cffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e47cffd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4750a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4750a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4750a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4750a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e45c9f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e45c9f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e45c9f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e45c9f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4750978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4750978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4750978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4750978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e45c9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e45c9e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e45c9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e45c9e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e47885f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e47885f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e47885f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e47885f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e465d208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e465d208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e465d208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e465d208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e465dc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e465dc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e465dc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e465dc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e445acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e445acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e445acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e445acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e462acc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e462acc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e462acc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e462acc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e455bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e455bfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e455bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e455bfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e45227b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e45227b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e45227b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e45227b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e41ef588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e41ef588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e41ef588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e41ef588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e445acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e445acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e445acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e445acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e465d7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e465d7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e465d7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e465d7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e41ef0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e41ef0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e41ef0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e41ef0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e465de80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e465de80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e465de80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e465de80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e42c8fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e42c8fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e42c8fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e42c8fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e414c470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e414c470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e414c470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e414c470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e410f518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e410f518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e410f518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e410f518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e40e34a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e40e34a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e40e34a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e40e34a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e42d1e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e42d1e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e42d1e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e42d1e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e41d1438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e41d1438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e41d1438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e41d1438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4096dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4096dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4096dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4096dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e40e3898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e40e3898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e40e3898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e40e3898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e414c518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e414c518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e414c518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e414c518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfe988d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfe988d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfe988d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfe988d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e5741b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e5741b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e5741b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e5741b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfe2bd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfe2bd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfe2bd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfe2bd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e5741b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e5741b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e5741b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e5741b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dff34668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dff34668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dff34668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dff34668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4998940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4998940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4998940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e4998940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dff34d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dff34d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dff34d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dff34d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfdb5c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfdb5c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfdb5c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfdb5c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e410fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e410fe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e410fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e410fe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfd61e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfd61e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfd61e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfd61e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e410fcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e410fcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e410fcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e410fcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfeb83c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfeb83c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfeb83c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfeb83c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfbd8320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfbd8320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfbd8320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfbd8320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e40e3b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e40e3b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e40e3b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e40e3b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfb8ab00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfb8ab00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfb8ab00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfb8ab00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfc4fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfc4fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfc4fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfc4fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfb7ccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfb7ccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfb7ccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfb7ccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfb8aa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfb8aa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfb8aa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfb8aa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfb03048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfb03048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfb03048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfb03048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfb65908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfb65908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfb65908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfb65908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfe2b160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfe2b160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfe2b160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dfe2b160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfd10c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfd10c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfd10c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfd10c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df8f5eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df8f5eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df8f5eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df8f5eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfc8e128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfc8e128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfc8e128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dfc8e128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df941cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df941cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df941cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df941cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96df8f5eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96df8f5eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96df8f5eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96df8f5eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df8702b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df8702b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df8702b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df8702b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96df915e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96df915e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96df915e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96df915e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df78a630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df78a630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df78a630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df78a630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df8c7710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df8c7710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df8c7710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df8c7710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df758eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df758eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df758eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df758eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:186: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:186: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df6c4208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df6c4208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df6c4208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df6c4208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df681a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df681a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df681a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df681a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df7f1080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df7f1080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df7f1080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df7f1080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df6b0e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df6b0e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df6b0e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df6b0e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df6b04a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df6b04a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df6b04a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df6b04a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df6605c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df6605c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df6605c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df6605c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df758c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df758c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df758c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df758c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dff9dbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dff9dbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dff9dbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dff9dbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df501d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df501d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df501d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df501d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df501ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df501ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df501ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df501ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df4bffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df4bffd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df4bffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df4bffd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df4d3fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df4d3fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df4d3fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df4d3fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df4d3320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df4d3320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df4d3320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df4d3320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df4bfe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df4bfe48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df4bfe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df4bfe48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df5fe710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df5fe710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df5fe710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df5fe710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df482da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df482da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df482da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df482da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df4d3320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df4d3320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df4d3320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df4d3320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df349c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df349c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df349c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df349c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df432eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df432eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df432eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df432eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df349c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df349c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df349c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df349c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df349da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df349da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df349da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df349da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df349da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df349da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df349da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df349da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df2b5518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df2b5518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df2b5518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df2b5518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df334a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df334a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df334a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96df334a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df334a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df334a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df334a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df334a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df208da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df208da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df208da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df208da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df349c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df349c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df349c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96df349c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:224: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:224: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:228: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:228: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:231: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:231: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:243: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:243: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:247: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:247: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - root - img_mean is not explicitly specified, using default value: None
INFO - root - preproces -- None
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded60eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded60eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded60eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded60eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded60ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded60ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded60ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded60ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded60d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded60d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded60d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded60d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee1b198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee1b198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee1b198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee1b198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96ded60f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee1b128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee1b128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee1b128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee1b128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f96df209400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f96df209400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f96df209400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f96df209400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96df152710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96df152710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96df152710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96df152710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee1b6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee1b6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee1b6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee1b6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee1b710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee1b710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee1b710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee1b710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee45978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee45978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee45978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee45978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee1bef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee1bef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee1bef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee1bef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee454e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee454e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee454e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee454e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee45ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee45ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee45ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee45ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def37828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee45978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee45978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee45978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee45978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deec1828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deec1828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deec1828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deec1828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def30e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def30e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def30e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def30e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deec1be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deec1be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deec1be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deec1be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51db278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51db278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51db278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51db278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deec16d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deec16d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deec16d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deec16d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee4a320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee4a320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee4a320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee4a320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51db240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51db240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51db240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96e51db240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee4a1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee4a1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee4a1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee4a1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee45b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee45b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee45b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee45b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee45588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee45588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee45588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee45588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def23630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def23630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def23630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def23630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee454e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee454e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee454e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee454e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deef2160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deef2160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deef2160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deef2160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51dba58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51dba58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51dba58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96e51dba58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deef2f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deef2f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deef2f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deef2f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deef2f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deef2f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deef2f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deef2f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee57630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee57630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee57630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee57630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deef2f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deef2f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deef2f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deef2f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deee0dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deee0dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deee0dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deee0dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deec7588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deec7588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deec7588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deec7588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deee0518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deee0518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deee0518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deee0518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee0080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee0080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee0080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee0080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deefbeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deefbeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deefbeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deefbeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee0dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee0dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee0dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee0dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee97668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee97668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee97668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dee97668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee04a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee04a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee04a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee04a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def3d748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def3d748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def3d748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def3d748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee0940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee0940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee0940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deee0940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def3d0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def3d0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def3d0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96def3d0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def37a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded174a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded174a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded174a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded174a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def3d0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def3d0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def3d0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96def3d0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deea8ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deea8ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deea8ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deea8ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded17ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded17ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded17ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded17ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee8deb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee8deb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee8deb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee8deb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded18b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee8d780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee8d780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee8d780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dee8d780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded17048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded17048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded17048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96ded17048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded20358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded20358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded20358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded20358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec80470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec80470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec80470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec80470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded17240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded17240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded17240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded17240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec803c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec803c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec803c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec803c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded09e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded09e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded09e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded09e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec80898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec80898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec80898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec80898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded26748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded26748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded26748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded26748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded26748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded26748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded26748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96ded26748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96decf04e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96decf04e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96decf04e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96decf04e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deca3550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deca3550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deca3550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deca3550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deced5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deced5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deced5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deced5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec41438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec41438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec41438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec41438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec41cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec41cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec41cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec41cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec40978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec40978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec40978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec40978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec41898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec41898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec41898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec41898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec65240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec65240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec65240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec65240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec40f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec40f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec40f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec40f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec40860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec40860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec40860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec40860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec86ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec400b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec400b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec400b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec400b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deca32e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deca32e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deca32e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deca32e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec865f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec865f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec865f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec865f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96debccfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96debccfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96debccfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96debccfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deca37f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deca37f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deca37f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deca37f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96debccc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96debccc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96debccc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96debccc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deca3d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deca3d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deca3d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96deca3d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec40860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec40860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec40860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec40860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec06588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec06588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec06588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec06588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec65828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec65828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec65828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec65828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec065f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec065f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec065f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec065f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec1bef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec1bef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec1bef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec1bef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec654a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec654a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec654a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec654a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec1b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec1b5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec1b5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec1b5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec400b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec400b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec400b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec400b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec57208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec57208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec57208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec57208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec2d8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec2d8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec2d8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec2d8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec57358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec57358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec57358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dec57358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec1b198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec1b198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec1b198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec1b198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb852e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb852e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb852e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb852e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec57320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec57320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec57320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f96dec57320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb85898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb85898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb85898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb85898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb85940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb85940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb85940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb85940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb58ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb58ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb58ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb58ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb85978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb85978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb85978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb85978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb58080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb58080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb58080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb58080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96debe4518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96debe4518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96debe4518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96debe4518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb58630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb58630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb58630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb58630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb58630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb58630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb58630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb58630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb4d908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb4d908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb4d908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb4d908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb9ef98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb9ef98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb9ef98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb9ef98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb4d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb4d400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb4d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deb4d400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb3fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb3fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb3fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb3fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deac7dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deac7dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deac7dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deac7dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deadaa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deadaa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deadaa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deadaa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dead6550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dead6550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dead6550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dead6550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deada2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deada2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deada2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deada2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dead6668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dead6668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dead6668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dead6668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dead62e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dead62e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dead62e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dead62e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deada7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deada7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deada7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deada7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb4d358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb4d358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb4d358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deb4d358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dea846d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dea846d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dea846d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96dea846d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deac7860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deac7860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deac7860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96deac7860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deac7860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deac7860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deac7860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deac7860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea84d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea84d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea84d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea84d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deaa52e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deaa52e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deaa52e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deaa52e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea9a1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea9a1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea9a1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea9a1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deaf8710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deaf8710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deaf8710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f96deaf8710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dff7eef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dff7eef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dff7eef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dff7eef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea46438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea46438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea46438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea46438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea98ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea98ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea98ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f96dea98ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From train.py:66: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From train.py:66: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING - tensorflow - From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING:tensorflow:From train.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING - tensorflow - From train.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING:tensorflow:From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING - tensorflow - From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING - tensorflow - From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING - tensorflow - From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

2019-11-03 23:46:17.667835: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-03 23:46:17.672921: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-11-03 23:46:17.766861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-03 23:46:17.767322: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f73476c700 executing computations on platform CUDA. Devices:
2019-11-03 23:46:17.767338: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-11-03 23:46:17.785854: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz
2019-11-03 23:46:17.786269: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f73482de40 executing computations on platform Host. Devices:
2019-11-03 23:46:17.786302: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-11-03 23:46:17.786502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-03 23:46:17.787006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-11-03 23:46:17.787261: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-03 23:46:17.788307: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-11-03 23:46:17.789242: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-11-03 23:46:17.789544: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-11-03 23:46:17.790688: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-11-03 23:46:17.791367: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-11-03 23:46:17.793353: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-11-03 23:46:17.793436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-03 23:46:17.793891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-03 23:46:17.794240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-11-03 23:46:17.794269: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-03 23:46:17.794891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-03 23:46:17.794903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-11-03 23:46:17.794908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-11-03 23:46:17.795115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-03 23:46:17.795574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-03 23:46:17.795967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6951 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
INFO - root - Restore from last checkpoint: Logs/bisenet/checkpoints/bisenet-v2/model.ckpt-30000
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from Logs/bisenet/checkpoints/bisenet-v2/model.ckpt-30000
INFO - tensorflow - Restoring parameters from Logs/bisenet/checkpoints/bisenet-v2/model.ckpt-30000
WARNING:tensorflow:From train.py:161: The name tf.train.global_step is deprecated. Please use tf.compat.v1.train.global_step instead.

WARNING - tensorflow - From train.py:161: The name tf.train.global_step is deprecated. Please use tf.compat.v1.train.global_step instead.

INFO - root - Train for 6000000 steps
2019-11-03 23:46:24.473010: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
INFO - root - 2019-11-03 23:46:26.435479: step 30010, total loss = 0.54, predict loss = 0.12 (83.6 examples/sec; 0.048 sec/batch; 79h:18m:15s remains)
2019-11-03 23:46:27.970460: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
INFO - root - 2019-11-03 23:46:29.460205: step 30020, total loss = 0.53, predict loss = 0.13 (79.7 examples/sec; 0.050 sec/batch; 83h:16m:33s remains)
INFO - root - 2019-11-03 23:46:30.097049: step 30030, total loss = 0.52, predict loss = 0.11 (60.3 examples/sec; 0.066 sec/batch; 110h:05m:30s remains)
INFO - root - 2019-11-03 23:46:30.741183: step 30040, total loss = 0.80, predict loss = 0.19 (77.9 examples/sec; 0.051 sec/batch; 85h:07m:32s remains)
INFO - root - 2019-11-03 23:46:31.360540: step 30050, total loss = 0.64, predict loss = 0.15 (73.3 examples/sec; 0.055 sec/batch; 90h:32m:23s remains)
INFO - root - 2019-11-03 23:46:31.980761: step 30060, total loss = 0.67, predict loss = 0.15 (74.5 examples/sec; 0.054 sec/batch; 89h:01m:41s remains)
INFO - root - 2019-11-03 23:46:32.628591: step 30070, total loss = 0.47, predict loss = 0.11 (60.8 examples/sec; 0.066 sec/batch; 109h:03m:21s remains)
INFO - root - 2019-11-03 23:46:33.315522: step 30080, total loss = 0.55, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 96h:34m:23s remains)
INFO - root - 2019-11-03 23:46:33.956106: step 30090, total loss = 0.59, predict loss = 0.13 (74.5 examples/sec; 0.054 sec/batch; 88h:59m:14s remains)
INFO - root - 2019-11-03 23:46:34.621962: step 30100, total loss = 0.98, predict loss = 0.24 (60.9 examples/sec; 0.066 sec/batch; 108h:58m:35s remains)
INFO - root - 2019-11-03 23:46:35.256309: step 30110, total loss = 1.16, predict loss = 0.25 (78.6 examples/sec; 0.051 sec/batch; 84h:24m:57s remains)
INFO - root - 2019-11-03 23:46:35.874422: step 30120, total loss = 0.82, predict loss = 0.18 (80.1 examples/sec; 0.050 sec/batch; 82h:50m:50s remains)
INFO - root - 2019-11-03 23:46:36.499727: step 30130, total loss = 1.06, predict loss = 0.26 (64.0 examples/sec; 0.062 sec/batch; 103h:34m:29s remains)
INFO - root - 2019-11-03 23:46:37.133260: step 30140, total loss = 0.66, predict loss = 0.15 (68.2 examples/sec; 0.059 sec/batch; 97h:16m:53s remains)
INFO - root - 2019-11-03 23:46:37.788123: step 30150, total loss = 0.71, predict loss = 0.16 (66.2 examples/sec; 0.060 sec/batch; 100h:14m:53s remains)
INFO - root - 2019-11-03 23:46:38.398941: step 30160, total loss = 0.88, predict loss = 0.21 (83.0 examples/sec; 0.048 sec/batch; 79h:55m:22s remains)
INFO - root - 2019-11-03 23:46:39.062099: step 30170, total loss = 0.67, predict loss = 0.15 (80.5 examples/sec; 0.050 sec/batch; 82h:22m:04s remains)
INFO - root - 2019-11-03 23:46:39.712707: step 30180, total loss = 0.58, predict loss = 0.12 (54.5 examples/sec; 0.073 sec/batch; 121h:45m:55s remains)
INFO - root - 2019-11-03 23:46:40.359032: step 30190, total loss = 0.69, predict loss = 0.15 (67.8 examples/sec; 0.059 sec/batch; 97h:46m:12s remains)
INFO - root - 2019-11-03 23:46:40.977713: step 30200, total loss = 0.60, predict loss = 0.13 (67.2 examples/sec; 0.060 sec/batch; 98h:41m:12s remains)
INFO - root - 2019-11-03 23:46:41.612190: step 30210, total loss = 0.56, predict loss = 0.13 (71.9 examples/sec; 0.056 sec/batch; 92h:12m:07s remains)
INFO - root - 2019-11-03 23:46:42.260763: step 30220, total loss = 0.71, predict loss = 0.16 (65.7 examples/sec; 0.061 sec/batch; 101h:00m:40s remains)
INFO - root - 2019-11-03 23:46:42.936383: step 30230, total loss = 0.66, predict loss = 0.16 (71.7 examples/sec; 0.056 sec/batch; 92h:28m:06s remains)
INFO - root - 2019-11-03 23:46:43.595356: step 30240, total loss = 0.55, predict loss = 0.13 (75.0 examples/sec; 0.053 sec/batch; 88h:24m:05s remains)
INFO - root - 2019-11-03 23:46:44.269887: step 30250, total loss = 0.74, predict loss = 0.16 (70.1 examples/sec; 0.057 sec/batch; 94h:36m:55s remains)
INFO - root - 2019-11-03 23:46:44.915728: step 30260, total loss = 0.70, predict loss = 0.15 (69.7 examples/sec; 0.057 sec/batch; 95h:07m:12s remains)
INFO - root - 2019-11-03 23:46:45.525149: step 30270, total loss = 0.94, predict loss = 0.22 (71.1 examples/sec; 0.056 sec/batch; 93h:16m:03s remains)
INFO - root - 2019-11-03 23:46:46.145098: step 30280, total loss = 0.67, predict loss = 0.15 (66.1 examples/sec; 0.061 sec/batch; 100h:24m:42s remains)
INFO - root - 2019-11-03 23:46:46.807799: step 30290, total loss = 0.91, predict loss = 0.21 (64.1 examples/sec; 0.062 sec/batch; 103h:27m:44s remains)
INFO - root - 2019-11-03 23:46:47.452872: step 30300, total loss = 0.79, predict loss = 0.19 (63.4 examples/sec; 0.063 sec/batch; 104h:39m:04s remains)
INFO - root - 2019-11-03 23:46:48.177000: step 30310, total loss = 0.74, predict loss = 0.17 (61.7 examples/sec; 0.065 sec/batch; 107h:26m:29s remains)
INFO - root - 2019-11-03 23:46:48.839295: step 30320, total loss = 0.70, predict loss = 0.16 (75.3 examples/sec; 0.053 sec/batch; 88h:04m:10s remains)
INFO - root - 2019-11-03 23:46:49.481591: step 30330, total loss = 0.64, predict loss = 0.14 (67.8 examples/sec; 0.059 sec/batch; 97h:53m:56s remains)
INFO - root - 2019-11-03 23:46:50.173019: step 30340, total loss = 0.88, predict loss = 0.19 (71.5 examples/sec; 0.056 sec/batch; 92h:44m:59s remains)
INFO - root - 2019-11-03 23:46:50.986439: step 30350, total loss = 0.76, predict loss = 0.18 (56.5 examples/sec; 0.071 sec/batch; 117h:19m:54s remains)
INFO - root - 2019-11-03 23:46:51.681937: step 30360, total loss = 0.69, predict loss = 0.15 (78.9 examples/sec; 0.051 sec/batch; 84h:04m:56s remains)
INFO - root - 2019-11-03 23:46:52.331971: step 30370, total loss = 0.80, predict loss = 0.19 (73.0 examples/sec; 0.055 sec/batch; 90h:55m:20s remains)
INFO - root - 2019-11-03 23:46:52.935402: step 30380, total loss = 0.56, predict loss = 0.11 (71.3 examples/sec; 0.056 sec/batch; 92h:59m:07s remains)
INFO - root - 2019-11-03 23:46:53.582586: step 30390, total loss = 0.76, predict loss = 0.17 (69.8 examples/sec; 0.057 sec/batch; 94h:57m:45s remains)
INFO - root - 2019-11-03 23:46:54.246015: step 30400, total loss = 0.79, predict loss = 0.18 (65.8 examples/sec; 0.061 sec/batch; 100h:44m:53s remains)
INFO - root - 2019-11-03 23:46:54.902412: step 30410, total loss = 2.32, predict loss = 0.62 (68.3 examples/sec; 0.059 sec/batch; 97h:02m:35s remains)
INFO - root - 2019-11-03 23:46:55.532949: step 30420, total loss = 1.02, predict loss = 0.22 (65.3 examples/sec; 0.061 sec/batch; 101h:31m:45s remains)
INFO - root - 2019-11-03 23:46:56.201286: step 30430, total loss = 0.93, predict loss = 0.20 (63.5 examples/sec; 0.063 sec/batch; 104h:28m:43s remains)
INFO - root - 2019-11-03 23:46:56.825892: step 30440, total loss = 0.79, predict loss = 0.17 (78.2 examples/sec; 0.051 sec/batch; 84h:50m:40s remains)
INFO - root - 2019-11-03 23:46:57.445357: step 30450, total loss = 0.80, predict loss = 0.19 (72.7 examples/sec; 0.055 sec/batch; 91h:17m:17s remains)
INFO - root - 2019-11-03 23:46:58.061485: step 30460, total loss = 0.81, predict loss = 0.20 (71.3 examples/sec; 0.056 sec/batch; 92h:59m:08s remains)
INFO - root - 2019-11-03 23:46:58.657733: step 30470, total loss = 0.75, predict loss = 0.16 (69.7 examples/sec; 0.057 sec/batch; 95h:13m:16s remains)
INFO - root - 2019-11-03 23:46:59.265997: step 30480, total loss = 0.80, predict loss = 0.20 (66.4 examples/sec; 0.060 sec/batch; 99h:50m:56s remains)
INFO - root - 2019-11-03 23:46:59.869323: step 30490, total loss = 0.81, predict loss = 0.21 (71.3 examples/sec; 0.056 sec/batch; 93h:03m:07s remains)
INFO - root - 2019-11-03 23:47:00.500266: step 30500, total loss = 0.55, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 93h:53m:56s remains)
INFO - root - 2019-11-03 23:47:01.143872: step 30510, total loss = 0.67, predict loss = 0.16 (72.3 examples/sec; 0.055 sec/batch; 91h:41m:59s remains)
INFO - root - 2019-11-03 23:47:01.796286: step 30520, total loss = 0.84, predict loss = 0.20 (62.4 examples/sec; 0.064 sec/batch; 106h:13m:58s remains)
INFO - root - 2019-11-03 23:47:02.472575: step 30530, total loss = 0.72, predict loss = 0.16 (61.4 examples/sec; 0.065 sec/batch; 108h:05m:26s remains)
INFO - root - 2019-11-03 23:47:03.133654: step 30540, total loss = 0.83, predict loss = 0.19 (63.0 examples/sec; 0.063 sec/batch; 105h:13m:34s remains)
INFO - root - 2019-11-03 23:47:03.745490: step 30550, total loss = 0.59, predict loss = 0.14 (78.3 examples/sec; 0.051 sec/batch; 84h:41m:23s remains)
INFO - root - 2019-11-03 23:47:04.396711: step 30560, total loss = 0.48, predict loss = 0.10 (60.1 examples/sec; 0.067 sec/batch; 110h:17m:20s remains)
INFO - root - 2019-11-03 23:47:05.001806: step 30570, total loss = 0.67, predict loss = 0.16 (68.4 examples/sec; 0.058 sec/batch; 96h:56m:22s remains)
INFO - root - 2019-11-03 23:47:05.635611: step 30580, total loss = 0.61, predict loss = 0.14 (78.1 examples/sec; 0.051 sec/batch; 84h:55m:22s remains)
INFO - root - 2019-11-03 23:47:06.284780: step 30590, total loss = 0.51, predict loss = 0.12 (67.7 examples/sec; 0.059 sec/batch; 97h:58m:26s remains)
INFO - root - 2019-11-03 23:47:06.906526: step 30600, total loss = 0.48, predict loss = 0.12 (72.8 examples/sec; 0.055 sec/batch; 91h:07m:12s remains)
INFO - root - 2019-11-03 23:47:07.551191: step 30610, total loss = 0.69, predict loss = 0.15 (73.8 examples/sec; 0.054 sec/batch; 89h:49m:42s remains)
INFO - root - 2019-11-03 23:47:08.236991: step 30620, total loss = 0.46, predict loss = 0.11 (67.9 examples/sec; 0.059 sec/batch; 97h:39m:08s remains)
INFO - root - 2019-11-03 23:47:08.838668: step 30630, total loss = 0.57, predict loss = 0.13 (79.7 examples/sec; 0.050 sec/batch; 83h:15m:59s remains)
INFO - root - 2019-11-03 23:47:09.451484: step 30640, total loss = 0.59, predict loss = 0.13 (66.7 examples/sec; 0.060 sec/batch; 99h:28m:07s remains)
INFO - root - 2019-11-03 23:47:10.094974: step 30650, total loss = 0.72, predict loss = 0.15 (70.7 examples/sec; 0.057 sec/batch; 93h:44m:55s remains)
INFO - root - 2019-11-03 23:47:10.710953: step 30660, total loss = 0.50, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 87h:51m:16s remains)
INFO - root - 2019-11-03 23:47:11.335374: step 30670, total loss = 0.56, predict loss = 0.12 (59.3 examples/sec; 0.067 sec/batch; 111h:51m:25s remains)
INFO - root - 2019-11-03 23:47:11.995539: step 30680, total loss = 0.58, predict loss = 0.13 (70.4 examples/sec; 0.057 sec/batch; 94h:12m:10s remains)
INFO - root - 2019-11-03 23:47:12.657641: step 30690, total loss = 0.72, predict loss = 0.17 (68.5 examples/sec; 0.058 sec/batch; 96h:48m:36s remains)
INFO - root - 2019-11-03 23:47:13.322575: step 30700, total loss = 0.77, predict loss = 0.18 (58.1 examples/sec; 0.069 sec/batch; 114h:13m:08s remains)
INFO - root - 2019-11-03 23:47:13.988999: step 30710, total loss = 0.73, predict loss = 0.18 (72.4 examples/sec; 0.055 sec/batch; 91h:38m:52s remains)
INFO - root - 2019-11-03 23:47:14.622000: step 30720, total loss = 0.74, predict loss = 0.17 (69.7 examples/sec; 0.057 sec/batch; 95h:11m:52s remains)
INFO - root - 2019-11-03 23:47:15.228368: step 30730, total loss = 0.99, predict loss = 0.25 (76.7 examples/sec; 0.052 sec/batch; 86h:28m:57s remains)
INFO - root - 2019-11-03 23:47:15.823611: step 30740, total loss = 0.62, predict loss = 0.14 (82.1 examples/sec; 0.049 sec/batch; 80h:44m:27s remains)
INFO - root - 2019-11-03 23:47:16.459672: step 30750, total loss = 0.86, predict loss = 0.21 (68.4 examples/sec; 0.059 sec/batch; 97h:02m:05s remains)
INFO - root - 2019-11-03 23:47:17.084602: step 30760, total loss = 0.77, predict loss = 0.20 (68.4 examples/sec; 0.059 sec/batch; 97h:00m:49s remains)
INFO - root - 2019-11-03 23:47:17.724188: step 30770, total loss = 0.68, predict loss = 0.16 (62.7 examples/sec; 0.064 sec/batch; 105h:49m:59s remains)
INFO - root - 2019-11-03 23:47:18.363844: step 30780, total loss = 0.69, predict loss = 0.17 (71.3 examples/sec; 0.056 sec/batch; 93h:00m:49s remains)
INFO - root - 2019-11-03 23:47:19.026094: step 30790, total loss = 0.68, predict loss = 0.13 (70.9 examples/sec; 0.056 sec/batch; 93h:34m:35s remains)
INFO - root - 2019-11-03 23:47:19.687474: step 30800, total loss = 0.70, predict loss = 0.18 (72.7 examples/sec; 0.055 sec/batch; 91h:10m:11s remains)
INFO - root - 2019-11-03 23:47:20.356592: step 30810, total loss = 0.45, predict loss = 0.10 (64.7 examples/sec; 0.062 sec/batch; 102h:32m:32s remains)
INFO - root - 2019-11-03 23:47:21.003990: step 30820, total loss = 0.47, predict loss = 0.10 (64.3 examples/sec; 0.062 sec/batch; 103h:05m:52s remains)
INFO - root - 2019-11-03 23:47:21.704120: step 30830, total loss = 0.46, predict loss = 0.10 (73.5 examples/sec; 0.054 sec/batch; 90h:11m:24s remains)
INFO - root - 2019-11-03 23:47:22.339131: step 30840, total loss = 0.37, predict loss = 0.08 (71.1 examples/sec; 0.056 sec/batch; 93h:15m:08s remains)
INFO - root - 2019-11-03 23:47:22.972709: step 30850, total loss = 0.53, predict loss = 0.13 (68.9 examples/sec; 0.058 sec/batch; 96h:12m:02s remains)
INFO - root - 2019-11-03 23:47:23.579975: step 30860, total loss = 0.59, predict loss = 0.13 (65.8 examples/sec; 0.061 sec/batch; 100h:45m:17s remains)
INFO - root - 2019-11-03 23:47:24.203319: step 30870, total loss = 0.32, predict loss = 0.07 (68.2 examples/sec; 0.059 sec/batch; 97h:12m:59s remains)
INFO - root - 2019-11-03 23:47:24.830295: step 30880, total loss = 0.32, predict loss = 0.07 (74.3 examples/sec; 0.054 sec/batch; 89h:13m:56s remains)
INFO - root - 2019-11-03 23:47:25.470833: step 30890, total loss = 0.48, predict loss = 0.10 (67.1 examples/sec; 0.060 sec/batch; 98h:53m:44s remains)
INFO - root - 2019-11-03 23:47:26.136406: step 30900, total loss = 0.60, predict loss = 0.13 (58.4 examples/sec; 0.068 sec/batch; 113h:33m:58s remains)
INFO - root - 2019-11-03 23:47:26.799592: step 30910, total loss = 0.45, predict loss = 0.10 (71.9 examples/sec; 0.056 sec/batch; 92h:15m:08s remains)
INFO - root - 2019-11-03 23:47:27.453489: step 30920, total loss = 0.45, predict loss = 0.10 (76.6 examples/sec; 0.052 sec/batch; 86h:38m:13s remains)
INFO - root - 2019-11-03 23:47:28.072778: step 30930, total loss = 0.60, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 91h:50m:41s remains)
INFO - root - 2019-11-03 23:47:28.661376: step 30940, total loss = 0.65, predict loss = 0.15 (70.9 examples/sec; 0.056 sec/batch; 93h:35m:34s remains)
INFO - root - 2019-11-03 23:47:29.328494: step 30950, total loss = 0.56, predict loss = 0.13 (60.7 examples/sec; 0.066 sec/batch; 109h:17m:16s remains)
INFO - root - 2019-11-03 23:47:29.943112: step 30960, total loss = 0.51, predict loss = 0.12 (80.3 examples/sec; 0.050 sec/batch; 82h:34m:29s remains)
INFO - root - 2019-11-03 23:47:30.573159: step 30970, total loss = 0.63, predict loss = 0.14 (73.5 examples/sec; 0.054 sec/batch; 90h:16m:16s remains)
INFO - root - 2019-11-03 23:47:31.238960: step 30980, total loss = 0.63, predict loss = 0.14 (70.5 examples/sec; 0.057 sec/batch; 94h:04m:58s remains)
INFO - root - 2019-11-03 23:47:31.888822: step 30990, total loss = 0.54, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 100h:12m:08s remains)
INFO - root - 2019-11-03 23:47:32.513212: step 31000, total loss = 0.51, predict loss = 0.11 (72.0 examples/sec; 0.056 sec/batch; 92h:03m:59s remains)
INFO - root - 2019-11-03 23:47:33.164212: step 31010, total loss = 0.68, predict loss = 0.16 (70.1 examples/sec; 0.057 sec/batch; 94h:34m:19s remains)
INFO - root - 2019-11-03 23:47:33.836382: step 31020, total loss = 0.88, predict loss = 0.20 (75.2 examples/sec; 0.053 sec/batch; 88h:12m:36s remains)
INFO - root - 2019-11-03 23:47:34.480179: step 31030, total loss = 0.72, predict loss = 0.18 (67.0 examples/sec; 0.060 sec/batch; 98h:55m:05s remains)
INFO - root - 2019-11-03 23:47:35.117661: step 31040, total loss = 0.69, predict loss = 0.16 (75.1 examples/sec; 0.053 sec/batch; 88h:16m:23s remains)
INFO - root - 2019-11-03 23:47:35.752760: step 31050, total loss = 0.48, predict loss = 0.11 (64.1 examples/sec; 0.062 sec/batch; 103h:31m:15s remains)
INFO - root - 2019-11-03 23:47:36.397912: step 31060, total loss = 0.46, predict loss = 0.11 (61.9 examples/sec; 0.065 sec/batch; 107h:07m:31s remains)
INFO - root - 2019-11-03 23:47:37.079326: step 31070, total loss = 0.68, predict loss = 0.17 (62.8 examples/sec; 0.064 sec/batch; 105h:38m:48s remains)
INFO - root - 2019-11-03 23:47:37.743322: step 31080, total loss = 0.60, predict loss = 0.14 (70.8 examples/sec; 0.056 sec/batch; 93h:38m:41s remains)
INFO - root - 2019-11-03 23:47:38.375721: step 31090, total loss = 0.23, predict loss = 0.04 (69.9 examples/sec; 0.057 sec/batch; 94h:54m:04s remains)
INFO - root - 2019-11-03 23:47:38.963021: step 31100, total loss = 0.39, predict loss = 0.09 (71.0 examples/sec; 0.056 sec/batch; 93h:21m:07s remains)
INFO - root - 2019-11-03 23:47:39.557269: step 31110, total loss = 0.43, predict loss = 0.09 (72.5 examples/sec; 0.055 sec/batch; 91h:31m:01s remains)
INFO - root - 2019-11-03 23:47:40.160914: step 31120, total loss = 0.55, predict loss = 0.13 (78.0 examples/sec; 0.051 sec/batch; 85h:01m:35s remains)
INFO - root - 2019-11-03 23:47:40.768185: step 31130, total loss = 0.52, predict loss = 0.12 (78.4 examples/sec; 0.051 sec/batch; 84h:33m:33s remains)
INFO - root - 2019-11-03 23:47:41.394924: step 31140, total loss = 0.50, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 93h:54m:37s remains)
INFO - root - 2019-11-03 23:47:42.087809: step 31150, total loss = 0.58, predict loss = 0.14 (66.2 examples/sec; 0.060 sec/batch; 100h:13m:00s remains)
INFO - root - 2019-11-03 23:47:42.742327: step 31160, total loss = 0.55, predict loss = 0.12 (62.6 examples/sec; 0.064 sec/batch; 105h:56m:06s remains)
INFO - root - 2019-11-03 23:47:43.389259: step 31170, total loss = 0.71, predict loss = 0.18 (75.3 examples/sec; 0.053 sec/batch; 88h:05m:21s remains)
INFO - root - 2019-11-03 23:47:43.999288: step 31180, total loss = 0.57, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 95h:17m:08s remains)
INFO - root - 2019-11-03 23:47:44.604251: step 31190, total loss = 0.62, predict loss = 0.14 (82.3 examples/sec; 0.049 sec/batch; 80h:37m:40s remains)
INFO - root - 2019-11-03 23:47:45.204574: step 31200, total loss = 0.72, predict loss = 0.16 (77.5 examples/sec; 0.052 sec/batch; 85h:31m:11s remains)
INFO - root - 2019-11-03 23:47:45.817156: step 31210, total loss = 0.48, predict loss = 0.10 (70.4 examples/sec; 0.057 sec/batch; 94h:09m:15s remains)
INFO - root - 2019-11-03 23:47:46.456133: step 31220, total loss = 0.51, predict loss = 0.11 (71.2 examples/sec; 0.056 sec/batch; 93h:11m:14s remains)
INFO - root - 2019-11-03 23:47:47.136053: step 31230, total loss = 0.50, predict loss = 0.11 (75.6 examples/sec; 0.053 sec/batch; 87h:43m:33s remains)
INFO - root - 2019-11-03 23:47:47.768761: step 31240, total loss = 0.82, predict loss = 0.20 (73.9 examples/sec; 0.054 sec/batch; 89h:46m:17s remains)
INFO - root - 2019-11-03 23:47:48.393404: step 31250, total loss = 0.76, predict loss = 0.19 (74.7 examples/sec; 0.054 sec/batch; 88h:47m:56s remains)
INFO - root - 2019-11-03 23:47:49.047341: step 31260, total loss = 1.05, predict loss = 0.29 (65.2 examples/sec; 0.061 sec/batch; 101h:46m:30s remains)
INFO - root - 2019-11-03 23:47:49.699048: step 31270, total loss = 0.54, predict loss = 0.12 (73.4 examples/sec; 0.055 sec/batch; 90h:24m:31s remains)
INFO - root - 2019-11-03 23:47:50.327484: step 31280, total loss = 0.68, predict loss = 0.15 (69.7 examples/sec; 0.057 sec/batch; 95h:05m:11s remains)
INFO - root - 2019-11-03 23:47:50.963185: step 31290, total loss = 0.57, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 94h:02m:32s remains)
INFO - root - 2019-11-03 23:47:51.569754: step 31300, total loss = 0.50, predict loss = 0.12 (64.4 examples/sec; 0.062 sec/batch; 103h:02m:19s remains)
INFO - root - 2019-11-03 23:47:52.218054: step 31310, total loss = 0.72, predict loss = 0.17 (70.0 examples/sec; 0.057 sec/batch; 94h:42m:26s remains)
INFO - root - 2019-11-03 23:47:52.871030: step 31320, total loss = 0.50, predict loss = 0.11 (77.0 examples/sec; 0.052 sec/batch; 86h:10m:40s remains)
INFO - root - 2019-11-03 23:47:53.547439: step 31330, total loss = 0.46, predict loss = 0.10 (75.4 examples/sec; 0.053 sec/batch; 87h:54m:47s remains)
INFO - root - 2019-11-03 23:47:54.209835: step 31340, total loss = 0.57, predict loss = 0.13 (61.8 examples/sec; 0.065 sec/batch; 107h:14m:47s remains)
INFO - root - 2019-11-03 23:47:54.846366: step 31350, total loss = 0.40, predict loss = 0.09 (70.3 examples/sec; 0.057 sec/batch; 94h:20m:10s remains)
INFO - root - 2019-11-03 23:47:55.535004: step 31360, total loss = 0.63, predict loss = 0.14 (73.3 examples/sec; 0.055 sec/batch; 90h:28m:41s remains)
INFO - root - 2019-11-03 23:47:56.208908: step 31370, total loss = 0.70, predict loss = 0.16 (73.1 examples/sec; 0.055 sec/batch; 90h:41m:16s remains)
INFO - root - 2019-11-03 23:47:56.833610: step 31380, total loss = 0.65, predict loss = 0.15 (74.1 examples/sec; 0.054 sec/batch; 89h:27m:47s remains)
INFO - root - 2019-11-03 23:47:57.444172: step 31390, total loss = 0.57, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 94h:06m:41s remains)
INFO - root - 2019-11-03 23:47:58.058445: step 31400, total loss = 0.45, predict loss = 0.10 (64.5 examples/sec; 0.062 sec/batch; 102h:51m:41s remains)
INFO - root - 2019-11-03 23:47:58.703723: step 31410, total loss = 0.65, predict loss = 0.16 (71.6 examples/sec; 0.056 sec/batch; 92h:35m:00s remains)
INFO - root - 2019-11-03 23:47:59.368713: step 31420, total loss = 0.79, predict loss = 0.19 (67.0 examples/sec; 0.060 sec/batch; 99h:02m:50s remains)
INFO - root - 2019-11-03 23:48:00.014735: step 31430, total loss = 0.77, predict loss = 0.18 (64.5 examples/sec; 0.062 sec/batch; 102h:53m:07s remains)
INFO - root - 2019-11-03 23:48:00.685750: step 31440, total loss = 0.77, predict loss = 0.18 (70.5 examples/sec; 0.057 sec/batch; 94h:03m:59s remains)
INFO - root - 2019-11-03 23:48:01.332326: step 31450, total loss = 1.28, predict loss = 0.28 (78.3 examples/sec; 0.051 sec/batch; 84h:39m:26s remains)
INFO - root - 2019-11-03 23:48:01.930505: step 31460, total loss = 1.06, predict loss = 0.25 (79.5 examples/sec; 0.050 sec/batch; 83h:22m:44s remains)
INFO - root - 2019-11-03 23:48:02.553909: step 31470, total loss = 1.01, predict loss = 0.24 (80.3 examples/sec; 0.050 sec/batch; 82h:36m:00s remains)
INFO - root - 2019-11-03 23:48:03.204039: step 31480, total loss = 1.10, predict loss = 0.26 (78.6 examples/sec; 0.051 sec/batch; 84h:20m:41s remains)
INFO - root - 2019-11-03 23:48:03.853592: step 31490, total loss = 0.79, predict loss = 0.18 (76.5 examples/sec; 0.052 sec/batch; 86h:42m:03s remains)
INFO - root - 2019-11-03 23:48:04.524483: step 31500, total loss = 0.92, predict loss = 0.22 (63.1 examples/sec; 0.063 sec/batch; 105h:07m:24s remains)
INFO - root - 2019-11-03 23:48:05.171760: step 31510, total loss = 0.68, predict loss = 0.16 (82.5 examples/sec; 0.049 sec/batch; 80h:24m:52s remains)
INFO - root - 2019-11-03 23:48:05.821366: step 31520, total loss = 0.74, predict loss = 0.17 (66.7 examples/sec; 0.060 sec/batch; 99h:27m:40s remains)
INFO - root - 2019-11-03 23:48:06.497260: step 31530, total loss = 0.77, predict loss = 0.19 (61.8 examples/sec; 0.065 sec/batch; 107h:21m:10s remains)
INFO - root - 2019-11-03 23:48:07.132165: step 31540, total loss = 0.68, predict loss = 0.17 (79.6 examples/sec; 0.050 sec/batch; 83h:20m:25s remains)
INFO - root - 2019-11-03 23:48:07.751768: step 31550, total loss = 0.78, predict loss = 0.18 (73.0 examples/sec; 0.055 sec/batch; 90h:50m:58s remains)
INFO - root - 2019-11-03 23:48:08.404147: step 31560, total loss = 0.61, predict loss = 0.14 (65.2 examples/sec; 0.061 sec/batch; 101h:39m:13s remains)
INFO - root - 2019-11-03 23:48:09.034052: step 31570, total loss = 0.98, predict loss = 0.22 (72.0 examples/sec; 0.056 sec/batch; 92h:06m:52s remains)
INFO - root - 2019-11-03 23:48:09.621298: step 31580, total loss = 0.68, predict loss = 0.16 (79.6 examples/sec; 0.050 sec/batch; 83h:16m:38s remains)
INFO - root - 2019-11-03 23:48:10.234431: step 31590, total loss = 0.88, predict loss = 0.21 (73.8 examples/sec; 0.054 sec/batch; 89h:49m:10s remains)
INFO - root - 2019-11-03 23:48:10.895440: step 31600, total loss = 0.56, predict loss = 0.13 (63.4 examples/sec; 0.063 sec/batch; 104h:39m:33s remains)
INFO - root - 2019-11-03 23:48:11.574804: step 31610, total loss = 0.52, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 96h:58m:27s remains)
INFO - root - 2019-11-03 23:48:12.200459: step 31620, total loss = 0.62, predict loss = 0.13 (83.5 examples/sec; 0.048 sec/batch; 79h:27m:28s remains)
INFO - root - 2019-11-03 23:48:12.800197: step 31630, total loss = 0.56, predict loss = 0.13 (85.2 examples/sec; 0.047 sec/batch; 77h:51m:45s remains)
INFO - root - 2019-11-03 23:48:13.407932: step 31640, total loss = 0.54, predict loss = 0.12 (74.4 examples/sec; 0.054 sec/batch; 89h:05m:44s remains)
INFO - root - 2019-11-03 23:48:14.048301: step 31650, total loss = 0.71, predict loss = 0.16 (75.9 examples/sec; 0.053 sec/batch; 87h:23m:31s remains)
INFO - root - 2019-11-03 23:48:14.710748: step 31660, total loss = 0.50, predict loss = 0.12 (62.3 examples/sec; 0.064 sec/batch; 106h:28m:19s remains)
INFO - root - 2019-11-03 23:48:15.334923: step 31670, total loss = 0.43, predict loss = 0.09 (75.1 examples/sec; 0.053 sec/batch; 88h:19m:09s remains)
INFO - root - 2019-11-03 23:48:15.986963: step 31680, total loss = 0.47, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 98h:19m:15s remains)
INFO - root - 2019-11-03 23:48:16.670479: step 31690, total loss = 0.64, predict loss = 0.11 (61.6 examples/sec; 0.065 sec/batch; 107h:43m:38s remains)
INFO - root - 2019-11-03 23:48:17.280126: step 31700, total loss = 0.55, predict loss = 0.12 (83.3 examples/sec; 0.048 sec/batch; 79h:37m:58s remains)
INFO - root - 2019-11-03 23:48:17.897394: step 31710, total loss = 0.42, predict loss = 0.09 (77.4 examples/sec; 0.052 sec/batch; 85h:37m:21s remains)
INFO - root - 2019-11-03 23:48:18.524367: step 31720, total loss = 0.65, predict loss = 0.16 (71.4 examples/sec; 0.056 sec/batch; 92h:55m:31s remains)
INFO - root - 2019-11-03 23:48:19.162207: step 31730, total loss = 0.53, predict loss = 0.11 (59.0 examples/sec; 0.068 sec/batch; 112h:27m:42s remains)
INFO - root - 2019-11-03 23:48:19.914300: step 31740, total loss = 0.48, predict loss = 0.10 (61.4 examples/sec; 0.065 sec/batch; 108h:01m:13s remains)
INFO - root - 2019-11-03 23:48:20.556338: step 31750, total loss = 0.56, predict loss = 0.13 (66.0 examples/sec; 0.061 sec/batch; 100h:25m:15s remains)
INFO - root - 2019-11-03 23:48:21.218601: step 31760, total loss = 0.54, predict loss = 0.13 (63.7 examples/sec; 0.063 sec/batch; 104h:08m:09s remains)
INFO - root - 2019-11-03 23:48:21.901071: step 31770, total loss = 0.62, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 96h:34m:36s remains)
INFO - root - 2019-11-03 23:48:22.557663: step 31780, total loss = 0.49, predict loss = 0.11 (72.0 examples/sec; 0.056 sec/batch; 92h:05m:33s remains)
INFO - root - 2019-11-03 23:48:23.180574: step 31790, total loss = 0.49, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 86h:24m:51s remains)
INFO - root - 2019-11-03 23:48:23.833779: step 31800, total loss = 0.65, predict loss = 0.15 (72.9 examples/sec; 0.055 sec/batch; 91h:00m:43s remains)
INFO - root - 2019-11-03 23:48:24.504077: step 31810, total loss = 0.67, predict loss = 0.13 (67.0 examples/sec; 0.060 sec/batch; 98h:54m:39s remains)
INFO - root - 2019-11-03 23:48:25.200224: step 31820, total loss = 0.67, predict loss = 0.15 (68.2 examples/sec; 0.059 sec/batch; 97h:10m:58s remains)
INFO - root - 2019-11-03 23:48:25.861829: step 31830, total loss = 0.70, predict loss = 0.16 (80.9 examples/sec; 0.049 sec/batch; 81h:59m:13s remains)
INFO - root - 2019-11-03 23:48:26.510774: step 31840, total loss = 0.85, predict loss = 0.20 (68.6 examples/sec; 0.058 sec/batch; 96h:43m:49s remains)
INFO - root - 2019-11-03 23:48:27.162185: step 31850, total loss = 0.91, predict loss = 0.22 (68.5 examples/sec; 0.058 sec/batch; 96h:48m:18s remains)
INFO - root - 2019-11-03 23:48:27.808321: step 31860, total loss = 0.61, predict loss = 0.14 (64.7 examples/sec; 0.062 sec/batch; 102h:25m:03s remains)
INFO - root - 2019-11-03 23:48:28.455586: step 31870, total loss = 0.94, predict loss = 0.23 (71.9 examples/sec; 0.056 sec/batch; 92h:17m:15s remains)
INFO - root - 2019-11-03 23:48:29.132598: step 31880, total loss = 0.81, predict loss = 0.20 (63.5 examples/sec; 0.063 sec/batch; 104h:20m:51s remains)
INFO - root - 2019-11-03 23:48:29.826163: step 31890, total loss = 0.74, predict loss = 0.18 (61.9 examples/sec; 0.065 sec/batch; 107h:08m:09s remains)
INFO - root - 2019-11-03 23:48:30.461434: step 31900, total loss = 0.92, predict loss = 0.23 (69.2 examples/sec; 0.058 sec/batch; 95h:48m:06s remains)
INFO - root - 2019-11-03 23:48:31.119946: step 31910, total loss = 1.01, predict loss = 0.24 (70.7 examples/sec; 0.057 sec/batch; 93h:45m:14s remains)
INFO - root - 2019-11-03 23:48:31.767723: step 31920, total loss = 0.70, predict loss = 0.16 (66.0 examples/sec; 0.061 sec/batch; 100h:32m:03s remains)
INFO - root - 2019-11-03 23:48:32.391725: step 31930, total loss = 0.82, predict loss = 0.19 (78.0 examples/sec; 0.051 sec/batch; 85h:03m:12s remains)
INFO - root - 2019-11-03 23:48:33.034109: step 31940, total loss = 0.53, predict loss = 0.12 (70.8 examples/sec; 0.056 sec/batch; 93h:37m:20s remains)
INFO - root - 2019-11-03 23:48:33.676096: step 31950, total loss = 0.51, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 92h:18m:56s remains)
INFO - root - 2019-11-03 23:48:34.324718: step 31960, total loss = 0.62, predict loss = 0.14 (76.4 examples/sec; 0.052 sec/batch; 86h:47m:37s remains)
INFO - root - 2019-11-03 23:48:34.990217: step 31970, total loss = 0.84, predict loss = 0.19 (70.1 examples/sec; 0.057 sec/batch; 94h:32m:39s remains)
INFO - root - 2019-11-03 23:48:35.666088: step 31980, total loss = 0.55, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 98h:54m:43s remains)
INFO - root - 2019-11-03 23:48:36.276459: step 31990, total loss = 0.49, predict loss = 0.11 (72.9 examples/sec; 0.055 sec/batch; 91h:01m:20s remains)
INFO - root - 2019-11-03 23:48:36.904003: step 32000, total loss = 0.46, predict loss = 0.10 (68.0 examples/sec; 0.059 sec/batch; 97h:30m:25s remains)
INFO - root - 2019-11-03 23:48:37.537445: step 32010, total loss = 0.64, predict loss = 0.14 (75.7 examples/sec; 0.053 sec/batch; 87h:38m:30s remains)
INFO - root - 2019-11-03 23:48:38.208035: step 32020, total loss = 0.42, predict loss = 0.09 (54.9 examples/sec; 0.073 sec/batch; 120h:48m:08s remains)
INFO - root - 2019-11-03 23:48:38.872567: step 32030, total loss = 0.68, predict loss = 0.15 (65.4 examples/sec; 0.061 sec/batch; 101h:26m:49s remains)
INFO - root - 2019-11-03 23:48:39.632744: step 32040, total loss = 0.76, predict loss = 0.18 (65.9 examples/sec; 0.061 sec/batch; 100h:36m:13s remains)
INFO - root - 2019-11-03 23:48:40.379355: step 32050, total loss = 0.63, predict loss = 0.15 (60.6 examples/sec; 0.066 sec/batch; 109h:22m:19s remains)
INFO - root - 2019-11-03 23:48:41.140847: step 32060, total loss = 0.55, predict loss = 0.13 (55.3 examples/sec; 0.072 sec/batch; 119h:53m:05s remains)
INFO - root - 2019-11-03 23:48:41.960704: step 32070, total loss = 0.66, predict loss = 0.16 (55.3 examples/sec; 0.072 sec/batch; 119h:56m:15s remains)
INFO - root - 2019-11-03 23:48:42.729623: step 32080, total loss = 0.65, predict loss = 0.15 (65.9 examples/sec; 0.061 sec/batch; 100h:33m:44s remains)
INFO - root - 2019-11-03 23:48:43.312446: step 32090, total loss = 0.54, predict loss = 0.13 (75.2 examples/sec; 0.053 sec/batch; 88h:10m:23s remains)
INFO - root - 2019-11-03 23:48:43.943847: step 32100, total loss = 0.64, predict loss = 0.15 (63.8 examples/sec; 0.063 sec/batch; 104h:00m:27s remains)
INFO - root - 2019-11-03 23:48:44.641615: step 32110, total loss = 0.68, predict loss = 0.15 (58.5 examples/sec; 0.068 sec/batch; 113h:17m:30s remains)
INFO - root - 2019-11-03 23:48:45.309364: step 32120, total loss = 0.71, predict loss = 0.18 (63.4 examples/sec; 0.063 sec/batch; 104h:39m:17s remains)
INFO - root - 2019-11-03 23:48:45.990988: step 32130, total loss = 0.68, predict loss = 0.17 (62.1 examples/sec; 0.064 sec/batch; 106h:47m:25s remains)
INFO - root - 2019-11-03 23:48:46.628529: step 32140, total loss = 0.67, predict loss = 0.14 (75.9 examples/sec; 0.053 sec/batch; 87h:24m:31s remains)
INFO - root - 2019-11-03 23:48:47.234573: step 32150, total loss = 0.68, predict loss = 0.17 (71.5 examples/sec; 0.056 sec/batch; 92h:47m:11s remains)
INFO - root - 2019-11-03 23:48:47.863584: step 32160, total loss = 0.70, predict loss = 0.15 (75.8 examples/sec; 0.053 sec/batch; 87h:30m:13s remains)
INFO - root - 2019-11-03 23:48:48.486053: step 32170, total loss = 0.66, predict loss = 0.15 (65.7 examples/sec; 0.061 sec/batch; 100h:56m:47s remains)
INFO - root - 2019-11-03 23:48:49.144820: step 32180, total loss = 0.83, predict loss = 0.19 (77.5 examples/sec; 0.052 sec/batch; 85h:36m:46s remains)
INFO - root - 2019-11-03 23:48:49.769701: step 32190, total loss = 0.60, predict loss = 0.14 (75.1 examples/sec; 0.053 sec/batch; 88h:15m:59s remains)
INFO - root - 2019-11-03 23:48:50.420069: step 32200, total loss = 0.60, predict loss = 0.14 (65.6 examples/sec; 0.061 sec/batch; 101h:05m:09s remains)
INFO - root - 2019-11-03 23:48:51.099051: step 32210, total loss = 0.61, predict loss = 0.14 (69.7 examples/sec; 0.057 sec/batch; 95h:05m:26s remains)
INFO - root - 2019-11-03 23:48:51.812215: step 32220, total loss = 0.57, predict loss = 0.13 (83.4 examples/sec; 0.048 sec/batch; 79h:30m:23s remains)
INFO - root - 2019-11-03 23:48:52.435504: step 32230, total loss = 0.57, predict loss = 0.13 (70.0 examples/sec; 0.057 sec/batch; 94h:40m:16s remains)
INFO - root - 2019-11-03 23:48:53.078537: step 32240, total loss = 0.70, predict loss = 0.16 (68.6 examples/sec; 0.058 sec/batch; 96h:42m:38s remains)
INFO - root - 2019-11-03 23:48:53.715567: step 32250, total loss = 0.56, predict loss = 0.11 (71.7 examples/sec; 0.056 sec/batch; 92h:29m:40s remains)
INFO - root - 2019-11-03 23:48:54.341212: step 32260, total loss = 0.48, predict loss = 0.11 (78.2 examples/sec; 0.051 sec/batch; 84h:48m:42s remains)
INFO - root - 2019-11-03 23:48:54.956751: step 32270, total loss = 0.61, predict loss = 0.14 (72.5 examples/sec; 0.055 sec/batch; 91h:29m:05s remains)
INFO - root - 2019-11-03 23:48:55.593955: step 32280, total loss = 0.66, predict loss = 0.15 (65.7 examples/sec; 0.061 sec/batch; 100h:51m:52s remains)
INFO - root - 2019-11-03 23:48:56.233271: step 32290, total loss = 0.56, predict loss = 0.13 (67.0 examples/sec; 0.060 sec/batch; 98h:53m:56s remains)
INFO - root - 2019-11-03 23:48:56.888442: step 32300, total loss = 0.55, predict loss = 0.12 (76.3 examples/sec; 0.052 sec/batch; 86h:55m:18s remains)
INFO - root - 2019-11-03 23:48:57.532863: step 32310, total loss = 0.65, predict loss = 0.16 (78.3 examples/sec; 0.051 sec/batch; 84h:44m:08s remains)
INFO - root - 2019-11-03 23:48:58.147728: step 32320, total loss = 0.58, predict loss = 0.13 (71.1 examples/sec; 0.056 sec/batch; 93h:12m:28s remains)
INFO - root - 2019-11-03 23:48:58.797687: step 32330, total loss = 0.58, predict loss = 0.13 (63.8 examples/sec; 0.063 sec/batch; 103h:55m:17s remains)
INFO - root - 2019-11-03 23:48:59.461320: step 32340, total loss = 0.72, predict loss = 0.16 (61.2 examples/sec; 0.065 sec/batch; 108h:17m:21s remains)
INFO - root - 2019-11-03 23:49:00.087853: step 32350, total loss = 0.59, predict loss = 0.14 (67.8 examples/sec; 0.059 sec/batch; 97h:43m:43s remains)
INFO - root - 2019-11-03 23:49:00.734621: step 32360, total loss = 0.70, predict loss = 0.17 (67.7 examples/sec; 0.059 sec/batch; 97h:58m:19s remains)
INFO - root - 2019-11-03 23:49:01.425447: step 32370, total loss = 0.72, predict loss = 0.16 (63.1 examples/sec; 0.063 sec/batch; 105h:07m:44s remains)
INFO - root - 2019-11-03 23:49:02.056941: step 32380, total loss = 0.79, predict loss = 0.19 (63.1 examples/sec; 0.063 sec/batch; 105h:09m:26s remains)
INFO - root - 2019-11-03 23:49:02.707336: step 32390, total loss = 0.67, predict loss = 0.15 (61.8 examples/sec; 0.065 sec/batch; 107h:13m:07s remains)
INFO - root - 2019-11-03 23:49:03.341315: step 32400, total loss = 0.61, predict loss = 0.14 (76.1 examples/sec; 0.053 sec/batch; 87h:06m:36s remains)
INFO - root - 2019-11-03 23:49:03.973480: step 32410, total loss = 0.62, predict loss = 0.14 (67.8 examples/sec; 0.059 sec/batch; 97h:44m:54s remains)
INFO - root - 2019-11-03 23:49:04.613836: step 32420, total loss = 0.63, predict loss = 0.14 (65.4 examples/sec; 0.061 sec/batch; 101h:27m:46s remains)
INFO - root - 2019-11-03 23:49:05.275302: step 32430, total loss = 0.64, predict loss = 0.15 (65.3 examples/sec; 0.061 sec/batch; 101h:28m:39s remains)
INFO - root - 2019-11-03 23:49:05.901052: step 32440, total loss = 0.63, predict loss = 0.15 (69.7 examples/sec; 0.057 sec/batch; 95h:05m:18s remains)
INFO - root - 2019-11-03 23:49:06.541953: step 32450, total loss = 0.49, predict loss = 0.11 (65.8 examples/sec; 0.061 sec/batch; 100h:45m:10s remains)
INFO - root - 2019-11-03 23:49:07.191017: step 32460, total loss = 0.90, predict loss = 0.21 (70.0 examples/sec; 0.057 sec/batch; 94h:46m:59s remains)
INFO - root - 2019-11-03 23:49:07.846278: step 32470, total loss = 0.79, predict loss = 0.19 (70.3 examples/sec; 0.057 sec/batch; 94h:18m:39s remains)
INFO - root - 2019-11-03 23:49:08.510056: step 32480, total loss = 0.69, predict loss = 0.16 (69.4 examples/sec; 0.058 sec/batch; 95h:34m:09s remains)
INFO - root - 2019-11-03 23:49:09.126095: step 32490, total loss = 0.80, predict loss = 0.19 (75.5 examples/sec; 0.053 sec/batch; 87h:50m:57s remains)
INFO - root - 2019-11-03 23:49:09.780499: step 32500, total loss = 0.61, predict loss = 0.15 (65.5 examples/sec; 0.061 sec/batch; 101h:10m:14s remains)
INFO - root - 2019-11-03 23:49:10.436887: step 32510, total loss = 0.76, predict loss = 0.19 (73.1 examples/sec; 0.055 sec/batch; 90h:40m:02s remains)
INFO - root - 2019-11-03 23:49:11.108150: step 32520, total loss = 0.81, predict loss = 0.18 (72.0 examples/sec; 0.056 sec/batch; 92h:01m:57s remains)
INFO - root - 2019-11-03 23:49:11.760506: step 32530, total loss = 0.97, predict loss = 0.22 (70.8 examples/sec; 0.057 sec/batch; 93h:40m:13s remains)
INFO - root - 2019-11-03 23:49:12.473324: step 32540, total loss = 0.87, predict loss = 0.21 (77.4 examples/sec; 0.052 sec/batch; 85h:42m:39s remains)
INFO - root - 2019-11-03 23:49:13.068457: step 32550, total loss = 0.81, predict loss = 0.18 (79.3 examples/sec; 0.050 sec/batch; 83h:37m:38s remains)
INFO - root - 2019-11-03 23:49:13.677286: step 32560, total loss = 0.87, predict loss = 0.22 (81.2 examples/sec; 0.049 sec/batch; 81h:42m:16s remains)
INFO - root - 2019-11-03 23:49:14.310412: step 32570, total loss = 0.72, predict loss = 0.16 (81.8 examples/sec; 0.049 sec/batch; 81h:06m:15s remains)
INFO - root - 2019-11-03 23:49:14.929986: step 32580, total loss = 0.97, predict loss = 0.23 (71.5 examples/sec; 0.056 sec/batch; 92h:43m:19s remains)
INFO - root - 2019-11-03 23:49:15.549613: step 32590, total loss = 0.62, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 95h:44m:20s remains)
INFO - root - 2019-11-03 23:49:16.208484: step 32600, total loss = 0.57, predict loss = 0.13 (71.2 examples/sec; 0.056 sec/batch; 93h:07m:52s remains)
INFO - root - 2019-11-03 23:49:16.821602: step 32610, total loss = 0.70, predict loss = 0.17 (82.0 examples/sec; 0.049 sec/batch; 80h:50m:37s remains)
INFO - root - 2019-11-03 23:49:17.463531: step 32620, total loss = 0.55, predict loss = 0.13 (73.0 examples/sec; 0.055 sec/batch; 90h:46m:15s remains)
INFO - root - 2019-11-03 23:49:18.091815: step 32630, total loss = 0.62, predict loss = 0.15 (73.2 examples/sec; 0.055 sec/batch; 90h:36m:24s remains)
INFO - root - 2019-11-03 23:49:18.752629: step 32640, total loss = 0.70, predict loss = 0.17 (64.4 examples/sec; 0.062 sec/batch; 102h:59m:23s remains)
INFO - root - 2019-11-03 23:49:19.393335: step 32650, total loss = 0.71, predict loss = 0.17 (61.1 examples/sec; 0.065 sec/batch; 108h:32m:09s remains)
INFO - root - 2019-11-03 23:49:20.081975: step 32660, total loss = 0.66, predict loss = 0.16 (68.3 examples/sec; 0.059 sec/batch; 97h:07m:18s remains)
INFO - root - 2019-11-03 23:49:20.709199: step 32670, total loss = 0.81, predict loss = 0.20 (68.6 examples/sec; 0.058 sec/batch; 96h:40m:12s remains)
INFO - root - 2019-11-03 23:49:21.353261: step 32680, total loss = 0.65, predict loss = 0.15 (67.7 examples/sec; 0.059 sec/batch; 97h:53m:02s remains)
INFO - root - 2019-11-03 23:49:21.995596: step 32690, total loss = 0.70, predict loss = 0.17 (72.0 examples/sec; 0.056 sec/batch; 92h:03m:17s remains)
INFO - root - 2019-11-03 23:49:22.604926: step 32700, total loss = 0.56, predict loss = 0.12 (77.7 examples/sec; 0.051 sec/batch; 85h:21m:49s remains)
INFO - root - 2019-11-03 23:49:23.149070: step 32710, total loss = 0.69, predict loss = 0.15 (99.2 examples/sec; 0.040 sec/batch; 66h:49m:43s remains)
INFO - root - 2019-11-03 23:49:23.624174: step 32720, total loss = 0.52, predict loss = 0.12 (95.0 examples/sec; 0.042 sec/batch; 69h:48m:34s remains)
INFO - root - 2019-11-03 23:49:24.625881: step 32730, total loss = 0.55, predict loss = 0.12 (6.9 examples/sec; 0.578 sec/batch; 958h:36m:34s remains)
INFO - root - 2019-11-03 23:49:25.240122: step 32740, total loss = 0.40, predict loss = 0.09 (86.9 examples/sec; 0.046 sec/batch; 76h:15m:22s remains)
INFO - root - 2019-11-03 23:49:25.873423: step 32750, total loss = 0.46, predict loss = 0.10 (70.8 examples/sec; 0.056 sec/batch; 93h:38m:43s remains)
INFO - root - 2019-11-03 23:49:26.487830: step 32760, total loss = 0.56, predict loss = 0.13 (73.4 examples/sec; 0.054 sec/batch; 90h:17m:33s remains)
INFO - root - 2019-11-03 23:49:27.135283: step 32770, total loss = 0.59, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:49m:35s remains)
INFO - root - 2019-11-03 23:49:27.787555: step 32780, total loss = 0.65, predict loss = 0.15 (65.4 examples/sec; 0.061 sec/batch; 101h:22m:11s remains)
INFO - root - 2019-11-03 23:49:28.429882: step 32790, total loss = 0.68, predict loss = 0.17 (65.9 examples/sec; 0.061 sec/batch; 100h:34m:18s remains)
INFO - root - 2019-11-03 23:49:29.059221: step 32800, total loss = 0.75, predict loss = 0.17 (76.0 examples/sec; 0.053 sec/batch; 87h:17m:25s remains)
INFO - root - 2019-11-03 23:49:29.709437: step 32810, total loss = 0.90, predict loss = 0.20 (69.9 examples/sec; 0.057 sec/batch; 94h:49m:42s remains)
INFO - root - 2019-11-03 23:49:30.344777: step 32820, total loss = 1.06, predict loss = 0.26 (67.4 examples/sec; 0.059 sec/batch; 98h:25m:57s remains)
INFO - root - 2019-11-03 23:49:31.015367: step 32830, total loss = 0.85, predict loss = 0.20 (63.2 examples/sec; 0.063 sec/batch; 104h:51m:26s remains)
INFO - root - 2019-11-03 23:49:31.639123: step 32840, total loss = 0.74, predict loss = 0.17 (72.0 examples/sec; 0.056 sec/batch; 92h:06m:34s remains)
INFO - root - 2019-11-03 23:49:32.321102: step 32850, total loss = 0.75, predict loss = 0.16 (70.6 examples/sec; 0.057 sec/batch; 93h:57m:04s remains)
INFO - root - 2019-11-03 23:49:32.952574: step 32860, total loss = 0.75, predict loss = 0.19 (70.6 examples/sec; 0.057 sec/batch; 93h:55m:19s remains)
INFO - root - 2019-11-03 23:49:33.573392: step 32870, total loss = 0.71, predict loss = 0.17 (69.7 examples/sec; 0.057 sec/batch; 95h:08m:40s remains)
INFO - root - 2019-11-03 23:49:34.195170: step 32880, total loss = 0.69, predict loss = 0.17 (72.5 examples/sec; 0.055 sec/batch; 91h:24m:05s remains)
INFO - root - 2019-11-03 23:49:34.817384: step 32890, total loss = 0.72, predict loss = 0.16 (67.9 examples/sec; 0.059 sec/batch; 97h:37m:40s remains)
INFO - root - 2019-11-03 23:49:35.441811: step 32900, total loss = 0.51, predict loss = 0.11 (78.3 examples/sec; 0.051 sec/batch; 84h:38m:56s remains)
INFO - root - 2019-11-03 23:49:36.058144: step 32910, total loss = 0.71, predict loss = 0.16 (68.7 examples/sec; 0.058 sec/batch; 96h:28m:16s remains)
INFO - root - 2019-11-03 23:49:36.719616: step 32920, total loss = 0.56, predict loss = 0.12 (72.8 examples/sec; 0.055 sec/batch; 91h:03m:18s remains)
INFO - root - 2019-11-03 23:49:37.409986: step 32930, total loss = 0.60, predict loss = 0.12 (78.4 examples/sec; 0.051 sec/batch; 84h:32m:22s remains)
INFO - root - 2019-11-03 23:49:38.087345: step 32940, total loss = 0.53, predict loss = 0.11 (57.6 examples/sec; 0.069 sec/batch; 115h:03m:09s remains)
INFO - root - 2019-11-03 23:49:38.724932: step 32950, total loss = 0.51, predict loss = 0.11 (63.5 examples/sec; 0.063 sec/batch; 104h:23m:11s remains)
INFO - root - 2019-11-03 23:49:39.414540: step 32960, total loss = 0.49, predict loss = 0.11 (76.8 examples/sec; 0.052 sec/batch; 86h:21m:32s remains)
INFO - root - 2019-11-03 23:49:40.064833: step 32970, total loss = 0.89, predict loss = 0.20 (70.6 examples/sec; 0.057 sec/batch; 93h:58m:05s remains)
INFO - root - 2019-11-03 23:49:40.663353: step 32980, total loss = 0.48, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 93h:56m:46s remains)
INFO - root - 2019-11-03 23:49:41.283922: step 32990, total loss = 0.64, predict loss = 0.15 (68.6 examples/sec; 0.058 sec/batch; 96h:37m:18s remains)
INFO - root - 2019-11-03 23:49:41.901035: step 33000, total loss = 0.67, predict loss = 0.16 (67.9 examples/sec; 0.059 sec/batch; 97h:39m:39s remains)
INFO - root - 2019-11-03 23:49:42.569842: step 33010, total loss = 0.64, predict loss = 0.15 (69.5 examples/sec; 0.058 sec/batch; 95h:26m:14s remains)
INFO - root - 2019-11-03 23:49:43.212102: step 33020, total loss = 0.67, predict loss = 0.15 (69.6 examples/sec; 0.057 sec/batch; 95h:12m:27s remains)
INFO - root - 2019-11-03 23:49:43.828572: step 33030, total loss = 0.51, predict loss = 0.11 (79.6 examples/sec; 0.050 sec/batch; 83h:18m:08s remains)
INFO - root - 2019-11-03 23:49:44.453694: step 33040, total loss = 0.70, predict loss = 0.16 (78.2 examples/sec; 0.051 sec/batch; 84h:46m:27s remains)
INFO - root - 2019-11-03 23:49:45.081380: step 33050, total loss = 0.80, predict loss = 0.19 (68.6 examples/sec; 0.058 sec/batch; 96h:39m:31s remains)
INFO - root - 2019-11-03 23:49:45.709568: step 33060, total loss = 0.64, predict loss = 0.14 (71.0 examples/sec; 0.056 sec/batch; 93h:23m:08s remains)
INFO - root - 2019-11-03 23:49:46.365854: step 33070, total loss = 0.65, predict loss = 0.14 (62.6 examples/sec; 0.064 sec/batch; 105h:50m:09s remains)
INFO - root - 2019-11-03 23:49:46.984502: step 33080, total loss = 0.83, predict loss = 0.18 (74.8 examples/sec; 0.053 sec/batch; 88h:36m:57s remains)
INFO - root - 2019-11-03 23:49:47.657347: step 33090, total loss = 0.63, predict loss = 0.14 (59.4 examples/sec; 0.067 sec/batch; 111h:39m:17s remains)
INFO - root - 2019-11-03 23:49:48.319959: step 33100, total loss = 0.56, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 93h:51m:59s remains)
INFO - root - 2019-11-03 23:49:48.947510: step 33110, total loss = 0.69, predict loss = 0.15 (74.8 examples/sec; 0.053 sec/batch; 88h:37m:16s remains)
INFO - root - 2019-11-03 23:49:49.622512: step 33120, total loss = 0.56, predict loss = 0.13 (65.6 examples/sec; 0.061 sec/batch; 100h:59m:29s remains)
INFO - root - 2019-11-03 23:49:50.247106: step 33130, total loss = 0.60, predict loss = 0.13 (64.9 examples/sec; 0.062 sec/batch; 102h:10m:12s remains)
INFO - root - 2019-11-03 23:49:50.886425: step 33140, total loss = 0.55, predict loss = 0.11 (69.2 examples/sec; 0.058 sec/batch; 95h:44m:54s remains)
INFO - root - 2019-11-03 23:49:51.562052: step 33150, total loss = 0.77, predict loss = 0.17 (65.3 examples/sec; 0.061 sec/batch; 101h:35m:25s remains)
INFO - root - 2019-11-03 23:49:52.255874: step 33160, total loss = 0.58, predict loss = 0.12 (59.9 examples/sec; 0.067 sec/batch; 110h:39m:45s remains)
INFO - root - 2019-11-03 23:49:52.877971: step 33170, total loss = 0.77, predict loss = 0.18 (65.4 examples/sec; 0.061 sec/batch; 101h:20m:37s remains)
INFO - root - 2019-11-03 23:49:53.556069: step 33180, total loss = 0.57, predict loss = 0.13 (58.4 examples/sec; 0.068 sec/batch; 113h:30m:06s remains)
INFO - root - 2019-11-03 23:49:54.262504: step 33190, total loss = 0.55, predict loss = 0.13 (62.7 examples/sec; 0.064 sec/batch; 105h:41m:02s remains)
INFO - root - 2019-11-03 23:49:54.900582: step 33200, total loss = 0.50, predict loss = 0.11 (72.0 examples/sec; 0.056 sec/batch; 92h:03m:25s remains)
INFO - root - 2019-11-03 23:49:55.520449: step 33210, total loss = 0.49, predict loss = 0.11 (68.4 examples/sec; 0.059 sec/batch; 96h:57m:51s remains)
INFO - root - 2019-11-03 23:49:56.119392: step 33220, total loss = 0.59, predict loss = 0.15 (67.7 examples/sec; 0.059 sec/batch; 97h:54m:46s remains)
INFO - root - 2019-11-03 23:49:56.774122: step 33230, total loss = 0.53, predict loss = 0.12 (65.7 examples/sec; 0.061 sec/batch; 100h:52m:59s remains)
INFO - root - 2019-11-03 23:49:57.428550: step 33240, total loss = 0.50, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 103h:02m:45s remains)
INFO - root - 2019-11-03 23:49:58.075532: step 33250, total loss = 0.56, predict loss = 0.13 (61.5 examples/sec; 0.065 sec/batch; 107h:44m:23s remains)
INFO - root - 2019-11-03 23:49:58.707422: step 33260, total loss = 0.93, predict loss = 0.22 (69.1 examples/sec; 0.058 sec/batch; 95h:54m:04s remains)
INFO - root - 2019-11-03 23:49:59.311760: step 33270, total loss = 0.75, predict loss = 0.18 (78.8 examples/sec; 0.051 sec/batch; 84h:05m:17s remains)
INFO - root - 2019-11-03 23:49:59.904935: step 33280, total loss = 0.65, predict loss = 0.15 (80.3 examples/sec; 0.050 sec/batch; 82h:33m:29s remains)
INFO - root - 2019-11-03 23:50:00.531266: step 33290, total loss = 0.39, predict loss = 0.09 (68.6 examples/sec; 0.058 sec/batch; 96h:34m:44s remains)
INFO - root - 2019-11-03 23:50:01.175078: step 33300, total loss = 0.47, predict loss = 0.10 (68.3 examples/sec; 0.059 sec/batch; 97h:04m:47s remains)
INFO - root - 2019-11-03 23:50:01.847806: step 33310, total loss = 0.40, predict loss = 0.09 (67.3 examples/sec; 0.059 sec/batch; 98h:32m:26s remains)
INFO - root - 2019-11-03 23:50:02.470430: step 33320, total loss = 0.61, predict loss = 0.15 (71.8 examples/sec; 0.056 sec/batch; 92h:18m:11s remains)
INFO - root - 2019-11-03 23:50:03.102989: step 33330, total loss = 0.42, predict loss = 0.10 (81.0 examples/sec; 0.049 sec/batch; 81h:48m:37s remains)
INFO - root - 2019-11-03 23:50:03.762195: step 33340, total loss = 0.39, predict loss = 0.08 (64.9 examples/sec; 0.062 sec/batch; 102h:11m:14s remains)
INFO - root - 2019-11-03 23:50:04.403559: step 33350, total loss = 0.41, predict loss = 0.09 (68.9 examples/sec; 0.058 sec/batch; 96h:14m:48s remains)
INFO - root - 2019-11-03 23:50:05.093465: step 33360, total loss = 0.54, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 91h:18m:19s remains)
INFO - root - 2019-11-03 23:50:05.732185: step 33370, total loss = 0.53, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 94h:14m:45s remains)
INFO - root - 2019-11-03 23:50:06.370229: step 33380, total loss = 0.55, predict loss = 0.13 (79.6 examples/sec; 0.050 sec/batch; 83h:15m:51s remains)
INFO - root - 2019-11-03 23:50:06.995339: step 33390, total loss = 0.72, predict loss = 0.17 (70.9 examples/sec; 0.056 sec/batch; 93h:33m:35s remains)
INFO - root - 2019-11-03 23:50:07.589715: step 33400, total loss = 0.54, predict loss = 0.12 (66.8 examples/sec; 0.060 sec/batch; 99h:12m:20s remains)
INFO - root - 2019-11-03 23:50:08.236444: step 33410, total loss = 0.58, predict loss = 0.13 (67.1 examples/sec; 0.060 sec/batch; 98h:46m:59s remains)
INFO - root - 2019-11-03 23:50:08.891642: step 33420, total loss = 0.64, predict loss = 0.15 (77.5 examples/sec; 0.052 sec/batch; 85h:32m:27s remains)
INFO - root - 2019-11-03 23:50:09.520777: step 33430, total loss = 1.15, predict loss = 0.28 (62.7 examples/sec; 0.064 sec/batch; 105h:48m:32s remains)
INFO - root - 2019-11-03 23:50:10.166456: step 33440, total loss = 0.96, predict loss = 0.22 (65.5 examples/sec; 0.061 sec/batch; 101h:12m:09s remains)
INFO - root - 2019-11-03 23:50:10.822941: step 33450, total loss = 0.50, predict loss = 0.12 (66.8 examples/sec; 0.060 sec/batch; 99h:13m:58s remains)
INFO - root - 2019-11-03 23:50:11.453543: step 33460, total loss = 0.82, predict loss = 0.19 (75.7 examples/sec; 0.053 sec/batch; 87h:33m:52s remains)
INFO - root - 2019-11-03 23:50:12.101441: step 33470, total loss = 0.61, predict loss = 0.15 (55.1 examples/sec; 0.073 sec/batch; 120h:18m:28s remains)
INFO - root - 2019-11-03 23:50:12.776737: step 33480, total loss = 0.65, predict loss = 0.14 (66.9 examples/sec; 0.060 sec/batch; 99h:09m:18s remains)
INFO - root - 2019-11-03 23:50:13.439740: step 33490, total loss = 0.59, predict loss = 0.14 (63.5 examples/sec; 0.063 sec/batch; 104h:22m:56s remains)
INFO - root - 2019-11-03 23:50:14.066852: step 33500, total loss = 0.63, predict loss = 0.15 (64.6 examples/sec; 0.062 sec/batch; 102h:36m:11s remains)
INFO - root - 2019-11-03 23:50:14.717415: step 33510, total loss = 0.80, predict loss = 0.20 (65.1 examples/sec; 0.061 sec/batch; 101h:47m:35s remains)
INFO - root - 2019-11-03 23:50:15.346763: step 33520, total loss = 0.78, predict loss = 0.18 (73.5 examples/sec; 0.054 sec/batch; 90h:14m:22s remains)
INFO - root - 2019-11-03 23:50:15.976343: step 33530, total loss = 0.73, predict loss = 0.17 (72.1 examples/sec; 0.055 sec/batch; 91h:55m:25s remains)
INFO - root - 2019-11-03 23:50:16.594998: step 33540, total loss = 0.48, predict loss = 0.10 (74.2 examples/sec; 0.054 sec/batch; 89h:19m:36s remains)
INFO - root - 2019-11-03 23:50:17.238613: step 33550, total loss = 0.41, predict loss = 0.10 (68.4 examples/sec; 0.058 sec/batch; 96h:55m:05s remains)
INFO - root - 2019-11-03 23:50:17.915580: step 33560, total loss = 0.49, predict loss = 0.11 (61.3 examples/sec; 0.065 sec/batch; 108h:13m:14s remains)
INFO - root - 2019-11-03 23:50:18.624659: step 33570, total loss = 0.40, predict loss = 0.08 (59.7 examples/sec; 0.067 sec/batch; 111h:01m:57s remains)
INFO - root - 2019-11-03 23:50:19.305271: step 33580, total loss = 0.32, predict loss = 0.06 (75.6 examples/sec; 0.053 sec/batch; 87h:42m:47s remains)
INFO - root - 2019-11-03 23:50:19.922133: step 33590, total loss = 0.39, predict loss = 0.08 (72.7 examples/sec; 0.055 sec/batch; 91h:12m:12s remains)
INFO - root - 2019-11-03 23:50:20.549633: step 33600, total loss = 0.33, predict loss = 0.07 (66.1 examples/sec; 0.060 sec/batch; 100h:15m:26s remains)
INFO - root - 2019-11-03 23:50:21.205982: step 33610, total loss = 0.35, predict loss = 0.07 (66.6 examples/sec; 0.060 sec/batch; 99h:35m:42s remains)
INFO - root - 2019-11-03 23:50:21.899694: step 33620, total loss = 0.49, predict loss = 0.11 (63.9 examples/sec; 0.063 sec/batch; 103h:49m:26s remains)
INFO - root - 2019-11-03 23:50:22.597077: step 33630, total loss = 0.45, predict loss = 0.10 (61.8 examples/sec; 0.065 sec/batch; 107h:17m:12s remains)
INFO - root - 2019-11-03 23:50:23.259789: step 33640, total loss = 0.47, predict loss = 0.11 (71.3 examples/sec; 0.056 sec/batch; 92h:57m:46s remains)
INFO - root - 2019-11-03 23:50:23.929711: step 33650, total loss = 0.44, predict loss = 0.10 (66.6 examples/sec; 0.060 sec/batch; 99h:28m:50s remains)
INFO - root - 2019-11-03 23:50:24.581021: step 33660, total loss = 0.47, predict loss = 0.11 (65.7 examples/sec; 0.061 sec/batch; 100h:54m:47s remains)
INFO - root - 2019-11-03 23:50:25.234549: step 33670, total loss = 0.56, predict loss = 0.13 (65.4 examples/sec; 0.061 sec/batch; 101h:19m:46s remains)
INFO - root - 2019-11-03 23:50:25.925152: step 33680, total loss = 0.41, predict loss = 0.09 (66.2 examples/sec; 0.060 sec/batch; 100h:10m:34s remains)
INFO - root - 2019-11-03 23:50:26.563747: step 33690, total loss = 0.46, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 99h:49m:47s remains)
INFO - root - 2019-11-03 23:50:27.192449: step 33700, total loss = 0.44, predict loss = 0.11 (72.0 examples/sec; 0.056 sec/batch; 92h:03m:34s remains)
INFO - root - 2019-11-03 23:50:27.845806: step 33710, total loss = 0.57, predict loss = 0.12 (63.2 examples/sec; 0.063 sec/batch; 104h:52m:43s remains)
INFO - root - 2019-11-03 23:50:28.479644: step 33720, total loss = 0.56, predict loss = 0.13 (63.8 examples/sec; 0.063 sec/batch; 103h:56m:35s remains)
INFO - root - 2019-11-03 23:50:29.118816: step 33730, total loss = 0.56, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 96h:18m:05s remains)
INFO - root - 2019-11-03 23:50:29.718534: step 33740, total loss = 0.74, predict loss = 0.18 (70.8 examples/sec; 0.056 sec/batch; 93h:36m:01s remains)
INFO - root - 2019-11-03 23:50:30.309588: step 33750, total loss = 0.63, predict loss = 0.16 (82.1 examples/sec; 0.049 sec/batch; 80h:47m:38s remains)
INFO - root - 2019-11-03 23:50:30.952770: step 33760, total loss = 0.69, predict loss = 0.15 (66.1 examples/sec; 0.061 sec/batch; 100h:21m:48s remains)
INFO - root - 2019-11-03 23:50:31.586240: step 33770, total loss = 0.79, predict loss = 0.20 (73.2 examples/sec; 0.055 sec/batch; 90h:33m:17s remains)
INFO - root - 2019-11-03 23:50:32.194941: step 33780, total loss = 0.45, predict loss = 0.11 (72.4 examples/sec; 0.055 sec/batch; 91h:30m:05s remains)
INFO - root - 2019-11-03 23:50:32.802578: step 33790, total loss = 0.43, predict loss = 0.09 (74.7 examples/sec; 0.054 sec/batch; 88h:47m:19s remains)
INFO - root - 2019-11-03 23:50:33.427922: step 33800, total loss = 0.26, predict loss = 0.05 (70.4 examples/sec; 0.057 sec/batch; 94h:05m:55s remains)
INFO - root - 2019-11-03 23:50:34.051589: step 33810, total loss = 0.40, predict loss = 0.09 (70.6 examples/sec; 0.057 sec/batch; 93h:53m:11s remains)
INFO - root - 2019-11-03 23:50:34.668049: step 33820, total loss = 0.22, predict loss = 0.04 (68.1 examples/sec; 0.059 sec/batch; 97h:24m:20s remains)
INFO - root - 2019-11-03 23:50:35.328632: step 33830, total loss = 0.28, predict loss = 0.05 (69.5 examples/sec; 0.058 sec/batch; 95h:25m:14s remains)
INFO - root - 2019-11-03 23:50:35.994949: step 33840, total loss = 0.54, predict loss = 0.13 (64.2 examples/sec; 0.062 sec/batch; 103h:11m:16s remains)
INFO - root - 2019-11-03 23:50:36.649476: step 33850, total loss = 0.67, predict loss = 0.16 (62.3 examples/sec; 0.064 sec/batch; 106h:22m:11s remains)
INFO - root - 2019-11-03 23:50:37.304002: step 33860, total loss = 0.67, predict loss = 0.17 (70.1 examples/sec; 0.057 sec/batch; 94h:36m:57s remains)
INFO - root - 2019-11-03 23:50:37.922612: step 33870, total loss = 0.61, predict loss = 0.14 (82.5 examples/sec; 0.048 sec/batch; 80h:22m:36s remains)
INFO - root - 2019-11-03 23:50:38.558790: step 33880, total loss = 0.66, predict loss = 0.15 (69.7 examples/sec; 0.057 sec/batch; 95h:02m:45s remains)
INFO - root - 2019-11-03 23:50:39.180849: step 33890, total loss = 0.57, predict loss = 0.13 (69.4 examples/sec; 0.058 sec/batch; 95h:27m:50s remains)
INFO - root - 2019-11-03 23:50:39.826649: step 33900, total loss = 0.48, predict loss = 0.11 (71.0 examples/sec; 0.056 sec/batch; 93h:23m:52s remains)
INFO - root - 2019-11-03 23:50:40.423838: step 33910, total loss = 0.75, predict loss = 0.16 (76.3 examples/sec; 0.052 sec/batch; 86h:53m:45s remains)
INFO - root - 2019-11-03 23:50:41.049670: step 33920, total loss = 0.62, predict loss = 0.15 (75.1 examples/sec; 0.053 sec/batch; 88h:18m:28s remains)
INFO - root - 2019-11-03 23:50:41.718851: step 33930, total loss = 0.40, predict loss = 0.08 (62.6 examples/sec; 0.064 sec/batch; 105h:53m:56s remains)
INFO - root - 2019-11-03 23:50:42.406365: step 33940, total loss = 0.55, predict loss = 0.13 (72.9 examples/sec; 0.055 sec/batch; 90h:56m:01s remains)
INFO - root - 2019-11-03 23:50:43.001974: step 33950, total loss = 0.46, predict loss = 0.10 (76.3 examples/sec; 0.052 sec/batch; 86h:50m:17s remains)
INFO - root - 2019-11-03 23:50:43.645667: step 33960, total loss = 0.53, predict loss = 0.10 (64.4 examples/sec; 0.062 sec/batch; 102h:59m:30s remains)
INFO - root - 2019-11-03 23:50:44.261914: step 33970, total loss = 0.45, predict loss = 0.10 (70.3 examples/sec; 0.057 sec/batch; 94h:18m:31s remains)
INFO - root - 2019-11-03 23:50:44.904971: step 33980, total loss = 0.66, predict loss = 0.16 (71.0 examples/sec; 0.056 sec/batch; 93h:24m:10s remains)
INFO - root - 2019-11-03 23:50:45.520949: step 33990, total loss = 0.62, predict loss = 0.14 (65.9 examples/sec; 0.061 sec/batch; 100h:37m:35s remains)
INFO - root - 2019-11-03 23:50:46.147635: step 34000, total loss = 0.75, predict loss = 0.17 (68.1 examples/sec; 0.059 sec/batch; 97h:21m:12s remains)
INFO - root - 2019-11-03 23:50:46.785298: step 34010, total loss = 0.71, predict loss = 0.16 (67.9 examples/sec; 0.059 sec/batch; 97h:38m:05s remains)
INFO - root - 2019-11-03 23:50:47.443368: step 34020, total loss = 0.77, predict loss = 0.18 (74.5 examples/sec; 0.054 sec/batch; 89h:02m:13s remains)
INFO - root - 2019-11-03 23:50:48.074190: step 34030, total loss = 0.66, predict loss = 0.15 (68.9 examples/sec; 0.058 sec/batch; 96h:16m:07s remains)
INFO - root - 2019-11-03 23:50:48.713283: step 34040, total loss = 0.74, predict loss = 0.16 (76.6 examples/sec; 0.052 sec/batch; 86h:31m:09s remains)
INFO - root - 2019-11-03 23:50:49.340054: step 34050, total loss = 0.53, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 92h:35m:09s remains)
INFO - root - 2019-11-03 23:50:49.946583: step 34060, total loss = 0.77, predict loss = 0.18 (90.1 examples/sec; 0.044 sec/batch; 73h:35m:24s remains)
INFO - root - 2019-11-03 23:50:50.574728: step 34070, total loss = 0.85, predict loss = 0.21 (64.9 examples/sec; 0.062 sec/batch; 102h:05m:19s remains)
INFO - root - 2019-11-03 23:50:51.288983: step 34080, total loss = 0.50, predict loss = 0.12 (61.9 examples/sec; 0.065 sec/batch; 107h:01m:04s remains)
INFO - root - 2019-11-03 23:50:51.939019: step 34090, total loss = 0.36, predict loss = 0.08 (72.9 examples/sec; 0.055 sec/batch; 90h:58m:38s remains)
INFO - root - 2019-11-03 23:50:52.583514: step 34100, total loss = 0.57, predict loss = 0.13 (78.1 examples/sec; 0.051 sec/batch; 84h:49m:45s remains)
INFO - root - 2019-11-03 23:50:53.262182: step 34110, total loss = 0.41, predict loss = 0.09 (63.5 examples/sec; 0.063 sec/batch; 104h:20m:54s remains)
INFO - root - 2019-11-03 23:50:53.881309: step 34120, total loss = 0.49, predict loss = 0.11 (74.0 examples/sec; 0.054 sec/batch; 89h:38m:08s remains)
INFO - root - 2019-11-03 23:50:54.499770: step 34130, total loss = 0.66, predict loss = 0.16 (66.8 examples/sec; 0.060 sec/batch; 99h:16m:27s remains)
INFO - root - 2019-11-03 23:50:55.128322: step 34140, total loss = 0.71, predict loss = 0.17 (66.5 examples/sec; 0.060 sec/batch; 99h:37m:35s remains)
INFO - root - 2019-11-03 23:50:55.755375: step 34150, total loss = 0.48, predict loss = 0.11 (77.4 examples/sec; 0.052 sec/batch; 85h:41m:17s remains)
INFO - root - 2019-11-03 23:50:56.389768: step 34160, total loss = 0.53, predict loss = 0.12 (68.5 examples/sec; 0.058 sec/batch; 96h:49m:56s remains)
INFO - root - 2019-11-03 23:50:57.055480: step 34170, total loss = 0.87, predict loss = 0.20 (70.6 examples/sec; 0.057 sec/batch; 93h:50m:32s remains)
INFO - root - 2019-11-03 23:50:57.705997: step 34180, total loss = 0.84, predict loss = 0.20 (73.6 examples/sec; 0.054 sec/batch; 90h:06m:33s remains)
INFO - root - 2019-11-03 23:50:58.355183: step 34190, total loss = 1.08, predict loss = 0.24 (76.4 examples/sec; 0.052 sec/batch; 86h:42m:36s remains)
INFO - root - 2019-11-03 23:50:58.985300: step 34200, total loss = 0.82, predict loss = 0.20 (73.2 examples/sec; 0.055 sec/batch; 90h:31m:46s remains)
INFO - root - 2019-11-03 23:50:59.600767: step 34210, total loss = 0.82, predict loss = 0.20 (72.7 examples/sec; 0.055 sec/batch; 91h:06m:57s remains)
INFO - root - 2019-11-03 23:51:00.231706: step 34220, total loss = 0.88, predict loss = 0.21 (69.9 examples/sec; 0.057 sec/batch; 94h:48m:09s remains)
INFO - root - 2019-11-03 23:51:00.860928: step 34230, total loss = 0.91, predict loss = 0.22 (67.1 examples/sec; 0.060 sec/batch; 98h:49m:55s remains)
INFO - root - 2019-11-03 23:51:01.503797: step 34240, total loss = 0.68, predict loss = 0.15 (64.2 examples/sec; 0.062 sec/batch; 103h:17m:01s remains)
INFO - root - 2019-11-03 23:51:02.148997: step 34250, total loss = 0.60, predict loss = 0.14 (70.1 examples/sec; 0.057 sec/batch; 94h:36m:59s remains)
INFO - root - 2019-11-03 23:51:02.819183: step 34260, total loss = 0.57, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 95h:53m:12s remains)
INFO - root - 2019-11-03 23:51:03.423583: step 34270, total loss = 0.82, predict loss = 0.20 (68.4 examples/sec; 0.058 sec/batch; 96h:52m:08s remains)
INFO - root - 2019-11-03 23:51:04.037737: step 34280, total loss = 0.64, predict loss = 0.15 (68.4 examples/sec; 0.058 sec/batch; 96h:54m:31s remains)
INFO - root - 2019-11-03 23:51:04.658803: step 34290, total loss = 0.59, predict loss = 0.13 (66.6 examples/sec; 0.060 sec/batch; 99h:29m:08s remains)
INFO - root - 2019-11-03 23:51:05.297179: step 34300, total loss = 0.52, predict loss = 0.13 (80.5 examples/sec; 0.050 sec/batch; 82h:22m:09s remains)
INFO - root - 2019-11-03 23:51:05.916351: step 34310, total loss = 0.59, predict loss = 0.13 (76.3 examples/sec; 0.052 sec/batch; 86h:53m:50s remains)
INFO - root - 2019-11-03 23:51:06.554669: step 34320, total loss = 0.57, predict loss = 0.13 (72.7 examples/sec; 0.055 sec/batch; 91h:11m:39s remains)
INFO - root - 2019-11-03 23:51:07.204173: step 34330, total loss = 0.71, predict loss = 0.17 (64.3 examples/sec; 0.062 sec/batch; 103h:00m:57s remains)
INFO - root - 2019-11-03 23:51:07.847301: step 34340, total loss = 0.54, predict loss = 0.12 (66.0 examples/sec; 0.061 sec/batch; 100h:23m:16s remains)
INFO - root - 2019-11-03 23:51:08.474548: step 34350, total loss = 0.59, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 93h:54m:35s remains)
INFO - root - 2019-11-03 23:51:09.127917: step 34360, total loss = 0.86, predict loss = 0.19 (79.1 examples/sec; 0.051 sec/batch; 83h:48m:43s remains)
INFO - root - 2019-11-03 23:51:09.813478: step 34370, total loss = 0.63, predict loss = 0.14 (62.6 examples/sec; 0.064 sec/batch; 105h:54m:03s remains)
INFO - root - 2019-11-03 23:51:10.412821: step 34380, total loss = 0.65, predict loss = 0.15 (74.2 examples/sec; 0.054 sec/batch; 89h:18m:04s remains)
INFO - root - 2019-11-03 23:51:11.023258: step 34390, total loss = 0.69, predict loss = 0.17 (69.8 examples/sec; 0.057 sec/batch; 94h:56m:05s remains)
INFO - root - 2019-11-03 23:51:11.674606: step 34400, total loss = 0.48, predict loss = 0.10 (75.0 examples/sec; 0.053 sec/batch; 88h:23m:58s remains)
INFO - root - 2019-11-03 23:51:12.325921: step 34410, total loss = 0.40, predict loss = 0.08 (60.8 examples/sec; 0.066 sec/batch; 109h:00m:44s remains)
INFO - root - 2019-11-03 23:51:12.938500: step 34420, total loss = 0.51, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 97h:01m:35s remains)
INFO - root - 2019-11-03 23:51:13.547604: step 34430, total loss = 0.54, predict loss = 0.12 (66.6 examples/sec; 0.060 sec/batch; 99h:30m:58s remains)
INFO - root - 2019-11-03 23:51:14.178350: step 34440, total loss = 0.45, predict loss = 0.10 (79.6 examples/sec; 0.050 sec/batch; 83h:13m:41s remains)
INFO - root - 2019-11-03 23:51:14.817086: step 34450, total loss = 0.53, predict loss = 0.13 (59.6 examples/sec; 0.067 sec/batch; 111h:11m:54s remains)
INFO - root - 2019-11-03 23:51:15.426000: step 34460, total loss = 0.60, predict loss = 0.14 (73.5 examples/sec; 0.054 sec/batch; 90h:13m:33s remains)
INFO - root - 2019-11-03 23:51:16.044360: step 34470, total loss = 0.57, predict loss = 0.13 (75.7 examples/sec; 0.053 sec/batch; 87h:30m:53s remains)
INFO - root - 2019-11-03 23:51:16.669437: step 34480, total loss = 0.57, predict loss = 0.14 (71.2 examples/sec; 0.056 sec/batch; 93h:08m:54s remains)
INFO - root - 2019-11-03 23:51:17.278387: step 34490, total loss = 0.56, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 89h:38m:17s remains)
INFO - root - 2019-11-03 23:51:17.918234: step 34500, total loss = 0.55, predict loss = 0.12 (74.1 examples/sec; 0.054 sec/batch; 89h:30m:06s remains)
INFO - root - 2019-11-03 23:51:18.546529: step 34510, total loss = 0.56, predict loss = 0.14 (72.9 examples/sec; 0.055 sec/batch; 90h:57m:32s remains)
INFO - root - 2019-11-03 23:51:19.159119: step 34520, total loss = 0.54, predict loss = 0.12 (68.1 examples/sec; 0.059 sec/batch; 97h:17m:43s remains)
INFO - root - 2019-11-03 23:51:19.794623: step 34530, total loss = 0.69, predict loss = 0.16 (68.0 examples/sec; 0.059 sec/batch; 97h:26m:09s remains)
INFO - root - 2019-11-03 23:51:20.411343: step 34540, total loss = 0.55, predict loss = 0.11 (68.5 examples/sec; 0.058 sec/batch; 96h:48m:51s remains)
INFO - root - 2019-11-03 23:51:21.164995: step 34550, total loss = 0.71, predict loss = 0.18 (76.7 examples/sec; 0.052 sec/batch; 86h:22m:53s remains)
INFO - root - 2019-11-03 23:51:22.249761: step 34560, total loss = 0.70, predict loss = 0.17 (73.7 examples/sec; 0.054 sec/batch; 89h:56m:28s remains)
INFO - root - 2019-11-03 23:51:22.847400: step 34570, total loss = 0.59, predict loss = 0.14 (74.9 examples/sec; 0.053 sec/batch; 88h:29m:51s remains)
INFO - root - 2019-11-03 23:51:23.476571: step 34580, total loss = 0.67, predict loss = 0.16 (79.9 examples/sec; 0.050 sec/batch; 82h:59m:05s remains)
INFO - root - 2019-11-03 23:51:24.122469: step 34590, total loss = 0.71, predict loss = 0.16 (71.2 examples/sec; 0.056 sec/batch; 93h:07m:24s remains)
INFO - root - 2019-11-03 23:51:24.739671: step 34600, total loss = 0.91, predict loss = 0.22 (70.0 examples/sec; 0.057 sec/batch; 94h:44m:31s remains)
INFO - root - 2019-11-03 23:51:25.379609: step 34610, total loss = 0.84, predict loss = 0.21 (65.3 examples/sec; 0.061 sec/batch; 101h:28m:27s remains)
INFO - root - 2019-11-03 23:51:26.025115: step 34620, total loss = 0.83, predict loss = 0.20 (66.3 examples/sec; 0.060 sec/batch; 99h:57m:31s remains)
INFO - root - 2019-11-03 23:51:26.649229: step 34630, total loss = 0.65, predict loss = 0.16 (77.0 examples/sec; 0.052 sec/batch; 86h:07m:48s remains)
INFO - root - 2019-11-03 23:51:27.250348: step 34640, total loss = 0.71, predict loss = 0.16 (80.6 examples/sec; 0.050 sec/batch; 82h:11m:14s remains)
INFO - root - 2019-11-03 23:51:27.845645: step 34650, total loss = 0.76, predict loss = 0.18 (67.5 examples/sec; 0.059 sec/batch; 98h:13m:07s remains)
INFO - root - 2019-11-03 23:51:28.465564: step 34660, total loss = 0.63, predict loss = 0.13 (71.8 examples/sec; 0.056 sec/batch; 92h:16m:31s remains)
INFO - root - 2019-11-03 23:51:29.098095: step 34670, total loss = 0.83, predict loss = 0.21 (74.1 examples/sec; 0.054 sec/batch; 89h:25m:48s remains)
INFO - root - 2019-11-03 23:51:29.741109: step 34680, total loss = 0.59, predict loss = 0.13 (73.4 examples/sec; 0.054 sec/batch; 90h:14m:44s remains)
INFO - root - 2019-11-03 23:51:30.351198: step 34690, total loss = 0.42, predict loss = 0.09 (66.2 examples/sec; 0.060 sec/batch; 100h:11m:10s remains)
INFO - root - 2019-11-03 23:51:30.997937: step 34700, total loss = 0.52, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 89h:41m:54s remains)
INFO - root - 2019-11-03 23:51:31.646719: step 34710, total loss = 0.50, predict loss = 0.11 (62.4 examples/sec; 0.064 sec/batch; 106h:13m:42s remains)
INFO - root - 2019-11-03 23:51:32.368844: step 34720, total loss = 0.49, predict loss = 0.11 (60.8 examples/sec; 0.066 sec/batch; 109h:02m:56s remains)
INFO - root - 2019-11-03 23:51:33.012793: step 34730, total loss = 0.49, predict loss = 0.11 (69.2 examples/sec; 0.058 sec/batch; 95h:47m:34s remains)
INFO - root - 2019-11-03 23:51:33.642019: step 34740, total loss = 0.51, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 94h:34m:00s remains)
INFO - root - 2019-11-03 23:51:34.259126: step 34750, total loss = 0.54, predict loss = 0.12 (75.3 examples/sec; 0.053 sec/batch; 88h:01m:02s remains)
INFO - root - 2019-11-03 23:51:34.888231: step 34760, total loss = 0.69, predict loss = 0.16 (71.7 examples/sec; 0.056 sec/batch; 92h:26m:36s remains)
INFO - root - 2019-11-03 23:51:35.538025: step 34770, total loss = 0.68, predict loss = 0.16 (70.2 examples/sec; 0.057 sec/batch; 94h:28m:41s remains)
INFO - root - 2019-11-03 23:51:36.197470: step 34780, total loss = 0.56, predict loss = 0.13 (61.8 examples/sec; 0.065 sec/batch; 107h:19m:35s remains)
INFO - root - 2019-11-03 23:51:36.878582: step 34790, total loss = 0.67, predict loss = 0.16 (73.8 examples/sec; 0.054 sec/batch; 89h:48m:31s remains)
INFO - root - 2019-11-03 23:51:37.541465: step 34800, total loss = 0.69, predict loss = 0.16 (69.4 examples/sec; 0.058 sec/batch; 95h:28m:33s remains)
INFO - root - 2019-11-03 23:51:38.189409: step 34810, total loss = 0.91, predict loss = 0.21 (70.0 examples/sec; 0.057 sec/batch; 94h:42m:14s remains)
INFO - root - 2019-11-03 23:51:38.804362: step 34820, total loss = 0.75, predict loss = 0.19 (66.7 examples/sec; 0.060 sec/batch; 99h:19m:23s remains)
INFO - root - 2019-11-03 23:51:39.488247: step 34830, total loss = 0.75, predict loss = 0.17 (62.3 examples/sec; 0.064 sec/batch; 106h:20m:19s remains)
INFO - root - 2019-11-03 23:51:40.152091: step 34840, total loss = 0.62, predict loss = 0.15 (68.2 examples/sec; 0.059 sec/batch; 97h:07m:46s remains)
INFO - root - 2019-11-03 23:51:40.796272: step 34850, total loss = 0.69, predict loss = 0.17 (66.6 examples/sec; 0.060 sec/batch; 99h:34m:44s remains)
INFO - root - 2019-11-03 23:51:41.512699: step 34860, total loss = 0.61, predict loss = 0.15 (60.4 examples/sec; 0.066 sec/batch; 109h:48m:43s remains)
INFO - root - 2019-11-03 23:51:42.280773: step 34870, total loss = 0.71, predict loss = 0.17 (48.8 examples/sec; 0.082 sec/batch; 135h:55m:19s remains)
INFO - root - 2019-11-03 23:51:42.900895: step 34880, total loss = 0.66, predict loss = 0.16 (76.3 examples/sec; 0.052 sec/batch; 86h:51m:50s remains)
INFO - root - 2019-11-03 23:51:43.549511: step 34890, total loss = 0.61, predict loss = 0.14 (70.4 examples/sec; 0.057 sec/batch; 94h:11m:05s remains)
INFO - root - 2019-11-03 23:51:44.210571: step 34900, total loss = 0.59, predict loss = 0.14 (61.0 examples/sec; 0.066 sec/batch; 108h:35m:54s remains)
INFO - root - 2019-11-03 23:51:44.859641: step 34910, total loss = 0.66, predict loss = 0.16 (65.1 examples/sec; 0.061 sec/batch; 101h:45m:14s remains)
INFO - root - 2019-11-03 23:51:45.487076: step 34920, total loss = 0.53, predict loss = 0.12 (84.1 examples/sec; 0.048 sec/batch; 78h:50m:02s remains)
INFO - root - 2019-11-03 23:51:46.148471: step 34930, total loss = 0.65, predict loss = 0.15 (67.5 examples/sec; 0.059 sec/batch; 98h:15m:01s remains)
INFO - root - 2019-11-03 23:51:46.860237: step 34940, total loss = 0.73, predict loss = 0.16 (61.1 examples/sec; 0.066 sec/batch; 108h:31m:54s remains)
INFO - root - 2019-11-03 23:51:47.576523: step 34950, total loss = 0.53, predict loss = 0.12 (62.3 examples/sec; 0.064 sec/batch; 106h:20m:41s remains)
INFO - root - 2019-11-03 23:51:48.246586: step 34960, total loss = 0.67, predict loss = 0.16 (73.8 examples/sec; 0.054 sec/batch; 89h:45m:44s remains)
INFO - root - 2019-11-03 23:51:48.856270: step 34970, total loss = 0.67, predict loss = 0.15 (78.2 examples/sec; 0.051 sec/batch; 84h:45m:47s remains)
INFO - root - 2019-11-03 23:51:49.482798: step 34980, total loss = 0.63, predict loss = 0.15 (69.6 examples/sec; 0.057 sec/batch; 95h:12m:48s remains)
INFO - root - 2019-11-03 23:51:50.102630: step 34990, total loss = 0.60, predict loss = 0.14 (76.1 examples/sec; 0.053 sec/batch; 87h:04m:04s remains)
INFO - root - 2019-11-03 23:51:50.750708: step 35000, total loss = 0.64, predict loss = 0.15 (69.2 examples/sec; 0.058 sec/batch; 95h:47m:38s remains)
INFO - root - 2019-11-03 23:51:51.384034: step 35010, total loss = 0.57, predict loss = 0.12 (77.8 examples/sec; 0.051 sec/batch; 85h:08m:17s remains)
INFO - root - 2019-11-03 23:51:52.023545: step 35020, total loss = 0.47, predict loss = 0.10 (63.1 examples/sec; 0.063 sec/batch; 105h:02m:21s remains)
INFO - root - 2019-11-03 23:51:52.667316: step 35030, total loss = 0.59, predict loss = 0.14 (72.9 examples/sec; 0.055 sec/batch; 90h:57m:46s remains)
INFO - root - 2019-11-03 23:51:53.350914: step 35040, total loss = 0.64, predict loss = 0.15 (65.6 examples/sec; 0.061 sec/batch; 101h:01m:18s remains)
INFO - root - 2019-11-03 23:51:53.996345: step 35050, total loss = 0.57, predict loss = 0.13 (79.4 examples/sec; 0.050 sec/batch; 83h:28m:32s remains)
INFO - root - 2019-11-03 23:51:54.660316: step 35060, total loss = 0.52, predict loss = 0.12 (74.8 examples/sec; 0.054 sec/batch; 88h:39m:10s remains)
INFO - root - 2019-11-03 23:51:55.310270: step 35070, total loss = 0.74, predict loss = 0.17 (68.9 examples/sec; 0.058 sec/batch; 96h:08m:34s remains)
INFO - root - 2019-11-03 23:51:55.935814: step 35080, total loss = 0.60, predict loss = 0.14 (73.4 examples/sec; 0.055 sec/batch; 90h:19m:17s remains)
INFO - root - 2019-11-03 23:51:56.532315: step 35090, total loss = 0.61, predict loss = 0.14 (75.9 examples/sec; 0.053 sec/batch; 87h:21m:31s remains)
INFO - root - 2019-11-03 23:51:57.151761: step 35100, total loss = 0.59, predict loss = 0.13 (69.4 examples/sec; 0.058 sec/batch; 95h:28m:42s remains)
INFO - root - 2019-11-03 23:51:57.752549: step 35110, total loss = 0.54, predict loss = 0.12 (76.2 examples/sec; 0.053 sec/batch; 87h:00m:23s remains)
INFO - root - 2019-11-03 23:51:58.380500: step 35120, total loss = 0.66, predict loss = 0.14 (71.0 examples/sec; 0.056 sec/batch; 93h:20m:59s remains)
INFO - root - 2019-11-03 23:51:58.977847: step 35130, total loss = 0.69, predict loss = 0.17 (68.2 examples/sec; 0.059 sec/batch; 97h:14m:03s remains)
INFO - root - 2019-11-03 23:51:59.592544: step 35140, total loss = 0.66, predict loss = 0.16 (65.4 examples/sec; 0.061 sec/batch; 101h:20m:19s remains)
INFO - root - 2019-11-03 23:52:00.262889: step 35150, total loss = 0.56, predict loss = 0.13 (71.1 examples/sec; 0.056 sec/batch; 93h:13m:59s remains)
INFO - root - 2019-11-03 23:52:00.908932: step 35160, total loss = 0.81, predict loss = 0.19 (76.7 examples/sec; 0.052 sec/batch; 86h:25m:07s remains)
INFO - root - 2019-11-03 23:52:01.549514: step 35170, total loss = 0.51, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 96h:06m:30s remains)
INFO - root - 2019-11-03 23:52:02.177200: step 35180, total loss = 0.91, predict loss = 0.21 (71.8 examples/sec; 0.056 sec/batch; 92h:17m:08s remains)
INFO - root - 2019-11-03 23:52:02.806292: step 35190, total loss = 0.89, predict loss = 0.22 (63.3 examples/sec; 0.063 sec/batch; 104h:38m:08s remains)
INFO - root - 2019-11-03 23:52:03.496142: step 35200, total loss = 0.81, predict loss = 0.19 (53.1 examples/sec; 0.075 sec/batch; 124h:41m:54s remains)
INFO - root - 2019-11-03 23:52:04.229136: step 35210, total loss = 0.71, predict loss = 0.17 (54.0 examples/sec; 0.074 sec/batch; 122h:49m:07s remains)
INFO - root - 2019-11-03 23:52:05.022186: step 35220, total loss = 0.64, predict loss = 0.15 (56.3 examples/sec; 0.071 sec/batch; 117h:43m:32s remains)
INFO - root - 2019-11-03 23:52:05.796989: step 35230, total loss = 0.86, predict loss = 0.20 (70.6 examples/sec; 0.057 sec/batch; 93h:49m:28s remains)
INFO - root - 2019-11-03 23:52:06.420688: step 35240, total loss = 1.08, predict loss = 0.28 (77.0 examples/sec; 0.052 sec/batch; 86h:06m:58s remains)
INFO - root - 2019-11-03 23:52:07.099266: step 35250, total loss = 0.83, predict loss = 0.19 (65.9 examples/sec; 0.061 sec/batch; 100h:31m:32s remains)
INFO - root - 2019-11-03 23:52:07.688816: step 35260, total loss = 0.91, predict loss = 0.22 (79.3 examples/sec; 0.050 sec/batch; 83h:32m:18s remains)
INFO - root - 2019-11-03 23:52:08.285621: step 35270, total loss = 0.74, predict loss = 0.18 (71.0 examples/sec; 0.056 sec/batch; 93h:17m:26s remains)
INFO - root - 2019-11-03 23:52:08.908513: step 35280, total loss = 0.98, predict loss = 0.25 (72.7 examples/sec; 0.055 sec/batch; 91h:07m:35s remains)
INFO - root - 2019-11-03 23:52:09.541828: step 35290, total loss = 0.67, predict loss = 0.16 (77.0 examples/sec; 0.052 sec/batch; 86h:01m:24s remains)
INFO - root - 2019-11-03 23:52:10.171562: step 35300, total loss = 0.66, predict loss = 0.16 (69.7 examples/sec; 0.057 sec/batch; 95h:01m:54s remains)
INFO - root - 2019-11-03 23:52:10.804033: step 35310, total loss = 0.52, predict loss = 0.11 (60.8 examples/sec; 0.066 sec/batch; 108h:57m:03s remains)
INFO - root - 2019-11-03 23:52:11.443897: step 35320, total loss = 0.75, predict loss = 0.17 (77.8 examples/sec; 0.051 sec/batch; 85h:09m:47s remains)
INFO - root - 2019-11-03 23:52:12.094736: step 35330, total loss = 0.51, predict loss = 0.12 (68.5 examples/sec; 0.058 sec/batch; 96h:48m:55s remains)
INFO - root - 2019-11-03 23:52:12.767016: step 35340, total loss = 0.73, predict loss = 0.17 (57.0 examples/sec; 0.070 sec/batch; 116h:13m:40s remains)
INFO - root - 2019-11-03 23:52:13.435832: step 35350, total loss = 0.58, predict loss = 0.14 (66.7 examples/sec; 0.060 sec/batch; 99h:24m:25s remains)
INFO - root - 2019-11-03 23:52:14.114943: step 35360, total loss = 0.40, predict loss = 0.09 (68.6 examples/sec; 0.058 sec/batch; 96h:33m:44s remains)
INFO - root - 2019-11-03 23:52:14.763954: step 35370, total loss = 0.66, predict loss = 0.15 (71.9 examples/sec; 0.056 sec/batch; 92h:12m:59s remains)
INFO - root - 2019-11-03 23:52:15.410122: step 35380, total loss = 0.60, predict loss = 0.14 (61.5 examples/sec; 0.065 sec/batch; 107h:47m:56s remains)
INFO - root - 2019-11-03 23:52:16.071645: step 35390, total loss = 0.61, predict loss = 0.15 (66.3 examples/sec; 0.060 sec/batch; 99h:57m:28s remains)
INFO - root - 2019-11-03 23:52:16.716106: step 35400, total loss = 0.56, predict loss = 0.13 (78.7 examples/sec; 0.051 sec/batch; 84h:14m:32s remains)
INFO - root - 2019-11-03 23:52:17.344069: step 35410, total loss = 0.59, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:48m:22s remains)
INFO - root - 2019-11-03 23:52:18.015023: step 35420, total loss = 0.75, predict loss = 0.18 (68.5 examples/sec; 0.058 sec/batch; 96h:41m:39s remains)
INFO - root - 2019-11-03 23:52:18.614297: step 35430, total loss = 0.81, predict loss = 0.19 (75.7 examples/sec; 0.053 sec/batch; 87h:34m:37s remains)
INFO - root - 2019-11-03 23:52:19.175188: step 35440, total loss = 0.53, predict loss = 0.12 (97.2 examples/sec; 0.041 sec/batch; 68h:10m:56s remains)
INFO - root - 2019-11-03 23:52:19.645005: step 35450, total loss = 0.77, predict loss = 0.17 (88.9 examples/sec; 0.045 sec/batch; 74h:32m:43s remains)
INFO - root - 2019-11-03 23:52:20.710625: step 35460, total loss = 0.30, predict loss = 0.06 (73.6 examples/sec; 0.054 sec/batch; 89h:59m:23s remains)
INFO - root - 2019-11-03 23:52:21.299059: step 35470, total loss = 0.53, predict loss = 0.11 (78.5 examples/sec; 0.051 sec/batch; 84h:23m:22s remains)
INFO - root - 2019-11-03 23:52:21.935076: step 35480, total loss = 0.46, predict loss = 0.09 (72.6 examples/sec; 0.055 sec/batch; 91h:13m:55s remains)
INFO - root - 2019-11-03 23:52:22.527067: step 35490, total loss = 0.71, predict loss = 0.17 (83.2 examples/sec; 0.048 sec/batch; 79h:37m:06s remains)
INFO - root - 2019-11-03 23:52:23.132251: step 35500, total loss = 0.64, predict loss = 0.14 (72.4 examples/sec; 0.055 sec/batch; 91h:34m:26s remains)
INFO - root - 2019-11-03 23:52:23.761880: step 35510, total loss = 0.78, predict loss = 0.18 (64.7 examples/sec; 0.062 sec/batch; 102h:23m:39s remains)
INFO - root - 2019-11-03 23:52:24.440044: step 35520, total loss = 0.78, predict loss = 0.18 (67.0 examples/sec; 0.060 sec/batch; 98h:52m:00s remains)
INFO - root - 2019-11-03 23:52:25.121304: step 35530, total loss = 0.51, predict loss = 0.11 (63.8 examples/sec; 0.063 sec/batch; 103h:49m:26s remains)
INFO - root - 2019-11-03 23:52:25.731697: step 35540, total loss = 0.60, predict loss = 0.13 (74.3 examples/sec; 0.054 sec/batch; 89h:09m:16s remains)
INFO - root - 2019-11-03 23:52:26.348619: step 35550, total loss = 0.80, predict loss = 0.16 (73.7 examples/sec; 0.054 sec/batch; 89h:53m:36s remains)
INFO - root - 2019-11-03 23:52:26.960169: step 35560, total loss = 0.95, predict loss = 0.22 (68.2 examples/sec; 0.059 sec/batch; 97h:12m:01s remains)
INFO - root - 2019-11-03 23:52:27.607670: step 35570, total loss = 1.09, predict loss = 0.27 (78.1 examples/sec; 0.051 sec/batch; 84h:53m:59s remains)
INFO - root - 2019-11-03 23:52:28.239702: step 35580, total loss = 0.58, predict loss = 0.13 (68.5 examples/sec; 0.058 sec/batch; 96h:42m:52s remains)
INFO - root - 2019-11-03 23:52:28.877998: step 35590, total loss = 0.64, predict loss = 0.13 (72.1 examples/sec; 0.055 sec/batch; 91h:52m:29s remains)
INFO - root - 2019-11-03 23:52:29.502803: step 35600, total loss = 0.73, predict loss = 0.18 (72.2 examples/sec; 0.055 sec/batch; 91h:47m:18s remains)
INFO - root - 2019-11-03 23:52:30.165695: step 35610, total loss = 0.57, predict loss = 0.13 (63.1 examples/sec; 0.063 sec/batch; 104h:59m:29s remains)
INFO - root - 2019-11-03 23:52:30.804317: step 35620, total loss = 0.70, predict loss = 0.16 (71.5 examples/sec; 0.056 sec/batch; 92h:40m:52s remains)
INFO - root - 2019-11-03 23:52:31.496943: step 35630, total loss = 0.58, predict loss = 0.13 (64.3 examples/sec; 0.062 sec/batch; 103h:05m:23s remains)
INFO - root - 2019-11-03 23:52:32.128770: step 35640, total loss = 0.65, predict loss = 0.15 (72.6 examples/sec; 0.055 sec/batch; 91h:18m:05s remains)
INFO - root - 2019-11-03 23:52:32.782617: step 35650, total loss = 0.51, predict loss = 0.10 (68.1 examples/sec; 0.059 sec/batch; 97h:17m:37s remains)
INFO - root - 2019-11-03 23:52:33.395189: step 35660, total loss = 0.65, predict loss = 0.14 (81.5 examples/sec; 0.049 sec/batch; 81h:21m:11s remains)
INFO - root - 2019-11-03 23:52:34.027467: step 35670, total loss = 0.65, predict loss = 0.14 (64.9 examples/sec; 0.062 sec/batch; 102h:07m:01s remains)
INFO - root - 2019-11-03 23:52:34.678568: step 35680, total loss = 0.49, predict loss = 0.11 (72.9 examples/sec; 0.055 sec/batch; 90h:54m:37s remains)
INFO - root - 2019-11-03 23:52:35.361860: step 35690, total loss = 0.81, predict loss = 0.19 (68.0 examples/sec; 0.059 sec/batch; 97h:25m:42s remains)
INFO - root - 2019-11-03 23:52:35.979331: step 35700, total loss = 0.54, predict loss = 0.12 (85.5 examples/sec; 0.047 sec/batch; 77h:28m:53s remains)
INFO - root - 2019-11-03 23:52:36.606089: step 35710, total loss = 0.76, predict loss = 0.16 (73.1 examples/sec; 0.055 sec/batch; 90h:37m:41s remains)
INFO - root - 2019-11-03 23:52:37.270467: step 35720, total loss = 0.66, predict loss = 0.15 (68.9 examples/sec; 0.058 sec/batch; 96h:13m:49s remains)
INFO - root - 2019-11-03 23:52:37.908876: step 35730, total loss = 0.65, predict loss = 0.15 (77.8 examples/sec; 0.051 sec/batch; 85h:10m:28s remains)
INFO - root - 2019-11-03 23:52:38.513611: step 35740, total loss = 0.76, predict loss = 0.16 (74.4 examples/sec; 0.054 sec/batch; 89h:05m:05s remains)
INFO - root - 2019-11-03 23:52:39.136939: step 35750, total loss = 0.79, predict loss = 0.17 (74.8 examples/sec; 0.053 sec/batch; 88h:34m:59s remains)
INFO - root - 2019-11-03 23:52:39.815925: step 35760, total loss = 0.69, predict loss = 0.17 (67.0 examples/sec; 0.060 sec/batch; 98h:50m:40s remains)
INFO - root - 2019-11-03 23:52:40.460208: step 35770, total loss = 0.74, predict loss = 0.17 (76.8 examples/sec; 0.052 sec/batch; 86h:20m:25s remains)
INFO - root - 2019-11-03 23:52:41.110393: step 35780, total loss = 0.63, predict loss = 0.14 (66.1 examples/sec; 0.060 sec/batch; 100h:11m:32s remains)
INFO - root - 2019-11-03 23:52:41.760506: step 35790, total loss = 0.61, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 97h:17m:04s remains)
INFO - root - 2019-11-03 23:52:42.437568: step 35800, total loss = 0.74, predict loss = 0.15 (64.3 examples/sec; 0.062 sec/batch; 103h:03m:27s remains)
INFO - root - 2019-11-03 23:52:43.056841: step 35810, total loss = 0.67, predict loss = 0.13 (75.3 examples/sec; 0.053 sec/batch; 88h:00m:27s remains)
INFO - root - 2019-11-03 23:52:43.692668: step 35820, total loss = 0.66, predict loss = 0.15 (70.2 examples/sec; 0.057 sec/batch; 94h:23m:17s remains)
INFO - root - 2019-11-03 23:52:44.285157: step 35830, total loss = 0.58, predict loss = 0.13 (75.5 examples/sec; 0.053 sec/batch; 87h:47m:02s remains)
INFO - root - 2019-11-03 23:52:44.911572: step 35840, total loss = 0.49, predict loss = 0.10 (75.9 examples/sec; 0.053 sec/batch; 87h:20m:40s remains)
INFO - root - 2019-11-03 23:52:45.540831: step 35850, total loss = 0.65, predict loss = 0.15 (64.1 examples/sec; 0.062 sec/batch; 103h:21m:05s remains)
INFO - root - 2019-11-03 23:52:46.168037: step 35860, total loss = 0.75, predict loss = 0.16 (68.7 examples/sec; 0.058 sec/batch; 96h:23m:56s remains)
INFO - root - 2019-11-03 23:52:46.787324: step 35870, total loss = 0.70, predict loss = 0.17 (73.9 examples/sec; 0.054 sec/batch; 89h:41m:51s remains)
INFO - root - 2019-11-03 23:52:47.425247: step 35880, total loss = 0.74, predict loss = 0.17 (83.1 examples/sec; 0.048 sec/batch; 79h:46m:24s remains)
INFO - root - 2019-11-03 23:52:48.062136: step 35890, total loss = 0.57, predict loss = 0.13 (65.9 examples/sec; 0.061 sec/batch; 100h:37m:15s remains)
INFO - root - 2019-11-03 23:52:48.734080: step 35900, total loss = 0.45, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 96h:39m:14s remains)
INFO - root - 2019-11-03 23:52:49.361540: step 35910, total loss = 0.59, predict loss = 0.13 (81.7 examples/sec; 0.049 sec/batch; 81h:07m:48s remains)
INFO - root - 2019-11-03 23:52:49.995216: step 35920, total loss = 0.51, predict loss = 0.11 (77.1 examples/sec; 0.052 sec/batch; 85h:57m:00s remains)
INFO - root - 2019-11-03 23:52:50.615062: step 35930, total loss = 0.54, predict loss = 0.13 (81.0 examples/sec; 0.049 sec/batch; 81h:51m:40s remains)
INFO - root - 2019-11-03 23:52:51.241231: step 35940, total loss = 0.64, predict loss = 0.16 (67.2 examples/sec; 0.060 sec/batch; 98h:34m:52s remains)
INFO - root - 2019-11-03 23:52:51.851237: step 35950, total loss = 0.65, predict loss = 0.15 (70.4 examples/sec; 0.057 sec/batch; 94h:11m:01s remains)
INFO - root - 2019-11-03 23:52:52.472849: step 35960, total loss = 0.79, predict loss = 0.19 (70.8 examples/sec; 0.056 sec/batch; 93h:33m:23s remains)
INFO - root - 2019-11-03 23:52:53.119815: step 35970, total loss = 0.69, predict loss = 0.16 (70.3 examples/sec; 0.057 sec/batch; 94h:17m:54s remains)
INFO - root - 2019-11-03 23:52:53.733315: step 35980, total loss = 0.88, predict loss = 0.21 (68.2 examples/sec; 0.059 sec/batch; 97h:06m:57s remains)
INFO - root - 2019-11-03 23:52:54.341189: step 35990, total loss = 0.92, predict loss = 0.22 (67.9 examples/sec; 0.059 sec/batch; 97h:35m:19s remains)
INFO - root - 2019-11-03 23:52:54.963857: step 36000, total loss = 0.64, predict loss = 0.15 (77.9 examples/sec; 0.051 sec/batch; 85h:01m:27s remains)
INFO - root - 2019-11-03 23:52:55.578085: step 36010, total loss = 0.77, predict loss = 0.19 (77.7 examples/sec; 0.051 sec/batch; 85h:16m:33s remains)
INFO - root - 2019-11-03 23:52:56.213556: step 36020, total loss = 0.35, predict loss = 0.08 (83.9 examples/sec; 0.048 sec/batch; 79h:00m:43s remains)
INFO - root - 2019-11-03 23:52:56.824740: step 36030, total loss = 0.48, predict loss = 0.12 (71.6 examples/sec; 0.056 sec/batch; 92h:36m:00s remains)
INFO - root - 2019-11-03 23:52:57.447547: step 36040, total loss = 0.46, predict loss = 0.10 (79.3 examples/sec; 0.050 sec/batch; 83h:34m:12s remains)
INFO - root - 2019-11-03 23:52:58.054419: step 36050, total loss = 0.38, predict loss = 0.08 (68.5 examples/sec; 0.058 sec/batch; 96h:47m:50s remains)
INFO - root - 2019-11-03 23:52:58.661947: step 36060, total loss = 0.46, predict loss = 0.10 (63.5 examples/sec; 0.063 sec/batch; 104h:23m:48s remains)
INFO - root - 2019-11-03 23:52:59.313621: step 36070, total loss = 0.41, predict loss = 0.08 (71.0 examples/sec; 0.056 sec/batch; 93h:16m:35s remains)
INFO - root - 2019-11-03 23:52:59.941279: step 36080, total loss = 0.59, predict loss = 0.14 (65.9 examples/sec; 0.061 sec/batch; 100h:36m:38s remains)
INFO - root - 2019-11-03 23:53:00.589094: step 36090, total loss = 0.37, predict loss = 0.08 (65.3 examples/sec; 0.061 sec/batch; 101h:31m:08s remains)
INFO - root - 2019-11-03 23:53:01.240652: step 36100, total loss = 0.53, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 86h:22m:43s remains)
INFO - root - 2019-11-03 23:53:01.972309: step 36110, total loss = 0.42, predict loss = 0.08 (65.3 examples/sec; 0.061 sec/batch; 101h:25m:10s remains)
INFO - root - 2019-11-03 23:53:02.750037: step 36120, total loss = 0.53, predict loss = 0.14 (52.7 examples/sec; 0.076 sec/batch; 125h:48m:43s remains)
INFO - root - 2019-11-03 23:53:03.575670: step 36130, total loss = 0.54, predict loss = 0.12 (54.9 examples/sec; 0.073 sec/batch; 120h:42m:30s remains)
INFO - root - 2019-11-03 23:53:04.276315: step 36140, total loss = 0.45, predict loss = 0.11 (64.2 examples/sec; 0.062 sec/batch; 103h:10m:44s remains)
INFO - root - 2019-11-03 23:53:04.942439: step 36150, total loss = 0.67, predict loss = 0.16 (58.4 examples/sec; 0.069 sec/batch; 113h:31m:34s remains)
INFO - root - 2019-11-03 23:53:05.573351: step 36160, total loss = 0.79, predict loss = 0.19 (76.7 examples/sec; 0.052 sec/batch; 86h:21m:12s remains)
INFO - root - 2019-11-03 23:53:06.202807: step 36170, total loss = 0.60, predict loss = 0.13 (71.5 examples/sec; 0.056 sec/batch; 92h:43m:47s remains)
INFO - root - 2019-11-03 23:53:06.795401: step 36180, total loss = 0.61, predict loss = 0.14 (82.5 examples/sec; 0.048 sec/batch; 80h:16m:39s remains)
INFO - root - 2019-11-03 23:53:07.457911: step 36190, total loss = 0.64, predict loss = 0.16 (67.9 examples/sec; 0.059 sec/batch; 97h:38m:52s remains)
INFO - root - 2019-11-03 23:53:08.089977: step 36200, total loss = 0.74, predict loss = 0.18 (65.2 examples/sec; 0.061 sec/batch; 101h:38m:52s remains)
INFO - root - 2019-11-03 23:53:08.820793: step 36210, total loss = 0.71, predict loss = 0.16 (64.3 examples/sec; 0.062 sec/batch; 103h:01m:07s remains)
INFO - root - 2019-11-03 23:53:09.454965: step 36220, total loss = 0.50, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 93h:49m:29s remains)
INFO - root - 2019-11-03 23:53:10.092886: step 36230, total loss = 0.58, predict loss = 0.13 (77.1 examples/sec; 0.052 sec/batch; 85h:53m:39s remains)
INFO - root - 2019-11-03 23:53:10.737540: step 36240, total loss = 0.71, predict loss = 0.17 (57.4 examples/sec; 0.070 sec/batch; 115h:25m:10s remains)
INFO - root - 2019-11-03 23:53:11.426837: step 36250, total loss = 0.70, predict loss = 0.17 (76.0 examples/sec; 0.053 sec/batch; 87h:09m:08s remains)
INFO - root - 2019-11-03 23:53:12.049589: step 36260, total loss = 0.60, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 98h:28m:22s remains)
INFO - root - 2019-11-03 23:53:12.657521: step 36270, total loss = 0.46, predict loss = 0.11 (58.2 examples/sec; 0.069 sec/batch; 113h:55m:11s remains)
INFO - root - 2019-11-03 23:53:13.287147: step 36280, total loss = 0.38, predict loss = 0.08 (72.5 examples/sec; 0.055 sec/batch; 91h:22m:51s remains)
INFO - root - 2019-11-03 23:53:13.914191: step 36290, total loss = 0.33, predict loss = 0.06 (67.1 examples/sec; 0.060 sec/batch; 98h:47m:05s remains)
INFO - root - 2019-11-03 23:53:14.610701: step 36300, total loss = 0.43, predict loss = 0.09 (69.0 examples/sec; 0.058 sec/batch; 96h:00m:57s remains)
INFO - root - 2019-11-03 23:53:15.281714: step 36310, total loss = 0.31, predict loss = 0.06 (68.6 examples/sec; 0.058 sec/batch; 96h:36m:29s remains)
INFO - root - 2019-11-03 23:53:15.994499: step 36320, total loss = 0.34, predict loss = 0.07 (47.1 examples/sec; 0.085 sec/batch; 140h:32m:37s remains)
INFO - root - 2019-11-03 23:53:16.745137: step 36330, total loss = 0.46, predict loss = 0.10 (76.6 examples/sec; 0.052 sec/batch; 86h:28m:08s remains)
INFO - root - 2019-11-03 23:53:17.324135: step 36340, total loss = 0.44, predict loss = 0.09 (82.4 examples/sec; 0.049 sec/batch; 80h:22m:37s remains)
INFO - root - 2019-11-03 23:53:17.930473: step 36350, total loss = 0.40, predict loss = 0.09 (70.6 examples/sec; 0.057 sec/batch; 93h:54m:27s remains)
INFO - root - 2019-11-03 23:53:18.552457: step 36360, total loss = 0.56, predict loss = 0.13 (70.7 examples/sec; 0.057 sec/batch; 93h:44m:25s remains)
INFO - root - 2019-11-03 23:53:19.244702: step 36370, total loss = 0.61, predict loss = 0.13 (74.8 examples/sec; 0.053 sec/batch; 88h:33m:47s remains)
INFO - root - 2019-11-03 23:53:19.895482: step 36380, total loss = 0.64, predict loss = 0.14 (57.2 examples/sec; 0.070 sec/batch; 115h:47m:14s remains)
INFO - root - 2019-11-03 23:53:20.594215: step 36390, total loss = 0.40, predict loss = 0.09 (79.0 examples/sec; 0.051 sec/batch; 83h:54m:11s remains)
INFO - root - 2019-11-03 23:53:21.190889: step 36400, total loss = 0.39, predict loss = 0.08 (79.3 examples/sec; 0.050 sec/batch; 83h:31m:30s remains)
INFO - root - 2019-11-03 23:53:21.831940: step 36410, total loss = 0.47, predict loss = 0.10 (77.3 examples/sec; 0.052 sec/batch; 85h:43m:55s remains)
INFO - root - 2019-11-03 23:53:22.478931: step 36420, total loss = 0.34, predict loss = 0.07 (68.8 examples/sec; 0.058 sec/batch; 96h:14m:52s remains)
INFO - root - 2019-11-03 23:53:23.113826: step 36430, total loss = 0.57, predict loss = 0.13 (70.1 examples/sec; 0.057 sec/batch; 94h:35m:03s remains)
INFO - root - 2019-11-03 23:53:23.744190: step 36440, total loss = 0.52, predict loss = 0.12 (78.5 examples/sec; 0.051 sec/batch; 84h:22m:46s remains)
INFO - root - 2019-11-03 23:53:24.380427: step 36450, total loss = 0.58, predict loss = 0.13 (64.3 examples/sec; 0.062 sec/batch; 103h:01m:46s remains)
INFO - root - 2019-11-03 23:53:25.007683: step 36460, total loss = 0.66, predict loss = 0.16 (76.9 examples/sec; 0.052 sec/batch; 86h:12m:48s remains)
INFO - root - 2019-11-03 23:53:25.723991: step 36470, total loss = 0.68, predict loss = 0.17 (62.9 examples/sec; 0.064 sec/batch; 105h:16m:35s remains)
INFO - root - 2019-11-03 23:53:26.421428: step 36480, total loss = 0.83, predict loss = 0.20 (64.6 examples/sec; 0.062 sec/batch; 102h:36m:55s remains)
INFO - root - 2019-11-03 23:53:27.076890: step 36490, total loss = 0.55, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 90h:35m:02s remains)
INFO - root - 2019-11-03 23:53:27.773962: step 36500, total loss = 0.57, predict loss = 0.13 (70.8 examples/sec; 0.057 sec/batch; 93h:37m:41s remains)
INFO - root - 2019-11-03 23:53:28.446703: step 36510, total loss = 0.52, predict loss = 0.13 (70.0 examples/sec; 0.057 sec/batch; 94h:37m:05s remains)
INFO - root - 2019-11-03 23:53:29.132490: step 36520, total loss = 0.52, predict loss = 0.12 (60.7 examples/sec; 0.066 sec/batch; 109h:08m:31s remains)
INFO - root - 2019-11-03 23:53:29.860239: step 36530, total loss = 0.32, predict loss = 0.07 (71.7 examples/sec; 0.056 sec/batch; 92h:22m:46s remains)
INFO - root - 2019-11-03 23:53:30.459787: step 36540, total loss = 0.38, predict loss = 0.09 (73.4 examples/sec; 0.054 sec/batch; 90h:15m:52s remains)
INFO - root - 2019-11-03 23:53:31.058112: step 36550, total loss = 0.31, predict loss = 0.06 (82.2 examples/sec; 0.049 sec/batch; 80h:35m:19s remains)
INFO - root - 2019-11-03 23:53:31.691980: step 36560, total loss = 0.77, predict loss = 0.18 (65.5 examples/sec; 0.061 sec/batch; 101h:07m:16s remains)
INFO - root - 2019-11-03 23:53:32.310553: step 36570, total loss = 0.50, predict loss = 0.11 (76.8 examples/sec; 0.052 sec/batch; 86h:17m:21s remains)
INFO - root - 2019-11-03 23:53:32.920195: step 36580, total loss = 0.57, predict loss = 0.14 (71.4 examples/sec; 0.056 sec/batch; 92h:51m:35s remains)
INFO - root - 2019-11-03 23:53:33.640699: step 36590, total loss = 0.53, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 95h:25m:01s remains)
INFO - root - 2019-11-03 23:53:34.289284: step 36600, total loss = 0.53, predict loss = 0.12 (69.7 examples/sec; 0.057 sec/batch; 94h:59m:56s remains)
INFO - root - 2019-11-03 23:53:34.901187: step 36610, total loss = 0.70, predict loss = 0.16 (73.6 examples/sec; 0.054 sec/batch; 90h:03m:32s remains)
INFO - root - 2019-11-03 23:53:35.570608: step 36620, total loss = 0.57, predict loss = 0.14 (70.5 examples/sec; 0.057 sec/batch; 93h:59m:48s remains)
INFO - root - 2019-11-03 23:53:36.243789: step 36630, total loss = 0.66, predict loss = 0.15 (61.5 examples/sec; 0.065 sec/batch; 107h:42m:30s remains)
INFO - root - 2019-11-03 23:53:36.870565: step 36640, total loss = 0.54, predict loss = 0.13 (73.0 examples/sec; 0.055 sec/batch; 90h:45m:12s remains)
INFO - root - 2019-11-03 23:53:37.509888: step 36650, total loss = 0.41, predict loss = 0.08 (74.3 examples/sec; 0.054 sec/batch; 89h:08m:46s remains)
INFO - root - 2019-11-03 23:53:38.136475: step 36660, total loss = 0.59, predict loss = 0.13 (76.8 examples/sec; 0.052 sec/batch; 86h:14m:10s remains)
INFO - root - 2019-11-03 23:53:38.777098: step 36670, total loss = 0.58, predict loss = 0.13 (72.3 examples/sec; 0.055 sec/batch; 91h:39m:32s remains)
INFO - root - 2019-11-03 23:53:39.408652: step 36680, total loss = 0.61, predict loss = 0.14 (67.2 examples/sec; 0.060 sec/batch; 98h:35m:59s remains)
INFO - root - 2019-11-03 23:53:40.036101: step 36690, total loss = 0.62, predict loss = 0.14 (75.7 examples/sec; 0.053 sec/batch; 87h:33m:46s remains)
INFO - root - 2019-11-03 23:53:40.728379: step 36700, total loss = 0.52, predict loss = 0.11 (67.3 examples/sec; 0.059 sec/batch; 98h:30m:51s remains)
INFO - root - 2019-11-03 23:53:41.361525: step 36710, total loss = 0.79, predict loss = 0.19 (61.9 examples/sec; 0.065 sec/batch; 107h:03m:50s remains)
INFO - root - 2019-11-03 23:53:42.086219: step 36720, total loss = 0.75, predict loss = 0.19 (61.0 examples/sec; 0.066 sec/batch; 108h:33m:22s remains)
INFO - root - 2019-11-03 23:53:42.839947: step 36730, total loss = 0.58, predict loss = 0.13 (65.5 examples/sec; 0.061 sec/batch; 101h:13m:34s remains)
INFO - root - 2019-11-03 23:53:43.566006: step 36740, total loss = 0.40, predict loss = 0.08 (59.4 examples/sec; 0.067 sec/batch; 111h:30m:31s remains)
INFO - root - 2019-11-03 23:53:44.387195: step 36750, total loss = 0.67, predict loss = 0.16 (44.0 examples/sec; 0.091 sec/batch; 150h:30m:55s remains)
INFO - root - 2019-11-03 23:53:45.074133: step 36760, total loss = 0.67, predict loss = 0.15 (82.6 examples/sec; 0.048 sec/batch; 80h:10m:24s remains)
INFO - root - 2019-11-03 23:53:45.674328: step 36770, total loss = 0.56, predict loss = 0.14 (76.7 examples/sec; 0.052 sec/batch; 86h:20m:31s remains)
INFO - root - 2019-11-03 23:53:46.282914: step 36780, total loss = 0.72, predict loss = 0.16 (79.0 examples/sec; 0.051 sec/batch; 83h:50m:11s remains)
INFO - root - 2019-11-03 23:53:46.908837: step 36790, total loss = 0.54, predict loss = 0.13 (77.2 examples/sec; 0.052 sec/batch; 85h:49m:19s remains)
INFO - root - 2019-11-03 23:53:47.543287: step 36800, total loss = 0.54, predict loss = 0.12 (72.9 examples/sec; 0.055 sec/batch; 90h:56m:05s remains)
INFO - root - 2019-11-03 23:53:48.185666: step 36810, total loss = 0.56, predict loss = 0.12 (62.8 examples/sec; 0.064 sec/batch; 105h:31m:03s remains)
INFO - root - 2019-11-03 23:53:48.955079: step 36820, total loss = 0.88, predict loss = 0.21 (60.4 examples/sec; 0.066 sec/batch; 109h:39m:52s remains)
INFO - root - 2019-11-03 23:53:49.615114: step 36830, total loss = 0.48, predict loss = 0.11 (68.3 examples/sec; 0.059 sec/batch; 96h:58m:11s remains)
INFO - root - 2019-11-03 23:53:50.220205: step 36840, total loss = 0.57, predict loss = 0.13 (85.3 examples/sec; 0.047 sec/batch; 77h:39m:17s remains)
INFO - root - 2019-11-03 23:53:50.854241: step 36850, total loss = 0.45, predict loss = 0.10 (75.0 examples/sec; 0.053 sec/batch; 88h:22m:46s remains)
INFO - root - 2019-11-03 23:53:51.505044: step 36860, total loss = 0.63, predict loss = 0.15 (62.3 examples/sec; 0.064 sec/batch; 106h:18m:13s remains)
INFO - root - 2019-11-03 23:53:52.115111: step 36870, total loss = 0.73, predict loss = 0.17 (80.0 examples/sec; 0.050 sec/batch; 82h:49m:19s remains)
INFO - root - 2019-11-03 23:53:52.746148: step 36880, total loss = 1.45, predict loss = 0.35 (68.8 examples/sec; 0.058 sec/batch; 96h:14m:58s remains)
INFO - root - 2019-11-03 23:53:53.431609: step 36890, total loss = 0.75, predict loss = 0.18 (65.2 examples/sec; 0.061 sec/batch; 101h:40m:47s remains)
INFO - root - 2019-11-03 23:53:54.039890: step 36900, total loss = 1.07, predict loss = 0.25 (75.2 examples/sec; 0.053 sec/batch; 88h:04m:37s remains)
INFO - root - 2019-11-03 23:53:54.633065: step 36910, total loss = 0.83, predict loss = 0.19 (79.4 examples/sec; 0.050 sec/batch; 83h:26m:33s remains)
INFO - root - 2019-11-03 23:53:55.256687: step 36920, total loss = 0.82, predict loss = 0.18 (71.8 examples/sec; 0.056 sec/batch; 92h:17m:47s remains)
INFO - root - 2019-11-03 23:53:55.921294: step 36930, total loss = 0.94, predict loss = 0.23 (64.1 examples/sec; 0.062 sec/batch; 103h:23m:46s remains)
INFO - root - 2019-11-03 23:53:56.631679: step 36940, total loss = 0.93, predict loss = 0.22 (59.4 examples/sec; 0.067 sec/batch; 111h:27m:31s remains)
INFO - root - 2019-11-03 23:53:57.255359: step 36950, total loss = 0.66, predict loss = 0.15 (78.6 examples/sec; 0.051 sec/batch; 84h:16m:48s remains)
INFO - root - 2019-11-03 23:53:57.871335: step 36960, total loss = 0.74, predict loss = 0.17 (74.1 examples/sec; 0.054 sec/batch; 89h:23m:40s remains)
INFO - root - 2019-11-03 23:53:58.487379: step 36970, total loss = 0.69, predict loss = 0.17 (78.3 examples/sec; 0.051 sec/batch; 84h:34m:26s remains)
INFO - root - 2019-11-03 23:53:59.116636: step 36980, total loss = 0.60, predict loss = 0.13 (72.0 examples/sec; 0.056 sec/batch; 91h:57m:57s remains)
INFO - root - 2019-11-03 23:53:59.767926: step 36990, total loss = 0.62, predict loss = 0.13 (60.5 examples/sec; 0.066 sec/batch; 109h:27m:34s remains)
INFO - root - 2019-11-03 23:54:00.427211: step 37000, total loss = 0.66, predict loss = 0.15 (69.8 examples/sec; 0.057 sec/batch; 94h:54m:53s remains)
INFO - root - 2019-11-03 23:54:01.082989: step 37010, total loss = 0.63, predict loss = 0.14 (63.9 examples/sec; 0.063 sec/batch; 103h:41m:32s remains)
INFO - root - 2019-11-03 23:54:01.745859: step 37020, total loss = 0.67, predict loss = 0.16 (66.2 examples/sec; 0.060 sec/batch; 100h:05m:55s remains)
INFO - root - 2019-11-03 23:54:02.394741: step 37030, total loss = 0.72, predict loss = 0.16 (61.8 examples/sec; 0.065 sec/batch; 107h:15m:04s remains)
INFO - root - 2019-11-03 23:54:03.089439: step 37040, total loss = 0.47, predict loss = 0.11 (64.9 examples/sec; 0.062 sec/batch; 102h:08m:41s remains)
INFO - root - 2019-11-03 23:54:03.739650: step 37050, total loss = 0.61, predict loss = 0.15 (68.2 examples/sec; 0.059 sec/batch; 97h:10m:43s remains)
INFO - root - 2019-11-03 23:54:04.380406: step 37060, total loss = 0.52, predict loss = 0.11 (67.5 examples/sec; 0.059 sec/batch; 98h:08m:29s remains)
INFO - root - 2019-11-03 23:54:05.015422: step 37070, total loss = 0.54, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 100h:09m:14s remains)
INFO - root - 2019-11-03 23:54:05.640610: step 37080, total loss = 0.55, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 96h:57m:47s remains)
INFO - root - 2019-11-03 23:54:06.262559: step 37090, total loss = 0.70, predict loss = 0.16 (70.8 examples/sec; 0.056 sec/batch; 93h:31m:49s remains)
INFO - root - 2019-11-03 23:54:06.849482: step 37100, total loss = 0.52, predict loss = 0.12 (79.8 examples/sec; 0.050 sec/batch; 83h:00m:32s remains)
INFO - root - 2019-11-03 23:54:07.475153: step 37110, total loss = 0.69, predict loss = 0.16 (64.7 examples/sec; 0.062 sec/batch; 102h:21m:08s remains)
INFO - root - 2019-11-03 23:54:08.100610: step 37120, total loss = 0.63, predict loss = 0.14 (67.5 examples/sec; 0.059 sec/batch; 98h:10m:27s remains)
INFO - root - 2019-11-03 23:54:08.778524: step 37130, total loss = 0.48, predict loss = 0.11 (72.0 examples/sec; 0.056 sec/batch; 92h:04m:34s remains)
INFO - root - 2019-11-03 23:54:09.418264: step 37140, total loss = 0.56, predict loss = 0.13 (73.2 examples/sec; 0.055 sec/batch; 90h:30m:16s remains)
INFO - root - 2019-11-03 23:54:10.056911: step 37150, total loss = 0.52, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 96h:03m:41s remains)
INFO - root - 2019-11-03 23:54:10.673095: step 37160, total loss = 0.61, predict loss = 0.14 (78.2 examples/sec; 0.051 sec/batch; 84h:42m:45s remains)
INFO - root - 2019-11-03 23:54:11.316051: step 37170, total loss = 0.48, predict loss = 0.10 (69.8 examples/sec; 0.057 sec/batch; 94h:53m:30s remains)
INFO - root - 2019-11-03 23:54:11.985516: step 37180, total loss = 0.41, predict loss = 0.09 (63.0 examples/sec; 0.064 sec/batch; 105h:11m:45s remains)
INFO - root - 2019-11-03 23:54:12.607613: step 37190, total loss = 0.49, predict loss = 0.11 (69.5 examples/sec; 0.058 sec/batch; 95h:20m:02s remains)
INFO - root - 2019-11-03 23:54:13.233513: step 37200, total loss = 0.50, predict loss = 0.12 (73.8 examples/sec; 0.054 sec/batch; 89h:46m:41s remains)
INFO - root - 2019-11-03 23:54:13.880141: step 37210, total loss = 0.55, predict loss = 0.13 (66.2 examples/sec; 0.060 sec/batch; 100h:04m:03s remains)
INFO - root - 2019-11-03 23:54:14.554039: step 37220, total loss = 0.44, predict loss = 0.10 (79.5 examples/sec; 0.050 sec/batch; 83h:17m:40s remains)
INFO - root - 2019-11-03 23:54:15.166603: step 37230, total loss = 0.55, predict loss = 0.12 (71.1 examples/sec; 0.056 sec/batch; 93h:09m:03s remains)
INFO - root - 2019-11-03 23:54:15.795029: step 37240, total loss = 0.58, predict loss = 0.14 (75.1 examples/sec; 0.053 sec/batch; 88h:12m:42s remains)
INFO - root - 2019-11-03 23:54:16.420767: step 37250, total loss = 0.71, predict loss = 0.16 (71.5 examples/sec; 0.056 sec/batch; 92h:38m:03s remains)
INFO - root - 2019-11-03 23:54:17.022006: step 37260, total loss = 0.52, predict loss = 0.12 (76.9 examples/sec; 0.052 sec/batch; 86h:06m:33s remains)
INFO - root - 2019-11-03 23:54:17.624171: step 37270, total loss = 0.52, predict loss = 0.12 (77.5 examples/sec; 0.052 sec/batch; 85h:26m:22s remains)
INFO - root - 2019-11-03 23:54:18.242031: step 37280, total loss = 0.58, predict loss = 0.13 (90.0 examples/sec; 0.044 sec/batch; 73h:34m:56s remains)
INFO - root - 2019-11-03 23:54:18.895209: step 37290, total loss = 0.72, predict loss = 0.17 (63.7 examples/sec; 0.063 sec/batch; 103h:57m:31s remains)
INFO - root - 2019-11-03 23:54:19.558092: step 37300, total loss = 0.57, predict loss = 0.13 (68.0 examples/sec; 0.059 sec/batch; 97h:29m:57s remains)
INFO - root - 2019-11-03 23:54:20.239080: step 37310, total loss = 0.64, predict loss = 0.15 (62.1 examples/sec; 0.064 sec/batch; 106h:46m:19s remains)
INFO - root - 2019-11-03 23:54:20.908796: step 37320, total loss = 0.89, predict loss = 0.22 (75.9 examples/sec; 0.053 sec/batch; 87h:17m:26s remains)
INFO - root - 2019-11-03 23:54:21.538736: step 37330, total loss = 0.76, predict loss = 0.19 (68.9 examples/sec; 0.058 sec/batch; 96h:13m:21s remains)
INFO - root - 2019-11-03 23:54:22.199823: step 37340, total loss = 0.78, predict loss = 0.19 (62.8 examples/sec; 0.064 sec/batch; 105h:27m:44s remains)
INFO - root - 2019-11-03 23:54:22.860720: step 37350, total loss = 0.77, predict loss = 0.18 (62.1 examples/sec; 0.064 sec/batch; 106h:42m:42s remains)
INFO - root - 2019-11-03 23:54:23.546449: step 37360, total loss = 0.78, predict loss = 0.19 (67.6 examples/sec; 0.059 sec/batch; 97h:58m:15s remains)
INFO - root - 2019-11-03 23:54:24.230608: step 37370, total loss = 0.63, predict loss = 0.15 (56.5 examples/sec; 0.071 sec/batch; 117h:10m:43s remains)
INFO - root - 2019-11-03 23:54:24.897266: step 37380, total loss = 0.85, predict loss = 0.22 (76.9 examples/sec; 0.052 sec/batch; 86h:06m:08s remains)
INFO - root - 2019-11-03 23:54:25.538168: step 37390, total loss = 0.63, predict loss = 0.15 (68.2 examples/sec; 0.059 sec/batch; 97h:10m:33s remains)
INFO - root - 2019-11-03 23:54:26.198638: step 37400, total loss = 0.52, predict loss = 0.12 (65.1 examples/sec; 0.061 sec/batch; 101h:49m:45s remains)
INFO - root - 2019-11-03 23:54:26.882541: step 37410, total loss = 0.60, predict loss = 0.14 (67.4 examples/sec; 0.059 sec/batch; 98h:13m:32s remains)
INFO - root - 2019-11-03 23:54:27.519469: step 37420, total loss = 0.62, predict loss = 0.14 (67.2 examples/sec; 0.059 sec/batch; 98h:30m:56s remains)
INFO - root - 2019-11-03 23:54:28.197619: step 37430, total loss = 0.53, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 93h:51m:12s remains)
INFO - root - 2019-11-03 23:54:28.799055: step 37440, total loss = 0.53, predict loss = 0.12 (77.4 examples/sec; 0.052 sec/batch; 85h:34m:01s remains)
INFO - root - 2019-11-03 23:54:29.404165: step 37450, total loss = 0.59, predict loss = 0.14 (70.4 examples/sec; 0.057 sec/batch; 94h:02m:44s remains)
INFO - root - 2019-11-03 23:54:30.059564: step 37460, total loss = 0.49, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 93h:14m:19s remains)
INFO - root - 2019-11-03 23:54:30.732087: step 37470, total loss = 0.47, predict loss = 0.11 (75.9 examples/sec; 0.053 sec/batch; 87h:14m:07s remains)
INFO - root - 2019-11-03 23:54:31.377195: step 37480, total loss = 0.57, predict loss = 0.13 (72.9 examples/sec; 0.055 sec/batch; 90h:55m:43s remains)
INFO - root - 2019-11-03 23:54:31.982294: step 37490, total loss = 0.61, predict loss = 0.14 (67.3 examples/sec; 0.059 sec/batch; 98h:22m:35s remains)
INFO - root - 2019-11-03 23:54:32.600832: step 37500, total loss = 0.47, predict loss = 0.10 (64.8 examples/sec; 0.062 sec/batch; 102h:12m:37s remains)
INFO - root - 2019-11-03 23:54:33.198872: step 37510, total loss = 0.77, predict loss = 0.18 (69.5 examples/sec; 0.058 sec/batch; 95h:19m:22s remains)
INFO - root - 2019-11-03 23:54:33.846677: step 37520, total loss = 0.57, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 94h:15m:10s remains)
INFO - root - 2019-11-03 23:54:34.468910: step 37530, total loss = 0.69, predict loss = 0.16 (72.6 examples/sec; 0.055 sec/batch; 91h:16m:06s remains)
INFO - root - 2019-11-03 23:54:35.093709: step 37540, total loss = 0.71, predict loss = 0.16 (80.9 examples/sec; 0.049 sec/batch; 81h:51m:11s remains)
INFO - root - 2019-11-03 23:54:35.748033: step 37550, total loss = 0.70, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 96h:28m:32s remains)
INFO - root - 2019-11-03 23:54:36.394770: step 37560, total loss = 0.82, predict loss = 0.21 (71.8 examples/sec; 0.056 sec/batch; 92h:12m:57s remains)
INFO - root - 2019-11-03 23:54:37.069688: step 37570, total loss = 0.61, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 101h:29m:09s remains)
INFO - root - 2019-11-03 23:54:37.696566: step 37580, total loss = 0.66, predict loss = 0.15 (75.0 examples/sec; 0.053 sec/batch; 88h:18m:30s remains)
INFO - root - 2019-11-03 23:54:38.329834: step 37590, total loss = 0.62, predict loss = 0.15 (67.9 examples/sec; 0.059 sec/batch; 97h:33m:25s remains)
INFO - root - 2019-11-03 23:54:38.978654: step 37600, total loss = 0.53, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 93h:55m:57s remains)
INFO - root - 2019-11-03 23:54:39.601498: step 37610, total loss = 0.67, predict loss = 0.16 (83.0 examples/sec; 0.048 sec/batch; 79h:48m:08s remains)
INFO - root - 2019-11-03 23:54:40.204080: step 37620, total loss = 0.56, predict loss = 0.13 (71.2 examples/sec; 0.056 sec/batch; 92h:59m:58s remains)
INFO - root - 2019-11-03 23:54:40.827956: step 37630, total loss = 0.54, predict loss = 0.12 (73.1 examples/sec; 0.055 sec/batch; 90h:38m:06s remains)
INFO - root - 2019-11-03 23:54:41.446853: step 37640, total loss = 0.67, predict loss = 0.15 (73.1 examples/sec; 0.055 sec/batch; 90h:39m:27s remains)
INFO - root - 2019-11-03 23:54:42.060735: step 37650, total loss = 0.56, predict loss = 0.13 (69.6 examples/sec; 0.057 sec/batch; 95h:13m:17s remains)
INFO - root - 2019-11-03 23:54:42.688040: step 37660, total loss = 0.60, predict loss = 0.14 (74.1 examples/sec; 0.054 sec/batch; 89h:26m:08s remains)
INFO - root - 2019-11-03 23:54:43.312205: step 37670, total loss = 0.65, predict loss = 0.14 (79.4 examples/sec; 0.050 sec/batch; 83h:24m:36s remains)
INFO - root - 2019-11-03 23:54:43.970881: step 37680, total loss = 0.77, predict loss = 0.18 (66.1 examples/sec; 0.060 sec/batch; 100h:10m:54s remains)
INFO - root - 2019-11-03 23:54:44.629344: step 37690, total loss = 0.56, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 95h:54m:17s remains)
INFO - root - 2019-11-03 23:54:45.242727: step 37700, total loss = 0.66, predict loss = 0.15 (71.3 examples/sec; 0.056 sec/batch; 92h:52m:30s remains)
INFO - root - 2019-11-03 23:54:45.861165: step 37710, total loss = 0.62, predict loss = 0.15 (69.2 examples/sec; 0.058 sec/batch; 95h:45m:03s remains)
INFO - root - 2019-11-03 23:54:46.512543: step 37720, total loss = 0.55, predict loss = 0.12 (63.2 examples/sec; 0.063 sec/batch; 104h:52m:11s remains)
INFO - root - 2019-11-03 23:54:47.146839: step 37730, total loss = 0.48, predict loss = 0.11 (68.3 examples/sec; 0.059 sec/batch; 97h:03m:39s remains)
INFO - root - 2019-11-03 23:54:47.784011: step 37740, total loss = 0.44, predict loss = 0.10 (66.2 examples/sec; 0.060 sec/batch; 100h:05m:32s remains)
INFO - root - 2019-11-03 23:54:48.418396: step 37750, total loss = 0.65, predict loss = 0.15 (64.5 examples/sec; 0.062 sec/batch; 102h:47m:02s remains)
INFO - root - 2019-11-03 23:54:49.107317: step 37760, total loss = 0.51, predict loss = 0.12 (67.0 examples/sec; 0.060 sec/batch; 98h:56m:24s remains)
INFO - root - 2019-11-03 23:54:49.758334: step 37770, total loss = 0.51, predict loss = 0.11 (66.3 examples/sec; 0.060 sec/batch; 99h:50m:49s remains)
INFO - root - 2019-11-03 23:54:50.388251: step 37780, total loss = 0.43, predict loss = 0.10 (72.3 examples/sec; 0.055 sec/batch; 91h:34m:06s remains)
INFO - root - 2019-11-03 23:54:51.040424: step 37790, total loss = 0.46, predict loss = 0.10 (61.6 examples/sec; 0.065 sec/batch; 107h:33m:54s remains)
INFO - root - 2019-11-03 23:54:51.697559: step 37800, total loss = 0.67, predict loss = 0.15 (69.5 examples/sec; 0.058 sec/batch; 95h:15m:35s remains)
INFO - root - 2019-11-03 23:54:52.330987: step 37810, total loss = 0.52, predict loss = 0.11 (79.6 examples/sec; 0.050 sec/batch; 83h:13m:55s remains)
INFO - root - 2019-11-03 23:54:52.943503: step 37820, total loss = 0.62, predict loss = 0.15 (75.2 examples/sec; 0.053 sec/batch; 88h:09m:01s remains)
INFO - root - 2019-11-03 23:54:53.578396: step 37830, total loss = 0.65, predict loss = 0.15 (71.6 examples/sec; 0.056 sec/batch; 92h:34m:30s remains)
INFO - root - 2019-11-03 23:54:54.227072: step 37840, total loss = 0.73, predict loss = 0.16 (77.8 examples/sec; 0.051 sec/batch; 85h:12m:06s remains)
INFO - root - 2019-11-03 23:54:54.861387: step 37850, total loss = 0.65, predict loss = 0.15 (70.0 examples/sec; 0.057 sec/batch; 94h:36m:31s remains)
INFO - root - 2019-11-03 23:54:55.494710: step 37860, total loss = 0.54, predict loss = 0.12 (61.4 examples/sec; 0.065 sec/batch; 107h:55m:07s remains)
INFO - root - 2019-11-03 23:54:56.140818: step 37870, total loss = 0.53, predict loss = 0.13 (62.6 examples/sec; 0.064 sec/batch; 105h:53m:43s remains)
INFO - root - 2019-11-03 23:54:56.815631: step 37880, total loss = 0.61, predict loss = 0.14 (61.9 examples/sec; 0.065 sec/batch; 107h:06m:25s remains)
INFO - root - 2019-11-03 23:54:57.476682: step 37890, total loss = 0.77, predict loss = 0.19 (65.1 examples/sec; 0.061 sec/batch; 101h:41m:27s remains)
INFO - root - 2019-11-03 23:54:58.119051: step 37900, total loss = 0.79, predict loss = 0.19 (67.8 examples/sec; 0.059 sec/batch; 97h:38m:18s remains)
INFO - root - 2019-11-03 23:54:58.770032: step 37910, total loss = 0.55, predict loss = 0.14 (64.7 examples/sec; 0.062 sec/batch; 102h:19m:07s remains)
INFO - root - 2019-11-03 23:54:59.504277: step 37920, total loss = 0.61, predict loss = 0.15 (66.6 examples/sec; 0.060 sec/batch; 99h:30m:31s remains)
INFO - root - 2019-11-03 23:55:00.174285: step 37930, total loss = 0.84, predict loss = 0.20 (67.2 examples/sec; 0.060 sec/batch; 98h:34m:19s remains)
INFO - root - 2019-11-03 23:55:00.856673: step 37940, total loss = 0.78, predict loss = 0.18 (65.9 examples/sec; 0.061 sec/batch; 100h:33m:28s remains)
INFO - root - 2019-11-03 23:55:01.524515: step 37950, total loss = 0.68, predict loss = 0.16 (74.5 examples/sec; 0.054 sec/batch; 88h:58m:38s remains)
INFO - root - 2019-11-03 23:55:02.145309: step 37960, total loss = 0.74, predict loss = 0.18 (79.3 examples/sec; 0.050 sec/batch; 83h:34m:30s remains)
INFO - root - 2019-11-03 23:55:02.801165: step 37970, total loss = 0.55, predict loss = 0.13 (69.0 examples/sec; 0.058 sec/batch; 95h:58m:46s remains)
INFO - root - 2019-11-03 23:55:03.453479: step 37980, total loss = 0.84, predict loss = 0.19 (71.7 examples/sec; 0.056 sec/batch; 92h:22m:48s remains)
INFO - root - 2019-11-03 23:55:04.088980: step 37990, total loss = 0.74, predict loss = 0.17 (68.4 examples/sec; 0.058 sec/batch; 96h:52m:49s remains)
INFO - root - 2019-11-03 23:55:04.727195: step 38000, total loss = 0.58, predict loss = 0.14 (73.6 examples/sec; 0.054 sec/batch; 90h:01m:13s remains)
INFO - root - 2019-11-03 23:55:05.382890: step 38010, total loss = 0.84, predict loss = 0.21 (68.1 examples/sec; 0.059 sec/batch; 97h:17m:35s remains)
INFO - root - 2019-11-03 23:55:06.045164: step 38020, total loss = 0.72, predict loss = 0.17 (63.0 examples/sec; 0.063 sec/batch; 105h:05m:22s remains)
INFO - root - 2019-11-03 23:55:06.720363: step 38030, total loss = 0.68, predict loss = 0.17 (64.0 examples/sec; 0.062 sec/batch; 103h:27m:41s remains)
INFO - root - 2019-11-03 23:55:07.344730: step 38040, total loss = 0.54, predict loss = 0.13 (76.8 examples/sec; 0.052 sec/batch; 86h:13m:16s remains)
INFO - root - 2019-11-03 23:55:07.972898: step 38050, total loss = 0.75, predict loss = 0.18 (78.8 examples/sec; 0.051 sec/batch; 84h:02m:53s remains)
INFO - root - 2019-11-03 23:55:08.612171: step 38060, total loss = 0.59, predict loss = 0.14 (71.9 examples/sec; 0.056 sec/batch; 92h:06m:45s remains)
INFO - root - 2019-11-03 23:55:09.268389: step 38070, total loss = 0.62, predict loss = 0.14 (67.2 examples/sec; 0.060 sec/batch; 98h:35m:29s remains)
INFO - root - 2019-11-03 23:55:09.864204: step 38080, total loss = 0.62, predict loss = 0.15 (80.0 examples/sec; 0.050 sec/batch; 82h:45m:19s remains)
INFO - root - 2019-11-03 23:55:10.475119: step 38090, total loss = 0.64, predict loss = 0.15 (74.8 examples/sec; 0.053 sec/batch; 88h:35m:44s remains)
INFO - root - 2019-11-03 23:55:11.130246: step 38100, total loss = 0.63, predict loss = 0.15 (68.6 examples/sec; 0.058 sec/batch; 96h:36m:29s remains)
INFO - root - 2019-11-03 23:55:11.758825: step 38110, total loss = 0.73, predict loss = 0.17 (76.0 examples/sec; 0.053 sec/batch; 87h:07m:01s remains)
INFO - root - 2019-11-03 23:55:12.403988: step 38120, total loss = 0.74, predict loss = 0.17 (68.6 examples/sec; 0.058 sec/batch; 96h:31m:26s remains)
INFO - root - 2019-11-03 23:55:13.077908: step 38130, total loss = 0.45, predict loss = 0.10 (64.3 examples/sec; 0.062 sec/batch; 103h:02m:40s remains)
INFO - root - 2019-11-03 23:55:13.765862: step 38140, total loss = 0.68, predict loss = 0.17 (69.5 examples/sec; 0.058 sec/batch; 95h:22m:43s remains)
INFO - root - 2019-11-03 23:55:14.448256: step 38150, total loss = 0.35, predict loss = 0.07 (75.6 examples/sec; 0.053 sec/batch; 87h:38m:33s remains)
INFO - root - 2019-11-03 23:55:15.112546: step 38160, total loss = 0.49, predict loss = 0.11 (85.3 examples/sec; 0.047 sec/batch; 77h:40m:51s remains)
INFO - root - 2019-11-03 23:55:15.610471: step 38170, total loss = 0.55, predict loss = 0.13 (101.4 examples/sec; 0.039 sec/batch; 65h:19m:04s remains)
INFO - root - 2019-11-03 23:55:16.063036: step 38180, total loss = 0.61, predict loss = 0.14 (100.7 examples/sec; 0.040 sec/batch; 65h:46m:50s remains)
INFO - root - 2019-11-03 23:55:17.146333: step 38190, total loss = 0.58, predict loss = 0.12 (63.9 examples/sec; 0.063 sec/batch; 103h:39m:23s remains)
INFO - root - 2019-11-03 23:55:17.790867: step 38200, total loss = 0.54, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 99h:37m:58s remains)
INFO - root - 2019-11-03 23:55:18.438026: step 38210, total loss = 0.77, predict loss = 0.19 (76.3 examples/sec; 0.052 sec/batch; 86h:52m:21s remains)
INFO - root - 2019-11-03 23:55:19.120654: step 38220, total loss = 0.46, predict loss = 0.10 (61.6 examples/sec; 0.065 sec/batch; 107h:36m:01s remains)
INFO - root - 2019-11-03 23:55:19.762201: step 38230, total loss = 0.67, predict loss = 0.15 (68.2 examples/sec; 0.059 sec/batch; 97h:04m:23s remains)
INFO - root - 2019-11-03 23:55:20.422012: step 38240, total loss = 0.48, predict loss = 0.11 (63.2 examples/sec; 0.063 sec/batch; 104h:48m:03s remains)
INFO - root - 2019-11-03 23:55:21.087350: step 38250, total loss = 0.48, predict loss = 0.10 (70.5 examples/sec; 0.057 sec/batch; 93h:56m:57s remains)
INFO - root - 2019-11-03 23:55:21.702923: step 38260, total loss = 0.57, predict loss = 0.13 (74.6 examples/sec; 0.054 sec/batch; 88h:46m:05s remains)
INFO - root - 2019-11-03 23:55:22.313272: step 38270, total loss = 0.70, predict loss = 0.15 (67.6 examples/sec; 0.059 sec/batch; 98h:01m:30s remains)
INFO - root - 2019-11-03 23:55:22.945549: step 38280, total loss = 0.83, predict loss = 0.19 (64.1 examples/sec; 0.062 sec/batch; 103h:16m:01s remains)
INFO - root - 2019-11-03 23:55:23.618136: step 38290, total loss = 0.71, predict loss = 0.16 (71.7 examples/sec; 0.056 sec/batch; 92h:26m:14s remains)
INFO - root - 2019-11-03 23:55:24.300677: step 38300, total loss = 0.84, predict loss = 0.18 (62.4 examples/sec; 0.064 sec/batch; 106h:14m:12s remains)
INFO - root - 2019-11-03 23:55:24.913473: step 38310, total loss = 0.75, predict loss = 0.18 (80.1 examples/sec; 0.050 sec/batch; 82h:43m:39s remains)
INFO - root - 2019-11-03 23:55:25.508069: step 38320, total loss = 0.70, predict loss = 0.16 (72.0 examples/sec; 0.056 sec/batch; 92h:02m:02s remains)
INFO - root - 2019-11-03 23:55:26.151447: step 38330, total loss = 0.60, predict loss = 0.15 (64.1 examples/sec; 0.062 sec/batch; 103h:20m:25s remains)
INFO - root - 2019-11-03 23:55:26.782895: step 38340, total loss = 0.55, predict loss = 0.11 (66.7 examples/sec; 0.060 sec/batch; 99h:20m:02s remains)
INFO - root - 2019-11-03 23:55:27.429282: step 38350, total loss = 0.59, predict loss = 0.13 (75.7 examples/sec; 0.053 sec/batch; 87h:32m:04s remains)
INFO - root - 2019-11-03 23:55:28.030252: step 38360, total loss = 0.61, predict loss = 0.14 (75.4 examples/sec; 0.053 sec/batch; 87h:49m:20s remains)
INFO - root - 2019-11-03 23:55:28.661643: step 38370, total loss = 0.64, predict loss = 0.15 (68.0 examples/sec; 0.059 sec/batch; 97h:23m:23s remains)
INFO - root - 2019-11-03 23:55:29.308629: step 38380, total loss = 0.47, predict loss = 0.10 (65.6 examples/sec; 0.061 sec/batch; 100h:56m:54s remains)
INFO - root - 2019-11-03 23:55:29.926536: step 38390, total loss = 0.55, predict loss = 0.12 (78.7 examples/sec; 0.051 sec/batch; 84h:10m:30s remains)
INFO - root - 2019-11-03 23:55:30.535229: step 38400, total loss = 0.70, predict loss = 0.17 (77.6 examples/sec; 0.052 sec/batch; 85h:23m:35s remains)
INFO - root - 2019-11-03 23:55:31.175462: step 38410, total loss = 0.81, predict loss = 0.21 (78.4 examples/sec; 0.051 sec/batch; 84h:28m:41s remains)
INFO - root - 2019-11-03 23:55:31.809157: step 38420, total loss = 0.91, predict loss = 0.21 (82.9 examples/sec; 0.048 sec/batch; 79h:54m:13s remains)
INFO - root - 2019-11-03 23:55:32.468311: step 38430, total loss = 0.72, predict loss = 0.16 (66.9 examples/sec; 0.060 sec/batch; 99h:01m:32s remains)
INFO - root - 2019-11-03 23:55:33.088194: step 38440, total loss = 0.78, predict loss = 0.17 (71.7 examples/sec; 0.056 sec/batch; 92h:25m:11s remains)
INFO - root - 2019-11-03 23:55:33.689795: step 38450, total loss = 0.56, predict loss = 0.12 (73.7 examples/sec; 0.054 sec/batch; 89h:50m:05s remains)
INFO - root - 2019-11-03 23:55:34.273214: step 38460, total loss = 0.62, predict loss = 0.14 (71.7 examples/sec; 0.056 sec/batch; 92h:26m:52s remains)
INFO - root - 2019-11-03 23:55:34.915811: step 38470, total loss = 0.79, predict loss = 0.17 (72.9 examples/sec; 0.055 sec/batch; 90h:54m:56s remains)
INFO - root - 2019-11-03 23:55:35.559058: step 38480, total loss = 0.83, predict loss = 0.19 (68.4 examples/sec; 0.058 sec/batch; 96h:51m:50s remains)
INFO - root - 2019-11-03 23:55:36.211672: step 38490, total loss = 0.81, predict loss = 0.19 (64.8 examples/sec; 0.062 sec/batch; 102h:11m:29s remains)
INFO - root - 2019-11-03 23:55:36.877304: step 38500, total loss = 0.77, predict loss = 0.17 (69.9 examples/sec; 0.057 sec/batch; 94h:43m:27s remains)
INFO - root - 2019-11-03 23:55:37.540733: step 38510, total loss = 0.62, predict loss = 0.13 (62.0 examples/sec; 0.065 sec/batch; 106h:55m:03s remains)
INFO - root - 2019-11-03 23:55:38.190569: step 38520, total loss = 0.75, predict loss = 0.17 (77.0 examples/sec; 0.052 sec/batch; 86h:03m:52s remains)
INFO - root - 2019-11-03 23:55:38.809413: step 38530, total loss = 0.68, predict loss = 0.14 (73.0 examples/sec; 0.055 sec/batch; 90h:45m:41s remains)
INFO - root - 2019-11-03 23:55:39.435565: step 38540, total loss = 0.76, predict loss = 0.18 (79.5 examples/sec; 0.050 sec/batch; 83h:20m:28s remains)
INFO - root - 2019-11-03 23:55:40.081521: step 38550, total loss = 0.60, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 95h:53m:18s remains)
INFO - root - 2019-11-03 23:55:40.703615: step 38560, total loss = 0.55, predict loss = 0.12 (77.9 examples/sec; 0.051 sec/batch; 85h:04m:29s remains)
INFO - root - 2019-11-03 23:55:41.301350: step 38570, total loss = 0.63, predict loss = 0.14 (74.9 examples/sec; 0.053 sec/batch; 88h:25m:54s remains)
INFO - root - 2019-11-03 23:55:41.895057: step 38580, total loss = 0.80, predict loss = 0.19 (81.2 examples/sec; 0.049 sec/batch; 81h:33m:06s remains)
INFO - root - 2019-11-03 23:55:42.482416: step 38590, total loss = 0.64, predict loss = 0.14 (71.0 examples/sec; 0.056 sec/batch; 93h:15m:12s remains)
INFO - root - 2019-11-03 23:55:43.112102: step 38600, total loss = 0.77, predict loss = 0.18 (66.2 examples/sec; 0.060 sec/batch; 100h:06m:42s remains)
INFO - root - 2019-11-03 23:55:43.752745: step 38610, total loss = 0.54, predict loss = 0.12 (65.0 examples/sec; 0.062 sec/batch; 101h:55m:59s remains)
INFO - root - 2019-11-03 23:55:44.415845: step 38620, total loss = 1.23, predict loss = 0.25 (75.2 examples/sec; 0.053 sec/batch; 88h:05m:49s remains)
INFO - root - 2019-11-03 23:55:44.999292: step 38630, total loss = 0.77, predict loss = 0.19 (77.6 examples/sec; 0.052 sec/batch; 85h:20m:54s remains)
INFO - root - 2019-11-03 23:55:45.600399: step 38640, total loss = 0.85, predict loss = 0.20 (73.1 examples/sec; 0.055 sec/batch; 90h:35m:36s remains)
INFO - root - 2019-11-03 23:55:46.220335: step 38650, total loss = 0.72, predict loss = 0.19 (64.0 examples/sec; 0.062 sec/batch; 103h:28m:34s remains)
INFO - root - 2019-11-03 23:55:46.860936: step 38660, total loss = 0.43, predict loss = 0.09 (65.1 examples/sec; 0.061 sec/batch; 101h:47m:25s remains)
INFO - root - 2019-11-03 23:55:47.527578: step 38670, total loss = 0.73, predict loss = 0.16 (64.8 examples/sec; 0.062 sec/batch; 102h:08m:40s remains)
INFO - root - 2019-11-03 23:55:48.195120: step 38680, total loss = 0.59, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 91h:15m:42s remains)
INFO - root - 2019-11-03 23:55:48.854155: step 38690, total loss = 0.79, predict loss = 0.20 (63.1 examples/sec; 0.063 sec/batch; 104h:53m:33s remains)
INFO - root - 2019-11-03 23:55:49.520232: step 38700, total loss = 0.75, predict loss = 0.17 (64.3 examples/sec; 0.062 sec/batch; 103h:00m:30s remains)
INFO - root - 2019-11-03 23:55:50.183402: step 38710, total loss = 0.73, predict loss = 0.17 (70.6 examples/sec; 0.057 sec/batch; 93h:49m:33s remains)
INFO - root - 2019-11-03 23:55:50.804049: step 38720, total loss = 0.73, predict loss = 0.18 (73.8 examples/sec; 0.054 sec/batch; 89h:42m:57s remains)
INFO - root - 2019-11-03 23:55:51.452679: step 38730, total loss = 0.64, predict loss = 0.15 (68.8 examples/sec; 0.058 sec/batch; 96h:17m:21s remains)
INFO - root - 2019-11-03 23:55:52.077434: step 38740, total loss = 0.57, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 93h:51m:35s remains)
INFO - root - 2019-11-03 23:55:52.686845: step 38750, total loss = 0.69, predict loss = 0.17 (68.9 examples/sec; 0.058 sec/batch; 96h:04m:02s remains)
INFO - root - 2019-11-03 23:55:53.333605: step 38760, total loss = 0.65, predict loss = 0.15 (71.9 examples/sec; 0.056 sec/batch; 92h:07m:40s remains)
INFO - root - 2019-11-03 23:55:54.016258: step 38770, total loss = 0.48, predict loss = 0.11 (66.2 examples/sec; 0.060 sec/batch; 100h:01m:38s remains)
INFO - root - 2019-11-03 23:55:54.688735: step 38780, total loss = 0.67, predict loss = 0.14 (69.4 examples/sec; 0.058 sec/batch; 95h:23m:16s remains)
INFO - root - 2019-11-03 23:55:55.306118: step 38790, total loss = 0.47, predict loss = 0.09 (68.9 examples/sec; 0.058 sec/batch; 96h:08m:21s remains)
INFO - root - 2019-11-03 23:55:55.930913: step 38800, total loss = 0.74, predict loss = 0.16 (62.0 examples/sec; 0.065 sec/batch; 106h:52m:35s remains)
INFO - root - 2019-11-03 23:55:56.598467: step 38810, total loss = 0.35, predict loss = 0.07 (65.4 examples/sec; 0.061 sec/batch; 101h:19m:58s remains)
INFO - root - 2019-11-03 23:55:57.251321: step 38820, total loss = 0.62, predict loss = 0.14 (62.6 examples/sec; 0.064 sec/batch; 105h:46m:57s remains)
INFO - root - 2019-11-03 23:55:57.916464: step 38830, total loss = 0.46, predict loss = 0.09 (72.6 examples/sec; 0.055 sec/batch; 91h:12m:40s remains)
INFO - root - 2019-11-03 23:55:58.588550: step 38840, total loss = 0.57, predict loss = 0.13 (77.6 examples/sec; 0.052 sec/batch; 85h:22m:25s remains)
INFO - root - 2019-11-03 23:55:59.222835: step 38850, total loss = 1.05, predict loss = 0.21 (71.7 examples/sec; 0.056 sec/batch; 92h:25m:47s remains)
INFO - root - 2019-11-03 23:55:59.833962: step 38860, total loss = 0.80, predict loss = 0.20 (71.5 examples/sec; 0.056 sec/batch; 92h:34m:40s remains)
INFO - root - 2019-11-03 23:56:00.461260: step 38870, total loss = 0.62, predict loss = 0.15 (66.4 examples/sec; 0.060 sec/batch; 99h:49m:21s remains)
INFO - root - 2019-11-03 23:56:01.051738: step 38880, total loss = 0.67, predict loss = 0.16 (68.8 examples/sec; 0.058 sec/batch; 96h:19m:03s remains)
INFO - root - 2019-11-03 23:56:01.677704: step 38890, total loss = 0.80, predict loss = 0.20 (73.5 examples/sec; 0.054 sec/batch; 90h:10m:09s remains)
INFO - root - 2019-11-03 23:56:02.337712: step 38900, total loss = 0.55, predict loss = 0.13 (68.4 examples/sec; 0.058 sec/batch; 96h:52m:01s remains)
INFO - root - 2019-11-03 23:56:03.007975: step 38910, total loss = 0.84, predict loss = 0.20 (70.3 examples/sec; 0.057 sec/batch; 94h:12m:39s remains)
INFO - root - 2019-11-03 23:56:03.686318: step 38920, total loss = 0.75, predict loss = 0.18 (67.9 examples/sec; 0.059 sec/batch; 97h:36m:53s remains)
INFO - root - 2019-11-03 23:56:04.400492: step 38930, total loss = 0.58, predict loss = 0.14 (57.0 examples/sec; 0.070 sec/batch; 116h:12m:23s remains)
INFO - root - 2019-11-03 23:56:05.030031: step 38940, total loss = 0.41, predict loss = 0.08 (71.1 examples/sec; 0.056 sec/batch; 93h:06m:26s remains)
INFO - root - 2019-11-03 23:56:05.656782: step 38950, total loss = 0.59, predict loss = 0.13 (59.3 examples/sec; 0.067 sec/batch; 111h:43m:35s remains)
INFO - root - 2019-11-03 23:56:06.308571: step 38960, total loss = 0.78, predict loss = 0.17 (63.2 examples/sec; 0.063 sec/batch; 104h:49m:31s remains)
INFO - root - 2019-11-03 23:56:06.967657: step 38970, total loss = 0.86, predict loss = 0.21 (73.2 examples/sec; 0.055 sec/batch; 90h:25m:19s remains)
INFO - root - 2019-11-03 23:56:07.622270: step 38980, total loss = 0.61, predict loss = 0.14 (65.6 examples/sec; 0.061 sec/batch; 100h:53m:59s remains)
INFO - root - 2019-11-03 23:56:08.281559: step 38990, total loss = 0.73, predict loss = 0.19 (73.2 examples/sec; 0.055 sec/batch; 90h:30m:02s remains)
INFO - root - 2019-11-03 23:56:08.895060: step 39000, total loss = 0.45, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 95h:02m:02s remains)
INFO - root - 2019-11-03 23:56:09.489203: step 39010, total loss = 0.39, predict loss = 0.08 (71.9 examples/sec; 0.056 sec/batch; 92h:08m:32s remains)
INFO - root - 2019-11-03 23:56:10.121140: step 39020, total loss = 0.30, predict loss = 0.06 (64.7 examples/sec; 0.062 sec/batch; 102h:19m:32s remains)
INFO - root - 2019-11-03 23:56:10.756868: step 39030, total loss = 0.43, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 93h:47m:37s remains)
INFO - root - 2019-11-03 23:56:11.365768: step 39040, total loss = 0.36, predict loss = 0.07 (80.0 examples/sec; 0.050 sec/batch; 82h:46m:02s remains)
INFO - root - 2019-11-03 23:56:11.982392: step 39050, total loss = 0.30, predict loss = 0.06 (69.5 examples/sec; 0.058 sec/batch; 95h:18m:36s remains)
INFO - root - 2019-11-03 23:56:12.593925: step 39060, total loss = 0.50, predict loss = 0.11 (73.5 examples/sec; 0.054 sec/batch; 90h:08m:42s remains)
INFO - root - 2019-11-03 23:56:13.225613: step 39070, total loss = 0.44, predict loss = 0.09 (73.9 examples/sec; 0.054 sec/batch; 89h:40m:58s remains)
INFO - root - 2019-11-03 23:56:13.831601: step 39080, total loss = 0.37, predict loss = 0.08 (80.6 examples/sec; 0.050 sec/batch; 82h:11m:01s remains)
INFO - root - 2019-11-03 23:56:14.501732: step 39090, total loss = 0.62, predict loss = 0.13 (82.3 examples/sec; 0.049 sec/batch; 80h:26m:13s remains)
INFO - root - 2019-11-03 23:56:15.633193: step 39100, total loss = 0.46, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 94h:32m:17s remains)
INFO - root - 2019-11-03 23:56:16.250859: step 39110, total loss = 0.52, predict loss = 0.12 (76.5 examples/sec; 0.052 sec/batch; 86h:36m:38s remains)
INFO - root - 2019-11-03 23:56:16.910927: step 39120, total loss = 0.43, predict loss = 0.10 (70.1 examples/sec; 0.057 sec/batch; 94h:30m:52s remains)
INFO - root - 2019-11-03 23:56:17.562954: step 39130, total loss = 0.77, predict loss = 0.19 (68.8 examples/sec; 0.058 sec/batch; 96h:17m:13s remains)
INFO - root - 2019-11-03 23:56:18.263661: step 39140, total loss = 0.56, predict loss = 0.14 (69.8 examples/sec; 0.057 sec/batch; 94h:50m:15s remains)
INFO - root - 2019-11-03 23:56:18.942276: step 39150, total loss = 0.38, predict loss = 0.08 (68.7 examples/sec; 0.058 sec/batch; 96h:24m:21s remains)
INFO - root - 2019-11-03 23:56:19.631515: step 39160, total loss = 0.55, predict loss = 0.13 (63.5 examples/sec; 0.063 sec/batch; 104h:18m:35s remains)
INFO - root - 2019-11-03 23:56:20.310418: step 39170, total loss = 0.77, predict loss = 0.19 (70.4 examples/sec; 0.057 sec/batch; 94h:07m:37s remains)
INFO - root - 2019-11-03 23:56:21.010512: step 39180, total loss = 0.76, predict loss = 0.17 (64.1 examples/sec; 0.062 sec/batch; 103h:23m:37s remains)
INFO - root - 2019-11-03 23:56:21.632051: step 39190, total loss = 0.62, predict loss = 0.14 (76.7 examples/sec; 0.052 sec/batch; 86h:24m:16s remains)
INFO - root - 2019-11-03 23:56:22.252828: step 39200, total loss = 0.51, predict loss = 0.12 (70.0 examples/sec; 0.057 sec/batch; 94h:34m:18s remains)
INFO - root - 2019-11-03 23:56:22.890060: step 39210, total loss = 0.78, predict loss = 0.19 (67.8 examples/sec; 0.059 sec/batch; 97h:37m:02s remains)
INFO - root - 2019-11-03 23:56:23.528280: step 39220, total loss = 0.77, predict loss = 0.18 (70.4 examples/sec; 0.057 sec/batch; 94h:01m:47s remains)
INFO - root - 2019-11-03 23:56:24.175830: step 39230, total loss = 0.68, predict loss = 0.16 (73.7 examples/sec; 0.054 sec/batch; 89h:51m:34s remains)
INFO - root - 2019-11-03 23:56:24.791548: step 39240, total loss = 0.67, predict loss = 0.16 (79.8 examples/sec; 0.050 sec/batch; 82h:58m:12s remains)
INFO - root - 2019-11-03 23:56:25.410214: step 39250, total loss = 0.50, predict loss = 0.12 (70.7 examples/sec; 0.057 sec/batch; 93h:43m:11s remains)
INFO - root - 2019-11-03 23:56:26.030312: step 39260, total loss = 0.54, predict loss = 0.14 (84.5 examples/sec; 0.047 sec/batch; 78h:25m:03s remains)
INFO - root - 2019-11-03 23:56:26.690675: step 39270, total loss = 0.32, predict loss = 0.07 (67.9 examples/sec; 0.059 sec/batch; 97h:29m:09s remains)
INFO - root - 2019-11-03 23:56:27.309233: step 39280, total loss = 0.57, predict loss = 0.13 (73.2 examples/sec; 0.055 sec/batch; 90h:28m:18s remains)
INFO - root - 2019-11-03 23:56:27.935990: step 39290, total loss = 0.57, predict loss = 0.12 (74.7 examples/sec; 0.054 sec/batch; 88h:39m:21s remains)
INFO - root - 2019-11-03 23:56:28.561737: step 39300, total loss = 0.51, predict loss = 0.12 (67.7 examples/sec; 0.059 sec/batch; 97h:50m:05s remains)
INFO - root - 2019-11-03 23:56:29.166889: step 39310, total loss = 0.32, predict loss = 0.06 (79.0 examples/sec; 0.051 sec/batch; 83h:51m:01s remains)
INFO - root - 2019-11-03 23:56:29.811097: step 39320, total loss = 0.56, predict loss = 0.13 (67.7 examples/sec; 0.059 sec/batch; 97h:45m:24s remains)
INFO - root - 2019-11-03 23:56:30.502389: step 39330, total loss = 0.62, predict loss = 0.13 (61.8 examples/sec; 0.065 sec/batch; 107h:10m:42s remains)
INFO - root - 2019-11-03 23:56:31.133445: step 39340, total loss = 0.55, predict loss = 0.13 (74.7 examples/sec; 0.054 sec/batch; 88h:39m:13s remains)
INFO - root - 2019-11-03 23:56:31.766897: step 39350, total loss = 0.74, predict loss = 0.17 (67.5 examples/sec; 0.059 sec/batch; 98h:06m:30s remains)
INFO - root - 2019-11-03 23:56:32.400679: step 39360, total loss = 0.60, predict loss = 0.13 (72.7 examples/sec; 0.055 sec/batch; 91h:03m:37s remains)
INFO - root - 2019-11-03 23:56:33.012621: step 39370, total loss = 0.59, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 96h:56m:34s remains)
INFO - root - 2019-11-03 23:56:33.664940: step 39380, total loss = 0.41, predict loss = 0.09 (65.9 examples/sec; 0.061 sec/batch; 100h:26m:13s remains)
INFO - root - 2019-11-03 23:56:34.334646: step 39390, total loss = 0.46, predict loss = 0.10 (68.0 examples/sec; 0.059 sec/batch; 97h:20m:02s remains)
INFO - root - 2019-11-03 23:56:34.998007: step 39400, total loss = 0.44, predict loss = 0.09 (63.0 examples/sec; 0.064 sec/batch; 105h:09m:07s remains)
INFO - root - 2019-11-03 23:56:35.672745: step 39410, total loss = 0.61, predict loss = 0.14 (61.7 examples/sec; 0.065 sec/batch; 107h:15m:14s remains)
INFO - root - 2019-11-03 23:56:36.271336: step 39420, total loss = 0.40, predict loss = 0.07 (76.3 examples/sec; 0.052 sec/batch; 86h:45m:40s remains)
INFO - root - 2019-11-03 23:56:36.889434: step 39430, total loss = 0.42, predict loss = 0.09 (71.1 examples/sec; 0.056 sec/batch; 93h:10m:10s remains)
INFO - root - 2019-11-03 23:56:37.584629: step 39440, total loss = 0.55, predict loss = 0.12 (67.5 examples/sec; 0.059 sec/batch; 98h:04m:49s remains)
INFO - root - 2019-11-03 23:56:38.251913: step 39450, total loss = 0.65, predict loss = 0.14 (61.6 examples/sec; 0.065 sec/batch; 107h:25m:56s remains)
INFO - root - 2019-11-03 23:56:38.982284: step 39460, total loss = 0.57, predict loss = 0.14 (63.5 examples/sec; 0.063 sec/batch; 104h:19m:23s remains)
INFO - root - 2019-11-03 23:56:39.636297: step 39470, total loss = 0.71, predict loss = 0.16 (77.7 examples/sec; 0.051 sec/batch; 85h:12m:40s remains)
INFO - root - 2019-11-03 23:56:40.256938: step 39480, total loss = 0.66, predict loss = 0.15 (71.2 examples/sec; 0.056 sec/batch; 93h:00m:15s remains)
INFO - root - 2019-11-03 23:56:40.877843: step 39490, total loss = 0.52, predict loss = 0.11 (69.6 examples/sec; 0.057 sec/batch; 95h:11m:57s remains)
INFO - root - 2019-11-03 23:56:41.509849: step 39500, total loss = 0.57, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 93h:19m:50s remains)
INFO - root - 2019-11-03 23:56:42.133072: step 39510, total loss = 0.56, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 94h:13m:11s remains)
INFO - root - 2019-11-03 23:56:42.766082: step 39520, total loss = 0.65, predict loss = 0.16 (63.9 examples/sec; 0.063 sec/batch; 103h:34m:55s remains)
INFO - root - 2019-11-03 23:56:43.426320: step 39530, total loss = 0.45, predict loss = 0.10 (72.8 examples/sec; 0.055 sec/batch; 91h:00m:38s remains)
INFO - root - 2019-11-03 23:56:44.097372: step 39540, total loss = 0.51, predict loss = 0.12 (57.0 examples/sec; 0.070 sec/batch; 116h:12m:13s remains)
INFO - root - 2019-11-03 23:56:44.720121: step 39550, total loss = 0.32, predict loss = 0.06 (76.7 examples/sec; 0.052 sec/batch; 86h:22m:28s remains)
INFO - root - 2019-11-03 23:56:45.328399: step 39560, total loss = 0.46, predict loss = 0.11 (77.7 examples/sec; 0.051 sec/batch; 85h:10m:49s remains)
INFO - root - 2019-11-03 23:56:45.940503: step 39570, total loss = 0.54, predict loss = 0.13 (79.8 examples/sec; 0.050 sec/batch; 83h:00m:52s remains)
INFO - root - 2019-11-03 23:56:46.539761: step 39580, total loss = 0.60, predict loss = 0.14 (74.8 examples/sec; 0.053 sec/batch; 88h:34m:17s remains)
INFO - root - 2019-11-03 23:56:47.119344: step 39590, total loss = 0.66, predict loss = 0.16 (77.3 examples/sec; 0.052 sec/batch; 85h:39m:48s remains)
INFO - root - 2019-11-03 23:56:47.766845: step 39600, total loss = 0.72, predict loss = 0.17 (66.4 examples/sec; 0.060 sec/batch; 99h:43m:20s remains)
INFO - root - 2019-11-03 23:56:48.387034: step 39610, total loss = 1.06, predict loss = 0.24 (73.0 examples/sec; 0.055 sec/batch; 90h:43m:18s remains)
INFO - root - 2019-11-03 23:56:49.042947: step 39620, total loss = 0.80, predict loss = 0.19 (66.6 examples/sec; 0.060 sec/batch; 99h:28m:05s remains)
INFO - root - 2019-11-03 23:56:49.672193: step 39630, total loss = 0.94, predict loss = 0.21 (67.7 examples/sec; 0.059 sec/batch; 97h:45m:21s remains)
INFO - root - 2019-11-03 23:56:50.345648: step 39640, total loss = 0.78, predict loss = 0.18 (69.8 examples/sec; 0.057 sec/batch; 94h:48m:55s remains)
INFO - root - 2019-11-03 23:56:50.966418: step 39650, total loss = 0.77, predict loss = 0.19 (78.2 examples/sec; 0.051 sec/batch; 84h:44m:00s remains)
INFO - root - 2019-11-03 23:56:51.576779: step 39660, total loss = 0.72, predict loss = 0.17 (79.3 examples/sec; 0.050 sec/batch; 83h:28m:05s remains)
INFO - root - 2019-11-03 23:56:52.189570: step 39670, total loss = 0.91, predict loss = 0.21 (68.7 examples/sec; 0.058 sec/batch; 96h:24m:43s remains)
INFO - root - 2019-11-03 23:56:52.790077: step 39680, total loss = 0.73, predict loss = 0.18 (73.9 examples/sec; 0.054 sec/batch; 89h:35m:26s remains)
INFO - root - 2019-11-03 23:56:53.438119: step 39690, total loss = 0.73, predict loss = 0.17 (65.6 examples/sec; 0.061 sec/batch; 101h:01m:16s remains)
INFO - root - 2019-11-03 23:56:54.122288: step 39700, total loss = 0.64, predict loss = 0.15 (64.6 examples/sec; 0.062 sec/batch; 102h:34m:41s remains)
INFO - root - 2019-11-03 23:56:54.793247: step 39710, total loss = 0.60, predict loss = 0.13 (74.6 examples/sec; 0.054 sec/batch; 88h:48m:19s remains)
INFO - root - 2019-11-03 23:56:55.434526: step 39720, total loss = 0.60, predict loss = 0.14 (73.0 examples/sec; 0.055 sec/batch; 90h:46m:28s remains)
INFO - root - 2019-11-03 23:56:56.034990: step 39730, total loss = 0.53, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 92h:10m:42s remains)
INFO - root - 2019-11-03 23:56:56.647731: step 39740, total loss = 0.52, predict loss = 0.11 (77.7 examples/sec; 0.051 sec/batch; 85h:11m:11s remains)
INFO - root - 2019-11-03 23:56:57.272328: step 39750, total loss = 0.72, predict loss = 0.16 (67.1 examples/sec; 0.060 sec/batch; 98h:41m:25s remains)
INFO - root - 2019-11-03 23:56:57.925332: step 39760, total loss = 0.61, predict loss = 0.14 (72.0 examples/sec; 0.056 sec/batch; 92h:00m:19s remains)
INFO - root - 2019-11-03 23:56:58.576750: step 39770, total loss = 0.51, predict loss = 0.13 (65.4 examples/sec; 0.061 sec/batch; 101h:14m:12s remains)
INFO - root - 2019-11-03 23:56:59.268967: step 39780, total loss = 0.61, predict loss = 0.15 (70.3 examples/sec; 0.057 sec/batch; 94h:12m:55s remains)
INFO - root - 2019-11-03 23:56:59.911986: step 39790, total loss = 0.61, predict loss = 0.13 (68.2 examples/sec; 0.059 sec/batch; 97h:08m:48s remains)
INFO - root - 2019-11-03 23:57:00.532229: step 39800, total loss = 0.71, predict loss = 0.15 (75.7 examples/sec; 0.053 sec/batch; 87h:27m:37s remains)
INFO - root - 2019-11-03 23:57:01.187452: step 39810, total loss = 0.75, predict loss = 0.18 (68.1 examples/sec; 0.059 sec/batch; 97h:11m:40s remains)
INFO - root - 2019-11-03 23:57:01.882265: step 39820, total loss = 0.67, predict loss = 0.16 (63.0 examples/sec; 0.063 sec/batch; 105h:04m:20s remains)
INFO - root - 2019-11-03 23:57:02.550472: step 39830, total loss = 0.58, predict loss = 0.13 (73.7 examples/sec; 0.054 sec/batch; 89h:54m:23s remains)
INFO - root - 2019-11-03 23:57:03.169209: step 39840, total loss = 0.67, predict loss = 0.14 (72.0 examples/sec; 0.056 sec/batch; 91h:56m:19s remains)
INFO - root - 2019-11-03 23:57:03.791904: step 39850, total loss = 0.42, predict loss = 0.10 (71.2 examples/sec; 0.056 sec/batch; 93h:02m:20s remains)
INFO - root - 2019-11-03 23:57:04.418070: step 39860, total loss = 0.58, predict loss = 0.13 (66.7 examples/sec; 0.060 sec/batch; 99h:20m:49s remains)
INFO - root - 2019-11-03 23:57:05.022560: step 39870, total loss = 0.54, predict loss = 0.13 (76.9 examples/sec; 0.052 sec/batch; 86h:04m:54s remains)
INFO - root - 2019-11-03 23:57:05.643111: step 39880, total loss = 0.62, predict loss = 0.15 (77.3 examples/sec; 0.052 sec/batch; 85h:37m:38s remains)
INFO - root - 2019-11-03 23:57:06.282096: step 39890, total loss = 0.49, predict loss = 0.11 (64.0 examples/sec; 0.062 sec/batch; 103h:27m:38s remains)
INFO - root - 2019-11-03 23:57:06.928673: step 39900, total loss = 0.53, predict loss = 0.13 (70.0 examples/sec; 0.057 sec/batch; 94h:33m:35s remains)
INFO - root - 2019-11-03 23:57:07.516309: step 39910, total loss = 0.51, predict loss = 0.12 (78.9 examples/sec; 0.051 sec/batch; 83h:56m:21s remains)
INFO - root - 2019-11-03 23:57:08.095957: step 39920, total loss = 0.45, predict loss = 0.10 (84.1 examples/sec; 0.048 sec/batch; 78h:46m:41s remains)
INFO - root - 2019-11-03 23:57:08.710445: step 39930, total loss = 0.51, predict loss = 0.12 (82.8 examples/sec; 0.048 sec/batch; 80h:01m:24s remains)
INFO - root - 2019-11-03 23:57:09.320860: step 39940, total loss = 0.53, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 97h:12m:39s remains)
INFO - root - 2019-11-03 23:57:09.936865: step 39950, total loss = 0.44, predict loss = 0.09 (68.1 examples/sec; 0.059 sec/batch; 97h:11m:05s remains)
INFO - root - 2019-11-03 23:57:10.561082: step 39960, total loss = 0.60, predict loss = 0.14 (80.7 examples/sec; 0.050 sec/batch; 82h:01m:56s remains)
INFO - root - 2019-11-03 23:57:11.203107: step 39970, total loss = 0.55, predict loss = 0.13 (68.2 examples/sec; 0.059 sec/batch; 97h:09m:44s remains)
INFO - root - 2019-11-03 23:57:11.860326: step 39980, total loss = 0.43, predict loss = 0.09 (64.1 examples/sec; 0.062 sec/batch; 103h:17m:27s remains)
INFO - root - 2019-11-03 23:57:12.527464: step 39990, total loss = 0.50, predict loss = 0.11 (76.4 examples/sec; 0.052 sec/batch; 86h:43m:47s remains)
INFO - root - 2019-11-03 23:57:13.182228: step 40000, total loss = 0.60, predict loss = 0.14 (78.4 examples/sec; 0.051 sec/batch; 84h:28m:15s remains)
INFO - root - 2019-11-03 23:57:13.839146: step 40010, total loss = 0.53, predict loss = 0.12 (73.6 examples/sec; 0.054 sec/batch; 89h:59m:28s remains)
INFO - root - 2019-11-03 23:57:14.505164: step 40020, total loss = 0.63, predict loss = 0.15 (75.9 examples/sec; 0.053 sec/batch; 87h:14m:51s remains)
INFO - root - 2019-11-03 23:57:15.123810: step 40030, total loss = 0.67, predict loss = 0.16 (67.7 examples/sec; 0.059 sec/batch; 97h:46m:27s remains)
INFO - root - 2019-11-03 23:57:15.760085: step 40040, total loss = 0.64, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 91h:45m:30s remains)
INFO - root - 2019-11-03 23:57:16.446502: step 40050, total loss = 0.64, predict loss = 0.14 (68.4 examples/sec; 0.058 sec/batch; 96h:50m:20s remains)
INFO - root - 2019-11-03 23:57:17.087423: step 40060, total loss = 0.65, predict loss = 0.16 (71.5 examples/sec; 0.056 sec/batch; 92h:37m:02s remains)
INFO - root - 2019-11-03 23:57:17.750346: step 40070, total loss = 0.78, predict loss = 0.19 (70.2 examples/sec; 0.057 sec/batch; 94h:23m:59s remains)
INFO - root - 2019-11-03 23:57:18.352197: step 40080, total loss = 0.69, predict loss = 0.16 (74.0 examples/sec; 0.054 sec/batch; 89h:32m:28s remains)
INFO - root - 2019-11-03 23:57:18.992877: step 40090, total loss = 0.79, predict loss = 0.19 (74.0 examples/sec; 0.054 sec/batch; 89h:30m:55s remains)
INFO - root - 2019-11-03 23:57:19.635887: step 40100, total loss = 0.79, predict loss = 0.19 (75.7 examples/sec; 0.053 sec/batch; 87h:26m:30s remains)
INFO - root - 2019-11-03 23:57:20.228671: step 40110, total loss = 0.65, predict loss = 0.15 (75.3 examples/sec; 0.053 sec/batch; 87h:55m:14s remains)
INFO - root - 2019-11-03 23:57:20.850244: step 40120, total loss = 0.68, predict loss = 0.16 (68.4 examples/sec; 0.058 sec/batch; 96h:48m:38s remains)
INFO - root - 2019-11-03 23:57:21.549312: step 40130, total loss = 0.62, predict loss = 0.14 (55.6 examples/sec; 0.072 sec/batch; 119h:08m:51s remains)
INFO - root - 2019-11-03 23:57:22.166996: step 40140, total loss = 0.53, predict loss = 0.12 (77.8 examples/sec; 0.051 sec/batch; 85h:07m:47s remains)
INFO - root - 2019-11-03 23:57:22.810246: step 40150, total loss = 0.57, predict loss = 0.12 (72.1 examples/sec; 0.056 sec/batch; 91h:53m:17s remains)
INFO - root - 2019-11-03 23:57:23.431204: step 40160, total loss = 0.53, predict loss = 0.12 (77.0 examples/sec; 0.052 sec/batch; 86h:01m:11s remains)
INFO - root - 2019-11-03 23:57:24.074220: step 40170, total loss = 0.34, predict loss = 0.07 (69.5 examples/sec; 0.058 sec/batch; 95h:14m:38s remains)
INFO - root - 2019-11-03 23:57:24.715266: step 40180, total loss = 0.53, predict loss = 0.12 (66.6 examples/sec; 0.060 sec/batch; 99h:29m:31s remains)
INFO - root - 2019-11-03 23:57:25.334773: step 40190, total loss = 0.54, predict loss = 0.12 (72.0 examples/sec; 0.056 sec/batch; 91h:56m:08s remains)
INFO - root - 2019-11-03 23:57:25.991276: step 40200, total loss = 0.48, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 98h:23m:05s remains)
INFO - root - 2019-11-03 23:57:26.620272: step 40210, total loss = 0.52, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 96h:03m:17s remains)
INFO - root - 2019-11-03 23:57:27.265047: step 40220, total loss = 0.48, predict loss = 0.11 (75.1 examples/sec; 0.053 sec/batch; 88h:11m:09s remains)
INFO - root - 2019-11-03 23:57:27.905634: step 40230, total loss = 0.45, predict loss = 0.10 (76.1 examples/sec; 0.053 sec/batch; 87h:04m:09s remains)
INFO - root - 2019-11-03 23:57:28.523127: step 40240, total loss = 0.55, predict loss = 0.13 (72.3 examples/sec; 0.055 sec/batch; 91h:34m:24s remains)
INFO - root - 2019-11-03 23:57:29.172428: step 40250, total loss = 0.55, predict loss = 0.12 (69.7 examples/sec; 0.057 sec/batch; 95h:03m:23s remains)
INFO - root - 2019-11-03 23:57:29.808259: step 40260, total loss = 0.66, predict loss = 0.16 (73.9 examples/sec; 0.054 sec/batch; 89h:36m:19s remains)
INFO - root - 2019-11-03 23:57:30.450337: step 40270, total loss = 0.68, predict loss = 0.16 (76.4 examples/sec; 0.052 sec/batch; 86h:40m:33s remains)
INFO - root - 2019-11-03 23:57:31.095308: step 40280, total loss = 0.70, predict loss = 0.17 (66.6 examples/sec; 0.060 sec/batch; 99h:22m:51s remains)
INFO - root - 2019-11-03 23:57:31.745874: step 40290, total loss = 0.62, predict loss = 0.15 (62.3 examples/sec; 0.064 sec/batch; 106h:16m:18s remains)
INFO - root - 2019-11-03 23:57:32.390514: step 40300, total loss = 0.58, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 94h:14m:41s remains)
INFO - root - 2019-11-03 23:57:33.065682: step 40310, total loss = 0.72, predict loss = 0.17 (69.2 examples/sec; 0.058 sec/batch; 95h:38m:19s remains)
INFO - root - 2019-11-03 23:57:33.734127: step 40320, total loss = 0.69, predict loss = 0.17 (61.5 examples/sec; 0.065 sec/batch; 107h:42m:36s remains)
INFO - root - 2019-11-03 23:57:34.341227: step 40330, total loss = 0.61, predict loss = 0.16 (77.0 examples/sec; 0.052 sec/batch; 85h:58m:18s remains)
INFO - root - 2019-11-03 23:57:34.956841: step 40340, total loss = 0.70, predict loss = 0.17 (74.2 examples/sec; 0.054 sec/batch; 89h:16m:03s remains)
INFO - root - 2019-11-03 23:57:35.599759: step 40350, total loss = 0.61, predict loss = 0.15 (67.1 examples/sec; 0.060 sec/batch; 98h:42m:27s remains)
INFO - root - 2019-11-03 23:57:36.258521: step 40360, total loss = 0.77, predict loss = 0.17 (67.8 examples/sec; 0.059 sec/batch; 97h:40m:58s remains)
INFO - root - 2019-11-03 23:57:36.933231: step 40370, total loss = 0.67, predict loss = 0.15 (62.1 examples/sec; 0.064 sec/batch; 106h:42m:36s remains)
INFO - root - 2019-11-03 23:57:37.594654: step 40380, total loss = 0.68, predict loss = 0.16 (71.3 examples/sec; 0.056 sec/batch; 92h:53m:12s remains)
INFO - root - 2019-11-03 23:57:38.238021: step 40390, total loss = 0.58, predict loss = 0.14 (73.5 examples/sec; 0.054 sec/batch; 90h:04m:13s remains)
INFO - root - 2019-11-03 23:57:38.914223: step 40400, total loss = 0.66, predict loss = 0.16 (61.3 examples/sec; 0.065 sec/batch; 107h:56m:07s remains)
INFO - root - 2019-11-03 23:57:39.571666: step 40410, total loss = 0.55, predict loss = 0.13 (70.7 examples/sec; 0.057 sec/batch; 93h:39m:27s remains)
INFO - root - 2019-11-03 23:57:40.227234: step 40420, total loss = 0.62, predict loss = 0.14 (67.7 examples/sec; 0.059 sec/batch; 97h:49m:32s remains)
INFO - root - 2019-11-03 23:57:40.866296: step 40430, total loss = 0.51, predict loss = 0.12 (77.5 examples/sec; 0.052 sec/batch; 85h:25m:43s remains)
INFO - root - 2019-11-03 23:57:41.484215: step 40440, total loss = 0.51, predict loss = 0.12 (75.3 examples/sec; 0.053 sec/batch; 87h:55m:48s remains)
INFO - root - 2019-11-03 23:57:42.125558: step 40450, total loss = 0.56, predict loss = 0.13 (74.0 examples/sec; 0.054 sec/batch; 89h:32m:17s remains)
INFO - root - 2019-11-03 23:57:42.756054: step 40460, total loss = 0.58, predict loss = 0.13 (67.5 examples/sec; 0.059 sec/batch; 98h:05m:29s remains)
INFO - root - 2019-11-03 23:57:43.387177: step 40470, total loss = 0.62, predict loss = 0.14 (78.2 examples/sec; 0.051 sec/batch; 84h:41m:44s remains)
INFO - root - 2019-11-03 23:57:44.059990: step 40480, total loss = 0.48, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 98h:25m:27s remains)
INFO - root - 2019-11-03 23:57:44.697073: step 40490, total loss = 0.51, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 94h:52m:00s remains)
INFO - root - 2019-11-03 23:57:45.310651: step 40500, total loss = 0.54, predict loss = 0.13 (83.1 examples/sec; 0.048 sec/batch; 79h:42m:31s remains)
INFO - root - 2019-11-03 23:57:45.973000: step 40510, total loss = 0.53, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 102h:59m:25s remains)
INFO - root - 2019-11-03 23:57:46.620506: step 40520, total loss = 0.59, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 93h:15m:56s remains)
INFO - root - 2019-11-03 23:57:47.261797: step 40530, total loss = 0.65, predict loss = 0.15 (68.3 examples/sec; 0.059 sec/batch; 96h:53m:36s remains)
INFO - root - 2019-11-03 23:57:47.901919: step 40540, total loss = 0.68, predict loss = 0.16 (67.8 examples/sec; 0.059 sec/batch; 97h:40m:42s remains)
INFO - root - 2019-11-03 23:57:48.583835: step 40550, total loss = 0.54, predict loss = 0.12 (66.4 examples/sec; 0.060 sec/batch; 99h:39m:57s remains)
INFO - root - 2019-11-03 23:57:49.248511: step 40560, total loss = 0.54, predict loss = 0.13 (76.8 examples/sec; 0.052 sec/batch; 86h:12m:03s remains)
INFO - root - 2019-11-03 23:57:49.863701: step 40570, total loss = 0.73, predict loss = 0.16 (72.8 examples/sec; 0.055 sec/batch; 90h:54m:58s remains)
INFO - root - 2019-11-03 23:57:50.488298: step 40580, total loss = 0.55, predict loss = 0.12 (72.9 examples/sec; 0.055 sec/batch; 90h:49m:24s remains)
INFO - root - 2019-11-03 23:57:51.134630: step 40590, total loss = 0.74, predict loss = 0.18 (69.3 examples/sec; 0.058 sec/batch; 95h:29m:19s remains)
INFO - root - 2019-11-03 23:57:51.778006: step 40600, total loss = 0.62, predict loss = 0.15 (75.5 examples/sec; 0.053 sec/batch; 87h:40m:10s remains)
INFO - root - 2019-11-03 23:57:52.429377: step 40610, total loss = 0.63, predict loss = 0.15 (68.2 examples/sec; 0.059 sec/batch; 97h:07m:40s remains)
INFO - root - 2019-11-03 23:57:53.088369: step 40620, total loss = 0.65, predict loss = 0.14 (78.2 examples/sec; 0.051 sec/batch; 84h:43m:16s remains)
INFO - root - 2019-11-03 23:57:53.755058: step 40630, total loss = 0.62, predict loss = 0.14 (63.2 examples/sec; 0.063 sec/batch; 104h:45m:20s remains)
INFO - root - 2019-11-03 23:57:54.424864: step 40640, total loss = 0.72, predict loss = 0.16 (60.8 examples/sec; 0.066 sec/batch; 108h:52m:01s remains)
INFO - root - 2019-11-03 23:57:55.061117: step 40650, total loss = 0.74, predict loss = 0.18 (81.9 examples/sec; 0.049 sec/batch; 80h:53m:09s remains)
INFO - root - 2019-11-03 23:57:55.744856: step 40660, total loss = 0.79, predict loss = 0.18 (77.4 examples/sec; 0.052 sec/batch; 85h:30m:01s remains)
INFO - root - 2019-11-03 23:57:56.390320: step 40670, total loss = 0.79, predict loss = 0.19 (72.5 examples/sec; 0.055 sec/batch; 91h:19m:22s remains)
INFO - root - 2019-11-03 23:57:57.031308: step 40680, total loss = 0.75, predict loss = 0.17 (70.3 examples/sec; 0.057 sec/batch; 94h:14m:06s remains)
INFO - root - 2019-11-03 23:57:57.661484: step 40690, total loss = 0.73, predict loss = 0.17 (64.3 examples/sec; 0.062 sec/batch; 102h:59m:04s remains)
INFO - root - 2019-11-03 23:57:58.287851: step 40700, total loss = 0.66, predict loss = 0.15 (80.4 examples/sec; 0.050 sec/batch; 82h:22m:15s remains)
INFO - root - 2019-11-03 23:57:58.896978: step 40710, total loss = 0.77, predict loss = 0.18 (69.8 examples/sec; 0.057 sec/batch; 94h:50m:09s remains)
INFO - root - 2019-11-03 23:57:59.525290: step 40720, total loss = 0.77, predict loss = 0.18 (71.5 examples/sec; 0.056 sec/batch; 92h:35m:40s remains)
INFO - root - 2019-11-03 23:58:00.198082: step 40730, total loss = 0.75, predict loss = 0.18 (63.1 examples/sec; 0.063 sec/batch; 104h:53m:24s remains)
INFO - root - 2019-11-03 23:58:00.803939: step 40740, total loss = 0.88, predict loss = 0.21 (77.7 examples/sec; 0.051 sec/batch; 85h:10m:11s remains)
INFO - root - 2019-11-03 23:58:01.420501: step 40750, total loss = 0.53, predict loss = 0.12 (70.9 examples/sec; 0.056 sec/batch; 93h:20m:31s remains)
INFO - root - 2019-11-03 23:58:02.077489: step 40760, total loss = 0.54, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 95h:25m:10s remains)
INFO - root - 2019-11-03 23:58:02.724490: step 40770, total loss = 0.67, predict loss = 0.16 (62.3 examples/sec; 0.064 sec/batch; 106h:12m:24s remains)
INFO - root - 2019-11-03 23:58:03.455963: step 40780, total loss = 0.55, predict loss = 0.13 (60.1 examples/sec; 0.067 sec/batch; 110h:13m:57s remains)
INFO - root - 2019-11-03 23:58:04.102821: step 40790, total loss = 0.63, predict loss = 0.15 (65.9 examples/sec; 0.061 sec/batch; 100h:29m:19s remains)
INFO - root - 2019-11-03 23:58:04.707801: step 40800, total loss = 0.74, predict loss = 0.18 (70.8 examples/sec; 0.056 sec/batch; 93h:28m:08s remains)
INFO - root - 2019-11-03 23:58:05.328872: step 40810, total loss = 0.67, predict loss = 0.16 (73.3 examples/sec; 0.055 sec/batch; 90h:22m:31s remains)
INFO - root - 2019-11-03 23:58:05.939918: step 40820, total loss = 0.66, predict loss = 0.15 (68.9 examples/sec; 0.058 sec/batch; 96h:05m:18s remains)
INFO - root - 2019-11-03 23:58:06.590864: step 40830, total loss = 0.76, predict loss = 0.16 (63.6 examples/sec; 0.063 sec/batch; 104h:03m:06s remains)
INFO - root - 2019-11-03 23:58:07.262634: step 40840, total loss = 0.58, predict loss = 0.13 (77.7 examples/sec; 0.052 sec/batch; 85h:16m:04s remains)
INFO - root - 2019-11-03 23:58:07.896834: step 40850, total loss = 0.84, predict loss = 0.20 (76.5 examples/sec; 0.052 sec/batch; 86h:36m:18s remains)
INFO - root - 2019-11-03 23:58:08.518284: step 40860, total loss = 0.72, predict loss = 0.16 (71.3 examples/sec; 0.056 sec/batch; 92h:55m:20s remains)
INFO - root - 2019-11-03 23:58:09.149810: step 40870, total loss = 0.79, predict loss = 0.19 (70.5 examples/sec; 0.057 sec/batch; 93h:58m:16s remains)
INFO - root - 2019-11-03 23:58:09.791507: step 40880, total loss = 0.68, predict loss = 0.15 (69.8 examples/sec; 0.057 sec/batch; 94h:51m:06s remains)
INFO - root - 2019-11-03 23:58:10.419417: step 40890, total loss = 0.69, predict loss = 0.16 (76.8 examples/sec; 0.052 sec/batch; 86h:12m:45s remains)
INFO - root - 2019-11-03 23:58:10.878936: step 40900, total loss = 0.56, predict loss = 0.13 (97.3 examples/sec; 0.041 sec/batch; 68h:03m:01s remains)
INFO - root - 2019-11-03 23:58:11.346700: step 40910, total loss = 0.58, predict loss = 0.13 (85.2 examples/sec; 0.047 sec/batch; 77h:45m:30s remains)
INFO - root - 2019-11-03 23:58:12.459752: step 40920, total loss = 0.37, predict loss = 0.08 (73.0 examples/sec; 0.055 sec/batch; 90h:43m:18s remains)
INFO - root - 2019-11-03 23:58:13.097782: step 40930, total loss = 0.76, predict loss = 0.16 (74.4 examples/sec; 0.054 sec/batch; 88h:58m:01s remains)
INFO - root - 2019-11-03 23:58:13.752240: step 40940, total loss = 0.68, predict loss = 0.16 (72.1 examples/sec; 0.055 sec/batch; 91h:48m:50s remains)
INFO - root - 2019-11-03 23:58:14.424433: step 40950, total loss = 0.62, predict loss = 0.15 (66.4 examples/sec; 0.060 sec/batch; 99h:38m:56s remains)
INFO - root - 2019-11-03 23:58:15.057730: step 40960, total loss = 0.54, predict loss = 0.12 (64.1 examples/sec; 0.062 sec/batch; 103h:17m:54s remains)
INFO - root - 2019-11-03 23:58:15.770836: step 40970, total loss = 0.54, predict loss = 0.11 (70.0 examples/sec; 0.057 sec/batch; 94h:39m:02s remains)
INFO - root - 2019-11-03 23:58:16.413401: step 40980, total loss = 0.62, predict loss = 0.15 (67.2 examples/sec; 0.060 sec/batch; 98h:34m:15s remains)
INFO - root - 2019-11-03 23:58:17.017139: step 40990, total loss = 0.71, predict loss = 0.16 (66.7 examples/sec; 0.060 sec/batch; 99h:15m:05s remains)
INFO - root - 2019-11-03 23:58:17.698388: step 41000, total loss = 0.54, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 92h:29m:39s remains)
INFO - root - 2019-11-03 23:58:18.341007: step 41010, total loss = 0.88, predict loss = 0.20 (65.9 examples/sec; 0.061 sec/batch; 100h:31m:19s remains)
INFO - root - 2019-11-03 23:58:18.965802: step 41020, total loss = 0.85, predict loss = 0.21 (80.3 examples/sec; 0.050 sec/batch; 82h:28m:52s remains)
INFO - root - 2019-11-03 23:58:19.591739: step 41030, total loss = 0.68, predict loss = 0.16 (67.0 examples/sec; 0.060 sec/batch; 98h:45m:03s remains)
INFO - root - 2019-11-03 23:58:20.229033: step 41040, total loss = 0.72, predict loss = 0.16 (72.9 examples/sec; 0.055 sec/batch; 90h:48m:27s remains)
INFO - root - 2019-11-03 23:58:20.878919: step 41050, total loss = 0.73, predict loss = 0.17 (75.4 examples/sec; 0.053 sec/batch; 87h:47m:47s remains)
INFO - root - 2019-11-03 23:58:21.539580: step 41060, total loss = 0.65, predict loss = 0.15 (69.3 examples/sec; 0.058 sec/batch; 95h:28m:22s remains)
INFO - root - 2019-11-03 23:58:22.184166: step 41070, total loss = 0.56, predict loss = 0.13 (70.4 examples/sec; 0.057 sec/batch; 94h:02m:26s remains)
INFO - root - 2019-11-03 23:58:22.787256: step 41080, total loss = 0.58, predict loss = 0.14 (67.1 examples/sec; 0.060 sec/batch; 98h:41m:38s remains)
INFO - root - 2019-11-03 23:58:23.418692: step 41090, total loss = 0.60, predict loss = 0.13 (70.1 examples/sec; 0.057 sec/batch; 94h:30m:04s remains)
INFO - root - 2019-11-03 23:58:24.061325: step 41100, total loss = 0.50, predict loss = 0.10 (74.9 examples/sec; 0.053 sec/batch; 88h:24m:09s remains)
INFO - root - 2019-11-03 23:58:24.687719: step 41110, total loss = 0.40, predict loss = 0.09 (65.0 examples/sec; 0.062 sec/batch; 101h:56m:22s remains)
INFO - root - 2019-11-03 23:58:25.275401: step 41120, total loss = 0.59, predict loss = 0.13 (81.4 examples/sec; 0.049 sec/batch; 81h:19m:42s remains)
INFO - root - 2019-11-03 23:58:25.953020: step 41130, total loss = 0.52, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 97h:47m:35s remains)
INFO - root - 2019-11-03 23:58:26.648257: step 41140, total loss = 0.66, predict loss = 0.16 (63.3 examples/sec; 0.063 sec/batch; 104h:37m:29s remains)
INFO - root - 2019-11-03 23:58:27.286176: step 41150, total loss = 0.64, predict loss = 0.14 (74.6 examples/sec; 0.054 sec/batch; 88h:45m:45s remains)
INFO - root - 2019-11-03 23:58:27.912089: step 41160, total loss = 0.63, predict loss = 0.16 (75.4 examples/sec; 0.053 sec/batch; 87h:46m:18s remains)
INFO - root - 2019-11-03 23:58:28.546296: step 41170, total loss = 0.78, predict loss = 0.17 (64.8 examples/sec; 0.062 sec/batch; 102h:14m:19s remains)
INFO - root - 2019-11-03 23:58:29.209528: step 41180, total loss = 0.94, predict loss = 0.22 (65.2 examples/sec; 0.061 sec/batch; 101h:29m:31s remains)
INFO - root - 2019-11-03 23:58:29.907565: step 41190, total loss = 0.56, predict loss = 0.13 (68.7 examples/sec; 0.058 sec/batch; 96h:22m:36s remains)
INFO - root - 2019-11-03 23:58:30.556807: step 41200, total loss = 0.66, predict loss = 0.15 (66.4 examples/sec; 0.060 sec/batch; 99h:42m:29s remains)
INFO - root - 2019-11-03 23:58:31.201595: step 41210, total loss = 0.76, predict loss = 0.18 (70.3 examples/sec; 0.057 sec/batch; 94h:14m:15s remains)
INFO - root - 2019-11-03 23:58:31.812486: step 41220, total loss = 0.81, predict loss = 0.19 (67.2 examples/sec; 0.060 sec/batch; 98h:32m:00s remains)
INFO - root - 2019-11-03 23:58:32.424210: step 41230, total loss = 0.99, predict loss = 0.23 (80.7 examples/sec; 0.050 sec/batch; 82h:04m:58s remains)
INFO - root - 2019-11-03 23:58:33.053019: step 41240, total loss = 0.70, predict loss = 0.15 (73.7 examples/sec; 0.054 sec/batch; 89h:48m:11s remains)
INFO - root - 2019-11-03 23:58:33.660508: step 41250, total loss = 0.60, predict loss = 0.14 (83.5 examples/sec; 0.048 sec/batch; 79h:16m:38s remains)
INFO - root - 2019-11-03 23:58:34.291905: step 41260, total loss = 0.66, predict loss = 0.15 (78.2 examples/sec; 0.051 sec/batch; 84h:42m:41s remains)
INFO - root - 2019-11-03 23:58:34.905968: step 41270, total loss = 0.79, predict loss = 0.19 (64.7 examples/sec; 0.062 sec/batch; 102h:21m:18s remains)
INFO - root - 2019-11-03 23:58:35.550443: step 41280, total loss = 0.73, predict loss = 0.17 (75.8 examples/sec; 0.053 sec/batch; 87h:22m:06s remains)
INFO - root - 2019-11-03 23:58:36.172223: step 41290, total loss = 0.64, predict loss = 0.16 (77.8 examples/sec; 0.051 sec/batch; 85h:04m:54s remains)
INFO - root - 2019-11-03 23:58:36.807731: step 41300, total loss = 0.62, predict loss = 0.14 (75.5 examples/sec; 0.053 sec/batch; 87h:43m:23s remains)
INFO - root - 2019-11-03 23:58:37.430245: step 41310, total loss = 0.74, predict loss = 0.16 (69.8 examples/sec; 0.057 sec/batch; 94h:49m:12s remains)
INFO - root - 2019-11-03 23:58:38.036963: step 41320, total loss = 0.86, predict loss = 0.20 (80.9 examples/sec; 0.049 sec/batch; 81h:51m:48s remains)
INFO - root - 2019-11-03 23:58:38.633276: step 41330, total loss = 0.70, predict loss = 0.15 (70.8 examples/sec; 0.056 sec/batch; 93h:29m:29s remains)
INFO - root - 2019-11-03 23:58:39.288316: step 41340, total loss = 0.70, predict loss = 0.15 (61.9 examples/sec; 0.065 sec/batch; 106h:55m:50s remains)
INFO - root - 2019-11-03 23:58:39.951368: step 41350, total loss = 0.51, predict loss = 0.12 (64.2 examples/sec; 0.062 sec/batch; 103h:06m:39s remains)
INFO - root - 2019-11-03 23:58:40.574876: step 41360, total loss = 0.62, predict loss = 0.15 (72.8 examples/sec; 0.055 sec/batch; 90h:53m:41s remains)
INFO - root - 2019-11-03 23:58:41.190291: step 41370, total loss = 0.59, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 94h:08m:11s remains)
INFO - root - 2019-11-03 23:58:41.808789: step 41380, total loss = 0.52, predict loss = 0.10 (68.1 examples/sec; 0.059 sec/batch; 97h:11m:33s remains)
INFO - root - 2019-11-03 23:58:42.459150: step 41390, total loss = 0.71, predict loss = 0.18 (74.6 examples/sec; 0.054 sec/batch; 88h:46m:00s remains)
INFO - root - 2019-11-03 23:58:43.124169: step 41400, total loss = 0.54, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 90h:37m:18s remains)
INFO - root - 2019-11-03 23:58:43.794561: step 41410, total loss = 0.62, predict loss = 0.15 (66.8 examples/sec; 0.060 sec/batch; 99h:08m:51s remains)
INFO - root - 2019-11-03 23:58:44.457814: step 41420, total loss = 0.81, predict loss = 0.19 (76.3 examples/sec; 0.052 sec/batch; 86h:47m:16s remains)
INFO - root - 2019-11-03 23:58:45.058125: step 41430, total loss = 0.82, predict loss = 0.19 (78.3 examples/sec; 0.051 sec/batch; 84h:31m:02s remains)
INFO - root - 2019-11-03 23:58:45.726954: step 41440, total loss = 0.73, predict loss = 0.17 (62.7 examples/sec; 0.064 sec/batch; 105h:32m:06s remains)
INFO - root - 2019-11-03 23:58:46.362663: step 41450, total loss = 0.56, predict loss = 0.14 (74.8 examples/sec; 0.053 sec/batch; 88h:27m:42s remains)
INFO - root - 2019-11-03 23:58:46.993845: step 41460, total loss = 0.43, predict loss = 0.10 (71.5 examples/sec; 0.056 sec/batch; 92h:34m:47s remains)
INFO - root - 2019-11-03 23:58:47.626375: step 41470, total loss = 0.51, predict loss = 0.12 (73.3 examples/sec; 0.055 sec/batch; 90h:19m:33s remains)
INFO - root - 2019-11-03 23:58:48.263952: step 41480, total loss = 0.51, predict loss = 0.12 (75.7 examples/sec; 0.053 sec/batch; 87h:24m:06s remains)
INFO - root - 2019-11-03 23:58:48.918088: step 41490, total loss = 0.44, predict loss = 0.10 (72.8 examples/sec; 0.055 sec/batch; 90h:55m:40s remains)
INFO - root - 2019-11-03 23:58:49.583542: step 41500, total loss = 0.60, predict loss = 0.15 (72.9 examples/sec; 0.055 sec/batch; 90h:45m:24s remains)
INFO - root - 2019-11-03 23:58:50.285782: step 41510, total loss = 0.43, predict loss = 0.10 (63.0 examples/sec; 0.064 sec/batch; 105h:06m:16s remains)
INFO - root - 2019-11-03 23:58:50.956440: step 41520, total loss = 0.48, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 97h:44m:04s remains)
INFO - root - 2019-11-03 23:58:51.625678: step 41530, total loss = 0.61, predict loss = 0.14 (63.5 examples/sec; 0.063 sec/batch; 104h:19m:49s remains)
INFO - root - 2019-11-03 23:58:52.280734: step 41540, total loss = 0.41, predict loss = 0.09 (69.8 examples/sec; 0.057 sec/batch; 94h:47m:26s remains)
INFO - root - 2019-11-03 23:58:52.955143: step 41550, total loss = 0.54, predict loss = 0.13 (65.4 examples/sec; 0.061 sec/batch; 101h:12m:56s remains)
INFO - root - 2019-11-03 23:58:53.662019: step 41560, total loss = 0.53, predict loss = 0.14 (62.8 examples/sec; 0.064 sec/batch; 105h:29m:08s remains)
INFO - root - 2019-11-03 23:58:54.265772: step 41570, total loss = 0.63, predict loss = 0.14 (76.6 examples/sec; 0.052 sec/batch; 86h:23m:32s remains)
INFO - root - 2019-11-03 23:58:54.898299: step 41580, total loss = 0.72, predict loss = 0.17 (76.3 examples/sec; 0.052 sec/batch; 86h:44m:17s remains)
INFO - root - 2019-11-03 23:58:55.560891: step 41590, total loss = 0.57, predict loss = 0.12 (64.1 examples/sec; 0.062 sec/batch; 103h:20m:42s remains)
INFO - root - 2019-11-03 23:58:56.172399: step 41600, total loss = 0.65, predict loss = 0.14 (66.3 examples/sec; 0.060 sec/batch; 99h:50m:02s remains)
INFO - root - 2019-11-03 23:58:56.759519: step 41610, total loss = 0.61, predict loss = 0.15 (79.8 examples/sec; 0.050 sec/batch; 82h:56m:32s remains)
INFO - root - 2019-11-03 23:58:57.390484: step 41620, total loss = 0.75, predict loss = 0.17 (68.7 examples/sec; 0.058 sec/batch; 96h:20m:57s remains)
INFO - root - 2019-11-03 23:58:57.995474: step 41630, total loss = 0.83, predict loss = 0.19 (77.5 examples/sec; 0.052 sec/batch; 85h:26m:03s remains)
INFO - root - 2019-11-03 23:58:58.657680: step 41640, total loss = 0.61, predict loss = 0.13 (69.3 examples/sec; 0.058 sec/batch; 95h:34m:52s remains)
INFO - root - 2019-11-03 23:58:59.319776: step 41650, total loss = 0.61, predict loss = 0.14 (64.1 examples/sec; 0.062 sec/batch; 103h:12m:35s remains)
INFO - root - 2019-11-03 23:59:00.015507: step 41660, total loss = 0.57, predict loss = 0.14 (67.0 examples/sec; 0.060 sec/batch; 98h:50m:33s remains)
INFO - root - 2019-11-03 23:59:00.723172: step 41670, total loss = 0.55, predict loss = 0.11 (65.2 examples/sec; 0.061 sec/batch; 101h:30m:49s remains)
INFO - root - 2019-11-03 23:59:01.459963: step 41680, total loss = 0.71, predict loss = 0.16 (61.5 examples/sec; 0.065 sec/batch; 107h:40m:56s remains)
INFO - root - 2019-11-03 23:59:02.194522: step 41690, total loss = 0.40, predict loss = 0.09 (59.9 examples/sec; 0.067 sec/batch; 110h:36m:56s remains)
INFO - root - 2019-11-03 23:59:02.981269: step 41700, total loss = 0.74, predict loss = 0.17 (61.8 examples/sec; 0.065 sec/batch; 107h:08m:33s remains)
INFO - root - 2019-11-03 23:59:03.733045: step 41710, total loss = 0.50, predict loss = 0.12 (61.0 examples/sec; 0.066 sec/batch; 108h:30m:53s remains)
INFO - root - 2019-11-03 23:59:04.485346: step 41720, total loss = 0.48, predict loss = 0.11 (67.8 examples/sec; 0.059 sec/batch; 97h:35m:28s remains)
INFO - root - 2019-11-03 23:59:05.203360: step 41730, total loss = 0.58, predict loss = 0.12 (57.8 examples/sec; 0.069 sec/batch; 114h:37m:54s remains)
INFO - root - 2019-11-03 23:59:05.948964: step 41740, total loss = 0.39, predict loss = 0.07 (57.3 examples/sec; 0.070 sec/batch; 115h:27m:15s remains)
INFO - root - 2019-11-03 23:59:06.677176: step 41750, total loss = 0.38, predict loss = 0.08 (65.3 examples/sec; 0.061 sec/batch; 101h:23m:02s remains)
INFO - root - 2019-11-03 23:59:07.439553: step 41760, total loss = 0.38, predict loss = 0.08 (55.9 examples/sec; 0.072 sec/batch; 118h:22m:26s remains)
INFO - root - 2019-11-03 23:59:08.199221: step 41770, total loss = 0.47, predict loss = 0.10 (69.6 examples/sec; 0.057 sec/batch; 95h:06m:27s remains)
INFO - root - 2019-11-03 23:59:08.959641: step 41780, total loss = 0.35, predict loss = 0.07 (58.1 examples/sec; 0.069 sec/batch; 113h:52m:19s remains)
INFO - root - 2019-11-03 23:59:09.680439: step 41790, total loss = 0.28, predict loss = 0.06 (71.2 examples/sec; 0.056 sec/batch; 92h:55m:21s remains)
INFO - root - 2019-11-03 23:59:10.385839: step 41800, total loss = 0.24, predict loss = 0.05 (61.0 examples/sec; 0.066 sec/batch; 108h:31m:11s remains)
INFO - root - 2019-11-03 23:59:11.077123: step 41810, total loss = 0.38, predict loss = 0.08 (71.1 examples/sec; 0.056 sec/batch; 93h:07m:49s remains)
INFO - root - 2019-11-03 23:59:11.702015: step 41820, total loss = 0.61, predict loss = 0.14 (68.2 examples/sec; 0.059 sec/batch; 97h:03m:50s remains)
INFO - root - 2019-11-03 23:59:12.327205: step 41830, total loss = 0.54, predict loss = 0.12 (75.0 examples/sec; 0.053 sec/batch; 88h:17m:05s remains)
INFO - root - 2019-11-03 23:59:12.965452: step 41840, total loss = 0.59, predict loss = 0.14 (61.1 examples/sec; 0.065 sec/batch; 108h:20m:28s remains)
INFO - root - 2019-11-03 23:59:13.604772: step 41850, total loss = 0.67, predict loss = 0.15 (65.4 examples/sec; 0.061 sec/batch; 101h:11m:07s remains)
INFO - root - 2019-11-03 23:59:14.249693: step 41860, total loss = 0.51, predict loss = 0.12 (80.6 examples/sec; 0.050 sec/batch; 82h:10m:42s remains)
INFO - root - 2019-11-03 23:59:14.892962: step 41870, total loss = 0.56, predict loss = 0.14 (85.2 examples/sec; 0.047 sec/batch; 77h:41m:19s remains)
INFO - root - 2019-11-03 23:59:15.558984: step 41880, total loss = 0.40, predict loss = 0.09 (59.3 examples/sec; 0.067 sec/batch; 111h:34m:50s remains)
INFO - root - 2019-11-03 23:59:16.194416: step 41890, total loss = 0.59, predict loss = 0.13 (77.0 examples/sec; 0.052 sec/batch; 86h:00m:54s remains)
INFO - root - 2019-11-03 23:59:16.830214: step 41900, total loss = 0.68, predict loss = 0.17 (70.4 examples/sec; 0.057 sec/batch; 93h:58m:28s remains)
INFO - root - 2019-11-03 23:59:17.451970: step 41910, total loss = 0.64, predict loss = 0.15 (73.9 examples/sec; 0.054 sec/batch; 89h:31m:35s remains)
INFO - root - 2019-11-03 23:59:18.074334: step 41920, total loss = 0.72, predict loss = 0.17 (77.6 examples/sec; 0.052 sec/batch; 85h:16m:49s remains)
INFO - root - 2019-11-03 23:59:18.716453: step 41930, total loss = 0.58, predict loss = 0.13 (68.5 examples/sec; 0.058 sec/batch; 96h:40m:11s remains)
INFO - root - 2019-11-03 23:59:19.383335: step 41940, total loss = 0.50, predict loss = 0.12 (68.2 examples/sec; 0.059 sec/batch; 97h:02m:39s remains)
INFO - root - 2019-11-03 23:59:20.026173: step 41950, total loss = 0.54, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 92h:22m:18s remains)
INFO - root - 2019-11-03 23:59:20.698238: step 41960, total loss = 0.52, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 93h:42m:47s remains)
INFO - root - 2019-11-03 23:59:21.337633: step 41970, total loss = 0.51, predict loss = 0.12 (78.9 examples/sec; 0.051 sec/batch; 83h:54m:09s remains)
INFO - root - 2019-11-03 23:59:21.957268: step 41980, total loss = 0.36, predict loss = 0.08 (71.7 examples/sec; 0.056 sec/batch; 92h:18m:43s remains)
INFO - root - 2019-11-03 23:59:22.622898: step 41990, total loss = 0.35, predict loss = 0.07 (75.5 examples/sec; 0.053 sec/batch; 87h:44m:06s remains)
INFO - root - 2019-11-03 23:59:23.241216: step 42000, total loss = 0.44, predict loss = 0.10 (66.9 examples/sec; 0.060 sec/batch; 98h:58m:47s remains)
INFO - root - 2019-11-03 23:59:23.921990: step 42010, total loss = 0.24, predict loss = 0.05 (62.5 examples/sec; 0.064 sec/batch; 105h:51m:40s remains)
INFO - root - 2019-11-03 23:59:24.582582: step 42020, total loss = 0.26, predict loss = 0.06 (66.9 examples/sec; 0.060 sec/batch; 98h:56m:17s remains)
INFO - root - 2019-11-03 23:59:25.212074: step 42030, total loss = 0.55, predict loss = 0.13 (71.8 examples/sec; 0.056 sec/batch; 92h:14m:32s remains)
INFO - root - 2019-11-03 23:59:25.825532: step 42040, total loss = 0.30, predict loss = 0.06 (70.8 examples/sec; 0.057 sec/batch; 93h:32m:15s remains)
INFO - root - 2019-11-03 23:59:26.462029: step 42050, total loss = 0.37, predict loss = 0.09 (64.4 examples/sec; 0.062 sec/batch; 102h:45m:52s remains)
INFO - root - 2019-11-03 23:59:27.116986: step 42060, total loss = 0.59, predict loss = 0.14 (64.2 examples/sec; 0.062 sec/batch; 103h:06m:29s remains)
INFO - root - 2019-11-03 23:59:27.732184: step 42070, total loss = 0.58, predict loss = 0.12 (68.0 examples/sec; 0.059 sec/batch; 97h:17m:11s remains)
INFO - root - 2019-11-03 23:59:28.369032: step 42080, total loss = 0.91, predict loss = 0.20 (74.9 examples/sec; 0.053 sec/batch; 88h:26m:13s remains)
INFO - root - 2019-11-03 23:59:29.039138: step 42090, total loss = 0.57, predict loss = 0.12 (62.2 examples/sec; 0.064 sec/batch; 106h:22m:54s remains)
INFO - root - 2019-11-03 23:59:29.723956: step 42100, total loss = 0.59, predict loss = 0.15 (76.7 examples/sec; 0.052 sec/batch; 86h:15m:22s remains)
INFO - root - 2019-11-03 23:59:30.332549: step 42110, total loss = 0.36, predict loss = 0.08 (72.5 examples/sec; 0.055 sec/batch; 91h:20m:17s remains)
INFO - root - 2019-11-03 23:59:30.938858: step 42120, total loss = 0.41, predict loss = 0.09 (79.6 examples/sec; 0.050 sec/batch; 83h:11m:00s remains)
INFO - root - 2019-11-03 23:59:31.578404: step 42130, total loss = 0.34, predict loss = 0.06 (68.4 examples/sec; 0.059 sec/batch; 96h:49m:28s remains)
INFO - root - 2019-11-03 23:59:32.236108: step 42140, total loss = 0.46, predict loss = 0.11 (65.0 examples/sec; 0.062 sec/batch; 101h:48m:39s remains)
INFO - root - 2019-11-03 23:59:32.896602: step 42150, total loss = 0.64, predict loss = 0.15 (63.6 examples/sec; 0.063 sec/batch; 104h:01m:12s remains)
INFO - root - 2019-11-03 23:59:33.536291: step 42160, total loss = 0.54, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 94h:54m:21s remains)
INFO - root - 2019-11-03 23:59:34.194301: step 42170, total loss = 0.62, predict loss = 0.14 (57.0 examples/sec; 0.070 sec/batch; 116h:10m:15s remains)
INFO - root - 2019-11-03 23:59:34.815655: step 42180, total loss = 0.58, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 101h:26m:17s remains)
INFO - root - 2019-11-03 23:59:35.494254: step 42190, total loss = 0.38, predict loss = 0.08 (68.8 examples/sec; 0.058 sec/batch; 96h:14m:31s remains)
INFO - root - 2019-11-03 23:59:36.151615: step 42200, total loss = 0.59, predict loss = 0.14 (71.4 examples/sec; 0.056 sec/batch; 92h:41m:37s remains)
INFO - root - 2019-11-03 23:59:36.755650: step 42210, total loss = 0.51, predict loss = 0.11 (78.4 examples/sec; 0.051 sec/batch; 84h:28m:25s remains)
INFO - root - 2019-11-03 23:59:37.368499: step 42220, total loss = 0.57, predict loss = 0.13 (67.1 examples/sec; 0.060 sec/batch; 98h:39m:26s remains)
INFO - root - 2019-11-03 23:59:37.991501: step 42230, total loss = 0.47, predict loss = 0.11 (80.8 examples/sec; 0.050 sec/batch; 81h:58m:02s remains)
INFO - root - 2019-11-03 23:59:38.598660: step 42240, total loss = 0.42, predict loss = 0.08 (77.7 examples/sec; 0.051 sec/batch; 85h:08m:41s remains)
INFO - root - 2019-11-03 23:59:39.244137: step 42250, total loss = 0.48, predict loss = 0.11 (77.8 examples/sec; 0.051 sec/batch; 85h:03m:52s remains)
INFO - root - 2019-11-03 23:59:39.868393: step 42260, total loss = 0.70, predict loss = 0.17 (74.7 examples/sec; 0.054 sec/batch; 88h:36m:07s remains)
INFO - root - 2019-11-03 23:59:40.495773: step 42270, total loss = 0.41, predict loss = 0.09 (75.5 examples/sec; 0.053 sec/batch; 87h:39m:34s remains)
INFO - root - 2019-11-03 23:59:41.124017: step 42280, total loss = 0.41, predict loss = 0.09 (68.0 examples/sec; 0.059 sec/batch; 97h:22m:45s remains)
INFO - root - 2019-11-03 23:59:41.743256: step 42290, total loss = 0.41, predict loss = 0.09 (72.9 examples/sec; 0.055 sec/batch; 90h:45m:32s remains)
INFO - root - 2019-11-03 23:59:42.364794: step 42300, total loss = 0.62, predict loss = 0.14 (70.9 examples/sec; 0.056 sec/batch; 93h:22m:33s remains)
INFO - root - 2019-11-03 23:59:42.987513: step 42310, total loss = 0.42, predict loss = 0.09 (77.6 examples/sec; 0.052 sec/batch; 85h:20m:16s remains)
INFO - root - 2019-11-03 23:59:43.625840: step 42320, total loss = 0.55, predict loss = 0.13 (80.9 examples/sec; 0.049 sec/batch; 81h:47m:44s remains)
INFO - root - 2019-11-03 23:59:44.301450: step 42330, total loss = 0.91, predict loss = 0.22 (64.3 examples/sec; 0.062 sec/batch; 102h:55m:36s remains)
INFO - root - 2019-11-03 23:59:44.972513: step 42340, total loss = 0.95, predict loss = 0.22 (67.8 examples/sec; 0.059 sec/batch; 97h:40m:31s remains)
INFO - root - 2019-11-03 23:59:45.613667: step 42350, total loss = 0.95, predict loss = 0.22 (71.6 examples/sec; 0.056 sec/batch; 92h:25m:34s remains)
INFO - root - 2019-11-03 23:59:46.251146: step 42360, total loss = 0.86, predict loss = 0.20 (72.9 examples/sec; 0.055 sec/batch; 90h:45m:20s remains)
INFO - root - 2019-11-03 23:59:46.884898: step 42370, total loss = 0.90, predict loss = 0.20 (63.7 examples/sec; 0.063 sec/batch; 103h:53m:22s remains)
INFO - root - 2019-11-03 23:59:47.507907: step 42380, total loss = 0.79, predict loss = 0.18 (66.5 examples/sec; 0.060 sec/batch; 99h:30m:33s remains)
INFO - root - 2019-11-03 23:59:48.194058: step 42390, total loss = 0.95, predict loss = 0.21 (57.5 examples/sec; 0.070 sec/batch; 115h:01m:40s remains)
INFO - root - 2019-11-03 23:59:48.817025: step 42400, total loss = 0.81, predict loss = 0.19 (86.1 examples/sec; 0.046 sec/batch; 76h:54m:33s remains)
INFO - root - 2019-11-03 23:59:49.470065: step 42410, total loss = 0.78, predict loss = 0.18 (67.3 examples/sec; 0.059 sec/batch; 98h:21m:08s remains)
INFO - root - 2019-11-03 23:59:50.125069: step 42420, total loss = 0.66, predict loss = 0.15 (73.6 examples/sec; 0.054 sec/batch; 89h:54m:21s remains)
INFO - root - 2019-11-03 23:59:50.745187: step 42430, total loss = 0.71, predict loss = 0.16 (73.9 examples/sec; 0.054 sec/batch; 89h:32m:21s remains)
INFO - root - 2019-11-03 23:59:51.391697: step 42440, total loss = 0.56, predict loss = 0.12 (65.6 examples/sec; 0.061 sec/batch; 100h:58m:44s remains)
INFO - root - 2019-11-03 23:59:52.056618: step 42450, total loss = 0.52, predict loss = 0.12 (70.0 examples/sec; 0.057 sec/batch; 94h:31m:13s remains)
INFO - root - 2019-11-03 23:59:52.703155: step 42460, total loss = 0.57, predict loss = 0.13 (68.3 examples/sec; 0.059 sec/batch; 96h:57m:57s remains)
INFO - root - 2019-11-03 23:59:53.370824: step 42470, total loss = 0.59, predict loss = 0.13 (74.4 examples/sec; 0.054 sec/batch; 88h:55m:20s remains)
INFO - root - 2019-11-03 23:59:54.021858: step 42480, total loss = 0.46, predict loss = 0.11 (66.6 examples/sec; 0.060 sec/batch; 99h:24m:14s remains)
INFO - root - 2019-11-03 23:59:54.680480: step 42490, total loss = 0.58, predict loss = 0.13 (77.3 examples/sec; 0.052 sec/batch; 85h:35m:44s remains)
INFO - root - 2019-11-03 23:59:55.313433: step 42500, total loss = 0.50, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 104h:11m:22s remains)
INFO - root - 2019-11-03 23:59:55.943584: step 42510, total loss = 0.57, predict loss = 0.13 (76.3 examples/sec; 0.052 sec/batch; 86h:47m:30s remains)
INFO - root - 2019-11-03 23:59:56.551354: step 42520, total loss = 0.57, predict loss = 0.13 (68.4 examples/sec; 0.059 sec/batch; 96h:50m:10s remains)
INFO - root - 2019-11-03 23:59:57.206876: step 42530, total loss = 0.62, predict loss = 0.14 (63.6 examples/sec; 0.063 sec/batch; 104h:05m:33s remains)
INFO - root - 2019-11-03 23:59:57.891791: step 42540, total loss = 0.69, predict loss = 0.15 (67.2 examples/sec; 0.060 sec/batch; 98h:31m:26s remains)
INFO - root - 2019-11-03 23:59:58.547711: step 42550, total loss = 0.68, predict loss = 0.16 (76.0 examples/sec; 0.053 sec/batch; 87h:03m:50s remains)
INFO - root - 2019-11-03 23:59:59.176930: step 42560, total loss = 0.65, predict loss = 0.14 (75.3 examples/sec; 0.053 sec/batch; 87h:51m:19s remains)
INFO - root - 2019-11-03 23:59:59.838001: step 42570, total loss = 0.49, predict loss = 0.10 (62.8 examples/sec; 0.064 sec/batch; 105h:23m:02s remains)
INFO - root - 2019-11-04 00:00:00.525424: step 42580, total loss = 0.43, predict loss = 0.10 (63.0 examples/sec; 0.063 sec/batch; 104h:59m:57s remains)
INFO - root - 2019-11-04 00:00:01.210008: step 42590, total loss = 0.47, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 92h:29m:52s remains)
INFO - root - 2019-11-04 00:00:01.860962: step 42600, total loss = 0.58, predict loss = 0.14 (61.3 examples/sec; 0.065 sec/batch; 107h:58m:07s remains)
INFO - root - 2019-11-04 00:00:02.494653: step 42610, total loss = 0.38, predict loss = 0.09 (74.9 examples/sec; 0.053 sec/batch; 88h:21m:39s remains)
INFO - root - 2019-11-04 00:00:03.095538: step 42620, total loss = 0.53, predict loss = 0.12 (84.7 examples/sec; 0.047 sec/batch; 78h:11m:09s remains)
INFO - root - 2019-11-04 00:00:03.708328: step 42630, total loss = 0.35, predict loss = 0.07 (65.0 examples/sec; 0.062 sec/batch; 101h:50m:10s remains)
INFO - root - 2019-11-04 00:00:04.338892: step 42640, total loss = 0.42, predict loss = 0.09 (65.6 examples/sec; 0.061 sec/batch; 100h:56m:18s remains)
INFO - root - 2019-11-04 00:00:04.980251: step 42650, total loss = 0.59, predict loss = 0.14 (76.6 examples/sec; 0.052 sec/batch; 86h:25m:30s remains)
INFO - root - 2019-11-04 00:00:05.624026: step 42660, total loss = 0.67, predict loss = 0.14 (79.2 examples/sec; 0.050 sec/batch; 83h:33m:49s remains)
INFO - root - 2019-11-04 00:00:06.238259: step 42670, total loss = 0.53, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 87h:15m:23s remains)
INFO - root - 2019-11-04 00:00:06.880046: step 42680, total loss = 0.48, predict loss = 0.10 (64.6 examples/sec; 0.062 sec/batch; 102h:30m:45s remains)
INFO - root - 2019-11-04 00:00:07.504861: step 42690, total loss = 0.47, predict loss = 0.11 (73.8 examples/sec; 0.054 sec/batch; 89h:44m:07s remains)
INFO - root - 2019-11-04 00:00:08.111068: step 42700, total loss = 0.65, predict loss = 0.16 (70.7 examples/sec; 0.057 sec/batch; 93h:39m:33s remains)
INFO - root - 2019-11-04 00:00:08.744969: step 42710, total loss = 0.57, predict loss = 0.14 (67.8 examples/sec; 0.059 sec/batch; 97h:38m:44s remains)
INFO - root - 2019-11-04 00:00:09.436022: step 42720, total loss = 0.44, predict loss = 0.10 (65.5 examples/sec; 0.061 sec/batch; 100h:59m:12s remains)
INFO - root - 2019-11-04 00:00:10.079884: step 42730, total loss = 0.49, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 98h:09m:37s remains)
INFO - root - 2019-11-04 00:00:10.667851: step 42740, total loss = 0.67, predict loss = 0.15 (77.3 examples/sec; 0.052 sec/batch; 85h:40m:19s remains)
INFO - root - 2019-11-04 00:00:11.257064: step 42750, total loss = 0.60, predict loss = 0.15 (79.9 examples/sec; 0.050 sec/batch; 82h:51m:08s remains)
INFO - root - 2019-11-04 00:00:11.856565: step 42760, total loss = 0.60, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 97h:11m:12s remains)
INFO - root - 2019-11-04 00:00:12.476352: step 42770, total loss = 0.59, predict loss = 0.13 (66.8 examples/sec; 0.060 sec/batch; 99h:03m:46s remains)
INFO - root - 2019-11-04 00:00:13.113725: step 42780, total loss = 0.78, predict loss = 0.19 (68.4 examples/sec; 0.058 sec/batch; 96h:43m:52s remains)
INFO - root - 2019-11-04 00:00:13.761017: step 42790, total loss = 0.65, predict loss = 0.16 (67.0 examples/sec; 0.060 sec/batch; 98h:45m:06s remains)
INFO - root - 2019-11-04 00:00:14.449944: step 42800, total loss = 0.69, predict loss = 0.17 (65.7 examples/sec; 0.061 sec/batch; 100h:42m:44s remains)
INFO - root - 2019-11-04 00:00:15.082963: step 42810, total loss = 0.63, predict loss = 0.15 (73.5 examples/sec; 0.054 sec/batch; 90h:02m:33s remains)
INFO - root - 2019-11-04 00:00:15.723922: step 42820, total loss = 0.67, predict loss = 0.16 (66.5 examples/sec; 0.060 sec/batch; 99h:33m:37s remains)
INFO - root - 2019-11-04 00:00:16.322221: step 42830, total loss = 0.69, predict loss = 0.16 (73.2 examples/sec; 0.055 sec/batch; 90h:23m:44s remains)
INFO - root - 2019-11-04 00:00:16.976626: step 42840, total loss = 0.81, predict loss = 0.20 (72.6 examples/sec; 0.055 sec/batch; 91h:12m:03s remains)
INFO - root - 2019-11-04 00:00:17.682925: step 42850, total loss = 0.67, predict loss = 0.16 (59.9 examples/sec; 0.067 sec/batch; 110h:25m:07s remains)
INFO - root - 2019-11-04 00:00:18.386108: step 42860, total loss = 0.51, predict loss = 0.12 (64.7 examples/sec; 0.062 sec/batch; 102h:14m:20s remains)
INFO - root - 2019-11-04 00:00:19.012351: step 42870, total loss = 0.54, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 98h:40m:57s remains)
INFO - root - 2019-11-04 00:00:19.651893: step 42880, total loss = 0.56, predict loss = 0.13 (63.8 examples/sec; 0.063 sec/batch; 103h:49m:10s remains)
INFO - root - 2019-11-04 00:00:20.322894: step 42890, total loss = 0.60, predict loss = 0.14 (68.9 examples/sec; 0.058 sec/batch; 96h:07m:31s remains)
INFO - root - 2019-11-04 00:00:21.003776: step 42900, total loss = 0.53, predict loss = 0.11 (65.6 examples/sec; 0.061 sec/batch; 100h:57m:09s remains)
INFO - root - 2019-11-04 00:00:21.727682: step 42910, total loss = 0.49, predict loss = 0.11 (55.1 examples/sec; 0.073 sec/batch; 120h:08m:28s remains)
INFO - root - 2019-11-04 00:00:22.354957: step 42920, total loss = 0.54, predict loss = 0.12 (84.8 examples/sec; 0.047 sec/batch; 78h:05m:42s remains)
INFO - root - 2019-11-04 00:00:23.009489: step 42930, total loss = 0.59, predict loss = 0.13 (67.1 examples/sec; 0.060 sec/batch; 98h:42m:47s remains)
INFO - root - 2019-11-04 00:00:23.666528: step 42940, total loss = 0.53, predict loss = 0.13 (66.0 examples/sec; 0.061 sec/batch; 100h:17m:27s remains)
INFO - root - 2019-11-04 00:00:24.280111: step 42950, total loss = 0.70, predict loss = 0.16 (70.2 examples/sec; 0.057 sec/batch; 94h:20m:53s remains)
INFO - root - 2019-11-04 00:00:24.908485: step 42960, total loss = 0.62, predict loss = 0.15 (72.8 examples/sec; 0.055 sec/batch; 90h:56m:44s remains)
INFO - root - 2019-11-04 00:00:25.578349: step 42970, total loss = 0.79, predict loss = 0.19 (69.9 examples/sec; 0.057 sec/batch; 94h:42m:30s remains)
INFO - root - 2019-11-04 00:00:26.255272: step 42980, total loss = 0.60, predict loss = 0.14 (67.1 examples/sec; 0.060 sec/batch; 98h:40m:29s remains)
INFO - root - 2019-11-04 00:00:26.953432: step 42990, total loss = 0.54, predict loss = 0.13 (58.3 examples/sec; 0.069 sec/batch; 113h:35m:34s remains)
INFO - root - 2019-11-04 00:00:27.684118: step 43000, total loss = 0.71, predict loss = 0.17 (62.3 examples/sec; 0.064 sec/batch; 106h:19m:09s remains)
INFO - root - 2019-11-04 00:00:28.324845: step 43010, total loss = 0.68, predict loss = 0.17 (78.8 examples/sec; 0.051 sec/batch; 83h:57m:00s remains)
INFO - root - 2019-11-04 00:00:28.955256: step 43020, total loss = 0.72, predict loss = 0.17 (63.9 examples/sec; 0.063 sec/batch; 103h:36m:05s remains)
INFO - root - 2019-11-04 00:00:29.560386: step 43030, total loss = 0.59, predict loss = 0.14 (68.7 examples/sec; 0.058 sec/batch; 96h:21m:15s remains)
INFO - root - 2019-11-04 00:00:30.232696: step 43040, total loss = 0.58, predict loss = 0.13 (60.9 examples/sec; 0.066 sec/batch; 108h:43m:33s remains)
INFO - root - 2019-11-04 00:00:30.914313: step 43050, total loss = 0.65, predict loss = 0.16 (71.2 examples/sec; 0.056 sec/batch; 92h:56m:44s remains)
INFO - root - 2019-11-04 00:00:31.629560: step 43060, total loss = 0.66, predict loss = 0.16 (61.5 examples/sec; 0.065 sec/batch; 107h:32m:14s remains)
INFO - root - 2019-11-04 00:00:32.240261: step 43070, total loss = 0.67, predict loss = 0.15 (66.1 examples/sec; 0.061 sec/batch; 100h:09m:36s remains)
INFO - root - 2019-11-04 00:00:32.880517: step 43080, total loss = 0.63, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 91h:38m:33s remains)
INFO - root - 2019-11-04 00:00:33.538125: step 43090, total loss = 0.60, predict loss = 0.14 (70.7 examples/sec; 0.057 sec/batch; 93h:34m:41s remains)
INFO - root - 2019-11-04 00:00:34.173798: step 43100, total loss = 0.56, predict loss = 0.13 (64.9 examples/sec; 0.062 sec/batch; 102h:01m:48s remains)
INFO - root - 2019-11-04 00:00:34.843903: step 43110, total loss = 0.60, predict loss = 0.13 (61.0 examples/sec; 0.066 sec/batch; 108h:25m:58s remains)
INFO - root - 2019-11-04 00:00:35.523170: step 43120, total loss = 0.52, predict loss = 0.11 (72.7 examples/sec; 0.055 sec/batch; 91h:00m:33s remains)
INFO - root - 2019-11-04 00:00:36.170406: step 43130, total loss = 0.63, predict loss = 0.15 (64.6 examples/sec; 0.062 sec/batch; 102h:24m:24s remains)
INFO - root - 2019-11-04 00:00:36.841518: step 43140, total loss = 0.56, predict loss = 0.14 (60.0 examples/sec; 0.067 sec/batch; 110h:18m:30s remains)
INFO - root - 2019-11-04 00:00:37.506708: step 43150, total loss = 0.61, predict loss = 0.15 (70.0 examples/sec; 0.057 sec/batch; 94h:35m:05s remains)
INFO - root - 2019-11-04 00:00:38.140385: step 43160, total loss = 0.60, predict loss = 0.14 (65.6 examples/sec; 0.061 sec/batch; 100h:54m:15s remains)
INFO - root - 2019-11-04 00:00:38.757940: step 43170, total loss = 0.53, predict loss = 0.12 (82.3 examples/sec; 0.049 sec/batch; 80h:22m:43s remains)
INFO - root - 2019-11-04 00:00:39.367609: step 43180, total loss = 0.50, predict loss = 0.12 (86.0 examples/sec; 0.047 sec/batch; 76h:58m:35s remains)
INFO - root - 2019-11-04 00:00:39.959901: step 43190, total loss = 0.47, predict loss = 0.10 (71.3 examples/sec; 0.056 sec/batch; 92h:46m:30s remains)
INFO - root - 2019-11-04 00:00:40.557219: step 43200, total loss = 0.55, predict loss = 0.13 (68.2 examples/sec; 0.059 sec/batch; 97h:04m:59s remains)
INFO - root - 2019-11-04 00:00:41.188215: step 43210, total loss = 0.68, predict loss = 0.15 (63.8 examples/sec; 0.063 sec/batch; 103h:41m:22s remains)
INFO - root - 2019-11-04 00:00:41.826363: step 43220, total loss = 0.56, predict loss = 0.13 (72.3 examples/sec; 0.055 sec/batch; 91h:30m:36s remains)
INFO - root - 2019-11-04 00:00:42.470716: step 43230, total loss = 0.51, predict loss = 0.12 (68.5 examples/sec; 0.058 sec/batch; 96h:38m:55s remains)
INFO - root - 2019-11-04 00:00:43.130602: step 43240, total loss = 0.52, predict loss = 0.12 (65.4 examples/sec; 0.061 sec/batch; 101h:15m:13s remains)
INFO - root - 2019-11-04 00:00:43.793164: step 43250, total loss = 0.59, predict loss = 0.13 (66.3 examples/sec; 0.060 sec/batch; 99h:53m:13s remains)
INFO - root - 2019-11-04 00:00:44.405950: step 43260, total loss = 0.58, predict loss = 0.14 (70.8 examples/sec; 0.057 sec/batch; 93h:30m:01s remains)
INFO - root - 2019-11-04 00:00:45.003504: step 43270, total loss = 0.63, predict loss = 0.14 (70.2 examples/sec; 0.057 sec/batch; 94h:18m:00s remains)
INFO - root - 2019-11-04 00:00:45.626765: step 43280, total loss = 0.72, predict loss = 0.18 (75.5 examples/sec; 0.053 sec/batch; 87h:39m:59s remains)
INFO - root - 2019-11-04 00:00:46.267281: step 43290, total loss = 0.58, predict loss = 0.13 (74.7 examples/sec; 0.054 sec/batch; 88h:33m:52s remains)
INFO - root - 2019-11-04 00:00:46.889323: step 43300, total loss = 0.68, predict loss = 0.16 (65.1 examples/sec; 0.061 sec/batch; 101h:35m:52s remains)
INFO - root - 2019-11-04 00:00:47.522634: step 43310, total loss = 0.68, predict loss = 0.15 (76.3 examples/sec; 0.052 sec/batch; 86h:44m:46s remains)
INFO - root - 2019-11-04 00:00:48.161424: step 43320, total loss = 0.74, predict loss = 0.18 (63.7 examples/sec; 0.063 sec/batch; 103h:54m:29s remains)
INFO - root - 2019-11-04 00:00:48.767224: step 43330, total loss = 0.66, predict loss = 0.16 (72.4 examples/sec; 0.055 sec/batch; 91h:28m:31s remains)
INFO - root - 2019-11-04 00:00:49.367877: step 43340, total loss = 0.73, predict loss = 0.18 (69.1 examples/sec; 0.058 sec/batch; 95h:47m:06s remains)
INFO - root - 2019-11-04 00:00:49.979144: step 43350, total loss = 0.70, predict loss = 0.16 (72.7 examples/sec; 0.055 sec/batch; 90h:58m:50s remains)
INFO - root - 2019-11-04 00:00:50.616589: step 43360, total loss = 0.61, predict loss = 0.15 (60.9 examples/sec; 0.066 sec/batch; 108h:43m:29s remains)
INFO - root - 2019-11-04 00:00:51.299341: step 43370, total loss = 0.70, predict loss = 0.17 (59.2 examples/sec; 0.068 sec/batch; 111h:48m:30s remains)
INFO - root - 2019-11-04 00:00:51.996310: step 43380, total loss = 0.82, predict loss = 0.20 (71.7 examples/sec; 0.056 sec/batch; 92h:21m:27s remains)
INFO - root - 2019-11-04 00:00:52.614527: step 43390, total loss = 0.66, predict loss = 0.15 (75.9 examples/sec; 0.053 sec/batch; 87h:09m:00s remains)
INFO - root - 2019-11-04 00:00:53.233197: step 43400, total loss = 0.67, predict loss = 0.16 (65.7 examples/sec; 0.061 sec/batch; 100h:48m:17s remains)
INFO - root - 2019-11-04 00:00:53.867647: step 43410, total loss = 0.91, predict loss = 0.22 (73.7 examples/sec; 0.054 sec/batch; 89h:48m:48s remains)
INFO - root - 2019-11-04 00:00:54.528461: step 43420, total loss = 0.88, predict loss = 0.21 (63.9 examples/sec; 0.063 sec/batch; 103h:33m:38s remains)
INFO - root - 2019-11-04 00:00:55.190618: step 43430, total loss = 0.56, predict loss = 0.13 (63.0 examples/sec; 0.063 sec/batch; 104h:58m:39s remains)
INFO - root - 2019-11-04 00:00:55.892329: step 43440, total loss = 0.88, predict loss = 0.22 (55.8 examples/sec; 0.072 sec/batch; 118h:37m:19s remains)
INFO - root - 2019-11-04 00:00:56.562478: step 43450, total loss = 0.67, predict loss = 0.15 (64.3 examples/sec; 0.062 sec/batch; 102h:59m:29s remains)
INFO - root - 2019-11-04 00:00:57.188395: step 43460, total loss = 0.60, predict loss = 0.14 (72.1 examples/sec; 0.055 sec/batch; 91h:47m:36s remains)
INFO - root - 2019-11-04 00:00:57.809360: step 43470, total loss = 0.70, predict loss = 0.16 (67.0 examples/sec; 0.060 sec/batch; 98h:48m:48s remains)
INFO - root - 2019-11-04 00:00:58.482997: step 43480, total loss = 0.85, predict loss = 0.21 (57.4 examples/sec; 0.070 sec/batch; 115h:16m:23s remains)
INFO - root - 2019-11-04 00:00:59.156178: step 43490, total loss = 0.59, predict loss = 0.14 (63.6 examples/sec; 0.063 sec/batch; 104h:07m:49s remains)
INFO - root - 2019-11-04 00:00:59.850419: step 43500, total loss = 0.52, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 98h:56m:05s remains)
INFO - root - 2019-11-04 00:01:00.565712: step 43510, total loss = 0.65, predict loss = 0.16 (61.5 examples/sec; 0.065 sec/batch; 107h:40m:25s remains)
INFO - root - 2019-11-04 00:01:01.239718: step 43520, total loss = 0.60, predict loss = 0.14 (76.7 examples/sec; 0.052 sec/batch; 86h:16m:32s remains)
INFO - root - 2019-11-04 00:01:01.922186: step 43530, total loss = 0.47, predict loss = 0.11 (63.0 examples/sec; 0.064 sec/batch; 105h:07m:28s remains)
INFO - root - 2019-11-04 00:01:02.600985: step 43540, total loss = 0.69, predict loss = 0.16 (55.9 examples/sec; 0.072 sec/batch; 118h:24m:46s remains)
INFO - root - 2019-11-04 00:01:03.273295: step 43550, total loss = 0.54, predict loss = 0.12 (64.9 examples/sec; 0.062 sec/batch; 102h:00m:18s remains)
INFO - root - 2019-11-04 00:01:03.962412: step 43560, total loss = 0.67, predict loss = 0.16 (73.5 examples/sec; 0.054 sec/batch; 90h:00m:45s remains)
INFO - root - 2019-11-04 00:01:04.587721: step 43570, total loss = 0.64, predict loss = 0.15 (64.4 examples/sec; 0.062 sec/batch; 102h:50m:48s remains)
INFO - root - 2019-11-04 00:01:05.212364: step 43580, total loss = 0.68, predict loss = 0.16 (77.8 examples/sec; 0.051 sec/batch; 85h:05m:30s remains)
INFO - root - 2019-11-04 00:01:05.834729: step 43590, total loss = 0.58, predict loss = 0.13 (78.1 examples/sec; 0.051 sec/batch; 84h:44m:55s remains)
INFO - root - 2019-11-04 00:01:06.500765: step 43600, total loss = 0.59, predict loss = 0.14 (64.9 examples/sec; 0.062 sec/batch; 101h:56m:40s remains)
INFO - root - 2019-11-04 00:01:07.149792: step 43610, total loss = 0.54, predict loss = 0.12 (64.8 examples/sec; 0.062 sec/batch; 102h:08m:30s remains)
INFO - root - 2019-11-04 00:01:07.767265: step 43620, total loss = 0.47, predict loss = 0.10 (90.2 examples/sec; 0.044 sec/batch; 73h:20m:01s remains)
INFO - root - 2019-11-04 00:01:08.227219: step 43630, total loss = 0.56, predict loss = 0.14 (94.3 examples/sec; 0.042 sec/batch; 70h:11m:20s remains)
INFO - root - 2019-11-04 00:01:09.830167: step 43640, total loss = 0.23, predict loss = 0.04 (68.8 examples/sec; 0.058 sec/batch; 96h:09m:12s remains)
INFO - root - 2019-11-04 00:01:10.420014: step 43650, total loss = 0.78, predict loss = 0.18 (74.2 examples/sec; 0.054 sec/batch; 89h:09m:44s remains)
INFO - root - 2019-11-04 00:01:11.034569: step 43660, total loss = 0.46, predict loss = 0.11 (62.6 examples/sec; 0.064 sec/batch; 105h:44m:45s remains)
INFO - root - 2019-11-04 00:01:11.690463: step 43670, total loss = 0.49, predict loss = 0.11 (58.1 examples/sec; 0.069 sec/batch; 113h:50m:26s remains)
INFO - root - 2019-11-04 00:01:12.320744: step 43680, total loss = 0.45, predict loss = 0.09 (67.5 examples/sec; 0.059 sec/batch; 97h:59m:53s remains)
INFO - root - 2019-11-04 00:01:12.923206: step 43690, total loss = 0.65, predict loss = 0.15 (82.5 examples/sec; 0.049 sec/batch; 80h:15m:50s remains)
INFO - root - 2019-11-04 00:01:13.555952: step 43700, total loss = 0.51, predict loss = 0.12 (79.1 examples/sec; 0.051 sec/batch; 83h:42m:13s remains)
INFO - root - 2019-11-04 00:01:14.249784: step 43710, total loss = 0.69, predict loss = 0.16 (62.0 examples/sec; 0.065 sec/batch; 106h:45m:24s remains)
INFO - root - 2019-11-04 00:01:14.893329: step 43720, total loss = 0.87, predict loss = 0.21 (71.1 examples/sec; 0.056 sec/batch; 93h:02m:33s remains)
INFO - root - 2019-11-04 00:01:15.539114: step 43730, total loss = 0.66, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:32m:40s remains)
INFO - root - 2019-11-04 00:01:16.178765: step 43740, total loss = 0.73, predict loss = 0.17 (67.4 examples/sec; 0.059 sec/batch; 98h:08m:35s remains)
INFO - root - 2019-11-04 00:01:16.814450: step 43750, total loss = 0.62, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:39m:47s remains)
INFO - root - 2019-11-04 00:01:17.464207: step 43760, total loss = 0.60, predict loss = 0.13 (63.9 examples/sec; 0.063 sec/batch; 103h:33m:35s remains)
INFO - root - 2019-11-04 00:01:18.099503: step 43770, total loss = 0.63, predict loss = 0.14 (64.9 examples/sec; 0.062 sec/batch; 101h:58m:06s remains)
INFO - root - 2019-11-04 00:01:18.747793: step 43780, total loss = 0.63, predict loss = 0.15 (67.5 examples/sec; 0.059 sec/batch; 97h:59m:21s remains)
INFO - root - 2019-11-04 00:01:19.395899: step 43790, total loss = 0.51, predict loss = 0.11 (69.0 examples/sec; 0.058 sec/batch; 95h:58m:53s remains)
INFO - root - 2019-11-04 00:01:20.035887: step 43800, total loss = 0.49, predict loss = 0.11 (73.7 examples/sec; 0.054 sec/batch; 89h:51m:23s remains)
INFO - root - 2019-11-04 00:01:20.676897: step 43810, total loss = 0.68, predict loss = 0.16 (70.4 examples/sec; 0.057 sec/batch; 94h:01m:53s remains)
INFO - root - 2019-11-04 00:01:21.313097: step 43820, total loss = 0.63, predict loss = 0.14 (66.8 examples/sec; 0.060 sec/batch; 99h:06m:11s remains)
INFO - root - 2019-11-04 00:01:21.941134: step 43830, total loss = 0.58, predict loss = 0.14 (73.3 examples/sec; 0.055 sec/batch; 90h:17m:16s remains)
INFO - root - 2019-11-04 00:01:22.586406: step 43840, total loss = 0.80, predict loss = 0.19 (65.9 examples/sec; 0.061 sec/batch; 100h:27m:57s remains)
INFO - root - 2019-11-04 00:01:23.214413: step 43850, total loss = 0.48, predict loss = 0.10 (74.6 examples/sec; 0.054 sec/batch; 88h:44m:54s remains)
INFO - root - 2019-11-04 00:01:23.870859: step 43860, total loss = 0.37, predict loss = 0.09 (73.4 examples/sec; 0.054 sec/batch; 90h:07m:19s remains)
INFO - root - 2019-11-04 00:01:24.517205: step 43870, total loss = 0.60, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 95h:29m:36s remains)
INFO - root - 2019-11-04 00:01:25.160681: step 43880, total loss = 0.57, predict loss = 0.13 (62.1 examples/sec; 0.064 sec/batch; 106h:32m:39s remains)
INFO - root - 2019-11-04 00:01:25.841615: step 43890, total loss = 0.81, predict loss = 0.20 (58.0 examples/sec; 0.069 sec/batch; 114h:04m:17s remains)
INFO - root - 2019-11-04 00:01:26.462020: step 43900, total loss = 0.73, predict loss = 0.17 (65.8 examples/sec; 0.061 sec/batch; 100h:37m:44s remains)
INFO - root - 2019-11-04 00:01:27.106267: step 43910, total loss = 0.79, predict loss = 0.18 (71.5 examples/sec; 0.056 sec/batch; 92h:32m:58s remains)
INFO - root - 2019-11-04 00:01:27.772503: step 43920, total loss = 0.71, predict loss = 0.15 (65.4 examples/sec; 0.061 sec/batch; 101h:10m:57s remains)
INFO - root - 2019-11-04 00:01:28.390084: step 43930, total loss = 0.77, predict loss = 0.17 (78.9 examples/sec; 0.051 sec/batch; 83h:49m:57s remains)
INFO - root - 2019-11-04 00:01:29.042001: step 43940, total loss = 0.84, predict loss = 0.19 (71.1 examples/sec; 0.056 sec/batch; 93h:05m:41s remains)
INFO - root - 2019-11-04 00:01:29.682681: step 43950, total loss = 0.67, predict loss = 0.15 (70.1 examples/sec; 0.057 sec/batch; 94h:20m:25s remains)
INFO - root - 2019-11-04 00:01:30.328215: step 43960, total loss = 0.77, predict loss = 0.17 (65.7 examples/sec; 0.061 sec/batch; 100h:40m:48s remains)
INFO - root - 2019-11-04 00:01:30.981808: step 43970, total loss = 0.63, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 95h:02m:08s remains)
INFO - root - 2019-11-04 00:01:31.611108: step 43980, total loss = 0.74, predict loss = 0.17 (72.5 examples/sec; 0.055 sec/batch; 91h:19m:10s remains)
INFO - root - 2019-11-04 00:01:32.275382: step 43990, total loss = 0.59, predict loss = 0.13 (56.8 examples/sec; 0.070 sec/batch; 116h:30m:12s remains)
INFO - root - 2019-11-04 00:01:32.914402: step 44000, total loss = 0.69, predict loss = 0.16 (69.1 examples/sec; 0.058 sec/batch; 95h:46m:28s remains)
INFO - root - 2019-11-04 00:01:33.533340: step 44010, total loss = 0.63, predict loss = 0.14 (73.0 examples/sec; 0.055 sec/batch; 90h:40m:48s remains)
INFO - root - 2019-11-04 00:01:34.141245: step 44020, total loss = 0.60, predict loss = 0.13 (67.8 examples/sec; 0.059 sec/batch; 97h:33m:41s remains)
INFO - root - 2019-11-04 00:01:34.836012: step 44030, total loss = 0.60, predict loss = 0.13 (67.0 examples/sec; 0.060 sec/batch; 98h:45m:09s remains)
INFO - root - 2019-11-04 00:01:35.516090: step 44040, total loss = 0.56, predict loss = 0.13 (62.5 examples/sec; 0.064 sec/batch; 105h:48m:05s remains)
INFO - root - 2019-11-04 00:01:36.147839: step 44050, total loss = 0.60, predict loss = 0.14 (69.1 examples/sec; 0.058 sec/batch; 95h:48m:23s remains)
INFO - root - 2019-11-04 00:01:36.776980: step 44060, total loss = 0.59, predict loss = 0.14 (68.1 examples/sec; 0.059 sec/batch; 97h:12m:30s remains)
INFO - root - 2019-11-04 00:01:37.453794: step 44070, total loss = 0.53, predict loss = 0.11 (64.1 examples/sec; 0.062 sec/batch; 103h:14m:03s remains)
INFO - root - 2019-11-04 00:01:38.114466: step 44080, total loss = 0.65, predict loss = 0.14 (67.7 examples/sec; 0.059 sec/batch; 97h:48m:52s remains)
INFO - root - 2019-11-04 00:01:38.795565: step 44090, total loss = 0.56, predict loss = 0.12 (65.3 examples/sec; 0.061 sec/batch; 101h:24m:20s remains)
INFO - root - 2019-11-04 00:01:39.425801: step 44100, total loss = 0.66, predict loss = 0.16 (67.3 examples/sec; 0.059 sec/batch; 98h:23m:14s remains)
INFO - root - 2019-11-04 00:01:40.094254: step 44110, total loss = 0.46, predict loss = 0.11 (63.8 examples/sec; 0.063 sec/batch; 103h:46m:47s remains)
INFO - root - 2019-11-04 00:01:40.772915: step 44120, total loss = 0.74, predict loss = 0.19 (64.6 examples/sec; 0.062 sec/batch; 102h:26m:29s remains)
INFO - root - 2019-11-04 00:01:41.436326: step 44130, total loss = 0.44, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 99h:31m:00s remains)
INFO - root - 2019-11-04 00:01:42.075931: step 44140, total loss = 0.58, predict loss = 0.13 (79.0 examples/sec; 0.051 sec/batch; 83h:45m:00s remains)
INFO - root - 2019-11-04 00:01:42.737490: step 44150, total loss = 0.79, predict loss = 0.20 (66.2 examples/sec; 0.060 sec/batch; 100h:01m:14s remains)
INFO - root - 2019-11-04 00:01:43.378333: step 44160, total loss = 0.65, predict loss = 0.16 (64.0 examples/sec; 0.063 sec/batch; 103h:27m:32s remains)
INFO - root - 2019-11-04 00:01:44.036015: step 44170, total loss = 0.60, predict loss = 0.15 (53.1 examples/sec; 0.075 sec/batch; 124h:31m:44s remains)
INFO - root - 2019-11-04 00:01:44.687946: step 44180, total loss = 0.66, predict loss = 0.16 (73.8 examples/sec; 0.054 sec/batch; 89h:40m:40s remains)
INFO - root - 2019-11-04 00:01:45.332900: step 44190, total loss = 0.54, predict loss = 0.13 (71.6 examples/sec; 0.056 sec/batch; 92h:21m:45s remains)
INFO - root - 2019-11-04 00:01:45.968869: step 44200, total loss = 0.26, predict loss = 0.05 (67.9 examples/sec; 0.059 sec/batch; 97h:28m:22s remains)
INFO - root - 2019-11-04 00:01:46.621099: step 44210, total loss = 0.63, predict loss = 0.15 (67.5 examples/sec; 0.059 sec/batch; 97h:58m:36s remains)
INFO - root - 2019-11-04 00:01:47.271385: step 44220, total loss = 0.51, predict loss = 0.12 (77.1 examples/sec; 0.052 sec/batch; 85h:51m:20s remains)
INFO - root - 2019-11-04 00:01:47.900525: step 44230, total loss = 0.59, predict loss = 0.14 (87.0 examples/sec; 0.046 sec/batch; 76h:05m:00s remains)
INFO - root - 2019-11-04 00:01:48.505195: step 44240, total loss = 0.45, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 99h:32m:29s remains)
INFO - root - 2019-11-04 00:01:49.169288: step 44250, total loss = 0.56, predict loss = 0.13 (68.3 examples/sec; 0.059 sec/batch; 96h:52m:09s remains)
INFO - root - 2019-11-04 00:01:49.824072: step 44260, total loss = 0.61, predict loss = 0.13 (65.9 examples/sec; 0.061 sec/batch; 100h:20m:40s remains)
INFO - root - 2019-11-04 00:01:50.472960: step 44270, total loss = 0.54, predict loss = 0.12 (62.0 examples/sec; 0.064 sec/batch; 106h:39m:19s remains)
INFO - root - 2019-11-04 00:01:51.150344: step 44280, total loss = 0.43, predict loss = 0.09 (68.5 examples/sec; 0.058 sec/batch; 96h:39m:25s remains)
INFO - root - 2019-11-04 00:01:51.834407: step 44290, total loss = 1.07, predict loss = 0.28 (73.5 examples/sec; 0.054 sec/batch; 90h:02m:08s remains)
INFO - root - 2019-11-04 00:01:52.481287: step 44300, total loss = 0.79, predict loss = 0.18 (67.9 examples/sec; 0.059 sec/batch; 97h:26m:04s remains)
INFO - root - 2019-11-04 00:01:53.128275: step 44310, total loss = 0.54, predict loss = 0.12 (63.7 examples/sec; 0.063 sec/batch; 103h:49m:29s remains)
INFO - root - 2019-11-04 00:01:53.777376: step 44320, total loss = 0.99, predict loss = 0.26 (62.0 examples/sec; 0.065 sec/batch; 106h:43m:20s remains)
INFO - root - 2019-11-04 00:01:54.500497: step 44330, total loss = 0.71, predict loss = 0.17 (65.8 examples/sec; 0.061 sec/batch; 100h:35m:06s remains)
INFO - root - 2019-11-04 00:01:55.151852: step 44340, total loss = 0.62, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 93h:40m:15s remains)
INFO - root - 2019-11-04 00:01:55.832908: step 44350, total loss = 0.57, predict loss = 0.13 (65.9 examples/sec; 0.061 sec/batch; 100h:25m:53s remains)
INFO - root - 2019-11-04 00:01:56.446389: step 44360, total loss = 0.77, predict loss = 0.19 (73.2 examples/sec; 0.055 sec/batch; 90h:23m:00s remains)
INFO - root - 2019-11-04 00:01:57.095141: step 44370, total loss = 0.83, predict loss = 0.20 (70.7 examples/sec; 0.057 sec/batch; 93h:35m:04s remains)
INFO - root - 2019-11-04 00:01:57.724118: step 44380, total loss = 0.62, predict loss = 0.14 (64.7 examples/sec; 0.062 sec/batch; 102h:17m:38s remains)
INFO - root - 2019-11-04 00:01:58.419568: step 44390, total loss = 0.45, predict loss = 0.10 (65.6 examples/sec; 0.061 sec/batch; 100h:55m:56s remains)
INFO - root - 2019-11-04 00:01:59.053425: step 44400, total loss = 0.47, predict loss = 0.12 (67.4 examples/sec; 0.059 sec/batch; 98h:14m:57s remains)
INFO - root - 2019-11-04 00:01:59.674736: step 44410, total loss = 0.59, predict loss = 0.14 (70.7 examples/sec; 0.057 sec/batch; 93h:32m:09s remains)
INFO - root - 2019-11-04 00:02:00.347250: step 44420, total loss = 0.68, predict loss = 0.16 (65.9 examples/sec; 0.061 sec/batch; 100h:22m:03s remains)
INFO - root - 2019-11-04 00:02:00.979968: step 44430, total loss = 0.75, predict loss = 0.17 (74.2 examples/sec; 0.054 sec/batch; 89h:08m:20s remains)
INFO - root - 2019-11-04 00:02:01.627739: step 44440, total loss = 0.55, predict loss = 0.13 (65.9 examples/sec; 0.061 sec/batch; 100h:22m:20s remains)
INFO - root - 2019-11-04 00:02:02.269766: step 44450, total loss = 0.51, predict loss = 0.11 (72.6 examples/sec; 0.055 sec/batch; 91h:10m:18s remains)
INFO - root - 2019-11-04 00:02:02.939616: step 44460, total loss = 0.40, predict loss = 0.08 (65.7 examples/sec; 0.061 sec/batch; 100h:43m:12s remains)
INFO - root - 2019-11-04 00:02:03.625366: step 44470, total loss = 0.39, predict loss = 0.08 (60.2 examples/sec; 0.066 sec/batch; 109h:55m:30s remains)
INFO - root - 2019-11-04 00:02:04.296099: step 44480, total loss = 0.29, predict loss = 0.06 (66.1 examples/sec; 0.061 sec/batch; 100h:07m:17s remains)
INFO - root - 2019-11-04 00:02:04.938383: step 44490, total loss = 0.41, predict loss = 0.09 (77.5 examples/sec; 0.052 sec/batch; 85h:26m:04s remains)
INFO - root - 2019-11-04 00:02:05.577769: step 44500, total loss = 0.46, predict loss = 0.10 (65.2 examples/sec; 0.061 sec/batch; 101h:29m:02s remains)
INFO - root - 2019-11-04 00:02:06.221784: step 44510, total loss = 0.23, predict loss = 0.05 (72.9 examples/sec; 0.055 sec/batch; 90h:43m:46s remains)
INFO - root - 2019-11-04 00:02:06.880121: step 44520, total loss = 0.38, predict loss = 0.07 (62.7 examples/sec; 0.064 sec/batch; 105h:34m:19s remains)
INFO - root - 2019-11-04 00:02:07.530530: step 44530, total loss = 0.41, predict loss = 0.09 (67.4 examples/sec; 0.059 sec/batch; 98h:10m:27s remains)
INFO - root - 2019-11-04 00:02:08.220717: step 44540, total loss = 0.46, predict loss = 0.10 (67.7 examples/sec; 0.059 sec/batch; 97h:46m:58s remains)
INFO - root - 2019-11-04 00:02:08.886541: step 44550, total loss = 0.40, predict loss = 0.08 (69.8 examples/sec; 0.057 sec/batch; 94h:47m:24s remains)
INFO - root - 2019-11-04 00:02:09.573204: step 44560, total loss = 0.41, predict loss = 0.08 (60.2 examples/sec; 0.066 sec/batch; 109h:55m:01s remains)
INFO - root - 2019-11-04 00:02:10.241199: step 44570, total loss = 0.56, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 101h:35m:35s remains)
INFO - root - 2019-11-04 00:02:10.916084: step 44580, total loss = 0.42, predict loss = 0.09 (80.5 examples/sec; 0.050 sec/batch; 82h:13m:30s remains)
INFO - root - 2019-11-04 00:02:11.554185: step 44590, total loss = 0.59, predict loss = 0.14 (74.1 examples/sec; 0.054 sec/batch; 89h:20m:38s remains)
INFO - root - 2019-11-04 00:02:12.209946: step 44600, total loss = 0.50, predict loss = 0.12 (68.6 examples/sec; 0.058 sec/batch; 96h:23m:48s remains)
INFO - root - 2019-11-04 00:02:12.894737: step 44610, total loss = 0.53, predict loss = 0.13 (57.9 examples/sec; 0.069 sec/batch; 114h:21m:14s remains)
INFO - root - 2019-11-04 00:02:13.569356: step 44620, total loss = 0.51, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 96h:45m:03s remains)
INFO - root - 2019-11-04 00:02:14.235960: step 44630, total loss = 0.70, predict loss = 0.16 (69.4 examples/sec; 0.058 sec/batch; 95h:23m:58s remains)
INFO - root - 2019-11-04 00:02:14.859982: step 44640, total loss = 0.63, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 91h:37m:41s remains)
INFO - root - 2019-11-04 00:02:15.486181: step 44650, total loss = 0.73, predict loss = 0.17 (80.4 examples/sec; 0.050 sec/batch; 82h:16m:01s remains)
INFO - root - 2019-11-04 00:02:16.127382: step 44660, total loss = 0.69, predict loss = 0.16 (66.2 examples/sec; 0.060 sec/batch; 99h:58m:24s remains)
INFO - root - 2019-11-04 00:02:16.747490: step 44670, total loss = 0.45, predict loss = 0.10 (66.7 examples/sec; 0.060 sec/batch; 99h:10m:42s remains)
INFO - root - 2019-11-04 00:02:17.367103: step 44680, total loss = 0.51, predict loss = 0.12 (74.8 examples/sec; 0.053 sec/batch; 88h:24m:27s remains)
INFO - root - 2019-11-04 00:02:18.011251: step 44690, total loss = 0.32, predict loss = 0.07 (63.7 examples/sec; 0.063 sec/batch; 103h:48m:47s remains)
INFO - root - 2019-11-04 00:02:18.623041: step 44700, total loss = 0.49, predict loss = 0.11 (84.2 examples/sec; 0.048 sec/batch; 78h:37m:23s remains)
INFO - root - 2019-11-04 00:02:19.263032: step 44710, total loss = 0.48, predict loss = 0.11 (64.9 examples/sec; 0.062 sec/batch; 101h:59m:45s remains)
INFO - root - 2019-11-04 00:02:19.901107: step 44720, total loss = 0.61, predict loss = 0.14 (68.8 examples/sec; 0.058 sec/batch; 96h:09m:10s remains)
INFO - root - 2019-11-04 00:02:20.533886: step 44730, total loss = 0.27, predict loss = 0.06 (65.5 examples/sec; 0.061 sec/batch; 100h:57m:46s remains)
INFO - root - 2019-11-04 00:02:21.199399: step 44740, total loss = 0.21, predict loss = 0.04 (71.2 examples/sec; 0.056 sec/batch; 92h:55m:12s remains)
INFO - root - 2019-11-04 00:02:21.842302: step 44750, total loss = 0.29, predict loss = 0.07 (75.2 examples/sec; 0.053 sec/batch; 88h:00m:56s remains)
INFO - root - 2019-11-04 00:02:22.496732: step 44760, total loss = 0.58, predict loss = 0.13 (73.6 examples/sec; 0.054 sec/batch; 89h:56m:44s remains)
INFO - root - 2019-11-04 00:02:23.123596: step 44770, total loss = 0.45, predict loss = 0.10 (71.9 examples/sec; 0.056 sec/batch; 91h:59m:12s remains)
INFO - root - 2019-11-04 00:02:23.743255: step 44780, total loss = 0.45, predict loss = 0.11 (69.2 examples/sec; 0.058 sec/batch; 95h:38m:48s remains)
INFO - root - 2019-11-04 00:02:24.375913: step 44790, total loss = 0.62, predict loss = 0.15 (73.6 examples/sec; 0.054 sec/batch; 89h:55m:54s remains)
INFO - root - 2019-11-04 00:02:24.986698: step 44800, total loss = 0.65, predict loss = 0.15 (73.3 examples/sec; 0.055 sec/batch; 90h:18m:11s remains)
INFO - root - 2019-11-04 00:02:25.617794: step 44810, total loss = 0.52, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 97h:31m:50s remains)
INFO - root - 2019-11-04 00:02:26.235976: step 44820, total loss = 0.52, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 92h:10m:52s remains)
INFO - root - 2019-11-04 00:02:26.870318: step 44830, total loss = 0.44, predict loss = 0.10 (67.1 examples/sec; 0.060 sec/batch; 98h:35m:48s remains)
INFO - root - 2019-11-04 00:02:27.517669: step 44840, total loss = 0.48, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 99h:32m:41s remains)
INFO - root - 2019-11-04 00:02:28.142687: step 44850, total loss = 0.36, predict loss = 0.08 (69.2 examples/sec; 0.058 sec/batch; 95h:33m:17s remains)
INFO - root - 2019-11-04 00:02:28.763493: step 44860, total loss = 0.45, predict loss = 0.10 (66.4 examples/sec; 0.060 sec/batch; 99h:41m:07s remains)
INFO - root - 2019-11-04 00:02:29.388158: step 44870, total loss = 0.60, predict loss = 0.14 (68.6 examples/sec; 0.058 sec/batch; 96h:23m:43s remains)
INFO - root - 2019-11-04 00:02:30.002938: step 44880, total loss = 0.52, predict loss = 0.12 (70.0 examples/sec; 0.057 sec/batch; 94h:28m:31s remains)
INFO - root - 2019-11-04 00:02:30.641834: step 44890, total loss = 0.60, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 94h:07m:54s remains)
INFO - root - 2019-11-04 00:02:31.277069: step 44900, total loss = 0.79, predict loss = 0.18 (65.4 examples/sec; 0.061 sec/batch; 101h:13m:19s remains)
INFO - root - 2019-11-04 00:02:31.946628: step 44910, total loss = 0.60, predict loss = 0.14 (55.2 examples/sec; 0.072 sec/batch; 119h:47m:55s remains)
INFO - root - 2019-11-04 00:02:32.653927: step 44920, total loss = 0.52, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 104h:07m:19s remains)
INFO - root - 2019-11-04 00:02:33.268647: step 44930, total loss = 0.70, predict loss = 0.16 (66.3 examples/sec; 0.060 sec/batch; 99h:43m:47s remains)
INFO - root - 2019-11-04 00:02:33.888282: step 44940, total loss = 0.63, predict loss = 0.15 (62.2 examples/sec; 0.064 sec/batch; 106h:21m:15s remains)
INFO - root - 2019-11-04 00:02:34.488914: step 44950, total loss = 0.65, predict loss = 0.16 (74.7 examples/sec; 0.054 sec/batch; 88h:34m:25s remains)
INFO - root - 2019-11-04 00:02:35.156115: step 44960, total loss = 0.59, predict loss = 0.12 (63.1 examples/sec; 0.063 sec/batch; 104h:55m:59s remains)
INFO - root - 2019-11-04 00:02:35.806388: step 44970, total loss = 0.54, predict loss = 0.12 (68.6 examples/sec; 0.058 sec/batch; 96h:27m:33s remains)
INFO - root - 2019-11-04 00:02:36.444871: step 44980, total loss = 0.61, predict loss = 0.15 (65.9 examples/sec; 0.061 sec/batch; 100h:24m:56s remains)
INFO - root - 2019-11-04 00:02:37.088144: step 44990, total loss = 0.32, predict loss = 0.07 (76.3 examples/sec; 0.052 sec/batch; 86h:46m:29s remains)
INFO - root - 2019-11-04 00:02:37.711466: step 45000, total loss = 0.49, predict loss = 0.11 (81.2 examples/sec; 0.049 sec/batch; 81h:30m:57s remains)
INFO - root - 2019-11-04 00:02:39.459065: step 45010, total loss = 0.63, predict loss = 0.15 (71.9 examples/sec; 0.056 sec/batch; 92h:03m:37s remains)
INFO - root - 2019-11-04 00:02:40.094941: step 45020, total loss = 0.45, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 100h:17m:53s remains)
INFO - root - 2019-11-04 00:02:40.754851: step 45030, total loss = 0.56, predict loss = 0.12 (72.0 examples/sec; 0.056 sec/batch; 91h:53m:46s remains)
INFO - root - 2019-11-04 00:02:41.406107: step 45040, total loss = 0.69, predict loss = 0.17 (64.4 examples/sec; 0.062 sec/batch; 102h:49m:16s remains)
INFO - root - 2019-11-04 00:02:42.052603: step 45050, total loss = 0.66, predict loss = 0.16 (67.1 examples/sec; 0.060 sec/batch; 98h:38m:21s remains)
INFO - root - 2019-11-04 00:02:42.694146: step 45060, total loss = 0.60, predict loss = 0.13 (71.9 examples/sec; 0.056 sec/batch; 92h:04m:58s remains)
INFO - root - 2019-11-04 00:02:43.290915: step 45070, total loss = 0.94, predict loss = 0.22 (66.2 examples/sec; 0.060 sec/batch; 100h:01m:02s remains)
INFO - root - 2019-11-04 00:02:43.941911: step 45080, total loss = 1.01, predict loss = 0.24 (66.9 examples/sec; 0.060 sec/batch; 98h:50m:21s remains)
INFO - root - 2019-11-04 00:02:44.600203: step 45090, total loss = 0.94, predict loss = 0.22 (65.5 examples/sec; 0.061 sec/batch; 100h:56m:43s remains)
INFO - root - 2019-11-04 00:02:45.240000: step 45100, total loss = 0.82, predict loss = 0.19 (72.3 examples/sec; 0.055 sec/batch; 91h:34m:15s remains)
INFO - root - 2019-11-04 00:02:45.923899: step 45110, total loss = 0.85, predict loss = 0.20 (73.5 examples/sec; 0.054 sec/batch; 89h:58m:14s remains)
INFO - root - 2019-11-04 00:02:46.546173: step 45120, total loss = 0.99, predict loss = 0.23 (77.7 examples/sec; 0.051 sec/batch; 85h:06m:22s remains)
INFO - root - 2019-11-04 00:02:47.208647: step 45130, total loss = 0.72, predict loss = 0.17 (64.7 examples/sec; 0.062 sec/batch; 102h:12m:14s remains)
INFO - root - 2019-11-04 00:02:47.825482: step 45140, total loss = 0.55, predict loss = 0.12 (76.4 examples/sec; 0.052 sec/batch; 86h:33m:52s remains)
INFO - root - 2019-11-04 00:02:48.455011: step 45150, total loss = 0.92, predict loss = 0.22 (79.0 examples/sec; 0.051 sec/batch; 83h:44m:10s remains)
INFO - root - 2019-11-04 00:02:49.163669: step 45160, total loss = 0.89, predict loss = 0.21 (65.9 examples/sec; 0.061 sec/batch; 100h:24m:02s remains)
INFO - root - 2019-11-04 00:02:49.795423: step 45170, total loss = 0.63, predict loss = 0.14 (76.3 examples/sec; 0.052 sec/batch; 86h:43m:46s remains)
INFO - root - 2019-11-04 00:02:50.463668: step 45180, total loss = 0.51, predict loss = 0.11 (61.5 examples/sec; 0.065 sec/batch; 107h:33m:32s remains)
INFO - root - 2019-11-04 00:02:51.134735: step 45190, total loss = 0.71, predict loss = 0.17 (66.6 examples/sec; 0.060 sec/batch; 99h:21m:08s remains)
INFO - root - 2019-11-04 00:02:51.794312: step 45200, total loss = 0.71, predict loss = 0.16 (61.1 examples/sec; 0.065 sec/batch; 108h:15m:39s remains)
INFO - root - 2019-11-04 00:02:52.430057: step 45210, total loss = 0.67, predict loss = 0.15 (70.7 examples/sec; 0.057 sec/batch; 93h:36m:16s remains)
INFO - root - 2019-11-04 00:02:53.061546: step 45220, total loss = 0.47, predict loss = 0.11 (69.0 examples/sec; 0.058 sec/batch; 95h:51m:45s remains)
INFO - root - 2019-11-04 00:02:53.761656: step 45230, total loss = 0.54, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 97h:25m:08s remains)
INFO - root - 2019-11-04 00:02:54.359361: step 45240, total loss = 0.63, predict loss = 0.14 (72.6 examples/sec; 0.055 sec/batch; 91h:05m:32s remains)
INFO - root - 2019-11-04 00:02:54.941216: step 45250, total loss = 0.66, predict loss = 0.16 (69.8 examples/sec; 0.057 sec/batch; 94h:44m:21s remains)
INFO - root - 2019-11-04 00:02:55.546029: step 45260, total loss = 0.63, predict loss = 0.15 (75.4 examples/sec; 0.053 sec/batch; 87h:44m:39s remains)
INFO - root - 2019-11-04 00:02:56.158559: step 45270, total loss = 0.54, predict loss = 0.12 (67.6 examples/sec; 0.059 sec/batch; 97h:53m:05s remains)
INFO - root - 2019-11-04 00:02:56.772954: step 45280, total loss = 0.46, predict loss = 0.10 (67.7 examples/sec; 0.059 sec/batch; 97h:46m:08s remains)
INFO - root - 2019-11-04 00:02:57.406651: step 45290, total loss = 0.60, predict loss = 0.13 (70.0 examples/sec; 0.057 sec/batch; 94h:33m:50s remains)
INFO - root - 2019-11-04 00:02:58.054198: step 45300, total loss = 0.48, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 94h:46m:44s remains)
INFO - root - 2019-11-04 00:02:58.674234: step 45310, total loss = 0.59, predict loss = 0.13 (72.7 examples/sec; 0.055 sec/batch; 90h:59m:53s remains)
INFO - root - 2019-11-04 00:02:59.342606: step 45320, total loss = 0.52, predict loss = 0.11 (72.9 examples/sec; 0.055 sec/batch; 90h:42m:02s remains)
INFO - root - 2019-11-04 00:02:59.995791: step 45330, total loss = 0.57, predict loss = 0.13 (82.8 examples/sec; 0.048 sec/batch; 79h:53m:16s remains)
INFO - root - 2019-11-04 00:03:00.614997: step 45340, total loss = 0.47, predict loss = 0.10 (73.3 examples/sec; 0.055 sec/batch; 90h:13m:15s remains)
INFO - root - 2019-11-04 00:03:01.260578: step 45350, total loss = 0.58, predict loss = 0.13 (70.1 examples/sec; 0.057 sec/batch; 94h:25m:39s remains)
INFO - root - 2019-11-04 00:03:01.888217: step 45360, total loss = 0.45, predict loss = 0.10 (80.1 examples/sec; 0.050 sec/batch; 82h:34m:40s remains)
INFO - root - 2019-11-04 00:03:02.529327: step 45370, total loss = 0.47, predict loss = 0.10 (74.3 examples/sec; 0.054 sec/batch; 89h:01m:45s remains)
INFO - root - 2019-11-04 00:03:03.233181: step 45380, total loss = 0.54, predict loss = 0.12 (54.4 examples/sec; 0.074 sec/batch; 121h:41m:29s remains)
INFO - root - 2019-11-04 00:03:03.898847: step 45390, total loss = 0.46, predict loss = 0.11 (69.0 examples/sec; 0.058 sec/batch; 95h:54m:14s remains)
INFO - root - 2019-11-04 00:03:04.553441: step 45400, total loss = 0.56, predict loss = 0.12 (57.9 examples/sec; 0.069 sec/batch; 114h:15m:54s remains)
INFO - root - 2019-11-04 00:03:05.169412: step 45410, total loss = 0.47, predict loss = 0.11 (86.7 examples/sec; 0.046 sec/batch; 76h:20m:31s remains)
INFO - root - 2019-11-04 00:03:05.761178: step 45420, total loss = 0.44, predict loss = 0.10 (78.9 examples/sec; 0.051 sec/batch; 83h:50m:03s remains)
INFO - root - 2019-11-04 00:03:06.410978: step 45430, total loss = 0.54, predict loss = 0.13 (66.5 examples/sec; 0.060 sec/batch; 99h:30m:39s remains)
INFO - root - 2019-11-04 00:03:07.039716: step 45440, total loss = 0.59, predict loss = 0.14 (69.2 examples/sec; 0.058 sec/batch; 95h:34m:10s remains)
INFO - root - 2019-11-04 00:03:07.711065: step 45450, total loss = 0.48, predict loss = 0.11 (69.6 examples/sec; 0.057 sec/batch; 95h:04m:39s remains)
INFO - root - 2019-11-04 00:03:08.334440: step 45460, total loss = 0.46, predict loss = 0.10 (67.6 examples/sec; 0.059 sec/batch; 97h:52m:08s remains)
INFO - root - 2019-11-04 00:03:08.981412: step 45470, total loss = 0.69, predict loss = 0.16 (65.4 examples/sec; 0.061 sec/batch; 101h:07m:46s remains)
INFO - root - 2019-11-04 00:03:09.624795: step 45480, total loss = 0.58, predict loss = 0.13 (68.3 examples/sec; 0.059 sec/batch; 96h:53m:33s remains)
INFO - root - 2019-11-04 00:03:10.267124: step 45490, total loss = 0.57, predict loss = 0.14 (73.0 examples/sec; 0.055 sec/batch; 90h:36m:25s remains)
INFO - root - 2019-11-04 00:03:10.926832: step 45500, total loss = 0.65, predict loss = 0.15 (64.1 examples/sec; 0.062 sec/batch; 103h:14m:26s remains)
INFO - root - 2019-11-04 00:03:11.598458: step 45510, total loss = 0.74, predict loss = 0.17 (58.0 examples/sec; 0.069 sec/batch; 113h:59m:20s remains)
INFO - root - 2019-11-04 00:03:12.267718: step 45520, total loss = 0.75, predict loss = 0.18 (70.8 examples/sec; 0.057 sec/batch; 93h:28m:55s remains)
INFO - root - 2019-11-04 00:03:12.894058: step 45530, total loss = 0.73, predict loss = 0.18 (67.5 examples/sec; 0.059 sec/batch; 97h:58m:39s remains)
INFO - root - 2019-11-04 00:03:13.545046: step 45540, total loss = 0.94, predict loss = 0.23 (62.5 examples/sec; 0.064 sec/batch; 105h:48m:06s remains)
INFO - root - 2019-11-04 00:03:14.223217: step 45550, total loss = 0.68, predict loss = 0.16 (69.7 examples/sec; 0.057 sec/batch; 94h:58m:44s remains)
INFO - root - 2019-11-04 00:03:14.893312: step 45560, total loss = 0.69, predict loss = 0.15 (60.8 examples/sec; 0.066 sec/batch; 108h:51m:04s remains)
INFO - root - 2019-11-04 00:03:15.560426: step 45570, total loss = 0.52, predict loss = 0.12 (63.7 examples/sec; 0.063 sec/batch; 103h:47m:16s remains)
INFO - root - 2019-11-04 00:03:16.249813: step 45580, total loss = 0.82, predict loss = 0.19 (74.3 examples/sec; 0.054 sec/batch; 89h:02m:59s remains)
INFO - root - 2019-11-04 00:03:16.863076: step 45590, total loss = 0.61, predict loss = 0.15 (78.6 examples/sec; 0.051 sec/batch; 84h:11m:17s remains)
INFO - root - 2019-11-04 00:03:17.482702: step 45600, total loss = 0.36, predict loss = 0.08 (73.6 examples/sec; 0.054 sec/batch; 89h:50m:43s remains)
INFO - root - 2019-11-04 00:03:18.167658: step 45610, total loss = 0.59, predict loss = 0.14 (66.4 examples/sec; 0.060 sec/batch; 99h:37m:13s remains)
INFO - root - 2019-11-04 00:03:18.826135: step 45620, total loss = 0.38, predict loss = 0.08 (61.0 examples/sec; 0.066 sec/batch; 108h:24m:51s remains)
INFO - root - 2019-11-04 00:03:19.476675: step 45630, total loss = 0.39, predict loss = 0.08 (68.2 examples/sec; 0.059 sec/batch; 97h:04m:05s remains)
INFO - root - 2019-11-04 00:03:20.122374: step 45640, total loss = 0.47, predict loss = 0.11 (67.8 examples/sec; 0.059 sec/batch; 97h:38m:38s remains)
INFO - root - 2019-11-04 00:03:20.768899: step 45650, total loss = 0.41, predict loss = 0.09 (70.2 examples/sec; 0.057 sec/batch; 94h:13m:34s remains)
INFO - root - 2019-11-04 00:03:21.402802: step 45660, total loss = 0.48, predict loss = 0.12 (72.7 examples/sec; 0.055 sec/batch; 91h:02m:28s remains)
INFO - root - 2019-11-04 00:03:22.046402: step 45670, total loss = 0.48, predict loss = 0.11 (64.7 examples/sec; 0.062 sec/batch; 102h:13m:07s remains)
INFO - root - 2019-11-04 00:03:22.719007: step 45680, total loss = 0.46, predict loss = 0.10 (72.2 examples/sec; 0.055 sec/batch; 91h:36m:57s remains)
INFO - root - 2019-11-04 00:03:23.360607: step 45690, total loss = 0.50, predict loss = 0.12 (67.5 examples/sec; 0.059 sec/batch; 98h:02m:16s remains)
INFO - root - 2019-11-04 00:03:23.983613: step 45700, total loss = 0.53, predict loss = 0.12 (74.3 examples/sec; 0.054 sec/batch; 88h:59m:09s remains)
INFO - root - 2019-11-04 00:03:24.629144: step 45710, total loss = 0.68, predict loss = 0.17 (78.5 examples/sec; 0.051 sec/batch; 84h:17m:30s remains)
INFO - root - 2019-11-04 00:03:25.220361: step 45720, total loss = 0.86, predict loss = 0.21 (75.9 examples/sec; 0.053 sec/batch; 87h:07m:41s remains)
INFO - root - 2019-11-04 00:03:25.876674: step 45730, total loss = 0.68, predict loss = 0.15 (60.5 examples/sec; 0.066 sec/batch; 109h:24m:17s remains)
INFO - root - 2019-11-04 00:03:26.532783: step 45740, total loss = 0.71, predict loss = 0.18 (64.9 examples/sec; 0.062 sec/batch; 101h:57m:39s remains)
INFO - root - 2019-11-04 00:03:27.161926: step 45750, total loss = 0.77, predict loss = 0.18 (82.4 examples/sec; 0.049 sec/batch; 80h:19m:24s remains)
INFO - root - 2019-11-04 00:03:27.816284: step 45760, total loss = 0.69, predict loss = 0.17 (62.8 examples/sec; 0.064 sec/batch; 105h:20m:13s remains)
INFO - root - 2019-11-04 00:03:28.513403: step 45770, total loss = 0.63, predict loss = 0.15 (63.1 examples/sec; 0.063 sec/batch; 104h:51m:27s remains)
INFO - root - 2019-11-04 00:03:29.215976: step 45780, total loss = 0.57, predict loss = 0.14 (56.3 examples/sec; 0.071 sec/batch; 117h:25m:45s remains)
INFO - root - 2019-11-04 00:03:29.846589: step 45790, total loss = 0.66, predict loss = 0.16 (70.5 examples/sec; 0.057 sec/batch; 93h:51m:36s remains)
INFO - root - 2019-11-04 00:03:30.485849: step 45800, total loss = 0.60, predict loss = 0.14 (62.6 examples/sec; 0.064 sec/batch; 105h:37m:23s remains)
INFO - root - 2019-11-04 00:03:31.209440: step 45810, total loss = 0.55, predict loss = 0.13 (65.0 examples/sec; 0.061 sec/batch; 101h:42m:29s remains)
INFO - root - 2019-11-04 00:03:31.943542: step 45820, total loss = 0.49, predict loss = 0.12 (58.9 examples/sec; 0.068 sec/batch; 112h:22m:05s remains)
INFO - root - 2019-11-04 00:03:32.589785: step 45830, total loss = 0.59, predict loss = 0.13 (68.5 examples/sec; 0.058 sec/batch; 96h:38m:26s remains)
INFO - root - 2019-11-04 00:03:33.214797: step 45840, total loss = 0.56, predict loss = 0.13 (65.6 examples/sec; 0.061 sec/batch; 100h:50m:05s remains)
INFO - root - 2019-11-04 00:03:33.884287: step 45850, total loss = 0.67, predict loss = 0.15 (62.9 examples/sec; 0.064 sec/batch; 105h:07m:58s remains)
INFO - root - 2019-11-04 00:03:34.542656: step 45860, total loss = 0.69, predict loss = 0.16 (76.1 examples/sec; 0.053 sec/batch; 86h:58m:39s remains)
INFO - root - 2019-11-04 00:03:35.132609: step 45870, total loss = 0.46, predict loss = 0.10 (72.3 examples/sec; 0.055 sec/batch; 91h:33m:25s remains)
INFO - root - 2019-11-04 00:03:35.718977: step 45880, total loss = 0.70, predict loss = 0.17 (72.5 examples/sec; 0.055 sec/batch; 91h:14m:33s remains)
INFO - root - 2019-11-04 00:03:36.360167: step 45890, total loss = 0.65, predict loss = 0.15 (71.3 examples/sec; 0.056 sec/batch; 92h:44m:10s remains)
INFO - root - 2019-11-04 00:03:36.972804: step 45900, total loss = 0.59, predict loss = 0.14 (79.2 examples/sec; 0.051 sec/batch; 83h:32m:53s remains)
INFO - root - 2019-11-04 00:03:37.606374: step 45910, total loss = 0.46, predict loss = 0.10 (71.7 examples/sec; 0.056 sec/batch; 92h:18m:00s remains)
INFO - root - 2019-11-04 00:03:38.228874: step 45920, total loss = 0.58, predict loss = 0.13 (73.4 examples/sec; 0.055 sec/batch; 90h:09m:11s remains)
INFO - root - 2019-11-04 00:03:38.832240: step 45930, total loss = 0.48, predict loss = 0.12 (75.5 examples/sec; 0.053 sec/batch; 87h:34m:57s remains)
INFO - root - 2019-11-04 00:03:39.463892: step 45940, total loss = 0.46, predict loss = 0.10 (65.7 examples/sec; 0.061 sec/batch; 100h:39m:22s remains)
INFO - root - 2019-11-04 00:03:40.091789: step 45950, total loss = 0.55, predict loss = 0.13 (72.0 examples/sec; 0.056 sec/batch; 91h:54m:47s remains)
INFO - root - 2019-11-04 00:03:40.721620: step 45960, total loss = 0.52, predict loss = 0.12 (77.2 examples/sec; 0.052 sec/batch; 85h:39m:08s remains)
INFO - root - 2019-11-04 00:03:41.366099: step 45970, total loss = 0.63, predict loss = 0.14 (73.1 examples/sec; 0.055 sec/batch; 90h:28m:43s remains)
INFO - root - 2019-11-04 00:03:41.993026: step 45980, total loss = 0.60, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 95h:32m:25s remains)
INFO - root - 2019-11-04 00:03:42.652329: step 45990, total loss = 0.55, predict loss = 0.13 (64.0 examples/sec; 0.062 sec/batch; 103h:19m:47s remains)
INFO - root - 2019-11-04 00:03:43.312644: step 46000, total loss = 0.52, predict loss = 0.12 (77.4 examples/sec; 0.052 sec/batch; 85h:31m:27s remains)
INFO - root - 2019-11-04 00:03:43.905135: step 46010, total loss = 0.57, predict loss = 0.14 (74.6 examples/sec; 0.054 sec/batch; 88h:43m:53s remains)
INFO - root - 2019-11-04 00:03:44.572774: step 46020, total loss = 0.57, predict loss = 0.15 (59.9 examples/sec; 0.067 sec/batch; 110h:23m:00s remains)
INFO - root - 2019-11-04 00:03:45.177266: step 46030, total loss = 0.63, predict loss = 0.14 (74.4 examples/sec; 0.054 sec/batch; 88h:57m:00s remains)
INFO - root - 2019-11-04 00:03:45.797970: step 46040, total loss = 0.51, predict loss = 0.13 (70.1 examples/sec; 0.057 sec/batch; 94h:19m:12s remains)
INFO - root - 2019-11-04 00:03:46.442499: step 46050, total loss = 0.68, predict loss = 0.17 (60.5 examples/sec; 0.066 sec/batch; 109h:21m:55s remains)
INFO - root - 2019-11-04 00:03:47.121779: step 46060, total loss = 0.44, predict loss = 0.10 (65.8 examples/sec; 0.061 sec/batch; 100h:27m:49s remains)
INFO - root - 2019-11-04 00:03:47.817032: step 46070, total loss = 0.52, predict loss = 0.12 (65.5 examples/sec; 0.061 sec/batch; 100h:57m:44s remains)
INFO - root - 2019-11-04 00:03:48.477927: step 46080, total loss = 0.51, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 95h:17m:36s remains)
INFO - root - 2019-11-04 00:03:49.101831: step 46090, total loss = 0.55, predict loss = 0.14 (69.4 examples/sec; 0.058 sec/batch; 95h:21m:50s remains)
INFO - root - 2019-11-04 00:03:49.732601: step 46100, total loss = 0.89, predict loss = 0.22 (81.4 examples/sec; 0.049 sec/batch; 81h:14m:28s remains)
INFO - root - 2019-11-04 00:03:50.357183: step 46110, total loss = 0.74, predict loss = 0.18 (76.4 examples/sec; 0.052 sec/batch; 86h:36m:44s remains)
INFO - root - 2019-11-04 00:03:50.988637: step 46120, total loss = 0.66, predict loss = 0.17 (73.1 examples/sec; 0.055 sec/batch; 90h:33m:00s remains)
INFO - root - 2019-11-04 00:03:51.602934: step 46130, total loss = 0.65, predict loss = 0.16 (79.2 examples/sec; 0.051 sec/batch; 83h:32m:45s remains)
INFO - root - 2019-11-04 00:03:52.202845: step 46140, total loss = 0.75, predict loss = 0.18 (78.3 examples/sec; 0.051 sec/batch; 84h:26m:19s remains)
INFO - root - 2019-11-04 00:03:52.802370: step 46150, total loss = 0.59, predict loss = 0.14 (70.4 examples/sec; 0.057 sec/batch; 93h:55m:59s remains)
INFO - root - 2019-11-04 00:03:53.405180: step 46160, total loss = 0.55, predict loss = 0.13 (75.6 examples/sec; 0.053 sec/batch; 87h:30m:39s remains)
INFO - root - 2019-11-04 00:03:54.060109: step 46170, total loss = 0.65, predict loss = 0.16 (72.0 examples/sec; 0.056 sec/batch; 91h:53m:48s remains)
INFO - root - 2019-11-04 00:03:54.682586: step 46180, total loss = 0.84, predict loss = 0.20 (67.8 examples/sec; 0.059 sec/batch; 97h:34m:46s remains)
INFO - root - 2019-11-04 00:03:55.323318: step 46190, total loss = 0.63, predict loss = 0.15 (65.6 examples/sec; 0.061 sec/batch; 100h:50m:07s remains)
INFO - root - 2019-11-04 00:03:56.025201: step 46200, total loss = 0.76, predict loss = 0.17 (68.4 examples/sec; 0.059 sec/batch; 96h:45m:01s remains)
INFO - root - 2019-11-04 00:03:56.712854: step 46210, total loss = 0.62, predict loss = 0.14 (69.0 examples/sec; 0.058 sec/batch; 95h:54m:51s remains)
INFO - root - 2019-11-04 00:03:57.404510: step 46220, total loss = 0.55, predict loss = 0.12 (61.3 examples/sec; 0.065 sec/batch; 107h:50m:35s remains)
INFO - root - 2019-11-04 00:03:58.085445: step 46230, total loss = 0.45, predict loss = 0.09 (59.6 examples/sec; 0.067 sec/batch; 111h:03m:07s remains)
INFO - root - 2019-11-04 00:03:58.736250: step 46240, total loss = 0.57, predict loss = 0.13 (70.6 examples/sec; 0.057 sec/batch; 93h:44m:13s remains)
INFO - root - 2019-11-04 00:03:59.354303: step 46250, total loss = 0.67, predict loss = 0.16 (68.3 examples/sec; 0.059 sec/batch; 96h:49m:55s remains)
INFO - root - 2019-11-04 00:03:59.971026: step 46260, total loss = 0.67, predict loss = 0.15 (70.3 examples/sec; 0.057 sec/batch; 94h:02m:29s remains)
INFO - root - 2019-11-04 00:04:00.609224: step 46270, total loss = 0.65, predict loss = 0.15 (69.2 examples/sec; 0.058 sec/batch; 95h:31m:51s remains)
INFO - root - 2019-11-04 00:04:01.247189: step 46280, total loss = 0.52, predict loss = 0.12 (66.1 examples/sec; 0.061 sec/batch; 100h:05m:15s remains)
INFO - root - 2019-11-04 00:04:02.015048: step 46290, total loss = 0.57, predict loss = 0.13 (55.0 examples/sec; 0.073 sec/batch; 120h:22m:25s remains)
INFO - root - 2019-11-04 00:04:02.627776: step 46300, total loss = 0.67, predict loss = 0.15 (74.5 examples/sec; 0.054 sec/batch; 88h:46m:59s remains)
INFO - root - 2019-11-04 00:04:03.257031: step 46310, total loss = 0.60, predict loss = 0.14 (67.7 examples/sec; 0.059 sec/batch; 97h:43m:52s remains)
INFO - root - 2019-11-04 00:04:03.889055: step 46320, total loss = 0.55, predict loss = 0.12 (72.2 examples/sec; 0.055 sec/batch; 91h:37m:34s remains)
INFO - root - 2019-11-04 00:04:04.522034: step 46330, total loss = 0.50, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 99h:25m:05s remains)
INFO - root - 2019-11-04 00:04:05.154241: step 46340, total loss = 0.56, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 100h:27m:27s remains)
INFO - root - 2019-11-04 00:04:05.659678: step 46350, total loss = 0.63, predict loss = 0.13 (93.4 examples/sec; 0.043 sec/batch; 70h:50m:45s remains)
INFO - root - 2019-11-04 00:04:06.119450: step 46360, total loss = 0.53, predict loss = 0.12 (94.5 examples/sec; 0.042 sec/batch; 69h:58m:55s remains)
INFO - root - 2019-11-04 00:04:07.159771: step 46370, total loss = 0.54, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 89h:29m:58s remains)
INFO - root - 2019-11-04 00:04:07.756838: step 46380, total loss = 0.36, predict loss = 0.08 (73.7 examples/sec; 0.054 sec/batch; 89h:47m:49s remains)
INFO - root - 2019-11-04 00:04:08.397151: step 46390, total loss = 0.79, predict loss = 0.19 (66.9 examples/sec; 0.060 sec/batch; 98h:49m:04s remains)
INFO - root - 2019-11-04 00:04:09.027258: step 46400, total loss = 0.52, predict loss = 0.12 (73.7 examples/sec; 0.054 sec/batch; 89h:45m:40s remains)
INFO - root - 2019-11-04 00:04:09.622547: step 46410, total loss = 0.59, predict loss = 0.13 (70.1 examples/sec; 0.057 sec/batch; 94h:18m:48s remains)
INFO - root - 2019-11-04 00:04:10.276928: step 46420, total loss = 0.56, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 104h:08m:22s remains)
INFO - root - 2019-11-04 00:04:10.965702: step 46430, total loss = 0.49, predict loss = 0.11 (62.1 examples/sec; 0.064 sec/batch; 106h:31m:20s remains)
INFO - root - 2019-11-04 00:04:11.638736: step 46440, total loss = 0.62, predict loss = 0.14 (73.1 examples/sec; 0.055 sec/batch; 90h:32m:41s remains)
INFO - root - 2019-11-04 00:04:12.281995: step 46450, total loss = 0.73, predict loss = 0.20 (71.1 examples/sec; 0.056 sec/batch; 93h:04m:23s remains)
INFO - root - 2019-11-04 00:04:12.894127: step 46460, total loss = 0.70, predict loss = 0.16 (59.7 examples/sec; 0.067 sec/batch; 110h:45m:00s remains)
INFO - root - 2019-11-04 00:04:13.532133: step 46470, total loss = 0.67, predict loss = 0.16 (72.7 examples/sec; 0.055 sec/batch; 91h:01m:27s remains)
INFO - root - 2019-11-04 00:04:14.207515: step 46480, total loss = 0.64, predict loss = 0.14 (84.0 examples/sec; 0.048 sec/batch; 78h:44m:10s remains)
INFO - root - 2019-11-04 00:04:14.804741: step 46490, total loss = 0.56, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 96h:50m:00s remains)
INFO - root - 2019-11-04 00:04:15.400777: step 46500, total loss = 0.55, predict loss = 0.12 (73.6 examples/sec; 0.054 sec/batch; 89h:52m:16s remains)
INFO - root - 2019-11-04 00:04:16.013252: step 46510, total loss = 0.58, predict loss = 0.13 (67.9 examples/sec; 0.059 sec/batch; 97h:28m:08s remains)
INFO - root - 2019-11-04 00:04:16.715422: step 46520, total loss = 0.81, predict loss = 0.20 (66.0 examples/sec; 0.061 sec/batch; 100h:11m:18s remains)
INFO - root - 2019-11-04 00:04:17.379063: step 46530, total loss = 0.59, predict loss = 0.13 (71.4 examples/sec; 0.056 sec/batch; 92h:42m:23s remains)
INFO - root - 2019-11-04 00:04:18.006059: step 46540, total loss = 0.48, predict loss = 0.11 (71.0 examples/sec; 0.056 sec/batch; 93h:09m:45s remains)
INFO - root - 2019-11-04 00:04:18.630530: step 46550, total loss = 0.54, predict loss = 0.11 (68.5 examples/sec; 0.058 sec/batch; 96h:33m:24s remains)
INFO - root - 2019-11-04 00:04:19.269814: step 46560, total loss = 0.78, predict loss = 0.19 (66.6 examples/sec; 0.060 sec/batch; 99h:23m:06s remains)
INFO - root - 2019-11-04 00:04:19.902554: step 46570, total loss = 0.53, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 91h:09m:37s remains)
INFO - root - 2019-11-04 00:04:20.547578: step 46580, total loss = 0.46, predict loss = 0.10 (71.2 examples/sec; 0.056 sec/batch; 92h:52m:02s remains)
INFO - root - 2019-11-04 00:04:21.211525: step 46590, total loss = 0.57, predict loss = 0.13 (66.3 examples/sec; 0.060 sec/batch; 99h:45m:48s remains)
INFO - root - 2019-11-04 00:04:21.839198: step 46600, total loss = 0.51, predict loss = 0.11 (73.5 examples/sec; 0.054 sec/batch; 89h:59m:34s remains)
INFO - root - 2019-11-04 00:04:22.518052: step 46610, total loss = 0.82, predict loss = 0.18 (57.1 examples/sec; 0.070 sec/batch; 115h:55m:20s remains)
INFO - root - 2019-11-04 00:04:23.172901: step 46620, total loss = 0.58, predict loss = 0.14 (70.5 examples/sec; 0.057 sec/batch; 93h:52m:38s remains)
INFO - root - 2019-11-04 00:04:23.791558: step 46630, total loss = 0.69, predict loss = 0.17 (82.8 examples/sec; 0.048 sec/batch; 79h:54m:49s remains)
INFO - root - 2019-11-04 00:04:24.387498: step 46640, total loss = 0.70, predict loss = 0.15 (81.6 examples/sec; 0.049 sec/batch; 81h:03m:01s remains)
INFO - root - 2019-11-04 00:04:24.976632: step 46650, total loss = 0.71, predict loss = 0.14 (76.2 examples/sec; 0.052 sec/batch; 86h:46m:53s remains)
INFO - root - 2019-11-04 00:04:25.551521: step 46660, total loss = 0.69, predict loss = 0.17 (74.0 examples/sec; 0.054 sec/batch; 89h:25m:48s remains)
INFO - root - 2019-11-04 00:04:26.216472: step 46670, total loss = 0.65, predict loss = 0.15 (61.3 examples/sec; 0.065 sec/batch; 107h:51m:42s remains)
INFO - root - 2019-11-04 00:04:26.905105: step 46680, total loss = 0.58, predict loss = 0.14 (61.3 examples/sec; 0.065 sec/batch; 107h:50m:19s remains)
INFO - root - 2019-11-04 00:04:27.557820: step 46690, total loss = 0.77, predict loss = 0.18 (64.2 examples/sec; 0.062 sec/batch; 103h:02m:16s remains)
INFO - root - 2019-11-04 00:04:28.242694: step 46700, total loss = 0.71, predict loss = 0.16 (61.4 examples/sec; 0.065 sec/batch; 107h:47m:05s remains)
INFO - root - 2019-11-04 00:04:28.949063: step 46710, total loss = 0.81, predict loss = 0.18 (63.1 examples/sec; 0.063 sec/batch; 104h:49m:23s remains)
INFO - root - 2019-11-04 00:04:29.627274: step 46720, total loss = 0.53, predict loss = 0.12 (62.0 examples/sec; 0.065 sec/batch; 106h:42m:41s remains)
INFO - root - 2019-11-04 00:04:30.295685: step 46730, total loss = 0.62, predict loss = 0.14 (61.7 examples/sec; 0.065 sec/batch; 107h:10m:55s remains)
INFO - root - 2019-11-04 00:04:30.956966: step 46740, total loss = 0.62, predict loss = 0.14 (73.1 examples/sec; 0.055 sec/batch; 90h:30m:41s remains)
INFO - root - 2019-11-04 00:04:31.544815: step 46750, total loss = 0.61, predict loss = 0.14 (73.3 examples/sec; 0.055 sec/batch; 90h:16m:22s remains)
INFO - root - 2019-11-04 00:04:32.146399: step 46760, total loss = 0.66, predict loss = 0.15 (76.9 examples/sec; 0.052 sec/batch; 86h:01m:12s remains)
INFO - root - 2019-11-04 00:04:32.772418: step 46770, total loss = 0.64, predict loss = 0.14 (67.5 examples/sec; 0.059 sec/batch; 98h:01m:56s remains)
INFO - root - 2019-11-04 00:04:33.435136: step 46780, total loss = 0.82, predict loss = 0.19 (66.3 examples/sec; 0.060 sec/batch; 99h:45m:23s remains)
INFO - root - 2019-11-04 00:04:34.048077: step 46790, total loss = 0.70, predict loss = 0.17 (75.9 examples/sec; 0.053 sec/batch; 87h:08m:35s remains)
INFO - root - 2019-11-04 00:04:34.655544: step 46800, total loss = 0.56, predict loss = 0.12 (77.7 examples/sec; 0.051 sec/batch; 85h:09m:41s remains)
INFO - root - 2019-11-04 00:04:35.257732: step 46810, total loss = 0.48, predict loss = 0.10 (71.8 examples/sec; 0.056 sec/batch; 92h:04m:52s remains)
INFO - root - 2019-11-04 00:04:35.872606: step 46820, total loss = 0.60, predict loss = 0.15 (75.6 examples/sec; 0.053 sec/batch; 87h:28m:11s remains)
INFO - root - 2019-11-04 00:04:36.487433: step 46830, total loss = 0.61, predict loss = 0.15 (69.2 examples/sec; 0.058 sec/batch; 95h:33m:07s remains)
INFO - root - 2019-11-04 00:04:37.126387: step 46840, total loss = 0.55, predict loss = 0.12 (72.3 examples/sec; 0.055 sec/batch; 91h:29m:48s remains)
INFO - root - 2019-11-04 00:04:37.723531: step 46850, total loss = 0.62, predict loss = 0.14 (75.5 examples/sec; 0.053 sec/batch; 87h:33m:10s remains)
INFO - root - 2019-11-04 00:04:38.392636: step 46860, total loss = 0.58, predict loss = 0.14 (69.4 examples/sec; 0.058 sec/batch; 95h:14m:49s remains)
INFO - root - 2019-11-04 00:04:39.047593: step 46870, total loss = 0.63, predict loss = 0.15 (71.5 examples/sec; 0.056 sec/batch; 92h:31m:00s remains)
INFO - root - 2019-11-04 00:04:39.653647: step 46880, total loss = 0.69, predict loss = 0.17 (73.7 examples/sec; 0.054 sec/batch; 89h:41m:39s remains)
INFO - root - 2019-11-04 00:04:40.283800: step 46890, total loss = 0.66, predict loss = 0.16 (67.4 examples/sec; 0.059 sec/batch; 98h:09m:45s remains)
INFO - root - 2019-11-04 00:04:40.895853: step 46900, total loss = 0.65, predict loss = 0.16 (77.0 examples/sec; 0.052 sec/batch; 85h:52m:39s remains)
INFO - root - 2019-11-04 00:04:41.516106: step 46910, total loss = 0.60, predict loss = 0.14 (77.6 examples/sec; 0.052 sec/batch; 85h:13m:34s remains)
INFO - root - 2019-11-04 00:04:42.149069: step 46920, total loss = 0.46, predict loss = 0.11 (60.8 examples/sec; 0.066 sec/batch; 108h:49m:47s remains)
INFO - root - 2019-11-04 00:04:42.804015: step 46930, total loss = 0.59, predict loss = 0.14 (67.5 examples/sec; 0.059 sec/batch; 98h:00m:43s remains)
INFO - root - 2019-11-04 00:04:43.462668: step 46940, total loss = 0.45, predict loss = 0.10 (66.2 examples/sec; 0.060 sec/batch; 99h:56m:09s remains)
INFO - root - 2019-11-04 00:04:44.182187: step 46950, total loss = 0.62, predict loss = 0.15 (56.7 examples/sec; 0.071 sec/batch; 116h:45m:30s remains)
INFO - root - 2019-11-04 00:04:44.827092: step 46960, total loss = 0.42, predict loss = 0.09 (70.4 examples/sec; 0.057 sec/batch; 93h:57m:04s remains)
INFO - root - 2019-11-04 00:04:45.441503: step 46970, total loss = 0.49, predict loss = 0.10 (67.5 examples/sec; 0.059 sec/batch; 98h:02m:12s remains)
INFO - root - 2019-11-04 00:04:46.081743: step 46980, total loss = 0.62, predict loss = 0.14 (75.0 examples/sec; 0.053 sec/batch; 88h:09m:06s remains)
INFO - root - 2019-11-04 00:04:46.773664: step 46990, total loss = 0.49, predict loss = 0.11 (65.4 examples/sec; 0.061 sec/batch; 101h:07m:55s remains)
INFO - root - 2019-11-04 00:04:47.413869: step 47000, total loss = 0.44, predict loss = 0.08 (68.9 examples/sec; 0.058 sec/batch; 95h:57m:50s remains)
INFO - root - 2019-11-04 00:04:48.041608: step 47010, total loss = 0.45, predict loss = 0.09 (64.9 examples/sec; 0.062 sec/batch; 101h:53m:20s remains)
INFO - root - 2019-11-04 00:04:48.708041: step 47020, total loss = 0.60, predict loss = 0.14 (59.7 examples/sec; 0.067 sec/batch; 110h:48m:59s remains)
INFO - root - 2019-11-04 00:04:49.366246: step 47030, total loss = 0.52, predict loss = 0.11 (69.6 examples/sec; 0.057 sec/batch; 95h:00m:33s remains)
INFO - root - 2019-11-04 00:04:50.005957: step 47040, total loss = 0.73, predict loss = 0.18 (76.1 examples/sec; 0.053 sec/batch; 86h:53m:24s remains)
INFO - root - 2019-11-04 00:04:50.622367: step 47050, total loss = 0.67, predict loss = 0.16 (76.4 examples/sec; 0.052 sec/batch; 86h:37m:47s remains)
INFO - root - 2019-11-04 00:04:51.263866: step 47060, total loss = 0.67, predict loss = 0.16 (70.3 examples/sec; 0.057 sec/batch; 94h:05m:42s remains)
INFO - root - 2019-11-04 00:04:51.899521: step 47070, total loss = 0.76, predict loss = 0.19 (73.4 examples/sec; 0.055 sec/batch; 90h:08m:23s remains)
INFO - root - 2019-11-04 00:04:52.531432: step 47080, total loss = 0.54, predict loss = 0.12 (77.7 examples/sec; 0.051 sec/batch; 85h:06m:24s remains)
INFO - root - 2019-11-04 00:04:53.112949: step 47090, total loss = 0.73, predict loss = 0.18 (76.4 examples/sec; 0.052 sec/batch; 86h:34m:39s remains)
INFO - root - 2019-11-04 00:04:53.749224: step 47100, total loss = 0.71, predict loss = 0.16 (68.7 examples/sec; 0.058 sec/batch; 96h:13m:14s remains)
INFO - root - 2019-11-04 00:04:54.383782: step 47110, total loss = 0.66, predict loss = 0.14 (70.0 examples/sec; 0.057 sec/batch; 94h:32m:16s remains)
INFO - root - 2019-11-04 00:04:55.036073: step 47120, total loss = 0.61, predict loss = 0.14 (66.6 examples/sec; 0.060 sec/batch; 99h:18m:55s remains)
INFO - root - 2019-11-04 00:04:55.653103: step 47130, total loss = 0.58, predict loss = 0.13 (78.7 examples/sec; 0.051 sec/batch; 84h:00m:10s remains)
INFO - root - 2019-11-04 00:04:56.343354: step 47140, total loss = 0.54, predict loss = 0.12 (61.7 examples/sec; 0.065 sec/batch; 107h:13m:12s remains)
INFO - root - 2019-11-04 00:04:57.038430: step 47150, total loss = 0.72, predict loss = 0.16 (65.3 examples/sec; 0.061 sec/batch; 101h:14m:55s remains)
INFO - root - 2019-11-04 00:04:57.690145: step 47160, total loss = 0.56, predict loss = 0.12 (70.9 examples/sec; 0.056 sec/batch; 93h:17m:46s remains)
INFO - root - 2019-11-04 00:04:58.316463: step 47170, total loss = 0.59, predict loss = 0.14 (79.1 examples/sec; 0.051 sec/batch; 83h:39m:02s remains)
INFO - root - 2019-11-04 00:04:58.947606: step 47180, total loss = 0.66, predict loss = 0.15 (66.9 examples/sec; 0.060 sec/batch; 98h:53m:22s remains)
INFO - root - 2019-11-04 00:04:59.572425: step 47190, total loss = 0.30, predict loss = 0.06 (64.3 examples/sec; 0.062 sec/batch; 102h:55m:32s remains)
INFO - root - 2019-11-04 00:05:00.201364: step 47200, total loss = 0.40, predict loss = 0.09 (75.7 examples/sec; 0.053 sec/batch; 87h:19m:01s remains)
INFO - root - 2019-11-04 00:05:00.832174: step 47210, total loss = 0.42, predict loss = 0.09 (63.7 examples/sec; 0.063 sec/batch; 103h:50m:54s remains)
INFO - root - 2019-11-04 00:05:01.512406: step 47220, total loss = 0.27, predict loss = 0.05 (64.3 examples/sec; 0.062 sec/batch; 102h:52m:31s remains)
INFO - root - 2019-11-04 00:05:02.151962: step 47230, total loss = 0.31, predict loss = 0.07 (66.3 examples/sec; 0.060 sec/batch; 99h:42m:19s remains)
INFO - root - 2019-11-04 00:05:02.797124: step 47240, total loss = 0.27, predict loss = 0.06 (59.9 examples/sec; 0.067 sec/batch; 110h:27m:39s remains)
INFO - root - 2019-11-04 00:05:03.439369: step 47250, total loss = 0.45, predict loss = 0.11 (74.5 examples/sec; 0.054 sec/batch; 88h:43m:58s remains)
INFO - root - 2019-11-04 00:05:04.079908: step 47260, total loss = 0.55, predict loss = 0.12 (74.3 examples/sec; 0.054 sec/batch; 89h:00m:34s remains)
INFO - root - 2019-11-04 00:05:04.703550: step 47270, total loss = 0.51, predict loss = 0.11 (69.9 examples/sec; 0.057 sec/batch; 94h:39m:11s remains)
INFO - root - 2019-11-04 00:05:05.317843: step 47280, total loss = 0.48, predict loss = 0.11 (71.3 examples/sec; 0.056 sec/batch; 92h:43m:15s remains)
INFO - root - 2019-11-04 00:05:05.964731: step 47290, total loss = 0.52, predict loss = 0.11 (63.6 examples/sec; 0.063 sec/batch; 104h:01m:19s remains)
INFO - root - 2019-11-04 00:05:06.602747: step 47300, total loss = 0.59, predict loss = 0.13 (76.6 examples/sec; 0.052 sec/batch; 86h:21m:20s remains)
INFO - root - 2019-11-04 00:05:07.257867: step 47310, total loss = 0.41, predict loss = 0.10 (69.5 examples/sec; 0.058 sec/batch; 95h:09m:10s remains)
INFO - root - 2019-11-04 00:05:07.877719: step 47320, total loss = 0.37, predict loss = 0.08 (77.5 examples/sec; 0.052 sec/batch; 85h:17m:48s remains)
INFO - root - 2019-11-04 00:05:08.478158: step 47330, total loss = 0.58, predict loss = 0.13 (79.2 examples/sec; 0.051 sec/batch; 83h:31m:38s remains)
INFO - root - 2019-11-04 00:05:09.132632: step 47340, total loss = 0.50, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 93h:51m:51s remains)
INFO - root - 2019-11-04 00:05:09.767995: step 47350, total loss = 0.50, predict loss = 0.11 (75.9 examples/sec; 0.053 sec/batch; 87h:06m:08s remains)
INFO - root - 2019-11-04 00:05:10.411292: step 47360, total loss = 0.47, predict loss = 0.10 (69.8 examples/sec; 0.057 sec/batch; 94h:46m:21s remains)
INFO - root - 2019-11-04 00:05:11.023997: step 47370, total loss = 0.75, predict loss = 0.18 (70.6 examples/sec; 0.057 sec/batch; 93h:42m:54s remains)
INFO - root - 2019-11-04 00:05:11.623494: step 47380, total loss = 0.68, predict loss = 0.15 (73.6 examples/sec; 0.054 sec/batch; 89h:55m:17s remains)
INFO - root - 2019-11-04 00:05:12.259546: step 47390, total loss = 0.61, predict loss = 0.14 (62.9 examples/sec; 0.064 sec/batch; 105h:13m:49s remains)
INFO - root - 2019-11-04 00:05:12.883253: step 47400, total loss = 0.63, predict loss = 0.16 (63.7 examples/sec; 0.063 sec/batch; 103h:47m:51s remains)
INFO - root - 2019-11-04 00:05:13.519514: step 47410, total loss = 0.52, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 90h:39m:40s remains)
INFO - root - 2019-11-04 00:05:14.182398: step 47420, total loss = 0.41, predict loss = 0.10 (57.9 examples/sec; 0.069 sec/batch; 114h:15m:36s remains)
INFO - root - 2019-11-04 00:05:14.775006: step 47430, total loss = 0.32, predict loss = 0.07 (69.6 examples/sec; 0.057 sec/batch; 94h:59m:01s remains)
INFO - root - 2019-11-04 00:05:15.379854: step 47440, total loss = 0.30, predict loss = 0.06 (66.9 examples/sec; 0.060 sec/batch; 98h:52m:36s remains)
INFO - root - 2019-11-04 00:05:15.978393: step 47450, total loss = 0.42, predict loss = 0.10 (73.4 examples/sec; 0.054 sec/batch; 90h:04m:24s remains)
INFO - root - 2019-11-04 00:05:16.598917: step 47460, total loss = 0.37, predict loss = 0.08 (67.3 examples/sec; 0.059 sec/batch; 98h:15m:11s remains)
INFO - root - 2019-11-04 00:05:17.256290: step 47470, total loss = 0.40, predict loss = 0.09 (66.9 examples/sec; 0.060 sec/batch; 98h:48m:11s remains)
INFO - root - 2019-11-04 00:05:17.906853: step 47480, total loss = 0.47, predict loss = 0.11 (64.8 examples/sec; 0.062 sec/batch; 102h:03m:31s remains)
INFO - root - 2019-11-04 00:05:18.550712: step 47490, total loss = 0.34, predict loss = 0.07 (64.8 examples/sec; 0.062 sec/batch; 102h:08m:04s remains)
INFO - root - 2019-11-04 00:05:19.183881: step 47500, total loss = 0.59, predict loss = 0.14 (73.6 examples/sec; 0.054 sec/batch; 89h:49m:06s remains)
INFO - root - 2019-11-04 00:05:19.802288: step 47510, total loss = 0.43, predict loss = 0.09 (68.9 examples/sec; 0.058 sec/batch; 95h:58m:47s remains)
INFO - root - 2019-11-04 00:05:20.418283: step 47520, total loss = 0.65, predict loss = 0.16 (76.0 examples/sec; 0.053 sec/batch; 86h:58m:25s remains)
INFO - root - 2019-11-04 00:05:21.026206: step 47530, total loss = 0.58, predict loss = 0.13 (71.3 examples/sec; 0.056 sec/batch; 92h:44m:46s remains)
INFO - root - 2019-11-04 00:05:21.684068: step 47540, total loss = 0.69, predict loss = 0.17 (61.6 examples/sec; 0.065 sec/batch; 107h:23m:23s remains)
INFO - root - 2019-11-04 00:05:22.345084: step 47550, total loss = 0.61, predict loss = 0.15 (64.8 examples/sec; 0.062 sec/batch; 102h:05m:19s remains)
INFO - root - 2019-11-04 00:05:23.001087: step 47560, total loss = 0.60, predict loss = 0.13 (76.7 examples/sec; 0.052 sec/batch; 86h:14m:02s remains)
INFO - root - 2019-11-04 00:05:23.681999: step 47570, total loss = 0.54, predict loss = 0.13 (64.2 examples/sec; 0.062 sec/batch; 102h:59m:02s remains)
INFO - root - 2019-11-04 00:05:24.367002: step 47580, total loss = 0.71, predict loss = 0.16 (67.0 examples/sec; 0.060 sec/batch; 98h:42m:56s remains)
INFO - root - 2019-11-04 00:05:24.967173: step 47590, total loss = 0.69, predict loss = 0.16 (78.6 examples/sec; 0.051 sec/batch; 84h:07m:28s remains)
INFO - root - 2019-11-04 00:05:25.602766: step 47600, total loss = 0.55, predict loss = 0.13 (64.0 examples/sec; 0.062 sec/batch; 103h:19m:28s remains)
INFO - root - 2019-11-04 00:05:26.233411: step 47610, total loss = 0.68, predict loss = 0.17 (67.5 examples/sec; 0.059 sec/batch; 97h:59m:15s remains)
INFO - root - 2019-11-04 00:05:26.863039: step 47620, total loss = 0.52, predict loss = 0.11 (79.4 examples/sec; 0.050 sec/batch; 83h:18m:32s remains)
INFO - root - 2019-11-04 00:05:27.497812: step 47630, total loss = 0.52, predict loss = 0.12 (70.4 examples/sec; 0.057 sec/batch; 93h:54m:41s remains)
INFO - root - 2019-11-04 00:05:28.154290: step 47640, total loss = 0.66, predict loss = 0.15 (76.5 examples/sec; 0.052 sec/batch; 86h:29m:57s remains)
INFO - root - 2019-11-04 00:05:28.832479: step 47650, total loss = 0.56, predict loss = 0.12 (76.3 examples/sec; 0.052 sec/batch; 86h:41m:45s remains)
INFO - root - 2019-11-04 00:05:29.434069: step 47660, total loss = 0.56, predict loss = 0.13 (66.4 examples/sec; 0.060 sec/batch; 99h:37m:59s remains)
INFO - root - 2019-11-04 00:05:30.072679: step 47670, total loss = 0.74, predict loss = 0.15 (64.5 examples/sec; 0.062 sec/batch; 102h:32m:50s remains)
INFO - root - 2019-11-04 00:05:30.714195: step 47680, total loss = 0.70, predict loss = 0.17 (65.5 examples/sec; 0.061 sec/batch; 100h:58m:25s remains)
INFO - root - 2019-11-04 00:05:31.312819: step 47690, total loss = 0.57, predict loss = 0.12 (72.4 examples/sec; 0.055 sec/batch; 91h:24m:33s remains)
INFO - root - 2019-11-04 00:05:31.945084: step 47700, total loss = 0.47, predict loss = 0.11 (61.5 examples/sec; 0.065 sec/batch; 107h:32m:57s remains)
INFO - root - 2019-11-04 00:05:32.664842: step 47710, total loss = 0.52, predict loss = 0.12 (59.6 examples/sec; 0.067 sec/batch; 110h:56m:48s remains)
INFO - root - 2019-11-04 00:05:33.319156: step 47720, total loss = 0.34, predict loss = 0.07 (69.1 examples/sec; 0.058 sec/batch; 95h:46m:12s remains)
INFO - root - 2019-11-04 00:05:33.943875: step 47730, total loss = 0.43, predict loss = 0.10 (66.9 examples/sec; 0.060 sec/batch; 98h:47m:08s remains)
INFO - root - 2019-11-04 00:05:34.607388: step 47740, total loss = 0.61, predict loss = 0.14 (58.0 examples/sec; 0.069 sec/batch; 114h:06m:49s remains)
INFO - root - 2019-11-04 00:05:35.259027: step 47750, total loss = 0.47, predict loss = 0.12 (79.3 examples/sec; 0.050 sec/batch; 83h:22m:39s remains)
INFO - root - 2019-11-04 00:05:35.887288: step 47760, total loss = 0.59, predict loss = 0.14 (64.7 examples/sec; 0.062 sec/batch; 102h:12m:06s remains)
INFO - root - 2019-11-04 00:05:36.542644: step 47770, total loss = 0.65, predict loss = 0.16 (70.4 examples/sec; 0.057 sec/batch; 93h:57m:24s remains)
INFO - root - 2019-11-04 00:05:37.187412: step 47780, total loss = 0.66, predict loss = 0.16 (64.8 examples/sec; 0.062 sec/batch; 102h:04m:59s remains)
INFO - root - 2019-11-04 00:05:37.842728: step 47790, total loss = 0.94, predict loss = 0.23 (59.3 examples/sec; 0.067 sec/batch; 111h:35m:15s remains)
INFO - root - 2019-11-04 00:05:38.516828: step 47800, total loss = 0.90, predict loss = 0.22 (60.9 examples/sec; 0.066 sec/batch; 108h:39m:27s remains)
INFO - root - 2019-11-04 00:05:39.171578: step 47810, total loss = 0.76, predict loss = 0.19 (65.9 examples/sec; 0.061 sec/batch; 100h:22m:00s remains)
INFO - root - 2019-11-04 00:05:39.824214: step 47820, total loss = 1.16, predict loss = 0.26 (73.4 examples/sec; 0.055 sec/batch; 90h:09m:40s remains)
INFO - root - 2019-11-04 00:05:40.463081: step 47830, total loss = 0.68, predict loss = 0.16 (64.6 examples/sec; 0.062 sec/batch; 102h:21m:46s remains)
INFO - root - 2019-11-04 00:05:41.097274: step 47840, total loss = 0.89, predict loss = 0.21 (71.9 examples/sec; 0.056 sec/batch; 91h:59m:21s remains)
INFO - root - 2019-11-04 00:05:41.748317: step 47850, total loss = 0.65, predict loss = 0.16 (67.4 examples/sec; 0.059 sec/batch; 98h:07m:57s remains)
INFO - root - 2019-11-04 00:05:42.412672: step 47860, total loss = 0.84, predict loss = 0.20 (64.2 examples/sec; 0.062 sec/batch; 103h:05m:14s remains)
INFO - root - 2019-11-04 00:05:43.072654: step 47870, total loss = 0.66, predict loss = 0.15 (74.6 examples/sec; 0.054 sec/batch; 88h:40m:57s remains)
INFO - root - 2019-11-04 00:05:43.723023: step 47880, total loss = 0.96, predict loss = 0.23 (63.8 examples/sec; 0.063 sec/batch; 103h:34m:51s remains)
INFO - root - 2019-11-04 00:05:44.419946: step 47890, total loss = 0.72, predict loss = 0.16 (66.7 examples/sec; 0.060 sec/batch; 99h:06m:44s remains)
INFO - root - 2019-11-04 00:05:45.097023: step 47900, total loss = 0.56, predict loss = 0.12 (62.4 examples/sec; 0.064 sec/batch; 105h:59m:52s remains)
INFO - root - 2019-11-04 00:05:45.681533: step 47910, total loss = 0.60, predict loss = 0.14 (87.5 examples/sec; 0.046 sec/batch; 75h:32m:21s remains)
INFO - root - 2019-11-04 00:05:46.325595: step 47920, total loss = 0.60, predict loss = 0.14 (66.8 examples/sec; 0.060 sec/batch; 98h:57m:57s remains)
INFO - root - 2019-11-04 00:05:46.935706: step 47930, total loss = 0.53, predict loss = 0.12 (67.7 examples/sec; 0.059 sec/batch; 97h:40m:30s remains)
INFO - root - 2019-11-04 00:05:47.583812: step 47940, total loss = 0.50, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 96h:06m:27s remains)
INFO - root - 2019-11-04 00:05:48.247044: step 47950, total loss = 0.68, predict loss = 0.16 (73.6 examples/sec; 0.054 sec/batch; 89h:53m:09s remains)
INFO - root - 2019-11-04 00:05:48.871571: step 47960, total loss = 0.51, predict loss = 0.12 (75.2 examples/sec; 0.053 sec/batch; 87h:56m:20s remains)
INFO - root - 2019-11-04 00:05:49.493652: step 47970, total loss = 0.47, predict loss = 0.10 (62.1 examples/sec; 0.064 sec/batch; 106h:34m:06s remains)
INFO - root - 2019-11-04 00:05:50.163762: step 47980, total loss = 0.56, predict loss = 0.12 (61.5 examples/sec; 0.065 sec/batch; 107h:27m:31s remains)
INFO - root - 2019-11-04 00:05:50.789612: step 47990, total loss = 0.79, predict loss = 0.18 (69.2 examples/sec; 0.058 sec/batch; 95h:33m:22s remains)
INFO - root - 2019-11-04 00:05:51.428506: step 48000, total loss = 0.69, predict loss = 0.15 (66.9 examples/sec; 0.060 sec/batch; 98h:50m:43s remains)
INFO - root - 2019-11-04 00:05:52.067973: step 48010, total loss = 0.52, predict loss = 0.12 (62.2 examples/sec; 0.064 sec/batch; 106h:21m:05s remains)
INFO - root - 2019-11-04 00:05:52.698294: step 48020, total loss = 0.65, predict loss = 0.15 (78.0 examples/sec; 0.051 sec/batch; 84h:45m:39s remains)
INFO - root - 2019-11-04 00:05:53.427849: step 48030, total loss = 0.44, predict loss = 0.10 (64.8 examples/sec; 0.062 sec/batch; 102h:05m:09s remains)
INFO - root - 2019-11-04 00:05:54.039385: step 48040, total loss = 0.41, predict loss = 0.09 (72.8 examples/sec; 0.055 sec/batch; 90h:47m:07s remains)
INFO - root - 2019-11-04 00:05:54.652120: step 48050, total loss = 0.40, predict loss = 0.09 (71.4 examples/sec; 0.056 sec/batch; 92h:40m:59s remains)
INFO - root - 2019-11-04 00:05:55.302627: step 48060, total loss = 0.36, predict loss = 0.08 (75.7 examples/sec; 0.053 sec/batch; 87h:21m:39s remains)
INFO - root - 2019-11-04 00:05:55.954177: step 48070, total loss = 0.63, predict loss = 0.15 (63.0 examples/sec; 0.063 sec/batch; 104h:57m:56s remains)
INFO - root - 2019-11-04 00:05:56.608244: step 48080, total loss = 0.58, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 101h:11m:53s remains)
INFO - root - 2019-11-04 00:05:57.280165: step 48090, total loss = 0.46, predict loss = 0.10 (59.8 examples/sec; 0.067 sec/batch; 110h:33m:25s remains)
INFO - root - 2019-11-04 00:05:57.922201: step 48100, total loss = 0.47, predict loss = 0.10 (76.0 examples/sec; 0.053 sec/batch; 87h:03m:55s remains)
INFO - root - 2019-11-04 00:05:58.530342: step 48110, total loss = 0.49, predict loss = 0.11 (81.4 examples/sec; 0.049 sec/batch; 81h:15m:09s remains)
INFO - root - 2019-11-04 00:05:59.123512: step 48120, total loss = 0.41, predict loss = 0.09 (75.6 examples/sec; 0.053 sec/batch; 87h:26m:48s remains)
INFO - root - 2019-11-04 00:05:59.718250: step 48130, total loss = 0.51, predict loss = 0.11 (81.4 examples/sec; 0.049 sec/batch; 81h:16m:38s remains)
INFO - root - 2019-11-04 00:06:00.313953: step 48140, total loss = 0.59, predict loss = 0.14 (85.1 examples/sec; 0.047 sec/batch; 77h:41m:41s remains)
INFO - root - 2019-11-04 00:06:00.930529: step 48150, total loss = 0.72, predict loss = 0.17 (69.1 examples/sec; 0.058 sec/batch; 95h:42m:02s remains)
INFO - root - 2019-11-04 00:06:01.562621: step 48160, total loss = 0.65, predict loss = 0.14 (76.7 examples/sec; 0.052 sec/batch; 86h:13m:24s remains)
INFO - root - 2019-11-04 00:06:02.216714: step 48170, total loss = 0.55, predict loss = 0.13 (65.6 examples/sec; 0.061 sec/batch; 100h:47m:05s remains)
INFO - root - 2019-11-04 00:06:03.326322: step 48180, total loss = 0.56, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 96h:06m:08s remains)
INFO - root - 2019-11-04 00:06:04.018370: step 48190, total loss = 0.52, predict loss = 0.11 (65.6 examples/sec; 0.061 sec/batch; 100h:47m:09s remains)
INFO - root - 2019-11-04 00:06:04.637838: step 48200, total loss = 0.53, predict loss = 0.12 (78.1 examples/sec; 0.051 sec/batch; 84h:37m:39s remains)
INFO - root - 2019-11-04 00:06:05.258755: step 48210, total loss = 0.53, predict loss = 0.12 (76.7 examples/sec; 0.052 sec/batch; 86h:12m:19s remains)
INFO - root - 2019-11-04 00:06:05.924316: step 48220, total loss = 0.56, predict loss = 0.13 (61.0 examples/sec; 0.066 sec/batch; 108h:23m:47s remains)
INFO - root - 2019-11-04 00:06:06.578646: step 48230, total loss = 0.71, predict loss = 0.17 (65.8 examples/sec; 0.061 sec/batch; 100h:34m:38s remains)
INFO - root - 2019-11-04 00:06:07.202061: step 48240, total loss = 0.69, predict loss = 0.16 (65.5 examples/sec; 0.061 sec/batch; 100h:59m:33s remains)
INFO - root - 2019-11-04 00:06:07.843515: step 48250, total loss = 0.69, predict loss = 0.16 (68.0 examples/sec; 0.059 sec/batch; 97h:16m:42s remains)
INFO - root - 2019-11-04 00:06:08.468517: step 48260, total loss = 0.79, predict loss = 0.19 (66.3 examples/sec; 0.060 sec/batch; 99h:49m:00s remains)
INFO - root - 2019-11-04 00:06:09.154496: step 48270, total loss = 0.69, predict loss = 0.17 (61.1 examples/sec; 0.065 sec/batch; 108h:08m:56s remains)
INFO - root - 2019-11-04 00:06:09.804991: step 48280, total loss = 0.65, predict loss = 0.16 (70.3 examples/sec; 0.057 sec/batch; 94h:00m:25s remains)
INFO - root - 2019-11-04 00:06:10.445273: step 48290, total loss = 0.75, predict loss = 0.17 (68.7 examples/sec; 0.058 sec/batch; 96h:19m:00s remains)
INFO - root - 2019-11-04 00:06:11.089001: step 48300, total loss = 0.55, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 96h:02m:01s remains)
INFO - root - 2019-11-04 00:06:11.786502: step 48310, total loss = 0.62, predict loss = 0.15 (64.5 examples/sec; 0.062 sec/batch; 102h:34m:45s remains)
INFO - root - 2019-11-04 00:06:12.481724: step 48320, total loss = 0.54, predict loss = 0.12 (64.6 examples/sec; 0.062 sec/batch; 102h:19m:54s remains)
INFO - root - 2019-11-04 00:06:13.141821: step 48330, total loss = 0.64, predict loss = 0.16 (66.0 examples/sec; 0.061 sec/batch; 100h:12m:47s remains)
INFO - root - 2019-11-04 00:06:13.755932: step 48340, total loss = 0.39, predict loss = 0.09 (69.2 examples/sec; 0.058 sec/batch; 95h:37m:48s remains)
INFO - root - 2019-11-04 00:06:14.425364: step 48350, total loss = 0.51, predict loss = 0.12 (74.8 examples/sec; 0.053 sec/batch; 88h:22m:13s remains)
INFO - root - 2019-11-04 00:06:15.083513: step 48360, total loss = 0.49, predict loss = 0.11 (62.0 examples/sec; 0.064 sec/batch; 106h:37m:31s remains)
INFO - root - 2019-11-04 00:06:15.701677: step 48370, total loss = 0.44, predict loss = 0.10 (83.0 examples/sec; 0.048 sec/batch; 79h:42m:34s remains)
INFO - root - 2019-11-04 00:06:16.351872: step 48380, total loss = 0.48, predict loss = 0.10 (66.1 examples/sec; 0.060 sec/batch; 100h:00m:02s remains)
INFO - root - 2019-11-04 00:06:16.994865: step 48390, total loss = 0.46, predict loss = 0.10 (70.8 examples/sec; 0.057 sec/batch; 93h:25m:19s remains)
INFO - root - 2019-11-04 00:06:17.639388: step 48400, total loss = 0.54, predict loss = 0.13 (62.3 examples/sec; 0.064 sec/batch; 106h:06m:08s remains)
INFO - root - 2019-11-04 00:06:18.298573: step 48410, total loss = 0.52, predict loss = 0.11 (61.7 examples/sec; 0.065 sec/batch; 107h:11m:10s remains)
INFO - root - 2019-11-04 00:06:18.954275: step 48420, total loss = 0.56, predict loss = 0.13 (64.1 examples/sec; 0.062 sec/batch; 103h:06m:12s remains)
INFO - root - 2019-11-04 00:06:19.701908: step 48430, total loss = 0.66, predict loss = 0.15 (57.9 examples/sec; 0.069 sec/batch; 114h:13m:40s remains)
INFO - root - 2019-11-04 00:06:20.369861: step 48440, total loss = 0.58, predict loss = 0.14 (67.6 examples/sec; 0.059 sec/batch; 97h:45m:59s remains)
INFO - root - 2019-11-04 00:06:21.019104: step 48450, total loss = 0.67, predict loss = 0.16 (66.9 examples/sec; 0.060 sec/batch; 98h:50m:35s remains)
INFO - root - 2019-11-04 00:06:21.661994: step 48460, total loss = 0.81, predict loss = 0.20 (68.3 examples/sec; 0.059 sec/batch; 96h:45m:27s remains)
INFO - root - 2019-11-04 00:06:22.305344: step 48470, total loss = 0.72, predict loss = 0.17 (69.0 examples/sec; 0.058 sec/batch; 95h:52m:15s remains)
INFO - root - 2019-11-04 00:06:22.952922: step 48480, total loss = 0.57, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 94h:06m:51s remains)
INFO - root - 2019-11-04 00:06:23.585387: step 48490, total loss = 0.65, predict loss = 0.16 (81.6 examples/sec; 0.049 sec/batch; 81h:01m:13s remains)
INFO - root - 2019-11-04 00:06:24.208610: step 48500, total loss = 0.54, predict loss = 0.13 (70.6 examples/sec; 0.057 sec/batch; 93h:40m:59s remains)
INFO - root - 2019-11-04 00:06:24.853798: step 48510, total loss = 0.64, predict loss = 0.16 (68.2 examples/sec; 0.059 sec/batch; 96h:54m:58s remains)
INFO - root - 2019-11-04 00:06:25.536850: step 48520, total loss = 0.58, predict loss = 0.14 (67.3 examples/sec; 0.059 sec/batch; 98h:17m:34s remains)
INFO - root - 2019-11-04 00:06:26.198172: step 48530, total loss = 0.69, predict loss = 0.16 (64.8 examples/sec; 0.062 sec/batch; 102h:02m:33s remains)
INFO - root - 2019-11-04 00:06:26.889723: step 48540, total loss = 0.60, predict loss = 0.14 (61.2 examples/sec; 0.065 sec/batch; 108h:03m:51s remains)
INFO - root - 2019-11-04 00:06:27.561413: step 48550, total loss = 0.52, predict loss = 0.12 (60.8 examples/sec; 0.066 sec/batch; 108h:43m:57s remains)
INFO - root - 2019-11-04 00:06:28.215898: step 48560, total loss = 0.55, predict loss = 0.12 (68.5 examples/sec; 0.058 sec/batch; 96h:29m:24s remains)
INFO - root - 2019-11-04 00:06:28.884543: step 48570, total loss = 0.61, predict loss = 0.15 (74.1 examples/sec; 0.054 sec/batch; 89h:15m:05s remains)
INFO - root - 2019-11-04 00:06:29.537378: step 48580, total loss = 0.45, predict loss = 0.10 (64.7 examples/sec; 0.062 sec/batch; 102h:11m:25s remains)
INFO - root - 2019-11-04 00:06:30.180173: step 48590, total loss = 0.62, predict loss = 0.15 (66.7 examples/sec; 0.060 sec/batch; 99h:05m:46s remains)
INFO - root - 2019-11-04 00:06:30.834083: step 48600, total loss = 0.59, predict loss = 0.13 (73.7 examples/sec; 0.054 sec/batch; 89h:40m:23s remains)
INFO - root - 2019-11-04 00:06:31.528825: step 48610, total loss = 0.48, predict loss = 0.11 (64.1 examples/sec; 0.062 sec/batch; 103h:14m:23s remains)
INFO - root - 2019-11-04 00:06:32.201785: step 48620, total loss = 0.49, predict loss = 0.11 (67.3 examples/sec; 0.059 sec/batch; 98h:12m:36s remains)
INFO - root - 2019-11-04 00:06:32.845107: step 48630, total loss = 0.70, predict loss = 0.16 (62.5 examples/sec; 0.064 sec/batch; 105h:49m:35s remains)
INFO - root - 2019-11-04 00:06:33.512743: step 48640, total loss = 0.49, predict loss = 0.11 (63.7 examples/sec; 0.063 sec/batch; 103h:51m:25s remains)
INFO - root - 2019-11-04 00:06:34.152977: step 48650, total loss = 0.48, predict loss = 0.11 (64.3 examples/sec; 0.062 sec/batch; 102h:46m:02s remains)
INFO - root - 2019-11-04 00:06:34.788658: step 48660, total loss = 0.56, predict loss = 0.12 (60.4 examples/sec; 0.066 sec/batch; 109h:23m:51s remains)
INFO - root - 2019-11-04 00:06:35.421621: step 48670, total loss = 0.63, predict loss = 0.15 (63.8 examples/sec; 0.063 sec/batch; 103h:36m:16s remains)
INFO - root - 2019-11-04 00:06:36.094518: step 48680, total loss = 0.59, predict loss = 0.13 (67.1 examples/sec; 0.060 sec/batch; 98h:32m:36s remains)
INFO - root - 2019-11-04 00:06:36.726131: step 48690, total loss = 0.50, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 104h:10m:28s remains)
INFO - root - 2019-11-04 00:06:37.335549: step 48700, total loss = 0.53, predict loss = 0.12 (80.5 examples/sec; 0.050 sec/batch; 82h:07m:27s remains)
INFO - root - 2019-11-04 00:06:37.977337: step 48710, total loss = 0.59, predict loss = 0.14 (68.9 examples/sec; 0.058 sec/batch; 95h:58m:42s remains)
INFO - root - 2019-11-04 00:06:38.613413: step 48720, total loss = 0.51, predict loss = 0.11 (71.4 examples/sec; 0.056 sec/batch; 92h:37m:50s remains)
INFO - root - 2019-11-04 00:06:39.235704: step 48730, total loss = 0.62, predict loss = 0.15 (67.5 examples/sec; 0.059 sec/batch; 97h:54m:13s remains)
INFO - root - 2019-11-04 00:06:39.864503: step 48740, total loss = 0.50, predict loss = 0.12 (75.5 examples/sec; 0.053 sec/batch; 87h:36m:19s remains)
INFO - root - 2019-11-04 00:06:40.510442: step 48750, total loss = 0.55, predict loss = 0.12 (77.1 examples/sec; 0.052 sec/batch; 85h:43m:38s remains)
INFO - root - 2019-11-04 00:06:41.164434: step 48760, total loss = 0.53, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 98h:43m:39s remains)
INFO - root - 2019-11-04 00:06:41.770534: step 48770, total loss = 0.60, predict loss = 0.14 (86.0 examples/sec; 0.047 sec/batch; 76h:53m:07s remains)
INFO - root - 2019-11-04 00:06:42.376116: step 48780, total loss = 0.58, predict loss = 0.13 (87.8 examples/sec; 0.046 sec/batch; 75h:21m:13s remains)
INFO - root - 2019-11-04 00:06:43.001698: step 48790, total loss = 0.48, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 88h:09m:30s remains)
INFO - root - 2019-11-04 00:06:43.609076: step 48800, total loss = 0.67, predict loss = 0.16 (69.5 examples/sec; 0.058 sec/batch; 95h:05m:18s remains)
INFO - root - 2019-11-04 00:06:44.264317: step 48810, total loss = 0.44, predict loss = 0.10 (77.1 examples/sec; 0.052 sec/batch; 85h:44m:48s remains)
INFO - root - 2019-11-04 00:06:44.871042: step 48820, total loss = 0.68, predict loss = 0.16 (65.1 examples/sec; 0.061 sec/batch; 101h:30m:16s remains)
INFO - root - 2019-11-04 00:06:45.553470: step 48830, total loss = 0.52, predict loss = 0.12 (65.4 examples/sec; 0.061 sec/batch; 101h:04m:47s remains)
INFO - root - 2019-11-04 00:06:46.171040: step 48840, total loss = 0.90, predict loss = 0.23 (84.7 examples/sec; 0.047 sec/batch; 78h:03m:32s remains)
INFO - root - 2019-11-04 00:06:46.819009: step 48850, total loss = 0.73, predict loss = 0.18 (78.0 examples/sec; 0.051 sec/batch; 84h:43m:46s remains)
INFO - root - 2019-11-04 00:06:47.438000: step 48860, total loss = 0.71, predict loss = 0.17 (77.0 examples/sec; 0.052 sec/batch; 85h:55m:04s remains)
INFO - root - 2019-11-04 00:06:48.052721: step 48870, total loss = 0.67, predict loss = 0.16 (79.1 examples/sec; 0.051 sec/batch; 83h:33m:19s remains)
INFO - root - 2019-11-04 00:06:48.648127: step 48880, total loss = 0.56, predict loss = 0.12 (77.1 examples/sec; 0.052 sec/batch; 85h:48m:53s remains)
INFO - root - 2019-11-04 00:06:49.298271: step 48890, total loss = 0.69, predict loss = 0.17 (66.8 examples/sec; 0.060 sec/batch; 98h:55m:05s remains)
INFO - root - 2019-11-04 00:06:50.006771: step 48900, total loss = 0.81, predict loss = 0.19 (63.1 examples/sec; 0.063 sec/batch; 104h:49m:26s remains)
INFO - root - 2019-11-04 00:06:50.710623: step 48910, total loss = 0.71, predict loss = 0.16 (66.6 examples/sec; 0.060 sec/batch; 99h:14m:06s remains)
INFO - root - 2019-11-04 00:06:51.341269: step 48920, total loss = 0.61, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:32m:46s remains)
INFO - root - 2019-11-04 00:06:51.968767: step 48930, total loss = 0.80, predict loss = 0.18 (75.2 examples/sec; 0.053 sec/batch; 87h:54m:26s remains)
INFO - root - 2019-11-04 00:06:52.600279: step 48940, total loss = 0.64, predict loss = 0.14 (81.3 examples/sec; 0.049 sec/batch; 81h:22m:05s remains)
INFO - root - 2019-11-04 00:06:53.207591: step 48950, total loss = 0.58, predict loss = 0.14 (67.2 examples/sec; 0.060 sec/batch; 98h:27m:57s remains)
INFO - root - 2019-11-04 00:06:53.806769: step 48960, total loss = 0.49, predict loss = 0.11 (76.1 examples/sec; 0.053 sec/batch; 86h:53m:49s remains)
INFO - root - 2019-11-04 00:06:54.430822: step 48970, total loss = 0.46, predict loss = 0.10 (63.5 examples/sec; 0.063 sec/batch; 104h:03m:58s remains)
INFO - root - 2019-11-04 00:06:55.113744: step 48980, total loss = 0.77, predict loss = 0.18 (69.6 examples/sec; 0.057 sec/batch; 94h:58m:48s remains)
INFO - root - 2019-11-04 00:06:55.758183: step 48990, total loss = 0.68, predict loss = 0.16 (72.1 examples/sec; 0.055 sec/batch; 91h:39m:02s remains)
INFO - root - 2019-11-04 00:06:56.417511: step 49000, total loss = 0.65, predict loss = 0.16 (67.4 examples/sec; 0.059 sec/batch; 98h:07m:08s remains)
INFO - root - 2019-11-04 00:06:57.051693: step 49010, total loss = 0.59, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:27m:37s remains)
INFO - root - 2019-11-04 00:06:57.653389: step 49020, total loss = 0.69, predict loss = 0.16 (72.4 examples/sec; 0.055 sec/batch; 91h:18m:50s remains)
INFO - root - 2019-11-04 00:06:58.275715: step 49030, total loss = 0.69, predict loss = 0.17 (69.9 examples/sec; 0.057 sec/batch; 94h:32m:07s remains)
INFO - root - 2019-11-04 00:06:58.929328: step 49040, total loss = 0.68, predict loss = 0.16 (65.6 examples/sec; 0.061 sec/batch; 100h:51m:07s remains)
INFO - root - 2019-11-04 00:06:59.614530: step 49050, total loss = 0.72, predict loss = 0.18 (66.7 examples/sec; 0.060 sec/batch; 99h:06m:24s remains)
INFO - root - 2019-11-04 00:07:00.285540: step 49060, total loss = 0.56, predict loss = 0.12 (65.0 examples/sec; 0.062 sec/batch; 101h:47m:52s remains)
INFO - root - 2019-11-04 00:07:00.878548: step 49070, total loss = 0.55, predict loss = 0.13 (93.4 examples/sec; 0.043 sec/batch; 70h:48m:53s remains)
INFO - root - 2019-11-04 00:07:01.428481: step 49080, total loss = 0.54, predict loss = 0.12 (99.2 examples/sec; 0.040 sec/batch; 66h:38m:50s remains)
INFO - root - 2019-11-04 00:07:01.910592: step 49090, total loss = 0.45, predict loss = 0.10 (86.5 examples/sec; 0.046 sec/batch; 76h:25m:06s remains)
INFO - root - 2019-11-04 00:07:03.023517: step 49100, total loss = 0.41, predict loss = 0.09 (61.4 examples/sec; 0.065 sec/batch; 107h:43m:24s remains)
INFO - root - 2019-11-04 00:07:03.728923: step 49110, total loss = 0.67, predict loss = 0.15 (69.4 examples/sec; 0.058 sec/batch; 95h:17m:11s remains)
INFO - root - 2019-11-04 00:07:04.358671: step 49120, total loss = 0.64, predict loss = 0.14 (71.7 examples/sec; 0.056 sec/batch; 92h:14m:55s remains)
INFO - root - 2019-11-04 00:07:04.959296: step 49130, total loss = 0.62, predict loss = 0.16 (81.1 examples/sec; 0.049 sec/batch; 81h:34m:35s remains)
INFO - root - 2019-11-04 00:07:05.618982: step 49140, total loss = 0.60, predict loss = 0.14 (60.7 examples/sec; 0.066 sec/batch; 108h:53m:50s remains)
INFO - root - 2019-11-04 00:07:06.299539: step 49150, total loss = 0.59, predict loss = 0.14 (59.0 examples/sec; 0.068 sec/batch; 112h:06m:22s remains)
INFO - root - 2019-11-04 00:07:06.944091: step 49160, total loss = 0.79, predict loss = 0.17 (70.0 examples/sec; 0.057 sec/batch; 94h:29m:17s remains)
INFO - root - 2019-11-04 00:07:07.563921: step 49170, total loss = 0.55, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 91h:32m:13s remains)
INFO - root - 2019-11-04 00:07:08.201836: step 49180, total loss = 0.87, predict loss = 0.20 (79.6 examples/sec; 0.050 sec/batch; 83h:00m:53s remains)
INFO - root - 2019-11-04 00:07:08.840441: step 49190, total loss = 0.64, predict loss = 0.14 (68.3 examples/sec; 0.059 sec/batch; 96h:50m:41s remains)
INFO - root - 2019-11-04 00:07:09.460033: step 49200, total loss = 0.63, predict loss = 0.14 (79.6 examples/sec; 0.050 sec/batch; 83h:01m:41s remains)
INFO - root - 2019-11-04 00:07:10.077047: step 49210, total loss = 0.70, predict loss = 0.16 (71.7 examples/sec; 0.056 sec/batch; 92h:14m:06s remains)
INFO - root - 2019-11-04 00:07:10.728054: step 49220, total loss = 0.73, predict loss = 0.16 (63.6 examples/sec; 0.063 sec/batch; 103h:55m:08s remains)
INFO - root - 2019-11-04 00:07:11.408369: step 49230, total loss = 0.58, predict loss = 0.13 (61.0 examples/sec; 0.066 sec/batch; 108h:19m:45s remains)
INFO - root - 2019-11-04 00:07:12.044128: step 49240, total loss = 0.67, predict loss = 0.16 (76.2 examples/sec; 0.053 sec/batch; 86h:47m:09s remains)
INFO - root - 2019-11-04 00:07:12.683068: step 49250, total loss = 0.55, predict loss = 0.12 (70.1 examples/sec; 0.057 sec/batch; 94h:21m:54s remains)
INFO - root - 2019-11-04 00:07:13.335269: step 49260, total loss = 0.66, predict loss = 0.15 (67.8 examples/sec; 0.059 sec/batch; 97h:33m:52s remains)
INFO - root - 2019-11-04 00:07:14.010978: step 49270, total loss = 0.64, predict loss = 0.15 (56.2 examples/sec; 0.071 sec/batch; 117h:44m:17s remains)
INFO - root - 2019-11-04 00:07:14.651460: step 49280, total loss = 0.55, predict loss = 0.13 (83.6 examples/sec; 0.048 sec/batch; 79h:02m:51s remains)
INFO - root - 2019-11-04 00:07:15.252098: step 49290, total loss = 0.56, predict loss = 0.13 (74.0 examples/sec; 0.054 sec/batch; 89h:24m:29s remains)
INFO - root - 2019-11-04 00:07:15.891344: step 49300, total loss = 0.38, predict loss = 0.08 (71.6 examples/sec; 0.056 sec/batch; 92h:22m:59s remains)
INFO - root - 2019-11-04 00:07:16.521468: step 49310, total loss = 0.58, predict loss = 0.14 (69.5 examples/sec; 0.058 sec/batch; 95h:04m:23s remains)
INFO - root - 2019-11-04 00:07:17.150963: step 49320, total loss = 0.50, predict loss = 0.11 (65.3 examples/sec; 0.061 sec/batch; 101h:19m:24s remains)
INFO - root - 2019-11-04 00:07:17.858193: step 49330, total loss = 0.46, predict loss = 0.10 (66.2 examples/sec; 0.060 sec/batch; 99h:51m:54s remains)
INFO - root - 2019-11-04 00:07:18.511523: step 49340, total loss = 0.57, predict loss = 0.13 (60.9 examples/sec; 0.066 sec/batch; 108h:36m:21s remains)
INFO - root - 2019-11-04 00:07:19.219587: step 49350, total loss = 0.69, predict loss = 0.16 (52.0 examples/sec; 0.077 sec/batch; 127h:15m:17s remains)
INFO - root - 2019-11-04 00:07:19.932264: step 49360, total loss = 0.60, predict loss = 0.14 (58.2 examples/sec; 0.069 sec/batch; 113h:35m:55s remains)
INFO - root - 2019-11-04 00:07:20.664096: step 49370, total loss = 0.64, predict loss = 0.14 (67.0 examples/sec; 0.060 sec/batch; 98h:42m:36s remains)
INFO - root - 2019-11-04 00:07:21.291415: step 49380, total loss = 0.60, predict loss = 0.14 (73.2 examples/sec; 0.055 sec/batch; 90h:21m:15s remains)
INFO - root - 2019-11-04 00:07:21.938120: step 49390, total loss = 0.80, predict loss = 0.19 (66.7 examples/sec; 0.060 sec/batch; 99h:06m:56s remains)
INFO - root - 2019-11-04 00:07:22.591378: step 49400, total loss = 0.49, predict loss = 0.11 (67.9 examples/sec; 0.059 sec/batch; 97h:21m:43s remains)
INFO - root - 2019-11-04 00:07:23.227844: step 49410, total loss = 0.69, predict loss = 0.15 (70.7 examples/sec; 0.057 sec/batch; 93h:34m:09s remains)
INFO - root - 2019-11-04 00:07:23.866869: step 49420, total loss = 0.84, predict loss = 0.19 (76.3 examples/sec; 0.052 sec/batch; 86h:40m:40s remains)
INFO - root - 2019-11-04 00:07:24.514069: step 49430, total loss = 0.75, predict loss = 0.17 (68.1 examples/sec; 0.059 sec/batch; 97h:03m:31s remains)
INFO - root - 2019-11-04 00:07:25.137745: step 49440, total loss = 0.70, predict loss = 0.16 (71.6 examples/sec; 0.056 sec/batch; 92h:17m:55s remains)
INFO - root - 2019-11-04 00:07:25.752090: step 49450, total loss = 0.63, predict loss = 0.14 (71.9 examples/sec; 0.056 sec/batch; 91h:57m:06s remains)
INFO - root - 2019-11-04 00:07:26.364340: step 49460, total loss = 0.66, predict loss = 0.15 (72.9 examples/sec; 0.055 sec/batch; 90h:42m:30s remains)
INFO - root - 2019-11-04 00:07:27.010419: step 49470, total loss = 0.60, predict loss = 0.14 (68.2 examples/sec; 0.059 sec/batch; 96h:55m:57s remains)
INFO - root - 2019-11-04 00:07:27.647640: step 49480, total loss = 0.54, predict loss = 0.12 (64.8 examples/sec; 0.062 sec/batch; 102h:03m:04s remains)
INFO - root - 2019-11-04 00:07:28.289990: step 49490, total loss = 0.65, predict loss = 0.15 (68.4 examples/sec; 0.058 sec/batch; 96h:38m:23s remains)
INFO - root - 2019-11-04 00:07:28.917182: step 49500, total loss = 0.63, predict loss = 0.15 (73.3 examples/sec; 0.055 sec/batch; 90h:11m:50s remains)
INFO - root - 2019-11-04 00:07:29.536726: step 49510, total loss = 0.60, predict loss = 0.13 (73.8 examples/sec; 0.054 sec/batch; 89h:33m:04s remains)
INFO - root - 2019-11-04 00:07:30.213537: step 49520, total loss = 0.55, predict loss = 0.12 (61.5 examples/sec; 0.065 sec/batch; 107h:32m:24s remains)
INFO - root - 2019-11-04 00:07:30.834451: step 49530, total loss = 0.54, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 95h:50m:07s remains)
INFO - root - 2019-11-04 00:07:31.494281: step 49540, total loss = 0.62, predict loss = 0.16 (63.8 examples/sec; 0.063 sec/batch; 103h:38m:32s remains)
INFO - root - 2019-11-04 00:07:32.154701: step 49550, total loss = 0.53, predict loss = 0.12 (60.9 examples/sec; 0.066 sec/batch; 108h:35m:54s remains)
INFO - root - 2019-11-04 00:07:32.847989: step 49560, total loss = 0.66, predict loss = 0.16 (66.0 examples/sec; 0.061 sec/batch; 100h:11m:13s remains)
INFO - root - 2019-11-04 00:07:33.513290: step 49570, total loss = 0.76, predict loss = 0.19 (67.9 examples/sec; 0.059 sec/batch; 97h:22m:53s remains)
INFO - root - 2019-11-04 00:07:34.184081: step 49580, total loss = 0.52, predict loss = 0.13 (75.1 examples/sec; 0.053 sec/batch; 88h:01m:04s remains)
INFO - root - 2019-11-04 00:07:34.873558: step 49590, total loss = 0.45, predict loss = 0.10 (68.8 examples/sec; 0.058 sec/batch; 96h:02m:02s remains)
INFO - root - 2019-11-04 00:07:35.510243: step 49600, total loss = 0.73, predict loss = 0.18 (74.5 examples/sec; 0.054 sec/batch; 88h:46m:14s remains)
INFO - root - 2019-11-04 00:07:36.142800: step 49610, total loss = 0.56, predict loss = 0.13 (74.0 examples/sec; 0.054 sec/batch; 89h:19m:27s remains)
INFO - root - 2019-11-04 00:07:36.770850: step 49620, total loss = 0.58, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 94h:45m:02s remains)
INFO - root - 2019-11-04 00:07:37.423242: step 49630, total loss = 0.57, predict loss = 0.14 (78.2 examples/sec; 0.051 sec/batch; 84h:34m:24s remains)
INFO - root - 2019-11-04 00:07:38.059647: step 49640, total loss = 0.49, predict loss = 0.12 (75.0 examples/sec; 0.053 sec/batch; 88h:11m:46s remains)
INFO - root - 2019-11-04 00:07:38.693437: step 49650, total loss = 0.35, predict loss = 0.08 (79.6 examples/sec; 0.050 sec/batch; 83h:02m:37s remains)
INFO - root - 2019-11-04 00:07:39.329360: step 49660, total loss = 0.49, predict loss = 0.12 (82.1 examples/sec; 0.049 sec/batch; 80h:31m:29s remains)
INFO - root - 2019-11-04 00:07:39.954292: step 49670, total loss = 0.42, predict loss = 0.09 (67.6 examples/sec; 0.059 sec/batch; 97h:44m:25s remains)
INFO - root - 2019-11-04 00:07:40.584479: step 49680, total loss = 0.50, predict loss = 0.12 (65.1 examples/sec; 0.061 sec/batch; 101h:30m:47s remains)
INFO - root - 2019-11-04 00:07:41.236257: step 49690, total loss = 0.38, predict loss = 0.08 (66.2 examples/sec; 0.060 sec/batch; 99h:54m:39s remains)
INFO - root - 2019-11-04 00:07:41.870163: step 49700, total loss = 0.40, predict loss = 0.09 (71.1 examples/sec; 0.056 sec/batch; 93h:02m:23s remains)
INFO - root - 2019-11-04 00:07:42.527038: step 49710, total loss = 0.42, predict loss = 0.09 (63.8 examples/sec; 0.063 sec/batch; 103h:35m:08s remains)
INFO - root - 2019-11-04 00:07:43.219777: step 49720, total loss = 0.49, predict loss = 0.12 (67.0 examples/sec; 0.060 sec/batch; 98h:37m:24s remains)
INFO - root - 2019-11-04 00:07:43.881066: step 49730, total loss = 0.57, predict loss = 0.14 (61.4 examples/sec; 0.065 sec/batch; 107h:44m:45s remains)
INFO - root - 2019-11-04 00:07:44.561185: step 49740, total loss = 0.35, predict loss = 0.08 (74.4 examples/sec; 0.054 sec/batch; 88h:49m:38s remains)
INFO - root - 2019-11-04 00:07:45.238771: step 49750, total loss = 0.60, predict loss = 0.14 (61.1 examples/sec; 0.065 sec/batch; 108h:10m:13s remains)
INFO - root - 2019-11-04 00:07:45.833997: step 49760, total loss = 0.42, predict loss = 0.09 (74.5 examples/sec; 0.054 sec/batch; 88h:46m:10s remains)
INFO - root - 2019-11-04 00:07:46.423661: step 49770, total loss = 0.61, predict loss = 0.15 (79.1 examples/sec; 0.051 sec/batch; 83h:35m:34s remains)
INFO - root - 2019-11-04 00:07:47.054091: step 49780, total loss = 0.47, predict loss = 0.10 (63.1 examples/sec; 0.063 sec/batch; 104h:45m:33s remains)
INFO - root - 2019-11-04 00:07:47.720210: step 49790, total loss = 0.75, predict loss = 0.18 (61.7 examples/sec; 0.065 sec/batch; 107h:06m:20s remains)
INFO - root - 2019-11-04 00:07:48.351525: step 49800, total loss = 0.50, predict loss = 0.11 (67.3 examples/sec; 0.059 sec/batch; 98h:17m:05s remains)
INFO - root - 2019-11-04 00:07:49.001223: step 49810, total loss = 0.61, predict loss = 0.14 (67.5 examples/sec; 0.059 sec/batch; 97h:56m:46s remains)
INFO - root - 2019-11-04 00:07:49.685820: step 49820, total loss = 0.71, predict loss = 0.17 (68.8 examples/sec; 0.058 sec/batch; 96h:08m:12s remains)
INFO - root - 2019-11-04 00:07:50.342370: step 49830, total loss = 0.65, predict loss = 0.15 (70.6 examples/sec; 0.057 sec/batch; 93h:36m:06s remains)
INFO - root - 2019-11-04 00:07:50.977250: step 49840, total loss = 0.53, predict loss = 0.12 (79.1 examples/sec; 0.051 sec/batch; 83h:34m:24s remains)
INFO - root - 2019-11-04 00:07:51.667447: step 49850, total loss = 0.84, predict loss = 0.21 (58.8 examples/sec; 0.068 sec/batch; 112h:31m:06s remains)
INFO - root - 2019-11-04 00:07:52.293995: step 49860, total loss = 0.58, predict loss = 0.14 (65.4 examples/sec; 0.061 sec/batch; 101h:00m:52s remains)
INFO - root - 2019-11-04 00:07:52.943831: step 49870, total loss = 0.42, predict loss = 0.10 (63.3 examples/sec; 0.063 sec/batch; 104h:30m:55s remains)
INFO - root - 2019-11-04 00:07:53.576612: step 49880, total loss = 0.73, predict loss = 0.17 (71.1 examples/sec; 0.056 sec/batch; 92h:55m:40s remains)
INFO - root - 2019-11-04 00:07:54.231099: step 49890, total loss = 0.69, predict loss = 0.17 (67.5 examples/sec; 0.059 sec/batch; 97h:53m:42s remains)
INFO - root - 2019-11-04 00:07:54.923000: step 49900, total loss = 0.74, predict loss = 0.18 (62.2 examples/sec; 0.064 sec/batch; 106h:21m:29s remains)
INFO - root - 2019-11-04 00:07:55.563745: step 49910, total loss = 0.44, predict loss = 0.09 (79.0 examples/sec; 0.051 sec/batch; 83h:42m:04s remains)
INFO - root - 2019-11-04 00:07:56.197801: step 49920, total loss = 0.26, predict loss = 0.05 (80.4 examples/sec; 0.050 sec/batch; 82h:15m:10s remains)
INFO - root - 2019-11-04 00:07:56.838753: step 49930, total loss = 0.34, predict loss = 0.08 (64.1 examples/sec; 0.062 sec/batch; 103h:04m:43s remains)
INFO - root - 2019-11-04 00:07:57.499959: step 49940, total loss = 0.34, predict loss = 0.07 (68.2 examples/sec; 0.059 sec/batch; 96h:53m:59s remains)
INFO - root - 2019-11-04 00:07:58.147670: step 49950, total loss = 0.49, predict loss = 0.12 (68.1 examples/sec; 0.059 sec/batch; 97h:04m:29s remains)
INFO - root - 2019-11-04 00:07:58.791165: step 49960, total loss = 0.39, predict loss = 0.08 (65.4 examples/sec; 0.061 sec/batch; 101h:04m:29s remains)
INFO - root - 2019-11-04 00:07:59.433234: step 49970, total loss = 0.35, predict loss = 0.08 (71.7 examples/sec; 0.056 sec/batch; 92h:09m:33s remains)
INFO - root - 2019-11-04 00:08:00.080378: step 49980, total loss = 0.41, predict loss = 0.10 (65.3 examples/sec; 0.061 sec/batch; 101h:10m:28s remains)
INFO - root - 2019-11-04 00:08:00.722781: step 49990, total loss = 0.30, predict loss = 0.06 (63.3 examples/sec; 0.063 sec/batch; 104h:22m:35s remains)
INFO - root - 2019-11-04 00:08:01.368679: step 50000, total loss = 0.46, predict loss = 0.10 (72.9 examples/sec; 0.055 sec/batch; 90h:44m:33s remains)
INFO - root - 2019-11-04 00:08:02.026748: step 50010, total loss = 0.50, predict loss = 0.11 (66.8 examples/sec; 0.060 sec/batch; 98h:55m:45s remains)
INFO - root - 2019-11-04 00:08:02.665771: step 50020, total loss = 0.46, predict loss = 0.10 (73.6 examples/sec; 0.054 sec/batch; 89h:46m:29s remains)
INFO - root - 2019-11-04 00:08:03.286620: step 50030, total loss = 0.38, predict loss = 0.09 (72.9 examples/sec; 0.055 sec/batch; 90h:42m:46s remains)
INFO - root - 2019-11-04 00:08:03.922944: step 50040, total loss = 0.48, predict loss = 0.11 (69.5 examples/sec; 0.058 sec/batch; 95h:04m:25s remains)
INFO - root - 2019-11-04 00:08:04.517904: step 50050, total loss = 0.49, predict loss = 0.11 (72.5 examples/sec; 0.055 sec/batch; 91h:10m:39s remains)
INFO - root - 2019-11-04 00:08:05.159334: step 50060, total loss = 0.44, predict loss = 0.10 (64.8 examples/sec; 0.062 sec/batch; 101h:58m:33s remains)
INFO - root - 2019-11-04 00:08:05.784960: step 50070, total loss = 0.63, predict loss = 0.15 (71.0 examples/sec; 0.056 sec/batch; 93h:06m:23s remains)
INFO - root - 2019-11-04 00:08:06.420201: step 50080, total loss = 0.67, predict loss = 0.15 (60.2 examples/sec; 0.066 sec/batch; 109h:53m:41s remains)
INFO - root - 2019-11-04 00:08:07.109390: step 50090, total loss = 0.48, predict loss = 0.11 (63.4 examples/sec; 0.063 sec/batch; 104h:13m:52s remains)
INFO - root - 2019-11-04 00:08:07.801097: step 50100, total loss = 0.62, predict loss = 0.15 (69.2 examples/sec; 0.058 sec/batch; 95h:31m:01s remains)
INFO - root - 2019-11-04 00:08:08.418562: step 50110, total loss = 0.62, predict loss = 0.15 (66.3 examples/sec; 0.060 sec/batch; 99h:40m:21s remains)
INFO - root - 2019-11-04 00:08:09.020089: step 50120, total loss = 0.49, predict loss = 0.11 (76.8 examples/sec; 0.052 sec/batch; 86h:03m:39s remains)
INFO - root - 2019-11-04 00:08:09.654444: step 50130, total loss = 0.50, predict loss = 0.12 (73.4 examples/sec; 0.055 sec/batch; 90h:06m:27s remains)
INFO - root - 2019-11-04 00:08:10.277261: step 50140, total loss = 0.55, predict loss = 0.12 (72.0 examples/sec; 0.056 sec/batch; 91h:50m:11s remains)
INFO - root - 2019-11-04 00:08:10.897478: step 50150, total loss = 0.42, predict loss = 0.10 (74.5 examples/sec; 0.054 sec/batch; 88h:43m:41s remains)
INFO - root - 2019-11-04 00:08:11.530437: step 50160, total loss = 0.63, predict loss = 0.16 (73.8 examples/sec; 0.054 sec/batch; 89h:35m:09s remains)
INFO - root - 2019-11-04 00:08:12.118356: step 50170, total loss = 0.19, predict loss = 0.04 (74.1 examples/sec; 0.054 sec/batch; 89h:10m:15s remains)
INFO - root - 2019-11-04 00:08:12.795136: step 50180, total loss = 0.42, predict loss = 0.09 (68.6 examples/sec; 0.058 sec/batch; 96h:21m:28s remains)
INFO - root - 2019-11-04 00:08:13.434500: step 50190, total loss = 0.39, predict loss = 0.09 (80.6 examples/sec; 0.050 sec/batch; 82h:00m:21s remains)
INFO - root - 2019-11-04 00:08:14.077131: step 50200, total loss = 0.24, predict loss = 0.05 (64.6 examples/sec; 0.062 sec/batch; 102h:24m:26s remains)
INFO - root - 2019-11-04 00:08:14.715032: step 50210, total loss = 0.45, predict loss = 0.10 (75.0 examples/sec; 0.053 sec/batch; 88h:08m:47s remains)
INFO - root - 2019-11-04 00:08:15.336774: step 50220, total loss = 0.67, predict loss = 0.17 (72.0 examples/sec; 0.056 sec/batch; 91h:45m:32s remains)
INFO - root - 2019-11-04 00:08:15.971352: step 50230, total loss = 0.52, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 91h:31m:16s remains)
INFO - root - 2019-11-04 00:08:16.613936: step 50240, total loss = 0.58, predict loss = 0.15 (83.1 examples/sec; 0.048 sec/batch; 79h:33m:57s remains)
INFO - root - 2019-11-04 00:08:17.228744: step 50250, total loss = 0.59, predict loss = 0.14 (69.7 examples/sec; 0.057 sec/batch; 94h:48m:22s remains)
INFO - root - 2019-11-04 00:08:17.844460: step 50260, total loss = 0.59, predict loss = 0.13 (68.6 examples/sec; 0.058 sec/batch; 96h:22m:40s remains)
INFO - root - 2019-11-04 00:08:18.483292: step 50270, total loss = 0.30, predict loss = 0.07 (69.4 examples/sec; 0.058 sec/batch; 95h:14m:47s remains)
INFO - root - 2019-11-04 00:08:19.144367: step 50280, total loss = 0.53, predict loss = 0.12 (74.0 examples/sec; 0.054 sec/batch; 89h:19m:57s remains)
INFO - root - 2019-11-04 00:08:19.791528: step 50290, total loss = 0.51, predict loss = 0.12 (70.0 examples/sec; 0.057 sec/batch; 94h:23m:26s remains)
INFO - root - 2019-11-04 00:08:20.422089: step 50300, total loss = 0.44, predict loss = 0.10 (69.0 examples/sec; 0.058 sec/batch; 95h:46m:53s remains)
INFO - root - 2019-11-04 00:08:21.044723: step 50310, total loss = 0.55, predict loss = 0.13 (65.8 examples/sec; 0.061 sec/batch; 100h:25m:07s remains)
INFO - root - 2019-11-04 00:08:21.683192: step 50320, total loss = 0.45, predict loss = 0.10 (66.3 examples/sec; 0.060 sec/batch; 99h:44m:52s remains)
INFO - root - 2019-11-04 00:08:22.340997: step 50330, total loss = 0.33, predict loss = 0.07 (62.7 examples/sec; 0.064 sec/batch; 105h:29m:53s remains)
INFO - root - 2019-11-04 00:08:23.050230: step 50340, total loss = 0.36, predict loss = 0.08 (75.6 examples/sec; 0.053 sec/batch; 87h:26m:35s remains)
INFO - root - 2019-11-04 00:08:23.687227: step 50350, total loss = 0.63, predict loss = 0.15 (73.5 examples/sec; 0.054 sec/batch; 89h:59m:39s remains)
INFO - root - 2019-11-04 00:08:24.319205: step 50360, total loss = 0.65, predict loss = 0.14 (72.7 examples/sec; 0.055 sec/batch; 90h:58m:32s remains)
INFO - root - 2019-11-04 00:08:24.956470: step 50370, total loss = 0.53, predict loss = 0.12 (69.7 examples/sec; 0.057 sec/batch; 94h:54m:31s remains)
INFO - root - 2019-11-04 00:08:25.587821: step 50380, total loss = 0.70, predict loss = 0.16 (76.9 examples/sec; 0.052 sec/batch; 85h:58m:32s remains)
INFO - root - 2019-11-04 00:08:26.205992: step 50390, total loss = 0.58, predict loss = 0.13 (75.1 examples/sec; 0.053 sec/batch; 88h:04m:55s remains)
INFO - root - 2019-11-04 00:08:26.848756: step 50400, total loss = 0.58, predict loss = 0.11 (68.9 examples/sec; 0.058 sec/batch; 95h:56m:52s remains)
INFO - root - 2019-11-04 00:08:27.485990: step 50410, total loss = 0.52, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 95h:58m:53s remains)
INFO - root - 2019-11-04 00:08:28.127819: step 50420, total loss = 0.57, predict loss = 0.13 (65.8 examples/sec; 0.061 sec/batch; 100h:26m:24s remains)
INFO - root - 2019-11-04 00:08:28.772729: step 50430, total loss = 0.43, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 96h:21m:24s remains)
INFO - root - 2019-11-04 00:08:29.396253: step 50440, total loss = 0.47, predict loss = 0.11 (71.8 examples/sec; 0.056 sec/batch; 92h:02m:24s remains)
INFO - root - 2019-11-04 00:08:30.031809: step 50450, total loss = 0.49, predict loss = 0.11 (78.9 examples/sec; 0.051 sec/batch; 83h:48m:18s remains)
INFO - root - 2019-11-04 00:08:30.661011: step 50460, total loss = 0.48, predict loss = 0.12 (63.3 examples/sec; 0.063 sec/batch; 104h:24m:36s remains)
INFO - root - 2019-11-04 00:08:31.298366: step 50470, total loss = 0.70, predict loss = 0.17 (77.3 examples/sec; 0.052 sec/batch; 85h:32m:58s remains)
INFO - root - 2019-11-04 00:08:31.955993: step 50480, total loss = 0.55, predict loss = 0.12 (78.4 examples/sec; 0.051 sec/batch; 84h:21m:27s remains)
INFO - root - 2019-11-04 00:08:32.612590: step 50490, total loss = 0.67, predict loss = 0.16 (74.9 examples/sec; 0.053 sec/batch; 88h:14m:13s remains)
INFO - root - 2019-11-04 00:08:33.240209: step 50500, total loss = 0.57, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 97h:02m:11s remains)
INFO - root - 2019-11-04 00:08:33.880066: step 50510, total loss = 0.73, predict loss = 0.17 (73.5 examples/sec; 0.054 sec/batch; 89h:56m:52s remains)
INFO - root - 2019-11-04 00:08:34.464301: step 50520, total loss = 0.94, predict loss = 0.22 (72.6 examples/sec; 0.055 sec/batch; 91h:05m:49s remains)
INFO - root - 2019-11-04 00:08:35.114341: step 50530, total loss = 0.98, predict loss = 0.24 (72.3 examples/sec; 0.055 sec/batch; 91h:28m:47s remains)
INFO - root - 2019-11-04 00:08:35.781368: step 50540, total loss = 0.99, predict loss = 0.21 (67.1 examples/sec; 0.060 sec/batch; 98h:32m:45s remains)
INFO - root - 2019-11-04 00:08:36.441788: step 50550, total loss = 0.86, predict loss = 0.21 (59.7 examples/sec; 0.067 sec/batch; 110h:40m:15s remains)
INFO - root - 2019-11-04 00:08:37.088238: step 50560, total loss = 0.70, predict loss = 0.17 (76.1 examples/sec; 0.053 sec/batch; 86h:51m:21s remains)
INFO - root - 2019-11-04 00:08:37.707837: step 50570, total loss = 0.77, predict loss = 0.19 (85.4 examples/sec; 0.047 sec/batch; 77h:26m:34s remains)
INFO - root - 2019-11-04 00:08:38.332075: step 50580, total loss = 1.00, predict loss = 0.23 (67.0 examples/sec; 0.060 sec/batch; 98h:43m:06s remains)
INFO - root - 2019-11-04 00:08:38.986198: step 50590, total loss = 0.79, predict loss = 0.19 (75.2 examples/sec; 0.053 sec/batch; 87h:56m:02s remains)
INFO - root - 2019-11-04 00:08:39.619628: step 50600, total loss = 0.56, predict loss = 0.13 (69.5 examples/sec; 0.058 sec/batch; 95h:05m:53s remains)
INFO - root - 2019-11-04 00:08:40.247724: step 50610, total loss = 0.68, predict loss = 0.16 (66.5 examples/sec; 0.060 sec/batch; 99h:27m:06s remains)
INFO - root - 2019-11-04 00:08:40.889440: step 50620, total loss = 0.61, predict loss = 0.13 (69.5 examples/sec; 0.058 sec/batch; 95h:02m:46s remains)
INFO - root - 2019-11-04 00:08:41.529895: step 50630, total loss = 0.61, predict loss = 0.13 (79.9 examples/sec; 0.050 sec/batch; 82h:43m:35s remains)
INFO - root - 2019-11-04 00:08:42.224234: step 50640, total loss = 0.88, predict loss = 0.22 (65.2 examples/sec; 0.061 sec/batch; 101h:21m:24s remains)
INFO - root - 2019-11-04 00:08:42.868533: step 50650, total loss = 0.64, predict loss = 0.15 (78.3 examples/sec; 0.051 sec/batch; 84h:26m:32s remains)
INFO - root - 2019-11-04 00:08:43.515491: step 50660, total loss = 0.68, predict loss = 0.16 (81.3 examples/sec; 0.049 sec/batch; 81h:21m:26s remains)
INFO - root - 2019-11-04 00:08:44.185740: step 50670, total loss = 0.52, predict loss = 0.12 (58.5 examples/sec; 0.068 sec/batch; 112h:56m:50s remains)
INFO - root - 2019-11-04 00:08:44.812510: step 50680, total loss = 0.49, predict loss = 0.11 (67.3 examples/sec; 0.059 sec/batch; 98h:15m:23s remains)
INFO - root - 2019-11-04 00:08:45.466666: step 50690, total loss = 0.57, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 96h:05m:25s remains)
INFO - root - 2019-11-04 00:08:46.114892: step 50700, total loss = 0.56, predict loss = 0.13 (61.5 examples/sec; 0.065 sec/batch; 107h:34m:09s remains)
INFO - root - 2019-11-04 00:08:46.766209: step 50710, total loss = 0.54, predict loss = 0.12 (66.3 examples/sec; 0.060 sec/batch; 99h:37m:43s remains)
INFO - root - 2019-11-04 00:08:47.379836: step 50720, total loss = 0.68, predict loss = 0.16 (66.2 examples/sec; 0.060 sec/batch; 99h:51m:28s remains)
INFO - root - 2019-11-04 00:08:48.043046: step 50730, total loss = 0.59, predict loss = 0.12 (73.5 examples/sec; 0.054 sec/batch; 89h:53m:55s remains)
INFO - root - 2019-11-04 00:08:48.713963: step 50740, total loss = 0.66, predict loss = 0.15 (67.4 examples/sec; 0.059 sec/batch; 98h:07m:47s remains)
INFO - root - 2019-11-04 00:08:49.366650: step 50750, total loss = 0.40, predict loss = 0.09 (62.3 examples/sec; 0.064 sec/batch; 106h:08m:38s remains)
INFO - root - 2019-11-04 00:08:49.962754: step 50760, total loss = 0.66, predict loss = 0.16 (75.5 examples/sec; 0.053 sec/batch; 87h:35m:56s remains)
INFO - root - 2019-11-04 00:08:50.586289: step 50770, total loss = 0.47, predict loss = 0.11 (64.3 examples/sec; 0.062 sec/batch; 102h:46m:00s remains)
INFO - root - 2019-11-04 00:08:51.227315: step 50780, total loss = 0.39, predict loss = 0.08 (74.1 examples/sec; 0.054 sec/batch; 89h:12m:59s remains)
INFO - root - 2019-11-04 00:08:51.870217: step 50790, total loss = 0.39, predict loss = 0.09 (70.6 examples/sec; 0.057 sec/batch; 93h:35m:20s remains)
INFO - root - 2019-11-04 00:08:52.480529: step 50800, total loss = 0.43, predict loss = 0.11 (63.7 examples/sec; 0.063 sec/batch; 103h:41m:50s remains)
INFO - root - 2019-11-04 00:08:53.137026: step 50810, total loss = 0.58, predict loss = 0.13 (75.0 examples/sec; 0.053 sec/batch; 88h:10m:00s remains)
INFO - root - 2019-11-04 00:08:53.744950: step 50820, total loss = 0.52, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 104h:09m:18s remains)
INFO - root - 2019-11-04 00:08:54.417194: step 50830, total loss = 0.41, predict loss = 0.10 (63.1 examples/sec; 0.063 sec/batch; 104h:42m:37s remains)
INFO - root - 2019-11-04 00:08:55.070062: step 50840, total loss = 0.46, predict loss = 0.10 (65.6 examples/sec; 0.061 sec/batch; 100h:49m:03s remains)
INFO - root - 2019-11-04 00:08:55.772085: step 50850, total loss = 0.53, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 96h:10m:59s remains)
INFO - root - 2019-11-04 00:08:56.390878: step 50860, total loss = 0.47, predict loss = 0.10 (76.7 examples/sec; 0.052 sec/batch; 86h:10m:32s remains)
INFO - root - 2019-11-04 00:08:57.065917: step 50870, total loss = 0.35, predict loss = 0.07 (60.7 examples/sec; 0.066 sec/batch; 108h:51m:57s remains)
INFO - root - 2019-11-04 00:08:57.768353: step 50880, total loss = 0.54, predict loss = 0.13 (68.9 examples/sec; 0.058 sec/batch; 95h:54m:06s remains)
INFO - root - 2019-11-04 00:08:58.420809: step 50890, total loss = 0.65, predict loss = 0.15 (68.1 examples/sec; 0.059 sec/batch; 97h:02m:10s remains)
INFO - root - 2019-11-04 00:08:59.070063: step 50900, total loss = 0.44, predict loss = 0.10 (66.6 examples/sec; 0.060 sec/batch; 99h:16m:25s remains)
INFO - root - 2019-11-04 00:08:59.724068: step 50910, total loss = 0.66, predict loss = 0.16 (68.0 examples/sec; 0.059 sec/batch; 97h:15m:11s remains)
INFO - root - 2019-11-04 00:09:00.358830: step 50920, total loss = 0.43, predict loss = 0.09 (69.7 examples/sec; 0.057 sec/batch; 94h:49m:43s remains)
INFO - root - 2019-11-04 00:09:01.000657: step 50930, total loss = 0.62, predict loss = 0.15 (70.8 examples/sec; 0.057 sec/batch; 93h:22m:40s remains)
INFO - root - 2019-11-04 00:09:01.633291: step 50940, total loss = 0.59, predict loss = 0.14 (69.1 examples/sec; 0.058 sec/batch; 95h:37m:37s remains)
INFO - root - 2019-11-04 00:09:02.262755: step 50950, total loss = 0.67, predict loss = 0.16 (71.1 examples/sec; 0.056 sec/batch; 92h:54m:59s remains)
INFO - root - 2019-11-04 00:09:02.884125: step 50960, total loss = 0.63, predict loss = 0.15 (75.5 examples/sec; 0.053 sec/batch; 87h:31m:20s remains)
INFO - root - 2019-11-04 00:09:03.542081: step 50970, total loss = 0.84, predict loss = 0.21 (76.3 examples/sec; 0.052 sec/batch; 86h:37m:47s remains)
INFO - root - 2019-11-04 00:09:04.158985: step 50980, total loss = 0.79, predict loss = 0.19 (86.4 examples/sec; 0.046 sec/batch; 76h:30m:39s remains)
INFO - root - 2019-11-04 00:09:04.788342: step 50990, total loss = 0.68, predict loss = 0.17 (80.5 examples/sec; 0.050 sec/batch; 82h:07m:40s remains)
INFO - root - 2019-11-04 00:09:05.402829: step 51000, total loss = 0.88, predict loss = 0.21 (70.4 examples/sec; 0.057 sec/batch; 93h:54m:59s remains)
INFO - root - 2019-11-04 00:09:06.039074: step 51010, total loss = 0.74, predict loss = 0.18 (63.5 examples/sec; 0.063 sec/batch; 104h:06m:07s remains)
INFO - root - 2019-11-04 00:09:06.656412: step 51020, total loss = 0.62, predict loss = 0.15 (74.1 examples/sec; 0.054 sec/batch; 89h:12m:22s remains)
INFO - root - 2019-11-04 00:09:07.273926: step 51030, total loss = 0.65, predict loss = 0.16 (64.9 examples/sec; 0.062 sec/batch; 101h:50m:15s remains)
INFO - root - 2019-11-04 00:09:07.885699: step 51040, total loss = 0.57, predict loss = 0.13 (72.4 examples/sec; 0.055 sec/batch; 91h:15m:36s remains)
INFO - root - 2019-11-04 00:09:08.504305: step 51050, total loss = 0.47, predict loss = 0.11 (69.6 examples/sec; 0.058 sec/batch; 95h:01m:50s remains)
INFO - root - 2019-11-04 00:09:09.133182: step 51060, total loss = 0.47, predict loss = 0.10 (74.5 examples/sec; 0.054 sec/batch; 88h:43m:35s remains)
INFO - root - 2019-11-04 00:09:09.765843: step 51070, total loss = 0.50, predict loss = 0.12 (70.1 examples/sec; 0.057 sec/batch; 94h:14m:47s remains)
INFO - root - 2019-11-04 00:09:10.422398: step 51080, total loss = 0.55, predict loss = 0.12 (65.4 examples/sec; 0.061 sec/batch; 101h:03m:39s remains)
INFO - root - 2019-11-04 00:09:11.081337: step 51090, total loss = 0.45, predict loss = 0.10 (66.0 examples/sec; 0.061 sec/batch; 100h:10m:14s remains)
INFO - root - 2019-11-04 00:09:11.731512: step 51100, total loss = 0.57, predict loss = 0.13 (62.5 examples/sec; 0.064 sec/batch; 105h:47m:33s remains)
INFO - root - 2019-11-04 00:09:12.401020: step 51110, total loss = 0.63, predict loss = 0.15 (73.8 examples/sec; 0.054 sec/batch; 89h:35m:11s remains)
INFO - root - 2019-11-04 00:09:13.045868: step 51120, total loss = 0.59, predict loss = 0.14 (64.8 examples/sec; 0.062 sec/batch; 102h:04m:41s remains)
INFO - root - 2019-11-04 00:09:13.687984: step 51130, total loss = 0.41, predict loss = 0.09 (75.6 examples/sec; 0.053 sec/batch; 87h:23m:07s remains)
INFO - root - 2019-11-04 00:09:14.396001: step 51140, total loss = 0.55, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 93h:43m:23s remains)
INFO - root - 2019-11-04 00:09:15.116878: step 51150, total loss = 0.67, predict loss = 0.15 (59.1 examples/sec; 0.068 sec/batch; 111h:55m:35s remains)
INFO - root - 2019-11-04 00:09:15.748870: step 51160, total loss = 0.67, predict loss = 0.16 (72.1 examples/sec; 0.055 sec/batch; 91h:37m:16s remains)
INFO - root - 2019-11-04 00:09:16.394656: step 51170, total loss = 0.69, predict loss = 0.15 (71.3 examples/sec; 0.056 sec/batch; 92h:41m:50s remains)
INFO - root - 2019-11-04 00:09:17.041530: step 51180, total loss = 0.76, predict loss = 0.19 (68.7 examples/sec; 0.058 sec/batch; 96h:15m:59s remains)
INFO - root - 2019-11-04 00:09:17.671246: step 51190, total loss = 0.55, predict loss = 0.13 (76.0 examples/sec; 0.053 sec/batch; 86h:58m:36s remains)
INFO - root - 2019-11-04 00:09:18.300642: step 51200, total loss = 0.53, predict loss = 0.12 (74.9 examples/sec; 0.053 sec/batch; 88h:16m:12s remains)
INFO - root - 2019-11-04 00:09:18.946204: step 51210, total loss = 0.67, predict loss = 0.16 (72.7 examples/sec; 0.055 sec/batch; 90h:56m:05s remains)
INFO - root - 2019-11-04 00:09:19.604411: step 51220, total loss = 0.58, predict loss = 0.14 (73.9 examples/sec; 0.054 sec/batch; 89h:27m:34s remains)
INFO - root - 2019-11-04 00:09:20.260547: step 51230, total loss = 0.58, predict loss = 0.14 (69.1 examples/sec; 0.058 sec/batch; 95h:42m:26s remains)
INFO - root - 2019-11-04 00:09:20.931551: step 51240, total loss = 0.69, predict loss = 0.16 (67.6 examples/sec; 0.059 sec/batch; 97h:42m:38s remains)
INFO - root - 2019-11-04 00:09:21.597460: step 51250, total loss = 0.62, predict loss = 0.14 (64.9 examples/sec; 0.062 sec/batch; 101h:47m:48s remains)
INFO - root - 2019-11-04 00:09:22.289399: step 51260, total loss = 0.56, predict loss = 0.13 (67.6 examples/sec; 0.059 sec/batch; 97h:50m:18s remains)
INFO - root - 2019-11-04 00:09:23.029929: step 51270, total loss = 0.66, predict loss = 0.15 (59.0 examples/sec; 0.068 sec/batch; 112h:01m:10s remains)
INFO - root - 2019-11-04 00:09:23.615229: step 51280, total loss = 0.46, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 88h:07m:22s remains)
INFO - root - 2019-11-04 00:09:24.222554: step 51290, total loss = 0.57, predict loss = 0.14 (72.8 examples/sec; 0.055 sec/batch; 90h:46m:32s remains)
INFO - root - 2019-11-04 00:09:24.842642: step 51300, total loss = 0.58, predict loss = 0.14 (63.8 examples/sec; 0.063 sec/batch; 103h:31m:59s remains)
INFO - root - 2019-11-04 00:09:25.455128: step 51310, total loss = 0.67, predict loss = 0.15 (75.6 examples/sec; 0.053 sec/batch; 87h:27m:56s remains)
INFO - root - 2019-11-04 00:09:26.060128: step 51320, total loss = 0.56, predict loss = 0.14 (68.0 examples/sec; 0.059 sec/batch; 97h:08m:42s remains)
INFO - root - 2019-11-04 00:09:26.741591: step 51330, total loss = 0.65, predict loss = 0.14 (73.0 examples/sec; 0.055 sec/batch; 90h:33m:15s remains)
INFO - root - 2019-11-04 00:09:27.391222: step 51340, total loss = 0.57, predict loss = 0.14 (74.1 examples/sec; 0.054 sec/batch; 89h:12m:44s remains)
INFO - root - 2019-11-04 00:09:28.030353: step 51350, total loss = 0.55, predict loss = 0.13 (68.9 examples/sec; 0.058 sec/batch; 95h:57m:06s remains)
INFO - root - 2019-11-04 00:09:28.628168: step 51360, total loss = 0.51, predict loss = 0.12 (66.0 examples/sec; 0.061 sec/batch; 100h:05m:20s remains)
INFO - root - 2019-11-04 00:09:29.247055: step 51370, total loss = 0.53, predict loss = 0.12 (71.6 examples/sec; 0.056 sec/batch; 92h:17m:18s remains)
INFO - root - 2019-11-04 00:09:29.856266: step 51380, total loss = 0.57, predict loss = 0.14 (69.5 examples/sec; 0.058 sec/batch; 95h:07m:51s remains)
INFO - root - 2019-11-04 00:09:30.481637: step 51390, total loss = 0.53, predict loss = 0.12 (78.3 examples/sec; 0.051 sec/batch; 84h:22m:49s remains)
INFO - root - 2019-11-04 00:09:31.101416: step 51400, total loss = 0.55, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 91h:34m:26s remains)
INFO - root - 2019-11-04 00:09:31.716812: step 51410, total loss = 0.54, predict loss = 0.12 (75.1 examples/sec; 0.053 sec/batch; 87h:58m:48s remains)
INFO - root - 2019-11-04 00:09:32.331807: step 51420, total loss = 0.53, predict loss = 0.12 (81.8 examples/sec; 0.049 sec/batch; 80h:47m:16s remains)
INFO - root - 2019-11-04 00:09:32.960051: step 51430, total loss = 0.55, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 101h:08m:45s remains)
INFO - root - 2019-11-04 00:09:33.590731: step 51440, total loss = 0.55, predict loss = 0.12 (66.8 examples/sec; 0.060 sec/batch; 98h:56m:45s remains)
INFO - root - 2019-11-04 00:09:34.232914: step 51450, total loss = 0.52, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 97h:21m:57s remains)
INFO - root - 2019-11-04 00:09:34.894173: step 51460, total loss = 0.53, predict loss = 0.12 (59.9 examples/sec; 0.067 sec/batch; 110h:15m:41s remains)
INFO - root - 2019-11-04 00:09:35.522684: step 51470, total loss = 0.54, predict loss = 0.12 (74.9 examples/sec; 0.053 sec/batch; 88h:17m:13s remains)
INFO - root - 2019-11-04 00:09:36.142709: step 51480, total loss = 0.52, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 93h:07m:34s remains)
INFO - root - 2019-11-04 00:09:36.734096: step 51490, total loss = 0.60, predict loss = 0.15 (79.6 examples/sec; 0.050 sec/batch; 83h:03m:26s remains)
INFO - root - 2019-11-04 00:09:37.331681: step 51500, total loss = 0.51, predict loss = 0.11 (80.3 examples/sec; 0.050 sec/batch; 82h:17m:24s remains)
INFO - root - 2019-11-04 00:09:37.964110: step 51510, total loss = 0.51, predict loss = 0.11 (70.9 examples/sec; 0.056 sec/batch; 93h:13m:42s remains)
INFO - root - 2019-11-04 00:09:38.627909: step 51520, total loss = 0.53, predict loss = 0.12 (61.9 examples/sec; 0.065 sec/batch; 106h:42m:12s remains)
INFO - root - 2019-11-04 00:09:39.277486: step 51530, total loss = 0.63, predict loss = 0.15 (69.4 examples/sec; 0.058 sec/batch; 95h:18m:00s remains)
INFO - root - 2019-11-04 00:09:39.938792: step 51540, total loss = 0.83, predict loss = 0.21 (61.3 examples/sec; 0.065 sec/batch; 107h:54m:05s remains)
INFO - root - 2019-11-04 00:09:40.627132: step 51550, total loss = 0.61, predict loss = 0.15 (63.6 examples/sec; 0.063 sec/batch; 103h:56m:03s remains)
INFO - root - 2019-11-04 00:09:41.299449: step 51560, total loss = 0.78, predict loss = 0.19 (61.9 examples/sec; 0.065 sec/batch; 106h:50m:29s remains)
INFO - root - 2019-11-04 00:09:41.963366: step 51570, total loss = 0.55, predict loss = 0.13 (62.1 examples/sec; 0.064 sec/batch; 106h:20m:46s remains)
INFO - root - 2019-11-04 00:09:42.683005: step 51580, total loss = 0.61, predict loss = 0.14 (64.8 examples/sec; 0.062 sec/batch; 101h:59m:14s remains)
INFO - root - 2019-11-04 00:09:43.382353: step 51590, total loss = 0.76, predict loss = 0.18 (65.6 examples/sec; 0.061 sec/batch; 100h:46m:00s remains)
INFO - root - 2019-11-04 00:09:44.014695: step 51600, total loss = 0.67, predict loss = 0.15 (64.3 examples/sec; 0.062 sec/batch; 102h:44m:43s remains)
INFO - root - 2019-11-04 00:09:44.649326: step 51610, total loss = 0.68, predict loss = 0.16 (63.3 examples/sec; 0.063 sec/batch; 104h:19m:52s remains)
INFO - root - 2019-11-04 00:09:45.257739: step 51620, total loss = 0.65, predict loss = 0.15 (70.9 examples/sec; 0.056 sec/batch; 93h:14m:29s remains)
INFO - root - 2019-11-04 00:09:45.901325: step 51630, total loss = 0.66, predict loss = 0.15 (65.9 examples/sec; 0.061 sec/batch; 100h:13m:53s remains)
INFO - root - 2019-11-04 00:09:46.539770: step 51640, total loss = 0.81, predict loss = 0.20 (70.6 examples/sec; 0.057 sec/batch; 93h:38m:40s remains)
INFO - root - 2019-11-04 00:09:47.219033: step 51650, total loss = 0.67, predict loss = 0.15 (64.0 examples/sec; 0.062 sec/batch; 103h:15m:46s remains)
INFO - root - 2019-11-04 00:09:47.898973: step 51660, total loss = 0.60, predict loss = 0.14 (60.3 examples/sec; 0.066 sec/batch; 109h:31m:39s remains)
INFO - root - 2019-11-04 00:09:48.538000: step 51670, total loss = 0.78, predict loss = 0.18 (70.5 examples/sec; 0.057 sec/batch; 93h:46m:43s remains)
INFO - root - 2019-11-04 00:09:49.167164: step 51680, total loss = 0.54, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 98h:27m:49s remains)
INFO - root - 2019-11-04 00:09:49.808348: step 51690, total loss = 0.55, predict loss = 0.12 (67.4 examples/sec; 0.059 sec/batch; 98h:05m:06s remains)
INFO - root - 2019-11-04 00:09:50.437877: step 51700, total loss = 0.53, predict loss = 0.12 (75.1 examples/sec; 0.053 sec/batch; 87h:58m:47s remains)
INFO - root - 2019-11-04 00:09:51.039849: step 51710, total loss = 0.64, predict loss = 0.15 (83.4 examples/sec; 0.048 sec/batch; 79h:15m:05s remains)
INFO - root - 2019-11-04 00:09:51.673997: step 51720, total loss = 0.51, predict loss = 0.12 (72.4 examples/sec; 0.055 sec/batch; 91h:17m:41s remains)
INFO - root - 2019-11-04 00:09:52.305988: step 51730, total loss = 0.64, predict loss = 0.15 (69.0 examples/sec; 0.058 sec/batch; 95h:47m:31s remains)
INFO - root - 2019-11-04 00:09:52.961845: step 51740, total loss = 0.66, predict loss = 0.15 (74.3 examples/sec; 0.054 sec/batch; 88h:54m:32s remains)
INFO - root - 2019-11-04 00:09:53.612933: step 51750, total loss = 0.74, predict loss = 0.17 (73.4 examples/sec; 0.054 sec/batch; 90h:02m:47s remains)
INFO - root - 2019-11-04 00:09:54.294569: step 51760, total loss = 0.59, predict loss = 0.13 (60.9 examples/sec; 0.066 sec/batch; 108h:28m:24s remains)
INFO - root - 2019-11-04 00:09:54.978499: step 51770, total loss = 0.68, predict loss = 0.17 (77.8 examples/sec; 0.051 sec/batch; 84h:56m:49s remains)
INFO - root - 2019-11-04 00:09:55.600845: step 51780, total loss = 0.52, predict loss = 0.12 (84.5 examples/sec; 0.047 sec/batch; 78h:13m:40s remains)
INFO - root - 2019-11-04 00:09:56.209097: step 51790, total loss = 0.47, predict loss = 0.10 (71.8 examples/sec; 0.056 sec/batch; 92h:00m:32s remains)
INFO - root - 2019-11-04 00:09:56.810841: step 51800, total loss = 0.56, predict loss = 0.12 (75.3 examples/sec; 0.053 sec/batch; 87h:45m:19s remains)
INFO - root - 2019-11-04 00:09:57.266370: step 51810, total loss = 0.49, predict loss = 0.11 (89.6 examples/sec; 0.045 sec/batch; 73h:46m:42s remains)
INFO - root - 2019-11-04 00:09:57.742422: step 51820, total loss = 0.60, predict loss = 0.14 (95.0 examples/sec; 0.042 sec/batch; 69h:33m:46s remains)
INFO - root - 2019-11-04 00:09:58.877374: step 51830, total loss = 0.53, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 92h:03m:42s remains)
INFO - root - 2019-11-04 00:09:59.546521: step 51840, total loss = 0.41, predict loss = 0.09 (68.8 examples/sec; 0.058 sec/batch; 96h:00m:29s remains)
INFO - root - 2019-11-04 00:10:00.187314: step 51850, total loss = 0.52, predict loss = 0.11 (63.5 examples/sec; 0.063 sec/batch; 104h:09m:35s remains)
INFO - root - 2019-11-04 00:10:00.834253: step 51860, total loss = 0.80, predict loss = 0.20 (73.6 examples/sec; 0.054 sec/batch; 89h:45m:37s remains)
INFO - root - 2019-11-04 00:10:01.476340: step 51870, total loss = 0.55, predict loss = 0.13 (62.6 examples/sec; 0.064 sec/batch; 105h:36m:41s remains)
INFO - root - 2019-11-04 00:10:02.136628: step 51880, total loss = 0.76, predict loss = 0.18 (72.8 examples/sec; 0.055 sec/batch; 90h:49m:02s remains)
INFO - root - 2019-11-04 00:10:02.743559: step 51890, total loss = 0.60, predict loss = 0.13 (80.8 examples/sec; 0.050 sec/batch; 81h:48m:37s remains)
INFO - root - 2019-11-04 00:10:03.365413: step 51900, total loss = 0.61, predict loss = 0.13 (68.5 examples/sec; 0.058 sec/batch; 96h:28m:03s remains)
INFO - root - 2019-11-04 00:10:03.983871: step 51910, total loss = 0.73, predict loss = 0.17 (75.0 examples/sec; 0.053 sec/batch; 88h:07m:28s remains)
INFO - root - 2019-11-04 00:10:04.665128: step 51920, total loss = 0.71, predict loss = 0.16 (61.3 examples/sec; 0.065 sec/batch; 107h:45m:03s remains)
INFO - root - 2019-11-04 00:10:05.347690: step 51930, total loss = 0.65, predict loss = 0.14 (64.8 examples/sec; 0.062 sec/batch; 102h:03m:02s remains)
INFO - root - 2019-11-04 00:10:05.986727: step 51940, total loss = 0.59, predict loss = 0.14 (74.0 examples/sec; 0.054 sec/batch; 89h:19m:40s remains)
INFO - root - 2019-11-04 00:10:06.636586: step 51950, total loss = 0.50, predict loss = 0.12 (67.6 examples/sec; 0.059 sec/batch; 97h:41m:37s remains)
INFO - root - 2019-11-04 00:10:07.248318: step 51960, total loss = 0.59, predict loss = 0.14 (85.8 examples/sec; 0.047 sec/batch; 77h:02m:54s remains)
INFO - root - 2019-11-04 00:10:07.898085: step 51970, total loss = 0.63, predict loss = 0.15 (65.9 examples/sec; 0.061 sec/batch; 100h:13m:05s remains)
INFO - root - 2019-11-04 00:10:08.536709: step 51980, total loss = 0.54, predict loss = 0.12 (64.4 examples/sec; 0.062 sec/batch; 102h:35m:13s remains)
INFO - root - 2019-11-04 00:10:09.178848: step 51990, total loss = 0.50, predict loss = 0.10 (68.9 examples/sec; 0.058 sec/batch; 95h:55m:36s remains)
INFO - root - 2019-11-04 00:10:09.821646: step 52000, total loss = 0.56, predict loss = 0.12 (69.7 examples/sec; 0.057 sec/batch; 94h:46m:26s remains)
INFO - root - 2019-11-04 00:10:10.447748: step 52010, total loss = 0.61, predict loss = 0.14 (79.2 examples/sec; 0.051 sec/batch; 83h:28m:47s remains)
INFO - root - 2019-11-04 00:10:11.080122: step 52020, total loss = 0.52, predict loss = 0.11 (68.3 examples/sec; 0.059 sec/batch; 96h:43m:28s remains)
INFO - root - 2019-11-04 00:10:11.725388: step 52030, total loss = 0.44, predict loss = 0.10 (64.2 examples/sec; 0.062 sec/batch; 102h:53m:52s remains)
INFO - root - 2019-11-04 00:10:12.361437: step 52040, total loss = 0.58, predict loss = 0.13 (68.5 examples/sec; 0.058 sec/batch; 96h:26m:52s remains)
INFO - root - 2019-11-04 00:10:12.937798: step 52050, total loss = 0.37, predict loss = 0.08 (68.3 examples/sec; 0.059 sec/batch; 96h:47m:01s remains)
INFO - root - 2019-11-04 00:10:13.584646: step 52060, total loss = 0.51, predict loss = 0.11 (70.2 examples/sec; 0.057 sec/batch; 94h:11m:57s remains)
INFO - root - 2019-11-04 00:10:14.275248: step 52070, total loss = 0.56, predict loss = 0.12 (74.2 examples/sec; 0.054 sec/batch; 89h:01m:27s remains)
INFO - root - 2019-11-04 00:10:14.914573: step 52080, total loss = 0.58, predict loss = 0.14 (63.9 examples/sec; 0.063 sec/batch; 103h:29m:45s remains)
INFO - root - 2019-11-04 00:10:15.525458: step 52090, total loss = 0.65, predict loss = 0.15 (79.8 examples/sec; 0.050 sec/batch; 82h:46m:15s remains)
INFO - root - 2019-11-04 00:10:16.196216: step 52100, total loss = 0.67, predict loss = 0.16 (64.5 examples/sec; 0.062 sec/batch; 102h:25m:43s remains)
INFO - root - 2019-11-04 00:10:16.852228: step 52110, total loss = 0.71, predict loss = 0.17 (70.3 examples/sec; 0.057 sec/batch; 93h:56m:55s remains)
INFO - root - 2019-11-04 00:10:17.508359: step 52120, total loss = 0.63, predict loss = 0.15 (66.3 examples/sec; 0.060 sec/batch; 99h:42m:38s remains)
INFO - root - 2019-11-04 00:10:18.167724: step 52130, total loss = 0.64, predict loss = 0.15 (66.3 examples/sec; 0.060 sec/batch; 99h:42m:30s remains)
INFO - root - 2019-11-04 00:10:18.808603: step 52140, total loss = 0.62, predict loss = 0.14 (72.6 examples/sec; 0.055 sec/batch; 91h:03m:29s remains)
INFO - root - 2019-11-04 00:10:19.437439: step 52150, total loss = 0.63, predict loss = 0.14 (67.9 examples/sec; 0.059 sec/batch; 97h:18m:53s remains)
INFO - root - 2019-11-04 00:10:20.050788: step 52160, total loss = 0.64, predict loss = 0.15 (73.4 examples/sec; 0.055 sec/batch; 90h:05m:03s remains)
INFO - root - 2019-11-04 00:10:20.644092: step 52170, total loss = 0.64, predict loss = 0.15 (86.1 examples/sec; 0.046 sec/batch; 76h:46m:53s remains)
INFO - root - 2019-11-04 00:10:21.243004: step 52180, total loss = 0.63, predict loss = 0.15 (73.4 examples/sec; 0.054 sec/batch; 89h:58m:38s remains)
INFO - root - 2019-11-04 00:10:21.853977: step 52190, total loss = 0.51, predict loss = 0.11 (69.1 examples/sec; 0.058 sec/batch; 95h:40m:28s remains)
INFO - root - 2019-11-04 00:10:22.498967: step 52200, total loss = 0.76, predict loss = 0.17 (69.6 examples/sec; 0.057 sec/batch; 94h:57m:07s remains)
INFO - root - 2019-11-04 00:10:23.161233: step 52210, total loss = 0.55, predict loss = 0.11 (60.8 examples/sec; 0.066 sec/batch; 108h:46m:29s remains)
INFO - root - 2019-11-04 00:10:23.810987: step 52220, total loss = 0.51, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 94h:15m:20s remains)
INFO - root - 2019-11-04 00:10:24.472780: step 52230, total loss = 0.55, predict loss = 0.13 (62.1 examples/sec; 0.064 sec/batch; 106h:24m:15s remains)
INFO - root - 2019-11-04 00:10:25.129284: step 52240, total loss = 0.68, predict loss = 0.15 (66.7 examples/sec; 0.060 sec/batch; 99h:04m:35s remains)
INFO - root - 2019-11-04 00:10:25.817541: step 52250, total loss = 0.75, predict loss = 0.16 (65.2 examples/sec; 0.061 sec/batch; 101h:20m:41s remains)
INFO - root - 2019-11-04 00:10:26.467941: step 52260, total loss = 0.70, predict loss = 0.16 (81.2 examples/sec; 0.049 sec/batch; 81h:21m:13s remains)
INFO - root - 2019-11-04 00:10:27.084754: step 52270, total loss = 0.50, predict loss = 0.11 (80.7 examples/sec; 0.050 sec/batch; 81h:50m:33s remains)
INFO - root - 2019-11-04 00:10:27.719163: step 52280, total loss = 0.80, predict loss = 0.18 (72.0 examples/sec; 0.056 sec/batch; 91h:48m:01s remains)
INFO - root - 2019-11-04 00:10:28.375004: step 52290, total loss = 0.69, predict loss = 0.16 (65.7 examples/sec; 0.061 sec/batch; 100h:34m:53s remains)
INFO - root - 2019-11-04 00:10:29.051584: step 52300, total loss = 0.39, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 95h:04m:17s remains)
INFO - root - 2019-11-04 00:10:29.681530: step 52310, total loss = 0.60, predict loss = 0.14 (75.6 examples/sec; 0.053 sec/batch; 87h:26m:49s remains)
INFO - root - 2019-11-04 00:10:30.357622: step 52320, total loss = 0.68, predict loss = 0.17 (73.7 examples/sec; 0.054 sec/batch; 89h:38m:05s remains)
INFO - root - 2019-11-04 00:10:30.994352: step 52330, total loss = 0.81, predict loss = 0.20 (69.8 examples/sec; 0.057 sec/batch; 94h:42m:43s remains)
INFO - root - 2019-11-04 00:10:31.614474: step 52340, total loss = 0.48, predict loss = 0.10 (69.6 examples/sec; 0.057 sec/batch; 94h:53m:05s remains)
INFO - root - 2019-11-04 00:10:32.258460: step 52350, total loss = 0.71, predict loss = 0.17 (67.0 examples/sec; 0.060 sec/batch; 98h:38m:50s remains)
INFO - root - 2019-11-04 00:10:32.923370: step 52360, total loss = 0.51, predict loss = 0.12 (61.7 examples/sec; 0.065 sec/batch; 107h:09m:10s remains)
INFO - root - 2019-11-04 00:10:33.593708: step 52370, total loss = 0.46, predict loss = 0.12 (73.3 examples/sec; 0.055 sec/batch; 90h:11m:59s remains)
INFO - root - 2019-11-04 00:10:34.248829: step 52380, total loss = 0.60, predict loss = 0.15 (70.5 examples/sec; 0.057 sec/batch; 93h:47m:46s remains)
INFO - root - 2019-11-04 00:10:34.922616: step 52390, total loss = 0.44, predict loss = 0.10 (64.5 examples/sec; 0.062 sec/batch; 102h:28m:35s remains)
INFO - root - 2019-11-04 00:10:35.604226: step 52400, total loss = 0.50, predict loss = 0.12 (69.9 examples/sec; 0.057 sec/batch; 94h:34m:38s remains)
INFO - root - 2019-11-04 00:10:36.242674: step 52410, total loss = 0.39, predict loss = 0.09 (66.6 examples/sec; 0.060 sec/batch; 99h:10m:25s remains)
INFO - root - 2019-11-04 00:10:36.871305: step 52420, total loss = 0.54, predict loss = 0.14 (70.7 examples/sec; 0.057 sec/batch; 93h:25m:13s remains)
INFO - root - 2019-11-04 00:10:37.526042: step 52430, total loss = 0.36, predict loss = 0.08 (63.5 examples/sec; 0.063 sec/batch; 104h:03m:58s remains)
INFO - root - 2019-11-04 00:10:38.177452: step 52440, total loss = 0.47, predict loss = 0.10 (67.5 examples/sec; 0.059 sec/batch; 97h:54m:16s remains)
INFO - root - 2019-11-04 00:10:38.825426: step 52450, total loss = 0.44, predict loss = 0.10 (73.0 examples/sec; 0.055 sec/batch; 90h:27m:50s remains)
INFO - root - 2019-11-04 00:10:39.458183: step 52460, total loss = 0.46, predict loss = 0.11 (70.0 examples/sec; 0.057 sec/batch; 94h:24m:27s remains)
INFO - root - 2019-11-04 00:10:40.096052: step 52470, total loss = 0.57, predict loss = 0.13 (66.1 examples/sec; 0.060 sec/batch; 99h:55m:37s remains)
INFO - root - 2019-11-04 00:10:40.695473: step 52480, total loss = 0.46, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 96h:34m:06s remains)
INFO - root - 2019-11-04 00:10:41.308570: step 52490, total loss = 0.57, predict loss = 0.14 (72.7 examples/sec; 0.055 sec/batch; 90h:53m:58s remains)
INFO - root - 2019-11-04 00:10:41.891319: step 52500, total loss = 0.57, predict loss = 0.14 (77.0 examples/sec; 0.052 sec/batch; 85h:48m:30s remains)
INFO - root - 2019-11-04 00:10:42.492139: step 52510, total loss = 0.72, predict loss = 0.17 (82.1 examples/sec; 0.049 sec/batch; 80h:29m:33s remains)
INFO - root - 2019-11-04 00:10:43.096899: step 52520, total loss = 0.65, predict loss = 0.15 (77.8 examples/sec; 0.051 sec/batch; 84h:59m:09s remains)
INFO - root - 2019-11-04 00:10:43.742461: step 52530, total loss = 0.69, predict loss = 0.17 (69.8 examples/sec; 0.057 sec/batch; 94h:37m:01s remains)
INFO - root - 2019-11-04 00:10:44.361583: step 52540, total loss = 0.68, predict loss = 0.16 (74.4 examples/sec; 0.054 sec/batch; 88h:51m:27s remains)
INFO - root - 2019-11-04 00:10:44.957425: step 52550, total loss = 0.69, predict loss = 0.15 (67.3 examples/sec; 0.059 sec/batch; 98h:08m:23s remains)
INFO - root - 2019-11-04 00:10:45.597488: step 52560, total loss = 0.68, predict loss = 0.15 (69.7 examples/sec; 0.057 sec/batch; 94h:44m:53s remains)
INFO - root - 2019-11-04 00:10:46.206605: step 52570, total loss = 0.41, predict loss = 0.09 (71.3 examples/sec; 0.056 sec/batch; 92h:44m:15s remains)
INFO - root - 2019-11-04 00:10:46.870853: step 52580, total loss = 0.54, predict loss = 0.12 (65.0 examples/sec; 0.062 sec/batch; 101h:40m:14s remains)
INFO - root - 2019-11-04 00:10:47.533014: step 52590, total loss = 0.67, predict loss = 0.16 (60.2 examples/sec; 0.066 sec/batch; 109h:49m:32s remains)
INFO - root - 2019-11-04 00:10:48.224449: step 52600, total loss = 0.51, predict loss = 0.11 (64.4 examples/sec; 0.062 sec/batch; 102h:39m:55s remains)
INFO - root - 2019-11-04 00:10:48.942612: step 52610, total loss = 0.55, predict loss = 0.13 (74.4 examples/sec; 0.054 sec/batch; 88h:49m:09s remains)
INFO - root - 2019-11-04 00:10:49.624220: step 52620, total loss = 0.71, predict loss = 0.17 (63.0 examples/sec; 0.063 sec/batch; 104h:53m:03s remains)
INFO - root - 2019-11-04 00:10:50.271825: step 52630, total loss = 0.43, predict loss = 0.10 (66.0 examples/sec; 0.061 sec/batch; 100h:07m:49s remains)
INFO - root - 2019-11-04 00:10:50.932666: step 52640, total loss = 0.41, predict loss = 0.09 (72.2 examples/sec; 0.055 sec/batch; 91h:31m:13s remains)
INFO - root - 2019-11-04 00:10:51.594345: step 52650, total loss = 0.27, predict loss = 0.05 (60.5 examples/sec; 0.066 sec/batch; 109h:12m:27s remains)
INFO - root - 2019-11-04 00:10:52.278847: step 52660, total loss = 0.34, predict loss = 0.07 (66.3 examples/sec; 0.060 sec/batch; 99h:37m:19s remains)
INFO - root - 2019-11-04 00:10:52.906608: step 52670, total loss = 0.29, predict loss = 0.06 (69.2 examples/sec; 0.058 sec/batch; 95h:27m:40s remains)
INFO - root - 2019-11-04 00:10:53.567001: step 52680, total loss = 0.32, predict loss = 0.06 (61.6 examples/sec; 0.065 sec/batch; 107h:18m:38s remains)
INFO - root - 2019-11-04 00:10:54.207658: step 52690, total loss = 0.25, predict loss = 0.06 (77.7 examples/sec; 0.051 sec/batch; 85h:03m:33s remains)
INFO - root - 2019-11-04 00:10:54.849847: step 52700, total loss = 0.48, predict loss = 0.11 (77.0 examples/sec; 0.052 sec/batch; 85h:50m:13s remains)
INFO - root - 2019-11-04 00:10:55.505026: step 52710, total loss = 0.39, predict loss = 0.09 (63.2 examples/sec; 0.063 sec/batch; 104h:35m:11s remains)
INFO - root - 2019-11-04 00:10:56.630903: step 52720, total loss = 0.51, predict loss = 0.12 (66.3 examples/sec; 0.060 sec/batch; 99h:43m:06s remains)
INFO - root - 2019-11-04 00:10:57.301146: step 52730, total loss = 0.45, predict loss = 0.10 (67.2 examples/sec; 0.060 sec/batch; 98h:21m:25s remains)
INFO - root - 2019-11-04 00:10:57.936675: step 52740, total loss = 0.52, predict loss = 0.12 (69.3 examples/sec; 0.058 sec/batch; 95h:20m:44s remains)
INFO - root - 2019-11-04 00:10:58.532522: step 52750, total loss = 0.43, predict loss = 0.09 (83.2 examples/sec; 0.048 sec/batch; 79h:26m:19s remains)
INFO - root - 2019-11-04 00:10:59.188122: step 52760, total loss = 0.32, predict loss = 0.07 (64.5 examples/sec; 0.062 sec/batch; 102h:30m:53s remains)
INFO - root - 2019-11-04 00:10:59.839785: step 52770, total loss = 0.48, predict loss = 0.12 (78.2 examples/sec; 0.051 sec/batch; 84h:32m:35s remains)
INFO - root - 2019-11-04 00:11:00.465045: step 52780, total loss = 0.54, predict loss = 0.12 (74.2 examples/sec; 0.054 sec/batch; 89h:05m:21s remains)
INFO - root - 2019-11-04 00:11:01.094617: step 52790, total loss = 0.60, predict loss = 0.14 (62.3 examples/sec; 0.064 sec/batch; 105h:58m:58s remains)
INFO - root - 2019-11-04 00:11:01.789286: step 52800, total loss = 0.51, predict loss = 0.11 (59.6 examples/sec; 0.067 sec/batch; 110h:55m:31s remains)
INFO - root - 2019-11-04 00:11:02.485993: step 52810, total loss = 0.70, predict loss = 0.16 (67.6 examples/sec; 0.059 sec/batch; 97h:44m:14s remains)
INFO - root - 2019-11-04 00:11:03.166093: step 52820, total loss = 0.49, predict loss = 0.11 (64.6 examples/sec; 0.062 sec/batch; 102h:15m:24s remains)
INFO - root - 2019-11-04 00:11:03.829474: step 52830, total loss = 0.75, predict loss = 0.19 (71.9 examples/sec; 0.056 sec/batch; 91h:56m:19s remains)
INFO - root - 2019-11-04 00:11:04.480876: step 52840, total loss = 0.45, predict loss = 0.11 (68.9 examples/sec; 0.058 sec/batch; 95h:50m:30s remains)
INFO - root - 2019-11-04 00:11:05.106117: step 52850, total loss = 0.69, predict loss = 0.17 (75.5 examples/sec; 0.053 sec/batch; 87h:31m:18s remains)
INFO - root - 2019-11-04 00:11:05.776757: step 52860, total loss = 0.51, predict loss = 0.11 (72.6 examples/sec; 0.055 sec/batch; 91h:00m:56s remains)
INFO - root - 2019-11-04 00:11:06.456011: step 52870, total loss = 0.59, predict loss = 0.14 (67.1 examples/sec; 0.060 sec/batch; 98h:25m:13s remains)
INFO - root - 2019-11-04 00:11:07.110097: step 52880, total loss = 0.31, predict loss = 0.07 (63.5 examples/sec; 0.063 sec/batch; 104h:04m:17s remains)
INFO - root - 2019-11-04 00:11:07.752536: step 52890, total loss = 0.28, predict loss = 0.06 (74.7 examples/sec; 0.054 sec/batch; 88h:28m:55s remains)
INFO - root - 2019-11-04 00:11:08.389044: step 52900, total loss = 0.36, predict loss = 0.08 (64.3 examples/sec; 0.062 sec/batch; 102h:47m:58s remains)
INFO - root - 2019-11-04 00:11:09.016437: step 52910, total loss = 0.41, predict loss = 0.10 (77.1 examples/sec; 0.052 sec/batch; 85h:39m:02s remains)
INFO - root - 2019-11-04 00:11:09.656608: step 52920, total loss = 0.35, predict loss = 0.08 (82.9 examples/sec; 0.048 sec/batch; 79h:44m:38s remains)
INFO - root - 2019-11-04 00:11:10.266165: step 52930, total loss = 0.48, predict loss = 0.11 (82.9 examples/sec; 0.048 sec/batch; 79h:43m:24s remains)
INFO - root - 2019-11-04 00:11:10.916107: step 52940, total loss = 0.60, predict loss = 0.15 (76.8 examples/sec; 0.052 sec/batch; 86h:03m:18s remains)
INFO - root - 2019-11-04 00:11:11.512303: step 52950, total loss = 0.39, predict loss = 0.09 (73.3 examples/sec; 0.055 sec/batch; 90h:10m:40s remains)
INFO - root - 2019-11-04 00:11:12.146036: step 52960, total loss = 0.44, predict loss = 0.10 (68.5 examples/sec; 0.058 sec/batch; 96h:28m:32s remains)
INFO - root - 2019-11-04 00:11:12.789087: step 52970, total loss = 0.60, predict loss = 0.14 (67.7 examples/sec; 0.059 sec/batch; 97h:37m:39s remains)
INFO - root - 2019-11-04 00:11:13.436504: step 52980, total loss = 0.52, predict loss = 0.11 (69.6 examples/sec; 0.057 sec/batch; 94h:53m:36s remains)
INFO - root - 2019-11-04 00:11:14.065534: step 52990, total loss = 0.60, predict loss = 0.14 (59.4 examples/sec; 0.067 sec/batch; 111h:09m:31s remains)
INFO - root - 2019-11-04 00:11:14.702901: step 53000, total loss = 0.41, predict loss = 0.09 (68.6 examples/sec; 0.058 sec/batch; 96h:17m:39s remains)
INFO - root - 2019-11-04 00:11:15.376839: step 53010, total loss = 0.46, predict loss = 0.10 (70.0 examples/sec; 0.057 sec/batch; 94h:25m:29s remains)
INFO - root - 2019-11-04 00:11:16.015944: step 53020, total loss = 0.33, predict loss = 0.06 (62.8 examples/sec; 0.064 sec/batch; 105h:15m:42s remains)
INFO - root - 2019-11-04 00:11:16.648803: step 53030, total loss = 0.61, predict loss = 0.14 (83.1 examples/sec; 0.048 sec/batch; 79h:33m:08s remains)
INFO - root - 2019-11-04 00:11:17.268416: step 53040, total loss = 0.50, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 100h:16m:45s remains)
INFO - root - 2019-11-04 00:11:17.875566: step 53050, total loss = 0.40, predict loss = 0.08 (74.8 examples/sec; 0.053 sec/batch; 88h:17m:13s remains)
INFO - root - 2019-11-04 00:11:18.481787: step 53060, total loss = 0.45, predict loss = 0.10 (72.4 examples/sec; 0.055 sec/batch; 91h:14m:14s remains)
INFO - root - 2019-11-04 00:11:19.110104: step 53070, total loss = 0.50, predict loss = 0.11 (70.0 examples/sec; 0.057 sec/batch; 94h:25m:23s remains)
INFO - root - 2019-11-04 00:11:19.733605: step 53080, total loss = 0.54, predict loss = 0.13 (63.5 examples/sec; 0.063 sec/batch; 104h:03m:48s remains)
INFO - root - 2019-11-04 00:11:20.354124: step 53090, total loss = 0.69, predict loss = 0.17 (72.0 examples/sec; 0.056 sec/batch; 91h:44m:09s remains)
INFO - root - 2019-11-04 00:11:20.981626: step 53100, total loss = 0.66, predict loss = 0.16 (77.1 examples/sec; 0.052 sec/batch; 85h:41m:09s remains)
INFO - root - 2019-11-04 00:11:21.614546: step 53110, total loss = 0.59, predict loss = 0.14 (63.2 examples/sec; 0.063 sec/batch; 104h:37m:14s remains)
INFO - root - 2019-11-04 00:11:22.206265: step 53120, total loss = 0.53, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 94h:02m:00s remains)
INFO - root - 2019-11-04 00:11:22.811708: step 53130, total loss = 0.48, predict loss = 0.10 (64.9 examples/sec; 0.062 sec/batch; 101h:49m:53s remains)
INFO - root - 2019-11-04 00:11:23.443346: step 53140, total loss = 0.62, predict loss = 0.15 (70.4 examples/sec; 0.057 sec/batch; 93h:47m:40s remains)
INFO - root - 2019-11-04 00:11:24.091611: step 53150, total loss = 0.28, predict loss = 0.06 (72.9 examples/sec; 0.055 sec/batch; 90h:36m:40s remains)
INFO - root - 2019-11-04 00:11:24.731839: step 53160, total loss = 0.62, predict loss = 0.14 (69.0 examples/sec; 0.058 sec/batch; 95h:45m:37s remains)
INFO - root - 2019-11-04 00:11:25.375316: step 53170, total loss = 0.46, predict loss = 0.11 (73.8 examples/sec; 0.054 sec/batch; 89h:30m:58s remains)
INFO - root - 2019-11-04 00:11:25.995799: step 53180, total loss = 0.54, predict loss = 0.12 (68.1 examples/sec; 0.059 sec/batch; 96h:58m:41s remains)
INFO - root - 2019-11-04 00:11:26.620322: step 53190, total loss = 0.52, predict loss = 0.11 (70.3 examples/sec; 0.057 sec/batch; 93h:58m:35s remains)
INFO - root - 2019-11-04 00:11:27.226002: step 53200, total loss = 0.43, predict loss = 0.10 (65.6 examples/sec; 0.061 sec/batch; 100h:39m:25s remains)
INFO - root - 2019-11-04 00:11:27.840865: step 53210, total loss = 0.41, predict loss = 0.09 (81.8 examples/sec; 0.049 sec/batch; 80h:47m:53s remains)
INFO - root - 2019-11-04 00:11:28.458659: step 53220, total loss = 0.64, predict loss = 0.15 (80.9 examples/sec; 0.049 sec/batch; 81h:40m:42s remains)
INFO - root - 2019-11-04 00:11:29.113717: step 53230, total loss = 0.46, predict loss = 0.10 (71.1 examples/sec; 0.056 sec/batch; 92h:53m:20s remains)
INFO - root - 2019-11-04 00:11:29.756117: step 53240, total loss = 1.18, predict loss = 0.27 (71.5 examples/sec; 0.056 sec/batch; 92h:28m:05s remains)
INFO - root - 2019-11-04 00:11:30.412339: step 53250, total loss = 0.71, predict loss = 0.17 (61.5 examples/sec; 0.065 sec/batch; 107h:24m:39s remains)
INFO - root - 2019-11-04 00:11:31.082928: step 53260, total loss = 0.82, predict loss = 0.20 (69.5 examples/sec; 0.058 sec/batch; 95h:06m:46s remains)
INFO - root - 2019-11-04 00:11:31.782549: step 53270, total loss = 0.73, predict loss = 0.17 (58.3 examples/sec; 0.069 sec/batch; 113h:20m:20s remains)
INFO - root - 2019-11-04 00:11:32.404388: step 53280, total loss = 0.93, predict loss = 0.22 (73.7 examples/sec; 0.054 sec/batch; 89h:40m:36s remains)
INFO - root - 2019-11-04 00:11:33.022897: step 53290, total loss = 0.82, predict loss = 0.19 (72.2 examples/sec; 0.055 sec/batch; 91h:34m:42s remains)
INFO - root - 2019-11-04 00:11:33.647124: step 53300, total loss = 0.54, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 93h:43m:41s remains)
INFO - root - 2019-11-04 00:11:34.301064: step 53310, total loss = 0.74, predict loss = 0.17 (68.3 examples/sec; 0.059 sec/batch; 96h:47m:10s remains)
INFO - root - 2019-11-04 00:11:34.990527: step 53320, total loss = 0.69, predict loss = 0.16 (61.8 examples/sec; 0.065 sec/batch; 106h:51m:28s remains)
INFO - root - 2019-11-04 00:11:35.626096: step 53330, total loss = 0.64, predict loss = 0.15 (77.3 examples/sec; 0.052 sec/batch; 85h:26m:56s remains)
INFO - root - 2019-11-04 00:11:36.324469: step 53340, total loss = 0.68, predict loss = 0.16 (61.6 examples/sec; 0.065 sec/batch; 107h:19m:34s remains)
INFO - root - 2019-11-04 00:11:36.945642: step 53350, total loss = 0.47, predict loss = 0.11 (82.2 examples/sec; 0.049 sec/batch; 80h:22m:45s remains)
INFO - root - 2019-11-04 00:11:37.553804: step 53360, total loss = 0.64, predict loss = 0.14 (78.3 examples/sec; 0.051 sec/batch; 84h:22m:28s remains)
INFO - root - 2019-11-04 00:11:38.161171: step 53370, total loss = 0.53, predict loss = 0.12 (81.5 examples/sec; 0.049 sec/batch; 81h:04m:19s remains)
INFO - root - 2019-11-04 00:11:38.780537: step 53380, total loss = 0.62, predict loss = 0.14 (66.8 examples/sec; 0.060 sec/batch; 98h:57m:22s remains)
INFO - root - 2019-11-04 00:11:39.413351: step 53390, total loss = 0.50, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 91h:03m:11s remains)
INFO - root - 2019-11-04 00:11:40.071332: step 53400, total loss = 0.49, predict loss = 0.11 (64.0 examples/sec; 0.063 sec/batch; 103h:15m:41s remains)
INFO - root - 2019-11-04 00:11:40.746792: step 53410, total loss = 0.51, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 98h:28m:14s remains)
INFO - root - 2019-11-04 00:11:41.395827: step 53420, total loss = 0.58, predict loss = 0.13 (66.6 examples/sec; 0.060 sec/batch; 99h:13m:01s remains)
INFO - root - 2019-11-04 00:11:42.050928: step 53430, total loss = 0.43, predict loss = 0.09 (66.0 examples/sec; 0.061 sec/batch; 100h:09m:32s remains)
INFO - root - 2019-11-04 00:11:42.707445: step 53440, total loss = 0.51, predict loss = 0.12 (68.6 examples/sec; 0.058 sec/batch; 96h:15m:59s remains)
INFO - root - 2019-11-04 00:11:43.359437: step 53450, total loss = 0.72, predict loss = 0.16 (67.6 examples/sec; 0.059 sec/batch; 97h:47m:50s remains)
INFO - root - 2019-11-04 00:11:43.962341: step 53460, total loss = 0.57, predict loss = 0.13 (78.9 examples/sec; 0.051 sec/batch; 83h:42m:47s remains)
INFO - root - 2019-11-04 00:11:44.596053: step 53470, total loss = 0.48, predict loss = 0.11 (70.2 examples/sec; 0.057 sec/batch; 94h:07m:33s remains)
INFO - root - 2019-11-04 00:11:45.168883: step 53480, total loss = 0.63, predict loss = 0.15 (63.0 examples/sec; 0.063 sec/batch; 104h:50m:22s remains)
INFO - root - 2019-11-04 00:11:45.796798: step 53490, total loss = 0.35, predict loss = 0.07 (65.3 examples/sec; 0.061 sec/batch; 101h:08m:40s remains)
INFO - root - 2019-11-04 00:11:46.430935: step 53500, total loss = 0.49, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 97h:19m:49s remains)
INFO - root - 2019-11-04 00:11:47.090058: step 53510, total loss = 0.41, predict loss = 0.10 (61.1 examples/sec; 0.065 sec/batch; 108h:03m:24s remains)
INFO - root - 2019-11-04 00:11:47.738050: step 53520, total loss = 0.47, predict loss = 0.11 (69.1 examples/sec; 0.058 sec/batch; 95h:35m:09s remains)
INFO - root - 2019-11-04 00:11:48.354814: step 53530, total loss = 0.47, predict loss = 0.10 (66.9 examples/sec; 0.060 sec/batch; 98h:49m:56s remains)
INFO - root - 2019-11-04 00:11:48.982435: step 53540, total loss = 0.53, predict loss = 0.12 (65.6 examples/sec; 0.061 sec/batch; 100h:38m:59s remains)
INFO - root - 2019-11-04 00:11:49.641616: step 53550, total loss = 0.52, predict loss = 0.12 (60.8 examples/sec; 0.066 sec/batch; 108h:43m:43s remains)
INFO - root - 2019-11-04 00:11:50.295650: step 53560, total loss = 0.44, predict loss = 0.10 (69.1 examples/sec; 0.058 sec/batch; 95h:39m:43s remains)
INFO - root - 2019-11-04 00:11:50.993109: step 53570, total loss = 0.48, predict loss = 0.11 (60.9 examples/sec; 0.066 sec/batch; 108h:31m:53s remains)
INFO - root - 2019-11-04 00:11:51.648679: step 53580, total loss = 0.46, predict loss = 0.10 (70.9 examples/sec; 0.056 sec/batch; 93h:11m:02s remains)
INFO - root - 2019-11-04 00:11:52.262178: step 53590, total loss = 0.41, predict loss = 0.09 (74.7 examples/sec; 0.054 sec/batch; 88h:26m:51s remains)
INFO - root - 2019-11-04 00:11:52.880104: step 53600, total loss = 0.42, predict loss = 0.08 (67.6 examples/sec; 0.059 sec/batch; 97h:47m:07s remains)
INFO - root - 2019-11-04 00:11:53.501629: step 53610, total loss = 0.55, predict loss = 0.13 (72.9 examples/sec; 0.055 sec/batch; 90h:35m:48s remains)
INFO - root - 2019-11-04 00:11:54.148629: step 53620, total loss = 0.58, predict loss = 0.14 (74.8 examples/sec; 0.054 sec/batch; 88h:22m:23s remains)
INFO - root - 2019-11-04 00:11:54.811349: step 53630, total loss = 0.54, predict loss = 0.12 (65.6 examples/sec; 0.061 sec/batch; 100h:44m:18s remains)
INFO - root - 2019-11-04 00:11:55.471835: step 53640, total loss = 0.54, predict loss = 0.13 (66.2 examples/sec; 0.060 sec/batch; 99h:49m:21s remains)
INFO - root - 2019-11-04 00:11:56.148225: step 53650, total loss = 0.65, predict loss = 0.15 (71.4 examples/sec; 0.056 sec/batch; 92h:30m:35s remains)
INFO - root - 2019-11-04 00:11:56.827453: step 53660, total loss = 0.75, predict loss = 0.18 (72.6 examples/sec; 0.055 sec/batch; 91h:01m:10s remains)
INFO - root - 2019-11-04 00:11:57.450856: step 53670, total loss = 0.84, predict loss = 0.20 (71.4 examples/sec; 0.056 sec/batch; 92h:29m:58s remains)
INFO - root - 2019-11-04 00:11:58.110851: step 53680, total loss = 0.54, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 104h:07m:13s remains)
INFO - root - 2019-11-04 00:11:58.747436: step 53690, total loss = 0.65, predict loss = 0.16 (73.7 examples/sec; 0.054 sec/batch; 89h:36m:10s remains)
INFO - root - 2019-11-04 00:11:59.415089: step 53700, total loss = 0.75, predict loss = 0.19 (64.0 examples/sec; 0.063 sec/batch; 103h:14m:27s remains)
INFO - root - 2019-11-04 00:12:00.055310: step 53710, total loss = 0.70, predict loss = 0.16 (63.7 examples/sec; 0.063 sec/batch; 103h:45m:49s remains)
INFO - root - 2019-11-04 00:12:00.705551: step 53720, total loss = 0.67, predict loss = 0.16 (69.8 examples/sec; 0.057 sec/batch; 94h:41m:31s remains)
INFO - root - 2019-11-04 00:12:01.345437: step 53730, total loss = 0.62, predict loss = 0.15 (84.4 examples/sec; 0.047 sec/batch; 78h:15m:38s remains)
INFO - root - 2019-11-04 00:12:01.961948: step 53740, total loss = 0.70, predict loss = 0.15 (72.1 examples/sec; 0.055 sec/batch; 91h:35m:19s remains)
INFO - root - 2019-11-04 00:12:02.587736: step 53750, total loss = 0.66, predict loss = 0.16 (79.2 examples/sec; 0.050 sec/batch; 83h:22m:43s remains)
INFO - root - 2019-11-04 00:12:03.193383: step 53760, total loss = 0.65, predict loss = 0.15 (68.2 examples/sec; 0.059 sec/batch; 96h:52m:01s remains)
INFO - root - 2019-11-04 00:12:03.812597: step 53770, total loss = 0.51, predict loss = 0.12 (74.4 examples/sec; 0.054 sec/batch; 88h:49m:23s remains)
INFO - root - 2019-11-04 00:12:04.421633: step 53780, total loss = 0.66, predict loss = 0.15 (76.1 examples/sec; 0.053 sec/batch; 86h:47m:11s remains)
INFO - root - 2019-11-04 00:12:05.036179: step 53790, total loss = 0.53, predict loss = 0.12 (78.9 examples/sec; 0.051 sec/batch; 83h:41m:23s remains)
INFO - root - 2019-11-04 00:12:05.641875: step 53800, total loss = 0.44, predict loss = 0.10 (80.2 examples/sec; 0.050 sec/batch; 82h:25m:28s remains)
INFO - root - 2019-11-04 00:12:06.295709: step 53810, total loss = 0.45, predict loss = 0.10 (59.3 examples/sec; 0.067 sec/batch; 111h:26m:42s remains)
INFO - root - 2019-11-04 00:12:06.915905: step 53820, total loss = 0.46, predict loss = 0.10 (79.5 examples/sec; 0.050 sec/batch; 83h:03m:48s remains)
INFO - root - 2019-11-04 00:12:07.520876: step 53830, total loss = 0.41, predict loss = 0.09 (77.3 examples/sec; 0.052 sec/batch; 85h:29m:36s remains)
INFO - root - 2019-11-04 00:12:08.175439: step 53840, total loss = 0.47, predict loss = 0.11 (72.9 examples/sec; 0.055 sec/batch; 90h:36m:27s remains)
INFO - root - 2019-11-04 00:12:08.837602: step 53850, total loss = 0.60, predict loss = 0.14 (70.1 examples/sec; 0.057 sec/batch; 94h:16m:05s remains)
INFO - root - 2019-11-04 00:12:09.542099: step 53860, total loss = 0.48, predict loss = 0.11 (59.8 examples/sec; 0.067 sec/batch; 110h:28m:51s remains)
INFO - root - 2019-11-04 00:12:10.184488: step 53870, total loss = 0.52, predict loss = 0.11 (69.1 examples/sec; 0.058 sec/batch; 95h:37m:39s remains)
INFO - root - 2019-11-04 00:12:10.773024: step 53880, total loss = 0.58, predict loss = 0.14 (73.8 examples/sec; 0.054 sec/batch; 89h:29m:45s remains)
INFO - root - 2019-11-04 00:12:11.415419: step 53890, total loss = 0.87, predict loss = 0.19 (77.1 examples/sec; 0.052 sec/batch; 85h:44m:40s remains)
INFO - root - 2019-11-04 00:12:12.084228: step 53900, total loss = 0.66, predict loss = 0.15 (72.9 examples/sec; 0.055 sec/batch; 90h:37m:48s remains)
INFO - root - 2019-11-04 00:12:12.732533: step 53910, total loss = 0.79, predict loss = 0.19 (74.6 examples/sec; 0.054 sec/batch; 88h:34m:31s remains)
INFO - root - 2019-11-04 00:12:13.373245: step 53920, total loss = 0.69, predict loss = 0.16 (76.7 examples/sec; 0.052 sec/batch; 86h:08m:19s remains)
INFO - root - 2019-11-04 00:12:14.010775: step 53930, total loss = 0.65, predict loss = 0.16 (67.6 examples/sec; 0.059 sec/batch; 97h:40m:39s remains)
INFO - root - 2019-11-04 00:12:14.654418: step 53940, total loss = 0.72, predict loss = 0.18 (62.3 examples/sec; 0.064 sec/batch; 106h:05m:10s remains)
INFO - root - 2019-11-04 00:12:15.376703: step 53950, total loss = 0.66, predict loss = 0.16 (69.2 examples/sec; 0.058 sec/batch; 95h:32m:21s remains)
INFO - root - 2019-11-04 00:12:16.041911: step 53960, total loss = 0.55, predict loss = 0.14 (65.8 examples/sec; 0.061 sec/batch; 100h:24m:08s remains)
INFO - root - 2019-11-04 00:12:16.684022: step 53970, total loss = 0.53, predict loss = 0.13 (72.0 examples/sec; 0.056 sec/batch; 91h:45m:12s remains)
INFO - root - 2019-11-04 00:12:17.328163: step 53980, total loss = 0.65, predict loss = 0.16 (65.3 examples/sec; 0.061 sec/batch; 101h:08m:05s remains)
INFO - root - 2019-11-04 00:12:18.019677: step 53990, total loss = 0.57, predict loss = 0.14 (70.5 examples/sec; 0.057 sec/batch; 93h:40m:35s remains)
INFO - root - 2019-11-04 00:12:18.655186: step 54000, total loss = 0.54, predict loss = 0.13 (72.9 examples/sec; 0.055 sec/batch; 90h:37m:37s remains)
INFO - root - 2019-11-04 00:12:19.318019: step 54010, total loss = 0.51, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 99h:21m:26s remains)
INFO - root - 2019-11-04 00:12:19.932651: step 54020, total loss = 0.58, predict loss = 0.14 (67.2 examples/sec; 0.060 sec/batch; 98h:19m:41s remains)
INFO - root - 2019-11-04 00:12:20.552788: step 54030, total loss = 0.48, predict loss = 0.11 (71.5 examples/sec; 0.056 sec/batch; 92h:24m:47s remains)
INFO - root - 2019-11-04 00:12:21.159803: step 54040, total loss = 0.48, predict loss = 0.10 (75.9 examples/sec; 0.053 sec/batch; 87h:00m:13s remains)
INFO - root - 2019-11-04 00:12:21.794214: step 54050, total loss = 0.66, predict loss = 0.15 (68.1 examples/sec; 0.059 sec/batch; 97h:04m:08s remains)
INFO - root - 2019-11-04 00:12:22.464160: step 54060, total loss = 0.73, predict loss = 0.17 (63.2 examples/sec; 0.063 sec/batch; 104h:30m:01s remains)
INFO - root - 2019-11-04 00:12:23.083551: step 54070, total loss = 0.48, predict loss = 0.11 (73.5 examples/sec; 0.054 sec/batch; 89h:52m:03s remains)
INFO - root - 2019-11-04 00:12:23.720505: step 54080, total loss = 0.44, predict loss = 0.09 (62.6 examples/sec; 0.064 sec/batch; 105h:28m:16s remains)
INFO - root - 2019-11-04 00:12:24.358672: step 54090, total loss = 0.46, predict loss = 0.11 (70.0 examples/sec; 0.057 sec/batch; 94h:23m:33s remains)
INFO - root - 2019-11-04 00:12:24.971427: step 54100, total loss = 0.54, predict loss = 0.12 (74.7 examples/sec; 0.054 sec/batch; 88h:29m:23s remains)
INFO - root - 2019-11-04 00:12:25.628582: step 54110, total loss = 0.40, predict loss = 0.08 (71.5 examples/sec; 0.056 sec/batch; 92h:25m:07s remains)
INFO - root - 2019-11-04 00:12:26.277470: step 54120, total loss = 0.53, predict loss = 0.13 (68.6 examples/sec; 0.058 sec/batch; 96h:16m:09s remains)
INFO - root - 2019-11-04 00:12:26.955463: step 54130, total loss = 0.44, predict loss = 0.10 (63.7 examples/sec; 0.063 sec/batch; 103h:46m:38s remains)
INFO - root - 2019-11-04 00:12:27.670883: step 54140, total loss = 0.49, predict loss = 0.11 (78.7 examples/sec; 0.051 sec/batch; 83h:58m:12s remains)
INFO - root - 2019-11-04 00:12:28.334780: step 54150, total loss = 0.57, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 92h:59m:53s remains)
INFO - root - 2019-11-04 00:12:28.980338: step 54160, total loss = 0.57, predict loss = 0.13 (82.8 examples/sec; 0.048 sec/batch; 79h:49m:07s remains)
INFO - root - 2019-11-04 00:12:29.559372: step 54170, total loss = 0.56, predict loss = 0.13 (83.8 examples/sec; 0.048 sec/batch; 78h:48m:46s remains)
INFO - root - 2019-11-04 00:12:30.224723: step 54180, total loss = 0.51, predict loss = 0.11 (64.8 examples/sec; 0.062 sec/batch; 102h:01m:34s remains)
INFO - root - 2019-11-04 00:12:30.836031: step 54190, total loss = 0.48, predict loss = 0.10 (67.5 examples/sec; 0.059 sec/batch; 97h:52m:11s remains)
INFO - root - 2019-11-04 00:12:31.525540: step 54200, total loss = 0.50, predict loss = 0.12 (77.1 examples/sec; 0.052 sec/batch; 85h:43m:49s remains)
INFO - root - 2019-11-04 00:12:32.159237: step 54210, total loss = 0.62, predict loss = 0.14 (68.3 examples/sec; 0.059 sec/batch; 96h:43m:24s remains)
INFO - root - 2019-11-04 00:12:32.829498: step 54220, total loss = 0.50, predict loss = 0.12 (63.3 examples/sec; 0.063 sec/batch; 104h:23m:07s remains)
INFO - root - 2019-11-04 00:12:33.441104: step 54230, total loss = 0.57, predict loss = 0.14 (74.9 examples/sec; 0.053 sec/batch; 88h:13m:39s remains)
INFO - root - 2019-11-04 00:12:34.054776: step 54240, total loss = 0.58, predict loss = 0.14 (73.4 examples/sec; 0.054 sec/batch; 89h:59m:18s remains)
INFO - root - 2019-11-04 00:12:34.680058: step 54250, total loss = 0.60, predict loss = 0.13 (74.1 examples/sec; 0.054 sec/batch; 89h:06m:46s remains)
INFO - root - 2019-11-04 00:12:35.361194: step 54260, total loss = 0.53, predict loss = 0.13 (59.2 examples/sec; 0.068 sec/batch; 111h:31m:11s remains)
INFO - root - 2019-11-04 00:12:35.980303: step 54270, total loss = 0.54, predict loss = 0.13 (73.5 examples/sec; 0.054 sec/batch; 89h:50m:02s remains)
INFO - root - 2019-11-04 00:12:36.619454: step 54280, total loss = 0.85, predict loss = 0.21 (71.1 examples/sec; 0.056 sec/batch; 92h:58m:45s remains)
INFO - root - 2019-11-04 00:12:37.272765: step 54290, total loss = 0.88, predict loss = 0.21 (69.0 examples/sec; 0.058 sec/batch; 95h:47m:30s remains)
INFO - root - 2019-11-04 00:12:37.890489: step 54300, total loss = 0.70, predict loss = 0.17 (64.1 examples/sec; 0.062 sec/batch; 103h:03m:09s remains)
INFO - root - 2019-11-04 00:12:38.533277: step 54310, total loss = 0.75, predict loss = 0.17 (73.6 examples/sec; 0.054 sec/batch; 89h:47m:31s remains)
INFO - root - 2019-11-04 00:12:39.227668: step 54320, total loss = 0.68, predict loss = 0.17 (55.2 examples/sec; 0.072 sec/batch; 119h:37m:07s remains)
INFO - root - 2019-11-04 00:12:39.977543: step 54330, total loss = 0.53, predict loss = 0.12 (55.8 examples/sec; 0.072 sec/batch; 118h:23m:58s remains)
INFO - root - 2019-11-04 00:12:40.733917: step 54340, total loss = 0.87, predict loss = 0.20 (54.5 examples/sec; 0.073 sec/batch; 121h:14m:32s remains)
INFO - root - 2019-11-04 00:12:41.562173: step 54350, total loss = 0.70, predict loss = 0.17 (62.0 examples/sec; 0.065 sec/batch; 106h:32m:04s remains)
INFO - root - 2019-11-04 00:12:42.325205: step 54360, total loss = 0.78, predict loss = 0.18 (57.5 examples/sec; 0.070 sec/batch; 114h:54m:52s remains)
INFO - root - 2019-11-04 00:12:42.936319: step 54370, total loss = 0.69, predict loss = 0.17 (71.4 examples/sec; 0.056 sec/batch; 92h:32m:16s remains)
INFO - root - 2019-11-04 00:12:43.557546: step 54380, total loss = 0.68, predict loss = 0.17 (67.5 examples/sec; 0.059 sec/batch; 97h:52m:47s remains)
INFO - root - 2019-11-04 00:12:44.175960: step 54390, total loss = 0.64, predict loss = 0.15 (61.0 examples/sec; 0.066 sec/batch; 108h:19m:00s remains)
INFO - root - 2019-11-04 00:12:44.792119: step 54400, total loss = 0.53, predict loss = 0.12 (72.4 examples/sec; 0.055 sec/batch; 91h:14m:01s remains)
INFO - root - 2019-11-04 00:12:45.451272: step 54410, total loss = 0.65, predict loss = 0.15 (57.5 examples/sec; 0.070 sec/batch; 114h:58m:04s remains)
INFO - root - 2019-11-04 00:12:46.121179: step 54420, total loss = 0.48, predict loss = 0.10 (71.6 examples/sec; 0.056 sec/batch; 92h:19m:44s remains)
INFO - root - 2019-11-04 00:12:46.708891: step 54430, total loss = 0.60, predict loss = 0.14 (84.4 examples/sec; 0.047 sec/batch; 78h:18m:27s remains)
INFO - root - 2019-11-04 00:12:47.329547: step 54440, total loss = 0.54, predict loss = 0.12 (73.4 examples/sec; 0.054 sec/batch; 89h:57m:05s remains)
INFO - root - 2019-11-04 00:12:47.974637: step 54450, total loss = 0.55, predict loss = 0.15 (77.9 examples/sec; 0.051 sec/batch; 84h:50m:45s remains)
INFO - root - 2019-11-04 00:12:48.592131: step 54460, total loss = 0.55, predict loss = 0.13 (67.6 examples/sec; 0.059 sec/batch; 97h:41m:18s remains)
INFO - root - 2019-11-04 00:12:49.226777: step 54470, total loss = 0.60, predict loss = 0.15 (68.4 examples/sec; 0.059 sec/batch; 96h:38m:33s remains)
INFO - root - 2019-11-04 00:12:49.914844: step 54480, total loss = 0.52, predict loss = 0.12 (63.3 examples/sec; 0.063 sec/batch; 104h:21m:03s remains)
INFO - root - 2019-11-04 00:12:50.515405: step 54490, total loss = 0.38, predict loss = 0.08 (70.4 examples/sec; 0.057 sec/batch; 93h:53m:10s remains)
INFO - root - 2019-11-04 00:12:51.116274: step 54500, total loss = 0.54, predict loss = 0.13 (75.5 examples/sec; 0.053 sec/batch; 87h:33m:05s remains)
INFO - root - 2019-11-04 00:12:51.785708: step 54510, total loss = 0.61, predict loss = 0.14 (73.8 examples/sec; 0.054 sec/batch; 89h:29m:45s remains)
INFO - root - 2019-11-04 00:12:52.409833: step 54520, total loss = 0.56, predict loss = 0.13 (61.0 examples/sec; 0.066 sec/batch; 108h:12m:35s remains)
INFO - root - 2019-11-04 00:12:52.996350: step 54530, total loss = 0.37, predict loss = 0.08 (95.8 examples/sec; 0.042 sec/batch; 68h:56m:12s remains)
INFO - root - 2019-11-04 00:12:53.456005: step 54540, total loss = 0.61, predict loss = 0.13 (84.9 examples/sec; 0.047 sec/batch; 77h:48m:29s remains)
INFO - root - 2019-11-04 00:12:54.477567: step 54550, total loss = 0.65, predict loss = 0.14 (78.8 examples/sec; 0.051 sec/batch; 83h:51m:19s remains)
INFO - root - 2019-11-04 00:12:55.106179: step 54560, total loss = 0.36, predict loss = 0.07 (70.4 examples/sec; 0.057 sec/batch; 93h:51m:48s remains)
INFO - root - 2019-11-04 00:12:55.726837: step 54570, total loss = 0.60, predict loss = 0.14 (71.9 examples/sec; 0.056 sec/batch; 91h:51m:31s remains)
INFO - root - 2019-11-04 00:12:56.449117: step 54580, total loss = 0.62, predict loss = 0.15 (55.9 examples/sec; 0.072 sec/batch; 118h:13m:43s remains)
INFO - root - 2019-11-04 00:12:57.071640: step 54590, total loss = 0.45, predict loss = 0.10 (69.5 examples/sec; 0.058 sec/batch; 95h:00m:06s remains)
INFO - root - 2019-11-04 00:12:57.693433: step 54600, total loss = 0.41, predict loss = 0.10 (68.7 examples/sec; 0.058 sec/batch; 96h:10m:04s remains)
INFO - root - 2019-11-04 00:12:58.361249: step 54610, total loss = 0.68, predict loss = 0.16 (69.2 examples/sec; 0.058 sec/batch; 95h:25m:38s remains)
INFO - root - 2019-11-04 00:12:58.992695: step 54620, total loss = 0.91, predict loss = 0.21 (82.6 examples/sec; 0.048 sec/batch; 80h:01m:14s remains)
INFO - root - 2019-11-04 00:12:59.603108: step 54630, total loss = 0.39, predict loss = 0.08 (72.6 examples/sec; 0.055 sec/batch; 91h:02m:27s remains)
INFO - root - 2019-11-04 00:13:00.209584: step 54640, total loss = 0.62, predict loss = 0.14 (77.2 examples/sec; 0.052 sec/batch; 85h:32m:00s remains)
INFO - root - 2019-11-04 00:13:00.852835: step 54650, total loss = 0.62, predict loss = 0.14 (69.2 examples/sec; 0.058 sec/batch; 95h:30m:22s remains)
INFO - root - 2019-11-04 00:13:01.576483: step 54660, total loss = 0.62, predict loss = 0.14 (59.3 examples/sec; 0.067 sec/batch; 111h:25m:04s remains)
INFO - root - 2019-11-04 00:13:02.207339: step 54670, total loss = 0.59, predict loss = 0.14 (67.3 examples/sec; 0.059 sec/batch; 98h:07m:52s remains)
INFO - root - 2019-11-04 00:13:02.861491: step 54680, total loss = 0.69, predict loss = 0.16 (67.6 examples/sec; 0.059 sec/batch; 97h:43m:48s remains)
INFO - root - 2019-11-04 00:13:03.472485: step 54690, total loss = 0.73, predict loss = 0.17 (72.3 examples/sec; 0.055 sec/batch; 91h:22m:45s remains)
INFO - root - 2019-11-04 00:13:04.090668: step 54700, total loss = 0.62, predict loss = 0.14 (74.7 examples/sec; 0.054 sec/batch; 88h:28m:04s remains)
INFO - root - 2019-11-04 00:13:04.703197: step 54710, total loss = 0.63, predict loss = 0.15 (68.9 examples/sec; 0.058 sec/batch; 95h:49m:30s remains)
INFO - root - 2019-11-04 00:13:05.352878: step 54720, total loss = 0.51, predict loss = 0.12 (67.2 examples/sec; 0.060 sec/batch; 98h:19m:27s remains)
INFO - root - 2019-11-04 00:13:05.993493: step 54730, total loss = 0.50, predict loss = 0.11 (73.6 examples/sec; 0.054 sec/batch; 89h:44m:55s remains)
INFO - root - 2019-11-04 00:13:06.628429: step 54740, total loss = 0.55, predict loss = 0.13 (66.1 examples/sec; 0.060 sec/batch; 99h:53m:39s remains)
INFO - root - 2019-11-04 00:13:07.264240: step 54750, total loss = 0.45, predict loss = 0.09 (72.8 examples/sec; 0.055 sec/batch; 90h:44m:07s remains)
INFO - root - 2019-11-04 00:13:07.919788: step 54760, total loss = 0.36, predict loss = 0.07 (63.9 examples/sec; 0.063 sec/batch; 103h:20m:15s remains)
INFO - root - 2019-11-04 00:13:08.535227: step 54770, total loss = 0.61, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 94h:01m:20s remains)
INFO - root - 2019-11-04 00:13:09.189099: step 54780, total loss = 0.49, predict loss = 0.11 (62.0 examples/sec; 0.065 sec/batch; 106h:37m:41s remains)
INFO - root - 2019-11-04 00:13:09.834869: step 54790, total loss = 0.59, predict loss = 0.13 (69.3 examples/sec; 0.058 sec/batch; 95h:18m:33s remains)
INFO - root - 2019-11-04 00:13:10.499142: step 54800, total loss = 0.59, predict loss = 0.13 (71.8 examples/sec; 0.056 sec/batch; 91h:58m:02s remains)
INFO - root - 2019-11-04 00:13:11.158253: step 54810, total loss = 0.70, predict loss = 0.16 (78.9 examples/sec; 0.051 sec/batch; 83h:41m:06s remains)
INFO - root - 2019-11-04 00:13:11.837971: step 54820, total loss = 0.62, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 95h:19m:01s remains)
INFO - root - 2019-11-04 00:13:12.448115: step 54830, total loss = 0.65, predict loss = 0.14 (77.4 examples/sec; 0.052 sec/batch; 85h:20m:53s remains)
INFO - root - 2019-11-04 00:13:13.167889: step 54840, total loss = 0.80, predict loss = 0.18 (65.6 examples/sec; 0.061 sec/batch; 100h:45m:27s remains)
INFO - root - 2019-11-04 00:13:13.760385: step 54850, total loss = 0.46, predict loss = 0.10 (78.8 examples/sec; 0.051 sec/batch; 83h:51m:50s remains)
INFO - root - 2019-11-04 00:13:14.360131: step 54860, total loss = 0.53, predict loss = 0.12 (80.2 examples/sec; 0.050 sec/batch; 82h:20m:41s remains)
INFO - root - 2019-11-04 00:13:15.031911: step 54870, total loss = 0.73, predict loss = 0.17 (63.3 examples/sec; 0.063 sec/batch; 104h:26m:11s remains)
INFO - root - 2019-11-04 00:13:15.675139: step 54880, total loss = 0.70, predict loss = 0.16 (68.7 examples/sec; 0.058 sec/batch; 96h:05m:52s remains)
INFO - root - 2019-11-04 00:13:16.290368: step 54890, total loss = 0.65, predict loss = 0.15 (77.6 examples/sec; 0.052 sec/batch; 85h:06m:54s remains)
INFO - root - 2019-11-04 00:13:16.910393: step 54900, total loss = 0.65, predict loss = 0.15 (71.8 examples/sec; 0.056 sec/batch; 92h:03m:53s remains)
INFO - root - 2019-11-04 00:13:17.581409: step 54910, total loss = 0.68, predict loss = 0.16 (84.5 examples/sec; 0.047 sec/batch; 78h:08m:49s remains)
INFO - root - 2019-11-04 00:13:18.180633: step 54920, total loss = 0.56, predict loss = 0.12 (74.5 examples/sec; 0.054 sec/batch; 88h:38m:24s remains)
INFO - root - 2019-11-04 00:13:18.808299: step 54930, total loss = 0.59, predict loss = 0.13 (77.1 examples/sec; 0.052 sec/batch; 85h:39m:14s remains)
INFO - root - 2019-11-04 00:13:19.423419: step 54940, total loss = 0.47, predict loss = 0.10 (75.1 examples/sec; 0.053 sec/batch; 87h:55m:21s remains)
INFO - root - 2019-11-04 00:13:20.143384: step 54950, total loss = 0.47, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 93h:30m:34s remains)
INFO - root - 2019-11-04 00:13:20.734363: step 54960, total loss = 0.71, predict loss = 0.17 (79.3 examples/sec; 0.050 sec/batch; 83h:17m:21s remains)
INFO - root - 2019-11-04 00:13:21.367177: step 54970, total loss = 0.56, predict loss = 0.12 (76.0 examples/sec; 0.053 sec/batch; 86h:53m:01s remains)
INFO - root - 2019-11-04 00:13:22.094658: step 54980, total loss = 0.63, predict loss = 0.15 (64.0 examples/sec; 0.063 sec/batch; 103h:13m:07s remains)
INFO - root - 2019-11-04 00:13:22.726929: step 54990, total loss = 0.60, predict loss = 0.14 (72.1 examples/sec; 0.055 sec/batch; 91h:39m:03s remains)
INFO - root - 2019-11-04 00:13:23.365945: step 55000, total loss = 0.49, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 98h:45m:14s remains)
INFO - root - 2019-11-04 00:13:24.024455: step 55010, total loss = 0.55, predict loss = 0.13 (74.8 examples/sec; 0.053 sec/batch; 88h:16m:25s remains)
INFO - root - 2019-11-04 00:13:24.635595: step 55020, total loss = 0.55, predict loss = 0.13 (76.1 examples/sec; 0.053 sec/batch; 86h:50m:57s remains)
INFO - root - 2019-11-04 00:13:25.250615: step 55030, total loss = 0.58, predict loss = 0.13 (70.8 examples/sec; 0.057 sec/batch; 93h:21m:48s remains)
INFO - root - 2019-11-04 00:13:25.855414: step 55040, total loss = 0.42, predict loss = 0.09 (74.4 examples/sec; 0.054 sec/batch; 88h:48m:06s remains)
INFO - root - 2019-11-04 00:13:26.527531: step 55050, total loss = 0.59, predict loss = 0.15 (59.1 examples/sec; 0.068 sec/batch; 111h:49m:41s remains)
INFO - root - 2019-11-04 00:13:27.189397: step 55060, total loss = 0.56, predict loss = 0.14 (66.2 examples/sec; 0.060 sec/batch; 99h:50m:40s remains)
INFO - root - 2019-11-04 00:13:27.802961: step 55070, total loss = 0.71, predict loss = 0.17 (78.0 examples/sec; 0.051 sec/batch; 84h:41m:35s remains)
INFO - root - 2019-11-04 00:13:28.469560: step 55080, total loss = 0.61, predict loss = 0.14 (73.0 examples/sec; 0.055 sec/batch; 90h:27m:03s remains)
INFO - root - 2019-11-04 00:13:29.125059: step 55090, total loss = 0.57, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 95h:18m:24s remains)
INFO - root - 2019-11-04 00:13:29.748046: step 55100, total loss = 0.51, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 95h:40m:47s remains)
INFO - root - 2019-11-04 00:13:30.384490: step 55110, total loss = 0.47, predict loss = 0.11 (66.7 examples/sec; 0.060 sec/batch; 99h:00m:36s remains)
INFO - root - 2019-11-04 00:13:31.026931: step 55120, total loss = 0.55, predict loss = 0.12 (80.1 examples/sec; 0.050 sec/batch; 82h:28m:32s remains)
INFO - root - 2019-11-04 00:13:31.688049: step 55130, total loss = 0.48, predict loss = 0.12 (66.7 examples/sec; 0.060 sec/batch; 99h:01m:37s remains)
INFO - root - 2019-11-04 00:13:32.367968: step 55140, total loss = 0.33, predict loss = 0.08 (64.1 examples/sec; 0.062 sec/batch; 102h:58m:06s remains)
INFO - root - 2019-11-04 00:13:33.046824: step 55150, total loss = 0.38, predict loss = 0.09 (63.6 examples/sec; 0.063 sec/batch; 103h:50m:35s remains)
INFO - root - 2019-11-04 00:13:33.668883: step 55160, total loss = 0.45, predict loss = 0.11 (68.9 examples/sec; 0.058 sec/batch; 95h:55m:55s remains)
INFO - root - 2019-11-04 00:13:34.300463: step 55170, total loss = 0.38, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 84h:20m:43s remains)
INFO - root - 2019-11-04 00:13:34.948180: step 55180, total loss = 0.46, predict loss = 0.11 (65.1 examples/sec; 0.061 sec/batch; 101h:26m:17s remains)
INFO - root - 2019-11-04 00:13:35.562539: step 55190, total loss = 0.32, predict loss = 0.06 (74.8 examples/sec; 0.053 sec/batch; 88h:17m:10s remains)
INFO - root - 2019-11-04 00:13:36.162162: step 55200, total loss = 0.42, predict loss = 0.09 (75.0 examples/sec; 0.053 sec/batch; 88h:06m:13s remains)
INFO - root - 2019-11-04 00:13:36.782583: step 55210, total loss = 0.64, predict loss = 0.14 (69.2 examples/sec; 0.058 sec/batch; 95h:27m:00s remains)
INFO - root - 2019-11-04 00:13:37.415114: step 55220, total loss = 0.56, predict loss = 0.13 (66.9 examples/sec; 0.060 sec/batch; 98h:47m:03s remains)
INFO - root - 2019-11-04 00:13:38.053246: step 55230, total loss = 0.68, predict loss = 0.17 (63.6 examples/sec; 0.063 sec/batch; 103h:47m:20s remains)
INFO - root - 2019-11-04 00:13:38.681378: step 55240, total loss = 0.65, predict loss = 0.16 (72.2 examples/sec; 0.055 sec/batch; 91h:32m:41s remains)
INFO - root - 2019-11-04 00:13:39.337424: step 55250, total loss = 0.65, predict loss = 0.15 (67.0 examples/sec; 0.060 sec/batch; 98h:33m:33s remains)
INFO - root - 2019-11-04 00:13:39.970141: step 55260, total loss = 0.75, predict loss = 0.17 (78.7 examples/sec; 0.051 sec/batch; 83h:57m:39s remains)
INFO - root - 2019-11-04 00:13:40.598884: step 55270, total loss = 0.50, predict loss = 0.12 (77.6 examples/sec; 0.052 sec/batch; 85h:04m:13s remains)
INFO - root - 2019-11-04 00:13:41.208501: step 55280, total loss = 0.62, predict loss = 0.15 (75.2 examples/sec; 0.053 sec/batch; 87h:49m:10s remains)
INFO - root - 2019-11-04 00:13:41.880983: step 55290, total loss = 0.54, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 98h:23m:00s remains)
INFO - root - 2019-11-04 00:13:42.501287: step 55300, total loss = 0.66, predict loss = 0.15 (70.0 examples/sec; 0.057 sec/batch; 94h:17m:43s remains)
INFO - root - 2019-11-04 00:13:43.134006: step 55310, total loss = 0.69, predict loss = 0.17 (66.6 examples/sec; 0.060 sec/batch; 99h:07m:43s remains)
INFO - root - 2019-11-04 00:13:43.756607: step 55320, total loss = 0.71, predict loss = 0.17 (70.9 examples/sec; 0.056 sec/batch; 93h:12m:45s remains)
INFO - root - 2019-11-04 00:13:44.374905: step 55330, total loss = 0.46, predict loss = 0.10 (79.6 examples/sec; 0.050 sec/batch; 83h:01m:30s remains)
INFO - root - 2019-11-04 00:13:45.009849: step 55340, total loss = 0.61, predict loss = 0.14 (78.2 examples/sec; 0.051 sec/batch; 84h:29m:00s remains)
INFO - root - 2019-11-04 00:13:45.663503: step 55350, total loss = 0.57, predict loss = 0.13 (72.4 examples/sec; 0.055 sec/batch; 91h:14m:17s remains)
INFO - root - 2019-11-04 00:13:46.337322: step 55360, total loss = 0.45, predict loss = 0.10 (69.9 examples/sec; 0.057 sec/batch; 94h:28m:25s remains)
INFO - root - 2019-11-04 00:13:46.996873: step 55370, total loss = 0.41, predict loss = 0.09 (68.3 examples/sec; 0.059 sec/batch; 96h:46m:07s remains)
INFO - root - 2019-11-04 00:13:47.640577: step 55380, total loss = 0.26, predict loss = 0.05 (70.4 examples/sec; 0.057 sec/batch; 93h:49m:31s remains)
INFO - root - 2019-11-04 00:13:48.271203: step 55390, total loss = 0.34, predict loss = 0.08 (80.8 examples/sec; 0.050 sec/batch; 81h:46m:20s remains)
INFO - root - 2019-11-04 00:13:48.940080: step 55400, total loss = 0.26, predict loss = 0.05 (66.2 examples/sec; 0.060 sec/batch; 99h:43m:27s remains)
INFO - root - 2019-11-04 00:13:49.564080: step 55410, total loss = 0.27, predict loss = 0.06 (72.4 examples/sec; 0.055 sec/batch; 91h:15m:24s remains)
INFO - root - 2019-11-04 00:13:50.185058: step 55420, total loss = 0.25, predict loss = 0.05 (84.9 examples/sec; 0.047 sec/batch; 77h:48m:14s remains)
INFO - root - 2019-11-04 00:13:50.811430: step 55430, total loss = 0.33, predict loss = 0.07 (65.8 examples/sec; 0.061 sec/batch; 100h:24m:59s remains)
INFO - root - 2019-11-04 00:13:51.432983: step 55440, total loss = 0.37, predict loss = 0.08 (74.3 examples/sec; 0.054 sec/batch; 88h:56m:19s remains)
INFO - root - 2019-11-04 00:13:52.075981: step 55450, total loss = 0.45, predict loss = 0.09 (65.1 examples/sec; 0.061 sec/batch; 101h:31m:57s remains)
INFO - root - 2019-11-04 00:13:52.715105: step 55460, total loss = 0.61, predict loss = 0.14 (61.8 examples/sec; 0.065 sec/batch; 106h:50m:25s remains)
INFO - root - 2019-11-04 00:13:53.372046: step 55470, total loss = 0.53, predict loss = 0.12 (69.2 examples/sec; 0.058 sec/batch; 95h:24m:45s remains)
INFO - root - 2019-11-04 00:13:54.033908: step 55480, total loss = 0.61, predict loss = 0.15 (67.3 examples/sec; 0.059 sec/batch; 98h:08m:04s remains)
INFO - root - 2019-11-04 00:13:54.677093: step 55490, total loss = 0.35, predict loss = 0.08 (76.0 examples/sec; 0.053 sec/batch; 86h:57m:02s remains)
INFO - root - 2019-11-04 00:13:55.339309: step 55500, total loss = 0.74, predict loss = 0.18 (66.9 examples/sec; 0.060 sec/batch; 98h:48m:06s remains)
INFO - root - 2019-11-04 00:13:55.984204: step 55510, total loss = 0.43, predict loss = 0.10 (69.0 examples/sec; 0.058 sec/batch; 95h:45m:52s remains)
INFO - root - 2019-11-04 00:13:56.628342: step 55520, total loss = 0.65, predict loss = 0.15 (73.9 examples/sec; 0.054 sec/batch; 89h:23m:36s remains)
INFO - root - 2019-11-04 00:13:57.269065: step 55530, total loss = 0.39, predict loss = 0.08 (78.6 examples/sec; 0.051 sec/batch; 84h:04m:09s remains)
INFO - root - 2019-11-04 00:13:57.889472: step 55540, total loss = 0.57, predict loss = 0.13 (84.2 examples/sec; 0.048 sec/batch; 78h:27m:10s remains)
INFO - root - 2019-11-04 00:13:58.505137: step 55550, total loss = 0.62, predict loss = 0.15 (73.4 examples/sec; 0.054 sec/batch; 89h:59m:01s remains)
INFO - root - 2019-11-04 00:13:59.078424: step 55560, total loss = 0.66, predict loss = 0.15 (74.2 examples/sec; 0.054 sec/batch; 89h:03m:42s remains)
INFO - root - 2019-11-04 00:13:59.677546: step 55570, total loss = 0.52, predict loss = 0.12 (77.8 examples/sec; 0.051 sec/batch; 84h:56m:35s remains)
INFO - root - 2019-11-04 00:14:00.347755: step 55580, total loss = 0.49, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 99h:27m:01s remains)
INFO - root - 2019-11-04 00:14:01.012254: step 55590, total loss = 0.46, predict loss = 0.11 (70.8 examples/sec; 0.057 sec/batch; 93h:19m:29s remains)
INFO - root - 2019-11-04 00:14:01.615817: step 55600, total loss = 0.27, predict loss = 0.05 (73.7 examples/sec; 0.054 sec/batch; 89h:35m:38s remains)
INFO - root - 2019-11-04 00:14:02.209899: step 55610, total loss = 0.28, predict loss = 0.05 (71.8 examples/sec; 0.056 sec/batch; 91h:56m:57s remains)
INFO - root - 2019-11-04 00:14:02.803522: step 55620, total loss = 0.42, predict loss = 0.09 (67.5 examples/sec; 0.059 sec/batch; 97h:51m:14s remains)
INFO - root - 2019-11-04 00:14:03.422255: step 55630, total loss = 0.37, predict loss = 0.08 (68.0 examples/sec; 0.059 sec/batch; 97h:09m:36s remains)
INFO - root - 2019-11-04 00:14:04.029831: step 55640, total loss = 0.27, predict loss = 0.05 (79.0 examples/sec; 0.051 sec/batch; 83h:38m:53s remains)
INFO - root - 2019-11-04 00:14:04.686166: step 55650, total loss = 0.43, predict loss = 0.10 (64.7 examples/sec; 0.062 sec/batch; 102h:00m:30s remains)
INFO - root - 2019-11-04 00:14:05.339196: step 55660, total loss = 0.38, predict loss = 0.09 (76.1 examples/sec; 0.053 sec/batch; 86h:45m:03s remains)
INFO - root - 2019-11-04 00:14:05.961586: step 55670, total loss = 0.38, predict loss = 0.08 (73.7 examples/sec; 0.054 sec/batch; 89h:36m:36s remains)
INFO - root - 2019-11-04 00:14:06.564495: step 55680, total loss = 0.48, predict loss = 0.12 (84.0 examples/sec; 0.048 sec/batch; 78h:36m:30s remains)
INFO - root - 2019-11-04 00:14:07.188647: step 55690, total loss = 0.61, predict loss = 0.14 (77.3 examples/sec; 0.052 sec/batch; 85h:26m:01s remains)
INFO - root - 2019-11-04 00:14:07.812091: step 55700, total loss = 0.49, predict loss = 0.11 (77.8 examples/sec; 0.051 sec/batch; 84h:55m:36s remains)
INFO - root - 2019-11-04 00:14:08.439349: step 55710, total loss = 0.65, predict loss = 0.15 (72.6 examples/sec; 0.055 sec/batch; 90h:55m:58s remains)
INFO - root - 2019-11-04 00:14:09.071558: step 55720, total loss = 0.44, predict loss = 0.10 (65.5 examples/sec; 0.061 sec/batch; 100h:46m:57s remains)
INFO - root - 2019-11-04 00:14:09.743146: step 55730, total loss = 0.47, predict loss = 0.11 (64.1 examples/sec; 0.062 sec/batch; 103h:03m:53s remains)
INFO - root - 2019-11-04 00:14:10.372759: step 55740, total loss = 0.41, predict loss = 0.08 (73.0 examples/sec; 0.055 sec/batch; 90h:31m:30s remains)
INFO - root - 2019-11-04 00:14:10.997873: step 55750, total loss = 0.38, predict loss = 0.08 (63.8 examples/sec; 0.063 sec/batch; 103h:33m:28s remains)
INFO - root - 2019-11-04 00:14:11.662447: step 55760, total loss = 0.50, predict loss = 0.12 (74.5 examples/sec; 0.054 sec/batch; 88h:36m:13s remains)
INFO - root - 2019-11-04 00:14:12.269068: step 55770, total loss = 0.41, predict loss = 0.09 (64.7 examples/sec; 0.062 sec/batch; 102h:09m:24s remains)
INFO - root - 2019-11-04 00:14:12.928033: step 55780, total loss = 0.33, predict loss = 0.07 (60.6 examples/sec; 0.066 sec/batch; 109h:01m:11s remains)
INFO - root - 2019-11-04 00:14:13.597287: step 55790, total loss = 0.47, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 92h:52m:49s remains)
INFO - root - 2019-11-04 00:14:14.279834: step 55800, total loss = 0.58, predict loss = 0.12 (64.0 examples/sec; 0.063 sec/batch; 103h:16m:10s remains)
INFO - root - 2019-11-04 00:14:14.897557: step 55810, total loss = 0.41, predict loss = 0.09 (76.0 examples/sec; 0.053 sec/batch; 86h:52m:24s remains)
INFO - root - 2019-11-04 00:14:15.511198: step 55820, total loss = 0.58, predict loss = 0.14 (65.6 examples/sec; 0.061 sec/batch; 100h:37m:21s remains)
INFO - root - 2019-11-04 00:14:16.218117: step 55830, total loss = 0.54, predict loss = 0.12 (70.0 examples/sec; 0.057 sec/batch; 94h:22m:02s remains)
INFO - root - 2019-11-04 00:14:16.895400: step 55840, total loss = 0.38, predict loss = 0.09 (67.6 examples/sec; 0.059 sec/batch; 97h:46m:13s remains)
INFO - root - 2019-11-04 00:14:17.516222: step 55850, total loss = 0.48, predict loss = 0.11 (73.8 examples/sec; 0.054 sec/batch; 89h:32m:11s remains)
INFO - root - 2019-11-04 00:14:18.103837: step 55860, total loss = 0.58, predict loss = 0.13 (77.7 examples/sec; 0.051 sec/batch; 85h:01m:53s remains)
INFO - root - 2019-11-04 00:14:18.723737: step 55870, total loss = 0.40, predict loss = 0.09 (71.9 examples/sec; 0.056 sec/batch; 91h:52m:02s remains)
INFO - root - 2019-11-04 00:14:19.403797: step 55880, total loss = 0.70, predict loss = 0.17 (59.2 examples/sec; 0.068 sec/batch; 111h:38m:16s remains)
INFO - root - 2019-11-04 00:14:20.080531: step 55890, total loss = 0.41, predict loss = 0.09 (61.7 examples/sec; 0.065 sec/batch; 107h:07m:34s remains)
INFO - root - 2019-11-04 00:14:20.798621: step 55900, total loss = 0.45, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 100h:09m:08s remains)
INFO - root - 2019-11-04 00:14:21.421877: step 55910, total loss = 0.48, predict loss = 0.11 (84.4 examples/sec; 0.047 sec/batch; 78h:17m:31s remains)
INFO - root - 2019-11-04 00:14:22.056891: step 55920, total loss = 0.35, predict loss = 0.08 (70.7 examples/sec; 0.057 sec/batch; 93h:26m:21s remains)
INFO - root - 2019-11-04 00:14:22.696382: step 55930, total loss = 0.44, predict loss = 0.10 (63.3 examples/sec; 0.063 sec/batch; 104h:21m:36s remains)
INFO - root - 2019-11-04 00:14:23.322533: step 55940, total loss = 0.63, predict loss = 0.16 (65.0 examples/sec; 0.062 sec/batch; 101h:36m:33s remains)
INFO - root - 2019-11-04 00:14:23.967892: step 55950, total loss = 0.52, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 93h:43m:45s remains)
INFO - root - 2019-11-04 00:14:24.607068: step 55960, total loss = 0.63, predict loss = 0.15 (71.0 examples/sec; 0.056 sec/batch; 93h:00m:48s remains)
INFO - root - 2019-11-04 00:14:25.277258: step 55970, total loss = 0.88, predict loss = 0.21 (64.5 examples/sec; 0.062 sec/batch; 102h:26m:56s remains)
INFO - root - 2019-11-04 00:14:25.957302: step 55980, total loss = 0.73, predict loss = 0.18 (71.0 examples/sec; 0.056 sec/batch; 93h:00m:56s remains)
INFO - root - 2019-11-04 00:14:26.564481: step 55990, total loss = 1.11, predict loss = 0.26 (79.3 examples/sec; 0.050 sec/batch; 83h:17m:11s remains)
INFO - root - 2019-11-04 00:14:27.176606: step 56000, total loss = 0.79, predict loss = 0.20 (67.2 examples/sec; 0.059 sec/batch; 98h:12m:27s remains)
INFO - root - 2019-11-04 00:14:27.810169: step 56010, total loss = 0.85, predict loss = 0.20 (73.6 examples/sec; 0.054 sec/batch; 89h:45m:51s remains)
INFO - root - 2019-11-04 00:14:28.442082: step 56020, total loss = 0.88, predict loss = 0.22 (67.2 examples/sec; 0.060 sec/batch; 98h:18m:36s remains)
INFO - root - 2019-11-04 00:14:29.099350: step 56030, total loss = 0.63, predict loss = 0.15 (76.0 examples/sec; 0.053 sec/batch; 86h:50m:59s remains)
INFO - root - 2019-11-04 00:14:29.785479: step 56040, total loss = 0.71, predict loss = 0.17 (66.2 examples/sec; 0.060 sec/batch; 99h:41m:37s remains)
INFO - root - 2019-11-04 00:14:30.415347: step 56050, total loss = 0.85, predict loss = 0.21 (73.9 examples/sec; 0.054 sec/batch; 89h:22m:26s remains)
INFO - root - 2019-11-04 00:14:31.075294: step 56060, total loss = 0.60, predict loss = 0.13 (69.5 examples/sec; 0.058 sec/batch; 95h:01m:53s remains)
INFO - root - 2019-11-04 00:14:31.718306: step 56070, total loss = 0.48, predict loss = 0.11 (62.1 examples/sec; 0.064 sec/batch; 106h:24m:28s remains)
INFO - root - 2019-11-04 00:14:32.351899: step 56080, total loss = 0.65, predict loss = 0.14 (72.4 examples/sec; 0.055 sec/batch; 91h:10m:14s remains)
INFO - root - 2019-11-04 00:14:33.016288: step 56090, total loss = 0.58, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 87h:00m:34s remains)
INFO - root - 2019-11-04 00:14:33.707258: step 56100, total loss = 0.70, predict loss = 0.16 (62.8 examples/sec; 0.064 sec/batch; 105h:13m:03s remains)
INFO - root - 2019-11-04 00:14:34.392964: step 56110, total loss = 0.66, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 93h:58m:26s remains)
INFO - root - 2019-11-04 00:14:35.013675: step 56120, total loss = 0.54, predict loss = 0.12 (84.0 examples/sec; 0.048 sec/batch; 78h:34m:39s remains)
INFO - root - 2019-11-04 00:14:35.626197: step 56130, total loss = 0.58, predict loss = 0.13 (78.4 examples/sec; 0.051 sec/batch; 84h:15m:08s remains)
INFO - root - 2019-11-04 00:14:36.256291: step 56140, total loss = 0.45, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 98h:02m:56s remains)
INFO - root - 2019-11-04 00:14:36.898686: step 56150, total loss = 0.46, predict loss = 0.10 (65.7 examples/sec; 0.061 sec/batch; 100h:30m:23s remains)
INFO - root - 2019-11-04 00:14:37.551407: step 56160, total loss = 0.54, predict loss = 0.12 (63.3 examples/sec; 0.063 sec/batch; 104h:22m:12s remains)
INFO - root - 2019-11-04 00:14:38.250440: step 56170, total loss = 0.56, predict loss = 0.13 (76.1 examples/sec; 0.053 sec/batch; 86h:50m:19s remains)
INFO - root - 2019-11-04 00:14:38.896672: step 56180, total loss = 0.57, predict loss = 0.13 (73.0 examples/sec; 0.055 sec/batch; 90h:28m:23s remains)
INFO - root - 2019-11-04 00:14:39.524027: step 56190, total loss = 0.51, predict loss = 0.11 (78.4 examples/sec; 0.051 sec/batch; 84h:15m:50s remains)
INFO - root - 2019-11-04 00:14:40.168600: step 56200, total loss = 0.50, predict loss = 0.11 (75.6 examples/sec; 0.053 sec/batch; 87h:24m:09s remains)
INFO - root - 2019-11-04 00:14:40.804686: step 56210, total loss = 0.43, predict loss = 0.10 (68.5 examples/sec; 0.058 sec/batch; 96h:22m:32s remains)
INFO - root - 2019-11-04 00:14:41.469529: step 56220, total loss = 0.45, predict loss = 0.11 (65.1 examples/sec; 0.061 sec/batch; 101h:22m:22s remains)
INFO - root - 2019-11-04 00:14:42.133969: step 56230, total loss = 0.47, predict loss = 0.11 (74.4 examples/sec; 0.054 sec/batch; 88h:49m:24s remains)
INFO - root - 2019-11-04 00:14:42.761483: step 56240, total loss = 0.43, predict loss = 0.10 (74.1 examples/sec; 0.054 sec/batch; 89h:04m:24s remains)
INFO - root - 2019-11-04 00:14:43.397301: step 56250, total loss = 0.48, predict loss = 0.11 (72.4 examples/sec; 0.055 sec/batch; 91h:16m:06s remains)
INFO - root - 2019-11-04 00:14:44.036617: step 56260, total loss = 0.37, predict loss = 0.08 (71.4 examples/sec; 0.056 sec/batch; 92h:32m:36s remains)
INFO - root - 2019-11-04 00:14:44.666976: step 56270, total loss = 0.42, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 93h:12m:04s remains)
INFO - root - 2019-11-04 00:14:45.300718: step 56280, total loss = 0.40, predict loss = 0.09 (69.1 examples/sec; 0.058 sec/batch; 95h:31m:31s remains)
INFO - root - 2019-11-04 00:14:45.941048: step 56290, total loss = 0.50, predict loss = 0.11 (66.7 examples/sec; 0.060 sec/batch; 99h:00m:12s remains)
INFO - root - 2019-11-04 00:14:46.553553: step 56300, total loss = 0.47, predict loss = 0.10 (82.2 examples/sec; 0.049 sec/batch; 80h:18m:51s remains)
INFO - root - 2019-11-04 00:14:47.207339: step 56310, total loss = 0.58, predict loss = 0.13 (66.7 examples/sec; 0.060 sec/batch; 99h:03m:38s remains)
INFO - root - 2019-11-04 00:14:47.856752: step 56320, total loss = 0.45, predict loss = 0.10 (72.6 examples/sec; 0.055 sec/batch; 90h:59m:34s remains)
INFO - root - 2019-11-04 00:14:48.503182: step 56330, total loss = 0.47, predict loss = 0.10 (68.0 examples/sec; 0.059 sec/batch; 97h:07m:29s remains)
INFO - root - 2019-11-04 00:14:49.168527: step 56340, total loss = 0.50, predict loss = 0.11 (63.6 examples/sec; 0.063 sec/batch; 103h:45m:30s remains)
INFO - root - 2019-11-04 00:14:49.818266: step 56350, total loss = 0.56, predict loss = 0.13 (69.0 examples/sec; 0.058 sec/batch; 95h:43m:21s remains)
INFO - root - 2019-11-04 00:14:50.440371: step 56360, total loss = 0.63, predict loss = 0.15 (73.5 examples/sec; 0.054 sec/batch; 89h:51m:04s remains)
INFO - root - 2019-11-04 00:14:51.068746: step 56370, total loss = 0.57, predict loss = 0.13 (68.5 examples/sec; 0.058 sec/batch; 96h:26m:38s remains)
INFO - root - 2019-11-04 00:14:51.723291: step 56380, total loss = 0.62, predict loss = 0.15 (72.4 examples/sec; 0.055 sec/batch; 91h:12m:50s remains)
INFO - root - 2019-11-04 00:14:52.356092: step 56390, total loss = 0.83, predict loss = 0.19 (71.3 examples/sec; 0.056 sec/batch; 92h:35m:25s remains)
INFO - root - 2019-11-04 00:14:52.971669: step 56400, total loss = 0.73, predict loss = 0.17 (77.3 examples/sec; 0.052 sec/batch; 85h:27m:33s remains)
INFO - root - 2019-11-04 00:14:53.634135: step 56410, total loss = 0.67, predict loss = 0.16 (63.4 examples/sec; 0.063 sec/batch; 104h:12m:06s remains)
INFO - root - 2019-11-04 00:14:54.335198: step 56420, total loss = 0.66, predict loss = 0.15 (57.5 examples/sec; 0.070 sec/batch; 114h:48m:11s remains)
INFO - root - 2019-11-04 00:14:55.028720: step 56430, total loss = 0.68, predict loss = 0.16 (66.3 examples/sec; 0.060 sec/batch; 99h:33m:22s remains)
INFO - root - 2019-11-04 00:14:55.720851: step 56440, total loss = 0.65, predict loss = 0.16 (66.2 examples/sec; 0.060 sec/batch; 99h:47m:28s remains)
INFO - root - 2019-11-04 00:14:56.360788: step 56450, total loss = 0.68, predict loss = 0.16 (76.3 examples/sec; 0.052 sec/batch; 86h:36m:24s remains)
INFO - root - 2019-11-04 00:14:57.009027: step 56460, total loss = 0.77, predict loss = 0.19 (69.4 examples/sec; 0.058 sec/batch; 95h:11m:30s remains)
INFO - root - 2019-11-04 00:14:57.611183: step 56470, total loss = 0.77, predict loss = 0.19 (69.2 examples/sec; 0.058 sec/batch; 95h:25m:53s remains)
INFO - root - 2019-11-04 00:14:58.287902: step 56480, total loss = 0.56, predict loss = 0.12 (64.2 examples/sec; 0.062 sec/batch; 102h:53m:48s remains)
INFO - root - 2019-11-04 00:14:58.917465: step 56490, total loss = 0.52, predict loss = 0.12 (62.3 examples/sec; 0.064 sec/batch; 105h:59m:45s remains)
INFO - root - 2019-11-04 00:14:59.561112: step 56500, total loss = 0.50, predict loss = 0.11 (65.1 examples/sec; 0.061 sec/batch; 101h:28m:17s remains)
INFO - root - 2019-11-04 00:15:00.191166: step 56510, total loss = 0.51, predict loss = 0.12 (65.6 examples/sec; 0.061 sec/batch; 100h:42m:59s remains)
INFO - root - 2019-11-04 00:15:00.799640: step 56520, total loss = 0.53, predict loss = 0.12 (65.3 examples/sec; 0.061 sec/batch; 101h:10m:06s remains)
INFO - root - 2019-11-04 00:15:01.441595: step 56530, total loss = 0.44, predict loss = 0.09 (81.8 examples/sec; 0.049 sec/batch; 80h:43m:09s remains)
INFO - root - 2019-11-04 00:15:02.043394: step 56540, total loss = 0.41, predict loss = 0.09 (79.3 examples/sec; 0.050 sec/batch; 83h:13m:35s remains)
INFO - root - 2019-11-04 00:15:02.639839: step 56550, total loss = 0.52, predict loss = 0.12 (72.2 examples/sec; 0.055 sec/batch; 91h:25m:17s remains)
INFO - root - 2019-11-04 00:15:03.256613: step 56560, total loss = 0.42, predict loss = 0.10 (88.1 examples/sec; 0.045 sec/batch; 74h:57m:22s remains)
INFO - root - 2019-11-04 00:15:03.886372: step 56570, total loss = 0.53, predict loss = 0.12 (63.0 examples/sec; 0.064 sec/batch; 104h:52m:58s remains)
INFO - root - 2019-11-04 00:15:04.526247: step 56580, total loss = 0.48, predict loss = 0.11 (66.2 examples/sec; 0.060 sec/batch; 99h:43m:11s remains)
INFO - root - 2019-11-04 00:15:05.203681: step 56590, total loss = 0.53, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 98h:05m:55s remains)
INFO - root - 2019-11-04 00:15:05.903841: step 56600, total loss = 0.55, predict loss = 0.12 (61.0 examples/sec; 0.066 sec/batch; 108h:17m:38s remains)
INFO - root - 2019-11-04 00:15:06.591047: step 56610, total loss = 0.69, predict loss = 0.16 (63.7 examples/sec; 0.063 sec/batch; 103h:37m:51s remains)
INFO - root - 2019-11-04 00:15:07.212373: step 56620, total loss = 0.59, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 90h:20m:53s remains)
INFO - root - 2019-11-04 00:15:07.799076: step 56630, total loss = 0.71, predict loss = 0.17 (74.7 examples/sec; 0.054 sec/batch; 88h:24m:57s remains)
INFO - root - 2019-11-04 00:15:08.415148: step 56640, total loss = 0.61, predict loss = 0.15 (82.6 examples/sec; 0.048 sec/batch; 79h:58m:28s remains)
INFO - root - 2019-11-04 00:15:09.075267: step 56650, total loss = 0.59, predict loss = 0.15 (70.2 examples/sec; 0.057 sec/batch; 94h:03m:58s remains)
INFO - root - 2019-11-04 00:15:09.692040: step 56660, total loss = 0.45, predict loss = 0.10 (73.0 examples/sec; 0.055 sec/batch; 90h:28m:50s remains)
INFO - root - 2019-11-04 00:15:10.268936: step 56670, total loss = 0.58, predict loss = 0.14 (73.9 examples/sec; 0.054 sec/batch; 89h:20m:00s remains)
INFO - root - 2019-11-04 00:15:10.907045: step 56680, total loss = 0.65, predict loss = 0.16 (73.2 examples/sec; 0.055 sec/batch; 90h:13m:22s remains)
INFO - root - 2019-11-04 00:15:11.581990: step 56690, total loss = 0.65, predict loss = 0.16 (62.7 examples/sec; 0.064 sec/batch; 105h:18m:04s remains)
INFO - root - 2019-11-04 00:15:12.217526: step 56700, total loss = 0.63, predict loss = 0.15 (61.9 examples/sec; 0.065 sec/batch; 106h:36m:17s remains)
INFO - root - 2019-11-04 00:15:12.879761: step 56710, total loss = 0.66, predict loss = 0.16 (64.9 examples/sec; 0.062 sec/batch; 101h:42m:10s remains)
INFO - root - 2019-11-04 00:15:13.490322: step 56720, total loss = 0.65, predict loss = 0.16 (81.6 examples/sec; 0.049 sec/batch; 80h:56m:13s remains)
INFO - root - 2019-11-04 00:15:14.132301: step 56730, total loss = 0.63, predict loss = 0.14 (64.9 examples/sec; 0.062 sec/batch; 101h:48m:30s remains)
INFO - root - 2019-11-04 00:15:14.761341: step 56740, total loss = 0.66, predict loss = 0.15 (71.9 examples/sec; 0.056 sec/batch; 91h:49m:23s remains)
INFO - root - 2019-11-04 00:15:15.399696: step 56750, total loss = 0.55, predict loss = 0.13 (72.5 examples/sec; 0.055 sec/batch; 91h:07m:43s remains)
INFO - root - 2019-11-04 00:15:16.024931: step 56760, total loss = 0.48, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 90h:26m:06s remains)
INFO - root - 2019-11-04 00:15:16.639586: step 56770, total loss = 0.50, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 102h:40m:23s remains)
INFO - root - 2019-11-04 00:15:17.252165: step 56780, total loss = 0.49, predict loss = 0.11 (58.8 examples/sec; 0.068 sec/batch; 112h:19m:26s remains)
INFO - root - 2019-11-04 00:15:17.918205: step 56790, total loss = 0.50, predict loss = 0.12 (62.1 examples/sec; 0.064 sec/batch; 106h:18m:15s remains)
INFO - root - 2019-11-04 00:15:18.601047: step 56800, total loss = 0.53, predict loss = 0.12 (64.5 examples/sec; 0.062 sec/batch; 102h:21m:55s remains)
INFO - root - 2019-11-04 00:15:19.271748: step 56810, total loss = 0.49, predict loss = 0.11 (62.3 examples/sec; 0.064 sec/batch; 105h:57m:13s remains)
INFO - root - 2019-11-04 00:15:19.966920: step 56820, total loss = 0.60, predict loss = 0.15 (58.5 examples/sec; 0.068 sec/batch; 112h:49m:04s remains)
INFO - root - 2019-11-04 00:15:20.674585: step 56830, total loss = 0.53, predict loss = 0.12 (57.8 examples/sec; 0.069 sec/batch; 114h:20m:09s remains)
INFO - root - 2019-11-04 00:15:21.364744: step 56840, total loss = 0.43, predict loss = 0.09 (59.9 examples/sec; 0.067 sec/batch; 110h:18m:28s remains)
INFO - root - 2019-11-04 00:15:21.996832: step 56850, total loss = 0.56, predict loss = 0.14 (81.6 examples/sec; 0.049 sec/batch; 80h:53m:42s remains)
INFO - root - 2019-11-04 00:15:22.595591: step 56860, total loss = 0.55, predict loss = 0.13 (70.4 examples/sec; 0.057 sec/batch; 93h:50m:31s remains)
INFO - root - 2019-11-04 00:15:23.185468: step 56870, total loss = 0.62, predict loss = 0.15 (85.1 examples/sec; 0.047 sec/batch; 77h:36m:41s remains)
INFO - root - 2019-11-04 00:15:23.783467: step 56880, total loss = 0.54, predict loss = 0.13 (64.8 examples/sec; 0.062 sec/batch; 101h:58m:13s remains)
INFO - root - 2019-11-04 00:15:24.404572: step 56890, total loss = 0.43, predict loss = 0.10 (71.0 examples/sec; 0.056 sec/batch; 93h:01m:18s remains)
INFO - root - 2019-11-04 00:15:25.003399: step 56900, total loss = 0.56, predict loss = 0.13 (78.0 examples/sec; 0.051 sec/batch; 84h:38m:41s remains)
INFO - root - 2019-11-04 00:15:25.620847: step 56910, total loss = 0.47, predict loss = 0.11 (74.0 examples/sec; 0.054 sec/batch; 89h:13m:32s remains)
INFO - root - 2019-11-04 00:15:26.220745: step 56920, total loss = 0.67, predict loss = 0.15 (87.6 examples/sec; 0.046 sec/batch; 75h:21m:44s remains)
INFO - root - 2019-11-04 00:15:26.815461: step 56930, total loss = 0.54, predict loss = 0.13 (67.6 examples/sec; 0.059 sec/batch; 97h:41m:49s remains)
INFO - root - 2019-11-04 00:15:27.427844: step 56940, total loss = 0.65, predict loss = 0.16 (60.4 examples/sec; 0.066 sec/batch; 109h:22m:02s remains)
INFO - root - 2019-11-04 00:15:28.097515: step 56950, total loss = 0.56, predict loss = 0.13 (66.4 examples/sec; 0.060 sec/batch; 99h:25m:06s remains)
INFO - root - 2019-11-04 00:15:28.807964: step 56960, total loss = 0.57, predict loss = 0.14 (65.0 examples/sec; 0.062 sec/batch; 101h:34m:50s remains)
INFO - root - 2019-11-04 00:15:29.437779: step 56970, total loss = 0.64, predict loss = 0.16 (65.2 examples/sec; 0.061 sec/batch; 101h:18m:34s remains)
INFO - root - 2019-11-04 00:15:30.085509: step 56980, total loss = 0.58, predict loss = 0.14 (69.1 examples/sec; 0.058 sec/batch; 95h:33m:17s remains)
INFO - root - 2019-11-04 00:15:30.723752: step 56990, total loss = 0.56, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 93h:41m:33s remains)
INFO - root - 2019-11-04 00:15:31.395680: step 57000, total loss = 0.71, predict loss = 0.17 (66.5 examples/sec; 0.060 sec/batch; 99h:17m:19s remains)
INFO - root - 2019-11-04 00:15:32.017216: step 57010, total loss = 0.65, predict loss = 0.16 (71.1 examples/sec; 0.056 sec/batch; 92h:52m:55s remains)
INFO - root - 2019-11-04 00:15:32.628881: step 57020, total loss = 0.78, predict loss = 0.19 (73.2 examples/sec; 0.055 sec/batch; 90h:11m:32s remains)
INFO - root - 2019-11-04 00:15:33.239677: step 57030, total loss = 0.77, predict loss = 0.18 (72.3 examples/sec; 0.055 sec/batch; 91h:21m:17s remains)
INFO - root - 2019-11-04 00:15:33.868586: step 57040, total loss = 0.65, predict loss = 0.16 (66.1 examples/sec; 0.061 sec/batch; 99h:52m:55s remains)
INFO - root - 2019-11-04 00:15:34.476778: step 57050, total loss = 0.65, predict loss = 0.15 (78.3 examples/sec; 0.051 sec/batch; 84h:22m:20s remains)
INFO - root - 2019-11-04 00:15:35.095789: step 57060, total loss = 0.66, predict loss = 0.16 (68.2 examples/sec; 0.059 sec/batch; 96h:46m:12s remains)
INFO - root - 2019-11-04 00:15:35.711029: step 57070, total loss = 0.67, predict loss = 0.16 (69.2 examples/sec; 0.058 sec/batch; 95h:23m:18s remains)
INFO - root - 2019-11-04 00:15:36.351098: step 57080, total loss = 0.72, predict loss = 0.16 (67.9 examples/sec; 0.059 sec/batch; 97h:11m:19s remains)
INFO - root - 2019-11-04 00:15:37.001200: step 57090, total loss = 0.84, predict loss = 0.20 (66.5 examples/sec; 0.060 sec/batch; 99h:14m:05s remains)
INFO - root - 2019-11-04 00:15:37.655175: step 57100, total loss = 0.63, predict loss = 0.15 (62.0 examples/sec; 0.064 sec/batch; 106h:26m:24s remains)
INFO - root - 2019-11-04 00:15:38.306192: step 57110, total loss = 0.74, predict loss = 0.17 (63.8 examples/sec; 0.063 sec/batch; 103h:32m:46s remains)
INFO - root - 2019-11-04 00:15:39.038055: step 57120, total loss = 0.60, predict loss = 0.13 (62.9 examples/sec; 0.064 sec/batch; 105h:02m:45s remains)
INFO - root - 2019-11-04 00:15:39.654224: step 57130, total loss = 0.75, predict loss = 0.17 (67.2 examples/sec; 0.060 sec/batch; 98h:18m:18s remains)
INFO - root - 2019-11-04 00:15:40.255102: step 57140, total loss = 0.41, predict loss = 0.09 (69.3 examples/sec; 0.058 sec/batch; 95h:15m:23s remains)
INFO - root - 2019-11-04 00:15:40.887432: step 57150, total loss = 0.68, predict loss = 0.15 (69.6 examples/sec; 0.057 sec/batch; 94h:48m:57s remains)
INFO - root - 2019-11-04 00:15:41.558257: step 57160, total loss = 0.77, predict loss = 0.18 (56.4 examples/sec; 0.071 sec/batch; 117h:10m:16s remains)
INFO - root - 2019-11-04 00:15:42.184842: step 57170, total loss = 0.54, predict loss = 0.12 (68.6 examples/sec; 0.058 sec/batch; 96h:16m:04s remains)
INFO - root - 2019-11-04 00:15:42.823139: step 57180, total loss = 0.61, predict loss = 0.14 (72.8 examples/sec; 0.055 sec/batch; 90h:42m:35s remains)
INFO - root - 2019-11-04 00:15:43.434889: step 57190, total loss = 0.56, predict loss = 0.12 (66.4 examples/sec; 0.060 sec/batch; 99h:24m:16s remains)
INFO - root - 2019-11-04 00:15:44.087144: step 57200, total loss = 0.53, predict loss = 0.13 (65.9 examples/sec; 0.061 sec/batch; 100h:09m:54s remains)
INFO - root - 2019-11-04 00:15:44.689529: step 57210, total loss = 0.48, predict loss = 0.11 (82.4 examples/sec; 0.049 sec/batch; 80h:08m:24s remains)
INFO - root - 2019-11-04 00:15:45.294585: step 57220, total loss = 0.55, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 98h:04m:56s remains)
INFO - root - 2019-11-04 00:15:45.934601: step 57230, total loss = 0.56, predict loss = 0.13 (67.2 examples/sec; 0.060 sec/batch; 98h:16m:48s remains)
INFO - root - 2019-11-04 00:15:46.593620: step 57240, total loss = 0.70, predict loss = 0.16 (71.1 examples/sec; 0.056 sec/batch; 92h:52m:18s remains)
INFO - root - 2019-11-04 00:15:47.235618: step 57250, total loss = 0.59, predict loss = 0.14 (65.2 examples/sec; 0.061 sec/batch; 101h:16m:55s remains)
INFO - root - 2019-11-04 00:15:48.292587: step 57260, total loss = 0.40, predict loss = 0.09 (86.2 examples/sec; 0.046 sec/batch; 76h:38m:33s remains)
INFO - root - 2019-11-04 00:15:48.766399: step 57270, total loss = 0.55, predict loss = 0.12 (101.2 examples/sec; 0.040 sec/batch; 65h:16m:20s remains)
INFO - root - 2019-11-04 00:15:49.830268: step 57280, total loss = 0.43, predict loss = 0.09 (76.8 examples/sec; 0.052 sec/batch; 85h:59m:30s remains)
INFO - root - 2019-11-04 00:15:50.431920: step 57290, total loss = 0.61, predict loss = 0.15 (73.2 examples/sec; 0.055 sec/batch; 90h:10m:06s remains)
INFO - root - 2019-11-04 00:15:51.027973: step 57300, total loss = 0.35, predict loss = 0.08 (75.2 examples/sec; 0.053 sec/batch; 87h:48m:16s remains)
INFO - root - 2019-11-04 00:15:51.618685: step 57310, total loss = 0.62, predict loss = 0.14 (76.6 examples/sec; 0.052 sec/batch; 86h:11m:05s remains)
INFO - root - 2019-11-04 00:15:52.218987: step 57320, total loss = 0.57, predict loss = 0.14 (66.9 examples/sec; 0.060 sec/batch; 98h:45m:53s remains)
INFO - root - 2019-11-04 00:15:52.859656: step 57330, total loss = 0.45, predict loss = 0.10 (64.6 examples/sec; 0.062 sec/batch; 102h:13m:09s remains)
INFO - root - 2019-11-04 00:15:53.494390: step 57340, total loss = 0.49, predict loss = 0.11 (67.2 examples/sec; 0.060 sec/batch; 98h:16m:40s remains)
INFO - root - 2019-11-04 00:15:54.160552: step 57350, total loss = 0.64, predict loss = 0.14 (77.6 examples/sec; 0.052 sec/batch; 85h:05m:58s remains)
INFO - root - 2019-11-04 00:15:54.810335: step 57360, total loss = 0.80, predict loss = 0.18 (66.5 examples/sec; 0.060 sec/batch; 99h:15m:13s remains)
INFO - root - 2019-11-04 00:15:55.452622: step 57370, total loss = 0.57, predict loss = 0.13 (68.4 examples/sec; 0.059 sec/batch; 96h:34m:22s remains)
INFO - root - 2019-11-04 00:15:56.092498: step 57380, total loss = 0.77, predict loss = 0.18 (68.0 examples/sec; 0.059 sec/batch; 97h:03m:53s remains)
INFO - root - 2019-11-04 00:15:56.740145: step 57390, total loss = 0.59, predict loss = 0.14 (63.9 examples/sec; 0.063 sec/batch; 103h:16m:04s remains)
INFO - root - 2019-11-04 00:15:57.410021: step 57400, total loss = 0.94, predict loss = 0.25 (61.3 examples/sec; 0.065 sec/batch; 107h:38m:12s remains)
INFO - root - 2019-11-04 00:15:58.083894: step 57410, total loss = 0.70, predict loss = 0.16 (67.2 examples/sec; 0.059 sec/batch; 98h:12m:49s remains)
INFO - root - 2019-11-04 00:15:58.769626: step 57420, total loss = 0.51, predict loss = 0.12 (65.4 examples/sec; 0.061 sec/batch; 100h:57m:24s remains)
INFO - root - 2019-11-04 00:15:59.435989: step 57430, total loss = 0.51, predict loss = 0.12 (64.7 examples/sec; 0.062 sec/batch; 102h:03m:11s remains)
INFO - root - 2019-11-04 00:16:00.144915: step 57440, total loss = 0.53, predict loss = 0.12 (61.5 examples/sec; 0.065 sec/batch; 107h:19m:20s remains)
INFO - root - 2019-11-04 00:16:00.789525: step 57450, total loss = 0.55, predict loss = 0.12 (65.4 examples/sec; 0.061 sec/batch; 100h:57m:19s remains)
INFO - root - 2019-11-04 00:16:01.458529: step 57460, total loss = 0.54, predict loss = 0.13 (61.7 examples/sec; 0.065 sec/batch; 107h:05m:40s remains)
INFO - root - 2019-11-04 00:16:02.133007: step 57470, total loss = 0.50, predict loss = 0.10 (62.9 examples/sec; 0.064 sec/batch; 104h:57m:31s remains)
INFO - root - 2019-11-04 00:16:02.796173: step 57480, total loss = 0.55, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 103h:57m:33s remains)
INFO - root - 2019-11-04 00:16:03.515005: step 57490, total loss = 0.54, predict loss = 0.13 (57.1 examples/sec; 0.070 sec/batch; 115h:35m:33s remains)
INFO - root - 2019-11-04 00:16:04.229861: step 57500, total loss = 0.44, predict loss = 0.10 (73.3 examples/sec; 0.055 sec/batch; 90h:01m:36s remains)
INFO - root - 2019-11-04 00:16:04.850319: step 57510, total loss = 0.58, predict loss = 0.14 (71.0 examples/sec; 0.056 sec/batch; 92h:56m:08s remains)
INFO - root - 2019-11-04 00:16:05.473192: step 57520, total loss = 0.65, predict loss = 0.16 (71.6 examples/sec; 0.056 sec/batch; 92h:10m:45s remains)
INFO - root - 2019-11-04 00:16:06.087523: step 57530, total loss = 0.63, predict loss = 0.14 (66.1 examples/sec; 0.060 sec/batch; 99h:50m:21s remains)
INFO - root - 2019-11-04 00:16:06.707663: step 57540, total loss = 0.62, predict loss = 0.15 (73.5 examples/sec; 0.054 sec/batch; 89h:48m:30s remains)
INFO - root - 2019-11-04 00:16:07.383716: step 57550, total loss = 0.60, predict loss = 0.14 (69.9 examples/sec; 0.057 sec/batch; 94h:31m:24s remains)
INFO - root - 2019-11-04 00:16:08.030778: step 57560, total loss = 0.57, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 92h:57m:01s remains)
INFO - root - 2019-11-04 00:16:08.667334: step 57570, total loss = 0.52, predict loss = 0.12 (64.7 examples/sec; 0.062 sec/batch; 102h:05m:31s remains)
INFO - root - 2019-11-04 00:16:09.363450: step 57580, total loss = 0.54, predict loss = 0.12 (61.7 examples/sec; 0.065 sec/batch; 106h:59m:12s remains)
INFO - root - 2019-11-04 00:16:10.037353: step 57590, total loss = 0.59, predict loss = 0.13 (64.4 examples/sec; 0.062 sec/batch; 102h:29m:43s remains)
INFO - root - 2019-11-04 00:16:10.707951: step 57600, total loss = 0.62, predict loss = 0.14 (65.7 examples/sec; 0.061 sec/batch; 100h:30m:05s remains)
INFO - root - 2019-11-04 00:16:11.385813: step 57610, total loss = 0.70, predict loss = 0.16 (65.5 examples/sec; 0.061 sec/batch; 100h:45m:17s remains)
INFO - root - 2019-11-04 00:16:12.041156: step 57620, total loss = 0.60, predict loss = 0.14 (77.4 examples/sec; 0.052 sec/batch; 85h:20m:28s remains)
INFO - root - 2019-11-04 00:16:12.675852: step 57630, total loss = 0.54, predict loss = 0.12 (69.1 examples/sec; 0.058 sec/batch; 95h:33m:22s remains)
INFO - root - 2019-11-04 00:16:13.336994: step 57640, total loss = 0.50, predict loss = 0.11 (69.2 examples/sec; 0.058 sec/batch; 95h:27m:38s remains)
INFO - root - 2019-11-04 00:16:13.956228: step 57650, total loss = 0.52, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 93h:36m:44s remains)
INFO - root - 2019-11-04 00:16:14.606326: step 57660, total loss = 0.62, predict loss = 0.13 (61.1 examples/sec; 0.065 sec/batch; 108h:01m:31s remains)
INFO - root - 2019-11-04 00:16:15.243623: step 57670, total loss = 0.49, predict loss = 0.11 (68.6 examples/sec; 0.058 sec/batch; 96h:17m:30s remains)
INFO - root - 2019-11-04 00:16:15.869510: step 57680, total loss = 0.53, predict loss = 0.12 (83.7 examples/sec; 0.048 sec/batch; 78h:54m:08s remains)
INFO - root - 2019-11-04 00:16:16.476547: step 57690, total loss = 0.58, predict loss = 0.14 (70.9 examples/sec; 0.056 sec/batch; 93h:08m:09s remains)
INFO - root - 2019-11-04 00:16:17.094329: step 57700, total loss = 0.57, predict loss = 0.13 (70.8 examples/sec; 0.057 sec/batch; 93h:18m:22s remains)
INFO - root - 2019-11-04 00:16:17.719753: step 57710, total loss = 0.58, predict loss = 0.13 (79.3 examples/sec; 0.050 sec/batch; 83h:15m:22s remains)
INFO - root - 2019-11-04 00:16:18.337138: step 57720, total loss = 0.57, predict loss = 0.13 (72.4 examples/sec; 0.055 sec/batch; 91h:09m:24s remains)
INFO - root - 2019-11-04 00:16:18.974699: step 57730, total loss = 0.49, predict loss = 0.11 (79.5 examples/sec; 0.050 sec/batch; 83h:04m:04s remains)
INFO - root - 2019-11-04 00:16:19.616009: step 57740, total loss = 0.66, predict loss = 0.17 (73.0 examples/sec; 0.055 sec/batch; 90h:27m:47s remains)
INFO - root - 2019-11-04 00:16:20.233092: step 57750, total loss = 0.61, predict loss = 0.14 (70.1 examples/sec; 0.057 sec/batch; 94h:14m:55s remains)
INFO - root - 2019-11-04 00:16:20.835253: step 57760, total loss = 0.37, predict loss = 0.08 (73.6 examples/sec; 0.054 sec/batch; 89h:44m:32s remains)
INFO - root - 2019-11-04 00:16:21.473443: step 57770, total loss = 0.55, predict loss = 0.13 (65.5 examples/sec; 0.061 sec/batch; 100h:49m:04s remains)
INFO - root - 2019-11-04 00:16:22.109623: step 57780, total loss = 0.56, predict loss = 0.13 (67.5 examples/sec; 0.059 sec/batch; 97h:53m:01s remains)
INFO - root - 2019-11-04 00:16:22.734252: step 57790, total loss = 0.46, predict loss = 0.10 (79.0 examples/sec; 0.051 sec/batch; 83h:33m:19s remains)
INFO - root - 2019-11-04 00:16:23.395053: step 57800, total loss = 0.55, predict loss = 0.13 (77.5 examples/sec; 0.052 sec/batch; 85h:12m:10s remains)
INFO - root - 2019-11-04 00:16:24.060317: step 57810, total loss = 0.52, predict loss = 0.12 (73.7 examples/sec; 0.054 sec/batch; 89h:36m:04s remains)
INFO - root - 2019-11-04 00:16:24.704671: step 57820, total loss = 0.43, predict loss = 0.10 (66.7 examples/sec; 0.060 sec/batch; 99h:00m:59s remains)
INFO - root - 2019-11-04 00:16:25.351352: step 57830, total loss = 0.57, predict loss = 0.14 (70.7 examples/sec; 0.057 sec/batch; 93h:25m:27s remains)
INFO - root - 2019-11-04 00:16:26.043551: step 57840, total loss = 0.41, predict loss = 0.10 (76.7 examples/sec; 0.052 sec/batch; 86h:06m:24s remains)
INFO - root - 2019-11-04 00:16:26.729163: step 57850, total loss = 0.43, predict loss = 0.10 (63.1 examples/sec; 0.063 sec/batch; 104h:42m:26s remains)
INFO - root - 2019-11-04 00:16:27.386988: step 57860, total loss = 0.31, predict loss = 0.07 (60.2 examples/sec; 0.066 sec/batch; 109h:38m:24s remains)
INFO - root - 2019-11-04 00:16:28.022401: step 57870, total loss = 0.64, predict loss = 0.15 (71.9 examples/sec; 0.056 sec/batch; 91h:52m:54s remains)
INFO - root - 2019-11-04 00:16:28.703066: step 57880, total loss = 0.43, predict loss = 0.09 (66.5 examples/sec; 0.060 sec/batch; 99h:21m:11s remains)
INFO - root - 2019-11-04 00:16:29.411211: step 57890, total loss = 0.32, predict loss = 0.07 (61.2 examples/sec; 0.065 sec/batch; 107h:54m:21s remains)
INFO - root - 2019-11-04 00:16:30.077985: step 57900, total loss = 0.51, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 94h:46m:18s remains)
INFO - root - 2019-11-04 00:16:30.698262: step 57910, total loss = 0.48, predict loss = 0.11 (79.7 examples/sec; 0.050 sec/batch; 82h:52m:32s remains)
INFO - root - 2019-11-04 00:16:31.313040: step 57920, total loss = 0.49, predict loss = 0.11 (80.2 examples/sec; 0.050 sec/batch; 82h:19m:45s remains)
INFO - root - 2019-11-04 00:16:32.004745: step 57930, total loss = 0.45, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 93h:31m:59s remains)
INFO - root - 2019-11-04 00:16:32.616258: step 57940, total loss = 0.41, predict loss = 0.09 (68.0 examples/sec; 0.059 sec/batch; 97h:06m:04s remains)
INFO - root - 2019-11-04 00:16:33.238524: step 57950, total loss = 0.60, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 91h:26m:26s remains)
INFO - root - 2019-11-04 00:16:33.898754: step 57960, total loss = 0.53, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 98h:05m:55s remains)
INFO - root - 2019-11-04 00:16:34.581060: step 57970, total loss = 0.61, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 91h:26m:15s remains)
INFO - root - 2019-11-04 00:16:35.202721: step 57980, total loss = 0.48, predict loss = 0.11 (74.0 examples/sec; 0.054 sec/batch; 89h:13m:11s remains)
INFO - root - 2019-11-04 00:16:35.818038: step 57990, total loss = 0.33, predict loss = 0.06 (72.8 examples/sec; 0.055 sec/batch; 90h:44m:20s remains)
INFO - root - 2019-11-04 00:16:36.539747: step 58000, total loss = 0.59, predict loss = 0.14 (60.2 examples/sec; 0.066 sec/batch; 109h:42m:59s remains)
INFO - root - 2019-11-04 00:16:37.196145: step 58010, total loss = 0.53, predict loss = 0.12 (80.5 examples/sec; 0.050 sec/batch; 82h:00m:23s remains)
INFO - root - 2019-11-04 00:16:37.806828: step 58020, total loss = 0.59, predict loss = 0.13 (63.9 examples/sec; 0.063 sec/batch; 103h:17m:51s remains)
INFO - root - 2019-11-04 00:16:38.468028: step 58030, total loss = 0.69, predict loss = 0.16 (70.3 examples/sec; 0.057 sec/batch; 93h:58m:06s remains)
INFO - root - 2019-11-04 00:16:39.134593: step 58040, total loss = 0.37, predict loss = 0.08 (56.1 examples/sec; 0.071 sec/batch; 117h:35m:31s remains)
INFO - root - 2019-11-04 00:16:39.760846: step 58050, total loss = 0.49, predict loss = 0.11 (63.3 examples/sec; 0.063 sec/batch; 104h:14m:44s remains)
INFO - root - 2019-11-04 00:16:40.396585: step 58060, total loss = 0.41, predict loss = 0.10 (71.6 examples/sec; 0.056 sec/batch; 92h:09m:38s remains)
INFO - root - 2019-11-04 00:16:41.002525: step 58070, total loss = 0.53, predict loss = 0.13 (70.6 examples/sec; 0.057 sec/batch; 93h:30m:04s remains)
INFO - root - 2019-11-04 00:16:41.648604: step 58080, total loss = 0.48, predict loss = 0.11 (62.9 examples/sec; 0.064 sec/batch; 104h:59m:48s remains)
INFO - root - 2019-11-04 00:16:42.270887: step 58090, total loss = 0.40, predict loss = 0.09 (75.0 examples/sec; 0.053 sec/batch; 88h:02m:24s remains)
INFO - root - 2019-11-04 00:16:42.864523: step 58100, total loss = 0.43, predict loss = 0.10 (73.0 examples/sec; 0.055 sec/batch; 90h:25m:08s remains)
INFO - root - 2019-11-04 00:16:43.482237: step 58110, total loss = 0.45, predict loss = 0.11 (87.7 examples/sec; 0.046 sec/batch; 75h:18m:10s remains)
INFO - root - 2019-11-04 00:16:44.112329: step 58120, total loss = 0.33, predict loss = 0.07 (69.3 examples/sec; 0.058 sec/batch; 95h:15m:21s remains)
INFO - root - 2019-11-04 00:16:44.773830: step 58130, total loss = 0.42, predict loss = 0.09 (80.8 examples/sec; 0.050 sec/batch; 81h:42m:05s remains)
INFO - root - 2019-11-04 00:16:45.436550: step 58140, total loss = 0.35, predict loss = 0.07 (78.1 examples/sec; 0.051 sec/batch; 84h:29m:47s remains)
INFO - root - 2019-11-04 00:16:46.066690: step 58150, total loss = 0.27, predict loss = 0.06 (75.7 examples/sec; 0.053 sec/batch; 87h:12m:47s remains)
INFO - root - 2019-11-04 00:16:46.684422: step 58160, total loss = 0.38, predict loss = 0.08 (72.1 examples/sec; 0.055 sec/batch; 91h:34m:21s remains)
INFO - root - 2019-11-04 00:16:47.301257: step 58170, total loss = 0.51, predict loss = 0.11 (87.1 examples/sec; 0.046 sec/batch; 75h:48m:02s remains)
INFO - root - 2019-11-04 00:16:47.920188: step 58180, total loss = 0.44, predict loss = 0.10 (61.0 examples/sec; 0.066 sec/batch; 108h:14m:32s remains)
INFO - root - 2019-11-04 00:16:48.549989: step 58190, total loss = 0.54, predict loss = 0.12 (72.7 examples/sec; 0.055 sec/batch; 90h:45m:51s remains)
INFO - root - 2019-11-04 00:16:49.169437: step 58200, total loss = 0.35, predict loss = 0.07 (73.7 examples/sec; 0.054 sec/batch; 89h:35m:23s remains)
INFO - root - 2019-11-04 00:16:49.813469: step 58210, total loss = 0.33, predict loss = 0.07 (69.6 examples/sec; 0.058 sec/batch; 94h:54m:26s remains)
INFO - root - 2019-11-04 00:16:50.466828: step 58220, total loss = 0.46, predict loss = 0.11 (60.8 examples/sec; 0.066 sec/batch; 108h:36m:20s remains)
INFO - root - 2019-11-04 00:16:51.106575: step 58230, total loss = 0.41, predict loss = 0.09 (71.4 examples/sec; 0.056 sec/batch; 92h:28m:50s remains)
INFO - root - 2019-11-04 00:16:51.761252: step 58240, total loss = 0.45, predict loss = 0.10 (68.4 examples/sec; 0.058 sec/batch; 96h:27m:07s remains)
INFO - root - 2019-11-04 00:16:52.463898: step 58250, total loss = 0.56, predict loss = 0.12 (55.3 examples/sec; 0.072 sec/batch; 119h:24m:07s remains)
INFO - root - 2019-11-04 00:16:53.156229: step 58260, total loss = 0.47, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 95h:59m:44s remains)
INFO - root - 2019-11-04 00:16:53.792116: step 58270, total loss = 0.52, predict loss = 0.12 (69.1 examples/sec; 0.058 sec/batch; 95h:29m:56s remains)
INFO - root - 2019-11-04 00:16:54.536839: step 58280, total loss = 0.51, predict loss = 0.13 (63.2 examples/sec; 0.063 sec/batch; 104h:30m:49s remains)
INFO - root - 2019-11-04 00:16:55.315987: step 58290, total loss = 0.79, predict loss = 0.20 (54.8 examples/sec; 0.073 sec/batch; 120h:24m:25s remains)
INFO - root - 2019-11-04 00:16:56.068587: step 58300, total loss = 0.46, predict loss = 0.11 (60.9 examples/sec; 0.066 sec/batch; 108h:25m:13s remains)
INFO - root - 2019-11-04 00:16:56.822714: step 58310, total loss = 0.53, predict loss = 0.13 (67.8 examples/sec; 0.059 sec/batch; 97h:20m:28s remains)
INFO - root - 2019-11-04 00:16:57.580682: step 58320, total loss = 0.44, predict loss = 0.10 (53.1 examples/sec; 0.075 sec/batch; 124h:14m:18s remains)
INFO - root - 2019-11-04 00:16:58.346473: step 58330, total loss = 0.50, predict loss = 0.12 (57.6 examples/sec; 0.069 sec/batch; 114h:31m:29s remains)
INFO - root - 2019-11-04 00:16:59.098726: step 58340, total loss = 0.44, predict loss = 0.11 (62.0 examples/sec; 0.065 sec/batch; 106h:30m:31s remains)
INFO - root - 2019-11-04 00:16:59.868914: step 58350, total loss = 0.36, predict loss = 0.08 (56.8 examples/sec; 0.070 sec/batch; 116h:19m:12s remains)
INFO - root - 2019-11-04 00:17:00.634233: step 58360, total loss = 0.30, predict loss = 0.06 (63.2 examples/sec; 0.063 sec/batch; 104h:26m:51s remains)
INFO - root - 2019-11-04 00:17:01.412306: step 58370, total loss = 0.25, predict loss = 0.05 (55.9 examples/sec; 0.072 sec/batch; 118h:07m:46s remains)
INFO - root - 2019-11-04 00:17:02.155611: step 58380, total loss = 0.63, predict loss = 0.16 (66.4 examples/sec; 0.060 sec/batch; 99h:27m:45s remains)
INFO - root - 2019-11-04 00:17:02.934109: step 58390, total loss = 0.45, predict loss = 0.10 (59.9 examples/sec; 0.067 sec/batch; 110h:14m:27s remains)
INFO - root - 2019-11-04 00:17:03.681354: step 58400, total loss = 0.38, predict loss = 0.09 (62.4 examples/sec; 0.064 sec/batch; 105h:46m:59s remains)
INFO - root - 2019-11-04 00:17:04.432801: step 58410, total loss = 0.47, predict loss = 0.11 (65.6 examples/sec; 0.061 sec/batch; 100h:34m:17s remains)
INFO - root - 2019-11-04 00:17:05.118395: step 58420, total loss = 0.49, predict loss = 0.11 (77.3 examples/sec; 0.052 sec/batch; 85h:22m:27s remains)
INFO - root - 2019-11-04 00:17:05.783963: step 58430, total loss = 0.42, predict loss = 0.09 (66.7 examples/sec; 0.060 sec/batch; 98h:54m:51s remains)
INFO - root - 2019-11-04 00:17:06.459135: step 58440, total loss = 0.62, predict loss = 0.15 (62.1 examples/sec; 0.064 sec/batch; 106h:15m:56s remains)
INFO - root - 2019-11-04 00:17:07.231000: step 58450, total loss = 0.44, predict loss = 0.11 (50.5 examples/sec; 0.079 sec/batch; 130h:40m:19s remains)
INFO - root - 2019-11-04 00:17:07.937182: step 58460, total loss = 0.57, predict loss = 0.13 (64.6 examples/sec; 0.062 sec/batch; 102h:12m:09s remains)
INFO - root - 2019-11-04 00:17:08.594179: step 58470, total loss = 0.49, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 89h:17m:26s remains)
INFO - root - 2019-11-04 00:17:09.278181: step 58480, total loss = 0.30, predict loss = 0.07 (68.2 examples/sec; 0.059 sec/batch; 96h:44m:27s remains)
INFO - root - 2019-11-04 00:17:09.956456: step 58490, total loss = 0.58, predict loss = 0.13 (80.5 examples/sec; 0.050 sec/batch; 81h:57m:46s remains)
INFO - root - 2019-11-04 00:17:10.712558: step 58500, total loss = 0.29, predict loss = 0.06 (54.8 examples/sec; 0.073 sec/batch; 120h:28m:46s remains)
INFO - root - 2019-11-04 00:17:11.587797: step 58510, total loss = 0.57, predict loss = 0.14 (52.9 examples/sec; 0.076 sec/batch; 124h:44m:05s remains)
INFO - root - 2019-11-04 00:17:12.326373: step 58520, total loss = 0.48, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 99h:18m:55s remains)
INFO - root - 2019-11-04 00:17:13.011662: step 58530, total loss = 0.55, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 93h:26m:58s remains)
INFO - root - 2019-11-04 00:17:13.808779: step 58540, total loss = 0.59, predict loss = 0.13 (64.2 examples/sec; 0.062 sec/batch; 102h:49m:32s remains)
INFO - root - 2019-11-04 00:17:14.582890: step 58550, total loss = 0.45, predict loss = 0.10 (52.7 examples/sec; 0.076 sec/batch; 125h:19m:23s remains)
INFO - root - 2019-11-04 00:17:15.289337: step 58560, total loss = 0.60, predict loss = 0.14 (65.5 examples/sec; 0.061 sec/batch; 100h:45m:27s remains)
INFO - root - 2019-11-04 00:17:16.013728: step 58570, total loss = 0.72, predict loss = 0.17 (72.0 examples/sec; 0.056 sec/batch; 91h:40m:08s remains)
INFO - root - 2019-11-04 00:17:16.701444: step 58580, total loss = 0.69, predict loss = 0.16 (70.6 examples/sec; 0.057 sec/batch; 93h:33m:31s remains)
INFO - root - 2019-11-04 00:17:17.423130: step 58590, total loss = 0.53, predict loss = 0.12 (62.6 examples/sec; 0.064 sec/batch; 105h:22m:59s remains)
INFO - root - 2019-11-04 00:17:18.113667: step 58600, total loss = 0.50, predict loss = 0.12 (71.0 examples/sec; 0.056 sec/batch; 92h:58m:25s remains)
INFO - root - 2019-11-04 00:17:18.806688: step 58610, total loss = 0.68, predict loss = 0.17 (67.3 examples/sec; 0.059 sec/batch; 98h:08m:14s remains)
INFO - root - 2019-11-04 00:17:19.529195: step 58620, total loss = 0.36, predict loss = 0.08 (71.6 examples/sec; 0.056 sec/batch; 92h:13m:17s remains)
INFO - root - 2019-11-04 00:17:20.229250: step 58630, total loss = 0.54, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 98h:37m:56s remains)
INFO - root - 2019-11-04 00:17:20.921635: step 58640, total loss = 0.49, predict loss = 0.11 (63.6 examples/sec; 0.063 sec/batch; 103h:46m:04s remains)
INFO - root - 2019-11-04 00:17:21.719491: step 58650, total loss = 0.59, predict loss = 0.13 (52.0 examples/sec; 0.077 sec/batch; 126h:54m:07s remains)
INFO - root - 2019-11-04 00:17:22.551438: step 58660, total loss = 0.54, predict loss = 0.13 (61.0 examples/sec; 0.066 sec/batch; 108h:17m:41s remains)
INFO - root - 2019-11-04 00:17:23.310983: step 58670, total loss = 0.59, predict loss = 0.14 (59.7 examples/sec; 0.067 sec/batch; 110h:33m:34s remains)
INFO - root - 2019-11-04 00:17:24.042739: step 58680, total loss = 0.46, predict loss = 0.10 (53.8 examples/sec; 0.074 sec/batch; 122h:39m:20s remains)
INFO - root - 2019-11-04 00:17:24.822083: step 58690, total loss = 0.62, predict loss = 0.15 (63.0 examples/sec; 0.063 sec/batch; 104h:43m:53s remains)
INFO - root - 2019-11-04 00:17:25.578207: step 58700, total loss = 0.55, predict loss = 0.13 (61.6 examples/sec; 0.065 sec/batch; 107h:14m:58s remains)
INFO - root - 2019-11-04 00:17:26.318922: step 58710, total loss = 0.97, predict loss = 0.23 (67.4 examples/sec; 0.059 sec/batch; 97h:56m:58s remains)
INFO - root - 2019-11-04 00:17:26.999821: step 58720, total loss = 0.84, predict loss = 0.20 (65.2 examples/sec; 0.061 sec/batch; 101h:11m:18s remains)
INFO - root - 2019-11-04 00:17:27.698815: step 58730, total loss = 0.95, predict loss = 0.23 (64.6 examples/sec; 0.062 sec/batch; 102h:07m:08s remains)
INFO - root - 2019-11-04 00:17:28.368875: step 58740, total loss = 0.54, predict loss = 0.12 (82.5 examples/sec; 0.048 sec/batch; 79h:59m:23s remains)
INFO - root - 2019-11-04 00:17:29.066115: step 58750, total loss = 0.81, predict loss = 0.19 (59.5 examples/sec; 0.067 sec/batch; 110h:54m:17s remains)
INFO - root - 2019-11-04 00:17:29.758237: step 58760, total loss = 0.79, predict loss = 0.19 (76.7 examples/sec; 0.052 sec/batch; 86h:02m:27s remains)
INFO - root - 2019-11-04 00:17:30.426748: step 58770, total loss = 0.53, predict loss = 0.12 (65.2 examples/sec; 0.061 sec/batch; 101h:13m:37s remains)
INFO - root - 2019-11-04 00:17:31.120145: step 58780, total loss = 0.60, predict loss = 0.14 (60.5 examples/sec; 0.066 sec/batch; 109h:08m:54s remains)
INFO - root - 2019-11-04 00:17:31.941045: step 58790, total loss = 0.58, predict loss = 0.13 (48.5 examples/sec; 0.082 sec/batch; 135h:59m:28s remains)
INFO - root - 2019-11-04 00:17:32.723866: step 58800, total loss = 0.60, predict loss = 0.13 (50.1 examples/sec; 0.080 sec/batch; 131h:45m:53s remains)
INFO - root - 2019-11-04 00:17:33.467456: step 58810, total loss = 0.69, predict loss = 0.17 (63.9 examples/sec; 0.063 sec/batch; 103h:20m:58s remains)
INFO - root - 2019-11-04 00:17:34.163249: step 58820, total loss = 0.51, predict loss = 0.12 (70.4 examples/sec; 0.057 sec/batch; 93h:44m:26s remains)
INFO - root - 2019-11-04 00:17:34.895591: step 58830, total loss = 0.51, predict loss = 0.12 (52.8 examples/sec; 0.076 sec/batch; 125h:05m:16s remains)
INFO - root - 2019-11-04 00:17:35.646592: step 58840, total loss = 0.57, predict loss = 0.14 (64.2 examples/sec; 0.062 sec/batch; 102h:54m:07s remains)
INFO - root - 2019-11-04 00:17:36.351601: step 58850, total loss = 0.59, predict loss = 0.13 (59.2 examples/sec; 0.068 sec/batch; 111h:30m:57s remains)
INFO - root - 2019-11-04 00:17:37.098512: step 58860, total loss = 0.54, predict loss = 0.13 (55.4 examples/sec; 0.072 sec/batch; 119h:13m:01s remains)
INFO - root - 2019-11-04 00:17:37.859179: step 58870, total loss = 0.42, predict loss = 0.09 (55.6 examples/sec; 0.072 sec/batch; 118h:43m:57s remains)
INFO - root - 2019-11-04 00:17:38.562767: step 58880, total loss = 0.56, predict loss = 0.13 (58.3 examples/sec; 0.069 sec/batch; 113h:13m:58s remains)
INFO - root - 2019-11-04 00:17:39.271724: step 58890, total loss = 0.62, predict loss = 0.14 (65.2 examples/sec; 0.061 sec/batch; 101h:13m:39s remains)
INFO - root - 2019-11-04 00:17:39.970153: step 58900, total loss = 0.60, predict loss = 0.14 (75.0 examples/sec; 0.053 sec/batch; 88h:04m:08s remains)
INFO - root - 2019-11-04 00:17:40.675084: step 58910, total loss = 0.45, predict loss = 0.10 (68.8 examples/sec; 0.058 sec/batch; 95h:55m:23s remains)
INFO - root - 2019-11-04 00:17:41.374173: step 58920, total loss = 0.57, predict loss = 0.13 (62.7 examples/sec; 0.064 sec/batch; 105h:12m:42s remains)
INFO - root - 2019-11-04 00:17:42.156195: step 58930, total loss = 0.51, predict loss = 0.12 (62.6 examples/sec; 0.064 sec/batch; 105h:31m:42s remains)
INFO - root - 2019-11-04 00:17:42.880490: step 58940, total loss = 0.55, predict loss = 0.13 (65.8 examples/sec; 0.061 sec/batch; 100h:17m:45s remains)
INFO - root - 2019-11-04 00:17:43.544998: step 58950, total loss = 0.59, predict loss = 0.14 (68.1 examples/sec; 0.059 sec/batch; 96h:55m:34s remains)
INFO - root - 2019-11-04 00:17:44.277622: step 58960, total loss = 0.39, predict loss = 0.09 (61.4 examples/sec; 0.065 sec/batch; 107h:35m:36s remains)
INFO - root - 2019-11-04 00:17:45.065972: step 58970, total loss = 0.62, predict loss = 0.15 (52.0 examples/sec; 0.077 sec/batch; 126h:57m:19s remains)
INFO - root - 2019-11-04 00:17:45.782657: step 58980, total loss = 0.38, predict loss = 0.09 (66.8 examples/sec; 0.060 sec/batch; 98h:50m:00s remains)
INFO - root - 2019-11-04 00:17:46.493289: step 58990, total loss = 0.43, predict loss = 0.10 (66.0 examples/sec; 0.061 sec/batch; 99h:56m:38s remains)
INFO - root - 2019-11-04 00:17:47.191554: step 59000, total loss = 0.49, predict loss = 0.11 (82.7 examples/sec; 0.048 sec/batch; 79h:50m:13s remains)
INFO - root - 2019-11-04 00:17:47.874589: step 59010, total loss = 0.56, predict loss = 0.13 (67.0 examples/sec; 0.060 sec/batch; 98h:31m:48s remains)
INFO - root - 2019-11-04 00:17:48.579418: step 59020, total loss = 0.47, predict loss = 0.11 (64.9 examples/sec; 0.062 sec/batch; 101h:39m:01s remains)
INFO - root - 2019-11-04 00:17:49.278123: step 59030, total loss = 0.48, predict loss = 0.11 (70.9 examples/sec; 0.056 sec/batch; 93h:09m:56s remains)
INFO - root - 2019-11-04 00:17:49.997443: step 59040, total loss = 0.44, predict loss = 0.10 (62.4 examples/sec; 0.064 sec/batch; 105h:45m:05s remains)
INFO - root - 2019-11-04 00:17:50.688723: step 59050, total loss = 0.57, predict loss = 0.14 (71.0 examples/sec; 0.056 sec/batch; 93h:01m:41s remains)
INFO - root - 2019-11-04 00:17:51.395657: step 59060, total loss = 0.55, predict loss = 0.12 (71.6 examples/sec; 0.056 sec/batch; 92h:12m:24s remains)
INFO - root - 2019-11-04 00:17:52.132673: step 59070, total loss = 0.45, predict loss = 0.10 (52.4 examples/sec; 0.076 sec/batch; 126h:04m:50s remains)
INFO - root - 2019-11-04 00:17:52.989718: step 59080, total loss = 0.61, predict loss = 0.14 (54.4 examples/sec; 0.074 sec/batch; 121h:18m:45s remains)
INFO - root - 2019-11-04 00:17:53.788892: step 59090, total loss = 0.49, predict loss = 0.12 (55.9 examples/sec; 0.072 sec/batch; 118h:06m:38s remains)
INFO - root - 2019-11-04 00:17:54.515910: step 59100, total loss = 0.59, predict loss = 0.13 (67.1 examples/sec; 0.060 sec/batch; 98h:21m:31s remains)
INFO - root - 2019-11-04 00:17:55.256523: step 59110, total loss = 0.73, predict loss = 0.18 (58.8 examples/sec; 0.068 sec/batch; 112h:12m:34s remains)
INFO - root - 2019-11-04 00:17:56.001478: step 59120, total loss = 0.65, predict loss = 0.15 (51.2 examples/sec; 0.078 sec/batch; 128h:53m:12s remains)
INFO - root - 2019-11-04 00:17:56.728978: step 59130, total loss = 0.52, predict loss = 0.12 (77.2 examples/sec; 0.052 sec/batch; 85h:27m:00s remains)
INFO - root - 2019-11-04 00:17:57.492069: step 59140, total loss = 0.77, predict loss = 0.18 (62.0 examples/sec; 0.065 sec/batch; 106h:27m:15s remains)
INFO - root - 2019-11-04 00:17:58.191513: step 59150, total loss = 0.65, predict loss = 0.16 (71.6 examples/sec; 0.056 sec/batch; 92h:13m:39s remains)
INFO - root - 2019-11-04 00:17:58.860944: step 59160, total loss = 0.74, predict loss = 0.17 (76.7 examples/sec; 0.052 sec/batch; 86h:06m:43s remains)
INFO - root - 2019-11-04 00:17:59.551883: step 59170, total loss = 0.69, predict loss = 0.18 (73.9 examples/sec; 0.054 sec/batch; 89h:18m:12s remains)
INFO - root - 2019-11-04 00:18:00.247795: step 59180, total loss = 0.71, predict loss = 0.18 (66.5 examples/sec; 0.060 sec/batch; 99h:12m:39s remains)
INFO - root - 2019-11-04 00:18:00.962650: step 59190, total loss = 0.66, predict loss = 0.16 (58.3 examples/sec; 0.069 sec/batch; 113h:10m:48s remains)
INFO - root - 2019-11-04 00:18:01.673275: step 59200, total loss = 0.66, predict loss = 0.15 (62.0 examples/sec; 0.064 sec/batch; 106h:23m:42s remains)
INFO - root - 2019-11-04 00:18:02.371255: step 59210, total loss = 0.51, predict loss = 0.11 (64.2 examples/sec; 0.062 sec/batch; 102h:51m:30s remains)
INFO - root - 2019-11-04 00:18:03.103295: step 59220, total loss = 0.59, predict loss = 0.14 (58.5 examples/sec; 0.068 sec/batch; 112h:51m:23s remains)
INFO - root - 2019-11-04 00:18:03.816155: step 59230, total loss = 0.54, predict loss = 0.14 (57.0 examples/sec; 0.070 sec/batch; 115h:49m:23s remains)
INFO - root - 2019-11-04 00:18:04.525369: step 59240, total loss = 0.46, predict loss = 0.10 (69.4 examples/sec; 0.058 sec/batch; 95h:07m:00s remains)
INFO - root - 2019-11-04 00:18:05.243467: step 59250, total loss = 0.41, predict loss = 0.08 (72.5 examples/sec; 0.055 sec/batch; 91h:00m:31s remains)
INFO - root - 2019-11-04 00:18:05.975904: step 59260, total loss = 0.43, predict loss = 0.10 (78.1 examples/sec; 0.051 sec/batch; 84h:30m:26s remains)
INFO - root - 2019-11-04 00:18:06.708633: step 59270, total loss = 0.49, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 98h:37m:18s remains)
INFO - root - 2019-11-04 00:18:07.405212: step 59280, total loss = 0.48, predict loss = 0.11 (60.8 examples/sec; 0.066 sec/batch; 108h:38m:07s remains)
INFO - root - 2019-11-04 00:18:08.052647: step 59290, total loss = 0.50, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 96h:38m:47s remains)
INFO - root - 2019-11-04 00:18:08.780668: step 59300, total loss = 0.49, predict loss = 0.11 (62.1 examples/sec; 0.064 sec/batch; 106h:20m:55s remains)
INFO - root - 2019-11-04 00:18:09.574227: step 59310, total loss = 0.65, predict loss = 0.15 (60.5 examples/sec; 0.066 sec/batch; 109h:09m:29s remains)
INFO - root - 2019-11-04 00:18:10.366804: step 59320, total loss = 0.45, predict loss = 0.10 (61.6 examples/sec; 0.065 sec/batch; 107h:07m:55s remains)
INFO - root - 2019-11-04 00:18:11.147173: step 59330, total loss = 0.63, predict loss = 0.15 (53.7 examples/sec; 0.074 sec/batch; 122h:48m:30s remains)
INFO - root - 2019-11-04 00:18:11.943719: step 59340, total loss = 0.59, predict loss = 0.14 (64.7 examples/sec; 0.062 sec/batch; 101h:57m:09s remains)
INFO - root - 2019-11-04 00:18:12.666446: step 59350, total loss = 0.55, predict loss = 0.13 (68.4 examples/sec; 0.058 sec/batch; 96h:31m:01s remains)
INFO - root - 2019-11-04 00:18:13.343633: step 59360, total loss = 0.71, predict loss = 0.16 (69.3 examples/sec; 0.058 sec/batch; 95h:12m:04s remains)
INFO - root - 2019-11-04 00:18:14.046725: step 59370, total loss = 0.67, predict loss = 0.16 (70.2 examples/sec; 0.057 sec/batch; 94h:02m:41s remains)
INFO - root - 2019-11-04 00:18:14.715169: step 59380, total loss = 0.56, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 91h:22m:07s remains)
INFO - root - 2019-11-04 00:18:15.392031: step 59390, total loss = 0.58, predict loss = 0.14 (63.0 examples/sec; 0.063 sec/batch; 104h:45m:33s remains)
INFO - root - 2019-11-04 00:18:16.157357: step 59400, total loss = 0.62, predict loss = 0.15 (46.6 examples/sec; 0.086 sec/batch; 141h:31m:33s remains)
INFO - root - 2019-11-04 00:18:16.966078: step 59410, total loss = 0.57, predict loss = 0.14 (59.2 examples/sec; 0.068 sec/batch; 111h:27m:48s remains)
INFO - root - 2019-11-04 00:18:17.733840: step 59420, total loss = 0.68, predict loss = 0.17 (61.2 examples/sec; 0.065 sec/batch; 107h:50m:39s remains)
INFO - root - 2019-11-04 00:18:18.451352: step 59430, total loss = 0.51, predict loss = 0.11 (59.5 examples/sec; 0.067 sec/batch; 110h:52m:29s remains)
INFO - root - 2019-11-04 00:18:19.191284: step 59440, total loss = 0.61, predict loss = 0.14 (67.1 examples/sec; 0.060 sec/batch; 98h:20m:11s remains)
INFO - root - 2019-11-04 00:18:19.995275: step 59450, total loss = 0.56, predict loss = 0.13 (57.3 examples/sec; 0.070 sec/batch; 115h:09m:58s remains)
INFO - root - 2019-11-04 00:18:20.765240: step 59460, total loss = 0.55, predict loss = 0.13 (63.6 examples/sec; 0.063 sec/batch; 103h:51m:32s remains)
INFO - root - 2019-11-04 00:18:21.546835: step 59470, total loss = 0.53, predict loss = 0.12 (60.6 examples/sec; 0.066 sec/batch; 108h:56m:29s remains)
INFO - root - 2019-11-04 00:18:22.278948: step 59480, total loss = 0.58, predict loss = 0.13 (69.3 examples/sec; 0.058 sec/batch; 95h:12m:24s remains)
INFO - root - 2019-11-04 00:18:23.047975: step 59490, total loss = 0.59, predict loss = 0.14 (59.1 examples/sec; 0.068 sec/batch; 111h:39m:33s remains)
INFO - root - 2019-11-04 00:18:23.774647: step 59500, total loss = 0.60, predict loss = 0.14 (65.2 examples/sec; 0.061 sec/batch; 101h:11m:57s remains)
INFO - root - 2019-11-04 00:18:24.526779: step 59510, total loss = 0.60, predict loss = 0.14 (52.5 examples/sec; 0.076 sec/batch; 125h:50m:01s remains)
INFO - root - 2019-11-04 00:18:25.297481: step 59520, total loss = 0.53, predict loss = 0.13 (59.5 examples/sec; 0.067 sec/batch; 110h:57m:02s remains)
INFO - root - 2019-11-04 00:18:25.994168: step 59530, total loss = 0.50, predict loss = 0.11 (73.2 examples/sec; 0.055 sec/batch; 90h:07m:59s remains)
INFO - root - 2019-11-04 00:18:26.756780: step 59540, total loss = 0.50, predict loss = 0.11 (56.6 examples/sec; 0.071 sec/batch; 116h:31m:55s remains)
INFO - root - 2019-11-04 00:18:27.554363: step 59550, total loss = 0.50, predict loss = 0.12 (58.3 examples/sec; 0.069 sec/batch; 113h:14m:38s remains)
INFO - root - 2019-11-04 00:18:28.285309: step 59560, total loss = 0.54, predict loss = 0.13 (66.9 examples/sec; 0.060 sec/batch; 98h:42m:55s remains)
INFO - root - 2019-11-04 00:18:29.008323: step 59570, total loss = 0.57, predict loss = 0.14 (61.9 examples/sec; 0.065 sec/batch; 106h:38m:30s remains)
INFO - root - 2019-11-04 00:18:29.713858: step 59580, total loss = 0.59, predict loss = 0.14 (53.4 examples/sec; 0.075 sec/batch; 123h:29m:34s remains)
INFO - root - 2019-11-04 00:18:30.459583: step 59590, total loss = 0.43, predict loss = 0.09 (70.5 examples/sec; 0.057 sec/batch; 93h:37m:41s remains)
INFO - root - 2019-11-04 00:18:31.162834: step 59600, total loss = 0.41, predict loss = 0.09 (74.4 examples/sec; 0.054 sec/batch; 88h:43m:54s remains)
INFO - root - 2019-11-04 00:18:31.878545: step 59610, total loss = 0.51, predict loss = 0.12 (57.2 examples/sec; 0.070 sec/batch; 115h:22m:31s remains)
INFO - root - 2019-11-04 00:18:32.585168: step 59620, total loss = 0.63, predict loss = 0.15 (77.8 examples/sec; 0.051 sec/batch; 84h:47m:48s remains)
INFO - root - 2019-11-04 00:18:33.236967: step 59630, total loss = 0.60, predict loss = 0.14 (75.4 examples/sec; 0.053 sec/batch; 87h:30m:57s remains)
INFO - root - 2019-11-04 00:18:33.906802: step 59640, total loss = 0.62, predict loss = 0.14 (72.1 examples/sec; 0.055 sec/batch; 91h:33m:43s remains)
INFO - root - 2019-11-04 00:18:34.637467: step 59650, total loss = 0.64, predict loss = 0.15 (59.3 examples/sec; 0.067 sec/batch; 111h:21m:16s remains)
INFO - root - 2019-11-04 00:18:35.380491: step 59660, total loss = 0.57, predict loss = 0.13 (66.1 examples/sec; 0.061 sec/batch; 99h:50m:21s remains)
INFO - root - 2019-11-04 00:18:36.236714: step 59670, total loss = 0.64, predict loss = 0.15 (53.5 examples/sec; 0.075 sec/batch; 123h:16m:06s remains)
INFO - root - 2019-11-04 00:18:37.027204: step 59680, total loss = 0.55, predict loss = 0.12 (60.4 examples/sec; 0.066 sec/batch; 109h:18m:07s remains)
INFO - root - 2019-11-04 00:18:37.802303: step 59690, total loss = 0.53, predict loss = 0.13 (57.2 examples/sec; 0.070 sec/batch; 115h:24m:01s remains)
INFO - root - 2019-11-04 00:18:38.571113: step 59700, total loss = 0.47, predict loss = 0.11 (62.7 examples/sec; 0.064 sec/batch; 105h:20m:38s remains)
INFO - root - 2019-11-04 00:18:39.313343: step 59710, total loss = 0.45, predict loss = 0.11 (63.0 examples/sec; 0.064 sec/batch; 104h:50m:13s remains)
INFO - root - 2019-11-04 00:18:40.080013: step 59720, total loss = 0.62, predict loss = 0.15 (64.4 examples/sec; 0.062 sec/batch; 102h:25m:04s remains)
INFO - root - 2019-11-04 00:18:40.875654: step 59730, total loss = 0.65, predict loss = 0.17 (57.9 examples/sec; 0.069 sec/batch; 113h:59m:20s remains)
INFO - root - 2019-11-04 00:18:41.646011: step 59740, total loss = 0.86, predict loss = 0.21 (49.2 examples/sec; 0.081 sec/batch; 134h:15m:27s remains)
INFO - root - 2019-11-04 00:18:42.311409: step 59750, total loss = 0.67, predict loss = 0.17 (70.5 examples/sec; 0.057 sec/batch; 93h:39m:02s remains)
INFO - root - 2019-11-04 00:18:43.055958: step 59760, total loss = 0.84, predict loss = 0.20 (53.2 examples/sec; 0.075 sec/batch; 124h:07m:40s remains)
INFO - root - 2019-11-04 00:18:43.886055: step 59770, total loss = 0.63, predict loss = 0.15 (66.3 examples/sec; 0.060 sec/batch; 99h:31m:18s remains)
INFO - root - 2019-11-04 00:18:44.584898: step 59780, total loss = 0.53, predict loss = 0.12 (52.8 examples/sec; 0.076 sec/batch; 124h:58m:58s remains)
INFO - root - 2019-11-04 00:18:45.322998: step 59790, total loss = 0.65, predict loss = 0.16 (65.0 examples/sec; 0.062 sec/batch; 101h:29m:34s remains)
INFO - root - 2019-11-04 00:18:46.094735: step 59800, total loss = 0.84, predict loss = 0.20 (67.2 examples/sec; 0.060 sec/batch; 98h:17m:05s remains)
INFO - root - 2019-11-04 00:18:46.803177: step 59810, total loss = 0.68, predict loss = 0.16 (63.6 examples/sec; 0.063 sec/batch; 103h:43m:04s remains)
INFO - root - 2019-11-04 00:18:47.483181: step 59820, total loss = 0.67, predict loss = 0.16 (75.1 examples/sec; 0.053 sec/batch; 87h:52m:48s remains)
INFO - root - 2019-11-04 00:18:48.178615: step 59830, total loss = 0.75, predict loss = 0.19 (55.5 examples/sec; 0.072 sec/batch; 118h:58m:15s remains)
INFO - root - 2019-11-04 00:18:48.959173: step 59840, total loss = 0.58, predict loss = 0.14 (56.5 examples/sec; 0.071 sec/batch; 116h:46m:59s remains)
INFO - root - 2019-11-04 00:18:49.696671: step 59850, total loss = 0.75, predict loss = 0.18 (50.7 examples/sec; 0.079 sec/batch; 130h:13m:52s remains)
INFO - root - 2019-11-04 00:18:50.397492: step 59860, total loss = 0.50, predict loss = 0.12 (62.4 examples/sec; 0.064 sec/batch; 105h:46m:38s remains)
INFO - root - 2019-11-04 00:18:51.104115: step 59870, total loss = 0.47, predict loss = 0.11 (57.2 examples/sec; 0.070 sec/batch; 115h:26m:27s remains)
INFO - root - 2019-11-04 00:18:51.930642: step 59880, total loss = 0.56, predict loss = 0.12 (56.0 examples/sec; 0.071 sec/batch; 117h:46m:48s remains)
INFO - root - 2019-11-04 00:18:52.658404: step 59890, total loss = 0.82, predict loss = 0.19 (63.8 examples/sec; 0.063 sec/batch; 103h:30m:46s remains)
INFO - root - 2019-11-04 00:18:53.339233: step 59900, total loss = 0.53, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 90h:52m:26s remains)
INFO - root - 2019-11-04 00:18:54.054560: step 59910, total loss = 0.62, predict loss = 0.15 (60.9 examples/sec; 0.066 sec/batch; 108h:24m:24s remains)
INFO - root - 2019-11-04 00:18:54.750510: step 59920, total loss = 0.69, predict loss = 0.16 (57.7 examples/sec; 0.069 sec/batch; 114h:25m:20s remains)
INFO - root - 2019-11-04 00:18:55.465317: step 59930, total loss = 0.66, predict loss = 0.15 (64.4 examples/sec; 0.062 sec/batch; 102h:32m:48s remains)
INFO - root - 2019-11-04 00:18:56.119618: step 59940, total loss = 0.56, predict loss = 0.13 (73.5 examples/sec; 0.054 sec/batch; 89h:49m:52s remains)
INFO - root - 2019-11-04 00:18:56.826362: step 59950, total loss = 0.66, predict loss = 0.14 (71.7 examples/sec; 0.056 sec/batch; 91h:59m:18s remains)
INFO - root - 2019-11-04 00:18:57.504541: step 59960, total loss = 0.57, predict loss = 0.13 (61.9 examples/sec; 0.065 sec/batch; 106h:32m:39s remains)
INFO - root - 2019-11-04 00:18:58.218289: step 59970, total loss = 0.59, predict loss = 0.14 (60.1 examples/sec; 0.067 sec/batch; 109h:47m:04s remains)
INFO - root - 2019-11-04 00:18:58.859393: step 59980, total loss = 0.47, predict loss = 0.11 (100.5 examples/sec; 0.040 sec/batch; 65h:40m:57s remains)
INFO - root - 2019-11-04 00:18:59.360250: step 59990, total loss = 0.60, predict loss = 0.14 (94.0 examples/sec; 0.043 sec/batch; 70h:13m:21s remains)
INFO - root - 2019-11-04 00:18:59.868531: step 60000, total loss = 0.54, predict loss = 0.12 (90.0 examples/sec; 0.044 sec/batch; 73h:20m:55s remains)
INFO - root - 2019-11-04 00:19:01.961481: step 60010, total loss = 0.46, predict loss = 0.11 (54.9 examples/sec; 0.073 sec/batch; 120h:10m:29s remains)
INFO - root - 2019-11-04 00:19:02.682168: step 60020, total loss = 0.37, predict loss = 0.08 (70.1 examples/sec; 0.057 sec/batch; 94h:07m:40s remains)
INFO - root - 2019-11-04 00:19:03.385973: step 60030, total loss = 0.49, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 93h:30m:34s remains)
INFO - root - 2019-11-04 00:19:04.102794: step 60040, total loss = 0.43, predict loss = 0.10 (64.3 examples/sec; 0.062 sec/batch; 102h:43m:13s remains)
INFO - root - 2019-11-04 00:19:04.816345: step 60050, total loss = 0.52, predict loss = 0.13 (69.9 examples/sec; 0.057 sec/batch; 94h:23m:52s remains)
INFO - root - 2019-11-04 00:19:05.548463: step 60060, total loss = 0.46, predict loss = 0.10 (61.7 examples/sec; 0.065 sec/batch; 106h:54m:48s remains)
INFO - root - 2019-11-04 00:19:06.235422: step 60070, total loss = 0.74, predict loss = 0.17 (60.9 examples/sec; 0.066 sec/batch; 108h:25m:17s remains)
INFO - root - 2019-11-04 00:19:06.962847: step 60080, total loss = 0.55, predict loss = 0.13 (60.0 examples/sec; 0.067 sec/batch; 109h:56m:48s remains)
INFO - root - 2019-11-04 00:19:07.654351: step 60090, total loss = 0.51, predict loss = 0.11 (65.6 examples/sec; 0.061 sec/batch; 100h:37m:04s remains)
INFO - root - 2019-11-04 00:19:08.379046: step 60100, total loss = 0.67, predict loss = 0.15 (57.0 examples/sec; 0.070 sec/batch; 115h:44m:13s remains)
INFO - root - 2019-11-04 00:19:09.116504: step 60110, total loss = 0.78, predict loss = 0.20 (58.9 examples/sec; 0.068 sec/batch; 112h:00m:27s remains)
INFO - root - 2019-11-04 00:19:09.849569: step 60120, total loss = 0.43, predict loss = 0.08 (58.1 examples/sec; 0.069 sec/batch; 113h:34m:41s remains)
INFO - root - 2019-11-04 00:19:10.592155: step 60130, total loss = 0.59, predict loss = 0.16 (62.8 examples/sec; 0.064 sec/batch; 105h:03m:15s remains)
INFO - root - 2019-11-04 00:19:11.403620: step 60140, total loss = 0.54, predict loss = 0.13 (55.9 examples/sec; 0.072 sec/batch; 118h:09m:58s remains)
INFO - root - 2019-11-04 00:19:12.167356: step 60150, total loss = 0.61, predict loss = 0.14 (74.4 examples/sec; 0.054 sec/batch; 88h:44m:18s remains)
INFO - root - 2019-11-04 00:19:12.886047: step 60160, total loss = 0.50, predict loss = 0.11 (65.0 examples/sec; 0.061 sec/batch; 101h:27m:35s remains)
INFO - root - 2019-11-04 00:19:13.600680: step 60170, total loss = 0.45, predict loss = 0.10 (55.4 examples/sec; 0.072 sec/batch; 119h:13m:11s remains)
INFO - root - 2019-11-04 00:19:14.300601: step 60180, total loss = 0.46, predict loss = 0.10 (63.1 examples/sec; 0.063 sec/batch; 104h:36m:50s remains)
INFO - root - 2019-11-04 00:19:14.959151: step 60190, total loss = 0.52, predict loss = 0.12 (69.5 examples/sec; 0.058 sec/batch; 94h:59m:37s remains)
INFO - root - 2019-11-04 00:19:15.685715: step 60200, total loss = 0.42, predict loss = 0.10 (64.7 examples/sec; 0.062 sec/batch; 102h:01m:54s remains)
INFO - root - 2019-11-04 00:19:16.381615: step 60210, total loss = 0.60, predict loss = 0.13 (56.2 examples/sec; 0.071 sec/batch; 117h:24m:18s remains)
INFO - root - 2019-11-04 00:19:17.065515: step 60220, total loss = 0.39, predict loss = 0.09 (71.2 examples/sec; 0.056 sec/batch; 92h:41m:45s remains)
INFO - root - 2019-11-04 00:19:17.753853: step 60230, total loss = 0.48, predict loss = 0.12 (58.9 examples/sec; 0.068 sec/batch; 112h:00m:30s remains)
INFO - root - 2019-11-04 00:19:18.429513: step 60240, total loss = 0.57, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 89h:16m:12s remains)
INFO - root - 2019-11-04 00:19:19.092055: step 60250, total loss = 0.55, predict loss = 0.13 (60.4 examples/sec; 0.066 sec/batch; 109h:12m:20s remains)
INFO - root - 2019-11-04 00:19:19.806306: step 60260, total loss = 0.54, predict loss = 0.13 (60.9 examples/sec; 0.066 sec/batch; 108h:22m:30s remains)
INFO - root - 2019-11-04 00:19:20.499654: step 60270, total loss = 0.51, predict loss = 0.12 (62.3 examples/sec; 0.064 sec/batch; 105h:56m:12s remains)
INFO - root - 2019-11-04 00:19:21.194459: step 60280, total loss = 0.56, predict loss = 0.13 (68.2 examples/sec; 0.059 sec/batch; 96h:47m:19s remains)
INFO - root - 2019-11-04 00:19:21.902249: step 60290, total loss = 0.57, predict loss = 0.12 (61.8 examples/sec; 0.065 sec/batch; 106h:43m:15s remains)
INFO - root - 2019-11-04 00:19:22.643707: step 60300, total loss = 0.50, predict loss = 0.11 (63.8 examples/sec; 0.063 sec/batch; 103h:30m:31s remains)
INFO - root - 2019-11-04 00:19:23.350965: step 60310, total loss = 0.79, predict loss = 0.18 (66.0 examples/sec; 0.061 sec/batch; 100h:00m:17s remains)
INFO - root - 2019-11-04 00:19:24.048576: step 60320, total loss = 0.69, predict loss = 0.15 (78.2 examples/sec; 0.051 sec/batch; 84h:22m:55s remains)
INFO - root - 2019-11-04 00:19:24.715366: step 60330, total loss = 0.73, predict loss = 0.17 (70.4 examples/sec; 0.057 sec/batch; 93h:43m:35s remains)
INFO - root - 2019-11-04 00:19:25.467253: step 60340, total loss = 0.54, predict loss = 0.13 (45.1 examples/sec; 0.089 sec/batch; 146h:18m:50s remains)
INFO - root - 2019-11-04 00:19:26.208968: step 60350, total loss = 0.67, predict loss = 0.15 (68.9 examples/sec; 0.058 sec/batch; 95h:47m:54s remains)
INFO - root - 2019-11-04 00:19:26.933194: step 60360, total loss = 0.54, predict loss = 0.11 (64.7 examples/sec; 0.062 sec/batch; 102h:04m:07s remains)
INFO - root - 2019-11-04 00:19:27.666549: step 60370, total loss = 0.51, predict loss = 0.11 (64.8 examples/sec; 0.062 sec/batch; 101h:53m:17s remains)
INFO - root - 2019-11-04 00:19:28.335320: step 60380, total loss = 0.67, predict loss = 0.16 (66.2 examples/sec; 0.060 sec/batch; 99h:45m:38s remains)
INFO - root - 2019-11-04 00:19:29.053611: step 60390, total loss = 0.49, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 94h:31m:00s remains)
INFO - root - 2019-11-04 00:19:29.788399: step 60400, total loss = 0.62, predict loss = 0.13 (57.5 examples/sec; 0.070 sec/batch; 114h:50m:40s remains)
INFO - root - 2019-11-04 00:19:30.514011: step 60410, total loss = 0.55, predict loss = 0.12 (62.3 examples/sec; 0.064 sec/batch; 105h:59m:24s remains)
INFO - root - 2019-11-04 00:19:31.188828: step 60420, total loss = 0.57, predict loss = 0.13 (64.3 examples/sec; 0.062 sec/batch; 102h:34m:22s remains)
INFO - root - 2019-11-04 00:19:31.901400: step 60430, total loss = 0.60, predict loss = 0.14 (77.1 examples/sec; 0.052 sec/batch; 85h:38m:25s remains)
INFO - root - 2019-11-04 00:19:32.600857: step 60440, total loss = 0.57, predict loss = 0.12 (65.8 examples/sec; 0.061 sec/batch; 100h:20m:44s remains)
INFO - root - 2019-11-04 00:19:33.355875: step 60450, total loss = 0.48, predict loss = 0.11 (52.4 examples/sec; 0.076 sec/batch; 125h:59m:05s remains)
INFO - root - 2019-11-04 00:19:34.055827: step 60460, total loss = 0.52, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 96h:51m:09s remains)
INFO - root - 2019-11-04 00:19:34.739104: step 60470, total loss = 0.45, predict loss = 0.11 (66.6 examples/sec; 0.060 sec/batch; 99h:04m:48s remains)
INFO - root - 2019-11-04 00:19:35.452209: step 60480, total loss = 0.50, predict loss = 0.11 (55.6 examples/sec; 0.072 sec/batch; 118h:43m:56s remains)
INFO - root - 2019-11-04 00:19:36.163965: step 60490, total loss = 0.82, predict loss = 0.21 (67.1 examples/sec; 0.060 sec/batch; 98h:22m:42s remains)
INFO - root - 2019-11-04 00:19:36.898251: step 60500, total loss = 0.70, predict loss = 0.17 (77.3 examples/sec; 0.052 sec/batch; 85h:20m:27s remains)
INFO - root - 2019-11-04 00:19:37.657957: step 60510, total loss = 0.57, predict loss = 0.14 (67.6 examples/sec; 0.059 sec/batch; 97h:41m:27s remains)
INFO - root - 2019-11-04 00:19:38.443507: step 60520, total loss = 0.53, predict loss = 0.12 (69.3 examples/sec; 0.058 sec/batch; 95h:15m:05s remains)
INFO - root - 2019-11-04 00:19:39.165450: step 60530, total loss = 0.50, predict loss = 0.12 (65.1 examples/sec; 0.061 sec/batch; 101h:21m:19s remains)
INFO - root - 2019-11-04 00:19:39.920444: step 60540, total loss = 0.65, predict loss = 0.15 (56.1 examples/sec; 0.071 sec/batch; 117h:38m:43s remains)
INFO - root - 2019-11-04 00:19:40.672455: step 60550, total loss = 0.63, predict loss = 0.16 (64.4 examples/sec; 0.062 sec/batch; 102h:32m:30s remains)
INFO - root - 2019-11-04 00:19:41.436221: step 60560, total loss = 0.61, predict loss = 0.15 (57.1 examples/sec; 0.070 sec/batch; 115h:40m:08s remains)
INFO - root - 2019-11-04 00:19:42.239160: step 60570, total loss = 0.43, predict loss = 0.10 (63.5 examples/sec; 0.063 sec/batch; 103h:58m:58s remains)
INFO - root - 2019-11-04 00:19:42.939312: step 60580, total loss = 0.31, predict loss = 0.07 (65.3 examples/sec; 0.061 sec/batch; 101h:05m:49s remains)
INFO - root - 2019-11-04 00:19:43.638579: step 60590, total loss = 0.47, predict loss = 0.12 (68.1 examples/sec; 0.059 sec/batch; 96h:52m:56s remains)
INFO - root - 2019-11-04 00:19:44.364300: step 60600, total loss = 0.40, predict loss = 0.10 (59.6 examples/sec; 0.067 sec/batch; 110h:43m:21s remains)
INFO - root - 2019-11-04 00:19:45.077029: step 60610, total loss = 0.31, predict loss = 0.07 (67.0 examples/sec; 0.060 sec/batch; 98h:29m:26s remains)
INFO - root - 2019-11-04 00:19:45.766356: step 60620, total loss = 0.45, predict loss = 0.10 (61.4 examples/sec; 0.065 sec/batch; 107h:31m:05s remains)
INFO - root - 2019-11-04 00:19:46.487441: step 60630, total loss = 0.30, predict loss = 0.06 (63.9 examples/sec; 0.063 sec/batch; 103h:19m:28s remains)
INFO - root - 2019-11-04 00:19:47.221571: step 60640, total loss = 0.39, predict loss = 0.08 (56.5 examples/sec; 0.071 sec/batch; 116h:53m:47s remains)
INFO - root - 2019-11-04 00:19:47.930607: step 60650, total loss = 0.77, predict loss = 0.19 (64.9 examples/sec; 0.062 sec/batch; 101h:41m:02s remains)
INFO - root - 2019-11-04 00:19:48.611753: step 60660, total loss = 0.55, predict loss = 0.13 (62.8 examples/sec; 0.064 sec/batch; 105h:08m:07s remains)
INFO - root - 2019-11-04 00:19:49.256673: step 60670, total loss = 0.62, predict loss = 0.16 (69.0 examples/sec; 0.058 sec/batch; 95h:40m:57s remains)
INFO - root - 2019-11-04 00:19:49.938315: step 60680, total loss = 0.57, predict loss = 0.14 (66.7 examples/sec; 0.060 sec/batch; 98h:59m:38s remains)
INFO - root - 2019-11-04 00:19:50.633165: step 60690, total loss = 0.83, predict loss = 0.21 (64.0 examples/sec; 0.063 sec/batch; 103h:08m:47s remains)
INFO - root - 2019-11-04 00:19:51.344883: step 60700, total loss = 0.44, predict loss = 0.10 (64.7 examples/sec; 0.062 sec/batch; 101h:56m:29s remains)
INFO - root - 2019-11-04 00:19:52.071648: step 60710, total loss = 0.61, predict loss = 0.14 (49.7 examples/sec; 0.080 sec/batch; 132h:47m:49s remains)
INFO - root - 2019-11-04 00:19:52.762750: step 60720, total loss = 0.63, predict loss = 0.15 (60.5 examples/sec; 0.066 sec/batch; 109h:02m:03s remains)
INFO - root - 2019-11-04 00:19:53.478301: step 60730, total loss = 0.78, predict loss = 0.19 (72.0 examples/sec; 0.056 sec/batch; 91h:41m:55s remains)
INFO - root - 2019-11-04 00:19:54.203382: step 60740, total loss = 0.74, predict loss = 0.18 (72.1 examples/sec; 0.055 sec/batch; 91h:31m:37s remains)
INFO - root - 2019-11-04 00:19:54.936047: step 60750, total loss = 0.44, predict loss = 0.10 (65.1 examples/sec; 0.061 sec/batch; 101h:25m:07s remains)
INFO - root - 2019-11-04 00:19:55.592546: step 60760, total loss = 0.63, predict loss = 0.15 (68.2 examples/sec; 0.059 sec/batch; 96h:42m:36s remains)
INFO - root - 2019-11-04 00:19:56.321017: step 60770, total loss = 0.45, predict loss = 0.11 (64.8 examples/sec; 0.062 sec/batch; 101h:53m:32s remains)
INFO - root - 2019-11-04 00:19:56.965863: step 60780, total loss = 0.43, predict loss = 0.09 (76.8 examples/sec; 0.052 sec/batch; 85h:54m:50s remains)
INFO - root - 2019-11-04 00:19:57.668860: step 60790, total loss = 0.70, predict loss = 0.18 (76.4 examples/sec; 0.052 sec/batch; 86h:21m:32s remains)
INFO - root - 2019-11-04 00:19:58.370015: step 60800, total loss = 0.48, predict loss = 0.12 (55.9 examples/sec; 0.072 sec/batch; 118h:09m:25s remains)
INFO - root - 2019-11-04 00:19:59.082546: step 60810, total loss = 0.60, predict loss = 0.14 (71.4 examples/sec; 0.056 sec/batch; 92h:25m:37s remains)
INFO - root - 2019-11-04 00:19:59.769906: step 60820, total loss = 0.49, predict loss = 0.12 (61.9 examples/sec; 0.065 sec/batch; 106h:31m:39s remains)
INFO - root - 2019-11-04 00:20:00.476984: step 60830, total loss = 0.30, predict loss = 0.07 (56.0 examples/sec; 0.071 sec/batch; 117h:54m:45s remains)
INFO - root - 2019-11-04 00:20:01.174974: step 60840, total loss = 0.47, predict loss = 0.11 (53.8 examples/sec; 0.074 sec/batch; 122h:43m:26s remains)
INFO - root - 2019-11-04 00:20:01.836189: step 60850, total loss = 0.34, predict loss = 0.08 (67.8 examples/sec; 0.059 sec/batch; 97h:23m:59s remains)
INFO - root - 2019-11-04 00:20:02.549189: step 60860, total loss = 0.29, predict loss = 0.06 (64.3 examples/sec; 0.062 sec/batch; 102h:33m:26s remains)
INFO - root - 2019-11-04 00:20:03.275655: step 60870, total loss = 0.39, predict loss = 0.09 (58.3 examples/sec; 0.069 sec/batch; 113h:12m:28s remains)
INFO - root - 2019-11-04 00:20:03.974419: step 60880, total loss = 0.33, predict loss = 0.07 (50.0 examples/sec; 0.080 sec/batch; 132h:04m:10s remains)
INFO - root - 2019-11-04 00:20:04.648475: step 60890, total loss = 0.42, predict loss = 0.09 (59.5 examples/sec; 0.067 sec/batch; 110h:59m:13s remains)
INFO - root - 2019-11-04 00:20:05.346369: step 60900, total loss = 0.44, predict loss = 0.10 (64.2 examples/sec; 0.062 sec/batch; 102h:42m:47s remains)
INFO - root - 2019-11-04 00:20:06.076909: step 60910, total loss = 0.41, predict loss = 0.08 (65.0 examples/sec; 0.062 sec/batch; 101h:28m:22s remains)
INFO - root - 2019-11-04 00:20:06.765896: step 60920, total loss = 0.47, predict loss = 0.10 (73.7 examples/sec; 0.054 sec/batch; 89h:33m:20s remains)
INFO - root - 2019-11-04 00:20:07.483434: step 60930, total loss = 0.53, predict loss = 0.12 (73.5 examples/sec; 0.054 sec/batch; 89h:46m:11s remains)
INFO - root - 2019-11-04 00:20:08.153686: step 60940, total loss = 0.57, predict loss = 0.14 (73.8 examples/sec; 0.054 sec/batch; 89h:23m:48s remains)
INFO - root - 2019-11-04 00:20:08.856602: step 60950, total loss = 0.37, predict loss = 0.09 (68.9 examples/sec; 0.058 sec/batch; 95h:50m:34s remains)
INFO - root - 2019-11-04 00:20:09.498904: step 60960, total loss = 0.45, predict loss = 0.10 (74.7 examples/sec; 0.054 sec/batch; 88h:17m:13s remains)
INFO - root - 2019-11-04 00:20:10.132342: step 60970, total loss = 0.51, predict loss = 0.12 (83.4 examples/sec; 0.048 sec/batch; 79h:05m:48s remains)
INFO - root - 2019-11-04 00:20:10.785620: step 60980, total loss = 0.39, predict loss = 0.09 (76.4 examples/sec; 0.052 sec/batch; 86h:21m:38s remains)
INFO - root - 2019-11-04 00:20:11.458042: step 60990, total loss = 0.54, predict loss = 0.13 (56.1 examples/sec; 0.071 sec/batch; 117h:40m:56s remains)
INFO - root - 2019-11-04 00:20:12.123455: step 61000, total loss = 0.66, predict loss = 0.15 (73.0 examples/sec; 0.055 sec/batch; 90h:21m:33s remains)
INFO - root - 2019-11-04 00:20:12.785675: step 61010, total loss = 0.77, predict loss = 0.18 (74.4 examples/sec; 0.054 sec/batch; 88h:41m:45s remains)
INFO - root - 2019-11-04 00:20:13.433562: step 61020, total loss = 0.55, predict loss = 0.12 (72.3 examples/sec; 0.055 sec/batch; 91h:16m:42s remains)
INFO - root - 2019-11-04 00:20:14.048738: step 61030, total loss = 0.46, predict loss = 0.10 (70.8 examples/sec; 0.056 sec/batch; 93h:08m:33s remains)
INFO - root - 2019-11-04 00:20:14.680694: step 61040, total loss = 0.49, predict loss = 0.11 (75.1 examples/sec; 0.053 sec/batch; 87h:49m:57s remains)
INFO - root - 2019-11-04 00:20:15.309550: step 61050, total loss = 0.51, predict loss = 0.12 (54.1 examples/sec; 0.074 sec/batch; 122h:04m:06s remains)
INFO - root - 2019-11-04 00:20:15.957009: step 61060, total loss = 0.45, predict loss = 0.10 (66.3 examples/sec; 0.060 sec/batch; 99h:31m:37s remains)
INFO - root - 2019-11-04 00:20:16.590673: step 61070, total loss = 0.54, predict loss = 0.12 (60.7 examples/sec; 0.066 sec/batch; 108h:47m:42s remains)
INFO - root - 2019-11-04 00:20:17.251170: step 61080, total loss = 0.38, predict loss = 0.09 (70.0 examples/sec; 0.057 sec/batch; 94h:19m:25s remains)
INFO - root - 2019-11-04 00:20:17.902563: step 61090, total loss = 0.36, predict loss = 0.07 (77.2 examples/sec; 0.052 sec/batch; 85h:26m:04s remains)
INFO - root - 2019-11-04 00:20:18.503222: step 61100, total loss = 0.28, predict loss = 0.06 (74.1 examples/sec; 0.054 sec/batch; 89h:00m:46s remains)
INFO - root - 2019-11-04 00:20:19.150753: step 61110, total loss = 0.56, predict loss = 0.13 (64.6 examples/sec; 0.062 sec/batch; 102h:08m:28s remains)
INFO - root - 2019-11-04 00:20:19.770476: step 61120, total loss = 0.30, predict loss = 0.05 (74.1 examples/sec; 0.054 sec/batch; 89h:01m:19s remains)
INFO - root - 2019-11-04 00:20:20.395846: step 61130, total loss = 0.39, predict loss = 0.08 (67.0 examples/sec; 0.060 sec/batch; 98h:28m:01s remains)
INFO - root - 2019-11-04 00:20:21.102072: step 61140, total loss = 0.51, predict loss = 0.12 (61.6 examples/sec; 0.065 sec/batch; 107h:02m:37s remains)
INFO - root - 2019-11-04 00:20:21.799612: step 61150, total loss = 0.36, predict loss = 0.08 (62.5 examples/sec; 0.064 sec/batch; 105h:35m:40s remains)
INFO - root - 2019-11-04 00:20:22.498529: step 61160, total loss = 0.50, predict loss = 0.12 (65.3 examples/sec; 0.061 sec/batch; 101h:07m:14s remains)
INFO - root - 2019-11-04 00:20:23.180645: step 61170, total loss = 0.45, predict loss = 0.11 (76.6 examples/sec; 0.052 sec/batch; 86h:11m:37s remains)
INFO - root - 2019-11-04 00:20:23.772175: step 61180, total loss = 0.53, predict loss = 0.13 (74.3 examples/sec; 0.054 sec/batch; 88h:52m:15s remains)
INFO - root - 2019-11-04 00:20:24.413633: step 61190, total loss = 0.53, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 93h:34m:00s remains)
INFO - root - 2019-11-04 00:20:25.067568: step 61200, total loss = 0.43, predict loss = 0.10 (76.4 examples/sec; 0.052 sec/batch; 86h:22m:16s remains)
INFO - root - 2019-11-04 00:20:25.714966: step 61210, total loss = 0.37, predict loss = 0.08 (77.8 examples/sec; 0.051 sec/batch; 84h:46m:41s remains)
INFO - root - 2019-11-04 00:20:26.327958: step 61220, total loss = 0.33, predict loss = 0.08 (64.5 examples/sec; 0.062 sec/batch; 102h:14m:37s remains)
INFO - root - 2019-11-04 00:20:26.950208: step 61230, total loss = 0.42, predict loss = 0.10 (56.0 examples/sec; 0.071 sec/batch; 117h:51m:38s remains)
INFO - root - 2019-11-04 00:20:27.654428: step 61240, total loss = 0.44, predict loss = 0.10 (63.8 examples/sec; 0.063 sec/batch; 103h:23m:33s remains)
INFO - root - 2019-11-04 00:20:28.363232: step 61250, total loss = 0.52, predict loss = 0.12 (68.4 examples/sec; 0.058 sec/batch; 96h:24m:30s remains)
INFO - root - 2019-11-04 00:20:28.950073: step 61260, total loss = 0.31, predict loss = 0.07 (68.6 examples/sec; 0.058 sec/batch; 96h:11m:08s remains)
INFO - root - 2019-11-04 00:20:29.600416: step 61270, total loss = 0.58, predict loss = 0.13 (80.2 examples/sec; 0.050 sec/batch; 82h:17m:40s remains)
INFO - root - 2019-11-04 00:20:30.223354: step 61280, total loss = 0.68, predict loss = 0.16 (78.7 examples/sec; 0.051 sec/batch; 83h:50m:27s remains)
INFO - root - 2019-11-04 00:20:30.834085: step 61290, total loss = 0.71, predict loss = 0.16 (80.0 examples/sec; 0.050 sec/batch; 82h:31m:52s remains)
INFO - root - 2019-11-04 00:20:31.458901: step 61300, total loss = 0.34, predict loss = 0.07 (65.1 examples/sec; 0.061 sec/batch; 101h:22m:15s remains)
INFO - root - 2019-11-04 00:20:32.101286: step 61310, total loss = 0.50, predict loss = 0.11 (72.4 examples/sec; 0.055 sec/batch; 91h:06m:58s remains)
INFO - root - 2019-11-04 00:20:32.767223: step 61320, total loss = 0.57, predict loss = 0.12 (64.7 examples/sec; 0.062 sec/batch; 102h:03m:18s remains)
INFO - root - 2019-11-04 00:20:33.512157: step 61330, total loss = 0.66, predict loss = 0.15 (63.4 examples/sec; 0.063 sec/batch; 104h:00m:41s remains)
INFO - root - 2019-11-04 00:20:34.206714: step 61340, total loss = 0.35, predict loss = 0.07 (62.3 examples/sec; 0.064 sec/batch; 105h:59m:08s remains)
INFO - root - 2019-11-04 00:20:34.848698: step 61350, total loss = 0.33, predict loss = 0.07 (71.6 examples/sec; 0.056 sec/batch; 92h:12m:38s remains)
INFO - root - 2019-11-04 00:20:35.492452: step 61360, total loss = 0.42, predict loss = 0.09 (69.6 examples/sec; 0.057 sec/batch; 94h:46m:25s remains)
INFO - root - 2019-11-04 00:20:36.116714: step 61370, total loss = 0.58, predict loss = 0.13 (75.8 examples/sec; 0.053 sec/batch; 87h:03m:13s remains)
INFO - root - 2019-11-04 00:20:36.789532: step 61380, total loss = 0.30, predict loss = 0.07 (60.3 examples/sec; 0.066 sec/batch; 109h:28m:35s remains)
INFO - root - 2019-11-04 00:20:37.525659: step 61390, total loss = 0.55, predict loss = 0.13 (67.2 examples/sec; 0.060 sec/batch; 98h:09m:49s remains)
INFO - root - 2019-11-04 00:20:38.176469: step 61400, total loss = 0.40, predict loss = 0.09 (71.7 examples/sec; 0.056 sec/batch; 92h:01m:00s remains)
INFO - root - 2019-11-04 00:20:38.775616: step 61410, total loss = 0.47, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 97h:31m:23s remains)
INFO - root - 2019-11-04 00:20:39.413962: step 61420, total loss = 0.79, predict loss = 0.19 (71.5 examples/sec; 0.056 sec/batch; 92h:20m:34s remains)
INFO - root - 2019-11-04 00:20:39.997608: step 61430, total loss = 0.85, predict loss = 0.20 (76.8 examples/sec; 0.052 sec/batch; 85h:51m:44s remains)
INFO - root - 2019-11-04 00:20:40.600346: step 61440, total loss = 0.95, predict loss = 0.22 (80.1 examples/sec; 0.050 sec/batch; 82h:22m:07s remains)
INFO - root - 2019-11-04 00:20:41.242695: step 61450, total loss = 0.79, predict loss = 0.19 (77.2 examples/sec; 0.052 sec/batch; 85h:30m:16s remains)
INFO - root - 2019-11-04 00:20:41.935007: step 61460, total loss = 0.70, predict loss = 0.16 (77.6 examples/sec; 0.052 sec/batch; 84h:58m:52s remains)
INFO - root - 2019-11-04 00:20:42.551176: step 61470, total loss = 0.90, predict loss = 0.20 (61.5 examples/sec; 0.065 sec/batch; 107h:12m:25s remains)
INFO - root - 2019-11-04 00:20:43.174062: step 61480, total loss = 0.79, predict loss = 0.18 (74.5 examples/sec; 0.054 sec/batch; 88h:35m:15s remains)
INFO - root - 2019-11-04 00:20:43.798403: step 61490, total loss = 0.75, predict loss = 0.18 (75.0 examples/sec; 0.053 sec/batch; 88h:02m:09s remains)
INFO - root - 2019-11-04 00:20:44.393315: step 61500, total loss = 0.65, predict loss = 0.15 (80.7 examples/sec; 0.050 sec/batch; 81h:46m:55s remains)
INFO - root - 2019-11-04 00:20:45.015616: step 61510, total loss = 0.64, predict loss = 0.16 (67.8 examples/sec; 0.059 sec/batch; 97h:15m:14s remains)
INFO - root - 2019-11-04 00:20:45.649400: step 61520, total loss = 0.61, predict loss = 0.14 (68.0 examples/sec; 0.059 sec/batch; 96h:59m:55s remains)
INFO - root - 2019-11-04 00:20:46.298932: step 61530, total loss = 0.52, predict loss = 0.12 (61.9 examples/sec; 0.065 sec/batch; 106h:36m:20s remains)
INFO - root - 2019-11-04 00:20:46.950878: step 61540, total loss = 0.55, predict loss = 0.12 (79.3 examples/sec; 0.050 sec/batch; 83h:10m:06s remains)
INFO - root - 2019-11-04 00:20:47.636881: step 61550, total loss = 0.53, predict loss = 0.13 (53.9 examples/sec; 0.074 sec/batch; 122h:20m:10s remains)
INFO - root - 2019-11-04 00:20:48.367270: step 61560, total loss = 0.45, predict loss = 0.10 (59.7 examples/sec; 0.067 sec/batch; 110h:26m:51s remains)
INFO - root - 2019-11-04 00:20:48.995419: step 61570, total loss = 0.38, predict loss = 0.09 (71.0 examples/sec; 0.056 sec/batch; 92h:58m:48s remains)
INFO - root - 2019-11-04 00:20:49.588022: step 61580, total loss = 0.47, predict loss = 0.11 (79.7 examples/sec; 0.050 sec/batch; 82h:50m:21s remains)
INFO - root - 2019-11-04 00:20:50.176175: step 61590, total loss = 0.59, predict loss = 0.13 (78.1 examples/sec; 0.051 sec/batch; 84h:29m:15s remains)
INFO - root - 2019-11-04 00:20:50.864573: step 61600, total loss = 0.52, predict loss = 0.12 (66.3 examples/sec; 0.060 sec/batch; 99h:31m:31s remains)
INFO - root - 2019-11-04 00:20:51.543175: step 61610, total loss = 0.55, predict loss = 0.13 (64.6 examples/sec; 0.062 sec/batch; 102h:08m:40s remains)
INFO - root - 2019-11-04 00:20:52.157923: step 61620, total loss = 0.54, predict loss = 0.13 (81.1 examples/sec; 0.049 sec/batch; 81h:24m:12s remains)
INFO - root - 2019-11-04 00:20:52.802392: step 61630, total loss = 0.54, predict loss = 0.11 (73.4 examples/sec; 0.054 sec/batch; 89h:53m:51s remains)
INFO - root - 2019-11-04 00:20:53.398583: step 61640, total loss = 0.59, predict loss = 0.14 (78.6 examples/sec; 0.051 sec/batch; 83h:58m:31s remains)
INFO - root - 2019-11-04 00:20:54.008069: step 61650, total loss = 0.39, predict loss = 0.09 (69.4 examples/sec; 0.058 sec/batch; 95h:05m:25s remains)
INFO - root - 2019-11-04 00:20:54.616687: step 61660, total loss = 0.56, predict loss = 0.12 (75.3 examples/sec; 0.053 sec/batch; 87h:39m:27s remains)
INFO - root - 2019-11-04 00:20:55.232911: step 61670, total loss = 0.49, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 94h:10m:56s remains)
INFO - root - 2019-11-04 00:20:55.847136: step 61680, total loss = 0.38, predict loss = 0.08 (67.4 examples/sec; 0.059 sec/batch; 97h:53m:46s remains)
INFO - root - 2019-11-04 00:20:56.487310: step 61690, total loss = 0.48, predict loss = 0.11 (64.1 examples/sec; 0.062 sec/batch; 103h:00m:28s remains)
INFO - root - 2019-11-04 00:20:57.158730: step 61700, total loss = 0.36, predict loss = 0.08 (68.5 examples/sec; 0.058 sec/batch; 96h:23m:11s remains)
INFO - root - 2019-11-04 00:20:57.821301: step 61710, total loss = 0.42, predict loss = 0.09 (77.9 examples/sec; 0.051 sec/batch; 84h:44m:46s remains)
INFO - root - 2019-11-04 00:20:58.428215: step 61720, total loss = 0.40, predict loss = 0.08 (68.7 examples/sec; 0.058 sec/batch; 96h:03m:21s remains)
INFO - root - 2019-11-04 00:20:59.041641: step 61730, total loss = 0.46, predict loss = 0.11 (56.4 examples/sec; 0.071 sec/batch; 117h:01m:02s remains)
INFO - root - 2019-11-04 00:20:59.658864: step 61740, total loss = 0.45, predict loss = 0.10 (77.3 examples/sec; 0.052 sec/batch; 85h:24m:30s remains)
INFO - root - 2019-11-04 00:21:00.280502: step 61750, total loss = 0.48, predict loss = 0.11 (78.1 examples/sec; 0.051 sec/batch; 84h:28m:18s remains)
INFO - root - 2019-11-04 00:21:00.900390: step 61760, total loss = 0.37, predict loss = 0.07 (66.8 examples/sec; 0.060 sec/batch; 98h:44m:13s remains)
INFO - root - 2019-11-04 00:21:01.567209: step 61770, total loss = 0.49, predict loss = 0.11 (74.4 examples/sec; 0.054 sec/batch; 88h:43m:56s remains)
INFO - root - 2019-11-04 00:21:02.220240: step 61780, total loss = 0.55, predict loss = 0.13 (70.4 examples/sec; 0.057 sec/batch; 93h:39m:45s remains)
INFO - root - 2019-11-04 00:21:02.887697: step 61790, total loss = 0.41, predict loss = 0.09 (78.6 examples/sec; 0.051 sec/batch; 83h:55m:06s remains)
INFO - root - 2019-11-04 00:21:04.136787: step 61800, total loss = 0.76, predict loss = 0.18 (70.8 examples/sec; 0.056 sec/batch; 93h:08m:08s remains)
INFO - root - 2019-11-04 00:21:04.746959: step 61810, total loss = 0.70, predict loss = 0.17 (68.0 examples/sec; 0.059 sec/batch; 97h:01m:30s remains)
INFO - root - 2019-11-04 00:21:05.395496: step 61820, total loss = 0.60, predict loss = 0.14 (72.7 examples/sec; 0.055 sec/batch; 90h:45m:07s remains)
INFO - root - 2019-11-04 00:21:05.989368: step 61830, total loss = 0.52, predict loss = 0.12 (72.1 examples/sec; 0.055 sec/batch; 91h:31m:44s remains)
INFO - root - 2019-11-04 00:21:06.615637: step 61840, total loss = 0.42, predict loss = 0.10 (79.7 examples/sec; 0.050 sec/batch; 82h:44m:17s remains)
INFO - root - 2019-11-04 00:21:07.272048: step 61850, total loss = 0.62, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 96h:00m:12s remains)
INFO - root - 2019-11-04 00:21:07.923483: step 61860, total loss = 0.56, predict loss = 0.13 (72.9 examples/sec; 0.055 sec/batch; 90h:31m:23s remains)
INFO - root - 2019-11-04 00:21:08.558002: step 61870, total loss = 0.87, predict loss = 0.21 (71.5 examples/sec; 0.056 sec/batch; 92h:16m:11s remains)
INFO - root - 2019-11-04 00:21:09.142777: step 61880, total loss = 0.58, predict loss = 0.14 (78.6 examples/sec; 0.051 sec/batch; 83h:59m:46s remains)
INFO - root - 2019-11-04 00:21:09.741665: step 61890, total loss = 0.65, predict loss = 0.15 (73.0 examples/sec; 0.055 sec/batch; 90h:19m:37s remains)
INFO - root - 2019-11-04 00:21:10.409347: step 61900, total loss = 0.61, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 95h:08m:48s remains)
INFO - root - 2019-11-04 00:21:11.063435: step 61910, total loss = 0.72, predict loss = 0.18 (64.8 examples/sec; 0.062 sec/batch; 101h:46m:22s remains)
INFO - root - 2019-11-04 00:21:11.775237: step 61920, total loss = 0.79, predict loss = 0.19 (68.4 examples/sec; 0.058 sec/batch; 96h:24m:30s remains)
INFO - root - 2019-11-04 00:21:12.400220: step 61930, total loss = 0.51, predict loss = 0.12 (82.9 examples/sec; 0.048 sec/batch; 79h:36m:54s remains)
INFO - root - 2019-11-04 00:21:13.017627: step 61940, total loss = 0.61, predict loss = 0.14 (60.8 examples/sec; 0.066 sec/batch; 108h:35m:03s remains)
INFO - root - 2019-11-04 00:21:13.697689: step 61950, total loss = 0.55, predict loss = 0.12 (65.1 examples/sec; 0.061 sec/batch; 101h:17m:18s remains)
INFO - root - 2019-11-04 00:21:14.351192: step 61960, total loss = 0.52, predict loss = 0.13 (73.9 examples/sec; 0.054 sec/batch; 89h:15m:23s remains)
INFO - root - 2019-11-04 00:21:15.017430: step 61970, total loss = 0.52, predict loss = 0.12 (63.6 examples/sec; 0.063 sec/batch; 103h:41m:01s remains)
INFO - root - 2019-11-04 00:21:15.689079: step 61980, total loss = 0.45, predict loss = 0.10 (62.5 examples/sec; 0.064 sec/batch; 105h:33m:27s remains)
INFO - root - 2019-11-04 00:21:16.354499: step 61990, total loss = 0.41, predict loss = 0.10 (60.0 examples/sec; 0.067 sec/batch; 109h:56m:55s remains)
INFO - root - 2019-11-04 00:21:17.008580: step 62000, total loss = 0.49, predict loss = 0.10 (77.1 examples/sec; 0.052 sec/batch; 85h:31m:55s remains)
INFO - root - 2019-11-04 00:21:17.640955: step 62010, total loss = 0.41, predict loss = 0.09 (69.9 examples/sec; 0.057 sec/batch; 94h:26m:35s remains)
INFO - root - 2019-11-04 00:21:18.255300: step 62020, total loss = 0.62, predict loss = 0.15 (80.4 examples/sec; 0.050 sec/batch; 82h:01m:53s remains)
INFO - root - 2019-11-04 00:21:18.908435: step 62030, total loss = 0.54, predict loss = 0.12 (64.2 examples/sec; 0.062 sec/batch; 102h:43m:01s remains)
INFO - root - 2019-11-04 00:21:19.583321: step 62040, total loss = 0.51, predict loss = 0.12 (65.1 examples/sec; 0.061 sec/batch; 101h:21m:13s remains)
INFO - root - 2019-11-04 00:21:20.222655: step 62050, total loss = 0.64, predict loss = 0.16 (62.7 examples/sec; 0.064 sec/batch; 105h:17m:59s remains)
INFO - root - 2019-11-04 00:21:20.856272: step 62060, total loss = 0.56, predict loss = 0.13 (65.5 examples/sec; 0.061 sec/batch; 100h:44m:57s remains)
INFO - root - 2019-11-04 00:21:21.483306: step 62070, total loss = 0.80, predict loss = 0.19 (73.5 examples/sec; 0.054 sec/batch; 89h:46m:10s remains)
INFO - root - 2019-11-04 00:21:22.137397: step 62080, total loss = 0.60, predict loss = 0.15 (65.8 examples/sec; 0.061 sec/batch; 100h:20m:35s remains)
INFO - root - 2019-11-04 00:21:22.842568: step 62090, total loss = 0.73, predict loss = 0.17 (63.6 examples/sec; 0.063 sec/batch; 103h:39m:37s remains)
INFO - root - 2019-11-04 00:21:23.580700: step 62100, total loss = 0.70, predict loss = 0.17 (65.3 examples/sec; 0.061 sec/batch; 101h:03m:39s remains)
INFO - root - 2019-11-04 00:21:24.220111: step 62110, total loss = 0.49, predict loss = 0.11 (64.8 examples/sec; 0.062 sec/batch; 101h:44m:49s remains)
INFO - root - 2019-11-04 00:21:24.904128: step 62120, total loss = 0.73, predict loss = 0.18 (65.6 examples/sec; 0.061 sec/batch; 100h:38m:54s remains)
INFO - root - 2019-11-04 00:21:25.612309: step 62130, total loss = 0.57, predict loss = 0.13 (64.9 examples/sec; 0.062 sec/batch; 101h:37m:17s remains)
INFO - root - 2019-11-04 00:21:26.307632: step 62140, total loss = 0.65, predict loss = 0.15 (59.9 examples/sec; 0.067 sec/batch; 110h:05m:39s remains)
INFO - root - 2019-11-04 00:21:26.922580: step 62150, total loss = 0.59, predict loss = 0.14 (76.6 examples/sec; 0.052 sec/batch; 86h:05m:32s remains)
INFO - root - 2019-11-04 00:21:27.643973: step 62160, total loss = 0.56, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 101h:02m:31s remains)
INFO - root - 2019-11-04 00:21:28.377028: step 62170, total loss = 0.55, predict loss = 0.13 (61.0 examples/sec; 0.066 sec/batch; 108h:12m:40s remains)
INFO - root - 2019-11-04 00:21:29.016560: step 62180, total loss = 0.58, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 93h:53m:33s remains)
INFO - root - 2019-11-04 00:21:29.640747: step 62190, total loss = 0.75, predict loss = 0.17 (68.1 examples/sec; 0.059 sec/batch; 96h:51m:08s remains)
INFO - root - 2019-11-04 00:21:30.313325: step 62200, total loss = 0.51, predict loss = 0.11 (63.5 examples/sec; 0.063 sec/batch; 103h:50m:49s remains)
INFO - root - 2019-11-04 00:21:30.941445: step 62210, total loss = 0.47, predict loss = 0.10 (67.7 examples/sec; 0.059 sec/batch; 97h:25m:48s remains)
INFO - root - 2019-11-04 00:21:31.595291: step 62220, total loss = 0.62, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 95h:58m:44s remains)
INFO - root - 2019-11-04 00:21:32.257182: step 62230, total loss = 0.53, predict loss = 0.12 (65.4 examples/sec; 0.061 sec/batch; 100h:52m:21s remains)
INFO - root - 2019-11-04 00:21:32.899563: step 62240, total loss = 0.57, predict loss = 0.13 (70.8 examples/sec; 0.057 sec/batch; 93h:11m:31s remains)
INFO - root - 2019-11-04 00:21:33.563031: step 62250, total loss = 0.49, predict loss = 0.12 (74.9 examples/sec; 0.053 sec/batch; 88h:07m:04s remains)
INFO - root - 2019-11-04 00:21:34.193595: step 62260, total loss = 0.60, predict loss = 0.14 (75.3 examples/sec; 0.053 sec/batch; 87h:34m:56s remains)
INFO - root - 2019-11-04 00:21:34.803739: step 62270, total loss = 0.43, predict loss = 0.10 (82.1 examples/sec; 0.049 sec/batch; 80h:23m:42s remains)
INFO - root - 2019-11-04 00:21:35.447538: step 62280, total loss = 0.56, predict loss = 0.13 (66.9 examples/sec; 0.060 sec/batch; 98h:38m:37s remains)
INFO - root - 2019-11-04 00:21:36.070542: step 62290, total loss = 0.46, predict loss = 0.09 (73.0 examples/sec; 0.055 sec/batch; 90h:20m:02s remains)
INFO - root - 2019-11-04 00:21:36.762878: step 62300, total loss = 0.47, predict loss = 0.10 (57.8 examples/sec; 0.069 sec/batch; 114h:13m:10s remains)
INFO - root - 2019-11-04 00:21:37.406313: step 62310, total loss = 0.50, predict loss = 0.11 (65.9 examples/sec; 0.061 sec/batch; 100h:09m:51s remains)
INFO - root - 2019-11-04 00:21:38.080605: step 62320, total loss = 0.47, predict loss = 0.10 (74.3 examples/sec; 0.054 sec/batch; 88h:45m:21s remains)
INFO - root - 2019-11-04 00:21:38.714312: step 62330, total loss = 0.58, predict loss = 0.14 (74.0 examples/sec; 0.054 sec/batch; 89h:08m:39s remains)
INFO - root - 2019-11-04 00:21:39.342050: step 62340, total loss = 0.54, predict loss = 0.12 (61.7 examples/sec; 0.065 sec/batch; 107h:00m:30s remains)
INFO - root - 2019-11-04 00:21:39.987428: step 62350, total loss = 0.62, predict loss = 0.15 (70.2 examples/sec; 0.057 sec/batch; 94h:00m:48s remains)
INFO - root - 2019-11-04 00:21:40.626222: step 62360, total loss = 0.51, predict loss = 0.12 (83.4 examples/sec; 0.048 sec/batch; 79h:08m:15s remains)
INFO - root - 2019-11-04 00:21:41.285054: step 62370, total loss = 0.53, predict loss = 0.12 (74.8 examples/sec; 0.054 sec/batch; 88h:15m:18s remains)
INFO - root - 2019-11-04 00:21:41.988602: step 62380, total loss = 0.60, predict loss = 0.14 (77.3 examples/sec; 0.052 sec/batch; 85h:19m:11s remains)
INFO - root - 2019-11-04 00:21:42.624245: step 62390, total loss = 0.50, predict loss = 0.11 (67.8 examples/sec; 0.059 sec/batch; 97h:21m:33s remains)
INFO - root - 2019-11-04 00:21:43.253571: step 62400, total loss = 0.48, predict loss = 0.10 (67.5 examples/sec; 0.059 sec/batch; 97h:47m:01s remains)
INFO - root - 2019-11-04 00:21:43.906317: step 62410, total loss = 0.52, predict loss = 0.11 (70.8 examples/sec; 0.056 sec/batch; 93h:08m:47s remains)
INFO - root - 2019-11-04 00:21:44.538977: step 62420, total loss = 0.57, predict loss = 0.13 (70.7 examples/sec; 0.057 sec/batch; 93h:22m:15s remains)
INFO - root - 2019-11-04 00:21:45.232380: step 62430, total loss = 0.55, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 95h:16m:23s remains)
INFO - root - 2019-11-04 00:21:45.880643: step 62440, total loss = 0.66, predict loss = 0.16 (70.3 examples/sec; 0.057 sec/batch; 93h:51m:17s remains)
INFO - root - 2019-11-04 00:21:46.616821: step 62450, total loss = 0.77, predict loss = 0.18 (56.6 examples/sec; 0.071 sec/batch; 116h:27m:30s remains)
INFO - root - 2019-11-04 00:21:47.256068: step 62460, total loss = 0.85, predict loss = 0.21 (77.2 examples/sec; 0.052 sec/batch; 85h:30m:05s remains)
INFO - root - 2019-11-04 00:21:47.861397: step 62470, total loss = 0.70, predict loss = 0.16 (69.4 examples/sec; 0.058 sec/batch; 95h:05m:31s remains)
INFO - root - 2019-11-04 00:21:48.548888: step 62480, total loss = 0.63, predict loss = 0.14 (81.3 examples/sec; 0.049 sec/batch; 81h:08m:44s remains)
INFO - root - 2019-11-04 00:21:49.187947: step 62490, total loss = 0.70, predict loss = 0.17 (71.4 examples/sec; 0.056 sec/batch; 92h:21m:16s remains)
INFO - root - 2019-11-04 00:21:49.820748: step 62500, total loss = 0.72, predict loss = 0.17 (70.2 examples/sec; 0.057 sec/batch; 94h:02m:10s remains)
INFO - root - 2019-11-04 00:21:50.431947: step 62510, total loss = 0.67, predict loss = 0.16 (82.9 examples/sec; 0.048 sec/batch; 79h:32m:22s remains)
INFO - root - 2019-11-04 00:21:51.150296: step 62520, total loss = 0.69, predict loss = 0.17 (57.2 examples/sec; 0.070 sec/batch; 115h:24m:54s remains)
INFO - root - 2019-11-04 00:21:51.834388: step 62530, total loss = 0.72, predict loss = 0.17 (54.8 examples/sec; 0.073 sec/batch; 120h:25m:45s remains)
INFO - root - 2019-11-04 00:21:52.499744: step 62540, total loss = 0.79, predict loss = 0.19 (52.0 examples/sec; 0.077 sec/batch; 126h:52m:00s remains)
INFO - root - 2019-11-04 00:21:53.174269: step 62550, total loss = 0.68, predict loss = 0.16 (73.2 examples/sec; 0.055 sec/batch; 90h:04m:32s remains)
INFO - root - 2019-11-04 00:21:53.830842: step 62560, total loss = 0.79, predict loss = 0.20 (61.8 examples/sec; 0.065 sec/batch; 106h:48m:07s remains)
INFO - root - 2019-11-04 00:21:54.498080: step 62570, total loss = 0.52, predict loss = 0.12 (67.7 examples/sec; 0.059 sec/batch; 97h:29m:33s remains)
INFO - root - 2019-11-04 00:21:55.121346: step 62580, total loss = 0.71, predict loss = 0.17 (66.0 examples/sec; 0.061 sec/batch; 100h:01m:25s remains)
INFO - root - 2019-11-04 00:21:55.723624: step 62590, total loss = 0.63, predict loss = 0.14 (64.7 examples/sec; 0.062 sec/batch; 101h:54m:16s remains)
INFO - root - 2019-11-04 00:21:56.353862: step 62600, total loss = 0.42, predict loss = 0.09 (64.8 examples/sec; 0.062 sec/batch; 101h:49m:31s remains)
INFO - root - 2019-11-04 00:21:57.069382: step 62610, total loss = 0.60, predict loss = 0.13 (66.0 examples/sec; 0.061 sec/batch; 99h:55m:14s remains)
INFO - root - 2019-11-04 00:21:57.714219: step 62620, total loss = 0.50, predict loss = 0.12 (63.9 examples/sec; 0.063 sec/batch; 103h:14m:12s remains)
INFO - root - 2019-11-04 00:21:58.319727: step 62630, total loss = 0.54, predict loss = 0.12 (77.0 examples/sec; 0.052 sec/batch; 85h:39m:07s remains)
INFO - root - 2019-11-04 00:21:58.959001: step 62640, total loss = 0.63, predict loss = 0.16 (70.7 examples/sec; 0.057 sec/batch; 93h:16m:12s remains)
INFO - root - 2019-11-04 00:21:59.608715: step 62650, total loss = 0.58, predict loss = 0.14 (67.0 examples/sec; 0.060 sec/batch; 98h:26m:58s remains)
INFO - root - 2019-11-04 00:22:00.273272: step 62660, total loss = 1.01, predict loss = 0.25 (66.8 examples/sec; 0.060 sec/batch; 98h:49m:32s remains)
INFO - root - 2019-11-04 00:22:01.013925: step 62670, total loss = 0.60, predict loss = 0.14 (67.6 examples/sec; 0.059 sec/batch; 97h:32m:02s remains)
INFO - root - 2019-11-04 00:22:01.678069: step 62680, total loss = 0.36, predict loss = 0.08 (79.3 examples/sec; 0.050 sec/batch; 83h:11m:10s remains)
INFO - root - 2019-11-04 00:22:02.314466: step 62690, total loss = 0.43, predict loss = 0.10 (64.5 examples/sec; 0.062 sec/batch; 102h:13m:47s remains)
INFO - root - 2019-11-04 00:22:02.947958: step 62700, total loss = 0.58, predict loss = 0.13 (66.0 examples/sec; 0.061 sec/batch; 99h:53m:21s remains)
INFO - root - 2019-11-04 00:22:03.548543: step 62710, total loss = 0.46, predict loss = 0.10 (97.0 examples/sec; 0.041 sec/batch; 67h:59m:40s remains)
INFO - root - 2019-11-04 00:22:04.002892: step 62720, total loss = 0.55, predict loss = 0.13 (93.3 examples/sec; 0.043 sec/batch; 70h:43m:06s remains)
INFO - root - 2019-11-04 00:22:05.009211: step 62730, total loss = 0.48, predict loss = 0.11 (6.9 examples/sec; 0.584 sec/batch; 962h:34m:21s remains)
INFO - root - 2019-11-04 00:22:05.610820: step 62740, total loss = 0.39, predict loss = 0.08 (75.4 examples/sec; 0.053 sec/batch; 87h:27m:51s remains)
INFO - root - 2019-11-04 00:22:06.275928: step 62750, total loss = 0.63, predict loss = 0.15 (75.1 examples/sec; 0.053 sec/batch; 87h:52m:00s remains)
INFO - root - 2019-11-04 00:22:06.863508: step 62760, total loss = 0.56, predict loss = 0.13 (75.0 examples/sec; 0.053 sec/batch; 87h:58m:05s remains)
INFO - root - 2019-11-04 00:22:07.470665: step 62770, total loss = 0.50, predict loss = 0.11 (64.4 examples/sec; 0.062 sec/batch; 102h:27m:58s remains)
INFO - root - 2019-11-04 00:22:08.093060: step 62780, total loss = 0.76, predict loss = 0.18 (70.4 examples/sec; 0.057 sec/batch; 93h:44m:38s remains)
INFO - root - 2019-11-04 00:22:08.692558: step 62790, total loss = 0.48, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 90h:24m:36s remains)
INFO - root - 2019-11-04 00:22:09.354616: step 62800, total loss = 0.49, predict loss = 0.12 (61.5 examples/sec; 0.065 sec/batch; 107h:14m:56s remains)
INFO - root - 2019-11-04 00:22:09.998748: step 62810, total loss = 0.66, predict loss = 0.16 (69.4 examples/sec; 0.058 sec/batch; 95h:04m:42s remains)
INFO - root - 2019-11-04 00:22:10.643462: step 62820, total loss = 0.55, predict loss = 0.12 (78.3 examples/sec; 0.051 sec/batch; 84h:14m:15s remains)
INFO - root - 2019-11-04 00:22:11.320249: step 62830, total loss = 0.69, predict loss = 0.15 (70.8 examples/sec; 0.057 sec/batch; 93h:12m:24s remains)
INFO - root - 2019-11-04 00:22:11.991450: step 62840, total loss = 0.67, predict loss = 0.16 (78.4 examples/sec; 0.051 sec/batch; 84h:09m:27s remains)
INFO - root - 2019-11-04 00:22:12.646348: step 62850, total loss = 0.70, predict loss = 0.17 (80.6 examples/sec; 0.050 sec/batch; 81h:53m:21s remains)
INFO - root - 2019-11-04 00:22:13.260215: step 62860, total loss = 0.49, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 87h:58m:59s remains)
INFO - root - 2019-11-04 00:22:13.909557: step 62870, total loss = 0.60, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 94h:43m:34s remains)
INFO - root - 2019-11-04 00:22:14.578509: step 62880, total loss = 0.56, predict loss = 0.13 (66.7 examples/sec; 0.060 sec/batch; 98h:52m:15s remains)
INFO - root - 2019-11-04 00:22:15.290473: step 62890, total loss = 0.44, predict loss = 0.10 (63.8 examples/sec; 0.063 sec/batch; 103h:25m:19s remains)
INFO - root - 2019-11-04 00:22:15.940934: step 62900, total loss = 0.71, predict loss = 0.17 (70.7 examples/sec; 0.057 sec/batch; 93h:20m:03s remains)
INFO - root - 2019-11-04 00:22:16.565443: step 62910, total loss = 0.55, predict loss = 0.12 (73.4 examples/sec; 0.054 sec/batch; 89h:50m:36s remains)
INFO - root - 2019-11-04 00:22:17.184472: step 62920, total loss = 0.71, predict loss = 0.17 (66.7 examples/sec; 0.060 sec/batch; 98h:56m:40s remains)
INFO - root - 2019-11-04 00:22:17.852215: step 62930, total loss = 0.50, predict loss = 0.11 (65.0 examples/sec; 0.062 sec/batch; 101h:28m:10s remains)
INFO - root - 2019-11-04 00:22:18.522436: step 62940, total loss = 0.47, predict loss = 0.11 (67.6 examples/sec; 0.059 sec/batch; 97h:35m:49s remains)
INFO - root - 2019-11-04 00:22:19.176811: step 62950, total loss = 0.44, predict loss = 0.09 (69.7 examples/sec; 0.057 sec/batch; 94h:40m:50s remains)
INFO - root - 2019-11-04 00:22:19.907421: step 62960, total loss = 0.44, predict loss = 0.10 (65.2 examples/sec; 0.061 sec/batch; 101h:07m:49s remains)
INFO - root - 2019-11-04 00:22:20.551138: step 62970, total loss = 0.51, predict loss = 0.12 (72.1 examples/sec; 0.055 sec/batch; 91h:27m:16s remains)
INFO - root - 2019-11-04 00:22:21.212651: step 62980, total loss = 0.66, predict loss = 0.15 (65.9 examples/sec; 0.061 sec/batch; 100h:06m:45s remains)
INFO - root - 2019-11-04 00:22:21.860762: step 62990, total loss = 0.53, predict loss = 0.12 (80.0 examples/sec; 0.050 sec/batch; 82h:25m:01s remains)
INFO - root - 2019-11-04 00:22:22.504485: step 63000, total loss = 0.60, predict loss = 0.14 (67.6 examples/sec; 0.059 sec/batch; 97h:34m:49s remains)
INFO - root - 2019-11-04 00:22:23.197497: step 63010, total loss = 0.49, predict loss = 0.11 (72.7 examples/sec; 0.055 sec/batch; 90h:47m:47s remains)
INFO - root - 2019-11-04 00:22:23.881673: step 63020, total loss = 0.62, predict loss = 0.14 (66.7 examples/sec; 0.060 sec/batch; 98h:55m:40s remains)
INFO - root - 2019-11-04 00:22:24.578153: step 63030, total loss = 0.69, predict loss = 0.15 (57.9 examples/sec; 0.069 sec/batch; 113h:50m:22s remains)
INFO - root - 2019-11-04 00:22:25.217267: step 63040, total loss = 0.69, predict loss = 0.16 (72.3 examples/sec; 0.055 sec/batch; 91h:12m:08s remains)
INFO - root - 2019-11-04 00:22:25.848037: step 63050, total loss = 0.54, predict loss = 0.12 (68.1 examples/sec; 0.059 sec/batch; 96h:53m:02s remains)
INFO - root - 2019-11-04 00:22:26.499129: step 63060, total loss = 0.63, predict loss = 0.14 (67.7 examples/sec; 0.059 sec/batch; 97h:27m:01s remains)
INFO - root - 2019-11-04 00:22:27.111453: step 63070, total loss = 0.55, predict loss = 0.13 (77.1 examples/sec; 0.052 sec/batch; 85h:32m:02s remains)
INFO - root - 2019-11-04 00:22:27.832773: step 63080, total loss = 0.61, predict loss = 0.14 (71.2 examples/sec; 0.056 sec/batch; 92h:37m:14s remains)
INFO - root - 2019-11-04 00:22:28.481057: step 63090, total loss = 0.65, predict loss = 0.15 (57.5 examples/sec; 0.070 sec/batch; 114h:39m:03s remains)
INFO - root - 2019-11-04 00:22:29.253530: step 63100, total loss = 0.59, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:17m:56s remains)
INFO - root - 2019-11-04 00:22:29.910194: step 63110, total loss = 0.48, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 89h:15m:36s remains)
INFO - root - 2019-11-04 00:22:30.549997: step 63120, total loss = 0.42, predict loss = 0.09 (75.6 examples/sec; 0.053 sec/batch; 87h:11m:54s remains)
INFO - root - 2019-11-04 00:22:31.156657: step 63130, total loss = 0.49, predict loss = 0.09 (73.1 examples/sec; 0.055 sec/batch; 90h:17m:33s remains)
INFO - root - 2019-11-04 00:22:31.765555: step 63140, total loss = 0.68, predict loss = 0.16 (69.5 examples/sec; 0.058 sec/batch; 94h:52m:58s remains)
INFO - root - 2019-11-04 00:22:32.395082: step 63150, total loss = 0.50, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 87h:57m:21s remains)
INFO - root - 2019-11-04 00:22:33.119643: step 63160, total loss = 0.53, predict loss = 0.12 (60.9 examples/sec; 0.066 sec/batch; 108h:21m:19s remains)
INFO - root - 2019-11-04 00:22:33.861645: step 63170, total loss = 0.53, predict loss = 0.12 (59.2 examples/sec; 0.068 sec/batch; 111h:27m:57s remains)
INFO - root - 2019-11-04 00:22:34.659565: step 63180, total loss = 0.56, predict loss = 0.12 (60.5 examples/sec; 0.066 sec/batch; 109h:07m:19s remains)
INFO - root - 2019-11-04 00:22:35.434193: step 63190, total loss = 0.65, predict loss = 0.15 (61.5 examples/sec; 0.065 sec/batch; 107h:17m:16s remains)
INFO - root - 2019-11-04 00:22:36.046947: step 63200, total loss = 0.50, predict loss = 0.12 (72.1 examples/sec; 0.055 sec/batch; 91h:28m:54s remains)
INFO - root - 2019-11-04 00:22:36.644289: step 63210, total loss = 0.55, predict loss = 0.12 (75.2 examples/sec; 0.053 sec/batch; 87h:43m:13s remains)
INFO - root - 2019-11-04 00:22:37.281216: step 63220, total loss = 0.68, predict loss = 0.17 (65.1 examples/sec; 0.061 sec/batch; 101h:22m:05s remains)
INFO - root - 2019-11-04 00:22:37.931784: step 63230, total loss = 0.61, predict loss = 0.15 (72.8 examples/sec; 0.055 sec/batch; 90h:36m:34s remains)
INFO - root - 2019-11-04 00:22:38.549058: step 63240, total loss = 0.66, predict loss = 0.17 (61.5 examples/sec; 0.065 sec/batch; 107h:17m:39s remains)
INFO - root - 2019-11-04 00:22:39.280778: step 63250, total loss = 0.76, predict loss = 0.19 (65.5 examples/sec; 0.061 sec/batch; 100h:40m:50s remains)
INFO - root - 2019-11-04 00:22:39.883455: step 63260, total loss = 0.63, predict loss = 0.15 (75.2 examples/sec; 0.053 sec/batch; 87h:44m:27s remains)
INFO - root - 2019-11-04 00:22:40.488969: step 63270, total loss = 0.59, predict loss = 0.14 (69.6 examples/sec; 0.058 sec/batch; 94h:50m:09s remains)
INFO - root - 2019-11-04 00:22:41.110404: step 63280, total loss = 0.43, predict loss = 0.10 (81.8 examples/sec; 0.049 sec/batch; 80h:37m:42s remains)
INFO - root - 2019-11-04 00:22:41.741159: step 63290, total loss = 0.35, predict loss = 0.08 (66.7 examples/sec; 0.060 sec/batch; 98h:52m:04s remains)
INFO - root - 2019-11-04 00:22:42.386280: step 63300, total loss = 0.46, predict loss = 0.12 (74.6 examples/sec; 0.054 sec/batch; 88h:23m:30s remains)
INFO - root - 2019-11-04 00:22:43.008561: step 63310, total loss = 0.48, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 95h:39m:28s remains)
INFO - root - 2019-11-04 00:22:43.621332: step 63320, total loss = 0.49, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 94h:39m:26s remains)
INFO - root - 2019-11-04 00:22:44.295229: step 63330, total loss = 0.43, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 93h:02m:38s remains)
INFO - root - 2019-11-04 00:22:44.932753: step 63340, total loss = 0.31, predict loss = 0.07 (73.0 examples/sec; 0.055 sec/batch; 90h:25m:06s remains)
INFO - root - 2019-11-04 00:22:45.545048: step 63350, total loss = 0.42, predict loss = 0.10 (73.8 examples/sec; 0.054 sec/batch; 89h:23m:01s remains)
INFO - root - 2019-11-04 00:22:46.202850: step 63360, total loss = 0.44, predict loss = 0.09 (68.8 examples/sec; 0.058 sec/batch; 95h:56m:18s remains)
INFO - root - 2019-11-04 00:22:46.889744: step 63370, total loss = 0.51, predict loss = 0.12 (54.8 examples/sec; 0.073 sec/batch; 120h:23m:06s remains)
INFO - root - 2019-11-04 00:22:47.604017: step 63380, total loss = 0.43, predict loss = 0.09 (78.7 examples/sec; 0.051 sec/batch; 83h:51m:57s remains)
INFO - root - 2019-11-04 00:22:48.295290: step 63390, total loss = 0.52, predict loss = 0.11 (75.7 examples/sec; 0.053 sec/batch; 87h:05m:27s remains)
INFO - root - 2019-11-04 00:22:48.898563: step 63400, total loss = 0.61, predict loss = 0.15 (75.1 examples/sec; 0.053 sec/batch; 87h:51m:39s remains)
INFO - root - 2019-11-04 00:22:49.500835: step 63410, total loss = 0.45, predict loss = 0.09 (85.9 examples/sec; 0.047 sec/batch; 76h:45m:30s remains)
INFO - root - 2019-11-04 00:22:50.130521: step 63420, total loss = 0.54, predict loss = 0.13 (70.0 examples/sec; 0.057 sec/batch; 94h:14m:51s remains)
INFO - root - 2019-11-04 00:22:50.788200: step 63430, total loss = 0.73, predict loss = 0.17 (70.7 examples/sec; 0.057 sec/batch; 93h:14m:36s remains)
INFO - root - 2019-11-04 00:22:51.398353: step 63440, total loss = 0.64, predict loss = 0.14 (79.2 examples/sec; 0.051 sec/batch; 83h:19m:25s remains)
INFO - root - 2019-11-04 00:22:52.061153: step 63450, total loss = 0.48, predict loss = 0.11 (71.3 examples/sec; 0.056 sec/batch; 92h:31m:22s remains)
INFO - root - 2019-11-04 00:22:52.706092: step 63460, total loss = 0.52, predict loss = 0.12 (76.9 examples/sec; 0.052 sec/batch; 85h:48m:07s remains)
INFO - root - 2019-11-04 00:22:53.352712: step 63470, total loss = 0.53, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 92h:24m:17s remains)
INFO - root - 2019-11-04 00:22:54.016937: step 63480, total loss = 0.51, predict loss = 0.12 (65.3 examples/sec; 0.061 sec/batch; 101h:01m:45s remains)
INFO - root - 2019-11-04 00:22:54.654461: step 63490, total loss = 0.41, predict loss = 0.09 (76.8 examples/sec; 0.052 sec/batch; 85h:52m:34s remains)
INFO - root - 2019-11-04 00:22:55.371161: step 63500, total loss = 0.55, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 99h:41m:35s remains)
INFO - root - 2019-11-04 00:22:56.018958: step 63510, total loss = 0.65, predict loss = 0.16 (67.8 examples/sec; 0.059 sec/batch; 97h:14m:54s remains)
INFO - root - 2019-11-04 00:22:56.677366: step 63520, total loss = 0.43, predict loss = 0.10 (63.3 examples/sec; 0.063 sec/batch; 104h:12m:00s remains)
INFO - root - 2019-11-04 00:22:57.335777: step 63530, total loss = 0.59, predict loss = 0.14 (82.3 examples/sec; 0.049 sec/batch; 80h:05m:59s remains)
INFO - root - 2019-11-04 00:22:57.975402: step 63540, total loss = 0.58, predict loss = 0.14 (68.8 examples/sec; 0.058 sec/batch; 95h:54m:27s remains)
INFO - root - 2019-11-04 00:22:58.600364: step 63550, total loss = 0.50, predict loss = 0.11 (74.6 examples/sec; 0.054 sec/batch; 88h:24m:09s remains)
INFO - root - 2019-11-04 00:22:59.251035: step 63560, total loss = 0.36, predict loss = 0.07 (64.2 examples/sec; 0.062 sec/batch; 102h:46m:55s remains)
INFO - root - 2019-11-04 00:22:59.897552: step 63570, total loss = 0.47, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 99h:14m:31s remains)
INFO - root - 2019-11-04 00:23:00.534696: step 63580, total loss = 0.25, predict loss = 0.05 (70.9 examples/sec; 0.056 sec/batch; 93h:04m:16s remains)
INFO - root - 2019-11-04 00:23:01.173999: step 63590, total loss = 0.29, predict loss = 0.06 (71.2 examples/sec; 0.056 sec/batch; 92h:34m:50s remains)
INFO - root - 2019-11-04 00:23:01.783449: step 63600, total loss = 0.32, predict loss = 0.06 (72.4 examples/sec; 0.055 sec/batch; 91h:08m:54s remains)
INFO - root - 2019-11-04 00:23:02.430096: step 63610, total loss = 0.29, predict loss = 0.06 (67.5 examples/sec; 0.059 sec/batch; 97h:40m:35s remains)
INFO - root - 2019-11-04 00:23:03.054409: step 63620, total loss = 0.35, predict loss = 0.08 (64.9 examples/sec; 0.062 sec/batch; 101h:41m:51s remains)
INFO - root - 2019-11-04 00:23:03.673374: step 63630, total loss = 0.39, predict loss = 0.09 (82.0 examples/sec; 0.049 sec/batch; 80h:29m:02s remains)
INFO - root - 2019-11-04 00:23:04.295968: step 63640, total loss = 0.52, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 98h:37m:44s remains)
INFO - root - 2019-11-04 00:23:05.040371: step 63650, total loss = 0.44, predict loss = 0.10 (62.7 examples/sec; 0.064 sec/batch; 105h:15m:42s remains)
INFO - root - 2019-11-04 00:23:05.672420: step 63660, total loss = 0.47, predict loss = 0.10 (66.9 examples/sec; 0.060 sec/batch; 98h:36m:56s remains)
INFO - root - 2019-11-04 00:23:06.318393: step 63670, total loss = 0.53, predict loss = 0.12 (67.0 examples/sec; 0.060 sec/batch; 98h:24m:58s remains)
INFO - root - 2019-11-04 00:23:06.944522: step 63680, total loss = 0.44, predict loss = 0.10 (69.9 examples/sec; 0.057 sec/batch; 94h:22m:04s remains)
INFO - root - 2019-11-04 00:23:07.558692: step 63690, total loss = 0.42, predict loss = 0.09 (71.8 examples/sec; 0.056 sec/batch; 91h:51m:16s remains)
INFO - root - 2019-11-04 00:23:08.224393: step 63700, total loss = 0.29, predict loss = 0.06 (63.9 examples/sec; 0.063 sec/batch; 103h:14m:56s remains)
INFO - root - 2019-11-04 00:23:08.897532: step 63710, total loss = 0.44, predict loss = 0.10 (69.0 examples/sec; 0.058 sec/batch; 95h:31m:58s remains)
INFO - root - 2019-11-04 00:23:09.561699: step 63720, total loss = 0.52, predict loss = 0.12 (63.9 examples/sec; 0.063 sec/batch; 103h:16m:07s remains)
INFO - root - 2019-11-04 00:23:10.184146: step 63730, total loss = 0.61, predict loss = 0.14 (67.2 examples/sec; 0.060 sec/batch; 98h:08m:45s remains)
INFO - root - 2019-11-04 00:23:10.825446: step 63740, total loss = 0.63, predict loss = 0.14 (67.6 examples/sec; 0.059 sec/batch; 97h:31m:52s remains)
INFO - root - 2019-11-04 00:23:11.448329: step 63750, total loss = 0.53, predict loss = 0.11 (68.6 examples/sec; 0.058 sec/batch; 96h:11m:35s remains)
INFO - root - 2019-11-04 00:23:12.075762: step 63760, total loss = 0.56, predict loss = 0.14 (70.8 examples/sec; 0.057 sec/batch; 93h:11m:05s remains)
INFO - root - 2019-11-04 00:23:12.736027: step 63770, total loss = 0.60, predict loss = 0.14 (69.5 examples/sec; 0.058 sec/batch; 94h:58m:13s remains)
INFO - root - 2019-11-04 00:23:13.392770: step 63780, total loss = 0.54, predict loss = 0.14 (78.0 examples/sec; 0.051 sec/batch; 84h:35m:00s remains)
INFO - root - 2019-11-04 00:23:14.025732: step 63790, total loss = 0.47, predict loss = 0.10 (66.2 examples/sec; 0.060 sec/batch; 99h:38m:45s remains)
INFO - root - 2019-11-04 00:23:14.678465: step 63800, total loss = 0.36, predict loss = 0.08 (71.8 examples/sec; 0.056 sec/batch; 91h:54m:13s remains)
INFO - root - 2019-11-04 00:23:15.341966: step 63810, total loss = 0.27, predict loss = 0.06 (70.8 examples/sec; 0.056 sec/batch; 93h:07m:32s remains)
INFO - root - 2019-11-04 00:23:15.997260: step 63820, total loss = 0.26, predict loss = 0.06 (73.1 examples/sec; 0.055 sec/batch; 90h:16m:42s remains)
INFO - root - 2019-11-04 00:23:16.645999: step 63830, total loss = 0.35, predict loss = 0.08 (65.4 examples/sec; 0.061 sec/batch; 100h:49m:44s remains)
INFO - root - 2019-11-04 00:23:17.329526: step 63840, total loss = 0.37, predict loss = 0.09 (73.1 examples/sec; 0.055 sec/batch; 90h:13m:19s remains)
INFO - root - 2019-11-04 00:23:18.004970: step 63850, total loss = 0.31, predict loss = 0.07 (68.5 examples/sec; 0.058 sec/batch; 96h:14m:22s remains)
INFO - root - 2019-11-04 00:23:18.665272: step 63860, total loss = 0.49, predict loss = 0.11 (73.3 examples/sec; 0.055 sec/batch; 89h:58m:23s remains)
INFO - root - 2019-11-04 00:23:19.328137: step 63870, total loss = 0.61, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 101h:16m:42s remains)
INFO - root - 2019-11-04 00:23:20.049872: step 63880, total loss = 0.44, predict loss = 0.10 (55.3 examples/sec; 0.072 sec/batch; 119h:15m:26s remains)
INFO - root - 2019-11-04 00:23:20.696187: step 63890, total loss = 0.54, predict loss = 0.12 (80.7 examples/sec; 0.050 sec/batch; 81h:43m:04s remains)
INFO - root - 2019-11-04 00:23:21.305964: step 63900, total loss = 0.41, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 93h:04m:30s remains)
INFO - root - 2019-11-04 00:23:21.915886: step 63910, total loss = 0.54, predict loss = 0.12 (77.9 examples/sec; 0.051 sec/batch; 84h:36m:57s remains)
INFO - root - 2019-11-04 00:23:22.599046: step 63920, total loss = 0.56, predict loss = 0.13 (55.2 examples/sec; 0.072 sec/batch; 119h:31m:22s remains)
INFO - root - 2019-11-04 00:23:23.206968: step 63930, total loss = 0.48, predict loss = 0.12 (79.2 examples/sec; 0.051 sec/batch; 83h:17m:28s remains)
INFO - root - 2019-11-04 00:23:23.859903: step 63940, total loss = 0.42, predict loss = 0.09 (76.1 examples/sec; 0.053 sec/batch; 86h:39m:35s remains)
INFO - root - 2019-11-04 00:23:24.541291: step 63950, total loss = 0.50, predict loss = 0.11 (62.6 examples/sec; 0.064 sec/batch; 105h:21m:32s remains)
INFO - root - 2019-11-04 00:23:25.234575: step 63960, total loss = 0.52, predict loss = 0.12 (69.9 examples/sec; 0.057 sec/batch; 94h:25m:07s remains)
INFO - root - 2019-11-04 00:23:25.896688: step 63970, total loss = 0.46, predict loss = 0.11 (66.6 examples/sec; 0.060 sec/batch; 99h:05m:41s remains)
INFO - root - 2019-11-04 00:23:26.542053: step 63980, total loss = 0.48, predict loss = 0.10 (69.1 examples/sec; 0.058 sec/batch; 95h:29m:18s remains)
INFO - root - 2019-11-04 00:23:27.173348: step 63990, total loss = 0.46, predict loss = 0.10 (65.7 examples/sec; 0.061 sec/batch; 100h:24m:00s remains)
INFO - root - 2019-11-04 00:23:27.780982: step 64000, total loss = 0.56, predict loss = 0.13 (72.0 examples/sec; 0.056 sec/batch; 91h:36m:32s remains)
INFO - root - 2019-11-04 00:23:28.398487: step 64010, total loss = 0.56, predict loss = 0.13 (71.6 examples/sec; 0.056 sec/batch; 92h:06m:44s remains)
INFO - root - 2019-11-04 00:23:29.046744: step 64020, total loss = 0.57, predict loss = 0.13 (80.8 examples/sec; 0.050 sec/batch; 81h:40m:13s remains)
INFO - root - 2019-11-04 00:23:29.693011: step 64030, total loss = 0.55, predict loss = 0.12 (77.3 examples/sec; 0.052 sec/batch; 85h:18m:38s remains)
INFO - root - 2019-11-04 00:23:30.367647: step 64040, total loss = 0.53, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 91h:11m:09s remains)
INFO - root - 2019-11-04 00:23:31.047077: step 64050, total loss = 0.41, predict loss = 0.08 (70.4 examples/sec; 0.057 sec/batch; 93h:43m:27s remains)
INFO - root - 2019-11-04 00:23:31.719217: step 64060, total loss = 0.58, predict loss = 0.13 (67.2 examples/sec; 0.060 sec/batch; 98h:08m:54s remains)
INFO - root - 2019-11-04 00:23:32.397250: step 64070, total loss = 0.41, predict loss = 0.09 (65.5 examples/sec; 0.061 sec/batch; 100h:42m:33s remains)
INFO - root - 2019-11-04 00:23:33.050076: step 64080, total loss = 0.37, predict loss = 0.08 (69.4 examples/sec; 0.058 sec/batch; 95h:01m:48s remains)
INFO - root - 2019-11-04 00:23:33.719058: step 64090, total loss = 0.33, predict loss = 0.07 (78.1 examples/sec; 0.051 sec/batch; 84h:26m:40s remains)
INFO - root - 2019-11-04 00:23:34.367194: step 64100, total loss = 0.42, predict loss = 0.10 (66.4 examples/sec; 0.060 sec/batch; 99h:22m:20s remains)
INFO - root - 2019-11-04 00:23:35.096989: step 64110, total loss = 0.47, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 99h:59m:42s remains)
INFO - root - 2019-11-04 00:23:35.712825: step 64120, total loss = 0.37, predict loss = 0.08 (71.0 examples/sec; 0.056 sec/batch; 92h:54m:02s remains)
INFO - root - 2019-11-04 00:23:36.358652: step 64130, total loss = 0.54, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 98h:00m:52s remains)
INFO - root - 2019-11-04 00:23:37.041161: step 64140, total loss = 0.48, predict loss = 0.11 (68.0 examples/sec; 0.059 sec/batch; 97h:00m:36s remains)
INFO - root - 2019-11-04 00:23:37.682536: step 64150, total loss = 0.64, predict loss = 0.15 (75.0 examples/sec; 0.053 sec/batch; 87h:53m:28s remains)
INFO - root - 2019-11-04 00:23:38.304674: step 64160, total loss = 0.62, predict loss = 0.14 (76.7 examples/sec; 0.052 sec/batch; 85h:58m:18s remains)
INFO - root - 2019-11-04 00:23:38.934965: step 64170, total loss = 0.85, predict loss = 0.20 (72.4 examples/sec; 0.055 sec/batch; 91h:09m:22s remains)
INFO - root - 2019-11-04 00:23:39.550598: step 64180, total loss = 0.87, predict loss = 0.21 (78.3 examples/sec; 0.051 sec/batch; 84h:16m:55s remains)
INFO - root - 2019-11-04 00:23:40.193339: step 64190, total loss = 0.70, predict loss = 0.15 (74.7 examples/sec; 0.054 sec/batch; 88h:19m:33s remains)
INFO - root - 2019-11-04 00:23:40.799329: step 64200, total loss = 0.69, predict loss = 0.16 (77.0 examples/sec; 0.052 sec/batch; 85h:40m:31s remains)
INFO - root - 2019-11-04 00:23:41.417758: step 64210, total loss = 0.71, predict loss = 0.16 (77.3 examples/sec; 0.052 sec/batch; 85h:17m:33s remains)
INFO - root - 2019-11-04 00:23:42.033945: step 64220, total loss = 0.57, predict loss = 0.12 (68.5 examples/sec; 0.058 sec/batch; 96h:17m:17s remains)
INFO - root - 2019-11-04 00:23:42.643976: step 64230, total loss = 0.76, predict loss = 0.18 (76.9 examples/sec; 0.052 sec/batch; 85h:45m:32s remains)
INFO - root - 2019-11-04 00:23:43.258720: step 64240, total loss = 0.63, predict loss = 0.15 (76.0 examples/sec; 0.053 sec/batch; 86h:48m:30s remains)
INFO - root - 2019-11-04 00:23:43.906038: step 64250, total loss = 0.67, predict loss = 0.15 (73.8 examples/sec; 0.054 sec/batch; 89h:25m:02s remains)
INFO - root - 2019-11-04 00:23:44.547884: step 64260, total loss = 0.55, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 95h:06m:32s remains)
INFO - root - 2019-11-04 00:23:45.169088: step 64270, total loss = 0.51, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 96h:01m:11s remains)
INFO - root - 2019-11-04 00:23:45.798401: step 64280, total loss = 0.61, predict loss = 0.14 (63.9 examples/sec; 0.063 sec/batch; 103h:16m:40s remains)
INFO - root - 2019-11-04 00:23:46.430072: step 64290, total loss = 0.82, predict loss = 0.19 (68.7 examples/sec; 0.058 sec/batch; 95h:56m:39s remains)
INFO - root - 2019-11-04 00:23:47.071675: step 64300, total loss = 0.59, predict loss = 0.14 (67.4 examples/sec; 0.059 sec/batch; 97h:50m:59s remains)
INFO - root - 2019-11-04 00:23:47.682803: step 64310, total loss = 0.51, predict loss = 0.12 (76.2 examples/sec; 0.052 sec/batch; 86h:32m:21s remains)
INFO - root - 2019-11-04 00:23:48.294773: step 64320, total loss = 0.44, predict loss = 0.10 (76.2 examples/sec; 0.052 sec/batch; 86h:30m:37s remains)
INFO - root - 2019-11-04 00:23:48.894935: step 64330, total loss = 0.48, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 93h:28m:49s remains)
INFO - root - 2019-11-04 00:23:49.514033: step 64340, total loss = 0.54, predict loss = 0.13 (66.4 examples/sec; 0.060 sec/batch; 99h:19m:24s remains)
INFO - root - 2019-11-04 00:23:50.129674: step 64350, total loss = 0.53, predict loss = 0.12 (79.1 examples/sec; 0.051 sec/batch; 83h:20m:00s remains)
INFO - root - 2019-11-04 00:23:50.789348: step 64360, total loss = 0.65, predict loss = 0.15 (70.8 examples/sec; 0.056 sec/batch; 93h:06m:12s remains)
INFO - root - 2019-11-04 00:23:51.417605: step 64370, total loss = 0.55, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 90h:49m:57s remains)
INFO - root - 2019-11-04 00:23:52.048972: step 64380, total loss = 0.69, predict loss = 0.17 (69.8 examples/sec; 0.057 sec/batch; 94h:30m:46s remains)
INFO - root - 2019-11-04 00:23:52.691727: step 64390, total loss = 0.68, predict loss = 0.15 (67.6 examples/sec; 0.059 sec/batch; 97h:37m:58s remains)
INFO - root - 2019-11-04 00:23:53.315015: step 64400, total loss = 0.42, predict loss = 0.09 (73.2 examples/sec; 0.055 sec/batch; 90h:07m:35s remains)
INFO - root - 2019-11-04 00:23:53.983800: step 64410, total loss = 0.49, predict loss = 0.11 (63.1 examples/sec; 0.063 sec/batch; 104h:30m:52s remains)
INFO - root - 2019-11-04 00:23:54.592350: step 64420, total loss = 0.36, predict loss = 0.08 (76.8 examples/sec; 0.052 sec/batch; 85h:49m:37s remains)
INFO - root - 2019-11-04 00:23:55.208811: step 64430, total loss = 0.39, predict loss = 0.09 (70.7 examples/sec; 0.057 sec/batch; 93h:16m:28s remains)
INFO - root - 2019-11-04 00:23:55.819065: step 64440, total loss = 0.41, predict loss = 0.09 (77.5 examples/sec; 0.052 sec/batch; 85h:07m:47s remains)
INFO - root - 2019-11-04 00:23:56.487592: step 64450, total loss = 0.54, predict loss = 0.12 (67.4 examples/sec; 0.059 sec/batch; 97h:52m:58s remains)
INFO - root - 2019-11-04 00:23:57.103335: step 64460, total loss = 0.45, predict loss = 0.09 (76.4 examples/sec; 0.052 sec/batch; 86h:20m:44s remains)
INFO - root - 2019-11-04 00:23:57.721866: step 64470, total loss = 0.48, predict loss = 0.10 (68.8 examples/sec; 0.058 sec/batch; 95h:49m:34s remains)
INFO - root - 2019-11-04 00:23:58.376680: step 64480, total loss = 0.44, predict loss = 0.10 (66.0 examples/sec; 0.061 sec/batch; 99h:57m:10s remains)
INFO - root - 2019-11-04 00:23:59.002688: step 64490, total loss = 0.50, predict loss = 0.12 (78.7 examples/sec; 0.051 sec/batch; 83h:46m:23s remains)
INFO - root - 2019-11-04 00:23:59.684171: step 64500, total loss = 0.48, predict loss = 0.11 (66.3 examples/sec; 0.060 sec/batch; 99h:27m:27s remains)
INFO - root - 2019-11-04 00:24:00.386674: step 64510, total loss = 0.52, predict loss = 0.12 (60.3 examples/sec; 0.066 sec/batch; 109h:17m:47s remains)
INFO - root - 2019-11-04 00:24:01.039368: step 64520, total loss = 0.41, predict loss = 0.09 (73.3 examples/sec; 0.055 sec/batch; 89h:59m:36s remains)
INFO - root - 2019-11-04 00:24:01.691010: step 64530, total loss = 0.65, predict loss = 0.15 (70.2 examples/sec; 0.057 sec/batch; 94h:00m:21s remains)
INFO - root - 2019-11-04 00:24:02.356856: step 64540, total loss = 0.48, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 96h:46m:39s remains)
INFO - root - 2019-11-04 00:24:03.024420: step 64550, total loss = 0.46, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 95h:09m:43s remains)
INFO - root - 2019-11-04 00:24:03.654017: step 64560, total loss = 0.70, predict loss = 0.16 (73.4 examples/sec; 0.055 sec/batch; 89h:53m:19s remains)
INFO - root - 2019-11-04 00:24:04.293539: step 64570, total loss = 0.60, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 95h:50m:45s remains)
INFO - root - 2019-11-04 00:24:05.038688: step 64580, total loss = 0.64, predict loss = 0.15 (62.8 examples/sec; 0.064 sec/batch; 105h:05m:15s remains)
INFO - root - 2019-11-04 00:24:05.649936: step 64590, total loss = 0.58, predict loss = 0.13 (80.8 examples/sec; 0.050 sec/batch; 81h:36m:46s remains)
INFO - root - 2019-11-04 00:24:06.256257: step 64600, total loss = 0.61, predict loss = 0.15 (76.1 examples/sec; 0.053 sec/batch; 86h:37m:51s remains)
INFO - root - 2019-11-04 00:24:06.873111: step 64610, total loss = 0.66, predict loss = 0.15 (67.2 examples/sec; 0.060 sec/batch; 98h:06m:04s remains)
INFO - root - 2019-11-04 00:24:07.526511: step 64620, total loss = 0.63, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 95h:13m:51s remains)
INFO - root - 2019-11-04 00:24:08.190897: step 64630, total loss = 0.66, predict loss = 0.15 (59.8 examples/sec; 0.067 sec/batch; 110h:14m:02s remains)
INFO - root - 2019-11-04 00:24:08.839786: step 64640, total loss = 0.60, predict loss = 0.15 (74.8 examples/sec; 0.053 sec/batch; 88h:08m:06s remains)
INFO - root - 2019-11-04 00:24:09.474733: step 64650, total loss = 0.69, predict loss = 0.16 (81.7 examples/sec; 0.049 sec/batch; 80h:44m:09s remains)
INFO - root - 2019-11-04 00:24:10.091713: step 64660, total loss = 0.68, predict loss = 0.16 (71.8 examples/sec; 0.056 sec/batch; 91h:50m:42s remains)
INFO - root - 2019-11-04 00:24:10.703541: step 64670, total loss = 0.65, predict loss = 0.16 (69.0 examples/sec; 0.058 sec/batch; 95h:30m:54s remains)
INFO - root - 2019-11-04 00:24:11.306951: step 64680, total loss = 0.48, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 87h:20m:03s remains)
INFO - root - 2019-11-04 00:24:11.951593: step 64690, total loss = 0.65, predict loss = 0.14 (63.5 examples/sec; 0.063 sec/batch; 103h:53m:00s remains)
INFO - root - 2019-11-04 00:24:12.600064: step 64700, total loss = 0.48, predict loss = 0.10 (74.7 examples/sec; 0.054 sec/batch; 88h:19m:43s remains)
INFO - root - 2019-11-04 00:24:13.282880: step 64710, total loss = 0.34, predict loss = 0.08 (60.7 examples/sec; 0.066 sec/batch; 108h:34m:28s remains)
INFO - root - 2019-11-04 00:24:13.946345: step 64720, total loss = 0.44, predict loss = 0.10 (67.2 examples/sec; 0.060 sec/batch; 98h:10m:53s remains)
INFO - root - 2019-11-04 00:24:14.615205: step 64730, total loss = 0.43, predict loss = 0.10 (67.8 examples/sec; 0.059 sec/batch; 97h:16m:57s remains)
INFO - root - 2019-11-04 00:24:15.273369: step 64740, total loss = 0.45, predict loss = 0.11 (65.4 examples/sec; 0.061 sec/batch; 100h:49m:18s remains)
INFO - root - 2019-11-04 00:24:15.931363: step 64750, total loss = 0.50, predict loss = 0.11 (69.5 examples/sec; 0.058 sec/batch; 94h:51m:51s remains)
INFO - root - 2019-11-04 00:24:16.593680: step 64760, total loss = 0.58, predict loss = 0.14 (64.3 examples/sec; 0.062 sec/batch; 102h:34m:45s remains)
INFO - root - 2019-11-04 00:24:17.240275: step 64770, total loss = 0.58, predict loss = 0.14 (72.8 examples/sec; 0.055 sec/batch; 90h:37m:42s remains)
INFO - root - 2019-11-04 00:24:17.881250: step 64780, total loss = 0.47, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 93h:21m:11s remains)
INFO - root - 2019-11-04 00:24:18.514310: step 64790, total loss = 0.50, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 90h:16m:46s remains)
INFO - root - 2019-11-04 00:24:19.174512: step 64800, total loss = 0.70, predict loss = 0.17 (71.9 examples/sec; 0.056 sec/batch; 91h:41m:07s remains)
INFO - root - 2019-11-04 00:24:19.820816: step 64810, total loss = 0.63, predict loss = 0.15 (71.5 examples/sec; 0.056 sec/batch; 92h:11m:02s remains)
INFO - root - 2019-11-04 00:24:20.460218: step 64820, total loss = 0.56, predict loss = 0.13 (68.9 examples/sec; 0.058 sec/batch; 95h:39m:49s remains)
INFO - root - 2019-11-04 00:24:21.081326: step 64830, total loss = 0.57, predict loss = 0.14 (80.7 examples/sec; 0.050 sec/batch; 81h:42m:40s remains)
INFO - root - 2019-11-04 00:24:21.749867: step 64840, total loss = 0.63, predict loss = 0.15 (70.4 examples/sec; 0.057 sec/batch; 93h:39m:10s remains)
INFO - root - 2019-11-04 00:24:22.387907: step 64850, total loss = 0.61, predict loss = 0.15 (74.5 examples/sec; 0.054 sec/batch; 88h:34m:17s remains)
INFO - root - 2019-11-04 00:24:23.061558: step 64860, total loss = 0.60, predict loss = 0.14 (71.7 examples/sec; 0.056 sec/batch; 91h:55m:11s remains)
INFO - root - 2019-11-04 00:24:23.771312: step 64870, total loss = 0.57, predict loss = 0.15 (59.0 examples/sec; 0.068 sec/batch; 111h:51m:59s remains)
INFO - root - 2019-11-04 00:24:24.429297: step 64880, total loss = 0.53, predict loss = 0.12 (69.5 examples/sec; 0.058 sec/batch; 94h:55m:39s remains)
INFO - root - 2019-11-04 00:24:25.034369: step 64890, total loss = 0.57, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 90h:48m:39s remains)
INFO - root - 2019-11-04 00:24:25.633776: step 64900, total loss = 0.57, predict loss = 0.14 (66.6 examples/sec; 0.060 sec/batch; 99h:04m:01s remains)
INFO - root - 2019-11-04 00:24:26.242294: step 64910, total loss = 0.58, predict loss = 0.14 (73.4 examples/sec; 0.054 sec/batch; 89h:47m:35s remains)
INFO - root - 2019-11-04 00:24:26.875532: step 64920, total loss = 0.54, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 93h:55m:30s remains)
INFO - root - 2019-11-04 00:24:27.503081: step 64930, total loss = 0.57, predict loss = 0.12 (77.0 examples/sec; 0.052 sec/batch; 85h:35m:27s remains)
INFO - root - 2019-11-04 00:24:28.180397: step 64940, total loss = 0.58, predict loss = 0.13 (81.1 examples/sec; 0.049 sec/batch; 81h:18m:21s remains)
INFO - root - 2019-11-04 00:24:28.829921: step 64950, total loss = 0.58, predict loss = 0.14 (79.7 examples/sec; 0.050 sec/batch; 82h:47m:07s remains)
INFO - root - 2019-11-04 00:24:29.451587: step 64960, total loss = 0.43, predict loss = 0.09 (66.4 examples/sec; 0.060 sec/batch; 99h:19m:45s remains)
INFO - root - 2019-11-04 00:24:30.058412: step 64970, total loss = 0.55, predict loss = 0.13 (67.8 examples/sec; 0.059 sec/batch; 97h:13m:38s remains)
INFO - root - 2019-11-04 00:24:30.695286: step 64980, total loss = 0.53, predict loss = 0.13 (75.8 examples/sec; 0.053 sec/batch; 87h:01m:52s remains)
INFO - root - 2019-11-04 00:24:31.313836: step 64990, total loss = 0.50, predict loss = 0.11 (69.4 examples/sec; 0.058 sec/batch; 95h:04m:22s remains)
INFO - root - 2019-11-04 00:24:32.002701: step 65000, total loss = 0.50, predict loss = 0.12 (74.0 examples/sec; 0.054 sec/batch; 89h:04m:40s remains)
INFO - root - 2019-11-04 00:24:32.685842: step 65010, total loss = 0.44, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 94h:33m:46s remains)
INFO - root - 2019-11-04 00:24:33.344450: step 65020, total loss = 0.45, predict loss = 0.10 (70.7 examples/sec; 0.057 sec/batch; 93h:13m:26s remains)
INFO - root - 2019-11-04 00:24:33.979980: step 65030, total loss = 0.47, predict loss = 0.11 (77.0 examples/sec; 0.052 sec/batch; 85h:40m:48s remains)
INFO - root - 2019-11-04 00:24:34.628704: step 65040, total loss = 0.50, predict loss = 0.11 (64.5 examples/sec; 0.062 sec/batch; 102h:10m:29s remains)
INFO - root - 2019-11-04 00:24:35.336798: step 65050, total loss = 0.59, predict loss = 0.14 (86.4 examples/sec; 0.046 sec/batch; 76h:21m:05s remains)
INFO - root - 2019-11-04 00:24:35.975272: step 65060, total loss = 0.57, predict loss = 0.14 (69.9 examples/sec; 0.057 sec/batch; 94h:21m:26s remains)
INFO - root - 2019-11-04 00:24:36.612193: step 65070, total loss = 0.53, predict loss = 0.12 (67.2 examples/sec; 0.059 sec/batch; 98h:04m:04s remains)
INFO - root - 2019-11-04 00:24:37.273598: step 65080, total loss = 0.63, predict loss = 0.15 (65.3 examples/sec; 0.061 sec/batch; 101h:02m:10s remains)
INFO - root - 2019-11-04 00:24:37.925345: step 65090, total loss = 0.54, predict loss = 0.13 (59.7 examples/sec; 0.067 sec/batch; 110h:29m:52s remains)
INFO - root - 2019-11-04 00:24:38.616029: step 65100, total loss = 0.63, predict loss = 0.15 (63.9 examples/sec; 0.063 sec/batch; 103h:14m:02s remains)
INFO - root - 2019-11-04 00:24:39.237621: step 65110, total loss = 0.51, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 89h:16m:06s remains)
INFO - root - 2019-11-04 00:24:39.863836: step 65120, total loss = 0.55, predict loss = 0.12 (78.3 examples/sec; 0.051 sec/batch; 84h:15m:14s remains)
INFO - root - 2019-11-04 00:24:40.476944: step 65130, total loss = 0.54, predict loss = 0.13 (75.6 examples/sec; 0.053 sec/batch; 87h:16m:05s remains)
INFO - root - 2019-11-04 00:24:41.106579: step 65140, total loss = 0.56, predict loss = 0.14 (87.6 examples/sec; 0.046 sec/batch; 75h:15m:43s remains)
INFO - root - 2019-11-04 00:24:41.747591: step 65150, total loss = 0.53, predict loss = 0.13 (71.3 examples/sec; 0.056 sec/batch; 92h:32m:25s remains)
INFO - root - 2019-11-04 00:24:42.402456: step 65160, total loss = 0.73, predict loss = 0.18 (73.4 examples/sec; 0.055 sec/batch; 89h:51m:13s remains)
INFO - root - 2019-11-04 00:24:43.069089: step 65170, total loss = 0.56, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 93h:45m:53s remains)
INFO - root - 2019-11-04 00:24:43.719832: step 65180, total loss = 0.67, predict loss = 0.17 (73.4 examples/sec; 0.054 sec/batch; 89h:48m:02s remains)
INFO - root - 2019-11-04 00:24:44.317482: step 65190, total loss = 0.50, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 91h:59m:54s remains)
INFO - root - 2019-11-04 00:24:44.952245: step 65200, total loss = 0.66, predict loss = 0.15 (67.3 examples/sec; 0.059 sec/batch; 98h:03m:00s remains)
INFO - root - 2019-11-04 00:24:45.584309: step 65210, total loss = 0.66, predict loss = 0.14 (68.1 examples/sec; 0.059 sec/batch; 96h:51m:14s remains)
INFO - root - 2019-11-04 00:24:46.236145: step 65220, total loss = 0.75, predict loss = 0.18 (71.8 examples/sec; 0.056 sec/batch; 91h:51m:05s remains)
INFO - root - 2019-11-04 00:24:46.923629: step 65230, total loss = 0.85, predict loss = 0.20 (59.1 examples/sec; 0.068 sec/batch; 111h:32m:24s remains)
INFO - root - 2019-11-04 00:24:47.556777: step 65240, total loss = 0.71, predict loss = 0.17 (74.7 examples/sec; 0.054 sec/batch; 88h:19m:10s remains)
INFO - root - 2019-11-04 00:24:48.250259: step 65250, total loss = 0.59, predict loss = 0.14 (63.8 examples/sec; 0.063 sec/batch; 103h:26m:16s remains)
INFO - root - 2019-11-04 00:24:48.874561: step 65260, total loss = 0.74, predict loss = 0.18 (71.3 examples/sec; 0.056 sec/batch; 92h:25m:28s remains)
INFO - root - 2019-11-04 00:24:49.508715: step 65270, total loss = 0.86, predict loss = 0.20 (65.8 examples/sec; 0.061 sec/batch; 100h:14m:14s remains)
INFO - root - 2019-11-04 00:24:50.190511: step 65280, total loss = 0.65, predict loss = 0.16 (71.1 examples/sec; 0.056 sec/batch; 92h:41m:36s remains)
INFO - root - 2019-11-04 00:24:50.826685: step 65290, total loss = 0.73, predict loss = 0.17 (70.2 examples/sec; 0.057 sec/batch; 93h:53m:19s remains)
INFO - root - 2019-11-04 00:24:51.470616: step 65300, total loss = 0.49, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 99h:56m:59s remains)
INFO - root - 2019-11-04 00:24:52.072024: step 65310, total loss = 0.64, predict loss = 0.15 (71.9 examples/sec; 0.056 sec/batch; 91h:39m:39s remains)
INFO - root - 2019-11-04 00:24:52.681906: step 65320, total loss = 0.55, predict loss = 0.12 (70.7 examples/sec; 0.057 sec/batch; 93h:18m:24s remains)
INFO - root - 2019-11-04 00:24:53.304849: step 65330, total loss = 0.40, predict loss = 0.09 (69.4 examples/sec; 0.058 sec/batch; 95h:00m:40s remains)
INFO - root - 2019-11-04 00:24:53.922150: step 65340, total loss = 0.54, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 98h:12m:40s remains)
INFO - root - 2019-11-04 00:24:54.547527: step 65350, total loss = 0.52, predict loss = 0.12 (78.8 examples/sec; 0.051 sec/batch; 83h:41m:02s remains)
INFO - root - 2019-11-04 00:24:55.195829: step 65360, total loss = 0.45, predict loss = 0.10 (68.0 examples/sec; 0.059 sec/batch; 96h:59m:34s remains)
INFO - root - 2019-11-04 00:24:55.854627: step 65370, total loss = 0.60, predict loss = 0.15 (69.9 examples/sec; 0.057 sec/batch; 94h:20m:44s remains)
INFO - root - 2019-11-04 00:24:56.552777: step 65380, total loss = 0.58, predict loss = 0.14 (64.0 examples/sec; 0.063 sec/batch; 103h:04m:22s remains)
INFO - root - 2019-11-04 00:24:57.207849: step 65390, total loss = 0.53, predict loss = 0.12 (75.1 examples/sec; 0.053 sec/batch; 87h:49m:01s remains)
INFO - root - 2019-11-04 00:24:57.861759: step 65400, total loss = 0.49, predict loss = 0.12 (65.6 examples/sec; 0.061 sec/batch; 100h:32m:54s remains)
INFO - root - 2019-11-04 00:24:58.516482: step 65410, total loss = 0.55, predict loss = 0.12 (63.7 examples/sec; 0.063 sec/batch; 103h:29m:13s remains)
INFO - root - 2019-11-04 00:24:59.185179: step 65420, total loss = 0.50, predict loss = 0.11 (64.4 examples/sec; 0.062 sec/batch; 102h:20m:09s remains)
INFO - root - 2019-11-04 00:24:59.879812: step 65430, total loss = 0.50, predict loss = 0.11 (59.5 examples/sec; 0.067 sec/batch; 110h:49m:05s remains)
INFO - root - 2019-11-04 00:25:00.464009: step 65440, total loss = 0.71, predict loss = 0.17 (100.5 examples/sec; 0.040 sec/batch; 65h:36m:40s remains)
INFO - root - 2019-11-04 00:25:00.927761: step 65450, total loss = 0.45, predict loss = 0.10 (96.5 examples/sec; 0.041 sec/batch; 68h:18m:22s remains)
INFO - root - 2019-11-04 00:25:01.976949: step 65460, total loss = 0.61, predict loss = 0.13 (70.4 examples/sec; 0.057 sec/batch; 93h:40m:36s remains)
INFO - root - 2019-11-04 00:25:02.588255: step 65470, total loss = 0.54, predict loss = 0.13 (83.0 examples/sec; 0.048 sec/batch; 79h:24m:45s remains)
INFO - root - 2019-11-04 00:25:03.199198: step 65480, total loss = 0.47, predict loss = 0.11 (69.4 examples/sec; 0.058 sec/batch; 95h:00m:16s remains)
INFO - root - 2019-11-04 00:25:03.827506: step 65490, total loss = 0.65, predict loss = 0.15 (68.1 examples/sec; 0.059 sec/batch; 96h:52m:01s remains)
INFO - root - 2019-11-04 00:25:04.484894: step 65500, total loss = 0.61, predict loss = 0.14 (69.4 examples/sec; 0.058 sec/batch; 95h:01m:36s remains)
INFO - root - 2019-11-04 00:25:05.205131: step 65510, total loss = 0.48, predict loss = 0.11 (71.2 examples/sec; 0.056 sec/batch; 92h:34m:15s remains)
INFO - root - 2019-11-04 00:25:05.794802: step 65520, total loss = 0.46, predict loss = 0.10 (72.1 examples/sec; 0.055 sec/batch; 91h:28m:58s remains)
INFO - root - 2019-11-04 00:25:06.508421: step 65530, total loss = 0.65, predict loss = 0.15 (57.5 examples/sec; 0.070 sec/batch; 114h:42m:24s remains)
INFO - root - 2019-11-04 00:25:07.204399: step 65540, total loss = 0.94, predict loss = 0.21 (70.6 examples/sec; 0.057 sec/batch; 93h:22m:34s remains)
INFO - root - 2019-11-04 00:25:07.813022: step 65550, total loss = 0.66, predict loss = 0.14 (68.0 examples/sec; 0.059 sec/batch; 96h:58m:01s remains)
INFO - root - 2019-11-04 00:25:08.470349: step 65560, total loss = 0.58, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 101h:15m:31s remains)
INFO - root - 2019-11-04 00:25:09.102196: step 65570, total loss = 0.66, predict loss = 0.15 (74.6 examples/sec; 0.054 sec/batch; 88h:26m:00s remains)
INFO - root - 2019-11-04 00:25:09.756808: step 65580, total loss = 0.58, predict loss = 0.13 (65.7 examples/sec; 0.061 sec/batch; 100h:17m:46s remains)
INFO - root - 2019-11-04 00:25:10.441011: step 65590, total loss = 0.66, predict loss = 0.15 (67.0 examples/sec; 0.060 sec/batch; 98h:22m:55s remains)
INFO - root - 2019-11-04 00:25:11.099620: step 65600, total loss = 0.41, predict loss = 0.09 (66.9 examples/sec; 0.060 sec/batch; 98h:30m:00s remains)
INFO - root - 2019-11-04 00:25:11.788136: step 65610, total loss = 0.59, predict loss = 0.14 (66.2 examples/sec; 0.060 sec/batch; 99h:39m:28s remains)
INFO - root - 2019-11-04 00:25:12.429482: step 65620, total loss = 0.71, predict loss = 0.17 (75.7 examples/sec; 0.053 sec/batch; 87h:08m:08s remains)
INFO - root - 2019-11-04 00:25:13.085315: step 65630, total loss = 0.49, predict loss = 0.11 (67.6 examples/sec; 0.059 sec/batch; 97h:30m:58s remains)
INFO - root - 2019-11-04 00:25:13.769967: step 65640, total loss = 0.43, predict loss = 0.09 (64.5 examples/sec; 0.062 sec/batch; 102h:13m:56s remains)
INFO - root - 2019-11-04 00:25:14.400497: step 65650, total loss = 0.49, predict loss = 0.11 (73.4 examples/sec; 0.055 sec/batch; 89h:51m:50s remains)
INFO - root - 2019-11-04 00:25:15.004020: step 65660, total loss = 0.46, predict loss = 0.10 (82.7 examples/sec; 0.048 sec/batch; 79h:44m:17s remains)
INFO - root - 2019-11-04 00:25:15.653497: step 65670, total loss = 0.42, predict loss = 0.09 (75.8 examples/sec; 0.053 sec/batch; 87h:02m:41s remains)
INFO - root - 2019-11-04 00:25:16.276363: step 65680, total loss = 0.49, predict loss = 0.11 (82.2 examples/sec; 0.049 sec/batch; 80h:11m:56s remains)
INFO - root - 2019-11-04 00:25:16.934362: step 65690, total loss = 0.58, predict loss = 0.13 (73.7 examples/sec; 0.054 sec/batch; 89h:31m:35s remains)
INFO - root - 2019-11-04 00:25:17.605739: step 65700, total loss = 0.37, predict loss = 0.07 (63.9 examples/sec; 0.063 sec/batch; 103h:12m:07s remains)
INFO - root - 2019-11-04 00:25:18.230579: step 65710, total loss = 0.49, predict loss = 0.12 (76.3 examples/sec; 0.052 sec/batch; 86h:25m:24s remains)
INFO - root - 2019-11-04 00:25:18.897478: step 65720, total loss = 0.63, predict loss = 0.14 (68.0 examples/sec; 0.059 sec/batch; 96h:59m:33s remains)
INFO - root - 2019-11-04 00:25:19.564022: step 65730, total loss = 0.69, predict loss = 0.16 (65.1 examples/sec; 0.061 sec/batch; 101h:19m:22s remains)
INFO - root - 2019-11-04 00:25:20.230595: step 65740, total loss = 0.69, predict loss = 0.16 (65.1 examples/sec; 0.061 sec/batch; 101h:13m:01s remains)
INFO - root - 2019-11-04 00:25:20.892613: step 65750, total loss = 0.63, predict loss = 0.15 (67.3 examples/sec; 0.059 sec/batch; 98h:02m:24s remains)
INFO - root - 2019-11-04 00:25:21.542694: step 65760, total loss = 0.57, predict loss = 0.13 (83.0 examples/sec; 0.048 sec/batch; 79h:28m:36s remains)
INFO - root - 2019-11-04 00:25:22.170197: step 65770, total loss = 0.74, predict loss = 0.17 (66.5 examples/sec; 0.060 sec/batch; 99h:09m:44s remains)
INFO - root - 2019-11-04 00:25:22.827269: step 65780, total loss = 0.71, predict loss = 0.17 (63.5 examples/sec; 0.063 sec/batch; 103h:52m:30s remains)
INFO - root - 2019-11-04 00:25:23.530407: step 65790, total loss = 0.56, predict loss = 0.13 (65.7 examples/sec; 0.061 sec/batch; 100h:23m:33s remains)
INFO - root - 2019-11-04 00:25:24.150280: step 65800, total loss = 0.64, predict loss = 0.15 (70.3 examples/sec; 0.057 sec/batch; 93h:47m:50s remains)
INFO - root - 2019-11-04 00:25:24.792715: step 65810, total loss = 0.64, predict loss = 0.14 (67.6 examples/sec; 0.059 sec/batch; 97h:35m:07s remains)
INFO - root - 2019-11-04 00:25:25.448026: step 65820, total loss = 0.66, predict loss = 0.16 (64.4 examples/sec; 0.062 sec/batch; 102h:21m:49s remains)
INFO - root - 2019-11-04 00:25:26.087507: step 65830, total loss = 0.63, predict loss = 0.15 (67.3 examples/sec; 0.059 sec/batch; 97h:58m:05s remains)
INFO - root - 2019-11-04 00:25:26.748003: step 65840, total loss = 0.49, predict loss = 0.11 (69.4 examples/sec; 0.058 sec/batch; 94h:59m:09s remains)
INFO - root - 2019-11-04 00:25:27.376077: step 65850, total loss = 0.55, predict loss = 0.12 (62.0 examples/sec; 0.065 sec/batch; 106h:21m:38s remains)
INFO - root - 2019-11-04 00:25:28.052722: step 65860, total loss = 0.50, predict loss = 0.11 (62.3 examples/sec; 0.064 sec/batch; 105h:45m:54s remains)
INFO - root - 2019-11-04 00:25:28.724267: step 65870, total loss = 0.40, predict loss = 0.08 (64.2 examples/sec; 0.062 sec/batch; 102h:41m:54s remains)
INFO - root - 2019-11-04 00:25:29.349141: step 65880, total loss = 0.65, predict loss = 0.15 (74.3 examples/sec; 0.054 sec/batch; 88h:42m:35s remains)
INFO - root - 2019-11-04 00:25:29.966243: step 65890, total loss = 0.61, predict loss = 0.14 (74.0 examples/sec; 0.054 sec/batch; 89h:04m:24s remains)
INFO - root - 2019-11-04 00:25:30.563426: step 65900, total loss = 0.50, predict loss = 0.11 (74.7 examples/sec; 0.054 sec/batch; 88h:15m:24s remains)
INFO - root - 2019-11-04 00:25:31.193177: step 65910, total loss = 0.54, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 102h:36m:57s remains)
INFO - root - 2019-11-04 00:25:31.838677: step 65920, total loss = 0.65, predict loss = 0.15 (68.3 examples/sec; 0.059 sec/batch; 96h:34m:24s remains)
INFO - root - 2019-11-04 00:25:32.460673: step 65930, total loss = 0.86, predict loss = 0.22 (80.8 examples/sec; 0.050 sec/batch; 81h:38m:53s remains)
INFO - root - 2019-11-04 00:25:33.090180: step 65940, total loss = 0.37, predict loss = 0.08 (71.7 examples/sec; 0.056 sec/batch; 91h:54m:31s remains)
INFO - root - 2019-11-04 00:25:33.720914: step 65950, total loss = 0.50, predict loss = 0.12 (66.4 examples/sec; 0.060 sec/batch; 99h:21m:28s remains)
INFO - root - 2019-11-04 00:25:34.425116: step 65960, total loss = 0.70, predict loss = 0.18 (59.3 examples/sec; 0.067 sec/batch; 111h:12m:35s remains)
INFO - root - 2019-11-04 00:25:35.179098: step 65970, total loss = 0.71, predict loss = 0.18 (74.3 examples/sec; 0.054 sec/batch; 88h:44m:43s remains)
INFO - root - 2019-11-04 00:25:35.802074: step 65980, total loss = 0.69, predict loss = 0.17 (69.0 examples/sec; 0.058 sec/batch; 95h:36m:24s remains)
INFO - root - 2019-11-04 00:25:36.428306: step 65990, total loss = 0.52, predict loss = 0.11 (75.8 examples/sec; 0.053 sec/batch; 86h:57m:30s remains)
INFO - root - 2019-11-04 00:25:37.060342: step 66000, total loss = 0.53, predict loss = 0.12 (66.7 examples/sec; 0.060 sec/batch; 98h:54m:29s remains)
INFO - root - 2019-11-04 00:25:37.710688: step 66010, total loss = 0.49, predict loss = 0.12 (62.4 examples/sec; 0.064 sec/batch; 105h:42m:25s remains)
INFO - root - 2019-11-04 00:25:38.342967: step 66020, total loss = 0.55, predict loss = 0.14 (74.1 examples/sec; 0.054 sec/batch; 89h:01m:59s remains)
INFO - root - 2019-11-04 00:25:39.027512: step 66030, total loss = 0.38, predict loss = 0.09 (66.3 examples/sec; 0.060 sec/batch; 99h:28m:37s remains)
INFO - root - 2019-11-04 00:25:39.693449: step 66040, total loss = 0.25, predict loss = 0.05 (68.6 examples/sec; 0.058 sec/batch; 96h:07m:27s remains)
INFO - root - 2019-11-04 00:25:40.350357: step 66050, total loss = 0.33, predict loss = 0.06 (75.7 examples/sec; 0.053 sec/batch; 87h:02m:44s remains)
INFO - root - 2019-11-04 00:25:40.995740: step 66060, total loss = 0.36, predict loss = 0.08 (65.1 examples/sec; 0.061 sec/batch; 101h:15m:10s remains)
INFO - root - 2019-11-04 00:25:41.616774: step 66070, total loss = 0.28, predict loss = 0.06 (71.8 examples/sec; 0.056 sec/batch; 91h:52m:35s remains)
INFO - root - 2019-11-04 00:25:42.283085: step 66080, total loss = 0.50, predict loss = 0.11 (70.8 examples/sec; 0.057 sec/batch; 93h:09m:11s remains)
INFO - root - 2019-11-04 00:25:42.920249: step 66090, total loss = 0.40, predict loss = 0.09 (63.8 examples/sec; 0.063 sec/batch; 103h:19m:07s remains)
INFO - root - 2019-11-04 00:25:43.573161: step 66100, total loss = 0.45, predict loss = 0.10 (63.5 examples/sec; 0.063 sec/batch; 103h:47m:21s remains)
INFO - root - 2019-11-04 00:25:44.200782: step 66110, total loss = 0.48, predict loss = 0.11 (70.0 examples/sec; 0.057 sec/batch; 94h:09m:55s remains)
INFO - root - 2019-11-04 00:25:44.836149: step 66120, total loss = 0.42, predict loss = 0.09 (67.2 examples/sec; 0.060 sec/batch; 98h:06m:41s remains)
INFO - root - 2019-11-04 00:25:45.510068: step 66130, total loss = 0.63, predict loss = 0.14 (65.5 examples/sec; 0.061 sec/batch; 100h:42m:55s remains)
INFO - root - 2019-11-04 00:25:46.127495: step 66140, total loss = 0.50, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 92h:23m:16s remains)
INFO - root - 2019-11-04 00:25:46.728080: step 66150, total loss = 0.85, predict loss = 0.20 (73.8 examples/sec; 0.054 sec/batch; 89h:22m:44s remains)
INFO - root - 2019-11-04 00:25:47.337457: step 66160, total loss = 0.49, predict loss = 0.11 (77.0 examples/sec; 0.052 sec/batch; 85h:37m:23s remains)
INFO - root - 2019-11-04 00:25:47.943365: step 66170, total loss = 0.85, predict loss = 0.20 (77.0 examples/sec; 0.052 sec/batch; 85h:38m:29s remains)
INFO - root - 2019-11-04 00:25:48.562697: step 66180, total loss = 0.61, predict loss = 0.15 (68.3 examples/sec; 0.059 sec/batch; 96h:30m:06s remains)
INFO - root - 2019-11-04 00:25:49.203389: step 66190, total loss = 0.66, predict loss = 0.16 (67.4 examples/sec; 0.059 sec/batch; 97h:50m:50s remains)
INFO - root - 2019-11-04 00:25:49.856920: step 66200, total loss = 0.60, predict loss = 0.14 (65.7 examples/sec; 0.061 sec/batch; 100h:19m:00s remains)
INFO - root - 2019-11-04 00:25:50.513809: step 66210, total loss = 0.48, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 97h:13m:11s remains)
INFO - root - 2019-11-04 00:25:51.148391: step 66220, total loss = 0.58, predict loss = 0.13 (65.2 examples/sec; 0.061 sec/batch; 101h:11m:45s remains)
INFO - root - 2019-11-04 00:25:51.763083: step 66230, total loss = 0.63, predict loss = 0.15 (67.2 examples/sec; 0.060 sec/batch; 98h:05m:53s remains)
INFO - root - 2019-11-04 00:25:52.374045: step 66240, total loss = 0.64, predict loss = 0.16 (75.5 examples/sec; 0.053 sec/batch; 87h:18m:59s remains)
INFO - root - 2019-11-04 00:25:52.997744: step 66250, total loss = 0.55, predict loss = 0.12 (78.0 examples/sec; 0.051 sec/batch; 84h:31m:52s remains)
INFO - root - 2019-11-04 00:25:53.624378: step 66260, total loss = 0.50, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 95h:05m:16s remains)
INFO - root - 2019-11-04 00:25:54.278934: step 66270, total loss = 0.53, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 93h:34m:42s remains)
INFO - root - 2019-11-04 00:25:54.923175: step 66280, total loss = 0.34, predict loss = 0.07 (61.7 examples/sec; 0.065 sec/batch; 106h:48m:54s remains)
INFO - root - 2019-11-04 00:25:55.574730: step 66290, total loss = 0.28, predict loss = 0.05 (66.2 examples/sec; 0.060 sec/batch; 99h:35m:10s remains)
INFO - root - 2019-11-04 00:25:56.245456: step 66300, total loss = 0.31, predict loss = 0.06 (67.1 examples/sec; 0.060 sec/batch; 98h:18m:21s remains)
INFO - root - 2019-11-04 00:25:56.876781: step 66310, total loss = 0.27, predict loss = 0.05 (70.2 examples/sec; 0.057 sec/batch; 93h:58m:47s remains)
INFO - root - 2019-11-04 00:25:57.544569: step 66320, total loss = 0.26, predict loss = 0.05 (59.5 examples/sec; 0.067 sec/batch; 110h:45m:18s remains)
INFO - root - 2019-11-04 00:25:58.173722: step 66330, total loss = 0.31, predict loss = 0.07 (65.4 examples/sec; 0.061 sec/batch; 100h:53m:09s remains)
INFO - root - 2019-11-04 00:25:59.328364: step 66340, total loss = 0.37, predict loss = 0.08 (73.7 examples/sec; 0.054 sec/batch; 89h:28m:58s remains)
INFO - root - 2019-11-04 00:25:59.990875: step 66350, total loss = 0.33, predict loss = 0.07 (62.5 examples/sec; 0.064 sec/batch; 105h:24m:39s remains)
INFO - root - 2019-11-04 00:26:00.667248: step 66360, total loss = 0.39, predict loss = 0.09 (65.7 examples/sec; 0.061 sec/batch; 100h:19m:33s remains)
INFO - root - 2019-11-04 00:26:01.321929: step 66370, total loss = 0.52, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 95h:41m:20s remains)
INFO - root - 2019-11-04 00:26:01.941864: step 66380, total loss = 0.39, predict loss = 0.09 (86.8 examples/sec; 0.046 sec/batch; 75h:56m:49s remains)
INFO - root - 2019-11-04 00:26:02.552366: step 66390, total loss = 0.67, predict loss = 0.16 (67.5 examples/sec; 0.059 sec/batch; 97h:41m:03s remains)
INFO - root - 2019-11-04 00:26:03.179019: step 66400, total loss = 0.40, predict loss = 0.09 (74.0 examples/sec; 0.054 sec/batch; 89h:08m:28s remains)
INFO - root - 2019-11-04 00:26:03.812327: step 66410, total loss = 0.44, predict loss = 0.10 (75.6 examples/sec; 0.053 sec/batch; 87h:13m:23s remains)
INFO - root - 2019-11-04 00:26:04.470943: step 66420, total loss = 0.59, predict loss = 0.14 (64.9 examples/sec; 0.062 sec/batch; 101h:38m:55s remains)
INFO - root - 2019-11-04 00:26:05.184234: step 66430, total loss = 0.45, predict loss = 0.10 (86.9 examples/sec; 0.046 sec/batch; 75h:53m:03s remains)
INFO - root - 2019-11-04 00:26:05.791023: step 66440, total loss = 0.58, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 93h:22m:25s remains)
INFO - root - 2019-11-04 00:26:06.433944: step 66450, total loss = 0.56, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 89h:11m:30s remains)
INFO - root - 2019-11-04 00:26:07.074492: step 66460, total loss = 0.59, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 90h:48m:47s remains)
INFO - root - 2019-11-04 00:26:07.701823: step 66470, total loss = 0.58, predict loss = 0.14 (73.7 examples/sec; 0.054 sec/batch; 89h:25m:06s remains)
INFO - root - 2019-11-04 00:26:08.378114: step 66480, total loss = 0.63, predict loss = 0.16 (60.5 examples/sec; 0.066 sec/batch; 108h:57m:54s remains)
INFO - root - 2019-11-04 00:26:09.013803: step 66490, total loss = 0.41, predict loss = 0.09 (72.3 examples/sec; 0.055 sec/batch; 91h:13m:42s remains)
INFO - root - 2019-11-04 00:26:09.640929: step 66500, total loss = 0.58, predict loss = 0.15 (72.8 examples/sec; 0.055 sec/batch; 90h:33m:21s remains)
INFO - root - 2019-11-04 00:26:10.265874: step 66510, total loss = 0.39, predict loss = 0.09 (70.7 examples/sec; 0.057 sec/batch; 93h:14m:36s remains)
INFO - root - 2019-11-04 00:26:10.948141: step 66520, total loss = 0.28, predict loss = 0.07 (62.1 examples/sec; 0.064 sec/batch; 106h:12m:19s remains)
INFO - root - 2019-11-04 00:26:11.601528: step 66530, total loss = 0.45, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 89h:09m:26s remains)
INFO - root - 2019-11-04 00:26:12.241786: step 66540, total loss = 0.23, predict loss = 0.05 (68.6 examples/sec; 0.058 sec/batch; 96h:07m:47s remains)
INFO - root - 2019-11-04 00:26:12.848334: step 66550, total loss = 0.35, predict loss = 0.08 (70.1 examples/sec; 0.057 sec/batch; 93h:59m:30s remains)
INFO - root - 2019-11-04 00:26:13.512196: step 66560, total loss = 0.23, predict loss = 0.04 (64.6 examples/sec; 0.062 sec/batch; 101h:58m:49s remains)
INFO - root - 2019-11-04 00:26:14.107995: step 66570, total loss = 0.41, predict loss = 0.10 (84.3 examples/sec; 0.047 sec/batch; 78h:13m:22s remains)
INFO - root - 2019-11-04 00:26:14.708662: step 66580, total loss = 0.39, predict loss = 0.08 (68.1 examples/sec; 0.059 sec/batch; 96h:44m:57s remains)
INFO - root - 2019-11-04 00:26:15.350875: step 66590, total loss = 0.52, predict loss = 0.13 (63.5 examples/sec; 0.063 sec/batch; 103h:48m:02s remains)
INFO - root - 2019-11-04 00:26:15.985816: step 66600, total loss = 0.43, predict loss = 0.10 (66.4 examples/sec; 0.060 sec/batch; 99h:20m:51s remains)
INFO - root - 2019-11-04 00:26:16.673104: step 66610, total loss = 0.60, predict loss = 0.14 (56.8 examples/sec; 0.070 sec/batch; 116h:04m:10s remains)
INFO - root - 2019-11-04 00:26:17.391593: step 66620, total loss = 0.53, predict loss = 0.12 (63.7 examples/sec; 0.063 sec/batch; 103h:28m:59s remains)
INFO - root - 2019-11-04 00:26:18.007213: step 66630, total loss = 0.50, predict loss = 0.11 (77.5 examples/sec; 0.052 sec/batch; 85h:00m:52s remains)
INFO - root - 2019-11-04 00:26:18.621128: step 66640, total loss = 0.45, predict loss = 0.11 (71.5 examples/sec; 0.056 sec/batch; 92h:15m:59s remains)
INFO - root - 2019-11-04 00:26:19.280142: step 66650, total loss = 0.43, predict loss = 0.09 (72.5 examples/sec; 0.055 sec/batch; 90h:57m:25s remains)
INFO - root - 2019-11-04 00:26:19.927070: step 66660, total loss = 0.50, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 96h:47m:42s remains)
INFO - root - 2019-11-04 00:26:20.579644: step 66670, total loss = 0.46, predict loss = 0.10 (70.1 examples/sec; 0.057 sec/batch; 94h:04m:24s remains)
INFO - root - 2019-11-04 00:26:21.264484: step 66680, total loss = 0.56, predict loss = 0.13 (61.6 examples/sec; 0.065 sec/batch; 106h:57m:23s remains)
INFO - root - 2019-11-04 00:26:21.898049: step 66690, total loss = 0.74, predict loss = 0.16 (75.9 examples/sec; 0.053 sec/batch; 86h:49m:49s remains)
INFO - root - 2019-11-04 00:26:22.571314: step 66700, total loss = 0.60, predict loss = 0.14 (65.8 examples/sec; 0.061 sec/batch; 100h:09m:28s remains)
INFO - root - 2019-11-04 00:26:23.235924: step 66710, total loss = 1.08, predict loss = 0.24 (76.4 examples/sec; 0.052 sec/batch; 86h:16m:38s remains)
INFO - root - 2019-11-04 00:26:23.923881: step 66720, total loss = 0.61, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 96h:45m:37s remains)
INFO - root - 2019-11-04 00:26:24.532168: step 66730, total loss = 0.79, predict loss = 0.18 (70.6 examples/sec; 0.057 sec/batch; 93h:22m:45s remains)
INFO - root - 2019-11-04 00:26:25.176123: step 66740, total loss = 0.66, predict loss = 0.14 (68.7 examples/sec; 0.058 sec/batch; 96h:00m:34s remains)
INFO - root - 2019-11-04 00:26:25.822101: step 66750, total loss = 0.78, predict loss = 0.19 (72.0 examples/sec; 0.056 sec/batch; 91h:33m:14s remains)
INFO - root - 2019-11-04 00:26:26.482756: step 66760, total loss = 0.66, predict loss = 0.13 (75.6 examples/sec; 0.053 sec/batch; 87h:11m:39s remains)
INFO - root - 2019-11-04 00:26:27.147502: step 66770, total loss = 0.82, predict loss = 0.19 (68.7 examples/sec; 0.058 sec/batch; 95h:58m:04s remains)
INFO - root - 2019-11-04 00:26:27.825038: step 66780, total loss = 0.65, predict loss = 0.13 (63.2 examples/sec; 0.063 sec/batch; 104h:17m:50s remains)
INFO - root - 2019-11-04 00:26:28.516276: step 66790, total loss = 0.29, predict loss = 0.06 (70.5 examples/sec; 0.057 sec/batch; 93h:34m:19s remains)
INFO - root - 2019-11-04 00:26:29.217049: step 66800, total loss = 0.43, predict loss = 0.09 (59.5 examples/sec; 0.067 sec/batch; 110h:42m:52s remains)
INFO - root - 2019-11-04 00:26:29.917702: step 66810, total loss = 0.42, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 92h:55m:24s remains)
INFO - root - 2019-11-04 00:26:30.601464: step 66820, total loss = 0.66, predict loss = 0.15 (71.4 examples/sec; 0.056 sec/batch; 92h:18m:07s remains)
INFO - root - 2019-11-04 00:26:31.225868: step 66830, total loss = 0.22, predict loss = 0.04 (79.5 examples/sec; 0.050 sec/batch; 82h:53m:16s remains)
INFO - root - 2019-11-04 00:26:31.853630: step 66840, total loss = 0.49, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 95h:39m:25s remains)
INFO - root - 2019-11-04 00:26:32.481470: step 66850, total loss = 0.64, predict loss = 0.15 (76.1 examples/sec; 0.053 sec/batch; 86h:40m:01s remains)
INFO - root - 2019-11-04 00:26:33.131949: step 66860, total loss = 0.45, predict loss = 0.10 (67.8 examples/sec; 0.059 sec/batch; 97h:10m:45s remains)
INFO - root - 2019-11-04 00:26:33.747902: step 66870, total loss = 0.76, predict loss = 0.18 (72.2 examples/sec; 0.055 sec/batch; 91h:19m:35s remains)
INFO - root - 2019-11-04 00:26:34.388474: step 66880, total loss = 1.13, predict loss = 0.27 (67.8 examples/sec; 0.059 sec/batch; 97h:09m:59s remains)
INFO - root - 2019-11-04 00:26:35.093729: step 66890, total loss = 0.89, predict loss = 0.21 (61.6 examples/sec; 0.065 sec/batch; 107h:05m:40s remains)
INFO - root - 2019-11-04 00:26:35.729183: step 66900, total loss = 1.07, predict loss = 0.25 (79.0 examples/sec; 0.051 sec/batch; 83h:28m:35s remains)
INFO - root - 2019-11-04 00:26:36.346588: step 66910, total loss = 1.03, predict loss = 0.24 (71.7 examples/sec; 0.056 sec/batch; 91h:59m:46s remains)
INFO - root - 2019-11-04 00:26:37.005840: step 66920, total loss = 0.90, predict loss = 0.21 (72.5 examples/sec; 0.055 sec/batch; 90h:57m:55s remains)
INFO - root - 2019-11-04 00:26:37.664967: step 66930, total loss = 0.72, predict loss = 0.16 (79.1 examples/sec; 0.051 sec/batch; 83h:22m:49s remains)
INFO - root - 2019-11-04 00:26:38.318129: step 66940, total loss = 0.81, predict loss = 0.20 (73.9 examples/sec; 0.054 sec/batch; 89h:12m:28s remains)
INFO - root - 2019-11-04 00:26:38.955252: step 66950, total loss = 0.93, predict loss = 0.22 (66.2 examples/sec; 0.060 sec/batch; 99h:36m:33s remains)
INFO - root - 2019-11-04 00:26:39.559190: step 66960, total loss = 0.77, predict loss = 0.18 (79.5 examples/sec; 0.050 sec/batch; 82h:53m:20s remains)
INFO - root - 2019-11-04 00:26:40.168514: step 66970, total loss = 0.70, predict loss = 0.16 (83.3 examples/sec; 0.048 sec/batch; 79h:10m:49s remains)
INFO - root - 2019-11-04 00:26:40.821891: step 66980, total loss = 0.76, predict loss = 0.18 (70.7 examples/sec; 0.057 sec/batch; 93h:16m:50s remains)
INFO - root - 2019-11-04 00:26:41.448890: step 66990, total loss = 0.47, predict loss = 0.10 (76.6 examples/sec; 0.052 sec/batch; 86h:00m:21s remains)
INFO - root - 2019-11-04 00:26:42.107426: step 67000, total loss = 0.65, predict loss = 0.15 (73.9 examples/sec; 0.054 sec/batch; 89h:12m:21s remains)
INFO - root - 2019-11-04 00:26:42.752978: step 67010, total loss = 0.49, predict loss = 0.11 (65.3 examples/sec; 0.061 sec/batch; 101h:01m:33s remains)
INFO - root - 2019-11-04 00:26:43.406892: step 67020, total loss = 0.58, predict loss = 0.14 (73.7 examples/sec; 0.054 sec/batch; 89h:28m:28s remains)
INFO - root - 2019-11-04 00:26:44.041082: step 67030, total loss = 0.55, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 99h:58m:03s remains)
INFO - root - 2019-11-04 00:26:44.692633: step 67040, total loss = 0.78, predict loss = 0.18 (75.0 examples/sec; 0.053 sec/batch; 87h:50m:46s remains)
INFO - root - 2019-11-04 00:26:45.314802: step 67050, total loss = 0.65, predict loss = 0.14 (76.1 examples/sec; 0.053 sec/batch; 86h:40m:13s remains)
INFO - root - 2019-11-04 00:26:45.965497: step 67060, total loss = 0.56, predict loss = 0.13 (71.3 examples/sec; 0.056 sec/batch; 92h:30m:12s remains)
INFO - root - 2019-11-04 00:26:46.653193: step 67070, total loss = 0.57, predict loss = 0.13 (58.6 examples/sec; 0.068 sec/batch; 112h:29m:59s remains)
INFO - root - 2019-11-04 00:26:47.332913: step 67080, total loss = 0.69, predict loss = 0.15 (64.6 examples/sec; 0.062 sec/batch; 102h:03m:17s remains)
INFO - root - 2019-11-04 00:26:47.954853: step 67090, total loss = 0.65, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 93h:25m:47s remains)
INFO - root - 2019-11-04 00:26:48.579227: step 67100, total loss = 0.63, predict loss = 0.14 (73.5 examples/sec; 0.054 sec/batch; 89h:40m:23s remains)
INFO - root - 2019-11-04 00:26:49.239650: step 67110, total loss = 0.65, predict loss = 0.15 (71.0 examples/sec; 0.056 sec/batch; 92h:54m:11s remains)
INFO - root - 2019-11-04 00:26:49.918303: step 67120, total loss = 0.65, predict loss = 0.15 (59.5 examples/sec; 0.067 sec/batch; 110h:45m:31s remains)
INFO - root - 2019-11-04 00:26:50.575984: step 67130, total loss = 0.55, predict loss = 0.13 (75.4 examples/sec; 0.053 sec/batch; 87h:27m:12s remains)
INFO - root - 2019-11-04 00:26:51.197380: step 67140, total loss = 0.44, predict loss = 0.10 (65.1 examples/sec; 0.061 sec/batch; 101h:17m:56s remains)
INFO - root - 2019-11-04 00:26:51.812806: step 67150, total loss = 0.53, predict loss = 0.12 (69.5 examples/sec; 0.058 sec/batch; 94h:53m:58s remains)
INFO - root - 2019-11-04 00:26:52.455988: step 67160, total loss = 0.43, predict loss = 0.10 (63.8 examples/sec; 0.063 sec/batch; 103h:20m:20s remains)
INFO - root - 2019-11-04 00:26:53.068983: step 67170, total loss = 0.45, predict loss = 0.10 (69.3 examples/sec; 0.058 sec/batch; 95h:07m:29s remains)
INFO - root - 2019-11-04 00:26:53.729168: step 67180, total loss = 0.43, predict loss = 0.09 (67.6 examples/sec; 0.059 sec/batch; 97h:32m:49s remains)
INFO - root - 2019-11-04 00:26:54.337852: step 67190, total loss = 0.43, predict loss = 0.10 (79.7 examples/sec; 0.050 sec/batch; 82h:41m:42s remains)
INFO - root - 2019-11-04 00:26:54.950898: step 67200, total loss = 0.43, predict loss = 0.09 (71.1 examples/sec; 0.056 sec/batch; 92h:44m:19s remains)
INFO - root - 2019-11-04 00:26:55.563956: step 67210, total loss = 0.52, predict loss = 0.11 (83.0 examples/sec; 0.048 sec/batch; 79h:26m:29s remains)
INFO - root - 2019-11-04 00:26:56.241202: step 67220, total loss = 0.48, predict loss = 0.11 (58.1 examples/sec; 0.069 sec/batch; 113h:24m:22s remains)
INFO - root - 2019-11-04 00:26:56.922859: step 67230, total loss = 0.51, predict loss = 0.12 (60.9 examples/sec; 0.066 sec/batch; 108h:18m:39s remains)
INFO - root - 2019-11-04 00:26:57.572576: step 67240, total loss = 0.58, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 91h:21m:46s remains)
INFO - root - 2019-11-04 00:26:58.178530: step 67250, total loss = 0.59, predict loss = 0.14 (78.4 examples/sec; 0.051 sec/batch; 84h:05m:39s remains)
INFO - root - 2019-11-04 00:26:58.812467: step 67260, total loss = 0.67, predict loss = 0.15 (67.5 examples/sec; 0.059 sec/batch; 97h:37m:14s remains)
INFO - root - 2019-11-04 00:26:59.441730: step 67270, total loss = 0.77, predict loss = 0.19 (71.8 examples/sec; 0.056 sec/batch; 91h:50m:21s remains)
INFO - root - 2019-11-04 00:27:00.102693: step 67280, total loss = 0.71, predict loss = 0.17 (68.0 examples/sec; 0.059 sec/batch; 97h:00m:07s remains)
INFO - root - 2019-11-04 00:27:00.760850: step 67290, total loss = 0.68, predict loss = 0.15 (66.6 examples/sec; 0.060 sec/batch; 98h:55m:23s remains)
INFO - root - 2019-11-04 00:27:01.396897: step 67300, total loss = 0.67, predict loss = 0.16 (65.8 examples/sec; 0.061 sec/batch; 100h:06m:31s remains)
INFO - root - 2019-11-04 00:27:02.073151: step 67310, total loss = 0.58, predict loss = 0.14 (63.6 examples/sec; 0.063 sec/batch; 103h:39m:16s remains)
INFO - root - 2019-11-04 00:27:02.737909: step 67320, total loss = 0.73, predict loss = 0.17 (63.1 examples/sec; 0.063 sec/batch; 104h:27m:44s remains)
INFO - root - 2019-11-04 00:27:03.471523: step 67330, total loss = 0.82, predict loss = 0.19 (63.6 examples/sec; 0.063 sec/batch; 103h:38m:18s remains)
INFO - root - 2019-11-04 00:27:04.156552: step 67340, total loss = 0.81, predict loss = 0.19 (62.5 examples/sec; 0.064 sec/batch; 105h:25m:50s remains)
INFO - root - 2019-11-04 00:27:04.850917: step 67350, total loss = 0.67, predict loss = 0.16 (56.8 examples/sec; 0.070 sec/batch; 116h:08m:04s remains)
INFO - root - 2019-11-04 00:27:05.534794: step 67360, total loss = 0.60, predict loss = 0.14 (74.9 examples/sec; 0.053 sec/batch; 87h:57m:30s remains)
INFO - root - 2019-11-04 00:27:06.159253: step 67370, total loss = 0.73, predict loss = 0.17 (76.5 examples/sec; 0.052 sec/batch; 86h:10m:31s remains)
INFO - root - 2019-11-04 00:27:06.821692: step 67380, total loss = 0.51, predict loss = 0.11 (68.8 examples/sec; 0.058 sec/batch; 95h:47m:49s remains)
INFO - root - 2019-11-04 00:27:07.470885: step 67390, total loss = 0.57, predict loss = 0.13 (62.8 examples/sec; 0.064 sec/batch; 104h:55m:34s remains)
INFO - root - 2019-11-04 00:27:08.099807: step 67400, total loss = 0.42, predict loss = 0.09 (67.2 examples/sec; 0.059 sec/batch; 98h:02m:32s remains)
INFO - root - 2019-11-04 00:27:08.757948: step 67410, total loss = 0.42, predict loss = 0.09 (70.7 examples/sec; 0.057 sec/batch; 93h:12m:06s remains)
INFO - root - 2019-11-04 00:27:09.392097: step 67420, total loss = 0.40, predict loss = 0.09 (73.6 examples/sec; 0.054 sec/batch; 89h:35m:21s remains)
INFO - root - 2019-11-04 00:27:10.016538: step 67430, total loss = 0.56, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 90h:15m:57s remains)
INFO - root - 2019-11-04 00:27:10.674921: step 67440, total loss = 0.52, predict loss = 0.12 (70.9 examples/sec; 0.056 sec/batch; 92h:59m:13s remains)
INFO - root - 2019-11-04 00:27:11.326413: step 67450, total loss = 0.48, predict loss = 0.10 (65.4 examples/sec; 0.061 sec/batch; 100h:45m:36s remains)
INFO - root - 2019-11-04 00:27:11.969107: step 67460, total loss = 0.52, predict loss = 0.10 (74.4 examples/sec; 0.054 sec/batch; 88h:35m:32s remains)
INFO - root - 2019-11-04 00:27:12.593231: step 67470, total loss = 0.54, predict loss = 0.13 (66.6 examples/sec; 0.060 sec/batch; 98h:57m:44s remains)
INFO - root - 2019-11-04 00:27:13.216431: step 67480, total loss = 0.46, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 96h:03m:15s remains)
INFO - root - 2019-11-04 00:27:13.928342: step 67490, total loss = 0.61, predict loss = 0.14 (67.7 examples/sec; 0.059 sec/batch; 97h:18m:29s remains)
INFO - root - 2019-11-04 00:27:14.580222: step 67500, total loss = 0.63, predict loss = 0.14 (76.8 examples/sec; 0.052 sec/batch; 85h:51m:24s remains)
INFO - root - 2019-11-04 00:27:15.200381: step 67510, total loss = 0.54, predict loss = 0.12 (80.5 examples/sec; 0.050 sec/batch; 81h:56m:03s remains)
INFO - root - 2019-11-04 00:27:15.876476: step 67520, total loss = 0.54, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 97h:15m:22s remains)
INFO - root - 2019-11-04 00:27:16.521108: step 67530, total loss = 0.61, predict loss = 0.15 (67.4 examples/sec; 0.059 sec/batch; 97h:51m:08s remains)
INFO - root - 2019-11-04 00:27:17.169907: step 67540, total loss = 0.62, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 91h:19m:27s remains)
INFO - root - 2019-11-04 00:27:17.817276: step 67550, total loss = 0.76, predict loss = 0.18 (67.9 examples/sec; 0.059 sec/batch; 97h:00m:40s remains)
INFO - root - 2019-11-04 00:27:18.453658: step 67560, total loss = 0.58, predict loss = 0.13 (71.5 examples/sec; 0.056 sec/batch; 92h:13m:38s remains)
INFO - root - 2019-11-04 00:27:19.090264: step 67570, total loss = 0.57, predict loss = 0.13 (66.6 examples/sec; 0.060 sec/batch; 98h:58m:33s remains)
INFO - root - 2019-11-04 00:27:19.710476: step 67580, total loss = 0.64, predict loss = 0.15 (65.8 examples/sec; 0.061 sec/batch; 100h:09m:58s remains)
INFO - root - 2019-11-04 00:27:20.338564: step 67590, total loss = 0.62, predict loss = 0.16 (72.9 examples/sec; 0.055 sec/batch; 90h:22m:22s remains)
INFO - root - 2019-11-04 00:27:20.998986: step 67600, total loss = 0.65, predict loss = 0.16 (73.1 examples/sec; 0.055 sec/batch; 90h:08m:00s remains)
INFO - root - 2019-11-04 00:27:21.657145: step 67610, total loss = 0.63, predict loss = 0.16 (81.3 examples/sec; 0.049 sec/batch; 81h:03m:06s remains)
INFO - root - 2019-11-04 00:27:22.306220: step 67620, total loss = 0.59, predict loss = 0.14 (72.1 examples/sec; 0.056 sec/batch; 91h:27m:58s remains)
INFO - root - 2019-11-04 00:27:22.947330: step 67630, total loss = 0.57, predict loss = 0.13 (76.4 examples/sec; 0.052 sec/batch; 86h:19m:47s remains)
INFO - root - 2019-11-04 00:27:23.561975: step 67640, total loss = 0.58, predict loss = 0.14 (76.3 examples/sec; 0.052 sec/batch; 86h:20m:19s remains)
INFO - root - 2019-11-04 00:27:24.183443: step 67650, total loss = 0.53, predict loss = 0.11 (74.8 examples/sec; 0.054 sec/batch; 88h:10m:14s remains)
INFO - root - 2019-11-04 00:27:24.845788: step 67660, total loss = 0.56, predict loss = 0.12 (64.2 examples/sec; 0.062 sec/batch; 102h:40m:25s remains)
INFO - root - 2019-11-04 00:27:25.471563: step 67670, total loss = 0.52, predict loss = 0.12 (68.4 examples/sec; 0.058 sec/batch; 96h:19m:31s remains)
INFO - root - 2019-11-04 00:27:26.118777: step 67680, total loss = 0.51, predict loss = 0.11 (68.9 examples/sec; 0.058 sec/batch; 95h:41m:25s remains)
INFO - root - 2019-11-04 00:27:26.803056: step 67690, total loss = 0.52, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 98h:16m:38s remains)
INFO - root - 2019-11-04 00:27:27.470237: step 67700, total loss = 0.53, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 92h:49m:51s remains)
INFO - root - 2019-11-04 00:27:28.127271: step 67710, total loss = 0.49, predict loss = 0.11 (62.8 examples/sec; 0.064 sec/batch; 105h:01m:55s remains)
INFO - root - 2019-11-04 00:27:28.778792: step 67720, total loss = 0.50, predict loss = 0.11 (72.5 examples/sec; 0.055 sec/batch; 90h:53m:40s remains)
INFO - root - 2019-11-04 00:27:29.446628: step 67730, total loss = 0.52, predict loss = 0.12 (61.4 examples/sec; 0.065 sec/batch; 107h:19m:41s remains)
INFO - root - 2019-11-04 00:27:30.085077: step 67740, total loss = 0.41, predict loss = 0.09 (66.9 examples/sec; 0.060 sec/batch; 98h:29m:16s remains)
INFO - root - 2019-11-04 00:27:30.707315: step 67750, total loss = 0.60, predict loss = 0.13 (72.4 examples/sec; 0.055 sec/batch; 91h:01m:12s remains)
INFO - root - 2019-11-04 00:27:31.339299: step 67760, total loss = 0.38, predict loss = 0.07 (73.3 examples/sec; 0.055 sec/batch; 89h:54m:35s remains)
INFO - root - 2019-11-04 00:27:31.969572: step 67770, total loss = 0.48, predict loss = 0.10 (63.3 examples/sec; 0.063 sec/batch; 104h:02m:48s remains)
INFO - root - 2019-11-04 00:27:32.618610: step 67780, total loss = 0.51, predict loss = 0.11 (72.4 examples/sec; 0.055 sec/batch; 91h:03m:38s remains)
INFO - root - 2019-11-04 00:27:33.245791: step 67790, total loss = 0.50, predict loss = 0.11 (67.5 examples/sec; 0.059 sec/batch; 97h:35m:21s remains)
INFO - root - 2019-11-04 00:27:33.894601: step 67800, total loss = 0.56, predict loss = 0.13 (72.8 examples/sec; 0.055 sec/batch; 90h:30m:32s remains)
INFO - root - 2019-11-04 00:27:34.512687: step 67810, total loss = 0.48, predict loss = 0.11 (75.1 examples/sec; 0.053 sec/batch; 87h:49m:29s remains)
INFO - root - 2019-11-04 00:27:35.229110: step 67820, total loss = 0.50, predict loss = 0.11 (75.1 examples/sec; 0.053 sec/batch; 87h:42m:34s remains)
INFO - root - 2019-11-04 00:27:35.854042: step 67830, total loss = 0.66, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 91h:18m:04s remains)
INFO - root - 2019-11-04 00:27:36.509760: step 67840, total loss = 0.58, predict loss = 0.14 (69.7 examples/sec; 0.057 sec/batch; 94h:38m:01s remains)
INFO - root - 2019-11-04 00:27:37.177978: step 67850, total loss = 0.52, predict loss = 0.09 (70.1 examples/sec; 0.057 sec/batch; 94h:04m:30s remains)
INFO - root - 2019-11-04 00:27:37.820115: step 67860, total loss = 0.52, predict loss = 0.12 (79.8 examples/sec; 0.050 sec/batch; 82h:32m:51s remains)
INFO - root - 2019-11-04 00:27:38.467704: step 67870, total loss = 0.63, predict loss = 0.16 (73.3 examples/sec; 0.055 sec/batch; 89h:55m:24s remains)
INFO - root - 2019-11-04 00:27:39.143353: step 67880, total loss = 0.63, predict loss = 0.15 (63.4 examples/sec; 0.063 sec/batch; 103h:56m:37s remains)
INFO - root - 2019-11-04 00:27:39.756154: step 67890, total loss = 0.87, predict loss = 0.20 (71.0 examples/sec; 0.056 sec/batch; 92h:53m:14s remains)
INFO - root - 2019-11-04 00:27:40.368619: step 67900, total loss = 0.85, predict loss = 0.20 (66.7 examples/sec; 0.060 sec/batch; 98h:51m:27s remains)
INFO - root - 2019-11-04 00:27:41.055590: step 67910, total loss = 0.45, predict loss = 0.10 (60.3 examples/sec; 0.066 sec/batch; 109h:21m:21s remains)
INFO - root - 2019-11-04 00:27:41.725410: step 67920, total loss = 0.83, predict loss = 0.20 (65.5 examples/sec; 0.061 sec/batch; 100h:36m:19s remains)
INFO - root - 2019-11-04 00:27:42.371177: step 67930, total loss = 0.63, predict loss = 0.15 (73.5 examples/sec; 0.054 sec/batch; 89h:43m:45s remains)
INFO - root - 2019-11-04 00:27:43.021122: step 67940, total loss = 0.80, predict loss = 0.19 (72.1 examples/sec; 0.055 sec/batch; 91h:25m:26s remains)
INFO - root - 2019-11-04 00:27:43.674759: step 67950, total loss = 0.81, predict loss = 0.18 (65.2 examples/sec; 0.061 sec/batch; 101h:04m:07s remains)
INFO - root - 2019-11-04 00:27:44.301247: step 67960, total loss = 0.61, predict loss = 0.14 (77.7 examples/sec; 0.051 sec/batch; 84h:47m:24s remains)
INFO - root - 2019-11-04 00:27:44.911986: step 67970, total loss = 0.68, predict loss = 0.16 (67.0 examples/sec; 0.060 sec/batch; 98h:26m:55s remains)
INFO - root - 2019-11-04 00:27:45.556247: step 67980, total loss = 0.72, predict loss = 0.17 (71.1 examples/sec; 0.056 sec/batch; 92h:44m:12s remains)
INFO - root - 2019-11-04 00:27:46.237353: step 67990, total loss = 0.67, predict loss = 0.14 (62.2 examples/sec; 0.064 sec/batch; 106h:02m:14s remains)
INFO - root - 2019-11-04 00:27:46.899969: step 68000, total loss = 0.75, predict loss = 0.18 (71.3 examples/sec; 0.056 sec/batch; 92h:29m:36s remains)
INFO - root - 2019-11-04 00:27:47.530085: step 68010, total loss = 0.46, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 94h:25m:51s remains)
INFO - root - 2019-11-04 00:27:48.215709: step 68020, total loss = 0.61, predict loss = 0.14 (71.1 examples/sec; 0.056 sec/batch; 92h:44m:11s remains)
INFO - root - 2019-11-04 00:27:48.902959: step 68030, total loss = 0.52, predict loss = 0.12 (58.1 examples/sec; 0.069 sec/batch; 113h:25m:03s remains)
INFO - root - 2019-11-04 00:27:49.601512: step 68040, total loss = 0.63, predict loss = 0.15 (64.9 examples/sec; 0.062 sec/batch; 101h:32m:59s remains)
INFO - root - 2019-11-04 00:27:50.275007: step 68050, total loss = 0.47, predict loss = 0.10 (71.7 examples/sec; 0.056 sec/batch; 91h:58m:37s remains)
INFO - root - 2019-11-04 00:27:50.891073: step 68060, total loss = 0.49, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 97h:49m:33s remains)
INFO - root - 2019-11-04 00:27:51.527035: step 68070, total loss = 0.63, predict loss = 0.15 (69.2 examples/sec; 0.058 sec/batch; 95h:12m:08s remains)
INFO - root - 2019-11-04 00:27:52.153360: step 68080, total loss = 0.68, predict loss = 0.16 (68.2 examples/sec; 0.059 sec/batch; 96h:36m:53s remains)
INFO - root - 2019-11-04 00:27:52.767909: step 68090, total loss = 0.52, predict loss = 0.11 (64.3 examples/sec; 0.062 sec/batch; 102h:29m:36s remains)
INFO - root - 2019-11-04 00:27:53.389572: step 68100, total loss = 0.51, predict loss = 0.12 (71.2 examples/sec; 0.056 sec/batch; 92h:34m:46s remains)
INFO - root - 2019-11-04 00:27:54.131181: step 68110, total loss = 0.59, predict loss = 0.13 (54.6 examples/sec; 0.073 sec/batch; 120h:47m:12s remains)
INFO - root - 2019-11-04 00:27:54.837960: step 68120, total loss = 0.53, predict loss = 0.13 (68.2 examples/sec; 0.059 sec/batch; 96h:40m:36s remains)
INFO - root - 2019-11-04 00:27:55.531516: step 68130, total loss = 0.44, predict loss = 0.10 (67.1 examples/sec; 0.060 sec/batch; 98h:13m:49s remains)
INFO - root - 2019-11-04 00:27:56.275832: step 68140, total loss = 0.52, predict loss = 0.11 (56.5 examples/sec; 0.071 sec/batch; 116h:45m:12s remains)
INFO - root - 2019-11-04 00:27:57.057668: step 68150, total loss = 0.39, predict loss = 0.07 (56.1 examples/sec; 0.071 sec/batch; 117h:24m:59s remains)
INFO - root - 2019-11-04 00:27:57.841448: step 68160, total loss = 0.43, predict loss = 0.10 (76.9 examples/sec; 0.052 sec/batch; 85h:40m:37s remains)
INFO - root - 2019-11-04 00:27:58.355174: step 68170, total loss = 0.69, predict loss = 0.15 (93.2 examples/sec; 0.043 sec/batch; 70h:43m:41s remains)
INFO - root - 2019-11-04 00:27:58.877621: step 68180, total loss = 0.44, predict loss = 0.11 (85.0 examples/sec; 0.047 sec/batch; 77h:33m:54s remains)
INFO - root - 2019-11-04 00:28:00.309987: step 68190, total loss = 0.42, predict loss = 0.09 (58.2 examples/sec; 0.069 sec/batch; 113h:16m:44s remains)
INFO - root - 2019-11-04 00:28:01.078203: step 68200, total loss = 0.55, predict loss = 0.13 (56.9 examples/sec; 0.070 sec/batch; 115h:55m:35s remains)
INFO - root - 2019-11-04 00:28:01.849516: step 68210, total loss = 0.49, predict loss = 0.12 (62.2 examples/sec; 0.064 sec/batch; 105h:53m:01s remains)
INFO - root - 2019-11-04 00:28:02.639790: step 68220, total loss = 0.60, predict loss = 0.14 (65.4 examples/sec; 0.061 sec/batch; 100h:50m:31s remains)
INFO - root - 2019-11-04 00:28:03.392158: step 68230, total loss = 0.54, predict loss = 0.12 (52.5 examples/sec; 0.076 sec/batch; 125h:30m:42s remains)
INFO - root - 2019-11-04 00:28:04.163953: step 68240, total loss = 0.42, predict loss = 0.09 (72.0 examples/sec; 0.056 sec/batch; 91h:33m:07s remains)
INFO - root - 2019-11-04 00:28:04.937250: step 68250, total loss = 0.58, predict loss = 0.13 (53.8 examples/sec; 0.074 sec/batch; 122h:34m:28s remains)
INFO - root - 2019-11-04 00:28:05.581190: step 68260, total loss = 0.48, predict loss = 0.09 (77.3 examples/sec; 0.052 sec/batch; 85h:16m:29s remains)
INFO - root - 2019-11-04 00:28:06.201099: step 68270, total loss = 1.00, predict loss = 0.25 (74.7 examples/sec; 0.054 sec/batch; 88h:17m:05s remains)
INFO - root - 2019-11-04 00:28:06.815644: step 68280, total loss = 0.61, predict loss = 0.13 (74.1 examples/sec; 0.054 sec/batch; 88h:56m:58s remains)
INFO - root - 2019-11-04 00:28:07.410273: step 68290, total loss = 0.67, predict loss = 0.15 (85.9 examples/sec; 0.047 sec/batch; 76h:42m:22s remains)
INFO - root - 2019-11-04 00:28:08.018235: step 68300, total loss = 1.05, predict loss = 0.29 (77.1 examples/sec; 0.052 sec/batch; 85h:27m:33s remains)
INFO - root - 2019-11-04 00:28:08.654485: step 68310, total loss = 0.90, predict loss = 0.21 (69.0 examples/sec; 0.058 sec/batch; 95h:27m:09s remains)
INFO - root - 2019-11-04 00:28:09.299512: step 68320, total loss = 0.57, predict loss = 0.12 (69.6 examples/sec; 0.057 sec/batch; 94h:43m:51s remains)
INFO - root - 2019-11-04 00:28:09.969217: step 68330, total loss = 0.63, predict loss = 0.12 (66.7 examples/sec; 0.060 sec/batch; 98h:47m:22s remains)
INFO - root - 2019-11-04 00:28:10.585612: step 68340, total loss = 0.67, predict loss = 0.15 (78.6 examples/sec; 0.051 sec/batch; 83h:54m:04s remains)
INFO - root - 2019-11-04 00:28:11.200487: step 68350, total loss = 0.66, predict loss = 0.16 (76.3 examples/sec; 0.052 sec/batch; 86h:19m:32s remains)
INFO - root - 2019-11-04 00:28:11.830052: step 68360, total loss = 0.51, predict loss = 0.12 (78.1 examples/sec; 0.051 sec/batch; 84h:22m:26s remains)
INFO - root - 2019-11-04 00:28:12.452724: step 68370, total loss = 0.79, predict loss = 0.19 (77.6 examples/sec; 0.052 sec/batch; 84h:55m:14s remains)
INFO - root - 2019-11-04 00:28:13.116047: step 68380, total loss = 0.72, predict loss = 0.17 (69.1 examples/sec; 0.058 sec/batch; 95h:20m:59s remains)
INFO - root - 2019-11-04 00:28:13.786154: step 68390, total loss = 0.47, predict loss = 0.09 (71.4 examples/sec; 0.056 sec/batch; 92h:15m:10s remains)
INFO - root - 2019-11-04 00:28:14.464089: step 68400, total loss = 0.63, predict loss = 0.14 (69.7 examples/sec; 0.057 sec/batch; 94h:34m:15s remains)
INFO - root - 2019-11-04 00:28:15.138094: step 68410, total loss = 0.41, predict loss = 0.09 (66.9 examples/sec; 0.060 sec/batch; 98h:32m:50s remains)
INFO - root - 2019-11-04 00:28:15.810203: step 68420, total loss = 0.56, predict loss = 0.13 (68.0 examples/sec; 0.059 sec/batch; 96h:54m:48s remains)
INFO - root - 2019-11-04 00:28:16.466448: step 68430, total loss = 0.61, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 93h:42m:47s remains)
INFO - root - 2019-11-04 00:28:17.136975: step 68440, total loss = 0.67, predict loss = 0.13 (58.8 examples/sec; 0.068 sec/batch; 112h:05m:03s remains)
INFO - root - 2019-11-04 00:28:17.778409: step 68450, total loss = 0.74, predict loss = 0.18 (66.0 examples/sec; 0.061 sec/batch; 99h:49m:06s remains)
INFO - root - 2019-11-04 00:28:18.439408: step 68460, total loss = 0.58, predict loss = 0.14 (78.4 examples/sec; 0.051 sec/batch; 84h:03m:08s remains)
INFO - root - 2019-11-04 00:28:19.087385: step 68470, total loss = 0.70, predict loss = 0.17 (77.5 examples/sec; 0.052 sec/batch; 85h:05m:22s remains)
INFO - root - 2019-11-04 00:28:19.700575: step 68480, total loss = 0.57, predict loss = 0.13 (78.5 examples/sec; 0.051 sec/batch; 83h:56m:10s remains)
INFO - root - 2019-11-04 00:28:20.312958: step 68490, total loss = 0.67, predict loss = 0.15 (69.0 examples/sec; 0.058 sec/batch; 95h:33m:31s remains)
INFO - root - 2019-11-04 00:28:20.927607: step 68500, total loss = 0.58, predict loss = 0.13 (68.3 examples/sec; 0.059 sec/batch; 96h:33m:43s remains)
INFO - root - 2019-11-04 00:28:21.605495: step 68510, total loss = 0.64, predict loss = 0.15 (65.4 examples/sec; 0.061 sec/batch; 100h:50m:32s remains)
INFO - root - 2019-11-04 00:28:22.301316: step 68520, total loss = 0.64, predict loss = 0.13 (63.4 examples/sec; 0.063 sec/batch; 103h:54m:22s remains)
INFO - root - 2019-11-04 00:28:22.970675: step 68530, total loss = 0.63, predict loss = 0.14 (73.2 examples/sec; 0.055 sec/batch; 90h:01m:03s remains)
INFO - root - 2019-11-04 00:28:23.655703: step 68540, total loss = 0.54, predict loss = 0.12 (65.5 examples/sec; 0.061 sec/batch; 100h:34m:52s remains)
INFO - root - 2019-11-04 00:28:24.320533: step 68550, total loss = 0.52, predict loss = 0.12 (60.2 examples/sec; 0.066 sec/batch; 109h:24m:25s remains)
INFO - root - 2019-11-04 00:28:24.973084: step 68560, total loss = 0.44, predict loss = 0.09 (63.4 examples/sec; 0.063 sec/batch; 103h:56m:16s remains)
INFO - root - 2019-11-04 00:28:25.597509: step 68570, total loss = 0.52, predict loss = 0.12 (74.1 examples/sec; 0.054 sec/batch; 88h:54m:10s remains)
INFO - root - 2019-11-04 00:28:26.262477: step 68580, total loss = 0.55, predict loss = 0.13 (61.5 examples/sec; 0.065 sec/batch; 107h:05m:00s remains)
INFO - root - 2019-11-04 00:28:26.882871: step 68590, total loss = 0.59, predict loss = 0.13 (80.1 examples/sec; 0.050 sec/batch; 82h:15m:47s remains)
INFO - root - 2019-11-04 00:28:27.483805: step 68600, total loss = 0.65, predict loss = 0.15 (79.9 examples/sec; 0.050 sec/batch; 82h:28m:09s remains)
INFO - root - 2019-11-04 00:28:28.099262: step 68610, total loss = 0.66, predict loss = 0.16 (77.7 examples/sec; 0.051 sec/batch; 84h:46m:36s remains)
INFO - root - 2019-11-04 00:28:28.719992: step 68620, total loss = 0.54, predict loss = 0.13 (74.9 examples/sec; 0.053 sec/batch; 87h:59m:48s remains)
INFO - root - 2019-11-04 00:28:29.363930: step 68630, total loss = 0.79, predict loss = 0.18 (63.3 examples/sec; 0.063 sec/batch; 104h:05m:40s remains)
INFO - root - 2019-11-04 00:28:30.060469: step 68640, total loss = 0.67, predict loss = 0.17 (63.2 examples/sec; 0.063 sec/batch; 104h:18m:08s remains)
INFO - root - 2019-11-04 00:28:30.716524: step 68650, total loss = 0.44, predict loss = 0.10 (65.4 examples/sec; 0.061 sec/batch; 100h:48m:02s remains)
INFO - root - 2019-11-04 00:28:31.370528: step 68660, total loss = 0.63, predict loss = 0.16 (70.1 examples/sec; 0.057 sec/batch; 94h:03m:30s remains)
INFO - root - 2019-11-04 00:28:32.027837: step 68670, total loss = 0.45, predict loss = 0.11 (68.6 examples/sec; 0.058 sec/batch; 96h:08m:20s remains)
INFO - root - 2019-11-04 00:28:32.689437: step 68680, total loss = 0.65, predict loss = 0.16 (79.9 examples/sec; 0.050 sec/batch; 82h:31m:57s remains)
INFO - root - 2019-11-04 00:28:33.362073: step 68690, total loss = 0.57, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:16m:35s remains)
INFO - root - 2019-11-04 00:28:34.037629: step 68700, total loss = 0.59, predict loss = 0.13 (66.3 examples/sec; 0.060 sec/batch; 99h:24m:34s remains)
INFO - root - 2019-11-04 00:28:34.676387: step 68710, total loss = 0.64, predict loss = 0.15 (70.7 examples/sec; 0.057 sec/batch; 93h:15m:12s remains)
INFO - root - 2019-11-04 00:28:35.361892: step 68720, total loss = 0.89, predict loss = 0.21 (67.9 examples/sec; 0.059 sec/batch; 96h:59m:40s remains)
INFO - root - 2019-11-04 00:28:35.960476: step 68730, total loss = 0.50, predict loss = 0.12 (74.6 examples/sec; 0.054 sec/batch; 88h:20m:32s remains)
INFO - root - 2019-11-04 00:28:36.575787: step 68740, total loss = 0.48, predict loss = 0.11 (71.3 examples/sec; 0.056 sec/batch; 92h:29m:09s remains)
INFO - root - 2019-11-04 00:28:37.238842: step 68750, total loss = 0.40, predict loss = 0.09 (72.2 examples/sec; 0.055 sec/batch; 91h:14m:38s remains)
INFO - root - 2019-11-04 00:28:37.970591: step 68760, total loss = 0.46, predict loss = 0.11 (60.6 examples/sec; 0.066 sec/batch; 108h:49m:44s remains)
INFO - root - 2019-11-04 00:28:38.651833: step 68770, total loss = 0.65, predict loss = 0.15 (61.0 examples/sec; 0.066 sec/batch; 107h:59m:18s remains)
INFO - root - 2019-11-04 00:28:39.313002: step 68780, total loss = 0.65, predict loss = 0.15 (65.7 examples/sec; 0.061 sec/batch; 100h:14m:28s remains)
INFO - root - 2019-11-04 00:28:39.944262: step 68790, total loss = 1.15, predict loss = 0.32 (70.3 examples/sec; 0.057 sec/batch; 93h:42m:30s remains)
INFO - root - 2019-11-04 00:28:40.591781: step 68800, total loss = 0.59, predict loss = 0.16 (62.7 examples/sec; 0.064 sec/batch; 105h:08m:55s remains)
INFO - root - 2019-11-04 00:28:41.271715: step 68810, total loss = 0.38, predict loss = 0.09 (73.1 examples/sec; 0.055 sec/batch; 90h:06m:18s remains)
INFO - root - 2019-11-04 00:28:41.920148: step 68820, total loss = 0.63, predict loss = 0.14 (71.2 examples/sec; 0.056 sec/batch; 92h:35m:09s remains)
INFO - root - 2019-11-04 00:28:42.566934: step 68830, total loss = 0.49, predict loss = 0.10 (66.7 examples/sec; 0.060 sec/batch; 98h:45m:03s remains)
INFO - root - 2019-11-04 00:28:43.246725: step 68840, total loss = 0.60, predict loss = 0.15 (64.6 examples/sec; 0.062 sec/batch; 102h:00m:14s remains)
INFO - root - 2019-11-04 00:28:43.860009: step 68850, total loss = 0.36, predict loss = 0.07 (72.2 examples/sec; 0.055 sec/batch; 91h:14m:07s remains)
INFO - root - 2019-11-04 00:28:44.454838: step 68860, total loss = 0.66, predict loss = 0.15 (80.8 examples/sec; 0.050 sec/batch; 81h:36m:26s remains)
INFO - root - 2019-11-04 00:28:45.082084: step 68870, total loss = 0.64, predict loss = 0.15 (66.9 examples/sec; 0.060 sec/batch; 98h:29m:43s remains)
INFO - root - 2019-11-04 00:28:45.754451: step 68880, total loss = 0.66, predict loss = 0.15 (66.2 examples/sec; 0.060 sec/batch; 99h:34m:44s remains)
INFO - root - 2019-11-04 00:28:46.433645: step 68890, total loss = 0.58, predict loss = 0.13 (67.8 examples/sec; 0.059 sec/batch; 97h:15m:15s remains)
INFO - root - 2019-11-04 00:28:47.097439: step 68900, total loss = 0.86, predict loss = 0.25 (64.2 examples/sec; 0.062 sec/batch; 102h:37m:50s remains)
INFO - root - 2019-11-04 00:28:47.764031: step 68910, total loss = 0.66, predict loss = 0.16 (68.3 examples/sec; 0.059 sec/batch; 96h:25m:16s remains)
INFO - root - 2019-11-04 00:28:48.407686: step 68920, total loss = 0.58, predict loss = 0.12 (74.1 examples/sec; 0.054 sec/batch; 88h:56m:10s remains)
INFO - root - 2019-11-04 00:28:49.054532: step 68930, total loss = 0.56, predict loss = 0.12 (75.1 examples/sec; 0.053 sec/batch; 87h:43m:51s remains)
INFO - root - 2019-11-04 00:28:49.662695: step 68940, total loss = 0.50, predict loss = 0.13 (80.1 examples/sec; 0.050 sec/batch; 82h:17m:38s remains)
INFO - root - 2019-11-04 00:28:50.269366: step 68950, total loss = 0.71, predict loss = 0.17 (74.3 examples/sec; 0.054 sec/batch; 88h:41m:13s remains)
INFO - root - 2019-11-04 00:28:50.877232: step 68960, total loss = 0.60, predict loss = 0.13 (75.6 examples/sec; 0.053 sec/batch; 87h:13m:12s remains)
INFO - root - 2019-11-04 00:28:51.504441: step 68970, total loss = 0.67, predict loss = 0.16 (76.1 examples/sec; 0.053 sec/batch; 86h:36m:34s remains)
INFO - root - 2019-11-04 00:28:52.141781: step 68980, total loss = 0.70, predict loss = 0.17 (76.4 examples/sec; 0.052 sec/batch; 86h:14m:23s remains)
INFO - root - 2019-11-04 00:28:52.782624: step 68990, total loss = 0.65, predict loss = 0.15 (72.9 examples/sec; 0.055 sec/batch; 90h:24m:15s remains)
INFO - root - 2019-11-04 00:28:53.402257: step 69000, total loss = 0.53, predict loss = 0.13 (72.1 examples/sec; 0.055 sec/batch; 91h:24m:41s remains)
INFO - root - 2019-11-04 00:28:54.008396: step 69010, total loss = 0.42, predict loss = 0.10 (66.1 examples/sec; 0.061 sec/batch; 99h:43m:21s remains)
INFO - root - 2019-11-04 00:28:54.648690: step 69020, total loss = 0.30, predict loss = 0.05 (73.3 examples/sec; 0.055 sec/batch; 89h:51m:09s remains)
INFO - root - 2019-11-04 00:28:55.306831: step 69030, total loss = 0.37, predict loss = 0.08 (73.0 examples/sec; 0.055 sec/batch; 90h:18m:53s remains)
INFO - root - 2019-11-04 00:28:55.915093: step 69040, total loss = 0.49, predict loss = 0.11 (72.9 examples/sec; 0.055 sec/batch; 90h:24m:43s remains)
INFO - root - 2019-11-04 00:28:56.552279: step 69050, total loss = 0.46, predict loss = 0.10 (69.9 examples/sec; 0.057 sec/batch; 94h:19m:15s remains)
INFO - root - 2019-11-04 00:28:57.173999: step 69060, total loss = 0.37, predict loss = 0.08 (66.0 examples/sec; 0.061 sec/batch; 99h:55m:14s remains)
INFO - root - 2019-11-04 00:28:57.809279: step 69070, total loss = 0.41, predict loss = 0.08 (71.6 examples/sec; 0.056 sec/batch; 91h:59m:16s remains)
INFO - root - 2019-11-04 00:28:58.473572: step 69080, total loss = 0.42, predict loss = 0.08 (62.5 examples/sec; 0.064 sec/batch; 105h:27m:21s remains)
INFO - root - 2019-11-04 00:28:59.130772: step 69090, total loss = 0.65, predict loss = 0.15 (74.4 examples/sec; 0.054 sec/batch; 88h:32m:03s remains)
INFO - root - 2019-11-04 00:28:59.790490: step 69100, total loss = 0.63, predict loss = 0.15 (63.4 examples/sec; 0.063 sec/batch; 104h:00m:55s remains)
INFO - root - 2019-11-04 00:29:00.478115: step 69110, total loss = 0.45, predict loss = 0.10 (61.8 examples/sec; 0.065 sec/batch; 106h:37m:11s remains)
INFO - root - 2019-11-04 00:29:01.129332: step 69120, total loss = 0.51, predict loss = 0.11 (73.3 examples/sec; 0.055 sec/batch; 89h:56m:58s remains)
INFO - root - 2019-11-04 00:29:01.757190: step 69130, total loss = 0.72, predict loss = 0.16 (72.7 examples/sec; 0.055 sec/batch; 90h:38m:27s remains)
INFO - root - 2019-11-04 00:29:02.372576: step 69140, total loss = 0.46, predict loss = 0.11 (81.9 examples/sec; 0.049 sec/batch; 80h:29m:30s remains)
INFO - root - 2019-11-04 00:29:03.000272: step 69150, total loss = 0.40, predict loss = 0.09 (68.4 examples/sec; 0.059 sec/batch; 96h:23m:45s remains)
INFO - root - 2019-11-04 00:29:03.656773: step 69160, total loss = 0.39, predict loss = 0.08 (65.4 examples/sec; 0.061 sec/batch; 100h:45m:02s remains)
INFO - root - 2019-11-04 00:29:04.354147: step 69170, total loss = 0.51, predict loss = 0.12 (64.8 examples/sec; 0.062 sec/batch; 101h:38m:03s remains)
INFO - root - 2019-11-04 00:29:05.061412: step 69180, total loss = 0.63, predict loss = 0.15 (73.4 examples/sec; 0.054 sec/batch; 89h:46m:33s remains)
INFO - root - 2019-11-04 00:29:05.691035: step 69190, total loss = 0.68, predict loss = 0.16 (68.4 examples/sec; 0.059 sec/batch; 96h:22m:34s remains)
INFO - root - 2019-11-04 00:29:06.315316: step 69200, total loss = 0.58, predict loss = 0.13 (69.7 examples/sec; 0.057 sec/batch; 94h:29m:37s remains)
INFO - root - 2019-11-04 00:29:06.960433: step 69210, total loss = 0.49, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 97h:23m:42s remains)
INFO - root - 2019-11-04 00:29:07.614257: step 69220, total loss = 0.33, predict loss = 0.07 (82.5 examples/sec; 0.048 sec/batch; 79h:50m:19s remains)
INFO - root - 2019-11-04 00:29:08.220568: step 69230, total loss = 0.60, predict loss = 0.13 (70.7 examples/sec; 0.057 sec/batch; 93h:12m:49s remains)
INFO - root - 2019-11-04 00:29:08.860377: step 69240, total loss = 0.28, predict loss = 0.06 (63.4 examples/sec; 0.063 sec/batch; 103h:58m:33s remains)
INFO - root - 2019-11-04 00:29:09.535974: step 69250, total loss = 0.35, predict loss = 0.07 (66.3 examples/sec; 0.060 sec/batch; 99h:19m:11s remains)
INFO - root - 2019-11-04 00:29:10.196496: step 69260, total loss = 0.35, predict loss = 0.08 (68.7 examples/sec; 0.058 sec/batch; 95h:53m:10s remains)
INFO - root - 2019-11-04 00:29:10.799881: step 69270, total loss = 0.38, predict loss = 0.09 (71.9 examples/sec; 0.056 sec/batch; 91h:38m:46s remains)
INFO - root - 2019-11-04 00:29:11.412548: step 69280, total loss = 0.20, predict loss = 0.04 (65.3 examples/sec; 0.061 sec/batch; 100h:55m:38s remains)
INFO - root - 2019-11-04 00:29:12.081811: step 69290, total loss = 0.31, predict loss = 0.06 (71.9 examples/sec; 0.056 sec/batch; 91h:38m:19s remains)
INFO - root - 2019-11-04 00:29:12.750823: step 69300, total loss = 0.40, predict loss = 0.09 (64.4 examples/sec; 0.062 sec/batch; 102h:18m:18s remains)
INFO - root - 2019-11-04 00:29:13.366821: step 69310, total loss = 0.35, predict loss = 0.08 (72.6 examples/sec; 0.055 sec/batch; 90h:44m:12s remains)
INFO - root - 2019-11-04 00:29:13.980531: step 69320, total loss = 0.65, predict loss = 0.16 (69.0 examples/sec; 0.058 sec/batch; 95h:33m:16s remains)
INFO - root - 2019-11-04 00:29:14.627050: step 69330, total loss = 0.51, predict loss = 0.13 (73.2 examples/sec; 0.055 sec/batch; 90h:04m:11s remains)
INFO - root - 2019-11-04 00:29:15.240152: step 69340, total loss = 0.56, predict loss = 0.14 (74.4 examples/sec; 0.054 sec/batch; 88h:32m:20s remains)
INFO - root - 2019-11-04 00:29:15.847486: step 69350, total loss = 0.51, predict loss = 0.12 (70.0 examples/sec; 0.057 sec/batch; 94h:11m:18s remains)
INFO - root - 2019-11-04 00:29:16.475151: step 69360, total loss = 0.48, predict loss = 0.11 (64.9 examples/sec; 0.062 sec/batch; 101h:28m:21s remains)
INFO - root - 2019-11-04 00:29:17.181144: step 69370, total loss = 0.61, predict loss = 0.14 (64.1 examples/sec; 0.062 sec/batch; 102h:48m:37s remains)
INFO - root - 2019-11-04 00:29:17.863147: step 69380, total loss = 0.58, predict loss = 0.14 (67.5 examples/sec; 0.059 sec/batch; 97h:38m:40s remains)
INFO - root - 2019-11-04 00:29:18.567258: step 69390, total loss = 0.57, predict loss = 0.14 (69.4 examples/sec; 0.058 sec/batch; 95h:01m:05s remains)
INFO - root - 2019-11-04 00:29:19.222788: step 69400, total loss = 0.61, predict loss = 0.14 (72.8 examples/sec; 0.055 sec/batch; 90h:28m:09s remains)
INFO - root - 2019-11-04 00:29:19.872703: step 69410, total loss = 0.61, predict loss = 0.14 (64.5 examples/sec; 0.062 sec/batch; 102h:07m:31s remains)
INFO - root - 2019-11-04 00:29:20.557355: step 69420, total loss = 0.52, predict loss = 0.12 (65.8 examples/sec; 0.061 sec/batch; 100h:07m:59s remains)
INFO - root - 2019-11-04 00:29:21.164941: step 69430, total loss = 0.50, predict loss = 0.11 (79.4 examples/sec; 0.050 sec/batch; 82h:58m:46s remains)
INFO - root - 2019-11-04 00:29:21.768641: step 69440, total loss = 0.50, predict loss = 0.11 (80.5 examples/sec; 0.050 sec/batch; 81h:53m:41s remains)
INFO - root - 2019-11-04 00:29:22.390799: step 69450, total loss = 0.48, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 87h:49m:08s remains)
INFO - root - 2019-11-04 00:29:23.004592: step 69460, total loss = 0.57, predict loss = 0.13 (69.9 examples/sec; 0.057 sec/batch; 94h:12m:37s remains)
INFO - root - 2019-11-04 00:29:23.630988: step 69470, total loss = 0.65, predict loss = 0.15 (80.1 examples/sec; 0.050 sec/batch; 82h:16m:29s remains)
INFO - root - 2019-11-04 00:29:24.295947: step 69480, total loss = 0.75, predict loss = 0.17 (74.8 examples/sec; 0.053 sec/batch; 88h:03m:59s remains)
INFO - root - 2019-11-04 00:29:24.936220: step 69490, total loss = 0.64, predict loss = 0.14 (72.9 examples/sec; 0.055 sec/batch; 90h:25m:47s remains)
INFO - root - 2019-11-04 00:29:25.583063: step 69500, total loss = 0.58, predict loss = 0.13 (69.5 examples/sec; 0.058 sec/batch; 94h:45m:51s remains)
INFO - root - 2019-11-04 00:29:26.217903: step 69510, total loss = 0.30, predict loss = 0.06 (72.2 examples/sec; 0.055 sec/batch; 91h:18m:16s remains)
INFO - root - 2019-11-04 00:29:26.883496: step 69520, total loss = 0.51, predict loss = 0.12 (69.8 examples/sec; 0.057 sec/batch; 94h:28m:12s remains)
INFO - root - 2019-11-04 00:29:27.489502: step 69530, total loss = 0.50, predict loss = 0.12 (78.4 examples/sec; 0.051 sec/batch; 84h:03m:58s remains)
INFO - root - 2019-11-04 00:29:28.095112: step 69540, total loss = 0.40, predict loss = 0.09 (77.1 examples/sec; 0.052 sec/batch; 85h:28m:26s remains)
INFO - root - 2019-11-04 00:29:28.749177: step 69550, total loss = 0.55, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 93h:45m:43s remains)
INFO - root - 2019-11-04 00:29:29.412090: step 69560, total loss = 0.61, predict loss = 0.14 (67.6 examples/sec; 0.059 sec/batch; 97h:27m:50s remains)
INFO - root - 2019-11-04 00:29:30.091860: step 69570, total loss = 0.41, predict loss = 0.09 (65.6 examples/sec; 0.061 sec/batch; 100h:30m:03s remains)
INFO - root - 2019-11-04 00:29:30.777126: step 69580, total loss = 0.33, predict loss = 0.07 (65.9 examples/sec; 0.061 sec/batch; 99h:59m:37s remains)
INFO - root - 2019-11-04 00:29:31.467721: step 69590, total loss = 0.64, predict loss = 0.15 (71.4 examples/sec; 0.056 sec/batch; 92h:18m:49s remains)
INFO - root - 2019-11-04 00:29:32.089349: step 69600, total loss = 0.61, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 95h:05m:16s remains)
INFO - root - 2019-11-04 00:29:32.730085: step 69610, total loss = 0.56, predict loss = 0.13 (73.6 examples/sec; 0.054 sec/batch; 89h:30m:49s remains)
INFO - root - 2019-11-04 00:29:33.354917: step 69620, total loss = 0.92, predict loss = 0.22 (69.0 examples/sec; 0.058 sec/batch; 95h:29m:01s remains)
INFO - root - 2019-11-04 00:29:33.996040: step 69630, total loss = 0.79, predict loss = 0.19 (70.8 examples/sec; 0.056 sec/batch; 93h:03m:22s remains)
INFO - root - 2019-11-04 00:29:34.627421: step 69640, total loss = 0.91, predict loss = 0.21 (70.8 examples/sec; 0.057 sec/batch; 93h:05m:35s remains)
INFO - root - 2019-11-04 00:29:35.303070: step 69650, total loss = 0.84, predict loss = 0.20 (72.6 examples/sec; 0.055 sec/batch; 90h:43m:33s remains)
INFO - root - 2019-11-04 00:29:35.914681: step 69660, total loss = 0.56, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 99h:04m:41s remains)
INFO - root - 2019-11-04 00:29:36.536068: step 69670, total loss = 0.62, predict loss = 0.15 (77.6 examples/sec; 0.052 sec/batch; 84h:53m:43s remains)
INFO - root - 2019-11-04 00:29:37.184689: step 69680, total loss = 0.74, predict loss = 0.18 (63.2 examples/sec; 0.063 sec/batch; 104h:18m:27s remains)
INFO - root - 2019-11-04 00:29:37.830515: step 69690, total loss = 0.75, predict loss = 0.18 (73.6 examples/sec; 0.054 sec/batch; 89h:33m:56s remains)
INFO - root - 2019-11-04 00:29:38.452474: step 69700, total loss = 0.64, predict loss = 0.15 (65.2 examples/sec; 0.061 sec/batch; 101h:07m:35s remains)
INFO - root - 2019-11-04 00:29:39.075621: step 69710, total loss = 0.66, predict loss = 0.14 (71.2 examples/sec; 0.056 sec/batch; 92h:35m:13s remains)
INFO - root - 2019-11-04 00:29:39.723038: step 69720, total loss = 0.58, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 101h:08m:30s remains)
INFO - root - 2019-11-04 00:29:40.340767: step 69730, total loss = 0.59, predict loss = 0.14 (76.3 examples/sec; 0.052 sec/batch; 86h:21m:55s remains)
INFO - root - 2019-11-04 00:29:40.984083: step 69740, total loss = 0.63, predict loss = 0.15 (67.4 examples/sec; 0.059 sec/batch; 97h:47m:32s remains)
INFO - root - 2019-11-04 00:29:41.636756: step 69750, total loss = 0.59, predict loss = 0.14 (62.9 examples/sec; 0.064 sec/batch; 104h:46m:09s remains)
INFO - root - 2019-11-04 00:29:42.307988: step 69760, total loss = 0.50, predict loss = 0.12 (62.3 examples/sec; 0.064 sec/batch; 105h:44m:56s remains)
INFO - root - 2019-11-04 00:29:42.970860: step 69770, total loss = 0.48, predict loss = 0.10 (61.1 examples/sec; 0.065 sec/batch; 107h:47m:49s remains)
INFO - root - 2019-11-04 00:29:43.639960: step 69780, total loss = 0.64, predict loss = 0.15 (64.6 examples/sec; 0.062 sec/batch; 101h:58m:26s remains)
INFO - root - 2019-11-04 00:29:44.333624: step 69790, total loss = 0.52, predict loss = 0.12 (66.4 examples/sec; 0.060 sec/batch; 99h:17m:53s remains)
INFO - root - 2019-11-04 00:29:44.956734: step 69800, total loss = 0.48, predict loss = 0.12 (69.1 examples/sec; 0.058 sec/batch; 95h:17m:52s remains)
INFO - root - 2019-11-04 00:29:45.612193: step 69810, total loss = 0.64, predict loss = 0.15 (64.5 examples/sec; 0.062 sec/batch; 102h:06m:42s remains)
INFO - root - 2019-11-04 00:29:46.266608: step 69820, total loss = 0.62, predict loss = 0.13 (74.7 examples/sec; 0.054 sec/batch; 88h:10m:52s remains)
INFO - root - 2019-11-04 00:29:46.901415: step 69830, total loss = 0.53, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 99h:30m:21s remains)
INFO - root - 2019-11-04 00:29:47.524207: step 69840, total loss = 0.57, predict loss = 0.13 (76.3 examples/sec; 0.052 sec/batch; 86h:24m:47s remains)
INFO - root - 2019-11-04 00:29:48.177424: step 69850, total loss = 0.44, predict loss = 0.09 (64.8 examples/sec; 0.062 sec/batch; 101h:43m:48s remains)
INFO - root - 2019-11-04 00:29:48.837265: step 69860, total loss = 0.37, predict loss = 0.08 (70.9 examples/sec; 0.056 sec/batch; 92h:57m:12s remains)
INFO - root - 2019-11-04 00:29:49.506435: step 69870, total loss = 0.44, predict loss = 0.10 (73.0 examples/sec; 0.055 sec/batch; 90h:16m:16s remains)
INFO - root - 2019-11-04 00:29:50.116440: step 69880, total loss = 0.60, predict loss = 0.14 (74.9 examples/sec; 0.053 sec/batch; 87h:56m:23s remains)
INFO - root - 2019-11-04 00:29:50.758542: step 69890, total loss = 0.46, predict loss = 0.09 (82.2 examples/sec; 0.049 sec/batch; 80h:08m:53s remains)
INFO - root - 2019-11-04 00:29:51.395292: step 69900, total loss = 0.53, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 93h:45m:26s remains)
INFO - root - 2019-11-04 00:29:52.032597: step 69910, total loss = 0.44, predict loss = 0.10 (67.1 examples/sec; 0.060 sec/batch; 98h:13m:44s remains)
INFO - root - 2019-11-04 00:29:52.653745: step 69920, total loss = 0.41, predict loss = 0.09 (73.6 examples/sec; 0.054 sec/batch; 89h:34m:43s remains)
INFO - root - 2019-11-04 00:29:53.275665: step 69930, total loss = 0.50, predict loss = 0.11 (74.9 examples/sec; 0.053 sec/batch; 87h:56m:43s remains)
INFO - root - 2019-11-04 00:29:53.927315: step 69940, total loss = 0.38, predict loss = 0.08 (70.1 examples/sec; 0.057 sec/batch; 93h:56m:48s remains)
INFO - root - 2019-11-04 00:29:54.570127: step 69950, total loss = 0.46, predict loss = 0.10 (63.7 examples/sec; 0.063 sec/batch; 103h:23m:58s remains)
INFO - root - 2019-11-04 00:29:55.232304: step 69960, total loss = 0.47, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 95h:02m:04s remains)
INFO - root - 2019-11-04 00:29:55.880920: step 69970, total loss = 0.52, predict loss = 0.12 (72.7 examples/sec; 0.055 sec/batch; 90h:37m:55s remains)
INFO - root - 2019-11-04 00:29:56.510265: step 69980, total loss = 0.40, predict loss = 0.09 (73.5 examples/sec; 0.054 sec/batch; 89h:35m:33s remains)
INFO - root - 2019-11-04 00:29:57.133367: step 69990, total loss = 0.50, predict loss = 0.12 (71.6 examples/sec; 0.056 sec/batch; 92h:00m:39s remains)
INFO - root - 2019-11-04 00:29:57.795597: step 70000, total loss = 0.56, predict loss = 0.13 (66.9 examples/sec; 0.060 sec/batch; 98h:28m:38s remains)
INFO - root - 2019-11-04 00:29:58.446672: step 70010, total loss = 0.45, predict loss = 0.10 (72.7 examples/sec; 0.055 sec/batch; 90h:37m:13s remains)
INFO - root - 2019-11-04 00:29:59.101318: step 70020, total loss = 0.57, predict loss = 0.12 (68.4 examples/sec; 0.059 sec/batch; 96h:22m:15s remains)
INFO - root - 2019-11-04 00:29:59.762884: step 70030, total loss = 0.49, predict loss = 0.11 (67.3 examples/sec; 0.059 sec/batch; 97h:56m:22s remains)
INFO - root - 2019-11-04 00:30:00.421431: step 70040, total loss = 0.67, predict loss = 0.16 (75.2 examples/sec; 0.053 sec/batch; 87h:36m:33s remains)
INFO - root - 2019-11-04 00:30:01.087564: step 70050, total loss = 0.75, predict loss = 0.18 (72.6 examples/sec; 0.055 sec/batch; 90h:48m:08s remains)
INFO - root - 2019-11-04 00:30:01.741439: step 70060, total loss = 0.63, predict loss = 0.15 (72.5 examples/sec; 0.055 sec/batch; 90h:56m:17s remains)
INFO - root - 2019-11-04 00:30:02.351817: step 70070, total loss = 0.67, predict loss = 0.16 (76.7 examples/sec; 0.052 sec/batch; 85h:56m:21s remains)
INFO - root - 2019-11-04 00:30:02.996710: step 70080, total loss = 0.64, predict loss = 0.16 (67.1 examples/sec; 0.060 sec/batch; 98h:10m:15s remains)
INFO - root - 2019-11-04 00:30:03.612349: step 70090, total loss = 0.77, predict loss = 0.17 (71.9 examples/sec; 0.056 sec/batch; 91h:35m:33s remains)
INFO - root - 2019-11-04 00:30:04.237097: step 70100, total loss = 0.54, predict loss = 0.12 (62.9 examples/sec; 0.064 sec/batch; 104h:45m:01s remains)
INFO - root - 2019-11-04 00:30:04.869218: step 70110, total loss = 0.59, predict loss = 0.13 (59.7 examples/sec; 0.067 sec/batch; 110h:17m:06s remains)
INFO - root - 2019-11-04 00:30:05.512338: step 70120, total loss = 0.48, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 96h:43m:03s remains)
INFO - root - 2019-11-04 00:30:06.144303: step 70130, total loss = 0.56, predict loss = 0.13 (68.0 examples/sec; 0.059 sec/batch; 96h:56m:52s remains)
INFO - root - 2019-11-04 00:30:06.795179: step 70140, total loss = 0.43, predict loss = 0.10 (72.0 examples/sec; 0.056 sec/batch; 91h:33m:02s remains)
INFO - root - 2019-11-04 00:30:07.473138: step 70150, total loss = 0.40, predict loss = 0.09 (73.9 examples/sec; 0.054 sec/batch; 89h:07m:11s remains)
INFO - root - 2019-11-04 00:30:08.082868: step 70160, total loss = 0.32, predict loss = 0.07 (67.4 examples/sec; 0.059 sec/batch; 97h:45m:51s remains)
INFO - root - 2019-11-04 00:30:08.689471: step 70170, total loss = 0.39, predict loss = 0.09 (65.6 examples/sec; 0.061 sec/batch; 100h:30m:29s remains)
INFO - root - 2019-11-04 00:30:09.331737: step 70180, total loss = 0.45, predict loss = 0.11 (69.9 examples/sec; 0.057 sec/batch; 94h:19m:07s remains)
INFO - root - 2019-11-04 00:30:09.957896: step 70190, total loss = 0.41, predict loss = 0.09 (69.4 examples/sec; 0.058 sec/batch; 94h:59m:42s remains)
INFO - root - 2019-11-04 00:30:10.597074: step 70200, total loss = 0.51, predict loss = 0.11 (70.2 examples/sec; 0.057 sec/batch; 93h:51m:43s remains)
INFO - root - 2019-11-04 00:30:11.191738: step 70210, total loss = 0.54, predict loss = 0.13 (76.1 examples/sec; 0.053 sec/batch; 86h:36m:26s remains)
INFO - root - 2019-11-04 00:30:11.797064: step 70220, total loss = 0.53, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 94h:57m:54s remains)
INFO - root - 2019-11-04 00:30:12.427780: step 70230, total loss = 0.46, predict loss = 0.10 (74.2 examples/sec; 0.054 sec/batch; 88h:46m:51s remains)
INFO - root - 2019-11-04 00:30:13.062555: step 70240, total loss = 0.54, predict loss = 0.12 (64.1 examples/sec; 0.062 sec/batch; 102h:44m:54s remains)
INFO - root - 2019-11-04 00:30:13.745701: step 70250, total loss = 0.50, predict loss = 0.12 (67.4 examples/sec; 0.059 sec/batch; 97h:46m:08s remains)
INFO - root - 2019-11-04 00:30:14.433549: step 70260, total loss = 0.56, predict loss = 0.13 (78.0 examples/sec; 0.051 sec/batch; 84h:27m:28s remains)
INFO - root - 2019-11-04 00:30:15.113569: step 70270, total loss = 0.74, predict loss = 0.17 (68.8 examples/sec; 0.058 sec/batch; 95h:45m:02s remains)
INFO - root - 2019-11-04 00:30:15.783111: step 70280, total loss = 0.65, predict loss = 0.15 (65.9 examples/sec; 0.061 sec/batch; 99h:56m:49s remains)
INFO - root - 2019-11-04 00:30:16.435919: step 70290, total loss = 0.60, predict loss = 0.14 (67.1 examples/sec; 0.060 sec/batch; 98h:11m:34s remains)
INFO - root - 2019-11-04 00:30:17.102328: step 70300, total loss = 0.68, predict loss = 0.17 (69.3 examples/sec; 0.058 sec/batch; 95h:07m:22s remains)
INFO - root - 2019-11-04 00:30:17.718530: step 70310, total loss = 0.67, predict loss = 0.16 (82.5 examples/sec; 0.048 sec/batch; 79h:49m:59s remains)
INFO - root - 2019-11-04 00:30:18.346943: step 70320, total loss = 0.73, predict loss = 0.18 (80.6 examples/sec; 0.050 sec/batch; 81h:47m:02s remains)
INFO - root - 2019-11-04 00:30:18.968108: step 70330, total loss = 0.62, predict loss = 0.14 (66.2 examples/sec; 0.060 sec/batch; 99h:28m:14s remains)
INFO - root - 2019-11-04 00:30:19.635266: step 70340, total loss = 0.44, predict loss = 0.10 (64.4 examples/sec; 0.062 sec/batch; 102h:16m:51s remains)
INFO - root - 2019-11-04 00:30:20.322669: step 70350, total loss = 0.62, predict loss = 0.15 (59.9 examples/sec; 0.067 sec/batch; 109h:54m:10s remains)
INFO - root - 2019-11-04 00:30:20.984469: step 70360, total loss = 0.64, predict loss = 0.16 (62.3 examples/sec; 0.064 sec/batch; 105h:48m:58s remains)
INFO - root - 2019-11-04 00:30:21.640948: step 70370, total loss = 0.57, predict loss = 0.13 (61.2 examples/sec; 0.065 sec/batch; 107h:34m:52s remains)
INFO - root - 2019-11-04 00:30:22.306029: step 70380, total loss = 0.47, predict loss = 0.11 (68.2 examples/sec; 0.059 sec/batch; 96h:39m:07s remains)
INFO - root - 2019-11-04 00:30:22.972261: step 70390, total loss = 0.60, predict loss = 0.14 (64.5 examples/sec; 0.062 sec/batch; 102h:05m:34s remains)
INFO - root - 2019-11-04 00:30:23.639292: step 70400, total loss = 0.52, predict loss = 0.12 (62.1 examples/sec; 0.064 sec/batch; 106h:07m:59s remains)
INFO - root - 2019-11-04 00:30:24.302422: step 70410, total loss = 0.56, predict loss = 0.13 (65.7 examples/sec; 0.061 sec/batch; 100h:15m:13s remains)
INFO - root - 2019-11-04 00:30:24.953676: step 70420, total loss = 0.45, predict loss = 0.10 (65.7 examples/sec; 0.061 sec/batch; 100h:20m:59s remains)
INFO - root - 2019-11-04 00:30:25.618834: step 70430, total loss = 0.57, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:15m:01s remains)
INFO - root - 2019-11-04 00:30:26.300590: step 70440, total loss = 0.50, predict loss = 0.12 (64.0 examples/sec; 0.062 sec/batch; 102h:53m:29s remains)
INFO - root - 2019-11-04 00:30:26.976954: step 70450, total loss = 0.46, predict loss = 0.10 (62.9 examples/sec; 0.064 sec/batch; 104h:45m:02s remains)
INFO - root - 2019-11-04 00:30:27.646827: step 70460, total loss = 0.46, predict loss = 0.10 (65.3 examples/sec; 0.061 sec/batch; 100h:49m:30s remains)
INFO - root - 2019-11-04 00:30:28.276663: step 70470, total loss = 0.51, predict loss = 0.12 (75.2 examples/sec; 0.053 sec/batch; 87h:38m:12s remains)
INFO - root - 2019-11-04 00:30:28.934373: step 70480, total loss = 0.47, predict loss = 0.11 (61.9 examples/sec; 0.065 sec/batch; 106h:31m:05s remains)
INFO - root - 2019-11-04 00:30:29.517821: step 70490, total loss = 0.54, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 90h:09m:53s remains)
INFO - root - 2019-11-04 00:30:30.137380: step 70500, total loss = 0.45, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 100h:02m:46s remains)
INFO - root - 2019-11-04 00:30:30.740538: step 70510, total loss = 0.45, predict loss = 0.11 (69.6 examples/sec; 0.057 sec/batch; 94h:41m:43s remains)
INFO - root - 2019-11-04 00:30:31.340805: step 70520, total loss = 0.56, predict loss = 0.13 (71.9 examples/sec; 0.056 sec/batch; 91h:36m:31s remains)
INFO - root - 2019-11-04 00:30:31.995606: step 70530, total loss = 0.53, predict loss = 0.12 (79.0 examples/sec; 0.051 sec/batch; 83h:25m:07s remains)
INFO - root - 2019-11-04 00:30:32.629709: step 70540, total loss = 0.55, predict loss = 0.13 (67.1 examples/sec; 0.060 sec/batch; 98h:09m:31s remains)
INFO - root - 2019-11-04 00:30:33.306152: step 70550, total loss = 0.46, predict loss = 0.11 (65.4 examples/sec; 0.061 sec/batch; 100h:41m:18s remains)
INFO - root - 2019-11-04 00:30:34.013072: step 70560, total loss = 0.47, predict loss = 0.11 (54.5 examples/sec; 0.073 sec/batch; 120h:57m:37s remains)
INFO - root - 2019-11-04 00:30:34.734133: step 70570, total loss = 0.48, predict loss = 0.11 (64.4 examples/sec; 0.062 sec/batch; 102h:14m:01s remains)
INFO - root - 2019-11-04 00:30:35.398706: step 70580, total loss = 0.47, predict loss = 0.10 (76.4 examples/sec; 0.052 sec/batch; 86h:16m:58s remains)
INFO - root - 2019-11-04 00:30:36.024401: step 70590, total loss = 0.72, predict loss = 0.19 (61.0 examples/sec; 0.066 sec/batch; 107h:58m:07s remains)
INFO - root - 2019-11-04 00:30:36.655600: step 70600, total loss = 0.51, predict loss = 0.12 (77.0 examples/sec; 0.052 sec/batch; 85h:31m:32s remains)
INFO - root - 2019-11-04 00:30:37.290676: step 70610, total loss = 0.48, predict loss = 0.11 (70.2 examples/sec; 0.057 sec/batch; 93h:52m:34s remains)
INFO - root - 2019-11-04 00:30:37.974397: step 70620, total loss = 0.60, predict loss = 0.15 (79.7 examples/sec; 0.050 sec/batch; 82h:37m:04s remains)
INFO - root - 2019-11-04 00:30:38.585307: step 70630, total loss = 0.50, predict loss = 0.12 (77.5 examples/sec; 0.052 sec/batch; 85h:02m:02s remains)
INFO - root - 2019-11-04 00:30:39.195947: step 70640, total loss = 0.56, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 95h:43m:55s remains)
INFO - root - 2019-11-04 00:30:39.839747: step 70650, total loss = 0.59, predict loss = 0.15 (73.8 examples/sec; 0.054 sec/batch; 89h:13m:05s remains)
INFO - root - 2019-11-04 00:30:40.500441: step 70660, total loss = 0.72, predict loss = 0.17 (60.7 examples/sec; 0.066 sec/batch; 108h:34m:46s remains)
INFO - root - 2019-11-04 00:30:41.194361: step 70670, total loss = 0.65, predict loss = 0.14 (59.6 examples/sec; 0.067 sec/batch; 110h:28m:49s remains)
INFO - root - 2019-11-04 00:30:41.825240: step 70680, total loss = 0.65, predict loss = 0.15 (70.4 examples/sec; 0.057 sec/batch; 93h:30m:57s remains)
INFO - root - 2019-11-04 00:30:42.460737: step 70690, total loss = 0.85, predict loss = 0.20 (70.6 examples/sec; 0.057 sec/batch; 93h:16m:50s remains)
INFO - root - 2019-11-04 00:30:43.088594: step 70700, total loss = 0.63, predict loss = 0.15 (69.8 examples/sec; 0.057 sec/batch; 94h:23m:48s remains)
INFO - root - 2019-11-04 00:30:43.722675: step 70710, total loss = 0.85, predict loss = 0.19 (67.0 examples/sec; 0.060 sec/batch; 98h:22m:38s remains)
INFO - root - 2019-11-04 00:30:44.403875: step 70720, total loss = 0.76, predict loss = 0.19 (63.5 examples/sec; 0.063 sec/batch; 103h:49m:48s remains)
INFO - root - 2019-11-04 00:30:45.014667: step 70730, total loss = 0.51, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 91h:45m:39s remains)
INFO - root - 2019-11-04 00:30:45.624184: step 70740, total loss = 0.84, predict loss = 0.21 (69.7 examples/sec; 0.057 sec/batch; 94h:29m:22s remains)
INFO - root - 2019-11-04 00:30:46.244891: step 70750, total loss = 0.45, predict loss = 0.10 (65.6 examples/sec; 0.061 sec/batch; 100h:24m:22s remains)
INFO - root - 2019-11-04 00:30:46.897050: step 70760, total loss = 0.54, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 99h:00m:24s remains)
INFO - root - 2019-11-04 00:30:47.540128: step 70770, total loss = 0.43, predict loss = 0.09 (68.7 examples/sec; 0.058 sec/batch; 95h:55m:25s remains)
INFO - root - 2019-11-04 00:30:48.150071: step 70780, total loss = 0.54, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 89h:08m:18s remains)
INFO - root - 2019-11-04 00:30:48.788312: step 70790, total loss = 0.41, predict loss = 0.09 (76.2 examples/sec; 0.053 sec/batch; 86h:29m:14s remains)
INFO - root - 2019-11-04 00:30:49.378552: step 70800, total loss = 0.60, predict loss = 0.14 (65.7 examples/sec; 0.061 sec/batch; 100h:16m:47s remains)
INFO - root - 2019-11-04 00:30:50.009337: step 70810, total loss = 0.47, predict loss = 0.10 (73.6 examples/sec; 0.054 sec/batch; 89h:33m:15s remains)
INFO - root - 2019-11-04 00:30:50.660126: step 70820, total loss = 0.75, predict loss = 0.18 (72.8 examples/sec; 0.055 sec/batch; 90h:31m:26s remains)
INFO - root - 2019-11-04 00:30:51.297765: step 70830, total loss = 0.57, predict loss = 0.13 (74.6 examples/sec; 0.054 sec/batch; 88h:17m:02s remains)
INFO - root - 2019-11-04 00:30:51.920078: step 70840, total loss = 0.60, predict loss = 0.14 (80.4 examples/sec; 0.050 sec/batch; 81h:56m:09s remains)
INFO - root - 2019-11-04 00:30:52.539133: step 70850, total loss = 0.66, predict loss = 0.15 (80.6 examples/sec; 0.050 sec/batch; 81h:43m:52s remains)
INFO - root - 2019-11-04 00:30:53.140115: step 70860, total loss = 0.69, predict loss = 0.16 (77.5 examples/sec; 0.052 sec/batch; 85h:02m:38s remains)
INFO - root - 2019-11-04 00:30:53.752349: step 70870, total loss = 0.42, predict loss = 0.10 (75.8 examples/sec; 0.053 sec/batch; 86h:51m:30s remains)
INFO - root - 2019-11-04 00:30:54.856002: step 70880, total loss = 0.60, predict loss = 0.14 (69.1 examples/sec; 0.058 sec/batch; 95h:21m:15s remains)
INFO - root - 2019-11-04 00:30:55.430277: step 70890, total loss = 0.57, predict loss = 0.13 (75.6 examples/sec; 0.053 sec/batch; 87h:10m:27s remains)
INFO - root - 2019-11-04 00:30:55.928421: step 70900, total loss = 0.46, predict loss = 0.11 (93.4 examples/sec; 0.043 sec/batch; 70h:30m:48s remains)
INFO - root - 2019-11-04 00:30:56.411642: step 70910, total loss = 0.46, predict loss = 0.10 (86.4 examples/sec; 0.046 sec/batch; 76h:15m:54s remains)
INFO - root - 2019-11-04 00:30:57.533213: step 70920, total loss = 0.65, predict loss = 0.15 (69.6 examples/sec; 0.057 sec/batch; 94h:41m:21s remains)
INFO - root - 2019-11-04 00:30:58.174309: step 70930, total loss = 0.41, predict loss = 0.09 (73.3 examples/sec; 0.055 sec/batch; 89h:49m:53s remains)
INFO - root - 2019-11-04 00:30:58.816373: step 70940, total loss = 0.34, predict loss = 0.07 (69.4 examples/sec; 0.058 sec/batch; 94h:51m:52s remains)
INFO - root - 2019-11-04 00:30:59.477931: step 70950, total loss = 0.60, predict loss = 0.14 (72.4 examples/sec; 0.055 sec/batch; 90h:58m:19s remains)
INFO - root - 2019-11-04 00:31:00.163183: step 70960, total loss = 0.33, predict loss = 0.07 (60.3 examples/sec; 0.066 sec/batch; 109h:17m:38s remains)
INFO - root - 2019-11-04 00:31:00.849885: step 70970, total loss = 0.51, predict loss = 0.13 (64.4 examples/sec; 0.062 sec/batch; 102h:13m:08s remains)
INFO - root - 2019-11-04 00:31:01.518219: step 70980, total loss = 0.52, predict loss = 0.11 (66.3 examples/sec; 0.060 sec/batch; 99h:18m:05s remains)
INFO - root - 2019-11-04 00:31:02.179860: step 70990, total loss = 0.70, predict loss = 0.16 (70.2 examples/sec; 0.057 sec/batch; 93h:51m:37s remains)
INFO - root - 2019-11-04 00:31:02.829648: step 71000, total loss = 0.67, predict loss = 0.15 (72.8 examples/sec; 0.055 sec/batch; 90h:27m:01s remains)
INFO - root - 2019-11-04 00:31:03.446127: step 71010, total loss = 0.77, predict loss = 0.17 (86.3 examples/sec; 0.046 sec/batch; 76h:20m:51s remains)
INFO - root - 2019-11-04 00:31:04.060280: step 71020, total loss = 0.69, predict loss = 0.16 (76.3 examples/sec; 0.052 sec/batch; 86h:20m:24s remains)
INFO - root - 2019-11-04 00:31:04.707825: step 71030, total loss = 0.48, predict loss = 0.11 (61.1 examples/sec; 0.065 sec/batch; 107h:51m:04s remains)
INFO - root - 2019-11-04 00:31:05.378958: step 71040, total loss = 0.72, predict loss = 0.17 (72.6 examples/sec; 0.055 sec/batch; 90h:41m:58s remains)
INFO - root - 2019-11-04 00:31:05.993026: step 71050, total loss = 0.46, predict loss = 0.10 (75.1 examples/sec; 0.053 sec/batch; 87h:42m:42s remains)
INFO - root - 2019-11-04 00:31:06.642726: step 71060, total loss = 0.49, predict loss = 0.11 (66.6 examples/sec; 0.060 sec/batch; 98h:58m:11s remains)
INFO - root - 2019-11-04 00:31:07.329415: step 71070, total loss = 0.51, predict loss = 0.12 (82.1 examples/sec; 0.049 sec/batch; 80h:12m:22s remains)
INFO - root - 2019-11-04 00:31:08.019880: step 71080, total loss = 0.51, predict loss = 0.11 (62.6 examples/sec; 0.064 sec/batch; 105h:18m:42s remains)
INFO - root - 2019-11-04 00:31:08.680447: step 71090, total loss = 0.48, predict loss = 0.09 (64.4 examples/sec; 0.062 sec/batch; 102h:21m:52s remains)
INFO - root - 2019-11-04 00:31:09.347290: step 71100, total loss = 0.44, predict loss = 0.10 (69.4 examples/sec; 0.058 sec/batch; 94h:58m:24s remains)
INFO - root - 2019-11-04 00:31:09.952679: step 71110, total loss = 0.54, predict loss = 0.12 (72.2 examples/sec; 0.055 sec/batch; 91h:15m:22s remains)
INFO - root - 2019-11-04 00:31:10.584413: step 71120, total loss = 0.31, predict loss = 0.06 (65.3 examples/sec; 0.061 sec/batch; 100h:51m:45s remains)
INFO - root - 2019-11-04 00:31:11.238427: step 71130, total loss = 0.53, predict loss = 0.12 (76.3 examples/sec; 0.052 sec/batch; 86h:20m:54s remains)
INFO - root - 2019-11-04 00:31:11.905495: step 71140, total loss = 0.41, predict loss = 0.09 (66.5 examples/sec; 0.060 sec/batch; 99h:03m:36s remains)
INFO - root - 2019-11-04 00:31:12.602795: step 71150, total loss = 0.49, predict loss = 0.10 (60.4 examples/sec; 0.066 sec/batch; 109h:07m:41s remains)
INFO - root - 2019-11-04 00:31:13.272589: step 71160, total loss = 0.45, predict loss = 0.09 (75.1 examples/sec; 0.053 sec/batch; 87h:45m:34s remains)
INFO - root - 2019-11-04 00:31:13.914962: step 71170, total loss = 0.54, predict loss = 0.12 (73.1 examples/sec; 0.055 sec/batch; 90h:08m:32s remains)
INFO - root - 2019-11-04 00:31:14.537178: step 71180, total loss = 0.59, predict loss = 0.14 (81.5 examples/sec; 0.049 sec/batch; 80h:48m:25s remains)
INFO - root - 2019-11-04 00:31:15.156018: step 71190, total loss = 0.53, predict loss = 0.11 (74.5 examples/sec; 0.054 sec/batch; 88h:26m:18s remains)
INFO - root - 2019-11-04 00:31:15.802454: step 71200, total loss = 0.75, predict loss = 0.18 (74.6 examples/sec; 0.054 sec/batch; 88h:15m:21s remains)
INFO - root - 2019-11-04 00:31:16.414718: step 71210, total loss = 0.54, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 93h:20m:01s remains)
INFO - root - 2019-11-04 00:31:17.009172: step 71220, total loss = 0.59, predict loss = 0.14 (79.5 examples/sec; 0.050 sec/batch; 82h:50m:33s remains)
INFO - root - 2019-11-04 00:31:17.625093: step 71230, total loss = 0.68, predict loss = 0.16 (69.3 examples/sec; 0.058 sec/batch; 95h:02m:47s remains)
INFO - root - 2019-11-04 00:31:18.237564: step 71240, total loss = 0.58, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 91h:16m:00s remains)
INFO - root - 2019-11-04 00:31:18.864360: step 71250, total loss = 0.64, predict loss = 0.15 (72.1 examples/sec; 0.056 sec/batch; 91h:24m:39s remains)
INFO - root - 2019-11-04 00:31:19.514341: step 71260, total loss = 0.62, predict loss = 0.14 (68.9 examples/sec; 0.058 sec/batch; 95h:37m:23s remains)
INFO - root - 2019-11-04 00:31:20.109201: step 71270, total loss = 0.62, predict loss = 0.14 (82.9 examples/sec; 0.048 sec/batch; 79h:30m:00s remains)
INFO - root - 2019-11-04 00:31:20.784246: step 71280, total loss = 0.51, predict loss = 0.11 (61.7 examples/sec; 0.065 sec/batch; 106h:45m:25s remains)
INFO - root - 2019-11-04 00:31:21.431565: step 71290, total loss = 0.55, predict loss = 0.12 (64.0 examples/sec; 0.063 sec/batch; 103h:00m:07s remains)
INFO - root - 2019-11-04 00:31:22.069346: step 71300, total loss = 0.49, predict loss = 0.11 (66.3 examples/sec; 0.060 sec/batch; 99h:22m:30s remains)
INFO - root - 2019-11-04 00:31:22.761900: step 71310, total loss = 0.44, predict loss = 0.10 (65.1 examples/sec; 0.061 sec/batch; 101h:15m:56s remains)
INFO - root - 2019-11-04 00:31:23.464438: step 71320, total loss = 0.59, predict loss = 0.13 (60.7 examples/sec; 0.066 sec/batch; 108h:34m:54s remains)
INFO - root - 2019-11-04 00:31:24.137591: step 71330, total loss = 0.68, predict loss = 0.15 (66.7 examples/sec; 0.060 sec/batch; 98h:42m:13s remains)
INFO - root - 2019-11-04 00:31:24.792702: step 71340, total loss = 0.60, predict loss = 0.14 (74.4 examples/sec; 0.054 sec/batch; 88h:31m:22s remains)
INFO - root - 2019-11-04 00:31:25.428511: step 71350, total loss = 0.41, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 94h:45m:36s remains)
INFO - root - 2019-11-04 00:31:26.108503: step 71360, total loss = 0.46, predict loss = 0.11 (71.3 examples/sec; 0.056 sec/batch; 92h:21m:37s remains)
INFO - root - 2019-11-04 00:31:26.766430: step 71370, total loss = 0.48, predict loss = 0.11 (65.9 examples/sec; 0.061 sec/batch; 99h:56m:05s remains)
INFO - root - 2019-11-04 00:31:27.426792: step 71380, total loss = 0.49, predict loss = 0.12 (75.0 examples/sec; 0.053 sec/batch; 87h:47m:13s remains)
INFO - root - 2019-11-04 00:31:28.119785: step 71390, total loss = 0.71, predict loss = 0.17 (64.1 examples/sec; 0.062 sec/batch; 102h:41m:42s remains)
INFO - root - 2019-11-04 00:31:28.837298: step 71400, total loss = 0.32, predict loss = 0.07 (70.9 examples/sec; 0.056 sec/batch; 92h:52m:47s remains)
INFO - root - 2019-11-04 00:31:29.486269: step 71410, total loss = 0.70, predict loss = 0.18 (69.4 examples/sec; 0.058 sec/batch; 94h:52m:20s remains)
INFO - root - 2019-11-04 00:31:30.128913: step 71420, total loss = 0.70, predict loss = 0.16 (68.0 examples/sec; 0.059 sec/batch; 96h:52m:59s remains)
INFO - root - 2019-11-04 00:31:30.762130: step 71430, total loss = 0.68, predict loss = 0.16 (72.5 examples/sec; 0.055 sec/batch; 90h:48m:01s remains)
INFO - root - 2019-11-04 00:31:31.393605: step 71440, total loss = 0.62, predict loss = 0.15 (63.0 examples/sec; 0.064 sec/batch; 104h:34m:34s remains)
INFO - root - 2019-11-04 00:31:32.026796: step 71450, total loss = 0.85, predict loss = 0.20 (73.6 examples/sec; 0.054 sec/batch; 89h:31m:04s remains)
INFO - root - 2019-11-04 00:31:32.712542: step 71460, total loss = 0.44, predict loss = 0.11 (70.7 examples/sec; 0.057 sec/batch; 93h:10m:21s remains)
INFO - root - 2019-11-04 00:31:33.375813: step 71470, total loss = 0.50, predict loss = 0.12 (73.7 examples/sec; 0.054 sec/batch; 89h:24m:55s remains)
INFO - root - 2019-11-04 00:31:33.984861: step 71480, total loss = 0.41, predict loss = 0.10 (78.3 examples/sec; 0.051 sec/batch; 84h:10m:52s remains)
INFO - root - 2019-11-04 00:31:34.629932: step 71490, total loss = 0.32, predict loss = 0.07 (77.0 examples/sec; 0.052 sec/batch; 85h:31m:27s remains)
INFO - root - 2019-11-04 00:31:35.286773: step 71500, total loss = 0.39, predict loss = 0.09 (81.3 examples/sec; 0.049 sec/batch; 81h:01m:26s remains)
INFO - root - 2019-11-04 00:31:35.908839: step 71510, total loss = 0.26, predict loss = 0.05 (73.6 examples/sec; 0.054 sec/batch; 89h:33m:34s remains)
INFO - root - 2019-11-04 00:31:36.552684: step 71520, total loss = 0.30, predict loss = 0.06 (75.6 examples/sec; 0.053 sec/batch; 87h:06m:34s remains)
INFO - root - 2019-11-04 00:31:37.164584: step 71530, total loss = 0.51, predict loss = 0.12 (75.2 examples/sec; 0.053 sec/batch; 87h:35m:53s remains)
INFO - root - 2019-11-04 00:31:37.797317: step 71540, total loss = 0.49, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 87h:52m:38s remains)
INFO - root - 2019-11-04 00:31:38.446060: step 71550, total loss = 0.36, predict loss = 0.07 (70.3 examples/sec; 0.057 sec/batch; 93h:39m:02s remains)
INFO - root - 2019-11-04 00:31:39.079443: step 71560, total loss = 0.47, predict loss = 0.10 (71.1 examples/sec; 0.056 sec/batch; 92h:39m:00s remains)
INFO - root - 2019-11-04 00:31:39.756259: step 71570, total loss = 0.50, predict loss = 0.12 (63.3 examples/sec; 0.063 sec/batch; 104h:03m:24s remains)
INFO - root - 2019-11-04 00:31:40.389287: step 71580, total loss = 0.67, predict loss = 0.17 (78.9 examples/sec; 0.051 sec/batch; 83h:31m:27s remains)
INFO - root - 2019-11-04 00:31:41.011325: step 71590, total loss = 0.48, predict loss = 0.11 (76.0 examples/sec; 0.053 sec/batch; 86h:38m:14s remains)
INFO - root - 2019-11-04 00:31:41.627051: step 71600, total loss = 0.53, predict loss = 0.12 (74.9 examples/sec; 0.053 sec/batch; 87h:59m:26s remains)
INFO - root - 2019-11-04 00:31:42.225714: step 71610, total loss = 0.64, predict loss = 0.15 (69.2 examples/sec; 0.058 sec/batch; 95h:14m:19s remains)
INFO - root - 2019-11-04 00:31:42.856228: step 71620, total loss = 0.54, predict loss = 0.12 (67.6 examples/sec; 0.059 sec/batch; 97h:25m:05s remains)
INFO - root - 2019-11-04 00:31:43.475766: step 71630, total loss = 0.56, predict loss = 0.13 (69.6 examples/sec; 0.057 sec/batch; 94h:38m:32s remains)
INFO - root - 2019-11-04 00:31:44.121804: step 71640, total loss = 0.60, predict loss = 0.14 (75.3 examples/sec; 0.053 sec/batch; 87h:30m:21s remains)
INFO - root - 2019-11-04 00:31:44.722265: step 71650, total loss = 0.52, predict loss = 0.12 (76.8 examples/sec; 0.052 sec/batch; 85h:49m:25s remains)
INFO - root - 2019-11-04 00:31:45.331415: step 71660, total loss = 0.43, predict loss = 0.09 (70.0 examples/sec; 0.057 sec/batch; 94h:06m:55s remains)
INFO - root - 2019-11-04 00:31:45.940966: step 71670, total loss = 0.50, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 90h:46m:42s remains)
INFO - root - 2019-11-04 00:31:46.572797: step 71680, total loss = 0.59, predict loss = 0.14 (69.5 examples/sec; 0.058 sec/batch; 94h:46m:06s remains)
INFO - root - 2019-11-04 00:31:47.194452: step 71690, total loss = 0.58, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 95h:23m:00s remains)
INFO - root - 2019-11-04 00:31:47.853832: step 71700, total loss = 0.70, predict loss = 0.17 (66.7 examples/sec; 0.060 sec/batch; 98h:48m:04s remains)
INFO - root - 2019-11-04 00:31:48.490341: step 71710, total loss = 0.47, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 99h:09m:13s remains)
INFO - root - 2019-11-04 00:31:49.104708: step 71720, total loss = 0.36, predict loss = 0.09 (75.3 examples/sec; 0.053 sec/batch; 87h:27m:38s remains)
INFO - root - 2019-11-04 00:31:49.734919: step 71730, total loss = 0.35, predict loss = 0.07 (68.7 examples/sec; 0.058 sec/batch; 95h:54m:02s remains)
INFO - root - 2019-11-04 00:31:50.369526: step 71740, total loss = 0.30, predict loss = 0.06 (70.4 examples/sec; 0.057 sec/batch; 93h:33m:15s remains)
INFO - root - 2019-11-04 00:31:51.039883: step 71750, total loss = 0.25, predict loss = 0.05 (69.7 examples/sec; 0.057 sec/batch; 94h:30m:56s remains)
INFO - root - 2019-11-04 00:31:51.725035: step 71760, total loss = 0.58, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:10m:45s remains)
INFO - root - 2019-11-04 00:31:52.376538: step 71770, total loss = 0.33, predict loss = 0.07 (70.3 examples/sec; 0.057 sec/batch; 93h:40m:52s remains)
INFO - root - 2019-11-04 00:31:52.972747: step 71780, total loss = 0.27, predict loss = 0.06 (80.5 examples/sec; 0.050 sec/batch; 81h:48m:28s remains)
INFO - root - 2019-11-04 00:31:53.589955: step 71790, total loss = 0.30, predict loss = 0.06 (72.4 examples/sec; 0.055 sec/batch; 91h:01m:42s remains)
INFO - root - 2019-11-04 00:31:54.214606: step 71800, total loss = 0.38, predict loss = 0.08 (71.9 examples/sec; 0.056 sec/batch; 91h:38m:39s remains)
INFO - root - 2019-11-04 00:31:54.857084: step 71810, total loss = 0.39, predict loss = 0.08 (70.2 examples/sec; 0.057 sec/batch; 93h:52m:01s remains)
INFO - root - 2019-11-04 00:31:55.469108: step 71820, total loss = 0.36, predict loss = 0.08 (63.3 examples/sec; 0.063 sec/batch; 104h:02m:53s remains)
INFO - root - 2019-11-04 00:31:56.053502: step 71830, total loss = 0.32, predict loss = 0.07 (80.4 examples/sec; 0.050 sec/batch; 81h:57m:18s remains)
INFO - root - 2019-11-04 00:31:56.656121: step 71840, total loss = 0.42, predict loss = 0.09 (73.1 examples/sec; 0.055 sec/batch; 90h:05m:55s remains)
INFO - root - 2019-11-04 00:31:57.322746: step 71850, total loss = 0.25, predict loss = 0.05 (67.7 examples/sec; 0.059 sec/batch; 97h:14m:53s remains)
INFO - root - 2019-11-04 00:31:57.950864: step 71860, total loss = 0.39, predict loss = 0.09 (77.0 examples/sec; 0.052 sec/batch; 85h:34m:55s remains)
INFO - root - 2019-11-04 00:31:58.574329: step 71870, total loss = 0.47, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 96h:14m:13s remains)
INFO - root - 2019-11-04 00:31:59.229776: step 71880, total loss = 0.36, predict loss = 0.08 (64.2 examples/sec; 0.062 sec/batch; 102h:38m:49s remains)
INFO - root - 2019-11-04 00:31:59.857836: step 71890, total loss = 0.54, predict loss = 0.13 (80.0 examples/sec; 0.050 sec/batch; 82h:21m:30s remains)
INFO - root - 2019-11-04 00:32:00.520209: step 71900, total loss = 0.58, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 101h:12m:01s remains)
INFO - root - 2019-11-04 00:32:01.166310: step 71910, total loss = 0.47, predict loss = 0.10 (66.6 examples/sec; 0.060 sec/batch; 98h:58m:07s remains)
INFO - root - 2019-11-04 00:32:01.820210: step 71920, total loss = 0.53, predict loss = 0.13 (63.1 examples/sec; 0.063 sec/batch; 104h:23m:08s remains)
INFO - root - 2019-11-04 00:32:02.468445: step 71930, total loss = 0.74, predict loss = 0.19 (73.8 examples/sec; 0.054 sec/batch; 89h:18m:35s remains)
INFO - root - 2019-11-04 00:32:03.103634: step 71940, total loss = 0.55, predict loss = 0.13 (64.5 examples/sec; 0.062 sec/batch; 102h:09m:33s remains)
INFO - root - 2019-11-04 00:32:03.726944: step 71950, total loss = 0.47, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 93h:21m:02s remains)
INFO - root - 2019-11-04 00:32:04.355699: step 71960, total loss = 0.39, predict loss = 0.09 (70.2 examples/sec; 0.057 sec/batch; 93h:49m:43s remains)
INFO - root - 2019-11-04 00:32:05.030876: step 71970, total loss = 0.49, predict loss = 0.11 (59.4 examples/sec; 0.067 sec/batch; 110h:55m:38s remains)
INFO - root - 2019-11-04 00:32:05.628869: step 71980, total loss = 0.37, predict loss = 0.08 (78.9 examples/sec; 0.051 sec/batch; 83h:26m:30s remains)
INFO - root - 2019-11-04 00:32:06.252272: step 71990, total loss = 0.49, predict loss = 0.11 (84.7 examples/sec; 0.047 sec/batch; 77h:48m:22s remains)
INFO - root - 2019-11-04 00:32:06.884441: step 72000, total loss = 0.29, predict loss = 0.07 (72.5 examples/sec; 0.055 sec/batch; 90h:53m:41s remains)
INFO - root - 2019-11-04 00:32:07.518518: step 72010, total loss = 0.27, predict loss = 0.05 (64.2 examples/sec; 0.062 sec/batch; 102h:33m:44s remains)
INFO - root - 2019-11-04 00:32:08.155711: step 72020, total loss = 0.32, predict loss = 0.07 (78.8 examples/sec; 0.051 sec/batch; 83h:36m:46s remains)
INFO - root - 2019-11-04 00:32:08.774988: step 72030, total loss = 0.48, predict loss = 0.11 (78.9 examples/sec; 0.051 sec/batch; 83h:31m:43s remains)
INFO - root - 2019-11-04 00:32:09.409215: step 72040, total loss = 0.33, predict loss = 0.07 (67.8 examples/sec; 0.059 sec/batch; 97h:05m:08s remains)
INFO - root - 2019-11-04 00:32:10.063706: step 72050, total loss = 0.59, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:08m:21s remains)
INFO - root - 2019-11-04 00:32:10.674561: step 72060, total loss = 0.50, predict loss = 0.12 (72.1 examples/sec; 0.056 sec/batch; 91h:23m:39s remains)
INFO - root - 2019-11-04 00:32:11.304555: step 72070, total loss = 0.55, predict loss = 0.12 (67.6 examples/sec; 0.059 sec/batch; 97h:25m:41s remains)
INFO - root - 2019-11-04 00:32:11.971803: step 72080, total loss = 0.47, predict loss = 0.11 (73.8 examples/sec; 0.054 sec/batch; 89h:16m:43s remains)
INFO - root - 2019-11-04 00:32:12.631027: step 72090, total loss = 0.58, predict loss = 0.14 (70.1 examples/sec; 0.057 sec/batch; 93h:55m:46s remains)
INFO - root - 2019-11-04 00:32:13.269183: step 72100, total loss = 0.52, predict loss = 0.12 (69.7 examples/sec; 0.057 sec/batch; 94h:31m:08s remains)
INFO - root - 2019-11-04 00:32:13.937254: step 72110, total loss = 0.31, predict loss = 0.06 (70.5 examples/sec; 0.057 sec/batch; 93h:24m:57s remains)
INFO - root - 2019-11-04 00:32:14.624819: step 72120, total loss = 0.48, predict loss = 0.11 (67.2 examples/sec; 0.060 sec/batch; 97h:58m:31s remains)
INFO - root - 2019-11-04 00:32:15.252851: step 72130, total loss = 0.44, predict loss = 0.10 (73.8 examples/sec; 0.054 sec/batch; 89h:11m:19s remains)
INFO - root - 2019-11-04 00:32:15.847385: step 72140, total loss = 0.46, predict loss = 0.10 (79.8 examples/sec; 0.050 sec/batch; 82h:33m:02s remains)
INFO - root - 2019-11-04 00:32:16.451370: step 72150, total loss = 0.51, predict loss = 0.12 (76.1 examples/sec; 0.053 sec/batch; 86h:29m:46s remains)
INFO - root - 2019-11-04 00:32:17.133573: step 72160, total loss = 0.62, predict loss = 0.14 (62.3 examples/sec; 0.064 sec/batch; 105h:43m:08s remains)
INFO - root - 2019-11-04 00:32:17.811243: step 72170, total loss = 0.87, predict loss = 0.21 (62.2 examples/sec; 0.064 sec/batch; 105h:55m:57s remains)
INFO - root - 2019-11-04 00:32:18.482541: step 72180, total loss = 0.66, predict loss = 0.15 (73.7 examples/sec; 0.054 sec/batch; 89h:23m:21s remains)
INFO - root - 2019-11-04 00:32:19.136637: step 72190, total loss = 0.64, predict loss = 0.15 (69.8 examples/sec; 0.057 sec/batch; 94h:18m:30s remains)
INFO - root - 2019-11-04 00:32:19.791618: step 72200, total loss = 0.43, predict loss = 0.09 (64.0 examples/sec; 0.063 sec/batch; 102h:58m:52s remains)
INFO - root - 2019-11-04 00:32:20.463011: step 72210, total loss = 0.69, predict loss = 0.16 (73.7 examples/sec; 0.054 sec/batch; 89h:22m:40s remains)
INFO - root - 2019-11-04 00:32:21.100611: step 72220, total loss = 0.54, predict loss = 0.12 (66.8 examples/sec; 0.060 sec/batch; 98h:31m:36s remains)
INFO - root - 2019-11-04 00:32:21.709600: step 72230, total loss = 0.54, predict loss = 0.12 (73.4 examples/sec; 0.054 sec/batch; 89h:42m:52s remains)
INFO - root - 2019-11-04 00:32:22.322146: step 72240, total loss = 0.39, predict loss = 0.08 (67.8 examples/sec; 0.059 sec/batch; 97h:06m:11s remains)
INFO - root - 2019-11-04 00:32:22.927444: step 72250, total loss = 0.52, predict loss = 0.13 (81.8 examples/sec; 0.049 sec/batch; 80h:28m:42s remains)
INFO - root - 2019-11-04 00:32:23.579235: step 72260, total loss = 0.30, predict loss = 0.07 (66.6 examples/sec; 0.060 sec/batch; 98h:56m:34s remains)
INFO - root - 2019-11-04 00:32:24.228382: step 72270, total loss = 0.40, predict loss = 0.09 (68.4 examples/sec; 0.059 sec/batch; 96h:21m:43s remains)
INFO - root - 2019-11-04 00:32:24.863033: step 72280, total loss = 0.22, predict loss = 0.04 (66.8 examples/sec; 0.060 sec/batch; 98h:32m:32s remains)
INFO - root - 2019-11-04 00:32:25.520341: step 72290, total loss = 0.44, predict loss = 0.10 (62.1 examples/sec; 0.064 sec/batch; 106h:01m:31s remains)
INFO - root - 2019-11-04 00:32:26.136054: step 72300, total loss = 0.48, predict loss = 0.10 (66.2 examples/sec; 0.060 sec/batch; 99h:29m:42s remains)
INFO - root - 2019-11-04 00:32:26.794813: step 72310, total loss = 0.50, predict loss = 0.12 (81.9 examples/sec; 0.049 sec/batch; 80h:23m:04s remains)
INFO - root - 2019-11-04 00:32:27.472293: step 72320, total loss = 0.51, predict loss = 0.11 (65.2 examples/sec; 0.061 sec/batch; 101h:02m:08s remains)
INFO - root - 2019-11-04 00:32:28.105410: step 72330, total loss = 1.02, predict loss = 0.24 (80.5 examples/sec; 0.050 sec/batch; 81h:47m:04s remains)
INFO - root - 2019-11-04 00:32:28.782902: step 72340, total loss = 0.55, predict loss = 0.12 (63.2 examples/sec; 0.063 sec/batch; 104h:16m:45s remains)
INFO - root - 2019-11-04 00:32:29.447627: step 72350, total loss = 0.76, predict loss = 0.19 (62.4 examples/sec; 0.064 sec/batch; 105h:29m:26s remains)
INFO - root - 2019-11-04 00:32:30.056992: step 72360, total loss = 0.88, predict loss = 0.19 (72.5 examples/sec; 0.055 sec/batch; 90h:54m:20s remains)
INFO - root - 2019-11-04 00:32:30.696641: step 72370, total loss = 0.77, predict loss = 0.18 (70.8 examples/sec; 0.056 sec/batch; 92h:58m:57s remains)
INFO - root - 2019-11-04 00:32:31.338878: step 72380, total loss = 0.95, predict loss = 0.23 (62.9 examples/sec; 0.064 sec/batch; 104h:45m:56s remains)
INFO - root - 2019-11-04 00:32:32.002328: step 72390, total loss = 0.75, predict loss = 0.17 (64.2 examples/sec; 0.062 sec/batch; 102h:35m:49s remains)
INFO - root - 2019-11-04 00:32:32.669776: step 72400, total loss = 0.74, predict loss = 0.18 (69.4 examples/sec; 0.058 sec/batch; 94h:53m:40s remains)
INFO - root - 2019-11-04 00:32:33.328021: step 72410, total loss = 0.71, predict loss = 0.17 (68.8 examples/sec; 0.058 sec/batch; 95h:46m:01s remains)
INFO - root - 2019-11-04 00:32:33.977996: step 72420, total loss = 0.66, predict loss = 0.16 (66.8 examples/sec; 0.060 sec/batch; 98h:33m:57s remains)
INFO - root - 2019-11-04 00:32:34.606088: step 72430, total loss = 0.63, predict loss = 0.13 (65.7 examples/sec; 0.061 sec/batch; 100h:15m:13s remains)
INFO - root - 2019-11-04 00:32:35.287010: step 72440, total loss = 0.53, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 91h:56m:57s remains)
INFO - root - 2019-11-04 00:32:35.899228: step 72450, total loss = 0.59, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 100h:47m:28s remains)
INFO - root - 2019-11-04 00:32:36.554036: step 72460, total loss = 0.52, predict loss = 0.12 (84.9 examples/sec; 0.047 sec/batch; 77h:34m:53s remains)
INFO - root - 2019-11-04 00:32:37.175118: step 72470, total loss = 0.48, predict loss = 0.11 (61.4 examples/sec; 0.065 sec/batch; 107h:11m:53s remains)
INFO - root - 2019-11-04 00:32:37.836442: step 72480, total loss = 0.58, predict loss = 0.14 (65.8 examples/sec; 0.061 sec/batch; 100h:02m:07s remains)
INFO - root - 2019-11-04 00:32:38.499197: step 72490, total loss = 0.47, predict loss = 0.11 (62.3 examples/sec; 0.064 sec/batch; 105h:37m:55s remains)
INFO - root - 2019-11-04 00:32:39.123327: step 72500, total loss = 0.65, predict loss = 0.16 (72.3 examples/sec; 0.055 sec/batch; 91h:01m:53s remains)
INFO - root - 2019-11-04 00:32:39.786281: step 72510, total loss = 0.54, predict loss = 0.12 (68.6 examples/sec; 0.058 sec/batch; 95h:57m:48s remains)
INFO - root - 2019-11-04 00:32:40.411878: step 72520, total loss = 0.55, predict loss = 0.13 (74.1 examples/sec; 0.054 sec/batch; 88h:55m:04s remains)
INFO - root - 2019-11-04 00:32:41.069291: step 72530, total loss = 0.65, predict loss = 0.15 (67.1 examples/sec; 0.060 sec/batch; 98h:12m:00s remains)
INFO - root - 2019-11-04 00:32:41.694676: step 72540, total loss = 0.60, predict loss = 0.13 (70.6 examples/sec; 0.057 sec/batch; 93h:18m:50s remains)
INFO - root - 2019-11-04 00:32:42.319277: step 72550, total loss = 0.59, predict loss = 0.14 (72.4 examples/sec; 0.055 sec/batch; 91h:00m:04s remains)
INFO - root - 2019-11-04 00:32:42.946013: step 72560, total loss = 0.49, predict loss = 0.11 (82.2 examples/sec; 0.049 sec/batch; 80h:09m:09s remains)
INFO - root - 2019-11-04 00:32:43.557825: step 72570, total loss = 0.46, predict loss = 0.10 (64.8 examples/sec; 0.062 sec/batch; 101h:41m:24s remains)
INFO - root - 2019-11-04 00:32:44.182234: step 72580, total loss = 0.35, predict loss = 0.08 (78.9 examples/sec; 0.051 sec/batch; 83h:29m:35s remains)
INFO - root - 2019-11-04 00:32:44.836769: step 72590, total loss = 0.32, predict loss = 0.07 (62.3 examples/sec; 0.064 sec/batch; 105h:42m:46s remains)
INFO - root - 2019-11-04 00:32:45.524039: step 72600, total loss = 0.37, predict loss = 0.08 (66.4 examples/sec; 0.060 sec/batch; 99h:09m:44s remains)
INFO - root - 2019-11-04 00:32:46.199831: step 72610, total loss = 0.36, predict loss = 0.08 (64.8 examples/sec; 0.062 sec/batch; 101h:40m:06s remains)
INFO - root - 2019-11-04 00:32:46.865993: step 72620, total loss = 0.51, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 96h:29m:39s remains)
INFO - root - 2019-11-04 00:32:47.516908: step 72630, total loss = 0.33, predict loss = 0.07 (68.4 examples/sec; 0.058 sec/batch; 96h:17m:41s remains)
INFO - root - 2019-11-04 00:32:48.151096: step 72640, total loss = 0.45, predict loss = 0.10 (79.3 examples/sec; 0.050 sec/batch; 83h:02m:18s remains)
INFO - root - 2019-11-04 00:32:48.786408: step 72650, total loss = 0.47, predict loss = 0.11 (73.3 examples/sec; 0.055 sec/batch; 89h:47m:44s remains)
INFO - root - 2019-11-04 00:32:49.407535: step 72660, total loss = 0.45, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 96h:45m:02s remains)
INFO - root - 2019-11-04 00:32:50.050225: step 72670, total loss = 0.44, predict loss = 0.10 (69.0 examples/sec; 0.058 sec/batch; 95h:27m:38s remains)
INFO - root - 2019-11-04 00:32:50.686825: step 72680, total loss = 0.51, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 97h:08m:39s remains)
INFO - root - 2019-11-04 00:32:51.311257: step 72690, total loss = 0.47, predict loss = 0.11 (77.9 examples/sec; 0.051 sec/batch; 84h:32m:15s remains)
INFO - root - 2019-11-04 00:32:51.984309: step 72700, total loss = 0.44, predict loss = 0.10 (57.7 examples/sec; 0.069 sec/batch; 114h:03m:43s remains)
INFO - root - 2019-11-04 00:32:52.647582: step 72710, total loss = 0.53, predict loss = 0.12 (75.5 examples/sec; 0.053 sec/batch; 87h:14m:29s remains)
INFO - root - 2019-11-04 00:32:53.295954: step 72720, total loss = 0.67, predict loss = 0.16 (71.3 examples/sec; 0.056 sec/batch; 92h:19m:51s remains)
INFO - root - 2019-11-04 00:32:53.918773: step 72730, total loss = 0.48, predict loss = 0.11 (72.6 examples/sec; 0.055 sec/batch; 90h:43m:23s remains)
INFO - root - 2019-11-04 00:32:54.528453: step 72740, total loss = 0.52, predict loss = 0.12 (76.9 examples/sec; 0.052 sec/batch; 85h:41m:33s remains)
INFO - root - 2019-11-04 00:32:55.127970: step 72750, total loss = 0.61, predict loss = 0.15 (78.4 examples/sec; 0.051 sec/batch; 83h:57m:40s remains)
INFO - root - 2019-11-04 00:32:55.747028: step 72760, total loss = 0.44, predict loss = 0.10 (73.3 examples/sec; 0.055 sec/batch; 89h:54m:04s remains)
INFO - root - 2019-11-04 00:32:56.399439: step 72770, total loss = 0.58, predict loss = 0.13 (76.7 examples/sec; 0.052 sec/batch; 85h:54m:45s remains)
INFO - root - 2019-11-04 00:32:57.016717: step 72780, total loss = 0.63, predict loss = 0.15 (68.3 examples/sec; 0.059 sec/batch; 96h:22m:41s remains)
INFO - root - 2019-11-04 00:32:57.635551: step 72790, total loss = 0.74, predict loss = 0.18 (68.1 examples/sec; 0.059 sec/batch; 96h:39m:13s remains)
INFO - root - 2019-11-04 00:32:58.301510: step 72800, total loss = 0.61, predict loss = 0.14 (63.7 examples/sec; 0.063 sec/batch; 103h:23m:02s remains)
INFO - root - 2019-11-04 00:32:58.926747: step 72810, total loss = 0.58, predict loss = 0.13 (68.6 examples/sec; 0.058 sec/batch; 96h:04m:03s remains)
INFO - root - 2019-11-04 00:32:59.560714: step 72820, total loss = 0.69, predict loss = 0.17 (64.8 examples/sec; 0.062 sec/batch; 101h:33m:17s remains)
INFO - root - 2019-11-04 00:33:00.198937: step 72830, total loss = 0.66, predict loss = 0.15 (68.3 examples/sec; 0.059 sec/batch; 96h:25m:22s remains)
INFO - root - 2019-11-04 00:33:00.882916: step 72840, total loss = 0.57, predict loss = 0.14 (63.0 examples/sec; 0.063 sec/batch; 104h:27m:36s remains)
INFO - root - 2019-11-04 00:33:01.496716: step 72850, total loss = 0.49, predict loss = 0.11 (74.2 examples/sec; 0.054 sec/batch; 88h:46m:06s remains)
INFO - root - 2019-11-04 00:33:02.136435: step 72860, total loss = 0.55, predict loss = 0.13 (69.0 examples/sec; 0.058 sec/batch; 95h:25m:14s remains)
INFO - root - 2019-11-04 00:33:02.800553: step 72870, total loss = 0.51, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 96h:25m:23s remains)
INFO - root - 2019-11-04 00:33:03.435379: step 72880, total loss = 0.46, predict loss = 0.11 (83.1 examples/sec; 0.048 sec/batch; 79h:15m:02s remains)
INFO - root - 2019-11-04 00:33:04.067836: step 72890, total loss = 0.46, predict loss = 0.10 (72.7 examples/sec; 0.055 sec/batch; 90h:33m:18s remains)
INFO - root - 2019-11-04 00:33:04.693877: step 72900, total loss = 0.35, predict loss = 0.07 (61.7 examples/sec; 0.065 sec/batch; 106h:48m:20s remains)
INFO - root - 2019-11-04 00:33:05.356680: step 72910, total loss = 0.46, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 90h:11m:14s remains)
INFO - root - 2019-11-04 00:33:05.934994: step 72920, total loss = 0.47, predict loss = 0.11 (80.5 examples/sec; 0.050 sec/batch; 81h:45m:31s remains)
INFO - root - 2019-11-04 00:33:06.580955: step 72930, total loss = 0.48, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 97h:14m:18s remains)
INFO - root - 2019-11-04 00:33:07.199360: step 72940, total loss = 0.43, predict loss = 0.10 (77.5 examples/sec; 0.052 sec/batch; 84h:56m:45s remains)
INFO - root - 2019-11-04 00:33:07.817199: step 72950, total loss = 0.59, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 101h:06m:11s remains)
INFO - root - 2019-11-04 00:33:08.476236: step 72960, total loss = 0.58, predict loss = 0.14 (67.5 examples/sec; 0.059 sec/batch; 97h:34m:36s remains)
INFO - root - 2019-11-04 00:33:09.122058: step 72970, total loss = 0.63, predict loss = 0.16 (65.1 examples/sec; 0.061 sec/batch; 101h:13m:05s remains)
INFO - root - 2019-11-04 00:33:09.747227: step 72980, total loss = 0.67, predict loss = 0.17 (64.5 examples/sec; 0.062 sec/batch; 102h:02m:45s remains)
INFO - root - 2019-11-04 00:33:10.406098: step 72990, total loss = 0.63, predict loss = 0.15 (64.2 examples/sec; 0.062 sec/batch; 102h:35m:14s remains)
INFO - root - 2019-11-04 00:33:11.035743: step 73000, total loss = 0.51, predict loss = 0.11 (82.4 examples/sec; 0.049 sec/batch; 79h:54m:51s remains)
INFO - root - 2019-11-04 00:33:11.680864: step 73010, total loss = 0.63, predict loss = 0.15 (76.5 examples/sec; 0.052 sec/batch; 86h:03m:05s remains)
INFO - root - 2019-11-04 00:33:12.319718: step 73020, total loss = 0.58, predict loss = 0.14 (65.8 examples/sec; 0.061 sec/batch; 100h:03m:38s remains)
INFO - root - 2019-11-04 00:33:12.976923: step 73030, total loss = 0.59, predict loss = 0.14 (68.6 examples/sec; 0.058 sec/batch; 96h:00m:08s remains)
INFO - root - 2019-11-04 00:33:13.594442: step 73040, total loss = 0.59, predict loss = 0.15 (75.6 examples/sec; 0.053 sec/batch; 87h:07m:21s remains)
INFO - root - 2019-11-04 00:33:14.208087: step 73050, total loss = 0.63, predict loss = 0.16 (78.8 examples/sec; 0.051 sec/batch; 83h:33m:17s remains)
INFO - root - 2019-11-04 00:33:14.858373: step 73060, total loss = 0.62, predict loss = 0.15 (71.2 examples/sec; 0.056 sec/batch; 92h:31m:18s remains)
INFO - root - 2019-11-04 00:33:15.455032: step 73070, total loss = 0.58, predict loss = 0.15 (86.4 examples/sec; 0.046 sec/batch; 76h:15m:42s remains)
INFO - root - 2019-11-04 00:33:16.075013: step 73080, total loss = 0.51, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 92h:22m:05s remains)
INFO - root - 2019-11-04 00:33:16.714330: step 73090, total loss = 0.51, predict loss = 0.12 (73.5 examples/sec; 0.054 sec/batch; 89h:33m:04s remains)
INFO - root - 2019-11-04 00:33:17.371124: step 73100, total loss = 0.55, predict loss = 0.12 (67.5 examples/sec; 0.059 sec/batch; 97h:34m:11s remains)
INFO - root - 2019-11-04 00:33:18.047347: step 73110, total loss = 0.52, predict loss = 0.12 (60.1 examples/sec; 0.067 sec/batch; 109h:33m:36s remains)
INFO - root - 2019-11-04 00:33:18.718888: step 73120, total loss = 0.45, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 97h:44m:43s remains)
INFO - root - 2019-11-04 00:33:19.370298: step 73130, total loss = 0.60, predict loss = 0.14 (68.3 examples/sec; 0.059 sec/batch; 96h:23m:51s remains)
INFO - root - 2019-11-04 00:33:20.024715: step 73140, total loss = 0.58, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 95h:43m:36s remains)
INFO - root - 2019-11-04 00:33:20.646225: step 73150, total loss = 0.53, predict loss = 0.12 (67.2 examples/sec; 0.060 sec/batch; 97h:58m:12s remains)
INFO - root - 2019-11-04 00:33:21.272300: step 73160, total loss = 0.47, predict loss = 0.11 (62.3 examples/sec; 0.064 sec/batch; 105h:42m:33s remains)
INFO - root - 2019-11-04 00:33:21.903033: step 73170, total loss = 0.52, predict loss = 0.12 (62.8 examples/sec; 0.064 sec/batch; 104h:51m:40s remains)
INFO - root - 2019-11-04 00:33:22.561539: step 73180, total loss = 0.42, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 92h:51m:34s remains)
INFO - root - 2019-11-04 00:33:23.202298: step 73190, total loss = 0.45, predict loss = 0.10 (74.1 examples/sec; 0.054 sec/batch; 88h:51m:48s remains)
INFO - root - 2019-11-04 00:33:23.903208: step 73200, total loss = 0.45, predict loss = 0.10 (63.1 examples/sec; 0.063 sec/batch; 104h:21m:37s remains)
INFO - root - 2019-11-04 00:33:24.577235: step 73210, total loss = 0.50, predict loss = 0.11 (64.3 examples/sec; 0.062 sec/batch; 102h:23m:32s remains)
INFO - root - 2019-11-04 00:33:25.250869: step 73220, total loss = 0.40, predict loss = 0.09 (67.8 examples/sec; 0.059 sec/batch; 97h:05m:20s remains)
INFO - root - 2019-11-04 00:33:25.950600: step 73230, total loss = 0.55, predict loss = 0.13 (63.2 examples/sec; 0.063 sec/batch; 104h:14m:21s remains)
INFO - root - 2019-11-04 00:33:26.621601: step 73240, total loss = 0.51, predict loss = 0.12 (65.4 examples/sec; 0.061 sec/batch; 100h:37m:59s remains)
INFO - root - 2019-11-04 00:33:27.324328: step 73250, total loss = 0.54, predict loss = 0.12 (62.9 examples/sec; 0.064 sec/batch; 104h:43m:37s remains)
INFO - root - 2019-11-04 00:33:28.025365: step 73260, total loss = 0.51, predict loss = 0.12 (59.6 examples/sec; 0.067 sec/batch; 110h:31m:19s remains)
INFO - root - 2019-11-04 00:33:28.720070: step 73270, total loss = 0.51, predict loss = 0.12 (74.0 examples/sec; 0.054 sec/batch; 88h:59m:22s remains)
INFO - root - 2019-11-04 00:33:29.408480: step 73280, total loss = 0.54, predict loss = 0.13 (73.7 examples/sec; 0.054 sec/batch; 89h:23m:14s remains)
INFO - root - 2019-11-04 00:33:30.045984: step 73290, total loss = 0.51, predict loss = 0.12 (74.4 examples/sec; 0.054 sec/batch; 88h:31m:11s remains)
INFO - root - 2019-11-04 00:33:30.692534: step 73300, total loss = 0.55, predict loss = 0.12 (77.5 examples/sec; 0.052 sec/batch; 84h:59m:28s remains)
INFO - root - 2019-11-04 00:33:31.344141: step 73310, total loss = 0.60, predict loss = 0.15 (68.8 examples/sec; 0.058 sec/batch; 95h:42m:11s remains)
INFO - root - 2019-11-04 00:33:31.993008: step 73320, total loss = 0.58, predict loss = 0.14 (73.8 examples/sec; 0.054 sec/batch; 89h:12m:29s remains)
INFO - root - 2019-11-04 00:33:32.596636: step 73330, total loss = 0.49, predict loss = 0.11 (73.3 examples/sec; 0.055 sec/batch; 89h:52m:49s remains)
INFO - root - 2019-11-04 00:33:33.214173: step 73340, total loss = 0.61, predict loss = 0.14 (66.1 examples/sec; 0.061 sec/batch; 99h:39m:31s remains)
INFO - root - 2019-11-04 00:33:33.840103: step 73350, total loss = 0.42, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 93h:12m:41s remains)
INFO - root - 2019-11-04 00:33:34.503965: step 73360, total loss = 0.47, predict loss = 0.11 (66.7 examples/sec; 0.060 sec/batch; 98h:47m:54s remains)
INFO - root - 2019-11-04 00:33:35.179987: step 73370, total loss = 0.59, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 91h:08m:44s remains)
INFO - root - 2019-11-04 00:33:35.841006: step 73380, total loss = 0.72, predict loss = 0.17 (69.2 examples/sec; 0.058 sec/batch; 95h:08m:54s remains)
INFO - root - 2019-11-04 00:33:36.458531: step 73390, total loss = 0.78, predict loss = 0.19 (70.9 examples/sec; 0.056 sec/batch; 92h:51m:49s remains)
INFO - root - 2019-11-04 00:33:37.087709: step 73400, total loss = 0.79, predict loss = 0.19 (64.5 examples/sec; 0.062 sec/batch; 102h:07m:28s remains)
INFO - root - 2019-11-04 00:33:37.777154: step 73410, total loss = 0.65, predict loss = 0.15 (58.6 examples/sec; 0.068 sec/batch; 112h:18m:45s remains)
INFO - root - 2019-11-04 00:33:38.514824: step 73420, total loss = 0.69, predict loss = 0.17 (73.2 examples/sec; 0.055 sec/batch; 89h:59m:17s remains)
INFO - root - 2019-11-04 00:33:39.126686: step 73430, total loss = 0.67, predict loss = 0.17 (80.9 examples/sec; 0.049 sec/batch; 81h:26m:11s remains)
INFO - root - 2019-11-04 00:33:39.734292: step 73440, total loss = 0.71, predict loss = 0.17 (75.4 examples/sec; 0.053 sec/batch; 87h:21m:17s remains)
INFO - root - 2019-11-04 00:33:40.347179: step 73450, total loss = 0.81, predict loss = 0.20 (73.2 examples/sec; 0.055 sec/batch; 89h:57m:01s remains)
INFO - root - 2019-11-04 00:33:40.996607: step 73460, total loss = 0.54, predict loss = 0.13 (65.6 examples/sec; 0.061 sec/batch; 100h:26m:59s remains)
INFO - root - 2019-11-04 00:33:41.630909: step 73470, total loss = 0.41, predict loss = 0.09 (64.5 examples/sec; 0.062 sec/batch; 102h:06m:06s remains)
INFO - root - 2019-11-04 00:33:42.304342: step 73480, total loss = 0.63, predict loss = 0.15 (64.9 examples/sec; 0.062 sec/batch; 101h:27m:07s remains)
INFO - root - 2019-11-04 00:33:42.997635: step 73490, total loss = 0.65, predict loss = 0.16 (61.8 examples/sec; 0.065 sec/batch; 106h:36m:51s remains)
INFO - root - 2019-11-04 00:33:43.673918: step 73500, total loss = 0.47, predict loss = 0.10 (57.8 examples/sec; 0.069 sec/batch; 113h:54m:09s remains)
INFO - root - 2019-11-04 00:33:44.309060: step 73510, total loss = 0.54, predict loss = 0.13 (70.6 examples/sec; 0.057 sec/batch; 93h:13m:57s remains)
INFO - root - 2019-11-04 00:33:44.989844: step 73520, total loss = 0.61, predict loss = 0.15 (57.4 examples/sec; 0.070 sec/batch; 114h:42m:12s remains)
INFO - root - 2019-11-04 00:33:45.696429: step 73530, total loss = 0.58, predict loss = 0.14 (61.5 examples/sec; 0.065 sec/batch; 107h:07m:44s remains)
INFO - root - 2019-11-04 00:33:46.377103: step 73540, total loss = 0.51, predict loss = 0.11 (65.5 examples/sec; 0.061 sec/batch; 100h:31m:48s remains)
INFO - root - 2019-11-04 00:33:47.048030: step 73550, total loss = 0.73, predict loss = 0.17 (68.7 examples/sec; 0.058 sec/batch; 95h:54m:51s remains)
INFO - root - 2019-11-04 00:33:47.728035: step 73560, total loss = 0.55, predict loss = 0.13 (64.9 examples/sec; 0.062 sec/batch; 101h:24m:31s remains)
INFO - root - 2019-11-04 00:33:48.366574: step 73570, total loss = 0.64, predict loss = 0.15 (70.5 examples/sec; 0.057 sec/batch; 93h:26m:51s remains)
INFO - root - 2019-11-04 00:33:49.023096: step 73580, total loss = 0.54, predict loss = 0.13 (67.6 examples/sec; 0.059 sec/batch; 97h:23m:00s remains)
INFO - root - 2019-11-04 00:33:49.640144: step 73590, total loss = 0.46, predict loss = 0.10 (60.1 examples/sec; 0.067 sec/batch; 109h:36m:50s remains)
INFO - root - 2019-11-04 00:33:50.257774: step 73600, total loss = 0.29, predict loss = 0.06 (71.2 examples/sec; 0.056 sec/batch; 92h:30m:04s remains)
INFO - root - 2019-11-04 00:33:50.872952: step 73610, total loss = 0.57, predict loss = 0.13 (73.2 examples/sec; 0.055 sec/batch; 89h:55m:17s remains)
INFO - root - 2019-11-04 00:33:51.458133: step 73620, total loss = 0.49, predict loss = 0.10 (94.6 examples/sec; 0.042 sec/batch; 69h:36m:11s remains)
INFO - root - 2019-11-04 00:33:51.913787: step 73630, total loss = 0.62, predict loss = 0.14 (97.9 examples/sec; 0.041 sec/batch; 67h:16m:09s remains)
INFO - root - 2019-11-04 00:33:52.930426: step 73640, total loss = 0.44, predict loss = 0.10 (67.1 examples/sec; 0.060 sec/batch; 98h:09m:31s remains)
INFO - root - 2019-11-04 00:33:53.540147: step 73650, total loss = 0.47, predict loss = 0.11 (74.4 examples/sec; 0.054 sec/batch; 88h:28m:36s remains)
INFO - root - 2019-11-04 00:33:54.145541: step 73660, total loss = 0.43, predict loss = 0.10 (72.6 examples/sec; 0.055 sec/batch; 90h:44m:59s remains)
INFO - root - 2019-11-04 00:33:54.767394: step 73670, total loss = 0.65, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 95h:47m:35s remains)
INFO - root - 2019-11-04 00:33:55.405792: step 73680, total loss = 0.45, predict loss = 0.10 (64.5 examples/sec; 0.062 sec/batch; 102h:01m:06s remains)
INFO - root - 2019-11-04 00:33:56.021664: step 73690, total loss = 0.48, predict loss = 0.12 (68.1 examples/sec; 0.059 sec/batch; 96h:40m:11s remains)
INFO - root - 2019-11-04 00:33:56.657459: step 73700, total loss = 0.71, predict loss = 0.16 (70.3 examples/sec; 0.057 sec/batch; 93h:41m:46s remains)
INFO - root - 2019-11-04 00:33:57.308261: step 73710, total loss = 0.47, predict loss = 0.11 (73.1 examples/sec; 0.055 sec/batch; 90h:06m:32s remains)
INFO - root - 2019-11-04 00:33:57.977516: step 73720, total loss = 0.85, predict loss = 0.20 (66.3 examples/sec; 0.060 sec/batch; 99h:15m:53s remains)
INFO - root - 2019-11-04 00:33:58.630008: step 73730, total loss = 0.59, predict loss = 0.13 (67.2 examples/sec; 0.060 sec/batch; 97h:57m:57s remains)
INFO - root - 2019-11-04 00:33:59.304631: step 73740, total loss = 0.49, predict loss = 0.11 (68.9 examples/sec; 0.058 sec/batch; 95h:32m:02s remains)
INFO - root - 2019-11-04 00:33:59.984542: step 73750, total loss = 0.59, predict loss = 0.13 (69.7 examples/sec; 0.057 sec/batch; 94h:28m:27s remains)
INFO - root - 2019-11-04 00:34:00.602126: step 73760, total loss = 0.39, predict loss = 0.08 (74.0 examples/sec; 0.054 sec/batch; 89h:00m:10s remains)
INFO - root - 2019-11-04 00:34:01.271990: step 73770, total loss = 0.45, predict loss = 0.10 (63.5 examples/sec; 0.063 sec/batch; 103h:43m:48s remains)
INFO - root - 2019-11-04 00:34:01.920792: step 73780, total loss = 0.56, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 100h:49m:37s remains)
INFO - root - 2019-11-04 00:34:02.561304: step 73790, total loss = 0.60, predict loss = 0.14 (68.8 examples/sec; 0.058 sec/batch; 95h:41m:34s remains)
INFO - root - 2019-11-04 00:34:03.205266: step 73800, total loss = 0.59, predict loss = 0.14 (65.8 examples/sec; 0.061 sec/batch; 99h:59m:45s remains)
INFO - root - 2019-11-04 00:34:03.882830: step 73810, total loss = 0.54, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 98h:23m:47s remains)
INFO - root - 2019-11-04 00:34:04.511622: step 73820, total loss = 0.46, predict loss = 0.10 (66.6 examples/sec; 0.060 sec/batch; 98h:48m:55s remains)
INFO - root - 2019-11-04 00:34:05.159936: step 73830, total loss = 0.55, predict loss = 0.12 (80.8 examples/sec; 0.049 sec/batch; 81h:27m:49s remains)
INFO - root - 2019-11-04 00:34:05.764840: step 73840, total loss = 0.56, predict loss = 0.12 (71.2 examples/sec; 0.056 sec/batch; 92h:29m:29s remains)
INFO - root - 2019-11-04 00:34:06.382867: step 73850, total loss = 0.46, predict loss = 0.11 (75.1 examples/sec; 0.053 sec/batch; 87h:39m:44s remains)
INFO - root - 2019-11-04 00:34:07.028710: step 73860, total loss = 0.54, predict loss = 0.13 (63.7 examples/sec; 0.063 sec/batch; 103h:25m:26s remains)
INFO - root - 2019-11-04 00:34:07.676154: step 73870, total loss = 0.48, predict loss = 0.12 (65.7 examples/sec; 0.061 sec/batch; 100h:14m:30s remains)
INFO - root - 2019-11-04 00:34:08.285675: step 73880, total loss = 0.80, predict loss = 0.20 (81.9 examples/sec; 0.049 sec/batch; 80h:22m:13s remains)
INFO - root - 2019-11-04 00:34:08.935139: step 73890, total loss = 0.46, predict loss = 0.11 (67.8 examples/sec; 0.059 sec/batch; 97h:06m:46s remains)
INFO - root - 2019-11-04 00:34:09.540879: step 73900, total loss = 0.68, predict loss = 0.15 (81.2 examples/sec; 0.049 sec/batch; 81h:05m:16s remains)
INFO - root - 2019-11-04 00:34:10.204864: step 73910, total loss = 0.59, predict loss = 0.13 (62.0 examples/sec; 0.064 sec/batch; 106h:08m:24s remains)
INFO - root - 2019-11-04 00:34:10.876626: step 73920, total loss = 0.70, predict loss = 0.17 (75.8 examples/sec; 0.053 sec/batch; 86h:49m:03s remains)
INFO - root - 2019-11-04 00:34:11.520325: step 73930, total loss = 0.46, predict loss = 0.11 (64.6 examples/sec; 0.062 sec/batch; 101h:52m:46s remains)
INFO - root - 2019-11-04 00:34:12.166309: step 73940, total loss = 0.57, predict loss = 0.12 (78.7 examples/sec; 0.051 sec/batch; 83h:43m:04s remains)
INFO - root - 2019-11-04 00:34:12.819382: step 73950, total loss = 0.53, predict loss = 0.12 (65.3 examples/sec; 0.061 sec/batch; 100h:54m:24s remains)
INFO - root - 2019-11-04 00:34:13.474556: step 73960, total loss = 0.64, predict loss = 0.15 (69.0 examples/sec; 0.058 sec/batch; 95h:25m:35s remains)
INFO - root - 2019-11-04 00:34:14.148457: step 73970, total loss = 0.56, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 101h:07m:45s remains)
INFO - root - 2019-11-04 00:34:14.768024: step 73980, total loss = 0.62, predict loss = 0.15 (72.7 examples/sec; 0.055 sec/batch; 90h:36m:24s remains)
INFO - root - 2019-11-04 00:34:15.460933: step 73990, total loss = 0.53, predict loss = 0.13 (57.5 examples/sec; 0.070 sec/batch; 114h:29m:52s remains)
INFO - root - 2019-11-04 00:34:16.109354: step 74000, total loss = 0.58, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 97h:53m:27s remains)
INFO - root - 2019-11-04 00:34:16.753674: step 74010, total loss = 0.41, predict loss = 0.09 (75.5 examples/sec; 0.053 sec/batch; 87h:12m:25s remains)
INFO - root - 2019-11-04 00:34:17.374586: step 74020, total loss = 0.44, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 97h:48m:24s remains)
INFO - root - 2019-11-04 00:34:17.986096: step 74030, total loss = 0.50, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 91h:42m:38s remains)
INFO - root - 2019-11-04 00:34:18.587761: step 74040, total loss = 0.66, predict loss = 0.15 (73.1 examples/sec; 0.055 sec/batch; 90h:06m:36s remains)
INFO - root - 2019-11-04 00:34:19.215930: step 74050, total loss = 0.60, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 95h:05m:28s remains)
INFO - root - 2019-11-04 00:34:19.830986: step 74060, total loss = 0.37, predict loss = 0.08 (82.1 examples/sec; 0.049 sec/batch; 80h:10m:09s remains)
INFO - root - 2019-11-04 00:34:20.446794: step 74070, total loss = 0.42, predict loss = 0.10 (81.4 examples/sec; 0.049 sec/batch; 80h:51m:52s remains)
INFO - root - 2019-11-04 00:34:21.132770: step 74080, total loss = 0.43, predict loss = 0.10 (68.9 examples/sec; 0.058 sec/batch; 95h:29m:54s remains)
INFO - root - 2019-11-04 00:34:21.801098: step 74090, total loss = 0.48, predict loss = 0.11 (69.1 examples/sec; 0.058 sec/batch; 95h:18m:01s remains)
INFO - root - 2019-11-04 00:34:22.422625: step 74100, total loss = 0.40, predict loss = 0.08 (70.3 examples/sec; 0.057 sec/batch; 93h:39m:53s remains)
INFO - root - 2019-11-04 00:34:23.079011: step 74110, total loss = 0.65, predict loss = 0.16 (76.8 examples/sec; 0.052 sec/batch; 85h:41m:01s remains)
INFO - root - 2019-11-04 00:34:23.724324: step 74120, total loss = 0.35, predict loss = 0.08 (71.4 examples/sec; 0.056 sec/batch; 92h:09m:47s remains)
INFO - root - 2019-11-04 00:34:24.325367: step 74130, total loss = 0.33, predict loss = 0.07 (73.5 examples/sec; 0.054 sec/batch; 89h:38m:21s remains)
INFO - root - 2019-11-04 00:34:25.006962: step 74140, total loss = 0.61, predict loss = 0.15 (68.3 examples/sec; 0.059 sec/batch; 96h:24m:35s remains)
INFO - root - 2019-11-04 00:34:25.690457: step 74150, total loss = 0.60, predict loss = 0.14 (61.1 examples/sec; 0.065 sec/batch; 107h:47m:41s remains)
INFO - root - 2019-11-04 00:34:26.318613: step 74160, total loss = 0.56, predict loss = 0.14 (77.5 examples/sec; 0.052 sec/batch; 84h:54m:15s remains)
INFO - root - 2019-11-04 00:34:26.921059: step 74170, total loss = 0.49, predict loss = 0.12 (70.1 examples/sec; 0.057 sec/batch; 93h:54m:48s remains)
INFO - root - 2019-11-04 00:34:27.571403: step 74180, total loss = 0.47, predict loss = 0.11 (68.5 examples/sec; 0.058 sec/batch; 96h:05m:57s remains)
INFO - root - 2019-11-04 00:34:28.225801: step 74190, total loss = 0.58, predict loss = 0.14 (68.3 examples/sec; 0.059 sec/batch; 96h:24m:45s remains)
INFO - root - 2019-11-04 00:34:28.887890: step 74200, total loss = 0.44, predict loss = 0.10 (76.5 examples/sec; 0.052 sec/batch; 86h:01m:04s remains)
INFO - root - 2019-11-04 00:34:29.527620: step 74210, total loss = 0.52, predict loss = 0.11 (70.5 examples/sec; 0.057 sec/batch; 93h:21m:28s remains)
INFO - root - 2019-11-04 00:34:30.200612: step 74220, total loss = 0.39, predict loss = 0.09 (68.8 examples/sec; 0.058 sec/batch; 95h:38m:35s remains)
INFO - root - 2019-11-04 00:34:30.864856: step 74230, total loss = 0.35, predict loss = 0.08 (69.4 examples/sec; 0.058 sec/batch; 94h:55m:28s remains)
INFO - root - 2019-11-04 00:34:31.516066: step 74240, total loss = 0.44, predict loss = 0.11 (63.3 examples/sec; 0.063 sec/batch; 103h:58m:11s remains)
INFO - root - 2019-11-04 00:34:32.157501: step 74250, total loss = 0.41, predict loss = 0.09 (72.4 examples/sec; 0.055 sec/batch; 90h:54m:41s remains)
INFO - root - 2019-11-04 00:34:32.792691: step 74260, total loss = 0.34, predict loss = 0.07 (68.9 examples/sec; 0.058 sec/batch; 95h:36m:15s remains)
INFO - root - 2019-11-04 00:34:33.406174: step 74270, total loss = 0.40, predict loss = 0.09 (80.5 examples/sec; 0.050 sec/batch; 81h:48m:57s remains)
INFO - root - 2019-11-04 00:34:34.035971: step 74280, total loss = 0.44, predict loss = 0.10 (69.4 examples/sec; 0.058 sec/batch; 94h:54m:30s remains)
INFO - root - 2019-11-04 00:34:34.724435: step 74290, total loss = 0.33, predict loss = 0.07 (56.6 examples/sec; 0.071 sec/batch; 116h:17m:17s remains)
INFO - root - 2019-11-04 00:34:35.416745: step 74300, total loss = 0.46, predict loss = 0.10 (72.0 examples/sec; 0.056 sec/batch; 91h:23m:57s remains)
INFO - root - 2019-11-04 00:34:36.043657: step 74310, total loss = 0.51, predict loss = 0.11 (70.3 examples/sec; 0.057 sec/batch; 93h:40m:43s remains)
INFO - root - 2019-11-04 00:34:36.641893: step 74320, total loss = 0.57, predict loss = 0.14 (78.4 examples/sec; 0.051 sec/batch; 84h:00m:27s remains)
INFO - root - 2019-11-04 00:34:37.257634: step 74330, total loss = 0.52, predict loss = 0.12 (70.4 examples/sec; 0.057 sec/batch; 93h:32m:08s remains)
INFO - root - 2019-11-04 00:34:37.912124: step 74340, total loss = 0.67, predict loss = 0.15 (68.1 examples/sec; 0.059 sec/batch; 96h:37m:30s remains)
INFO - root - 2019-11-04 00:34:38.562529: step 74350, total loss = 0.69, predict loss = 0.17 (78.0 examples/sec; 0.051 sec/batch; 84h:21m:57s remains)
INFO - root - 2019-11-04 00:34:39.218801: step 74360, total loss = 0.62, predict loss = 0.14 (62.6 examples/sec; 0.064 sec/batch; 105h:14m:40s remains)
INFO - root - 2019-11-04 00:34:39.903486: step 74370, total loss = 0.61, predict loss = 0.15 (62.8 examples/sec; 0.064 sec/batch; 104h:48m:11s remains)
INFO - root - 2019-11-04 00:34:40.584935: step 74380, total loss = 0.57, predict loss = 0.14 (62.3 examples/sec; 0.064 sec/batch; 105h:44m:18s remains)
INFO - root - 2019-11-04 00:34:41.233082: step 74390, total loss = 0.41, predict loss = 0.09 (72.7 examples/sec; 0.055 sec/batch; 90h:31m:07s remains)
INFO - root - 2019-11-04 00:34:41.891826: step 74400, total loss = 0.50, predict loss = 0.12 (73.3 examples/sec; 0.055 sec/batch; 89h:45m:56s remains)
INFO - root - 2019-11-04 00:34:42.556460: step 74410, total loss = 0.55, predict loss = 0.13 (73.4 examples/sec; 0.055 sec/batch; 89h:43m:21s remains)
INFO - root - 2019-11-04 00:34:43.238062: step 74420, total loss = 0.66, predict loss = 0.15 (63.4 examples/sec; 0.063 sec/batch; 103h:54m:38s remains)
INFO - root - 2019-11-04 00:34:43.878994: step 74430, total loss = 0.51, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 93h:19m:46s remains)
INFO - root - 2019-11-04 00:34:44.521446: step 74440, total loss = 0.59, predict loss = 0.14 (75.2 examples/sec; 0.053 sec/batch; 87h:31m:45s remains)
INFO - root - 2019-11-04 00:34:45.173552: step 74450, total loss = 0.43, predict loss = 0.11 (64.3 examples/sec; 0.062 sec/batch; 102h:22m:28s remains)
INFO - root - 2019-11-04 00:34:45.877621: step 74460, total loss = 0.38, predict loss = 0.09 (72.8 examples/sec; 0.055 sec/batch; 90h:22m:36s remains)
INFO - root - 2019-11-04 00:34:46.563136: step 74470, total loss = 0.35, predict loss = 0.08 (60.0 examples/sec; 0.067 sec/batch; 109h:49m:00s remains)
INFO - root - 2019-11-04 00:34:47.255048: step 74480, total loss = 0.36, predict loss = 0.08 (58.3 examples/sec; 0.069 sec/batch; 113h:01m:25s remains)
INFO - root - 2019-11-04 00:34:47.905905: step 74490, total loss = 0.30, predict loss = 0.05 (74.8 examples/sec; 0.053 sec/batch; 87h:58m:53s remains)
INFO - root - 2019-11-04 00:34:48.507449: step 74500, total loss = 0.44, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 96h:02m:20s remains)
INFO - root - 2019-11-04 00:34:49.114643: step 74510, total loss = 0.40, predict loss = 0.09 (76.9 examples/sec; 0.052 sec/batch; 85h:36m:08s remains)
INFO - root - 2019-11-04 00:34:49.734860: step 74520, total loss = 0.35, predict loss = 0.07 (66.1 examples/sec; 0.061 sec/batch; 99h:40m:38s remains)
INFO - root - 2019-11-04 00:34:50.380917: step 74530, total loss = 0.32, predict loss = 0.07 (64.3 examples/sec; 0.062 sec/batch; 102h:22m:10s remains)
INFO - root - 2019-11-04 00:34:51.013402: step 74540, total loss = 0.51, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 94h:48m:22s remains)
INFO - root - 2019-11-04 00:34:51.615922: step 74550, total loss = 0.43, predict loss = 0.10 (72.2 examples/sec; 0.055 sec/batch; 91h:14m:31s remains)
INFO - root - 2019-11-04 00:34:52.223349: step 74560, total loss = 0.59, predict loss = 0.13 (77.1 examples/sec; 0.052 sec/batch; 85h:23m:22s remains)
INFO - root - 2019-11-04 00:34:52.831945: step 74570, total loss = 0.51, predict loss = 0.12 (72.9 examples/sec; 0.055 sec/batch; 90h:16m:51s remains)
INFO - root - 2019-11-04 00:34:53.458275: step 74580, total loss = 0.38, predict loss = 0.09 (65.8 examples/sec; 0.061 sec/batch; 100h:00m:54s remains)
INFO - root - 2019-11-04 00:34:54.079294: step 74590, total loss = 0.51, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 96h:27m:10s remains)
INFO - root - 2019-11-04 00:34:54.697574: step 74600, total loss = 0.42, predict loss = 0.09 (67.7 examples/sec; 0.059 sec/batch; 97h:18m:44s remains)
INFO - root - 2019-11-04 00:34:55.338921: step 74610, total loss = 0.50, predict loss = 0.11 (65.0 examples/sec; 0.062 sec/batch; 101h:21m:25s remains)
INFO - root - 2019-11-04 00:34:56.004836: step 74620, total loss = 0.38, predict loss = 0.08 (72.1 examples/sec; 0.055 sec/batch; 91h:16m:33s remains)
INFO - root - 2019-11-04 00:34:56.623091: step 74630, total loss = 0.41, predict loss = 0.08 (66.7 examples/sec; 0.060 sec/batch; 98h:43m:11s remains)
INFO - root - 2019-11-04 00:34:57.296961: step 74640, total loss = 0.54, predict loss = 0.14 (64.0 examples/sec; 0.063 sec/batch; 102h:54m:43s remains)
INFO - root - 2019-11-04 00:34:57.973326: step 74650, total loss = 0.41, predict loss = 0.09 (74.5 examples/sec; 0.054 sec/batch; 88h:19m:02s remains)
INFO - root - 2019-11-04 00:34:58.636692: step 74660, total loss = 0.63, predict loss = 0.15 (73.9 examples/sec; 0.054 sec/batch; 89h:05m:05s remains)
INFO - root - 2019-11-04 00:34:59.287614: step 74670, total loss = 0.58, predict loss = 0.14 (77.1 examples/sec; 0.052 sec/batch; 85h:24m:19s remains)
INFO - root - 2019-11-04 00:34:59.917384: step 74680, total loss = 0.46, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 94h:58m:34s remains)
INFO - root - 2019-11-04 00:35:00.551706: step 74690, total loss = 0.52, predict loss = 0.13 (68.5 examples/sec; 0.058 sec/batch; 96h:03m:35s remains)
INFO - root - 2019-11-04 00:35:01.232393: step 74700, total loss = 0.35, predict loss = 0.08 (70.0 examples/sec; 0.057 sec/batch; 94h:00m:31s remains)
INFO - root - 2019-11-04 00:35:01.877240: step 74710, total loss = 0.20, predict loss = 0.04 (67.8 examples/sec; 0.059 sec/batch; 97h:02m:26s remains)
INFO - root - 2019-11-04 00:35:02.516053: step 74720, total loss = 0.41, predict loss = 0.09 (76.5 examples/sec; 0.052 sec/batch; 86h:04m:54s remains)
INFO - root - 2019-11-04 00:35:03.176364: step 74730, total loss = 0.35, predict loss = 0.07 (64.6 examples/sec; 0.062 sec/batch; 101h:53m:55s remains)
INFO - root - 2019-11-04 00:35:03.826702: step 74740, total loss = 0.25, predict loss = 0.05 (75.2 examples/sec; 0.053 sec/batch; 87h:30m:55s remains)
INFO - root - 2019-11-04 00:35:04.440337: step 74750, total loss = 0.35, predict loss = 0.07 (73.5 examples/sec; 0.054 sec/batch; 89h:31m:40s remains)
INFO - root - 2019-11-04 00:35:05.120255: step 74760, total loss = 0.32, predict loss = 0.07 (72.9 examples/sec; 0.055 sec/batch; 90h:18m:42s remains)
INFO - root - 2019-11-04 00:35:05.723282: step 74770, total loss = 0.53, predict loss = 0.12 (75.5 examples/sec; 0.053 sec/batch; 87h:08m:47s remains)
INFO - root - 2019-11-04 00:35:06.346959: step 74780, total loss = 0.45, predict loss = 0.10 (68.8 examples/sec; 0.058 sec/batch; 95h:37m:52s remains)
INFO - root - 2019-11-04 00:35:06.990484: step 74790, total loss = 0.57, predict loss = 0.14 (66.7 examples/sec; 0.060 sec/batch; 98h:43m:20s remains)
INFO - root - 2019-11-04 00:35:07.610122: step 74800, total loss = 0.37, predict loss = 0.08 (66.7 examples/sec; 0.060 sec/batch; 98h:46m:06s remains)
INFO - root - 2019-11-04 00:35:08.261592: step 74810, total loss = 0.56, predict loss = 0.13 (70.8 examples/sec; 0.056 sec/batch; 92h:55m:25s remains)
INFO - root - 2019-11-04 00:35:08.887580: step 74820, total loss = 0.36, predict loss = 0.09 (72.2 examples/sec; 0.055 sec/batch; 91h:14m:00s remains)
INFO - root - 2019-11-04 00:35:09.550951: step 74830, total loss = 0.41, predict loss = 0.09 (66.5 examples/sec; 0.060 sec/batch; 99h:00m:59s remains)
INFO - root - 2019-11-04 00:35:10.208746: step 74840, total loss = 0.41, predict loss = 0.09 (63.0 examples/sec; 0.063 sec/batch; 104h:29m:59s remains)
INFO - root - 2019-11-04 00:35:10.841822: step 74850, total loss = 0.32, predict loss = 0.07 (67.5 examples/sec; 0.059 sec/batch; 97h:27m:43s remains)
INFO - root - 2019-11-04 00:35:11.470692: step 74860, total loss = 0.47, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 89h:07m:22s remains)
INFO - root - 2019-11-04 00:35:12.083376: step 74870, total loss = 0.43, predict loss = 0.10 (77.6 examples/sec; 0.052 sec/batch; 84h:52m:48s remains)
INFO - root - 2019-11-04 00:35:12.749134: step 74880, total loss = 0.52, predict loss = 0.11 (65.7 examples/sec; 0.061 sec/batch; 100h:15m:44s remains)
INFO - root - 2019-11-04 00:35:13.425821: step 74890, total loss = 0.61, predict loss = 0.15 (67.6 examples/sec; 0.059 sec/batch; 97h:23m:24s remains)
INFO - root - 2019-11-04 00:35:14.076578: step 74900, total loss = 0.53, predict loss = 0.12 (68.2 examples/sec; 0.059 sec/batch; 96h:32m:53s remains)
INFO - root - 2019-11-04 00:35:14.693364: step 74910, total loss = 0.41, predict loss = 0.09 (73.1 examples/sec; 0.055 sec/batch; 90h:07m:01s remains)
INFO - root - 2019-11-04 00:35:15.312040: step 74920, total loss = 0.46, predict loss = 0.09 (69.3 examples/sec; 0.058 sec/batch; 94h:57m:43s remains)
INFO - root - 2019-11-04 00:35:15.906337: step 74930, total loss = 0.35, predict loss = 0.08 (77.0 examples/sec; 0.052 sec/batch; 85h:31m:15s remains)
INFO - root - 2019-11-04 00:35:16.532717: step 74940, total loss = 0.55, predict loss = 0.12 (64.5 examples/sec; 0.062 sec/batch; 102h:04m:52s remains)
INFO - root - 2019-11-04 00:35:17.145508: step 74950, total loss = 0.58, predict loss = 0.12 (78.8 examples/sec; 0.051 sec/batch; 83h:33m:23s remains)
INFO - root - 2019-11-04 00:35:17.824930: step 74960, total loss = 0.58, predict loss = 0.14 (61.9 examples/sec; 0.065 sec/batch; 106h:16m:50s remains)
INFO - root - 2019-11-04 00:35:18.506848: step 74970, total loss = 0.36, predict loss = 0.08 (62.4 examples/sec; 0.064 sec/batch; 105h:25m:50s remains)
INFO - root - 2019-11-04 00:35:19.168354: step 74980, total loss = 0.41, predict loss = 0.09 (71.2 examples/sec; 0.056 sec/batch; 92h:27m:32s remains)
INFO - root - 2019-11-04 00:35:19.872547: step 74990, total loss = 0.52, predict loss = 0.12 (53.8 examples/sec; 0.074 sec/batch; 122h:19m:38s remains)
INFO - root - 2019-11-04 00:35:20.527160: step 75000, total loss = 0.36, predict loss = 0.09 (81.6 examples/sec; 0.049 sec/batch; 80h:42m:29s remains)
INFO - root - 2019-11-04 00:35:21.806597: step 75010, total loss = 0.53, predict loss = 0.12 (82.2 examples/sec; 0.049 sec/batch; 80h:05m:25s remains)
INFO - root - 2019-11-04 00:35:22.410103: step 75020, total loss = 0.50, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 97h:45m:57s remains)
INFO - root - 2019-11-04 00:35:23.052708: step 75030, total loss = 0.40, predict loss = 0.09 (68.1 examples/sec; 0.059 sec/batch; 96h:36m:15s remains)
INFO - root - 2019-11-04 00:35:23.686857: step 75040, total loss = 0.58, predict loss = 0.14 (69.4 examples/sec; 0.058 sec/batch; 94h:53m:58s remains)
INFO - root - 2019-11-04 00:35:24.349834: step 75050, total loss = 0.49, predict loss = 0.11 (63.4 examples/sec; 0.063 sec/batch; 103h:50m:33s remains)
INFO - root - 2019-11-04 00:35:25.077343: step 75060, total loss = 0.61, predict loss = 0.15 (71.6 examples/sec; 0.056 sec/batch; 91h:58m:55s remains)
INFO - root - 2019-11-04 00:35:25.661601: step 75070, total loss = 0.85, predict loss = 0.21 (75.8 examples/sec; 0.053 sec/batch; 86h:51m:24s remains)
INFO - root - 2019-11-04 00:35:26.273849: step 75080, total loss = 0.77, predict loss = 0.17 (69.1 examples/sec; 0.058 sec/batch; 95h:18m:10s remains)
INFO - root - 2019-11-04 00:35:26.914868: step 75090, total loss = 0.54, predict loss = 0.13 (66.2 examples/sec; 0.060 sec/batch; 99h:22m:24s remains)
INFO - root - 2019-11-04 00:35:27.544658: step 75100, total loss = 0.79, predict loss = 0.18 (73.3 examples/sec; 0.055 sec/batch; 89h:52m:14s remains)
INFO - root - 2019-11-04 00:35:28.235781: step 75110, total loss = 0.64, predict loss = 0.14 (58.4 examples/sec; 0.068 sec/batch; 112h:42m:10s remains)
INFO - root - 2019-11-04 00:35:28.895823: step 75120, total loss = 0.70, predict loss = 0.16 (64.6 examples/sec; 0.062 sec/batch; 101h:56m:33s remains)
INFO - root - 2019-11-04 00:35:29.535282: step 75130, total loss = 0.86, predict loss = 0.19 (64.8 examples/sec; 0.062 sec/batch; 101h:32m:43s remains)
INFO - root - 2019-11-04 00:35:30.188103: step 75140, total loss = 0.58, predict loss = 0.13 (82.6 examples/sec; 0.048 sec/batch; 79h:43m:20s remains)
INFO - root - 2019-11-04 00:35:30.821525: step 75150, total loss = 0.67, predict loss = 0.16 (75.5 examples/sec; 0.053 sec/batch; 87h:10m:29s remains)
INFO - root - 2019-11-04 00:35:31.460497: step 75160, total loss = 0.82, predict loss = 0.20 (61.8 examples/sec; 0.065 sec/batch; 106h:34m:21s remains)
INFO - root - 2019-11-04 00:35:32.145992: step 75170, total loss = 0.55, predict loss = 0.13 (67.5 examples/sec; 0.059 sec/batch; 97h:30m:32s remains)
INFO - root - 2019-11-04 00:35:32.808568: step 75180, total loss = 0.68, predict loss = 0.15 (68.0 examples/sec; 0.059 sec/batch; 96h:48m:36s remains)
INFO - root - 2019-11-04 00:35:33.490087: step 75190, total loss = 0.57, predict loss = 0.13 (61.7 examples/sec; 0.065 sec/batch; 106h:44m:16s remains)
INFO - root - 2019-11-04 00:35:34.174557: step 75200, total loss = 0.65, predict loss = 0.15 (80.0 examples/sec; 0.050 sec/batch; 82h:20m:04s remains)
INFO - root - 2019-11-04 00:35:34.911910: step 75210, total loss = 0.70, predict loss = 0.17 (73.8 examples/sec; 0.054 sec/batch; 89h:09m:56s remains)
INFO - root - 2019-11-04 00:35:35.528525: step 75220, total loss = 0.48, predict loss = 0.11 (74.1 examples/sec; 0.054 sec/batch; 88h:48m:00s remains)
INFO - root - 2019-11-04 00:35:36.120541: step 75230, total loss = 0.51, predict loss = 0.12 (77.7 examples/sec; 0.051 sec/batch; 84h:40m:25s remains)
INFO - root - 2019-11-04 00:35:36.727503: step 75240, total loss = 0.50, predict loss = 0.12 (72.3 examples/sec; 0.055 sec/batch; 91h:06m:18s remains)
INFO - root - 2019-11-04 00:35:37.387334: step 75250, total loss = 0.56, predict loss = 0.12 (71.0 examples/sec; 0.056 sec/batch; 92h:40m:03s remains)
INFO - root - 2019-11-04 00:35:38.023307: step 75260, total loss = 0.60, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:04m:42s remains)
INFO - root - 2019-11-04 00:35:38.633911: step 75270, total loss = 0.55, predict loss = 0.12 (74.1 examples/sec; 0.054 sec/batch; 88h:53m:16s remains)
INFO - root - 2019-11-04 00:35:39.290302: step 75280, total loss = 0.50, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 90h:10m:28s remains)
INFO - root - 2019-11-04 00:35:39.956777: step 75290, total loss = 0.58, predict loss = 0.13 (64.9 examples/sec; 0.062 sec/batch; 101h:29m:48s remains)
INFO - root - 2019-11-04 00:35:40.586524: step 75300, total loss = 0.58, predict loss = 0.13 (73.6 examples/sec; 0.054 sec/batch; 89h:28m:00s remains)
INFO - root - 2019-11-04 00:35:41.253020: step 75310, total loss = 0.39, predict loss = 0.09 (72.6 examples/sec; 0.055 sec/batch; 90h:42m:34s remains)
INFO - root - 2019-11-04 00:35:41.918470: step 75320, total loss = 0.35, predict loss = 0.08 (68.4 examples/sec; 0.058 sec/batch; 96h:13m:30s remains)
INFO - root - 2019-11-04 00:35:42.567652: step 75330, total loss = 0.41, predict loss = 0.09 (68.1 examples/sec; 0.059 sec/batch; 96h:36m:15s remains)
INFO - root - 2019-11-04 00:35:43.208637: step 75340, total loss = 0.42, predict loss = 0.09 (65.6 examples/sec; 0.061 sec/batch; 100h:20m:57s remains)
INFO - root - 2019-11-04 00:35:43.871591: step 75350, total loss = 0.34, predict loss = 0.07 (66.1 examples/sec; 0.061 sec/batch; 99h:36m:30s remains)
INFO - root - 2019-11-04 00:35:44.544572: step 75360, total loss = 0.43, predict loss = 0.10 (62.0 examples/sec; 0.065 sec/batch; 106h:13m:15s remains)
INFO - root - 2019-11-04 00:35:45.220580: step 75370, total loss = 0.42, predict loss = 0.10 (60.0 examples/sec; 0.067 sec/batch; 109h:47m:33s remains)
INFO - root - 2019-11-04 00:35:45.910182: step 75380, total loss = 0.50, predict loss = 0.11 (63.2 examples/sec; 0.063 sec/batch; 104h:06m:26s remains)
INFO - root - 2019-11-04 00:35:46.593802: step 75390, total loss = 0.38, predict loss = 0.08 (65.0 examples/sec; 0.062 sec/batch; 101h:17m:32s remains)
INFO - root - 2019-11-04 00:35:47.212939: step 75400, total loss = 0.51, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 84h:50m:19s remains)
INFO - root - 2019-11-04 00:35:47.856101: step 75410, total loss = 0.35, predict loss = 0.08 (66.8 examples/sec; 0.060 sec/batch; 98h:34m:26s remains)
INFO - root - 2019-11-04 00:35:48.998427: step 75420, total loss = 0.47, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 93h:18m:08s remains)
INFO - root - 2019-11-04 00:35:49.620126: step 75430, total loss = 0.49, predict loss = 0.10 (75.1 examples/sec; 0.053 sec/batch; 87h:37m:29s remains)
INFO - root - 2019-11-04 00:35:50.274382: step 75440, total loss = 0.46, predict loss = 0.11 (67.6 examples/sec; 0.059 sec/batch; 97h:23m:21s remains)
INFO - root - 2019-11-04 00:35:50.923372: step 75450, total loss = 0.50, predict loss = 0.11 (75.1 examples/sec; 0.053 sec/batch; 87h:38m:28s remains)
INFO - root - 2019-11-04 00:35:51.543521: step 75460, total loss = 0.59, predict loss = 0.14 (73.5 examples/sec; 0.054 sec/batch; 89h:37m:09s remains)
INFO - root - 2019-11-04 00:35:52.200151: step 75470, total loss = 0.42, predict loss = 0.10 (59.1 examples/sec; 0.068 sec/batch; 111h:23m:25s remains)
INFO - root - 2019-11-04 00:35:52.888148: step 75480, total loss = 0.55, predict loss = 0.12 (70.1 examples/sec; 0.057 sec/batch; 93h:56m:07s remains)
INFO - root - 2019-11-04 00:35:53.596319: step 75490, total loss = 0.62, predict loss = 0.15 (58.9 examples/sec; 0.068 sec/batch; 111h:43m:23s remains)
INFO - root - 2019-11-04 00:35:54.215533: step 75500, total loss = 0.76, predict loss = 0.18 (77.6 examples/sec; 0.052 sec/batch; 84h:52m:08s remains)
INFO - root - 2019-11-04 00:35:54.836578: step 75510, total loss = 0.63, predict loss = 0.15 (70.5 examples/sec; 0.057 sec/batch; 93h:18m:45s remains)
INFO - root - 2019-11-04 00:35:55.488286: step 75520, total loss = 0.66, predict loss = 0.15 (71.2 examples/sec; 0.056 sec/batch; 92h:25m:01s remains)
INFO - root - 2019-11-04 00:35:56.118228: step 75530, total loss = 0.79, predict loss = 0.19 (74.7 examples/sec; 0.054 sec/batch; 88h:06m:33s remains)
INFO - root - 2019-11-04 00:35:56.745677: step 75540, total loss = 0.67, predict loss = 0.16 (75.1 examples/sec; 0.053 sec/batch; 87h:36m:47s remains)
INFO - root - 2019-11-04 00:35:57.400103: step 75550, total loss = 0.60, predict loss = 0.14 (72.1 examples/sec; 0.055 sec/batch; 91h:19m:36s remains)
INFO - root - 2019-11-04 00:35:58.068472: step 75560, total loss = 0.65, predict loss = 0.14 (66.6 examples/sec; 0.060 sec/batch; 98h:54m:16s remains)
INFO - root - 2019-11-04 00:35:58.723344: step 75570, total loss = 0.61, predict loss = 0.14 (68.4 examples/sec; 0.058 sec/batch; 96h:14m:27s remains)
INFO - root - 2019-11-04 00:35:59.379780: step 75580, total loss = 0.60, predict loss = 0.13 (70.4 examples/sec; 0.057 sec/batch; 93h:32m:10s remains)
INFO - root - 2019-11-04 00:36:00.007089: step 75590, total loss = 0.40, predict loss = 0.08 (70.4 examples/sec; 0.057 sec/batch; 93h:28m:28s remains)
INFO - root - 2019-11-04 00:36:00.630775: step 75600, total loss = 0.46, predict loss = 0.10 (71.8 examples/sec; 0.056 sec/batch; 91h:37m:11s remains)
INFO - root - 2019-11-04 00:36:01.264719: step 75610, total loss = 0.47, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 96h:43m:41s remains)
INFO - root - 2019-11-04 00:36:01.916709: step 75620, total loss = 0.36, predict loss = 0.08 (63.7 examples/sec; 0.063 sec/batch; 103h:16m:34s remains)
INFO - root - 2019-11-04 00:36:02.550270: step 75630, total loss = 0.45, predict loss = 0.10 (86.1 examples/sec; 0.046 sec/batch; 76h:27m:24s remains)
INFO - root - 2019-11-04 00:36:03.188033: step 75640, total loss = 0.38, predict loss = 0.08 (65.8 examples/sec; 0.061 sec/batch; 100h:01m:37s remains)
INFO - root - 2019-11-04 00:36:03.815984: step 75650, total loss = 0.45, predict loss = 0.10 (71.9 examples/sec; 0.056 sec/batch; 91h:32m:14s remains)
INFO - root - 2019-11-04 00:36:04.462568: step 75660, total loss = 0.57, predict loss = 0.14 (72.6 examples/sec; 0.055 sec/batch; 90h:40m:11s remains)
INFO - root - 2019-11-04 00:36:05.132895: step 75670, total loss = 0.42, predict loss = 0.10 (79.1 examples/sec; 0.051 sec/batch; 83h:14m:39s remains)
INFO - root - 2019-11-04 00:36:05.769815: step 75680, total loss = 0.58, predict loss = 0.13 (68.9 examples/sec; 0.058 sec/batch; 95h:32m:49s remains)
INFO - root - 2019-11-04 00:36:06.391400: step 75690, total loss = 0.56, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 90h:36m:41s remains)
INFO - root - 2019-11-04 00:36:07.015055: step 75700, total loss = 0.81, predict loss = 0.19 (69.8 examples/sec; 0.057 sec/batch; 94h:14m:25s remains)
INFO - root - 2019-11-04 00:36:07.648095: step 75710, total loss = 0.60, predict loss = 0.14 (70.7 examples/sec; 0.057 sec/batch; 93h:05m:52s remains)
INFO - root - 2019-11-04 00:36:08.304155: step 75720, total loss = 0.71, predict loss = 0.18 (58.4 examples/sec; 0.068 sec/batch; 112h:40m:07s remains)
INFO - root - 2019-11-04 00:36:08.990189: step 75730, total loss = 0.64, predict loss = 0.16 (65.3 examples/sec; 0.061 sec/batch; 100h:50m:56s remains)
INFO - root - 2019-11-04 00:36:09.647813: step 75740, total loss = 0.62, predict loss = 0.16 (61.3 examples/sec; 0.065 sec/batch; 107h:25m:02s remains)
INFO - root - 2019-11-04 00:36:10.273670: step 75750, total loss = 0.54, predict loss = 0.13 (75.3 examples/sec; 0.053 sec/batch; 87h:26m:45s remains)
INFO - root - 2019-11-04 00:36:10.882293: step 75760, total loss = 0.51, predict loss = 0.12 (71.5 examples/sec; 0.056 sec/batch; 92h:05m:15s remains)
INFO - root - 2019-11-04 00:36:11.520720: step 75770, total loss = 0.58, predict loss = 0.15 (62.7 examples/sec; 0.064 sec/batch; 104h:57m:53s remains)
INFO - root - 2019-11-04 00:36:12.230709: step 75780, total loss = 0.60, predict loss = 0.14 (61.3 examples/sec; 0.065 sec/batch; 107h:20m:53s remains)
INFO - root - 2019-11-04 00:36:12.845564: step 75790, total loss = 0.54, predict loss = 0.13 (75.2 examples/sec; 0.053 sec/batch; 87h:33m:10s remains)
INFO - root - 2019-11-04 00:36:13.461653: step 75800, total loss = 0.57, predict loss = 0.13 (69.3 examples/sec; 0.058 sec/batch; 94h:55m:28s remains)
INFO - root - 2019-11-04 00:36:14.093154: step 75810, total loss = 0.61, predict loss = 0.14 (65.9 examples/sec; 0.061 sec/batch; 99h:54m:42s remains)
INFO - root - 2019-11-04 00:36:14.705712: step 75820, total loss = 0.52, predict loss = 0.12 (72.1 examples/sec; 0.055 sec/batch; 91h:14m:30s remains)
INFO - root - 2019-11-04 00:36:15.352218: step 75830, total loss = 0.61, predict loss = 0.14 (77.6 examples/sec; 0.052 sec/batch; 84h:50m:13s remains)
INFO - root - 2019-11-04 00:36:15.995937: step 75840, total loss = 0.68, predict loss = 0.17 (73.4 examples/sec; 0.054 sec/batch; 89h:40m:48s remains)
INFO - root - 2019-11-04 00:36:16.630787: step 75850, total loss = 0.66, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 91h:12m:46s remains)
INFO - root - 2019-11-04 00:36:17.245618: step 75860, total loss = 0.53, predict loss = 0.12 (71.9 examples/sec; 0.056 sec/batch; 91h:34m:56s remains)
INFO - root - 2019-11-04 00:36:17.909510: step 75870, total loss = 0.52, predict loss = 0.12 (64.7 examples/sec; 0.062 sec/batch; 101h:45m:16s remains)
INFO - root - 2019-11-04 00:36:18.566884: step 75880, total loss = 0.45, predict loss = 0.10 (72.5 examples/sec; 0.055 sec/batch; 90h:45m:43s remains)
INFO - root - 2019-11-04 00:36:19.230548: step 75890, total loss = 0.53, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 98h:58m:02s remains)
INFO - root - 2019-11-04 00:36:19.852432: step 75900, total loss = 0.49, predict loss = 0.12 (64.7 examples/sec; 0.062 sec/batch; 101h:40m:28s remains)
INFO - root - 2019-11-04 00:36:20.452267: step 75910, total loss = 0.55, predict loss = 0.13 (75.5 examples/sec; 0.053 sec/batch; 87h:12m:14s remains)
INFO - root - 2019-11-04 00:36:21.140182: step 75920, total loss = 0.49, predict loss = 0.11 (58.4 examples/sec; 0.069 sec/batch; 112h:48m:00s remains)
INFO - root - 2019-11-04 00:36:21.806543: step 75930, total loss = 0.45, predict loss = 0.10 (64.9 examples/sec; 0.062 sec/batch; 101h:28m:32s remains)
INFO - root - 2019-11-04 00:36:22.451093: step 75940, total loss = 0.51, predict loss = 0.12 (86.3 examples/sec; 0.046 sec/batch; 76h:17m:37s remains)
INFO - root - 2019-11-04 00:36:23.094037: step 75950, total loss = 0.57, predict loss = 0.13 (75.7 examples/sec; 0.053 sec/batch; 86h:55m:43s remains)
INFO - root - 2019-11-04 00:36:23.705320: step 75960, total loss = 0.52, predict loss = 0.12 (71.1 examples/sec; 0.056 sec/batch; 92h:31m:43s remains)
INFO - root - 2019-11-04 00:36:24.372110: step 75970, total loss = 0.61, predict loss = 0.14 (67.3 examples/sec; 0.059 sec/batch; 97h:44m:16s remains)
INFO - root - 2019-11-04 00:36:25.038903: step 75980, total loss = 0.44, predict loss = 0.10 (64.0 examples/sec; 0.062 sec/batch; 102h:49m:54s remains)
INFO - root - 2019-11-04 00:36:25.640051: step 75990, total loss = 0.51, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 89h:01m:30s remains)
INFO - root - 2019-11-04 00:36:26.259690: step 76000, total loss = 0.50, predict loss = 0.11 (71.5 examples/sec; 0.056 sec/batch; 92h:01m:27s remains)
INFO - root - 2019-11-04 00:36:26.936598: step 76010, total loss = 0.45, predict loss = 0.11 (63.1 examples/sec; 0.063 sec/batch; 104h:22m:12s remains)
INFO - root - 2019-11-04 00:36:27.606818: step 76020, total loss = 0.47, predict loss = 0.11 (68.5 examples/sec; 0.058 sec/batch; 96h:09m:36s remains)
INFO - root - 2019-11-04 00:36:28.269204: step 76030, total loss = 0.49, predict loss = 0.11 (63.9 examples/sec; 0.063 sec/batch; 103h:03m:23s remains)
INFO - root - 2019-11-04 00:36:28.911296: step 76040, total loss = 0.55, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 91h:07m:26s remains)
INFO - root - 2019-11-04 00:36:29.532002: step 76050, total loss = 0.56, predict loss = 0.13 (74.6 examples/sec; 0.054 sec/batch; 88h:16m:04s remains)
INFO - root - 2019-11-04 00:36:30.191243: step 76060, total loss = 0.58, predict loss = 0.14 (68.4 examples/sec; 0.058 sec/batch; 96h:14m:03s remains)
INFO - root - 2019-11-04 00:36:30.854016: step 76070, total loss = 0.56, predict loss = 0.13 (64.3 examples/sec; 0.062 sec/batch; 102h:20m:46s remains)
INFO - root - 2019-11-04 00:36:31.518378: step 76080, total loss = 0.44, predict loss = 0.10 (69.6 examples/sec; 0.057 sec/batch; 94h:32m:05s remains)
INFO - root - 2019-11-04 00:36:32.183930: step 76090, total loss = 0.58, predict loss = 0.14 (74.5 examples/sec; 0.054 sec/batch; 88h:22m:16s remains)
INFO - root - 2019-11-04 00:36:32.854956: step 76100, total loss = 0.73, predict loss = 0.18 (63.1 examples/sec; 0.063 sec/batch; 104h:21m:56s remains)
INFO - root - 2019-11-04 00:36:33.479144: step 76110, total loss = 0.81, predict loss = 0.19 (66.2 examples/sec; 0.060 sec/batch; 99h:27m:44s remains)
INFO - root - 2019-11-04 00:36:34.169255: step 76120, total loss = 0.56, predict loss = 0.13 (62.2 examples/sec; 0.064 sec/batch; 105h:50m:56s remains)
INFO - root - 2019-11-04 00:36:34.865265: step 76130, total loss = 0.83, predict loss = 0.20 (54.8 examples/sec; 0.073 sec/batch; 120h:00m:19s remains)
INFO - root - 2019-11-04 00:36:35.503588: step 76140, total loss = 0.70, predict loss = 0.17 (70.9 examples/sec; 0.056 sec/batch; 92h:52m:16s remains)
INFO - root - 2019-11-04 00:36:36.143551: step 76150, total loss = 0.68, predict loss = 0.17 (74.3 examples/sec; 0.054 sec/batch; 88h:37m:29s remains)
INFO - root - 2019-11-04 00:36:36.777896: step 76160, total loss = 0.64, predict loss = 0.15 (73.7 examples/sec; 0.054 sec/batch; 89h:14m:53s remains)
INFO - root - 2019-11-04 00:36:37.436957: step 76170, total loss = 0.62, predict loss = 0.14 (61.8 examples/sec; 0.065 sec/batch; 106h:32m:12s remains)
INFO - root - 2019-11-04 00:36:38.120765: step 76180, total loss = 0.59, predict loss = 0.14 (65.4 examples/sec; 0.061 sec/batch; 100h:35m:27s remains)
INFO - root - 2019-11-04 00:36:38.782269: step 76190, total loss = 0.68, predict loss = 0.16 (67.4 examples/sec; 0.059 sec/batch; 97h:40m:05s remains)
INFO - root - 2019-11-04 00:36:39.442241: step 76200, total loss = 0.77, predict loss = 0.19 (77.9 examples/sec; 0.051 sec/batch; 84h:31m:26s remains)
INFO - root - 2019-11-04 00:36:40.111755: step 76210, total loss = 0.45, predict loss = 0.11 (68.3 examples/sec; 0.059 sec/batch; 96h:25m:38s remains)
INFO - root - 2019-11-04 00:36:40.764907: step 76220, total loss = 0.40, predict loss = 0.09 (68.2 examples/sec; 0.059 sec/batch; 96h:30m:19s remains)
INFO - root - 2019-11-04 00:36:41.377203: step 76230, total loss = 0.66, predict loss = 0.15 (74.3 examples/sec; 0.054 sec/batch; 88h:32m:41s remains)
INFO - root - 2019-11-04 00:36:42.021273: step 76240, total loss = 0.41, predict loss = 0.09 (70.8 examples/sec; 0.057 sec/batch; 92h:59m:35s remains)
INFO - root - 2019-11-04 00:36:42.656453: step 76250, total loss = 0.62, predict loss = 0.14 (78.3 examples/sec; 0.051 sec/batch; 84h:06m:42s remains)
INFO - root - 2019-11-04 00:36:43.276358: step 76260, total loss = 0.58, predict loss = 0.14 (73.6 examples/sec; 0.054 sec/batch; 89h:23m:46s remains)
INFO - root - 2019-11-04 00:36:43.921870: step 76270, total loss = 0.59, predict loss = 0.14 (59.4 examples/sec; 0.067 sec/batch; 110h:49m:00s remains)
INFO - root - 2019-11-04 00:36:44.564973: step 76280, total loss = 0.55, predict loss = 0.13 (69.5 examples/sec; 0.058 sec/batch; 94h:44m:44s remains)
INFO - root - 2019-11-04 00:36:45.191364: step 76290, total loss = 0.55, predict loss = 0.13 (67.0 examples/sec; 0.060 sec/batch; 98h:15m:17s remains)
INFO - root - 2019-11-04 00:36:45.877475: step 76300, total loss = 0.50, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 99h:04m:16s remains)
INFO - root - 2019-11-04 00:36:46.587818: step 76310, total loss = 0.61, predict loss = 0.15 (63.9 examples/sec; 0.063 sec/batch; 103h:04m:49s remains)
INFO - root - 2019-11-04 00:36:47.238228: step 76320, total loss = 0.55, predict loss = 0.13 (75.4 examples/sec; 0.053 sec/batch; 87h:16m:33s remains)
INFO - root - 2019-11-04 00:36:47.877041: step 76330, total loss = 0.57, predict loss = 0.14 (75.1 examples/sec; 0.053 sec/batch; 87h:39m:25s remains)
INFO - root - 2019-11-04 00:36:48.540347: step 76340, total loss = 0.48, predict loss = 0.11 (70.8 examples/sec; 0.056 sec/batch; 92h:54m:50s remains)
INFO - root - 2019-11-04 00:36:49.070329: step 76350, total loss = 0.46, predict loss = 0.11 (85.1 examples/sec; 0.047 sec/batch; 77h:19m:24s remains)
INFO - root - 2019-11-04 00:36:49.557466: step 76360, total loss = 0.47, predict loss = 0.11 (94.4 examples/sec; 0.042 sec/batch; 69h:41m:27s remains)
INFO - root - 2019-11-04 00:36:50.619973: step 76370, total loss = 0.32, predict loss = 0.07 (66.8 examples/sec; 0.060 sec/batch; 98h:30m:26s remains)
INFO - root - 2019-11-04 00:36:51.260311: step 76380, total loss = 0.57, predict loss = 0.14 (70.4 examples/sec; 0.057 sec/batch; 93h:31m:07s remains)
INFO - root - 2019-11-04 00:36:51.878532: step 76390, total loss = 0.49, predict loss = 0.12 (80.0 examples/sec; 0.050 sec/batch; 82h:14m:58s remains)
INFO - root - 2019-11-04 00:36:52.513838: step 76400, total loss = 0.63, predict loss = 0.15 (68.8 examples/sec; 0.058 sec/batch; 95h:42m:42s remains)
INFO - root - 2019-11-04 00:36:53.170208: step 76410, total loss = 0.36, predict loss = 0.09 (73.5 examples/sec; 0.054 sec/batch; 89h:30m:04s remains)
INFO - root - 2019-11-04 00:36:53.826431: step 76420, total loss = 0.77, predict loss = 0.19 (59.4 examples/sec; 0.067 sec/batch; 110h:49m:46s remains)
INFO - root - 2019-11-04 00:36:54.476181: step 76430, total loss = 0.72, predict loss = 0.17 (81.4 examples/sec; 0.049 sec/batch; 80h:50m:30s remains)
INFO - root - 2019-11-04 00:36:55.142674: step 76440, total loss = 0.61, predict loss = 0.14 (60.4 examples/sec; 0.066 sec/batch; 109h:00m:08s remains)
INFO - root - 2019-11-04 00:36:55.811896: step 76450, total loss = 0.42, predict loss = 0.09 (67.4 examples/sec; 0.059 sec/batch; 97h:38m:20s remains)
INFO - root - 2019-11-04 00:36:56.474371: step 76460, total loss = 0.58, predict loss = 0.13 (74.7 examples/sec; 0.054 sec/batch; 88h:08m:55s remains)
INFO - root - 2019-11-04 00:36:57.149507: step 76470, total loss = 0.48, predict loss = 0.11 (61.6 examples/sec; 0.065 sec/batch; 106h:52m:01s remains)
INFO - root - 2019-11-04 00:36:57.831197: step 76480, total loss = 0.61, predict loss = 0.14 (68.2 examples/sec; 0.059 sec/batch; 96h:28m:56s remains)
INFO - root - 2019-11-04 00:36:58.538324: step 76490, total loss = 0.51, predict loss = 0.11 (61.1 examples/sec; 0.065 sec/batch; 107h:39m:48s remains)
INFO - root - 2019-11-04 00:36:59.163770: step 76500, total loss = 0.45, predict loss = 0.11 (80.0 examples/sec; 0.050 sec/batch; 82h:16m:04s remains)
INFO - root - 2019-11-04 00:36:59.806363: step 76510, total loss = 0.60, predict loss = 0.13 (73.8 examples/sec; 0.054 sec/batch; 89h:10m:32s remains)
INFO - root - 2019-11-04 00:37:00.465626: step 76520, total loss = 0.55, predict loss = 0.14 (68.1 examples/sec; 0.059 sec/batch; 96h:34m:44s remains)
INFO - root - 2019-11-04 00:37:01.080691: step 76530, total loss = 0.60, predict loss = 0.14 (74.7 examples/sec; 0.054 sec/batch; 88h:04m:55s remains)
INFO - root - 2019-11-04 00:37:01.693061: step 76540, total loss = 0.53, predict loss = 0.11 (67.1 examples/sec; 0.060 sec/batch; 98h:06m:25s remains)
INFO - root - 2019-11-04 00:37:02.333241: step 76550, total loss = 0.50, predict loss = 0.11 (65.8 examples/sec; 0.061 sec/batch; 100h:02m:38s remains)
INFO - root - 2019-11-04 00:37:02.962116: step 76560, total loss = 0.47, predict loss = 0.11 (67.3 examples/sec; 0.059 sec/batch; 97h:46m:42s remains)
INFO - root - 2019-11-04 00:37:03.558948: step 76570, total loss = 0.49, predict loss = 0.11 (79.5 examples/sec; 0.050 sec/batch; 82h:46m:14s remains)
INFO - root - 2019-11-04 00:37:04.170453: step 76580, total loss = 0.53, predict loss = 0.13 (67.7 examples/sec; 0.059 sec/batch; 97h:11m:38s remains)
INFO - root - 2019-11-04 00:37:04.831458: step 76590, total loss = 0.60, predict loss = 0.14 (56.7 examples/sec; 0.071 sec/batch; 116h:03m:04s remains)
INFO - root - 2019-11-04 00:37:05.472125: step 76600, total loss = 0.47, predict loss = 0.10 (69.6 examples/sec; 0.057 sec/batch; 94h:32m:09s remains)
INFO - root - 2019-11-04 00:37:06.084015: step 76610, total loss = 0.63, predict loss = 0.15 (83.1 examples/sec; 0.048 sec/batch; 79h:11m:44s remains)
INFO - root - 2019-11-04 00:37:06.725508: step 76620, total loss = 0.50, predict loss = 0.10 (72.8 examples/sec; 0.055 sec/batch; 90h:21m:20s remains)
INFO - root - 2019-11-04 00:37:07.374390: step 76630, total loss = 0.60, predict loss = 0.14 (70.0 examples/sec; 0.057 sec/batch; 94h:00m:45s remains)
INFO - root - 2019-11-04 00:37:07.999263: step 76640, total loss = 0.59, predict loss = 0.12 (81.7 examples/sec; 0.049 sec/batch; 80h:33m:54s remains)
INFO - root - 2019-11-04 00:37:08.602130: step 76650, total loss = 0.62, predict loss = 0.14 (84.4 examples/sec; 0.047 sec/batch; 78h:00m:45s remains)
INFO - root - 2019-11-04 00:37:09.238004: step 76660, total loss = 0.54, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 98h:21m:19s remains)
INFO - root - 2019-11-04 00:37:09.901808: step 76670, total loss = 0.81, predict loss = 0.19 (68.3 examples/sec; 0.059 sec/batch; 96h:23m:02s remains)
INFO - root - 2019-11-04 00:37:10.558746: step 76680, total loss = 0.69, predict loss = 0.16 (73.1 examples/sec; 0.055 sec/batch; 90h:05m:22s remains)
INFO - root - 2019-11-04 00:37:11.238164: step 76690, total loss = 0.68, predict loss = 0.16 (64.0 examples/sec; 0.062 sec/batch; 102h:47m:30s remains)
INFO - root - 2019-11-04 00:37:11.933401: step 76700, total loss = 0.63, predict loss = 0.14 (61.9 examples/sec; 0.065 sec/batch; 106h:24m:34s remains)
INFO - root - 2019-11-04 00:37:12.633635: step 76710, total loss = 0.57, predict loss = 0.12 (61.1 examples/sec; 0.065 sec/batch; 107h:39m:50s remains)
INFO - root - 2019-11-04 00:37:13.335119: step 76720, total loss = 0.47, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 87h:44m:34s remains)
INFO - root - 2019-11-04 00:37:13.950808: step 76730, total loss = 0.58, predict loss = 0.13 (69.3 examples/sec; 0.058 sec/batch; 94h:57m:52s remains)
INFO - root - 2019-11-04 00:37:14.583476: step 76740, total loss = 0.40, predict loss = 0.08 (62.8 examples/sec; 0.064 sec/batch; 104h:43m:44s remains)
INFO - root - 2019-11-04 00:37:15.211728: step 76750, total loss = 0.54, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 101h:08m:59s remains)
INFO - root - 2019-11-04 00:37:15.840262: step 76760, total loss = 0.48, predict loss = 0.10 (70.9 examples/sec; 0.056 sec/batch; 92h:52m:13s remains)
INFO - root - 2019-11-04 00:37:16.501535: step 76770, total loss = 0.65, predict loss = 0.16 (65.0 examples/sec; 0.062 sec/batch; 101h:15m:45s remains)
INFO - root - 2019-11-04 00:37:17.146262: step 76780, total loss = 0.63, predict loss = 0.15 (75.0 examples/sec; 0.053 sec/batch; 87h:42m:52s remains)
INFO - root - 2019-11-04 00:37:17.783869: step 76790, total loss = 0.49, predict loss = 0.11 (77.9 examples/sec; 0.051 sec/batch; 84h:29m:10s remains)
INFO - root - 2019-11-04 00:37:18.436311: step 76800, total loss = 0.58, predict loss = 0.13 (61.3 examples/sec; 0.065 sec/batch; 107h:26m:00s remains)
INFO - root - 2019-11-04 00:37:19.098827: step 76810, total loss = 0.64, predict loss = 0.15 (71.1 examples/sec; 0.056 sec/batch; 92h:37m:41s remains)
INFO - root - 2019-11-04 00:37:19.737662: step 76820, total loss = 0.47, predict loss = 0.11 (68.0 examples/sec; 0.059 sec/batch; 96h:51m:14s remains)
INFO - root - 2019-11-04 00:37:20.386096: step 76830, total loss = 0.66, predict loss = 0.16 (68.1 examples/sec; 0.059 sec/batch; 96h:35m:37s remains)
INFO - root - 2019-11-04 00:37:20.996211: step 76840, total loss = 0.47, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 94h:20m:19s remains)
INFO - root - 2019-11-04 00:37:21.601692: step 76850, total loss = 0.49, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 93h:46m:28s remains)
INFO - root - 2019-11-04 00:37:22.216170: step 76860, total loss = 0.36, predict loss = 0.08 (80.6 examples/sec; 0.050 sec/batch; 81h:41m:44s remains)
INFO - root - 2019-11-04 00:37:22.854333: step 76870, total loss = 0.60, predict loss = 0.15 (68.0 examples/sec; 0.059 sec/batch; 96h:50m:03s remains)
INFO - root - 2019-11-04 00:37:23.507162: step 76880, total loss = 0.70, predict loss = 0.17 (64.5 examples/sec; 0.062 sec/batch; 101h:59m:00s remains)
INFO - root - 2019-11-04 00:37:24.133482: step 76890, total loss = 0.48, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 90h:12m:00s remains)
INFO - root - 2019-11-04 00:37:24.782520: step 76900, total loss = 0.52, predict loss = 0.12 (71.0 examples/sec; 0.056 sec/batch; 92h:43m:09s remains)
INFO - root - 2019-11-04 00:37:25.448461: step 76910, total loss = 0.41, predict loss = 0.10 (74.9 examples/sec; 0.053 sec/batch; 87h:52m:04s remains)
INFO - root - 2019-11-04 00:37:26.076406: step 76920, total loss = 0.39, predict loss = 0.09 (74.3 examples/sec; 0.054 sec/batch; 88h:36m:20s remains)
INFO - root - 2019-11-04 00:37:26.707649: step 76930, total loss = 0.34, predict loss = 0.08 (64.0 examples/sec; 0.063 sec/batch; 102h:53m:40s remains)
INFO - root - 2019-11-04 00:37:27.425088: step 76940, total loss = 0.34, predict loss = 0.08 (68.7 examples/sec; 0.058 sec/batch; 95h:47m:31s remains)
INFO - root - 2019-11-04 00:37:28.055536: step 76950, total loss = 0.53, predict loss = 0.13 (76.3 examples/sec; 0.052 sec/batch; 86h:12m:20s remains)
INFO - root - 2019-11-04 00:37:28.720457: step 76960, total loss = 0.40, predict loss = 0.10 (72.7 examples/sec; 0.055 sec/batch; 90h:27m:52s remains)
INFO - root - 2019-11-04 00:37:29.336374: step 76970, total loss = 0.36, predict loss = 0.08 (68.0 examples/sec; 0.059 sec/batch; 96h:44m:15s remains)
INFO - root - 2019-11-04 00:37:29.934854: step 76980, total loss = 0.43, predict loss = 0.10 (92.4 examples/sec; 0.043 sec/batch; 71h:15m:36s remains)
INFO - root - 2019-11-04 00:37:30.544360: step 76990, total loss = 0.43, predict loss = 0.10 (73.9 examples/sec; 0.054 sec/batch; 89h:05m:49s remains)
INFO - root - 2019-11-04 00:37:31.164804: step 77000, total loss = 0.39, predict loss = 0.09 (64.1 examples/sec; 0.062 sec/batch; 102h:44m:38s remains)
INFO - root - 2019-11-04 00:37:31.811948: step 77010, total loss = 0.30, predict loss = 0.06 (64.3 examples/sec; 0.062 sec/batch; 102h:18m:20s remains)
INFO - root - 2019-11-04 00:37:32.481243: step 77020, total loss = 0.54, predict loss = 0.13 (65.4 examples/sec; 0.061 sec/batch; 100h:35m:59s remains)
INFO - root - 2019-11-04 00:37:33.140413: step 77030, total loss = 0.34, predict loss = 0.08 (67.7 examples/sec; 0.059 sec/batch; 97h:08m:53s remains)
INFO - root - 2019-11-04 00:37:33.814861: step 77040, total loss = 0.71, predict loss = 0.17 (60.1 examples/sec; 0.067 sec/batch; 109h:27m:21s remains)
INFO - root - 2019-11-04 00:37:34.475387: step 77050, total loss = 0.57, predict loss = 0.13 (74.7 examples/sec; 0.054 sec/batch; 88h:04m:16s remains)
INFO - root - 2019-11-04 00:37:35.197774: step 77060, total loss = 0.70, predict loss = 0.16 (71.9 examples/sec; 0.056 sec/batch; 91h:28m:39s remains)
INFO - root - 2019-11-04 00:37:35.833809: step 77070, total loss = 0.57, predict loss = 0.13 (63.9 examples/sec; 0.063 sec/batch; 103h:02m:44s remains)
INFO - root - 2019-11-04 00:37:36.465634: step 77080, total loss = 0.73, predict loss = 0.18 (66.3 examples/sec; 0.060 sec/batch; 99h:15m:05s remains)
INFO - root - 2019-11-04 00:37:37.118201: step 77090, total loss = 0.67, predict loss = 0.16 (71.4 examples/sec; 0.056 sec/batch; 92h:08m:09s remains)
INFO - root - 2019-11-04 00:37:37.799119: step 77100, total loss = 0.63, predict loss = 0.15 (62.1 examples/sec; 0.064 sec/batch; 105h:54m:08s remains)
INFO - root - 2019-11-04 00:37:38.483000: step 77110, total loss = 0.54, predict loss = 0.12 (82.1 examples/sec; 0.049 sec/batch; 80h:08m:27s remains)
INFO - root - 2019-11-04 00:37:39.133040: step 77120, total loss = 0.70, predict loss = 0.17 (64.6 examples/sec; 0.062 sec/batch; 101h:53m:23s remains)
INFO - root - 2019-11-04 00:37:39.767541: step 77130, total loss = 0.53, predict loss = 0.13 (72.0 examples/sec; 0.056 sec/batch; 91h:22m:23s remains)
INFO - root - 2019-11-04 00:37:40.461641: step 77140, total loss = 0.54, predict loss = 0.13 (67.6 examples/sec; 0.059 sec/batch; 97h:18m:38s remains)
INFO - root - 2019-11-04 00:37:41.092211: step 77150, total loss = 0.59, predict loss = 0.15 (73.5 examples/sec; 0.054 sec/batch; 89h:33m:34s remains)
INFO - root - 2019-11-04 00:37:41.714257: step 77160, total loss = 0.66, predict loss = 0.16 (63.4 examples/sec; 0.063 sec/batch; 103h:48m:34s remains)
INFO - root - 2019-11-04 00:37:42.346758: step 77170, total loss = 0.50, predict loss = 0.11 (65.6 examples/sec; 0.061 sec/batch; 100h:15m:03s remains)
INFO - root - 2019-11-04 00:37:42.957491: step 77180, total loss = 0.49, predict loss = 0.12 (81.0 examples/sec; 0.049 sec/batch; 81h:12m:58s remains)
INFO - root - 2019-11-04 00:37:43.596205: step 77190, total loss = 0.38, predict loss = 0.08 (71.4 examples/sec; 0.056 sec/batch; 92h:11m:46s remains)
INFO - root - 2019-11-04 00:37:44.229479: step 77200, total loss = 0.42, predict loss = 0.10 (67.1 examples/sec; 0.060 sec/batch; 98h:03m:30s remains)
INFO - root - 2019-11-04 00:37:44.853885: step 77210, total loss = 0.28, predict loss = 0.06 (72.1 examples/sec; 0.056 sec/batch; 91h:20m:06s remains)
INFO - root - 2019-11-04 00:37:45.512568: step 77220, total loss = 0.49, predict loss = 0.11 (73.2 examples/sec; 0.055 sec/batch; 89h:57m:10s remains)
INFO - root - 2019-11-04 00:37:46.131810: step 77230, total loss = 0.33, predict loss = 0.07 (61.3 examples/sec; 0.065 sec/batch; 107h:17m:07s remains)
INFO - root - 2019-11-04 00:37:46.747893: step 77240, total loss = 0.24, predict loss = 0.05 (65.5 examples/sec; 0.061 sec/batch; 100h:28m:12s remains)
INFO - root - 2019-11-04 00:37:47.395110: step 77250, total loss = 0.35, predict loss = 0.08 (75.6 examples/sec; 0.053 sec/batch; 87h:01m:51s remains)
INFO - root - 2019-11-04 00:37:48.026850: step 77260, total loss = 0.59, predict loss = 0.13 (71.9 examples/sec; 0.056 sec/batch; 91h:31m:09s remains)
INFO - root - 2019-11-04 00:37:48.693613: step 77270, total loss = 0.36, predict loss = 0.08 (68.8 examples/sec; 0.058 sec/batch; 95h:38m:43s remains)
INFO - root - 2019-11-04 00:37:49.371204: step 77280, total loss = 0.44, predict loss = 0.10 (64.6 examples/sec; 0.062 sec/batch; 101h:50m:56s remains)
INFO - root - 2019-11-04 00:37:50.021419: step 77290, total loss = 0.53, predict loss = 0.12 (67.5 examples/sec; 0.059 sec/batch; 97h:33m:37s remains)
INFO - root - 2019-11-04 00:37:50.673564: step 77300, total loss = 0.56, predict loss = 0.14 (77.8 examples/sec; 0.051 sec/batch; 84h:32m:54s remains)
INFO - root - 2019-11-04 00:37:51.308597: step 77310, total loss = 0.41, predict loss = 0.09 (63.6 examples/sec; 0.063 sec/batch; 103h:29m:34s remains)
INFO - root - 2019-11-04 00:37:51.938879: step 77320, total loss = 0.31, predict loss = 0.07 (67.6 examples/sec; 0.059 sec/batch; 97h:22m:42s remains)
INFO - root - 2019-11-04 00:37:52.564843: step 77330, total loss = 0.53, predict loss = 0.13 (71.2 examples/sec; 0.056 sec/batch; 92h:26m:09s remains)
INFO - root - 2019-11-04 00:37:53.189205: step 77340, total loss = 0.44, predict loss = 0.11 (65.4 examples/sec; 0.061 sec/batch; 100h:38m:56s remains)
INFO - root - 2019-11-04 00:37:53.839279: step 77350, total loss = 0.52, predict loss = 0.12 (75.4 examples/sec; 0.053 sec/batch; 87h:18m:41s remains)
INFO - root - 2019-11-04 00:37:54.517998: step 77360, total loss = 0.55, predict loss = 0.13 (71.1 examples/sec; 0.056 sec/batch; 92h:31m:43s remains)
INFO - root - 2019-11-04 00:37:55.134422: step 77370, total loss = 0.52, predict loss = 0.12 (72.9 examples/sec; 0.055 sec/batch; 90h:14m:52s remains)
INFO - root - 2019-11-04 00:37:55.789817: step 77380, total loss = 0.55, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 99h:55m:17s remains)
INFO - root - 2019-11-04 00:37:56.434346: step 77390, total loss = 0.56, predict loss = 0.13 (70.0 examples/sec; 0.057 sec/batch; 94h:01m:08s remains)
INFO - root - 2019-11-04 00:37:57.070448: step 77400, total loss = 0.44, predict loss = 0.10 (70.8 examples/sec; 0.057 sec/batch; 92h:57m:29s remains)
INFO - root - 2019-11-04 00:37:57.720512: step 77410, total loss = 0.58, predict loss = 0.15 (68.0 examples/sec; 0.059 sec/batch; 96h:45m:04s remains)
INFO - root - 2019-11-04 00:37:58.359620: step 77420, total loss = 0.49, predict loss = 0.12 (65.2 examples/sec; 0.061 sec/batch; 100h:53m:01s remains)
INFO - root - 2019-11-04 00:37:58.990433: step 77430, total loss = 0.43, predict loss = 0.10 (83.9 examples/sec; 0.048 sec/batch; 78h:24m:19s remains)
INFO - root - 2019-11-04 00:37:59.659853: step 77440, total loss = 0.33, predict loss = 0.08 (65.9 examples/sec; 0.061 sec/batch; 99h:47m:27s remains)
INFO - root - 2019-11-04 00:38:00.327819: step 77450, total loss = 0.59, predict loss = 0.14 (67.0 examples/sec; 0.060 sec/batch; 98h:11m:23s remains)
INFO - root - 2019-11-04 00:38:00.954076: step 77460, total loss = 0.21, predict loss = 0.04 (68.1 examples/sec; 0.059 sec/batch; 96h:39m:11s remains)
INFO - root - 2019-11-04 00:38:01.599608: step 77470, total loss = 0.24, predict loss = 0.05 (72.8 examples/sec; 0.055 sec/batch; 90h:20m:00s remains)
INFO - root - 2019-11-04 00:38:02.244556: step 77480, total loss = 0.60, predict loss = 0.15 (75.1 examples/sec; 0.053 sec/batch; 87h:34m:11s remains)
INFO - root - 2019-11-04 00:38:02.863412: step 77490, total loss = 0.52, predict loss = 0.12 (69.3 examples/sec; 0.058 sec/batch; 94h:59m:56s remains)
INFO - root - 2019-11-04 00:38:03.537819: step 77500, total loss = 0.48, predict loss = 0.11 (62.7 examples/sec; 0.064 sec/batch; 104h:55m:37s remains)
INFO - root - 2019-11-04 00:38:04.206383: step 77510, total loss = 0.56, predict loss = 0.13 (75.6 examples/sec; 0.053 sec/batch; 87h:00m:11s remains)
INFO - root - 2019-11-04 00:38:04.901605: step 77520, total loss = 0.48, predict loss = 0.11 (55.5 examples/sec; 0.072 sec/batch; 118h:32m:06s remains)
INFO - root - 2019-11-04 00:38:05.543857: step 77530, total loss = 0.56, predict loss = 0.12 (74.1 examples/sec; 0.054 sec/batch; 88h:45m:19s remains)
INFO - root - 2019-11-04 00:38:06.173931: step 77540, total loss = 0.46, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 98h:14m:25s remains)
INFO - root - 2019-11-04 00:38:06.854421: step 77550, total loss = 0.54, predict loss = 0.12 (64.5 examples/sec; 0.062 sec/batch; 101h:59m:30s remains)
INFO - root - 2019-11-04 00:38:07.505232: step 77560, total loss = 0.42, predict loss = 0.09 (69.4 examples/sec; 0.058 sec/batch; 94h:52m:19s remains)
INFO - root - 2019-11-04 00:38:08.171889: step 77570, total loss = 0.36, predict loss = 0.08 (67.0 examples/sec; 0.060 sec/batch; 98h:11m:20s remains)
INFO - root - 2019-11-04 00:38:08.876341: step 77580, total loss = 0.68, predict loss = 0.16 (64.8 examples/sec; 0.062 sec/batch; 101h:32m:33s remains)
INFO - root - 2019-11-04 00:38:09.555247: step 77590, total loss = 0.64, predict loss = 0.15 (61.5 examples/sec; 0.065 sec/batch; 107h:02m:46s remains)
INFO - root - 2019-11-04 00:38:10.186799: step 77600, total loss = 0.63, predict loss = 0.16 (83.8 examples/sec; 0.048 sec/batch; 78h:29m:23s remains)
INFO - root - 2019-11-04 00:38:10.809156: step 77610, total loss = 0.56, predict loss = 0.13 (60.6 examples/sec; 0.066 sec/batch; 108h:31m:27s remains)
INFO - root - 2019-11-04 00:38:11.503742: step 77620, total loss = 0.46, predict loss = 0.10 (67.2 examples/sec; 0.060 sec/batch; 97h:59m:22s remains)
INFO - root - 2019-11-04 00:38:12.198740: step 77630, total loss = 0.44, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 99h:00m:48s remains)
INFO - root - 2019-11-04 00:38:12.882688: step 77640, total loss = 0.48, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 97h:49m:34s remains)
INFO - root - 2019-11-04 00:38:13.507996: step 77650, total loss = 0.45, predict loss = 0.10 (67.7 examples/sec; 0.059 sec/batch; 97h:11m:40s remains)
INFO - root - 2019-11-04 00:38:14.135010: step 77660, total loss = 0.42, predict loss = 0.09 (70.2 examples/sec; 0.057 sec/batch; 93h:42m:28s remains)
INFO - root - 2019-11-04 00:38:14.793861: step 77670, total loss = 0.59, predict loss = 0.14 (65.0 examples/sec; 0.062 sec/batch; 101h:14m:13s remains)
INFO - root - 2019-11-04 00:38:15.467987: step 77680, total loss = 0.38, predict loss = 0.09 (68.4 examples/sec; 0.058 sec/batch; 96h:12m:54s remains)
INFO - root - 2019-11-04 00:38:16.131983: step 77690, total loss = 0.64, predict loss = 0.15 (77.2 examples/sec; 0.052 sec/batch; 85h:11m:36s remains)
INFO - root - 2019-11-04 00:38:16.757746: step 77700, total loss = 0.58, predict loss = 0.13 (73.0 examples/sec; 0.055 sec/batch; 90h:09m:17s remains)
INFO - root - 2019-11-04 00:38:17.406117: step 77710, total loss = 0.19, predict loss = 0.03 (66.2 examples/sec; 0.060 sec/batch; 99h:24m:51s remains)
INFO - root - 2019-11-04 00:38:18.085213: step 77720, total loss = 0.58, predict loss = 0.14 (64.2 examples/sec; 0.062 sec/batch; 102h:26m:51s remains)
INFO - root - 2019-11-04 00:38:18.750340: step 77730, total loss = 0.64, predict loss = 0.14 (68.0 examples/sec; 0.059 sec/batch; 96h:45m:16s remains)
INFO - root - 2019-11-04 00:38:19.385515: step 77740, total loss = 0.53, predict loss = 0.13 (71.5 examples/sec; 0.056 sec/batch; 92h:01m:44s remains)
INFO - root - 2019-11-04 00:38:20.003101: step 77750, total loss = 0.47, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 94h:28m:27s remains)
INFO - root - 2019-11-04 00:38:20.634001: step 77760, total loss = 0.43, predict loss = 0.10 (85.6 examples/sec; 0.047 sec/batch; 76h:53m:34s remains)
INFO - root - 2019-11-04 00:38:21.272350: step 77770, total loss = 0.60, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 93h:14m:51s remains)
INFO - root - 2019-11-04 00:38:21.938920: step 77780, total loss = 0.44, predict loss = 0.10 (62.8 examples/sec; 0.064 sec/batch; 104h:46m:49s remains)
INFO - root - 2019-11-04 00:38:22.553481: step 77790, total loss = 0.61, predict loss = 0.14 (76.7 examples/sec; 0.052 sec/batch; 85h:45m:13s remains)
INFO - root - 2019-11-04 00:38:23.212833: step 77800, total loss = 0.76, predict loss = 0.18 (72.9 examples/sec; 0.055 sec/batch; 90h:15m:49s remains)
INFO - root - 2019-11-04 00:38:23.848908: step 77810, total loss = 0.85, predict loss = 0.21 (68.4 examples/sec; 0.058 sec/batch; 96h:10m:35s remains)
INFO - root - 2019-11-04 00:38:24.463239: step 77820, total loss = 0.76, predict loss = 0.18 (66.0 examples/sec; 0.061 sec/batch; 99h:41m:54s remains)
INFO - root - 2019-11-04 00:38:25.095076: step 77830, total loss = 0.88, predict loss = 0.21 (70.0 examples/sec; 0.057 sec/batch; 93h:58m:52s remains)
INFO - root - 2019-11-04 00:38:25.717350: step 77840, total loss = 0.70, predict loss = 0.15 (67.8 examples/sec; 0.059 sec/batch; 97h:02m:28s remains)
INFO - root - 2019-11-04 00:38:26.367825: step 77850, total loss = 0.68, predict loss = 0.16 (74.3 examples/sec; 0.054 sec/batch; 88h:35m:13s remains)
INFO - root - 2019-11-04 00:38:26.965379: step 77860, total loss = 0.58, predict loss = 0.13 (79.0 examples/sec; 0.051 sec/batch; 83h:17m:26s remains)
INFO - root - 2019-11-04 00:38:27.594326: step 77870, total loss = 0.57, predict loss = 0.13 (61.5 examples/sec; 0.065 sec/batch; 107h:02m:57s remains)
INFO - root - 2019-11-04 00:38:28.263753: step 77880, total loss = 0.69, predict loss = 0.16 (72.4 examples/sec; 0.055 sec/batch; 90h:51m:08s remains)
INFO - root - 2019-11-04 00:38:28.919657: step 77890, total loss = 0.66, predict loss = 0.16 (71.7 examples/sec; 0.056 sec/batch; 91h:43m:27s remains)
INFO - root - 2019-11-04 00:38:29.596215: step 77900, total loss = 0.48, predict loss = 0.10 (74.1 examples/sec; 0.054 sec/batch; 88h:49m:48s remains)
INFO - root - 2019-11-04 00:38:30.253930: step 77910, total loss = 0.65, predict loss = 0.15 (67.3 examples/sec; 0.059 sec/batch; 97h:42m:15s remains)
INFO - root - 2019-11-04 00:38:30.870032: step 77920, total loss = 0.60, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 91h:10m:19s remains)
INFO - root - 2019-11-04 00:38:31.477658: step 77930, total loss = 0.55, predict loss = 0.12 (74.3 examples/sec; 0.054 sec/batch; 88h:34m:44s remains)
INFO - root - 2019-11-04 00:38:32.082784: step 77940, total loss = 0.49, predict loss = 0.11 (71.2 examples/sec; 0.056 sec/batch; 92h:28m:03s remains)
INFO - root - 2019-11-04 00:38:32.717494: step 77950, total loss = 0.65, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 91h:09m:45s remains)
INFO - root - 2019-11-04 00:38:33.348927: step 77960, total loss = 0.52, predict loss = 0.12 (66.3 examples/sec; 0.060 sec/batch; 99h:16m:46s remains)
INFO - root - 2019-11-04 00:38:33.957825: step 77970, total loss = 0.55, predict loss = 0.12 (74.1 examples/sec; 0.054 sec/batch; 88h:46m:15s remains)
INFO - root - 2019-11-04 00:38:34.572039: step 77980, total loss = 0.51, predict loss = 0.12 (76.7 examples/sec; 0.052 sec/batch; 85h:49m:30s remains)
INFO - root - 2019-11-04 00:38:35.243571: step 77990, total loss = 0.56, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 94h:52m:05s remains)
INFO - root - 2019-11-04 00:38:35.866120: step 78000, total loss = 0.54, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 98h:24m:46s remains)
INFO - root - 2019-11-04 00:38:36.526914: step 78010, total loss = 0.57, predict loss = 0.13 (76.5 examples/sec; 0.052 sec/batch; 85h:58m:01s remains)
INFO - root - 2019-11-04 00:38:37.168065: step 78020, total loss = 0.45, predict loss = 0.10 (74.1 examples/sec; 0.054 sec/batch; 88h:47m:46s remains)
INFO - root - 2019-11-04 00:38:37.825518: step 78030, total loss = 0.58, predict loss = 0.14 (71.6 examples/sec; 0.056 sec/batch; 91h:50m:06s remains)
INFO - root - 2019-11-04 00:38:38.479535: step 78040, total loss = 0.45, predict loss = 0.10 (66.0 examples/sec; 0.061 sec/batch; 99h:42m:23s remains)
INFO - root - 2019-11-04 00:38:39.157015: step 78050, total loss = 0.52, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 98h:03m:59s remains)
INFO - root - 2019-11-04 00:38:39.804464: step 78060, total loss = 0.30, predict loss = 0.06 (65.0 examples/sec; 0.062 sec/batch; 101h:12m:01s remains)
INFO - root - 2019-11-04 00:38:40.417564: step 78070, total loss = 0.43, predict loss = 0.09 (69.2 examples/sec; 0.058 sec/batch; 95h:01m:57s remains)
INFO - root - 2019-11-04 00:38:41.030419: step 78080, total loss = 0.38, predict loss = 0.09 (75.1 examples/sec; 0.053 sec/batch; 87h:34m:50s remains)
INFO - root - 2019-11-04 00:38:41.675906: step 78090, total loss = 0.41, predict loss = 0.09 (68.4 examples/sec; 0.058 sec/batch; 96h:07m:56s remains)
INFO - root - 2019-11-04 00:38:42.287360: step 78100, total loss = 0.45, predict loss = 0.10 (85.5 examples/sec; 0.047 sec/batch; 76h:57m:18s remains)
INFO - root - 2019-11-04 00:38:42.884686: step 78110, total loss = 0.43, predict loss = 0.10 (78.2 examples/sec; 0.051 sec/batch; 84h:10m:25s remains)
INFO - root - 2019-11-04 00:38:43.528355: step 78120, total loss = 0.49, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 96h:34m:52s remains)
INFO - root - 2019-11-04 00:38:44.186966: step 78130, total loss = 0.39, predict loss = 0.09 (69.2 examples/sec; 0.058 sec/batch; 95h:09m:04s remains)
INFO - root - 2019-11-04 00:38:44.851689: step 78140, total loss = 0.38, predict loss = 0.08 (76.7 examples/sec; 0.052 sec/batch; 85h:45m:09s remains)
INFO - root - 2019-11-04 00:38:45.487726: step 78150, total loss = 0.56, predict loss = 0.13 (67.7 examples/sec; 0.059 sec/batch; 97h:07m:40s remains)
INFO - root - 2019-11-04 00:38:46.134341: step 78160, total loss = 0.53, predict loss = 0.12 (74.7 examples/sec; 0.054 sec/batch; 88h:01m:38s remains)
INFO - root - 2019-11-04 00:38:46.755945: step 78170, total loss = 0.53, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 97h:04m:13s remains)
INFO - root - 2019-11-04 00:38:47.381044: step 78180, total loss = 0.61, predict loss = 0.14 (65.0 examples/sec; 0.062 sec/batch; 101h:13m:38s remains)
INFO - root - 2019-11-04 00:38:48.002553: step 78190, total loss = 0.50, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 94h:15m:13s remains)
INFO - root - 2019-11-04 00:38:48.644621: step 78200, total loss = 0.57, predict loss = 0.13 (77.8 examples/sec; 0.051 sec/batch; 84h:32m:12s remains)
INFO - root - 2019-11-04 00:38:49.299834: step 78210, total loss = 0.54, predict loss = 0.12 (68.5 examples/sec; 0.058 sec/batch; 96h:04m:30s remains)
INFO - root - 2019-11-04 00:38:49.976989: step 78220, total loss = 0.74, predict loss = 0.18 (64.9 examples/sec; 0.062 sec/batch; 101h:25m:54s remains)
INFO - root - 2019-11-04 00:38:50.641591: step 78230, total loss = 0.64, predict loss = 0.15 (65.4 examples/sec; 0.061 sec/batch; 100h:39m:47s remains)
INFO - root - 2019-11-04 00:38:51.299712: step 78240, total loss = 0.68, predict loss = 0.16 (67.6 examples/sec; 0.059 sec/batch; 97h:17m:20s remains)
INFO - root - 2019-11-04 00:38:51.932481: step 78250, total loss = 0.68, predict loss = 0.16 (79.0 examples/sec; 0.051 sec/batch; 83h:16m:44s remains)
INFO - root - 2019-11-04 00:38:52.561481: step 78260, total loss = 0.79, predict loss = 0.20 (62.7 examples/sec; 0.064 sec/batch; 105h:00m:12s remains)
INFO - root - 2019-11-04 00:38:53.190642: step 78270, total loss = 0.69, predict loss = 0.17 (71.6 examples/sec; 0.056 sec/batch; 91h:54m:17s remains)
INFO - root - 2019-11-04 00:38:53.856135: step 78280, total loss = 0.65, predict loss = 0.16 (66.0 examples/sec; 0.061 sec/batch; 99h:45m:52s remains)
INFO - root - 2019-11-04 00:38:54.507732: step 78290, total loss = 0.52, predict loss = 0.12 (71.1 examples/sec; 0.056 sec/batch; 92h:33m:24s remains)
INFO - root - 2019-11-04 00:38:55.172866: step 78300, total loss = 0.59, predict loss = 0.14 (66.1 examples/sec; 0.061 sec/batch; 99h:36m:11s remains)
INFO - root - 2019-11-04 00:38:55.819334: step 78310, total loss = 0.59, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 94h:54m:38s remains)
INFO - root - 2019-11-04 00:38:56.481376: step 78320, total loss = 0.34, predict loss = 0.07 (76.0 examples/sec; 0.053 sec/batch; 86h:32m:10s remains)
INFO - root - 2019-11-04 00:38:57.155961: step 78330, total loss = 0.39, predict loss = 0.08 (66.3 examples/sec; 0.060 sec/batch; 99h:15m:03s remains)
INFO - root - 2019-11-04 00:38:57.806945: step 78340, total loss = 0.60, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 93h:37m:41s remains)
INFO - root - 2019-11-04 00:38:58.440671: step 78350, total loss = 0.44, predict loss = 0.10 (81.9 examples/sec; 0.049 sec/batch; 80h:20m:03s remains)
INFO - root - 2019-11-04 00:38:59.049635: step 78360, total loss = 0.45, predict loss = 0.10 (70.2 examples/sec; 0.057 sec/batch; 93h:45m:27s remains)
INFO - root - 2019-11-04 00:38:59.676716: step 78370, total loss = 0.34, predict loss = 0.08 (82.1 examples/sec; 0.049 sec/batch; 80h:06m:29s remains)
INFO - root - 2019-11-04 00:39:00.331330: step 78380, total loss = 0.42, predict loss = 0.09 (70.8 examples/sec; 0.056 sec/batch; 92h:53m:57s remains)
INFO - root - 2019-11-04 00:39:00.966685: step 78390, total loss = 0.38, predict loss = 0.08 (64.6 examples/sec; 0.062 sec/batch; 101h:49m:00s remains)
INFO - root - 2019-11-04 00:39:01.586559: step 78400, total loss = 0.38, predict loss = 0.07 (78.9 examples/sec; 0.051 sec/batch; 83h:23m:32s remains)
INFO - root - 2019-11-04 00:39:02.279891: step 78410, total loss = 0.53, predict loss = 0.12 (57.8 examples/sec; 0.069 sec/batch; 113h:53m:21s remains)
INFO - root - 2019-11-04 00:39:02.930779: step 78420, total loss = 0.37, predict loss = 0.08 (67.7 examples/sec; 0.059 sec/batch; 97h:11m:34s remains)
INFO - root - 2019-11-04 00:39:03.546632: step 78430, total loss = 0.62, predict loss = 0.15 (68.4 examples/sec; 0.059 sec/batch; 96h:14m:39s remains)
INFO - root - 2019-11-04 00:39:04.185829: step 78440, total loss = 0.54, predict loss = 0.13 (66.4 examples/sec; 0.060 sec/batch; 99h:03m:37s remains)
INFO - root - 2019-11-04 00:39:04.832098: step 78450, total loss = 0.73, predict loss = 0.17 (64.3 examples/sec; 0.062 sec/batch; 102h:18m:11s remains)
INFO - root - 2019-11-04 00:39:05.481827: step 78460, total loss = 0.66, predict loss = 0.16 (67.1 examples/sec; 0.060 sec/batch; 98h:05m:53s remains)
INFO - root - 2019-11-04 00:39:06.119926: step 78470, total loss = 0.40, predict loss = 0.09 (71.7 examples/sec; 0.056 sec/batch; 91h:46m:03s remains)
INFO - root - 2019-11-04 00:39:06.732663: step 78480, total loss = 0.64, predict loss = 0.16 (77.7 examples/sec; 0.051 sec/batch; 84h:39m:33s remains)
INFO - root - 2019-11-04 00:39:07.379724: step 78490, total loss = 0.58, predict loss = 0.14 (62.3 examples/sec; 0.064 sec/batch; 105h:38m:31s remains)
INFO - root - 2019-11-04 00:39:08.000988: step 78500, total loss = 0.55, predict loss = 0.14 (68.4 examples/sec; 0.058 sec/batch; 96h:08m:37s remains)
INFO - root - 2019-11-04 00:39:08.628266: step 78510, total loss = 0.58, predict loss = 0.13 (75.7 examples/sec; 0.053 sec/batch; 86h:54m:52s remains)
INFO - root - 2019-11-04 00:39:09.295696: step 78520, total loss = 0.56, predict loss = 0.13 (71.1 examples/sec; 0.056 sec/batch; 92h:30m:49s remains)
INFO - root - 2019-11-04 00:39:09.939206: step 78530, total loss = 0.54, predict loss = 0.12 (62.2 examples/sec; 0.064 sec/batch; 105h:51m:07s remains)
INFO - root - 2019-11-04 00:39:10.593046: step 78540, total loss = 0.59, predict loss = 0.14 (67.0 examples/sec; 0.060 sec/batch; 98h:11m:14s remains)
INFO - root - 2019-11-04 00:39:11.265497: step 78550, total loss = 0.61, predict loss = 0.14 (70.0 examples/sec; 0.057 sec/batch; 93h:59m:22s remains)
INFO - root - 2019-11-04 00:39:11.920483: step 78560, total loss = 0.51, predict loss = 0.11 (75.8 examples/sec; 0.053 sec/batch; 86h:47m:15s remains)
INFO - root - 2019-11-04 00:39:12.551517: step 78570, total loss = 0.56, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 93h:10m:18s remains)
INFO - root - 2019-11-04 00:39:13.165348: step 78580, total loss = 0.50, predict loss = 0.11 (74.7 examples/sec; 0.054 sec/batch; 88h:02m:30s remains)
INFO - root - 2019-11-04 00:39:13.824484: step 78590, total loss = 0.55, predict loss = 0.13 (61.8 examples/sec; 0.065 sec/batch; 106h:28m:47s remains)
INFO - root - 2019-11-04 00:39:14.454448: step 78600, total loss = 0.50, predict loss = 0.11 (79.7 examples/sec; 0.050 sec/batch; 82h:33m:58s remains)
INFO - root - 2019-11-04 00:39:15.059033: step 78610, total loss = 0.52, predict loss = 0.12 (70.0 examples/sec; 0.057 sec/batch; 94h:02m:27s remains)
INFO - root - 2019-11-04 00:39:15.677389: step 78620, total loss = 0.38, predict loss = 0.08 (68.6 examples/sec; 0.058 sec/batch; 95h:57m:45s remains)
INFO - root - 2019-11-04 00:39:16.382618: step 78630, total loss = 0.42, predict loss = 0.10 (69.6 examples/sec; 0.057 sec/batch; 94h:31m:48s remains)
INFO - root - 2019-11-04 00:39:17.075155: step 78640, total loss = 0.44, predict loss = 0.10 (71.9 examples/sec; 0.056 sec/batch; 91h:26m:55s remains)
INFO - root - 2019-11-04 00:39:17.743752: step 78650, total loss = 0.66, predict loss = 0.16 (72.6 examples/sec; 0.055 sec/batch; 90h:35m:50s remains)
INFO - root - 2019-11-04 00:39:18.444710: step 78660, total loss = 0.43, predict loss = 0.10 (64.8 examples/sec; 0.062 sec/batch; 101h:31m:48s remains)
INFO - root - 2019-11-04 00:39:19.114704: step 78670, total loss = 0.43, predict loss = 0.10 (74.9 examples/sec; 0.053 sec/batch; 87h:51m:12s remains)
INFO - root - 2019-11-04 00:39:19.768633: step 78680, total loss = 0.57, predict loss = 0.13 (69.7 examples/sec; 0.057 sec/batch; 94h:19m:48s remains)
INFO - root - 2019-11-04 00:39:20.434896: step 78690, total loss = 0.52, predict loss = 0.12 (66.1 examples/sec; 0.060 sec/batch; 99h:30m:38s remains)
INFO - root - 2019-11-04 00:39:21.122167: step 78700, total loss = 0.49, predict loss = 0.11 (64.7 examples/sec; 0.062 sec/batch; 101h:38m:56s remains)
INFO - root - 2019-11-04 00:39:21.838936: step 78710, total loss = 0.52, predict loss = 0.12 (62.8 examples/sec; 0.064 sec/batch; 104h:47m:47s remains)
INFO - root - 2019-11-04 00:39:22.490943: step 78720, total loss = 0.54, predict loss = 0.12 (69.6 examples/sec; 0.057 sec/batch; 94h:32m:51s remains)
INFO - root - 2019-11-04 00:39:23.119150: step 78730, total loss = 0.58, predict loss = 0.13 (67.8 examples/sec; 0.059 sec/batch; 97h:01m:59s remains)
INFO - root - 2019-11-04 00:39:23.764280: step 78740, total loss = 0.41, predict loss = 0.10 (67.2 examples/sec; 0.060 sec/batch; 97h:55m:23s remains)
INFO - root - 2019-11-04 00:39:24.384157: step 78750, total loss = 0.57, predict loss = 0.13 (81.8 examples/sec; 0.049 sec/batch; 80h:26m:11s remains)
INFO - root - 2019-11-04 00:39:25.050858: step 78760, total loss = 0.51, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 92h:16m:53s remains)
INFO - root - 2019-11-04 00:39:25.714386: step 78770, total loss = 0.49, predict loss = 0.10 (67.0 examples/sec; 0.060 sec/batch; 98h:11m:54s remains)
INFO - root - 2019-11-04 00:39:26.377690: step 78780, total loss = 0.53, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 95h:34m:33s remains)
INFO - root - 2019-11-04 00:39:26.992956: step 78790, total loss = 0.44, predict loss = 0.11 (76.4 examples/sec; 0.052 sec/batch; 86h:07m:20s remains)
INFO - root - 2019-11-04 00:39:27.598123: step 78800, total loss = 0.53, predict loss = 0.13 (67.7 examples/sec; 0.059 sec/batch; 97h:09m:41s remains)
INFO - root - 2019-11-04 00:39:28.239231: step 78810, total loss = 0.67, predict loss = 0.16 (77.9 examples/sec; 0.051 sec/batch; 84h:24m:48s remains)
INFO - root - 2019-11-04 00:39:28.905808: step 78820, total loss = 0.70, predict loss = 0.16 (75.2 examples/sec; 0.053 sec/batch; 87h:31m:58s remains)
INFO - root - 2019-11-04 00:39:29.517808: step 78830, total loss = 0.65, predict loss = 0.15 (68.9 examples/sec; 0.058 sec/batch; 95h:30m:58s remains)
INFO - root - 2019-11-04 00:39:30.186237: step 78840, total loss = 0.67, predict loss = 0.16 (63.5 examples/sec; 0.063 sec/batch; 103h:33m:41s remains)
INFO - root - 2019-11-04 00:39:30.878725: step 78850, total loss = 0.52, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 98h:59m:21s remains)
INFO - root - 2019-11-04 00:39:31.483095: step 78860, total loss = 0.81, predict loss = 0.19 (79.0 examples/sec; 0.051 sec/batch; 83h:16m:30s remains)
INFO - root - 2019-11-04 00:39:32.105744: step 78870, total loss = 0.69, predict loss = 0.17 (71.2 examples/sec; 0.056 sec/batch; 92h:22m:09s remains)
INFO - root - 2019-11-04 00:39:32.761960: step 78880, total loss = 0.68, predict loss = 0.17 (75.2 examples/sec; 0.053 sec/batch; 87h:31m:35s remains)
INFO - root - 2019-11-04 00:39:33.409846: step 78890, total loss = 1.02, predict loss = 0.24 (56.7 examples/sec; 0.071 sec/batch; 116h:01m:24s remains)
INFO - root - 2019-11-04 00:39:34.128778: step 78900, total loss = 0.63, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 89h:59m:09s remains)
INFO - root - 2019-11-04 00:39:34.819073: step 78910, total loss = 0.62, predict loss = 0.14 (56.3 examples/sec; 0.071 sec/batch; 116h:49m:21s remains)
INFO - root - 2019-11-04 00:39:35.487623: step 78920, total loss = 0.73, predict loss = 0.17 (80.0 examples/sec; 0.050 sec/batch; 82h:14m:58s remains)
INFO - root - 2019-11-04 00:39:36.126288: step 78930, total loss = 0.67, predict loss = 0.16 (78.3 examples/sec; 0.051 sec/batch; 84h:01m:41s remains)
INFO - root - 2019-11-04 00:39:36.757125: step 78940, total loss = 0.69, predict loss = 0.17 (76.2 examples/sec; 0.052 sec/batch; 86h:17m:29s remains)
INFO - root - 2019-11-04 00:39:37.393030: step 78950, total loss = 0.47, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 98h:07m:15s remains)
INFO - root - 2019-11-04 00:39:38.048983: step 78960, total loss = 0.47, predict loss = 0.11 (71.5 examples/sec; 0.056 sec/batch; 91h:58m:59s remains)
INFO - root - 2019-11-04 00:39:38.697940: step 78970, total loss = 0.66, predict loss = 0.15 (77.1 examples/sec; 0.052 sec/batch; 85h:18m:47s remains)
INFO - root - 2019-11-04 00:39:39.353606: step 78980, total loss = 0.44, predict loss = 0.10 (79.9 examples/sec; 0.050 sec/batch; 82h:19m:25s remains)
INFO - root - 2019-11-04 00:39:40.010165: step 78990, total loss = 0.73, predict loss = 0.17 (69.4 examples/sec; 0.058 sec/batch; 94h:46m:25s remains)
INFO - root - 2019-11-04 00:39:40.683423: step 79000, total loss = 0.51, predict loss = 0.12 (63.9 examples/sec; 0.063 sec/batch; 102h:54m:02s remains)
INFO - root - 2019-11-04 00:39:41.347400: step 79010, total loss = 0.44, predict loss = 0.10 (70.0 examples/sec; 0.057 sec/batch; 93h:57m:21s remains)
INFO - root - 2019-11-04 00:39:42.026760: step 79020, total loss = 0.64, predict loss = 0.16 (61.9 examples/sec; 0.065 sec/batch; 106h:20m:26s remains)
INFO - root - 2019-11-04 00:39:42.669840: step 79030, total loss = 0.53, predict loss = 0.12 (69.6 examples/sec; 0.057 sec/batch; 94h:29m:51s remains)
INFO - root - 2019-11-04 00:39:43.333840: step 79040, total loss = 0.59, predict loss = 0.14 (71.5 examples/sec; 0.056 sec/batch; 91h:58m:09s remains)
INFO - root - 2019-11-04 00:39:44.011212: step 79050, total loss = 0.61, predict loss = 0.15 (65.0 examples/sec; 0.062 sec/batch; 101h:09m:55s remains)
INFO - root - 2019-11-04 00:39:44.694528: step 79060, total loss = 0.51, predict loss = 0.12 (63.6 examples/sec; 0.063 sec/batch; 103h:21m:56s remains)
INFO - root - 2019-11-04 00:39:45.312790: step 79070, total loss = 0.47, predict loss = 0.11 (95.0 examples/sec; 0.042 sec/batch; 69h:15m:16s remains)
INFO - root - 2019-11-04 00:39:45.804449: step 79080, total loss = 0.44, predict loss = 0.10 (98.7 examples/sec; 0.041 sec/batch; 66h:39m:24s remains)
INFO - root - 2019-11-04 00:39:46.260118: step 79090, total loss = 0.53, predict loss = 0.12 (92.7 examples/sec; 0.043 sec/batch; 70h:58m:50s remains)
INFO - root - 2019-11-04 00:39:47.367299: step 79100, total loss = 0.32, predict loss = 0.06 (73.0 examples/sec; 0.055 sec/batch; 90h:08m:25s remains)
INFO - root - 2019-11-04 00:39:47.986980: step 79110, total loss = 0.46, predict loss = 0.10 (66.8 examples/sec; 0.060 sec/batch; 98h:26m:24s remains)
INFO - root - 2019-11-04 00:39:48.613768: step 79120, total loss = 0.45, predict loss = 0.11 (69.2 examples/sec; 0.058 sec/batch; 95h:02m:26s remains)
INFO - root - 2019-11-04 00:39:49.236267: step 79130, total loss = 0.56, predict loss = 0.14 (74.8 examples/sec; 0.053 sec/batch; 87h:54m:11s remains)
INFO - root - 2019-11-04 00:39:49.860393: step 79140, total loss = 0.61, predict loss = 0.15 (71.6 examples/sec; 0.056 sec/batch; 91h:49m:35s remains)
INFO - root - 2019-11-04 00:39:50.508634: step 79150, total loss = 0.51, predict loss = 0.11 (70.5 examples/sec; 0.057 sec/batch; 93h:20m:19s remains)
INFO - root - 2019-11-04 00:39:51.132525: step 79160, total loss = 0.69, predict loss = 0.17 (78.7 examples/sec; 0.051 sec/batch; 83h:37m:17s remains)
INFO - root - 2019-11-04 00:39:51.759229: step 79170, total loss = 0.61, predict loss = 0.14 (69.2 examples/sec; 0.058 sec/batch; 95h:01m:23s remains)
INFO - root - 2019-11-04 00:39:52.418076: step 79180, total loss = 0.58, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 96h:37m:17s remains)
INFO - root - 2019-11-04 00:39:53.076478: step 79190, total loss = 0.58, predict loss = 0.12 (58.7 examples/sec; 0.068 sec/batch; 112h:03m:16s remains)
INFO - root - 2019-11-04 00:39:53.752085: step 79200, total loss = 0.65, predict loss = 0.15 (59.3 examples/sec; 0.067 sec/batch; 110h:57m:01s remains)
INFO - root - 2019-11-04 00:39:54.369849: step 79210, total loss = 0.57, predict loss = 0.13 (71.8 examples/sec; 0.056 sec/batch; 91h:36m:59s remains)
INFO - root - 2019-11-04 00:39:55.020453: step 79220, total loss = 0.65, predict loss = 0.16 (64.4 examples/sec; 0.062 sec/batch; 102h:04m:25s remains)
INFO - root - 2019-11-04 00:39:55.647847: step 79230, total loss = 0.63, predict loss = 0.15 (67.3 examples/sec; 0.059 sec/batch; 97h:45m:34s remains)
INFO - root - 2019-11-04 00:39:56.272047: step 79240, total loss = 0.34, predict loss = 0.07 (57.9 examples/sec; 0.069 sec/batch; 113h:32m:12s remains)
INFO - root - 2019-11-04 00:39:56.951396: step 79250, total loss = 0.73, predict loss = 0.18 (67.5 examples/sec; 0.059 sec/batch; 97h:28m:52s remains)
INFO - root - 2019-11-04 00:39:57.569840: step 79260, total loss = 0.51, predict loss = 0.13 (67.9 examples/sec; 0.059 sec/batch; 96h:55m:55s remains)
INFO - root - 2019-11-04 00:39:58.233117: step 79270, total loss = 0.53, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 93h:38m:01s remains)
INFO - root - 2019-11-04 00:39:58.870646: step 79280, total loss = 0.47, predict loss = 0.11 (75.4 examples/sec; 0.053 sec/batch; 87h:12m:51s remains)
INFO - root - 2019-11-04 00:39:59.495850: step 79290, total loss = 0.49, predict loss = 0.11 (76.0 examples/sec; 0.053 sec/batch; 86h:34m:49s remains)
INFO - root - 2019-11-04 00:40:00.109805: step 79300, total loss = 0.54, predict loss = 0.12 (64.7 examples/sec; 0.062 sec/batch; 101h:42m:30s remains)
INFO - root - 2019-11-04 00:40:00.724195: step 79310, total loss = 0.54, predict loss = 0.13 (70.2 examples/sec; 0.057 sec/batch; 93h:44m:52s remains)
INFO - root - 2019-11-04 00:40:01.357164: step 79320, total loss = 0.33, predict loss = 0.07 (73.7 examples/sec; 0.054 sec/batch; 89h:13m:39s remains)
INFO - root - 2019-11-04 00:40:01.970229: step 79330, total loss = 0.45, predict loss = 0.10 (74.1 examples/sec; 0.054 sec/batch; 88h:49m:47s remains)
INFO - root - 2019-11-04 00:40:02.647553: step 79340, total loss = 0.62, predict loss = 0.15 (64.2 examples/sec; 0.062 sec/batch; 102h:28m:29s remains)
INFO - root - 2019-11-04 00:40:03.338094: step 79350, total loss = 0.75, predict loss = 0.18 (66.6 examples/sec; 0.060 sec/batch; 98h:50m:06s remains)
INFO - root - 2019-11-04 00:40:03.952126: step 79360, total loss = 0.56, predict loss = 0.11 (71.8 examples/sec; 0.056 sec/batch; 91h:35m:57s remains)
INFO - root - 2019-11-04 00:40:04.563606: step 79370, total loss = 0.59, predict loss = 0.13 (81.1 examples/sec; 0.049 sec/batch; 81h:09m:16s remains)
INFO - root - 2019-11-04 00:40:05.233005: step 79380, total loss = 0.56, predict loss = 0.12 (73.2 examples/sec; 0.055 sec/batch; 89h:53m:07s remains)
INFO - root - 2019-11-04 00:40:05.835997: step 79390, total loss = 0.71, predict loss = 0.17 (72.8 examples/sec; 0.055 sec/batch; 90h:18m:37s remains)
INFO - root - 2019-11-04 00:40:06.480639: step 79400, total loss = 0.63, predict loss = 0.14 (67.4 examples/sec; 0.059 sec/batch; 97h:33m:49s remains)
INFO - root - 2019-11-04 00:40:07.189948: step 79410, total loss = 0.69, predict loss = 0.15 (60.4 examples/sec; 0.066 sec/batch; 108h:50m:02s remains)
INFO - root - 2019-11-04 00:40:07.801870: step 79420, total loss = 0.71, predict loss = 0.16 (77.4 examples/sec; 0.052 sec/batch; 85h:01m:16s remains)
INFO - root - 2019-11-04 00:40:08.408837: step 79430, total loss = 0.68, predict loss = 0.16 (71.3 examples/sec; 0.056 sec/batch; 92h:19m:34s remains)
INFO - root - 2019-11-04 00:40:09.068685: step 79440, total loss = 0.65, predict loss = 0.15 (67.8 examples/sec; 0.059 sec/batch; 97h:02m:58s remains)
INFO - root - 2019-11-04 00:40:09.680825: step 79450, total loss = 0.56, predict loss = 0.13 (66.5 examples/sec; 0.060 sec/batch; 98h:58m:07s remains)
INFO - root - 2019-11-04 00:40:10.316871: step 79460, total loss = 0.75, predict loss = 0.19 (77.0 examples/sec; 0.052 sec/batch; 85h:24m:35s remains)
INFO - root - 2019-11-04 00:40:10.936258: step 79470, total loss = 0.54, predict loss = 0.12 (72.3 examples/sec; 0.055 sec/batch; 90h:57m:38s remains)
INFO - root - 2019-11-04 00:40:11.566260: step 79480, total loss = 0.41, predict loss = 0.09 (67.7 examples/sec; 0.059 sec/batch; 97h:12m:13s remains)
INFO - root - 2019-11-04 00:40:12.217157: step 79490, total loss = 0.50, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 97h:36m:30s remains)
INFO - root - 2019-11-04 00:40:12.839288: step 79500, total loss = 0.72, predict loss = 0.17 (77.0 examples/sec; 0.052 sec/batch; 85h:27m:19s remains)
INFO - root - 2019-11-04 00:40:13.473683: step 79510, total loss = 0.61, predict loss = 0.14 (67.3 examples/sec; 0.059 sec/batch; 97h:44m:26s remains)
INFO - root - 2019-11-04 00:40:14.111301: step 79520, total loss = 0.67, predict loss = 0.16 (66.8 examples/sec; 0.060 sec/batch; 98h:30m:27s remains)
INFO - root - 2019-11-04 00:40:14.747002: step 79530, total loss = 0.57, predict loss = 0.13 (64.1 examples/sec; 0.062 sec/batch; 102h:35m:51s remains)
INFO - root - 2019-11-04 00:40:15.424669: step 79540, total loss = 0.52, predict loss = 0.12 (64.4 examples/sec; 0.062 sec/batch; 102h:06m:01s remains)
INFO - root - 2019-11-04 00:40:16.104241: step 79550, total loss = 0.68, predict loss = 0.16 (70.1 examples/sec; 0.057 sec/batch; 93h:48m:16s remains)
INFO - root - 2019-11-04 00:40:16.829908: step 79560, total loss = 0.64, predict loss = 0.16 (60.2 examples/sec; 0.066 sec/batch; 109h:13m:57s remains)
INFO - root - 2019-11-04 00:40:17.440330: step 79570, total loss = 0.42, predict loss = 0.10 (77.3 examples/sec; 0.052 sec/batch; 85h:08m:44s remains)
INFO - root - 2019-11-04 00:40:18.058275: step 79580, total loss = 0.39, predict loss = 0.09 (69.7 examples/sec; 0.057 sec/batch; 94h:22m:38s remains)
INFO - root - 2019-11-04 00:40:18.701121: step 79590, total loss = 0.45, predict loss = 0.10 (77.0 examples/sec; 0.052 sec/batch; 85h:25m:55s remains)
INFO - root - 2019-11-04 00:40:19.322572: step 79600, total loss = 0.64, predict loss = 0.16 (71.6 examples/sec; 0.056 sec/batch; 91h:51m:43s remains)
INFO - root - 2019-11-04 00:40:19.965283: step 79610, total loss = 0.55, predict loss = 0.14 (77.4 examples/sec; 0.052 sec/batch; 85h:01m:41s remains)
INFO - root - 2019-11-04 00:40:20.588174: step 79620, total loss = 0.57, predict loss = 0.14 (72.5 examples/sec; 0.055 sec/batch; 90h:45m:44s remains)
INFO - root - 2019-11-04 00:40:21.218289: step 79630, total loss = 0.35, predict loss = 0.08 (73.6 examples/sec; 0.054 sec/batch; 89h:25m:11s remains)
INFO - root - 2019-11-04 00:40:21.836636: step 79640, total loss = 0.48, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 93h:52m:24s remains)
INFO - root - 2019-11-04 00:40:22.537241: step 79650, total loss = 0.54, predict loss = 0.13 (66.5 examples/sec; 0.060 sec/batch; 98h:55m:18s remains)
INFO - root - 2019-11-04 00:40:23.232096: step 79660, total loss = 0.46, predict loss = 0.11 (75.1 examples/sec; 0.053 sec/batch; 87h:36m:49s remains)
INFO - root - 2019-11-04 00:40:23.896239: step 79670, total loss = 0.45, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 92h:33m:33s remains)
INFO - root - 2019-11-04 00:40:24.551756: step 79680, total loss = 0.67, predict loss = 0.16 (71.7 examples/sec; 0.056 sec/batch; 91h:41m:57s remains)
INFO - root - 2019-11-04 00:40:25.164280: step 79690, total loss = 0.45, predict loss = 0.10 (71.5 examples/sec; 0.056 sec/batch; 91h:58m:42s remains)
INFO - root - 2019-11-04 00:40:25.755142: step 79700, total loss = 0.59, predict loss = 0.14 (78.8 examples/sec; 0.051 sec/batch; 83h:26m:03s remains)
INFO - root - 2019-11-04 00:40:26.347731: step 79710, total loss = 0.34, predict loss = 0.07 (76.2 examples/sec; 0.053 sec/batch; 86h:20m:33s remains)
INFO - root - 2019-11-04 00:40:26.952511: step 79720, total loss = 0.36, predict loss = 0.08 (75.5 examples/sec; 0.053 sec/batch; 87h:09m:06s remains)
INFO - root - 2019-11-04 00:40:27.559616: step 79730, total loss = 0.35, predict loss = 0.08 (70.4 examples/sec; 0.057 sec/batch; 93h:28m:20s remains)
INFO - root - 2019-11-04 00:40:28.180670: step 79740, total loss = 0.39, predict loss = 0.09 (74.9 examples/sec; 0.053 sec/batch; 87h:50m:14s remains)
INFO - root - 2019-11-04 00:40:28.813903: step 79750, total loss = 0.40, predict loss = 0.09 (70.2 examples/sec; 0.057 sec/batch; 93h:43m:13s remains)
INFO - root - 2019-11-04 00:40:29.458430: step 79760, total loss = 0.40, predict loss = 0.09 (60.5 examples/sec; 0.066 sec/batch; 108h:42m:42s remains)
INFO - root - 2019-11-04 00:40:30.099313: step 79770, total loss = 0.58, predict loss = 0.13 (62.9 examples/sec; 0.064 sec/batch; 104h:37m:14s remains)
INFO - root - 2019-11-04 00:40:30.684402: step 79780, total loss = 0.45, predict loss = 0.09 (77.7 examples/sec; 0.051 sec/batch; 84h:41m:10s remains)
INFO - root - 2019-11-04 00:40:31.300030: step 79790, total loss = 0.58, predict loss = 0.13 (69.6 examples/sec; 0.057 sec/batch; 94h:28m:09s remains)
INFO - root - 2019-11-04 00:40:31.897986: step 79800, total loss = 0.61, predict loss = 0.15 (81.7 examples/sec; 0.049 sec/batch; 80h:29m:23s remains)
INFO - root - 2019-11-04 00:40:32.502032: step 79810, total loss = 0.59, predict loss = 0.14 (68.3 examples/sec; 0.059 sec/batch; 96h:18m:43s remains)
INFO - root - 2019-11-04 00:40:33.123386: step 79820, total loss = 0.82, predict loss = 0.20 (72.4 examples/sec; 0.055 sec/batch; 90h:48m:27s remains)
INFO - root - 2019-11-04 00:40:33.796914: step 79830, total loss = 0.54, predict loss = 0.12 (64.6 examples/sec; 0.062 sec/batch; 101h:52m:03s remains)
INFO - root - 2019-11-04 00:40:34.471485: step 79840, total loss = 0.55, predict loss = 0.13 (64.4 examples/sec; 0.062 sec/batch; 102h:03m:49s remains)
INFO - root - 2019-11-04 00:40:35.178888: step 79850, total loss = 0.47, predict loss = 0.11 (65.5 examples/sec; 0.061 sec/batch; 100h:26m:49s remains)
INFO - root - 2019-11-04 00:40:35.784743: step 79860, total loss = 0.57, predict loss = 0.14 (67.3 examples/sec; 0.059 sec/batch; 97h:40m:34s remains)
INFO - root - 2019-11-04 00:40:36.396227: step 79870, total loss = 0.62, predict loss = 0.14 (64.0 examples/sec; 0.063 sec/batch; 102h:47m:53s remains)
INFO - root - 2019-11-04 00:40:37.028858: step 79880, total loss = 0.48, predict loss = 0.11 (76.2 examples/sec; 0.053 sec/batch; 86h:20m:19s remains)
INFO - root - 2019-11-04 00:40:37.654631: step 79890, total loss = 0.57, predict loss = 0.13 (63.6 examples/sec; 0.063 sec/batch; 103h:26m:18s remains)
INFO - root - 2019-11-04 00:40:38.309393: step 79900, total loss = 0.49, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 93h:48m:20s remains)
INFO - root - 2019-11-04 00:40:38.961565: step 79910, total loss = 0.35, predict loss = 0.08 (73.8 examples/sec; 0.054 sec/batch; 89h:09m:51s remains)
INFO - root - 2019-11-04 00:40:39.625869: step 79920, total loss = 0.29, predict loss = 0.05 (59.8 examples/sec; 0.067 sec/batch; 110h:03m:10s remains)
INFO - root - 2019-11-04 00:40:40.303801: step 79930, total loss = 0.36, predict loss = 0.08 (62.1 examples/sec; 0.064 sec/batch; 105h:53m:56s remains)
INFO - root - 2019-11-04 00:40:41.016840: step 79940, total loss = 0.27, predict loss = 0.05 (68.1 examples/sec; 0.059 sec/batch; 96h:34m:33s remains)
INFO - root - 2019-11-04 00:40:41.714689: step 79950, total loss = 0.29, predict loss = 0.06 (60.4 examples/sec; 0.066 sec/batch; 108h:59m:00s remains)
INFO - root - 2019-11-04 00:40:42.866698: step 79960, total loss = 0.33, predict loss = 0.07 (67.8 examples/sec; 0.059 sec/batch; 96h:57m:36s remains)
INFO - root - 2019-11-04 00:40:43.552717: step 79970, total loss = 0.47, predict loss = 0.11 (62.0 examples/sec; 0.064 sec/batch; 106h:00m:54s remains)
INFO - root - 2019-11-04 00:40:44.244239: step 79980, total loss = 0.25, predict loss = 0.05 (72.2 examples/sec; 0.055 sec/batch; 91h:04m:15s remains)
INFO - root - 2019-11-04 00:40:44.911897: step 79990, total loss = 0.34, predict loss = 0.07 (65.5 examples/sec; 0.061 sec/batch; 100h:27m:31s remains)
INFO - root - 2019-11-04 00:40:45.566572: step 80000, total loss = 0.47, predict loss = 0.11 (60.2 examples/sec; 0.066 sec/batch; 109h:19m:55s remains)
INFO - root - 2019-11-04 00:40:46.230230: step 80010, total loss = 0.43, predict loss = 0.09 (70.3 examples/sec; 0.057 sec/batch; 93h:30m:27s remains)
INFO - root - 2019-11-04 00:40:46.859404: step 80020, total loss = 0.40, predict loss = 0.09 (76.4 examples/sec; 0.052 sec/batch; 86h:04m:16s remains)
INFO - root - 2019-11-04 00:40:47.462154: step 80030, total loss = 0.35, predict loss = 0.07 (72.4 examples/sec; 0.055 sec/batch; 90h:51m:09s remains)
INFO - root - 2019-11-04 00:40:48.084981: step 80040, total loss = 0.37, predict loss = 0.08 (73.0 examples/sec; 0.055 sec/batch; 90h:03m:37s remains)
INFO - root - 2019-11-04 00:40:48.714566: step 80050, total loss = 0.30, predict loss = 0.06 (75.0 examples/sec; 0.053 sec/batch; 87h:45m:37s remains)
INFO - root - 2019-11-04 00:40:49.359703: step 80060, total loss = 0.55, predict loss = 0.12 (70.1 examples/sec; 0.057 sec/batch; 93h:46m:58s remains)
INFO - root - 2019-11-04 00:40:50.014568: step 80070, total loss = 0.37, predict loss = 0.08 (72.3 examples/sec; 0.055 sec/batch; 91h:01m:26s remains)
INFO - root - 2019-11-04 00:40:50.636947: step 80080, total loss = 0.42, predict loss = 0.09 (66.4 examples/sec; 0.060 sec/batch; 98h:59m:37s remains)
INFO - root - 2019-11-04 00:40:51.267947: step 80090, total loss = 0.53, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 90h:32m:49s remains)
INFO - root - 2019-11-04 00:40:51.893340: step 80100, total loss = 0.50, predict loss = 0.12 (76.9 examples/sec; 0.052 sec/batch; 85h:33m:23s remains)
INFO - root - 2019-11-04 00:40:52.500762: step 80110, total loss = 0.64, predict loss = 0.15 (69.1 examples/sec; 0.058 sec/batch; 95h:09m:33s remains)
INFO - root - 2019-11-04 00:40:53.145265: step 80120, total loss = 0.61, predict loss = 0.14 (76.2 examples/sec; 0.053 sec/batch; 86h:22m:33s remains)
INFO - root - 2019-11-04 00:40:53.784814: step 80130, total loss = 0.49, predict loss = 0.11 (66.7 examples/sec; 0.060 sec/batch; 98h:34m:38s remains)
INFO - root - 2019-11-04 00:40:54.438867: step 80140, total loss = 0.27, predict loss = 0.05 (68.6 examples/sec; 0.058 sec/batch; 95h:51m:01s remains)
INFO - root - 2019-11-04 00:40:55.085668: step 80150, total loss = 0.49, predict loss = 0.11 (70.4 examples/sec; 0.057 sec/batch; 93h:24m:44s remains)
INFO - root - 2019-11-04 00:40:55.700854: step 80160, total loss = 0.42, predict loss = 0.10 (65.4 examples/sec; 0.061 sec/batch; 100h:37m:38s remains)
INFO - root - 2019-11-04 00:40:56.373875: step 80170, total loss = 0.22, predict loss = 0.04 (66.7 examples/sec; 0.060 sec/batch; 98h:40m:40s remains)
INFO - root - 2019-11-04 00:40:57.074110: step 80180, total loss = 0.32, predict loss = 0.06 (62.7 examples/sec; 0.064 sec/batch; 104h:54m:32s remains)
INFO - root - 2019-11-04 00:40:57.748936: step 80190, total loss = 0.21, predict loss = 0.04 (66.2 examples/sec; 0.060 sec/batch; 99h:22m:01s remains)
INFO - root - 2019-11-04 00:40:58.454123: step 80200, total loss = 0.26, predict loss = 0.06 (59.1 examples/sec; 0.068 sec/batch; 111h:19m:36s remains)
INFO - root - 2019-11-04 00:40:59.097229: step 80210, total loss = 0.39, predict loss = 0.09 (72.1 examples/sec; 0.055 sec/batch; 91h:14m:45s remains)
INFO - root - 2019-11-04 00:40:59.716763: step 80220, total loss = 0.39, predict loss = 0.09 (74.0 examples/sec; 0.054 sec/batch; 88h:51m:00s remains)
INFO - root - 2019-11-04 00:41:00.345156: step 80230, total loss = 0.53, predict loss = 0.13 (74.2 examples/sec; 0.054 sec/batch; 88h:36m:04s remains)
INFO - root - 2019-11-04 00:41:00.958140: step 80240, total loss = 0.41, predict loss = 0.09 (76.7 examples/sec; 0.052 sec/batch; 85h:45m:42s remains)
INFO - root - 2019-11-04 00:41:01.625910: step 80250, total loss = 0.49, predict loss = 0.11 (69.1 examples/sec; 0.058 sec/batch; 95h:10m:43s remains)
INFO - root - 2019-11-04 00:41:02.248036: step 80260, total loss = 0.49, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 90h:39m:22s remains)
INFO - root - 2019-11-04 00:41:02.878363: step 80270, total loss = 0.30, predict loss = 0.07 (62.5 examples/sec; 0.064 sec/batch; 105h:13m:08s remains)
INFO - root - 2019-11-04 00:41:03.504404: step 80280, total loss = 0.42, predict loss = 0.10 (71.2 examples/sec; 0.056 sec/batch; 92h:20m:28s remains)
INFO - root - 2019-11-04 00:41:04.133702: step 80290, total loss = 0.48, predict loss = 0.11 (85.4 examples/sec; 0.047 sec/batch; 77h:02m:35s remains)
INFO - root - 2019-11-04 00:41:04.802700: step 80300, total loss = 0.40, predict loss = 0.09 (67.0 examples/sec; 0.060 sec/batch; 98h:08m:25s remains)
INFO - root - 2019-11-04 00:41:05.450013: step 80310, total loss = 0.31, predict loss = 0.07 (83.9 examples/sec; 0.048 sec/batch; 78h:24m:42s remains)
INFO - root - 2019-11-04 00:41:06.103223: step 80320, total loss = 0.50, predict loss = 0.12 (73.3 examples/sec; 0.055 sec/batch; 89h:42m:15s remains)
INFO - root - 2019-11-04 00:41:06.723526: step 80330, total loss = 0.47, predict loss = 0.10 (68.0 examples/sec; 0.059 sec/batch; 96h:42m:35s remains)
INFO - root - 2019-11-04 00:41:07.365156: step 80340, total loss = 0.66, predict loss = 0.15 (75.5 examples/sec; 0.053 sec/batch; 87h:04m:18s remains)
INFO - root - 2019-11-04 00:41:08.005344: step 80350, total loss = 0.49, predict loss = 0.11 (67.9 examples/sec; 0.059 sec/batch; 96h:48m:25s remains)
INFO - root - 2019-11-04 00:41:08.662908: step 80360, total loss = 0.56, predict loss = 0.13 (65.9 examples/sec; 0.061 sec/batch; 99h:47m:25s remains)
INFO - root - 2019-11-04 00:41:09.278295: step 80370, total loss = 0.54, predict loss = 0.13 (70.4 examples/sec; 0.057 sec/batch; 93h:25m:21s remains)
INFO - root - 2019-11-04 00:41:09.897362: step 80380, total loss = 0.39, predict loss = 0.08 (73.8 examples/sec; 0.054 sec/batch; 89h:03m:49s remains)
INFO - root - 2019-11-04 00:41:10.502370: step 80390, total loss = 0.44, predict loss = 0.10 (76.0 examples/sec; 0.053 sec/batch; 86h:34m:04s remains)
INFO - root - 2019-11-04 00:41:11.166282: step 80400, total loss = 0.34, predict loss = 0.06 (63.1 examples/sec; 0.063 sec/batch; 104h:18m:11s remains)
INFO - root - 2019-11-04 00:41:11.788102: step 80410, total loss = 0.42, predict loss = 0.09 (76.5 examples/sec; 0.052 sec/batch; 86h:01m:04s remains)
INFO - root - 2019-11-04 00:41:12.439923: step 80420, total loss = 0.45, predict loss = 0.11 (75.8 examples/sec; 0.053 sec/batch; 86h:44m:00s remains)
INFO - root - 2019-11-04 00:41:13.096938: step 80430, total loss = 0.41, predict loss = 0.09 (67.6 examples/sec; 0.059 sec/batch; 97h:18m:24s remains)
INFO - root - 2019-11-04 00:41:13.752294: step 80440, total loss = 0.50, predict loss = 0.11 (63.2 examples/sec; 0.063 sec/batch; 104h:05m:58s remains)
INFO - root - 2019-11-04 00:41:14.380555: step 80450, total loss = 0.41, predict loss = 0.09 (60.0 examples/sec; 0.067 sec/batch; 109h:39m:48s remains)
INFO - root - 2019-11-04 00:41:15.072327: step 80460, total loss = 0.33, predict loss = 0.08 (72.3 examples/sec; 0.055 sec/batch; 91h:01m:33s remains)
INFO - root - 2019-11-04 00:41:15.783401: step 80470, total loss = 0.55, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 101h:00m:39s remains)
INFO - root - 2019-11-04 00:41:16.439888: step 80480, total loss = 0.38, predict loss = 0.08 (66.1 examples/sec; 0.060 sec/batch; 99h:28m:48s remains)
INFO - root - 2019-11-04 00:41:17.079200: step 80490, total loss = 0.61, predict loss = 0.14 (77.3 examples/sec; 0.052 sec/batch; 85h:08m:15s remains)
INFO - root - 2019-11-04 00:41:17.700289: step 80500, total loss = 0.41, predict loss = 0.09 (73.3 examples/sec; 0.055 sec/batch; 89h:41m:28s remains)
INFO - root - 2019-11-04 00:41:18.340435: step 80510, total loss = 0.59, predict loss = 0.14 (78.3 examples/sec; 0.051 sec/batch; 84h:00m:43s remains)
INFO - root - 2019-11-04 00:41:19.017217: step 80520, total loss = 0.67, predict loss = 0.15 (61.6 examples/sec; 0.065 sec/batch; 106h:47m:34s remains)
INFO - root - 2019-11-04 00:41:19.742488: step 80530, total loss = 0.62, predict loss = 0.14 (61.3 examples/sec; 0.065 sec/batch; 107h:13m:30s remains)
INFO - root - 2019-11-04 00:41:20.402078: step 80540, total loss = 0.71, predict loss = 0.16 (76.0 examples/sec; 0.053 sec/batch; 86h:35m:01s remains)
INFO - root - 2019-11-04 00:41:21.022582: step 80550, total loss = 0.86, predict loss = 0.20 (67.0 examples/sec; 0.060 sec/batch; 98h:05m:49s remains)
INFO - root - 2019-11-04 00:41:21.685799: step 80560, total loss = 0.85, predict loss = 0.20 (72.8 examples/sec; 0.055 sec/batch; 90h:20m:33s remains)
INFO - root - 2019-11-04 00:41:22.322286: step 80570, total loss = 0.65, predict loss = 0.15 (75.8 examples/sec; 0.053 sec/batch; 86h:46m:43s remains)
INFO - root - 2019-11-04 00:41:22.964104: step 80580, total loss = 0.74, predict loss = 0.18 (78.4 examples/sec; 0.051 sec/batch; 83h:54m:00s remains)
INFO - root - 2019-11-04 00:41:23.669702: step 80590, total loss = 0.81, predict loss = 0.20 (57.1 examples/sec; 0.070 sec/batch; 115h:15m:07s remains)
INFO - root - 2019-11-04 00:41:24.347291: step 80600, total loss = 0.63, predict loss = 0.15 (77.1 examples/sec; 0.052 sec/batch; 85h:17m:19s remains)
INFO - root - 2019-11-04 00:41:24.959999: step 80610, total loss = 0.57, predict loss = 0.13 (72.5 examples/sec; 0.055 sec/batch; 90h:46m:12s remains)
INFO - root - 2019-11-04 00:41:25.581258: step 80620, total loss = 0.51, predict loss = 0.12 (69.9 examples/sec; 0.057 sec/batch; 94h:04m:45s remains)
INFO - root - 2019-11-04 00:41:26.248879: step 80630, total loss = 0.48, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 96h:31m:59s remains)
INFO - root - 2019-11-04 00:41:26.910078: step 80640, total loss = 0.49, predict loss = 0.11 (68.9 examples/sec; 0.058 sec/batch; 95h:28m:14s remains)
INFO - root - 2019-11-04 00:41:27.583356: step 80650, total loss = 0.48, predict loss = 0.11 (63.6 examples/sec; 0.063 sec/batch; 103h:27m:06s remains)
INFO - root - 2019-11-04 00:41:28.264952: step 80660, total loss = 0.57, predict loss = 0.13 (71.8 examples/sec; 0.056 sec/batch; 91h:32m:50s remains)
INFO - root - 2019-11-04 00:41:28.944609: step 80670, total loss = 0.45, predict loss = 0.10 (58.9 examples/sec; 0.068 sec/batch; 111h:38m:39s remains)
INFO - root - 2019-11-04 00:41:29.605376: step 80680, total loss = 0.52, predict loss = 0.12 (73.4 examples/sec; 0.054 sec/batch; 89h:34m:02s remains)
INFO - root - 2019-11-04 00:41:30.275915: step 80690, total loss = 0.56, predict loss = 0.14 (66.3 examples/sec; 0.060 sec/batch; 99h:09m:24s remains)
INFO - root - 2019-11-04 00:41:30.937540: step 80700, total loss = 0.54, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 93h:41m:38s remains)
INFO - root - 2019-11-04 00:41:31.606356: step 80710, total loss = 0.71, predict loss = 0.16 (64.2 examples/sec; 0.062 sec/batch; 102h:31m:02s remains)
INFO - root - 2019-11-04 00:41:32.258491: step 80720, total loss = 0.37, predict loss = 0.08 (75.5 examples/sec; 0.053 sec/batch; 87h:06m:00s remains)
INFO - root - 2019-11-04 00:41:32.931760: step 80730, total loss = 0.48, predict loss = 0.10 (64.8 examples/sec; 0.062 sec/batch; 101h:27m:47s remains)
INFO - root - 2019-11-04 00:41:33.577305: step 80740, total loss = 0.55, predict loss = 0.13 (66.7 examples/sec; 0.060 sec/batch; 98h:33m:29s remains)
INFO - root - 2019-11-04 00:41:34.219151: step 80750, total loss = 0.47, predict loss = 0.10 (77.6 examples/sec; 0.052 sec/batch; 84h:44m:58s remains)
INFO - root - 2019-11-04 00:41:34.873813: step 80760, total loss = 0.41, predict loss = 0.09 (57.8 examples/sec; 0.069 sec/batch; 113h:48m:47s remains)
INFO - root - 2019-11-04 00:41:35.484868: step 80770, total loss = 0.37, predict loss = 0.08 (88.8 examples/sec; 0.045 sec/batch; 74h:04m:47s remains)
INFO - root - 2019-11-04 00:41:36.097061: step 80780, total loss = 0.37, predict loss = 0.08 (66.8 examples/sec; 0.060 sec/batch; 98h:28m:49s remains)
INFO - root - 2019-11-04 00:41:36.752921: step 80790, total loss = 0.42, predict loss = 0.10 (67.0 examples/sec; 0.060 sec/batch; 98h:05m:52s remains)
INFO - root - 2019-11-04 00:41:37.354047: step 80800, total loss = 0.34, predict loss = 0.07 (75.5 examples/sec; 0.053 sec/batch; 87h:06m:24s remains)
INFO - root - 2019-11-04 00:41:37.964216: step 80810, total loss = 0.40, predict loss = 0.09 (69.2 examples/sec; 0.058 sec/batch; 94h:58m:33s remains)
INFO - root - 2019-11-04 00:41:38.612297: step 80820, total loss = 0.43, predict loss = 0.10 (65.6 examples/sec; 0.061 sec/batch; 100h:18m:13s remains)
INFO - root - 2019-11-04 00:41:39.292665: step 80830, total loss = 0.37, predict loss = 0.08 (70.4 examples/sec; 0.057 sec/batch; 93h:23m:25s remains)
INFO - root - 2019-11-04 00:41:39.966304: step 80840, total loss = 0.51, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 97h:43m:17s remains)
INFO - root - 2019-11-04 00:41:40.626879: step 80850, total loss = 0.47, predict loss = 0.11 (71.2 examples/sec; 0.056 sec/batch; 92h:18m:41s remains)
INFO - root - 2019-11-04 00:41:41.240390: step 80860, total loss = 0.40, predict loss = 0.08 (63.9 examples/sec; 0.063 sec/batch; 102h:54m:58s remains)
INFO - root - 2019-11-04 00:41:41.874512: step 80870, total loss = 0.50, predict loss = 0.11 (78.4 examples/sec; 0.051 sec/batch; 83h:50m:48s remains)
INFO - root - 2019-11-04 00:41:42.510831: step 80880, total loss = 0.50, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 92h:15m:59s remains)
INFO - root - 2019-11-04 00:41:43.123209: step 80890, total loss = 0.59, predict loss = 0.14 (80.2 examples/sec; 0.050 sec/batch; 81h:59m:29s remains)
INFO - root - 2019-11-04 00:41:43.792439: step 80900, total loss = 0.38, predict loss = 0.08 (67.9 examples/sec; 0.059 sec/batch; 96h:49m:40s remains)
INFO - root - 2019-11-04 00:41:44.451232: step 80910, total loss = 0.51, predict loss = 0.11 (69.5 examples/sec; 0.058 sec/batch; 94h:40m:46s remains)
INFO - root - 2019-11-04 00:41:45.135448: step 80920, total loss = 0.44, predict loss = 0.10 (67.1 examples/sec; 0.060 sec/batch; 97h:57m:42s remains)
INFO - root - 2019-11-04 00:41:45.791110: step 80930, total loss = 0.60, predict loss = 0.15 (67.2 examples/sec; 0.060 sec/batch; 97h:51m:47s remains)
INFO - root - 2019-11-04 00:41:46.408008: step 80940, total loss = 0.77, predict loss = 0.18 (74.4 examples/sec; 0.054 sec/batch; 88h:22m:26s remains)
INFO - root - 2019-11-04 00:41:47.047088: step 80950, total loss = 0.48, predict loss = 0.11 (65.7 examples/sec; 0.061 sec/batch; 100h:10m:32s remains)
INFO - root - 2019-11-04 00:41:47.722298: step 80960, total loss = 0.59, predict loss = 0.14 (62.6 examples/sec; 0.064 sec/batch; 105h:00m:40s remains)
INFO - root - 2019-11-04 00:41:48.416702: step 80970, total loss = 0.56, predict loss = 0.13 (71.2 examples/sec; 0.056 sec/batch; 92h:21m:04s remains)
INFO - root - 2019-11-04 00:41:49.072075: step 80980, total loss = 0.68, predict loss = 0.17 (71.1 examples/sec; 0.056 sec/batch; 92h:28m:13s remains)
INFO - root - 2019-11-04 00:41:49.740197: step 80990, total loss = 0.78, predict loss = 0.18 (65.3 examples/sec; 0.061 sec/batch; 100h:46m:28s remains)
INFO - root - 2019-11-04 00:41:50.412920: step 81000, total loss = 0.62, predict loss = 0.16 (65.7 examples/sec; 0.061 sec/batch; 100h:08m:51s remains)
INFO - root - 2019-11-04 00:41:51.072922: step 81010, total loss = 0.63, predict loss = 0.15 (68.6 examples/sec; 0.058 sec/batch; 95h:53m:45s remains)
INFO - root - 2019-11-04 00:41:51.705293: step 81020, total loss = 0.55, predict loss = 0.13 (65.8 examples/sec; 0.061 sec/batch; 99h:54m:34s remains)
INFO - root - 2019-11-04 00:41:52.363707: step 81030, total loss = 0.59, predict loss = 0.14 (65.7 examples/sec; 0.061 sec/batch; 100h:10m:10s remains)
INFO - root - 2019-11-04 00:41:53.019501: step 81040, total loss = 0.45, predict loss = 0.09 (64.7 examples/sec; 0.062 sec/batch; 101h:40m:47s remains)
INFO - root - 2019-11-04 00:41:53.691477: step 81050, total loss = 0.40, predict loss = 0.10 (67.2 examples/sec; 0.059 sec/batch; 97h:48m:38s remains)
INFO - root - 2019-11-04 00:41:54.324073: step 81060, total loss = 0.40, predict loss = 0.09 (75.9 examples/sec; 0.053 sec/batch; 86h:38m:00s remains)
INFO - root - 2019-11-04 00:41:54.937433: step 81070, total loss = 0.47, predict loss = 0.11 (70.0 examples/sec; 0.057 sec/batch; 93h:55m:18s remains)
INFO - root - 2019-11-04 00:41:55.538838: step 81080, total loss = 0.44, predict loss = 0.11 (78.3 examples/sec; 0.051 sec/batch; 83h:59m:22s remains)
INFO - root - 2019-11-04 00:41:56.146020: step 81090, total loss = 0.51, predict loss = 0.12 (67.6 examples/sec; 0.059 sec/batch; 97h:19m:21s remains)
INFO - root - 2019-11-04 00:41:56.776274: step 81100, total loss = 0.30, predict loss = 0.07 (72.3 examples/sec; 0.055 sec/batch; 91h:01m:16s remains)
INFO - root - 2019-11-04 00:41:57.391677: step 81110, total loss = 0.38, predict loss = 0.09 (68.8 examples/sec; 0.058 sec/batch; 95h:31m:42s remains)
INFO - root - 2019-11-04 00:41:58.041881: step 81120, total loss = 0.42, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 94h:24m:53s remains)
INFO - root - 2019-11-04 00:41:58.681102: step 81130, total loss = 0.52, predict loss = 0.12 (87.2 examples/sec; 0.046 sec/batch; 75h:26m:51s remains)
INFO - root - 2019-11-04 00:41:59.330033: step 81140, total loss = 0.53, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 95h:24m:05s remains)
INFO - root - 2019-11-04 00:41:59.963525: step 81150, total loss = 0.62, predict loss = 0.14 (68.8 examples/sec; 0.058 sec/batch; 95h:35m:41s remains)
INFO - root - 2019-11-04 00:42:00.635166: step 81160, total loss = 0.62, predict loss = 0.14 (63.3 examples/sec; 0.063 sec/batch; 103h:49m:06s remains)
INFO - root - 2019-11-04 00:42:01.260023: step 81170, total loss = 0.46, predict loss = 0.11 (76.1 examples/sec; 0.053 sec/batch; 86h:25m:26s remains)
INFO - root - 2019-11-04 00:42:01.896167: step 81180, total loss = 0.64, predict loss = 0.15 (67.6 examples/sec; 0.059 sec/batch; 97h:15m:57s remains)
INFO - root - 2019-11-04 00:42:02.517393: step 81190, total loss = 0.60, predict loss = 0.15 (68.4 examples/sec; 0.058 sec/batch; 96h:07m:17s remains)
INFO - root - 2019-11-04 00:42:03.117619: step 81200, total loss = 0.59, predict loss = 0.14 (86.3 examples/sec; 0.046 sec/batch; 76h:14m:40s remains)
INFO - root - 2019-11-04 00:42:03.744608: step 81210, total loss = 0.66, predict loss = 0.16 (74.1 examples/sec; 0.054 sec/batch; 88h:42m:35s remains)
INFO - root - 2019-11-04 00:42:04.366913: step 81220, total loss = 0.50, predict loss = 0.12 (67.0 examples/sec; 0.060 sec/batch; 98h:12m:24s remains)
INFO - root - 2019-11-04 00:42:05.011733: step 81230, total loss = 0.59, predict loss = 0.14 (62.7 examples/sec; 0.064 sec/batch; 104h:52m:30s remains)
INFO - root - 2019-11-04 00:42:05.631276: step 81240, total loss = 0.64, predict loss = 0.16 (70.5 examples/sec; 0.057 sec/batch; 93h:15m:05s remains)
INFO - root - 2019-11-04 00:42:06.257899: step 81250, total loss = 0.57, predict loss = 0.13 (66.5 examples/sec; 0.060 sec/batch; 98h:53m:01s remains)
INFO - root - 2019-11-04 00:42:06.857120: step 81260, total loss = 0.52, predict loss = 0.12 (76.2 examples/sec; 0.052 sec/batch; 86h:17m:09s remains)
INFO - root - 2019-11-04 00:42:07.466609: step 81270, total loss = 0.63, predict loss = 0.15 (67.8 examples/sec; 0.059 sec/batch; 97h:03m:32s remains)
INFO - root - 2019-11-04 00:42:08.128371: step 81280, total loss = 0.54, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 95h:24m:18s remains)
INFO - root - 2019-11-04 00:42:08.754416: step 81290, total loss = 0.52, predict loss = 0.12 (63.7 examples/sec; 0.063 sec/batch; 103h:10m:51s remains)
INFO - root - 2019-11-04 00:42:09.474011: step 81300, total loss = 0.49, predict loss = 0.12 (64.7 examples/sec; 0.062 sec/batch; 101h:37m:06s remains)
INFO - root - 2019-11-04 00:42:10.133023: step 81310, total loss = 0.44, predict loss = 0.10 (75.5 examples/sec; 0.053 sec/batch; 87h:03m:35s remains)
INFO - root - 2019-11-04 00:42:10.802506: step 81320, total loss = 0.50, predict loss = 0.11 (62.7 examples/sec; 0.064 sec/batch; 104h:55m:59s remains)
INFO - root - 2019-11-04 00:42:11.433250: step 81330, total loss = 0.50, predict loss = 0.11 (78.0 examples/sec; 0.051 sec/batch; 84h:18m:04s remains)
INFO - root - 2019-11-04 00:42:12.113114: step 81340, total loss = 0.43, predict loss = 0.10 (58.3 examples/sec; 0.069 sec/batch; 112h:45m:16s remains)
INFO - root - 2019-11-04 00:42:12.773976: step 81350, total loss = 0.51, predict loss = 0.12 (69.6 examples/sec; 0.057 sec/batch; 94h:29m:19s remains)
INFO - root - 2019-11-04 00:42:13.453172: step 81360, total loss = 0.46, predict loss = 0.10 (63.1 examples/sec; 0.063 sec/batch; 104h:14m:13s remains)
INFO - root - 2019-11-04 00:42:14.069784: step 81370, total loss = 0.50, predict loss = 0.12 (78.6 examples/sec; 0.051 sec/batch; 83h:40m:36s remains)
INFO - root - 2019-11-04 00:42:14.675851: step 81380, total loss = 0.48, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 87h:04m:32s remains)
INFO - root - 2019-11-04 00:42:15.293696: step 81390, total loss = 0.47, predict loss = 0.11 (65.2 examples/sec; 0.061 sec/batch; 100h:47m:40s remains)
INFO - root - 2019-11-04 00:42:15.923123: step 81400, total loss = 0.42, predict loss = 0.09 (70.7 examples/sec; 0.057 sec/batch; 93h:00m:47s remains)
INFO - root - 2019-11-04 00:42:16.598441: step 81410, total loss = 0.54, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 97h:45m:33s remains)
INFO - root - 2019-11-04 00:42:17.265524: step 81420, total loss = 0.43, predict loss = 0.09 (64.1 examples/sec; 0.062 sec/batch; 102h:36m:05s remains)
INFO - root - 2019-11-04 00:42:17.939657: step 81430, total loss = 0.59, predict loss = 0.13 (62.3 examples/sec; 0.064 sec/batch; 105h:32m:20s remains)
INFO - root - 2019-11-04 00:42:18.623244: step 81440, total loss = 0.63, predict loss = 0.15 (71.4 examples/sec; 0.056 sec/batch; 92h:05m:09s remains)
INFO - root - 2019-11-04 00:42:19.304304: step 81450, total loss = 0.51, predict loss = 0.12 (65.5 examples/sec; 0.061 sec/batch; 100h:25m:16s remains)
INFO - root - 2019-11-04 00:42:19.963273: step 81460, total loss = 0.47, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 98h:51m:16s remains)
INFO - root - 2019-11-04 00:42:20.636920: step 81470, total loss = 0.50, predict loss = 0.12 (65.8 examples/sec; 0.061 sec/batch; 99h:52m:46s remains)
INFO - root - 2019-11-04 00:42:21.304823: step 81480, total loss = 0.50, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 94h:53m:46s remains)
INFO - root - 2019-11-04 00:42:21.912116: step 81490, total loss = 0.55, predict loss = 0.13 (79.1 examples/sec; 0.051 sec/batch; 83h:05m:04s remains)
INFO - root - 2019-11-04 00:42:22.559381: step 81500, total loss = 0.57, predict loss = 0.13 (79.7 examples/sec; 0.050 sec/batch; 82h:33m:13s remains)
INFO - root - 2019-11-04 00:42:23.207800: step 81510, total loss = 0.49, predict loss = 0.11 (73.3 examples/sec; 0.055 sec/batch; 89h:45m:28s remains)
INFO - root - 2019-11-04 00:42:23.854214: step 81520, total loss = 0.52, predict loss = 0.13 (73.5 examples/sec; 0.054 sec/batch; 89h:31m:24s remains)
INFO - root - 2019-11-04 00:42:24.511949: step 81530, total loss = 0.58, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 100h:45m:17s remains)
INFO - root - 2019-11-04 00:42:25.166175: step 81540, total loss = 0.74, predict loss = 0.17 (70.4 examples/sec; 0.057 sec/batch; 93h:24m:28s remains)
INFO - root - 2019-11-04 00:42:25.815829: step 81550, total loss = 0.61, predict loss = 0.15 (72.0 examples/sec; 0.056 sec/batch; 91h:16m:43s remains)
INFO - root - 2019-11-04 00:42:26.459664: step 81560, total loss = 0.63, predict loss = 0.14 (69.1 examples/sec; 0.058 sec/batch; 95h:05m:55s remains)
INFO - root - 2019-11-04 00:42:27.085590: step 81570, total loss = 0.48, predict loss = 0.11 (74.6 examples/sec; 0.054 sec/batch; 88h:10m:58s remains)
INFO - root - 2019-11-04 00:42:27.739144: step 81580, total loss = 0.62, predict loss = 0.14 (62.1 examples/sec; 0.064 sec/batch; 105h:58m:01s remains)
INFO - root - 2019-11-04 00:42:28.326160: step 81590, total loss = 0.60, predict loss = 0.15 (79.8 examples/sec; 0.050 sec/batch; 82h:25m:03s remains)
INFO - root - 2019-11-04 00:42:28.917537: step 81600, total loss = 0.65, predict loss = 0.16 (76.1 examples/sec; 0.053 sec/batch; 86h:24m:34s remains)
INFO - root - 2019-11-04 00:42:29.543272: step 81610, total loss = 0.68, predict loss = 0.16 (81.1 examples/sec; 0.049 sec/batch; 81h:03m:42s remains)
INFO - root - 2019-11-04 00:42:30.155625: step 81620, total loss = 0.67, predict loss = 0.16 (78.0 examples/sec; 0.051 sec/batch; 84h:19m:08s remains)
INFO - root - 2019-11-04 00:42:30.788867: step 81630, total loss = 0.77, predict loss = 0.17 (76.0 examples/sec; 0.053 sec/batch; 86h:30m:18s remains)
INFO - root - 2019-11-04 00:42:31.416503: step 81640, total loss = 0.74, predict loss = 0.17 (67.1 examples/sec; 0.060 sec/batch; 98h:02m:16s remains)
INFO - root - 2019-11-04 00:42:32.041609: step 81650, total loss = 0.51, predict loss = 0.12 (70.1 examples/sec; 0.057 sec/batch; 93h:47m:03s remains)
INFO - root - 2019-11-04 00:42:32.659260: step 81660, total loss = 0.48, predict loss = 0.11 (77.2 examples/sec; 0.052 sec/batch; 85h:12m:53s remains)
INFO - root - 2019-11-04 00:42:33.269929: step 81670, total loss = 0.48, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 90h:23m:15s remains)
INFO - root - 2019-11-04 00:42:33.888486: step 81680, total loss = 0.31, predict loss = 0.07 (70.6 examples/sec; 0.057 sec/batch; 93h:05m:35s remains)
INFO - root - 2019-11-04 00:42:34.497723: step 81690, total loss = 0.43, predict loss = 0.10 (71.4 examples/sec; 0.056 sec/batch; 92h:04m:20s remains)
INFO - root - 2019-11-04 00:42:35.173730: step 81700, total loss = 0.54, predict loss = 0.12 (67.5 examples/sec; 0.059 sec/batch; 97h:22m:32s remains)
INFO - root - 2019-11-04 00:42:35.801491: step 81710, total loss = 0.48, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 93h:45m:25s remains)
INFO - root - 2019-11-04 00:42:36.420312: step 81720, total loss = 0.57, predict loss = 0.14 (73.9 examples/sec; 0.054 sec/batch; 88h:56m:33s remains)
INFO - root - 2019-11-04 00:42:37.073035: step 81730, total loss = 0.69, predict loss = 0.17 (68.3 examples/sec; 0.059 sec/batch; 96h:12m:55s remains)
INFO - root - 2019-11-04 00:42:37.687531: step 81740, total loss = 0.60, predict loss = 0.15 (65.2 examples/sec; 0.061 sec/batch; 100h:46m:56s remains)
INFO - root - 2019-11-04 00:42:38.306798: step 81750, total loss = 0.44, predict loss = 0.10 (73.1 examples/sec; 0.055 sec/batch; 89h:59m:06s remains)
INFO - root - 2019-11-04 00:42:38.975916: step 81760, total loss = 0.64, predict loss = 0.15 (72.4 examples/sec; 0.055 sec/batch; 90h:50m:31s remains)
INFO - root - 2019-11-04 00:42:39.649343: step 81770, total loss = 0.68, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 95h:44m:58s remains)
INFO - root - 2019-11-04 00:42:40.298475: step 81780, total loss = 0.53, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 91h:45m:18s remains)
INFO - root - 2019-11-04 00:42:40.937690: step 81790, total loss = 0.49, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 98h:11m:06s remains)
INFO - root - 2019-11-04 00:42:41.522478: step 81800, total loss = 0.51, predict loss = 0.12 (89.9 examples/sec; 0.044 sec/batch; 73h:07m:26s remains)
INFO - root - 2019-11-04 00:42:42.027822: step 81810, total loss = 0.62, predict loss = 0.14 (90.6 examples/sec; 0.044 sec/batch; 72h:33m:53s remains)
INFO - root - 2019-11-04 00:42:42.496518: step 81820, total loss = 0.44, predict loss = 0.09 (90.6 examples/sec; 0.044 sec/batch; 72h:36m:45s remains)
INFO - root - 2019-11-04 00:42:43.618319: step 81830, total loss = 0.66, predict loss = 0.16 (78.1 examples/sec; 0.051 sec/batch; 84h:09m:07s remains)
INFO - root - 2019-11-04 00:42:44.263038: step 81840, total loss = 0.38, predict loss = 0.08 (73.5 examples/sec; 0.054 sec/batch; 89h:24m:51s remains)
INFO - root - 2019-11-04 00:42:44.899988: step 81850, total loss = 0.52, predict loss = 0.12 (70.4 examples/sec; 0.057 sec/batch; 93h:22m:58s remains)
INFO - root - 2019-11-04 00:42:45.566341: step 81860, total loss = 0.69, predict loss = 0.17 (67.6 examples/sec; 0.059 sec/batch; 97h:19m:01s remains)
INFO - root - 2019-11-04 00:42:46.236807: step 81870, total loss = 0.59, predict loss = 0.14 (61.8 examples/sec; 0.065 sec/batch; 106h:23m:04s remains)
INFO - root - 2019-11-04 00:42:46.956177: step 81880, total loss = 0.50, predict loss = 0.11 (62.5 examples/sec; 0.064 sec/batch; 105h:16m:45s remains)
INFO - root - 2019-11-04 00:42:47.612472: step 81890, total loss = 0.61, predict loss = 0.14 (64.9 examples/sec; 0.062 sec/batch; 101h:23m:09s remains)
INFO - root - 2019-11-04 00:42:48.254461: step 81900, total loss = 0.59, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 91h:02m:32s remains)
INFO - root - 2019-11-04 00:42:48.899280: step 81910, total loss = 0.59, predict loss = 0.13 (84.5 examples/sec; 0.047 sec/batch; 77h:47m:36s remains)
INFO - root - 2019-11-04 00:42:49.523837: step 81920, total loss = 0.62, predict loss = 0.14 (68.9 examples/sec; 0.058 sec/batch; 95h:22m:12s remains)
INFO - root - 2019-11-04 00:42:50.162058: step 81930, total loss = 0.54, predict loss = 0.12 (61.3 examples/sec; 0.065 sec/batch; 107h:11m:02s remains)
INFO - root - 2019-11-04 00:42:50.826788: step 81940, total loss = 0.46, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 95h:52m:59s remains)
INFO - root - 2019-11-04 00:42:51.497922: step 81950, total loss = 0.48, predict loss = 0.11 (72.4 examples/sec; 0.055 sec/batch; 90h:49m:05s remains)
INFO - root - 2019-11-04 00:42:52.141171: step 81960, total loss = 0.59, predict loss = 0.13 (64.4 examples/sec; 0.062 sec/batch; 102h:02m:40s remains)
INFO - root - 2019-11-04 00:42:52.806270: step 81970, total loss = 0.45, predict loss = 0.10 (62.1 examples/sec; 0.064 sec/batch; 105h:52m:44s remains)
INFO - root - 2019-11-04 00:42:53.511709: step 81980, total loss = 0.64, predict loss = 0.15 (66.8 examples/sec; 0.060 sec/batch; 98h:26m:45s remains)
INFO - root - 2019-11-04 00:42:54.183326: step 81990, total loss = 0.46, predict loss = 0.10 (67.0 examples/sec; 0.060 sec/batch; 98h:08m:15s remains)
INFO - root - 2019-11-04 00:42:54.860538: step 82000, total loss = 0.49, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 90h:20m:05s remains)
INFO - root - 2019-11-04 00:42:55.513861: step 82010, total loss = 0.50, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 96h:08m:43s remains)
INFO - root - 2019-11-04 00:42:56.147843: step 82020, total loss = 0.44, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 94h:34m:43s remains)
INFO - root - 2019-11-04 00:42:56.764302: step 82030, total loss = 0.30, predict loss = 0.05 (75.9 examples/sec; 0.053 sec/batch; 86h:40m:16s remains)
INFO - root - 2019-11-04 00:42:57.426497: step 82040, total loss = 0.28, predict loss = 0.05 (72.5 examples/sec; 0.055 sec/batch; 90h:39m:39s remains)
INFO - root - 2019-11-04 00:42:58.072596: step 82050, total loss = 0.36, predict loss = 0.08 (78.4 examples/sec; 0.051 sec/batch; 83h:53m:35s remains)
INFO - root - 2019-11-04 00:42:58.690380: step 82060, total loss = 0.32, predict loss = 0.07 (73.0 examples/sec; 0.055 sec/batch; 90h:04m:53s remains)
INFO - root - 2019-11-04 00:42:59.333818: step 82070, total loss = 0.46, predict loss = 0.10 (77.0 examples/sec; 0.052 sec/batch; 85h:26m:03s remains)
INFO - root - 2019-11-04 00:43:00.006064: step 82080, total loss = 0.41, predict loss = 0.10 (74.5 examples/sec; 0.054 sec/batch; 88h:14m:46s remains)
INFO - root - 2019-11-04 00:43:00.666206: step 82090, total loss = 0.65, predict loss = 0.15 (71.5 examples/sec; 0.056 sec/batch; 92h:01m:13s remains)
INFO - root - 2019-11-04 00:43:01.325304: step 82100, total loss = 0.50, predict loss = 0.11 (65.2 examples/sec; 0.061 sec/batch; 100h:46m:48s remains)
INFO - root - 2019-11-04 00:43:01.958956: step 82110, total loss = 0.58, predict loss = 0.13 (67.8 examples/sec; 0.059 sec/batch; 96h:59m:05s remains)
INFO - root - 2019-11-04 00:43:02.618090: step 82120, total loss = 0.60, predict loss = 0.14 (72.7 examples/sec; 0.055 sec/batch; 90h:27m:33s remains)
INFO - root - 2019-11-04 00:43:03.251036: step 82130, total loss = 0.50, predict loss = 0.12 (76.8 examples/sec; 0.052 sec/batch; 85h:39m:18s remains)
INFO - root - 2019-11-04 00:43:03.872506: step 82140, total loss = 0.54, predict loss = 0.13 (71.3 examples/sec; 0.056 sec/batch; 92h:12m:07s remains)
INFO - root - 2019-11-04 00:43:04.515157: step 82150, total loss = 0.59, predict loss = 0.14 (80.6 examples/sec; 0.050 sec/batch; 81h:32m:37s remains)
INFO - root - 2019-11-04 00:43:05.183001: step 82160, total loss = 0.61, predict loss = 0.14 (74.8 examples/sec; 0.053 sec/batch; 87h:54m:14s remains)
INFO - root - 2019-11-04 00:43:05.832802: step 82170, total loss = 0.56, predict loss = 0.13 (64.1 examples/sec; 0.062 sec/batch; 102h:33m:38s remains)
INFO - root - 2019-11-04 00:43:06.515219: step 82180, total loss = 0.47, predict loss = 0.11 (68.5 examples/sec; 0.058 sec/batch; 96h:03m:27s remains)
INFO - root - 2019-11-04 00:43:07.133633: step 82190, total loss = 0.45, predict loss = 0.09 (71.6 examples/sec; 0.056 sec/batch; 91h:50m:10s remains)
INFO - root - 2019-11-04 00:43:07.774277: step 82200, total loss = 0.54, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 102h:14m:27s remains)
INFO - root - 2019-11-04 00:43:08.433400: step 82210, total loss = 0.46, predict loss = 0.09 (69.4 examples/sec; 0.058 sec/batch; 94h:40m:53s remains)
INFO - root - 2019-11-04 00:43:09.078085: step 82220, total loss = 0.44, predict loss = 0.10 (68.2 examples/sec; 0.059 sec/batch; 96h:27m:25s remains)
INFO - root - 2019-11-04 00:43:09.678531: step 82230, total loss = 0.63, predict loss = 0.15 (70.5 examples/sec; 0.057 sec/batch; 93h:13m:36s remains)
INFO - root - 2019-11-04 00:43:10.303072: step 82240, total loss = 0.39, predict loss = 0.09 (71.8 examples/sec; 0.056 sec/batch; 91h:35m:57s remains)
INFO - root - 2019-11-04 00:43:10.928090: step 82250, total loss = 0.59, predict loss = 0.14 (75.5 examples/sec; 0.053 sec/batch; 87h:07m:41s remains)
INFO - root - 2019-11-04 00:43:11.540452: step 82260, total loss = 0.61, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 94h:30m:05s remains)
INFO - root - 2019-11-04 00:43:12.177081: step 82270, total loss = 0.57, predict loss = 0.14 (63.8 examples/sec; 0.063 sec/batch; 102h:59m:54s remains)
INFO - root - 2019-11-04 00:43:12.853119: step 82280, total loss = 0.59, predict loss = 0.14 (65.7 examples/sec; 0.061 sec/batch; 100h:03m:24s remains)
INFO - root - 2019-11-04 00:43:13.487590: step 82290, total loss = 0.44, predict loss = 0.10 (79.9 examples/sec; 0.050 sec/batch; 82h:19m:00s remains)
INFO - root - 2019-11-04 00:43:14.112292: step 82300, total loss = 0.44, predict loss = 0.10 (64.5 examples/sec; 0.062 sec/batch; 101h:52m:59s remains)
INFO - root - 2019-11-04 00:43:14.768244: step 82310, total loss = 0.50, predict loss = 0.11 (71.9 examples/sec; 0.056 sec/batch; 91h:29m:25s remains)
INFO - root - 2019-11-04 00:43:15.404776: step 82320, total loss = 0.56, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 94h:25m:37s remains)
INFO - root - 2019-11-04 00:43:16.056834: step 82330, total loss = 0.48, predict loss = 0.12 (69.2 examples/sec; 0.058 sec/batch; 95h:03m:12s remains)
INFO - root - 2019-11-04 00:43:16.718057: step 82340, total loss = 0.70, predict loss = 0.16 (72.5 examples/sec; 0.055 sec/batch; 90h:43m:37s remains)
INFO - root - 2019-11-04 00:43:17.388780: step 82350, total loss = 0.57, predict loss = 0.14 (70.4 examples/sec; 0.057 sec/batch; 93h:20m:03s remains)
INFO - root - 2019-11-04 00:43:18.057002: step 82360, total loss = 0.53, predict loss = 0.13 (66.5 examples/sec; 0.060 sec/batch; 98h:56m:05s remains)
INFO - root - 2019-11-04 00:43:18.709768: step 82370, total loss = 0.50, predict loss = 0.13 (70.2 examples/sec; 0.057 sec/batch; 93h:41m:38s remains)
INFO - root - 2019-11-04 00:43:19.324629: step 82380, total loss = 0.59, predict loss = 0.14 (65.9 examples/sec; 0.061 sec/batch; 99h:45m:31s remains)
INFO - root - 2019-11-04 00:43:20.012065: step 82390, total loss = 0.57, predict loss = 0.14 (69.2 examples/sec; 0.058 sec/batch; 94h:59m:45s remains)
INFO - root - 2019-11-04 00:43:20.686962: step 82400, total loss = 0.46, predict loss = 0.11 (70.0 examples/sec; 0.057 sec/batch; 93h:59m:44s remains)
INFO - root - 2019-11-04 00:43:21.312294: step 82410, total loss = 0.34, predict loss = 0.07 (80.1 examples/sec; 0.050 sec/batch; 82h:03m:46s remains)
INFO - root - 2019-11-04 00:43:21.931549: step 82420, total loss = 0.25, predict loss = 0.05 (66.2 examples/sec; 0.060 sec/batch; 99h:17m:17s remains)
INFO - root - 2019-11-04 00:43:22.562424: step 82430, total loss = 0.45, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 92h:25m:08s remains)
INFO - root - 2019-11-04 00:43:23.180606: step 82440, total loss = 0.28, predict loss = 0.06 (66.2 examples/sec; 0.060 sec/batch; 99h:15m:24s remains)
INFO - root - 2019-11-04 00:43:23.827215: step 82450, total loss = 0.29, predict loss = 0.06 (68.2 examples/sec; 0.059 sec/batch; 96h:21m:49s remains)
INFO - root - 2019-11-04 00:43:24.510025: step 82460, total loss = 0.24, predict loss = 0.04 (73.3 examples/sec; 0.055 sec/batch; 89h:39m:07s remains)
INFO - root - 2019-11-04 00:43:25.193178: step 82470, total loss = 0.46, predict loss = 0.10 (62.1 examples/sec; 0.064 sec/batch; 105h:48m:57s remains)
INFO - root - 2019-11-04 00:43:25.914678: step 82480, total loss = 0.56, predict loss = 0.13 (59.1 examples/sec; 0.068 sec/batch; 111h:16m:27s remains)
INFO - root - 2019-11-04 00:43:26.574281: step 82490, total loss = 0.54, predict loss = 0.13 (73.8 examples/sec; 0.054 sec/batch; 89h:04m:46s remains)
INFO - root - 2019-11-04 00:43:27.212009: step 82500, total loss = 0.52, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 90h:34m:18s remains)
INFO - root - 2019-11-04 00:43:27.818203: step 82510, total loss = 0.47, predict loss = 0.11 (80.1 examples/sec; 0.050 sec/batch; 82h:04m:01s remains)
INFO - root - 2019-11-04 00:43:28.436255: step 82520, total loss = 0.57, predict loss = 0.13 (67.2 examples/sec; 0.060 sec/batch; 97h:50m:50s remains)
INFO - root - 2019-11-04 00:43:29.064428: step 82530, total loss = 0.51, predict loss = 0.12 (65.4 examples/sec; 0.061 sec/batch; 100h:30m:01s remains)
INFO - root - 2019-11-04 00:43:29.793351: step 82540, total loss = 0.47, predict loss = 0.11 (65.4 examples/sec; 0.061 sec/batch; 100h:29m:01s remains)
INFO - root - 2019-11-04 00:43:30.463121: step 82550, total loss = 0.57, predict loss = 0.14 (72.5 examples/sec; 0.055 sec/batch; 90h:37m:35s remains)
INFO - root - 2019-11-04 00:43:31.110001: step 82560, total loss = 0.51, predict loss = 0.12 (76.0 examples/sec; 0.053 sec/batch; 86h:30m:10s remains)
INFO - root - 2019-11-04 00:43:31.776525: step 82570, total loss = 0.44, predict loss = 0.10 (69.6 examples/sec; 0.057 sec/batch; 94h:28m:33s remains)
INFO - root - 2019-11-04 00:43:32.459833: step 82580, total loss = 0.52, predict loss = 0.13 (62.4 examples/sec; 0.064 sec/batch; 105h:25m:16s remains)
INFO - root - 2019-11-04 00:43:33.159998: step 82590, total loss = 0.69, predict loss = 0.17 (62.0 examples/sec; 0.064 sec/batch; 105h:58m:05s remains)
INFO - root - 2019-11-04 00:43:33.801298: step 82600, total loss = 0.63, predict loss = 0.14 (76.7 examples/sec; 0.052 sec/batch; 85h:40m:24s remains)
INFO - root - 2019-11-04 00:43:34.452411: step 82610, total loss = 0.52, predict loss = 0.12 (62.2 examples/sec; 0.064 sec/batch; 105h:45m:01s remains)
INFO - root - 2019-11-04 00:43:35.099566: step 82620, total loss = 0.55, predict loss = 0.12 (87.7 examples/sec; 0.046 sec/batch; 74h:59m:13s remains)
INFO - root - 2019-11-04 00:43:35.711855: step 82630, total loss = 0.46, predict loss = 0.11 (73.3 examples/sec; 0.055 sec/batch; 89h:42m:42s remains)
INFO - root - 2019-11-04 00:43:36.325058: step 82640, total loss = 0.39, predict loss = 0.09 (76.1 examples/sec; 0.053 sec/batch; 86h:24m:50s remains)
INFO - root - 2019-11-04 00:43:36.945477: step 82650, total loss = 0.40, predict loss = 0.09 (67.3 examples/sec; 0.059 sec/batch; 97h:43m:21s remains)
INFO - root - 2019-11-04 00:43:37.597918: step 82660, total loss = 0.24, predict loss = 0.04 (72.0 examples/sec; 0.056 sec/batch; 91h:21m:49s remains)
INFO - root - 2019-11-04 00:43:38.275542: step 82670, total loss = 0.29, predict loss = 0.06 (66.8 examples/sec; 0.060 sec/batch; 98h:24m:39s remains)
INFO - root - 2019-11-04 00:43:38.925726: step 82680, total loss = 0.37, predict loss = 0.08 (69.2 examples/sec; 0.058 sec/batch; 95h:00m:42s remains)
INFO - root - 2019-11-04 00:43:39.561640: step 82690, total loss = 0.38, predict loss = 0.08 (71.5 examples/sec; 0.056 sec/batch; 91h:54m:27s remains)
INFO - root - 2019-11-04 00:43:40.182025: step 82700, total loss = 0.36, predict loss = 0.08 (69.6 examples/sec; 0.057 sec/batch; 94h:29m:49s remains)
INFO - root - 2019-11-04 00:43:40.837202: step 82710, total loss = 0.37, predict loss = 0.08 (65.4 examples/sec; 0.061 sec/batch; 100h:28m:07s remains)
INFO - root - 2019-11-04 00:43:41.546326: step 82720, total loss = 0.39, predict loss = 0.09 (56.5 examples/sec; 0.071 sec/batch; 116h:27m:55s remains)
INFO - root - 2019-11-04 00:43:42.184470: step 82730, total loss = 0.36, predict loss = 0.07 (68.1 examples/sec; 0.059 sec/batch; 96h:29m:19s remains)
INFO - root - 2019-11-04 00:43:42.858824: step 82740, total loss = 0.45, predict loss = 0.11 (64.5 examples/sec; 0.062 sec/batch; 101h:54m:16s remains)
INFO - root - 2019-11-04 00:43:43.520792: step 82750, total loss = 0.55, predict loss = 0.13 (72.3 examples/sec; 0.055 sec/batch; 90h:56m:07s remains)
INFO - root - 2019-11-04 00:43:44.146666: step 82760, total loss = 0.36, predict loss = 0.08 (70.7 examples/sec; 0.057 sec/batch; 93h:03m:05s remains)
INFO - root - 2019-11-04 00:43:44.752688: step 82770, total loss = 0.52, predict loss = 0.13 (71.4 examples/sec; 0.056 sec/batch; 92h:05m:14s remains)
INFO - root - 2019-11-04 00:43:45.359780: step 82780, total loss = 0.49, predict loss = 0.12 (76.6 examples/sec; 0.052 sec/batch; 85h:47m:21s remains)
INFO - root - 2019-11-04 00:43:46.015946: step 82790, total loss = 0.54, predict loss = 0.12 (65.1 examples/sec; 0.061 sec/batch; 100h:57m:48s remains)
INFO - root - 2019-11-04 00:43:46.698704: step 82800, total loss = 0.39, predict loss = 0.08 (63.8 examples/sec; 0.063 sec/batch; 103h:06m:32s remains)
INFO - root - 2019-11-04 00:43:47.357207: step 82810, total loss = 0.36, predict loss = 0.08 (73.2 examples/sec; 0.055 sec/batch; 89h:51m:57s remains)
INFO - root - 2019-11-04 00:43:48.012822: step 82820, total loss = 0.45, predict loss = 0.11 (73.3 examples/sec; 0.055 sec/batch; 89h:38m:05s remains)
INFO - root - 2019-11-04 00:43:48.666264: step 82830, total loss = 0.57, predict loss = 0.13 (70.1 examples/sec; 0.057 sec/batch; 93h:49m:39s remains)
INFO - root - 2019-11-04 00:43:49.323514: step 82840, total loss = 0.44, predict loss = 0.10 (65.1 examples/sec; 0.061 sec/batch; 101h:03m:56s remains)
INFO - root - 2019-11-04 00:43:49.936587: step 82850, total loss = 0.54, predict loss = 0.13 (80.7 examples/sec; 0.050 sec/batch; 81h:27m:04s remains)
INFO - root - 2019-11-04 00:43:50.619802: step 82860, total loss = 0.65, predict loss = 0.16 (69.5 examples/sec; 0.058 sec/batch; 94h:33m:39s remains)
INFO - root - 2019-11-04 00:43:51.238403: step 82870, total loss = 0.31, predict loss = 0.07 (70.6 examples/sec; 0.057 sec/batch; 93h:09m:55s remains)
INFO - root - 2019-11-04 00:43:51.891871: step 82880, total loss = 0.28, predict loss = 0.06 (73.0 examples/sec; 0.055 sec/batch; 90h:00m:24s remains)
INFO - root - 2019-11-04 00:43:52.487949: step 82890, total loss = 0.25, predict loss = 0.05 (80.3 examples/sec; 0.050 sec/batch; 81h:50m:03s remains)
INFO - root - 2019-11-04 00:43:53.146299: step 82900, total loss = 0.25, predict loss = 0.05 (66.8 examples/sec; 0.060 sec/batch; 98h:20m:53s remains)
INFO - root - 2019-11-04 00:43:53.785736: step 82910, total loss = 0.27, predict loss = 0.05 (74.3 examples/sec; 0.054 sec/batch; 88h:32m:19s remains)
INFO - root - 2019-11-04 00:43:54.400985: step 82920, total loss = 0.57, predict loss = 0.13 (73.4 examples/sec; 0.054 sec/batch; 89h:32m:51s remains)
INFO - root - 2019-11-04 00:43:55.056741: step 82930, total loss = 0.37, predict loss = 0.08 (79.0 examples/sec; 0.051 sec/batch; 83h:14m:32s remains)
INFO - root - 2019-11-04 00:43:55.707732: step 82940, total loss = 0.28, predict loss = 0.06 (71.8 examples/sec; 0.056 sec/batch; 91h:35m:23s remains)
INFO - root - 2019-11-04 00:43:56.359590: step 82950, total loss = 0.44, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 94h:18m:23s remains)
INFO - root - 2019-11-04 00:43:57.004571: step 82960, total loss = 0.45, predict loss = 0.10 (69.0 examples/sec; 0.058 sec/batch; 95h:13m:03s remains)
INFO - root - 2019-11-04 00:43:57.630412: step 82970, total loss = 0.56, predict loss = 0.13 (64.1 examples/sec; 0.062 sec/batch; 102h:32m:36s remains)
INFO - root - 2019-11-04 00:43:58.300825: step 82980, total loss = 0.47, predict loss = 0.11 (72.7 examples/sec; 0.055 sec/batch; 90h:27m:46s remains)
INFO - root - 2019-11-04 00:43:58.983063: step 82990, total loss = 0.56, predict loss = 0.14 (70.5 examples/sec; 0.057 sec/batch; 93h:14m:46s remains)
INFO - root - 2019-11-04 00:43:59.636440: step 83000, total loss = 0.44, predict loss = 0.10 (77.7 examples/sec; 0.051 sec/batch; 84h:36m:46s remains)
INFO - root - 2019-11-04 00:44:00.273287: step 83010, total loss = 0.49, predict loss = 0.11 (69.2 examples/sec; 0.058 sec/batch; 95h:03m:48s remains)
INFO - root - 2019-11-04 00:44:00.935535: step 83020, total loss = 0.54, predict loss = 0.13 (59.0 examples/sec; 0.068 sec/batch; 111h:22m:10s remains)
INFO - root - 2019-11-04 00:44:01.571165: step 83030, total loss = 0.37, predict loss = 0.08 (66.1 examples/sec; 0.060 sec/batch; 99h:23m:43s remains)
INFO - root - 2019-11-04 00:44:02.180266: step 83040, total loss = 0.31, predict loss = 0.06 (70.4 examples/sec; 0.057 sec/batch; 93h:24m:10s remains)
INFO - root - 2019-11-04 00:44:02.815063: step 83050, total loss = 0.40, predict loss = 0.09 (77.4 examples/sec; 0.052 sec/batch; 84h:53m:42s remains)
INFO - root - 2019-11-04 00:44:03.437871: step 83060, total loss = 0.52, predict loss = 0.12 (80.4 examples/sec; 0.050 sec/batch; 81h:43m:14s remains)
INFO - root - 2019-11-04 00:44:04.097752: step 83070, total loss = 0.39, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 92h:46m:59s remains)
INFO - root - 2019-11-04 00:44:04.733036: step 83080, total loss = 0.48, predict loss = 0.12 (62.4 examples/sec; 0.064 sec/batch; 105h:19m:47s remains)
INFO - root - 2019-11-04 00:44:05.408169: step 83090, total loss = 0.65, predict loss = 0.15 (71.5 examples/sec; 0.056 sec/batch; 91h:54m:44s remains)
INFO - root - 2019-11-04 00:44:06.079962: step 83100, total loss = 0.58, predict loss = 0.14 (70.2 examples/sec; 0.057 sec/batch; 93h:35m:36s remains)
INFO - root - 2019-11-04 00:44:06.709019: step 83110, total loss = 0.49, predict loss = 0.11 (60.3 examples/sec; 0.066 sec/batch; 109h:01m:42s remains)
INFO - root - 2019-11-04 00:44:07.335730: step 83120, total loss = 0.51, predict loss = 0.11 (80.8 examples/sec; 0.050 sec/batch; 81h:21m:39s remains)
INFO - root - 2019-11-04 00:44:07.985171: step 83130, total loss = 0.52, predict loss = 0.12 (74.1 examples/sec; 0.054 sec/batch; 88h:40m:33s remains)
INFO - root - 2019-11-04 00:44:08.600444: step 83140, total loss = 0.57, predict loss = 0.14 (71.8 examples/sec; 0.056 sec/batch; 91h:30m:52s remains)
INFO - root - 2019-11-04 00:44:09.218210: step 83150, total loss = 0.46, predict loss = 0.10 (85.5 examples/sec; 0.047 sec/batch; 76h:52m:20s remains)
INFO - root - 2019-11-04 00:44:09.913285: step 83160, total loss = 0.48, predict loss = 0.11 (75.4 examples/sec; 0.053 sec/batch; 87h:08m:20s remains)
INFO - root - 2019-11-04 00:44:10.637680: step 83170, total loss = 0.33, predict loss = 0.07 (66.0 examples/sec; 0.061 sec/batch; 99h:36m:48s remains)
INFO - root - 2019-11-04 00:44:11.342338: step 83180, total loss = 0.39, predict loss = 0.09 (64.7 examples/sec; 0.062 sec/batch; 101h:37m:46s remains)
INFO - root - 2019-11-04 00:44:11.986895: step 83190, total loss = 0.44, predict loss = 0.10 (66.4 examples/sec; 0.060 sec/batch; 99h:04m:32s remains)
INFO - root - 2019-11-04 00:44:12.624491: step 83200, total loss = 0.25, predict loss = 0.05 (77.1 examples/sec; 0.052 sec/batch; 85h:16m:09s remains)
INFO - root - 2019-11-04 00:44:13.244500: step 83210, total loss = 0.43, predict loss = 0.09 (72.1 examples/sec; 0.055 sec/batch; 91h:12m:21s remains)
INFO - root - 2019-11-04 00:44:13.908058: step 83220, total loss = 0.56, predict loss = 0.13 (69.5 examples/sec; 0.058 sec/batch; 94h:33m:57s remains)
INFO - root - 2019-11-04 00:44:14.613632: step 83230, total loss = 0.61, predict loss = 0.14 (59.9 examples/sec; 0.067 sec/batch; 109h:44m:31s remains)
INFO - root - 2019-11-04 00:44:15.376210: step 83240, total loss = 0.53, predict loss = 0.12 (57.8 examples/sec; 0.069 sec/batch; 113h:45m:16s remains)
INFO - root - 2019-11-04 00:44:16.063656: step 83250, total loss = 0.80, predict loss = 0.20 (60.4 examples/sec; 0.066 sec/batch; 108h:54m:39s remains)
INFO - root - 2019-11-04 00:44:16.661117: step 83260, total loss = 0.79, predict loss = 0.19 (83.3 examples/sec; 0.048 sec/batch; 78h:54m:43s remains)
INFO - root - 2019-11-04 00:44:17.283484: step 83270, total loss = 0.68, predict loss = 0.16 (66.9 examples/sec; 0.060 sec/batch; 98h:19m:44s remains)
INFO - root - 2019-11-04 00:44:17.889104: step 83280, total loss = 0.66, predict loss = 0.15 (78.0 examples/sec; 0.051 sec/batch; 84h:13m:48s remains)
INFO - root - 2019-11-04 00:44:18.531114: step 83290, total loss = 0.63, predict loss = 0.15 (70.8 examples/sec; 0.057 sec/batch; 92h:53m:46s remains)
INFO - root - 2019-11-04 00:44:19.163692: step 83300, total loss = 0.77, predict loss = 0.18 (83.2 examples/sec; 0.048 sec/batch; 78h:58m:46s remains)
INFO - root - 2019-11-04 00:44:19.771043: step 83310, total loss = 0.78, predict loss = 0.19 (70.7 examples/sec; 0.057 sec/batch; 93h:02m:39s remains)
INFO - root - 2019-11-04 00:44:20.389622: step 83320, total loss = 0.81, predict loss = 0.20 (75.6 examples/sec; 0.053 sec/batch; 86h:55m:02s remains)
INFO - root - 2019-11-04 00:44:21.032652: step 83330, total loss = 0.46, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 97h:30m:55s remains)
INFO - root - 2019-11-04 00:44:21.690470: step 83340, total loss = 0.53, predict loss = 0.12 (76.4 examples/sec; 0.052 sec/batch; 86h:01m:23s remains)
INFO - root - 2019-11-04 00:44:22.352040: step 83350, total loss = 0.52, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 99h:43m:22s remains)
INFO - root - 2019-11-04 00:44:22.982260: step 83360, total loss = 0.60, predict loss = 0.13 (64.2 examples/sec; 0.062 sec/batch; 102h:25m:37s remains)
INFO - root - 2019-11-04 00:44:23.671729: step 83370, total loss = 0.55, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 91h:33m:11s remains)
INFO - root - 2019-11-04 00:44:24.291412: step 83380, total loss = 0.46, predict loss = 0.10 (68.0 examples/sec; 0.059 sec/batch; 96h:38m:42s remains)
INFO - root - 2019-11-04 00:44:24.912889: step 83390, total loss = 0.54, predict loss = 0.13 (71.7 examples/sec; 0.056 sec/batch; 91h:38m:07s remains)
INFO - root - 2019-11-04 00:44:25.538804: step 83400, total loss = 0.46, predict loss = 0.10 (66.9 examples/sec; 0.060 sec/batch; 98h:18m:39s remains)
INFO - root - 2019-11-04 00:44:26.159882: step 83410, total loss = 0.52, predict loss = 0.12 (69.2 examples/sec; 0.058 sec/batch; 95h:01m:08s remains)
INFO - root - 2019-11-04 00:44:26.784397: step 83420, total loss = 0.48, predict loss = 0.11 (76.0 examples/sec; 0.053 sec/batch; 86h:32m:13s remains)
INFO - root - 2019-11-04 00:44:27.441551: step 83430, total loss = 0.45, predict loss = 0.11 (61.1 examples/sec; 0.066 sec/batch; 107h:39m:49s remains)
INFO - root - 2019-11-04 00:44:28.106989: step 83440, total loss = 0.41, predict loss = 0.10 (65.3 examples/sec; 0.061 sec/batch; 100h:44m:08s remains)
INFO - root - 2019-11-04 00:44:28.751671: step 83450, total loss = 0.54, predict loss = 0.13 (72.9 examples/sec; 0.055 sec/batch; 90h:11m:52s remains)
INFO - root - 2019-11-04 00:44:29.410662: step 83460, total loss = 0.46, predict loss = 0.10 (62.0 examples/sec; 0.065 sec/batch; 106h:03m:58s remains)
INFO - root - 2019-11-04 00:44:30.035711: step 83470, total loss = 0.39, predict loss = 0.09 (75.5 examples/sec; 0.053 sec/batch; 87h:01m:14s remains)
INFO - root - 2019-11-04 00:44:30.642063: step 83480, total loss = 0.46, predict loss = 0.11 (74.2 examples/sec; 0.054 sec/batch; 88h:34m:06s remains)
INFO - root - 2019-11-04 00:44:31.259458: step 83490, total loss = 0.38, predict loss = 0.08 (71.7 examples/sec; 0.056 sec/batch; 91h:40m:56s remains)
INFO - root - 2019-11-04 00:44:31.914272: step 83500, total loss = 0.39, predict loss = 0.09 (71.2 examples/sec; 0.056 sec/batch; 92h:23m:09s remains)
INFO - root - 2019-11-04 00:44:32.537001: step 83510, total loss = 0.51, predict loss = 0.12 (78.2 examples/sec; 0.051 sec/batch; 84h:01m:46s remains)
INFO - root - 2019-11-04 00:44:33.161942: step 83520, total loss = 0.41, predict loss = 0.09 (70.8 examples/sec; 0.057 sec/batch; 92h:53m:23s remains)
INFO - root - 2019-11-04 00:44:33.777933: step 83530, total loss = 0.34, predict loss = 0.08 (80.9 examples/sec; 0.049 sec/batch; 81h:17m:57s remains)
INFO - root - 2019-11-04 00:44:34.411213: step 83540, total loss = 0.44, predict loss = 0.10 (76.3 examples/sec; 0.052 sec/batch; 86h:07m:04s remains)
INFO - root - 2019-11-04 00:44:35.105881: step 83550, total loss = 0.43, predict loss = 0.10 (74.3 examples/sec; 0.054 sec/batch; 88h:28m:11s remains)
INFO - root - 2019-11-04 00:44:35.754106: step 83560, total loss = 0.43, predict loss = 0.10 (71.6 examples/sec; 0.056 sec/batch; 91h:45m:47s remains)
INFO - root - 2019-11-04 00:44:36.414792: step 83570, total loss = 0.40, predict loss = 0.09 (61.0 examples/sec; 0.066 sec/batch; 107h:42m:13s remains)
INFO - root - 2019-11-04 00:44:37.059141: step 83580, total loss = 0.48, predict loss = 0.11 (68.0 examples/sec; 0.059 sec/batch; 96h:41m:20s remains)
INFO - root - 2019-11-04 00:44:37.725630: step 83590, total loss = 0.51, predict loss = 0.12 (67.6 examples/sec; 0.059 sec/batch; 97h:13m:03s remains)
INFO - root - 2019-11-04 00:44:38.355325: step 83600, total loss = 0.48, predict loss = 0.11 (74.0 examples/sec; 0.054 sec/batch; 88h:49m:39s remains)
INFO - root - 2019-11-04 00:44:39.010511: step 83610, total loss = 0.49, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 95h:44m:23s remains)
INFO - root - 2019-11-04 00:44:39.655988: step 83620, total loss = 0.44, predict loss = 0.11 (74.5 examples/sec; 0.054 sec/batch; 88h:12m:25s remains)
INFO - root - 2019-11-04 00:44:40.304050: step 83630, total loss = 0.44, predict loss = 0.10 (67.7 examples/sec; 0.059 sec/batch; 97h:04m:44s remains)
INFO - root - 2019-11-04 00:44:40.965038: step 83640, total loss = 0.58, predict loss = 0.13 (76.3 examples/sec; 0.052 sec/batch; 86h:10m:39s remains)
INFO - root - 2019-11-04 00:44:41.645840: step 83650, total loss = 0.67, predict loss = 0.16 (67.1 examples/sec; 0.060 sec/batch; 97h:58m:15s remains)
INFO - root - 2019-11-04 00:44:42.283751: step 83660, total loss = 0.57, predict loss = 0.14 (73.9 examples/sec; 0.054 sec/batch; 88h:58m:55s remains)
INFO - root - 2019-11-04 00:44:42.923183: step 83670, total loss = 0.53, predict loss = 0.13 (74.4 examples/sec; 0.054 sec/batch; 88h:19m:10s remains)
INFO - root - 2019-11-04 00:44:43.545993: step 83680, total loss = 0.59, predict loss = 0.13 (60.5 examples/sec; 0.066 sec/batch; 108h:37m:24s remains)
INFO - root - 2019-11-04 00:44:44.229865: step 83690, total loss = 0.64, predict loss = 0.15 (70.2 examples/sec; 0.057 sec/batch; 93h:41m:41s remains)
INFO - root - 2019-11-04 00:44:44.917357: step 83700, total loss = 0.70, predict loss = 0.17 (70.3 examples/sec; 0.057 sec/batch; 93h:27m:20s remains)
INFO - root - 2019-11-04 00:44:45.533441: step 83710, total loss = 0.69, predict loss = 0.17 (61.7 examples/sec; 0.065 sec/batch; 106h:30m:50s remains)
INFO - root - 2019-11-04 00:44:46.179814: step 83720, total loss = 0.54, predict loss = 0.13 (73.0 examples/sec; 0.055 sec/batch; 90h:06m:08s remains)
INFO - root - 2019-11-04 00:44:46.819346: step 83730, total loss = 0.79, predict loss = 0.19 (65.1 examples/sec; 0.061 sec/batch; 100h:59m:44s remains)
INFO - root - 2019-11-04 00:44:47.480791: step 83740, total loss = 0.64, predict loss = 0.15 (71.8 examples/sec; 0.056 sec/batch; 91h:35m:25s remains)
INFO - root - 2019-11-04 00:44:48.109954: step 83750, total loss = 0.47, predict loss = 0.11 (68.6 examples/sec; 0.058 sec/batch; 95h:51m:15s remains)
INFO - root - 2019-11-04 00:44:48.776711: step 83760, total loss = 0.63, predict loss = 0.15 (60.1 examples/sec; 0.067 sec/batch; 109h:27m:37s remains)
INFO - root - 2019-11-04 00:44:49.421804: step 83770, total loss = 0.42, predict loss = 0.09 (69.3 examples/sec; 0.058 sec/batch; 94h:54m:15s remains)
INFO - root - 2019-11-04 00:44:50.065969: step 83780, total loss = 0.42, predict loss = 0.10 (61.9 examples/sec; 0.065 sec/batch; 106h:13m:58s remains)
INFO - root - 2019-11-04 00:44:50.717923: step 83790, total loss = 0.57, predict loss = 0.13 (66.9 examples/sec; 0.060 sec/batch; 98h:18m:59s remains)
INFO - root - 2019-11-04 00:44:51.347304: step 83800, total loss = 0.43, predict loss = 0.10 (68.7 examples/sec; 0.058 sec/batch; 95h:38m:01s remains)
INFO - root - 2019-11-04 00:44:51.981690: step 83810, total loss = 0.40, predict loss = 0.09 (62.3 examples/sec; 0.064 sec/batch; 105h:29m:18s remains)
INFO - root - 2019-11-04 00:44:52.634684: step 83820, total loss = 0.50, predict loss = 0.12 (71.9 examples/sec; 0.056 sec/batch; 91h:23m:38s remains)
INFO - root - 2019-11-04 00:44:53.262141: step 83830, total loss = 0.44, predict loss = 0.10 (73.3 examples/sec; 0.055 sec/batch; 89h:42m:43s remains)
INFO - root - 2019-11-04 00:44:53.932936: step 83840, total loss = 0.47, predict loss = 0.11 (67.5 examples/sec; 0.059 sec/batch; 97h:19m:43s remains)
INFO - root - 2019-11-04 00:44:54.568292: step 83850, total loss = 0.53, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 103h:30m:31s remains)
INFO - root - 2019-11-04 00:44:55.240335: step 83860, total loss = 0.51, predict loss = 0.11 (62.7 examples/sec; 0.064 sec/batch; 104h:51m:42s remains)
INFO - root - 2019-11-04 00:44:55.927462: step 83870, total loss = 0.57, predict loss = 0.13 (61.5 examples/sec; 0.065 sec/batch; 106h:47m:59s remains)
INFO - root - 2019-11-04 00:44:56.555632: step 83880, total loss = 0.59, predict loss = 0.14 (77.7 examples/sec; 0.052 sec/batch; 84h:38m:59s remains)
INFO - root - 2019-11-04 00:44:57.196208: step 83890, total loss = 0.59, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 93h:31m:10s remains)
INFO - root - 2019-11-04 00:44:57.864044: step 83900, total loss = 0.64, predict loss = 0.15 (66.2 examples/sec; 0.060 sec/batch; 99h:21m:46s remains)
INFO - root - 2019-11-04 00:44:58.527420: step 83910, total loss = 0.58, predict loss = 0.14 (71.4 examples/sec; 0.056 sec/batch; 92h:02m:00s remains)
INFO - root - 2019-11-04 00:44:59.222098: step 83920, total loss = 0.56, predict loss = 0.13 (58.7 examples/sec; 0.068 sec/batch; 112h:02m:37s remains)
INFO - root - 2019-11-04 00:44:59.853615: step 83930, total loss = 0.49, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 95h:44m:39s remains)
INFO - root - 2019-11-04 00:45:00.466669: step 83940, total loss = 0.55, predict loss = 0.13 (70.9 examples/sec; 0.056 sec/batch; 92h:41m:13s remains)
INFO - root - 2019-11-04 00:45:01.087005: step 83950, total loss = 0.54, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 100h:36m:46s remains)
INFO - root - 2019-11-04 00:45:01.712160: step 83960, total loss = 0.62, predict loss = 0.16 (68.4 examples/sec; 0.058 sec/batch; 96h:07m:33s remains)
INFO - root - 2019-11-04 00:45:02.349288: step 83970, total loss = 0.54, predict loss = 0.13 (74.1 examples/sec; 0.054 sec/batch; 88h:44m:06s remains)
INFO - root - 2019-11-04 00:45:02.995252: step 83980, total loss = 0.59, predict loss = 0.14 (60.8 examples/sec; 0.066 sec/batch; 108h:05m:13s remains)
INFO - root - 2019-11-04 00:45:03.697523: step 83990, total loss = 0.48, predict loss = 0.11 (64.1 examples/sec; 0.062 sec/batch; 102h:29m:29s remains)
INFO - root - 2019-11-04 00:45:04.307009: step 84000, total loss = 0.51, predict loss = 0.12 (78.9 examples/sec; 0.051 sec/batch; 83h:16m:25s remains)
INFO - root - 2019-11-04 00:45:04.930047: step 84010, total loss = 0.59, predict loss = 0.13 (70.2 examples/sec; 0.057 sec/batch; 93h:35m:13s remains)
INFO - root - 2019-11-04 00:45:05.548097: step 84020, total loss = 0.51, predict loss = 0.11 (70.2 examples/sec; 0.057 sec/batch; 93h:36m:28s remains)
INFO - root - 2019-11-04 00:45:06.155201: step 84030, total loss = 0.49, predict loss = 0.12 (68.0 examples/sec; 0.059 sec/batch; 96h:40m:32s remains)
INFO - root - 2019-11-04 00:45:06.758525: step 84040, total loss = 0.57, predict loss = 0.13 (69.9 examples/sec; 0.057 sec/batch; 94h:03m:17s remains)
INFO - root - 2019-11-04 00:45:07.371254: step 84050, total loss = 0.38, predict loss = 0.09 (76.0 examples/sec; 0.053 sec/batch; 86h:27m:06s remains)
INFO - root - 2019-11-04 00:45:08.036754: step 84060, total loss = 0.44, predict loss = 0.10 (61.6 examples/sec; 0.065 sec/batch; 106h:43m:15s remains)
INFO - root - 2019-11-04 00:45:08.660500: step 84070, total loss = 0.40, predict loss = 0.09 (74.8 examples/sec; 0.054 sec/batch; 87h:55m:58s remains)
INFO - root - 2019-11-04 00:45:09.273375: step 84080, total loss = 0.55, predict loss = 0.13 (75.0 examples/sec; 0.053 sec/batch; 87h:41m:50s remains)
INFO - root - 2019-11-04 00:45:09.921170: step 84090, total loss = 0.43, predict loss = 0.10 (67.4 examples/sec; 0.059 sec/batch; 97h:32m:06s remains)
INFO - root - 2019-11-04 00:45:10.543409: step 84100, total loss = 0.41, predict loss = 0.09 (68.8 examples/sec; 0.058 sec/batch; 95h:31m:15s remains)
INFO - root - 2019-11-04 00:45:11.181763: step 84110, total loss = 0.35, predict loss = 0.08 (65.8 examples/sec; 0.061 sec/batch; 99h:54m:01s remains)
INFO - root - 2019-11-04 00:45:11.804977: step 84120, total loss = 0.46, predict loss = 0.11 (69.6 examples/sec; 0.057 sec/batch; 94h:23m:39s remains)
INFO - root - 2019-11-04 00:45:12.400151: step 84130, total loss = 0.50, predict loss = 0.11 (72.7 examples/sec; 0.055 sec/batch; 90h:22m:03s remains)
INFO - root - 2019-11-04 00:45:12.996318: step 84140, total loss = 0.53, predict loss = 0.13 (66.5 examples/sec; 0.060 sec/batch; 98h:50m:27s remains)
INFO - root - 2019-11-04 00:45:13.645698: step 84150, total loss = 0.44, predict loss = 0.10 (63.7 examples/sec; 0.063 sec/batch; 103h:14m:51s remains)
INFO - root - 2019-11-04 00:45:14.312447: step 84160, total loss = 0.50, predict loss = 0.12 (60.5 examples/sec; 0.066 sec/batch; 108h:42m:37s remains)
INFO - root - 2019-11-04 00:45:14.916375: step 84170, total loss = 0.64, predict loss = 0.15 (70.4 examples/sec; 0.057 sec/batch; 93h:19m:38s remains)
INFO - root - 2019-11-04 00:45:15.528941: step 84180, total loss = 0.56, predict loss = 0.14 (62.8 examples/sec; 0.064 sec/batch; 104h:38m:31s remains)
INFO - root - 2019-11-04 00:45:16.155725: step 84190, total loss = 0.50, predict loss = 0.12 (69.8 examples/sec; 0.057 sec/batch; 94h:07m:36s remains)
INFO - root - 2019-11-04 00:45:16.788170: step 84200, total loss = 0.46, predict loss = 0.11 (76.9 examples/sec; 0.052 sec/batch; 85h:30m:17s remains)
INFO - root - 2019-11-04 00:45:17.443658: step 84210, total loss = 0.51, predict loss = 0.13 (63.8 examples/sec; 0.063 sec/batch; 103h:01m:53s remains)
INFO - root - 2019-11-04 00:45:18.101399: step 84220, total loss = 0.55, predict loss = 0.12 (75.7 examples/sec; 0.053 sec/batch; 86h:47m:52s remains)
INFO - root - 2019-11-04 00:45:18.749513: step 84230, total loss = 0.52, predict loss = 0.12 (69.5 examples/sec; 0.058 sec/batch; 94h:37m:07s remains)
INFO - root - 2019-11-04 00:45:19.433179: step 84240, total loss = 0.42, predict loss = 0.10 (65.7 examples/sec; 0.061 sec/batch; 99h:59m:41s remains)
INFO - root - 2019-11-04 00:45:20.048477: step 84250, total loss = 0.56, predict loss = 0.13 (71.1 examples/sec; 0.056 sec/batch; 92h:29m:40s remains)
INFO - root - 2019-11-04 00:45:20.658733: step 84260, total loss = 0.35, predict loss = 0.08 (69.0 examples/sec; 0.058 sec/batch; 95h:14m:47s remains)
INFO - root - 2019-11-04 00:45:21.306879: step 84270, total loss = 0.46, predict loss = 0.11 (66.2 examples/sec; 0.060 sec/batch; 99h:13m:15s remains)
INFO - root - 2019-11-04 00:45:21.994752: step 84280, total loss = 0.61, predict loss = 0.14 (60.1 examples/sec; 0.067 sec/batch; 109h:24m:33s remains)
INFO - root - 2019-11-04 00:45:22.612998: step 84290, total loss = 0.60, predict loss = 0.15 (72.1 examples/sec; 0.055 sec/batch; 91h:10m:54s remains)
INFO - root - 2019-11-04 00:45:23.222596: step 84300, total loss = 0.70, predict loss = 0.16 (71.0 examples/sec; 0.056 sec/batch; 92h:35m:42s remains)
INFO - root - 2019-11-04 00:45:23.820391: step 84310, total loss = 0.68, predict loss = 0.16 (83.3 examples/sec; 0.048 sec/batch; 78h:53m:18s remains)
INFO - root - 2019-11-04 00:45:24.429259: step 84320, total loss = 0.63, predict loss = 0.15 (66.2 examples/sec; 0.060 sec/batch; 99h:16m:19s remains)
INFO - root - 2019-11-04 00:45:25.092290: step 84330, total loss = 0.71, predict loss = 0.17 (67.0 examples/sec; 0.060 sec/batch; 98h:05m:51s remains)
INFO - root - 2019-11-04 00:45:25.703100: step 84340, total loss = 0.57, predict loss = 0.14 (65.9 examples/sec; 0.061 sec/batch; 99h:40m:54s remains)
INFO - root - 2019-11-04 00:45:26.348366: step 84350, total loss = 0.69, predict loss = 0.17 (72.4 examples/sec; 0.055 sec/batch; 90h:45m:13s remains)
INFO - root - 2019-11-04 00:45:26.985641: step 84360, total loss = 0.60, predict loss = 0.14 (71.0 examples/sec; 0.056 sec/batch; 92h:31m:28s remains)
INFO - root - 2019-11-04 00:45:27.610252: step 84370, total loss = 0.53, predict loss = 0.13 (65.7 examples/sec; 0.061 sec/batch; 100h:04m:10s remains)
INFO - root - 2019-11-04 00:45:28.221971: step 84380, total loss = 0.55, predict loss = 0.13 (71.3 examples/sec; 0.056 sec/batch; 92h:15m:03s remains)
INFO - root - 2019-11-04 00:45:28.890176: step 84390, total loss = 0.52, predict loss = 0.12 (61.6 examples/sec; 0.065 sec/batch; 106h:40m:07s remains)
INFO - root - 2019-11-04 00:45:29.553989: step 84400, total loss = 0.52, predict loss = 0.12 (73.6 examples/sec; 0.054 sec/batch; 89h:17m:48s remains)
INFO - root - 2019-11-04 00:45:30.218847: step 84410, total loss = 0.61, predict loss = 0.14 (63.0 examples/sec; 0.064 sec/batch; 104h:22m:09s remains)
INFO - root - 2019-11-04 00:45:30.886121: step 84420, total loss = 0.48, predict loss = 0.11 (63.0 examples/sec; 0.063 sec/batch; 104h:18m:27s remains)
INFO - root - 2019-11-04 00:45:31.552576: step 84430, total loss = 0.46, predict loss = 0.10 (71.1 examples/sec; 0.056 sec/batch; 92h:27m:51s remains)
INFO - root - 2019-11-04 00:45:32.213253: step 84440, total loss = 0.40, predict loss = 0.09 (73.7 examples/sec; 0.054 sec/batch; 89h:10m:50s remains)
INFO - root - 2019-11-04 00:45:32.872284: step 84450, total loss = 0.48, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 99h:38m:40s remains)
INFO - root - 2019-11-04 00:45:33.494970: step 84460, total loss = 0.52, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 91h:38m:10s remains)
INFO - root - 2019-11-04 00:45:34.119176: step 84470, total loss = 0.59, predict loss = 0.15 (74.1 examples/sec; 0.054 sec/batch; 88h:41m:08s remains)
INFO - root - 2019-11-04 00:45:34.757128: step 84480, total loss = 0.71, predict loss = 0.16 (64.5 examples/sec; 0.062 sec/batch; 101h:50m:14s remains)
INFO - root - 2019-11-04 00:45:35.403022: step 84490, total loss = 0.61, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 91h:03m:07s remains)
INFO - root - 2019-11-04 00:45:36.554760: step 84500, total loss = 0.42, predict loss = 0.11 (66.2 examples/sec; 0.060 sec/batch; 99h:18m:30s remains)
INFO - root - 2019-11-04 00:45:37.230821: step 84510, total loss = 0.41, predict loss = 0.09 (65.0 examples/sec; 0.062 sec/batch; 101h:04m:37s remains)
INFO - root - 2019-11-04 00:45:37.935777: step 84520, total loss = 0.55, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 95h:20m:08s remains)
INFO - root - 2019-11-04 00:45:38.528573: step 84530, total loss = 0.57, predict loss = 0.13 (86.8 examples/sec; 0.046 sec/batch; 75h:44m:51s remains)
INFO - root - 2019-11-04 00:45:38.999842: step 84540, total loss = 0.40, predict loss = 0.09 (97.0 examples/sec; 0.041 sec/batch; 67h:43m:35s remains)
INFO - root - 2019-11-04 00:45:40.029031: step 84550, total loss = 0.31, predict loss = 0.07 (67.9 examples/sec; 0.059 sec/batch; 96h:47m:23s remains)
INFO - root - 2019-11-04 00:45:40.622791: step 84560, total loss = 0.37, predict loss = 0.07 (86.6 examples/sec; 0.046 sec/batch; 75h:52m:14s remains)
INFO - root - 2019-11-04 00:45:41.252604: step 84570, total loss = 0.76, predict loss = 0.18 (70.5 examples/sec; 0.057 sec/batch; 93h:14m:46s remains)
INFO - root - 2019-11-04 00:45:41.891752: step 84580, total loss = 0.58, predict loss = 0.14 (77.6 examples/sec; 0.052 sec/batch; 84h:40m:46s remains)
INFO - root - 2019-11-04 00:45:42.491789: step 84590, total loss = 0.43, predict loss = 0.10 (75.4 examples/sec; 0.053 sec/batch; 87h:08m:11s remains)
INFO - root - 2019-11-04 00:45:43.107634: step 84600, total loss = 0.58, predict loss = 0.14 (78.8 examples/sec; 0.051 sec/batch; 83h:25m:27s remains)
INFO - root - 2019-11-04 00:45:43.740651: step 84610, total loss = 0.37, predict loss = 0.08 (65.1 examples/sec; 0.061 sec/batch; 100h:57m:48s remains)
INFO - root - 2019-11-04 00:45:44.383909: step 84620, total loss = 0.65, predict loss = 0.15 (69.2 examples/sec; 0.058 sec/batch; 94h:55m:10s remains)
INFO - root - 2019-11-04 00:45:44.997074: step 84630, total loss = 0.54, predict loss = 0.12 (75.5 examples/sec; 0.053 sec/batch; 87h:03m:10s remains)
INFO - root - 2019-11-04 00:45:45.609952: step 84640, total loss = 0.58, predict loss = 0.13 (71.7 examples/sec; 0.056 sec/batch; 91h:38m:58s remains)
INFO - root - 2019-11-04 00:45:46.277767: step 84650, total loss = 0.52, predict loss = 0.12 (78.1 examples/sec; 0.051 sec/batch; 84h:11m:47s remains)
INFO - root - 2019-11-04 00:45:46.946746: step 84660, total loss = 0.45, predict loss = 0.09 (71.5 examples/sec; 0.056 sec/batch; 91h:55m:09s remains)
INFO - root - 2019-11-04 00:45:47.559210: step 84670, total loss = 0.58, predict loss = 0.14 (78.0 examples/sec; 0.051 sec/batch; 84h:15m:00s remains)
INFO - root - 2019-11-04 00:45:48.225559: step 84680, total loss = 0.42, predict loss = 0.09 (65.7 examples/sec; 0.061 sec/batch; 100h:00m:03s remains)
INFO - root - 2019-11-04 00:45:48.861560: step 84690, total loss = 0.64, predict loss = 0.14 (70.9 examples/sec; 0.056 sec/batch; 92h:41m:40s remains)
INFO - root - 2019-11-04 00:45:49.508028: step 84700, total loss = 0.55, predict loss = 0.12 (76.6 examples/sec; 0.052 sec/batch; 85h:48m:19s remains)
INFO - root - 2019-11-04 00:45:50.170305: step 84710, total loss = 0.41, predict loss = 0.08 (72.9 examples/sec; 0.055 sec/batch; 90h:08m:39s remains)
INFO - root - 2019-11-04 00:45:50.889122: step 84720, total loss = 0.58, predict loss = 0.13 (59.8 examples/sec; 0.067 sec/batch; 109h:59m:51s remains)
INFO - root - 2019-11-04 00:45:51.529906: step 84730, total loss = 0.47, predict loss = 0.10 (78.1 examples/sec; 0.051 sec/batch; 84h:11m:52s remains)
INFO - root - 2019-11-04 00:45:52.136862: step 84740, total loss = 0.46, predict loss = 0.10 (65.7 examples/sec; 0.061 sec/batch; 100h:05m:32s remains)
INFO - root - 2019-11-04 00:45:52.769922: step 84750, total loss = 0.45, predict loss = 0.10 (71.4 examples/sec; 0.056 sec/batch; 92h:03m:19s remains)
INFO - root - 2019-11-04 00:45:53.388347: step 84760, total loss = 0.38, predict loss = 0.09 (71.2 examples/sec; 0.056 sec/batch; 92h:17m:07s remains)
INFO - root - 2019-11-04 00:45:54.003717: step 84770, total loss = 0.52, predict loss = 0.12 (72.8 examples/sec; 0.055 sec/batch; 90h:19m:09s remains)
INFO - root - 2019-11-04 00:45:54.618209: step 84780, total loss = 0.67, predict loss = 0.16 (66.5 examples/sec; 0.060 sec/batch; 98h:46m:48s remains)
INFO - root - 2019-11-04 00:45:55.257893: step 84790, total loss = 0.45, predict loss = 0.10 (63.9 examples/sec; 0.063 sec/batch; 102h:47m:21s remains)
INFO - root - 2019-11-04 00:45:55.908681: step 84800, total loss = 0.60, predict loss = 0.14 (64.8 examples/sec; 0.062 sec/batch; 101h:22m:34s remains)
INFO - root - 2019-11-04 00:45:56.569135: step 84810, total loss = 0.54, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 90h:34m:47s remains)
INFO - root - 2019-11-04 00:45:57.250868: step 84820, total loss = 0.69, predict loss = 0.16 (63.3 examples/sec; 0.063 sec/batch; 103h:52m:53s remains)
INFO - root - 2019-11-04 00:45:57.918760: step 84830, total loss = 0.52, predict loss = 0.11 (64.2 examples/sec; 0.062 sec/batch; 102h:21m:46s remains)
INFO - root - 2019-11-04 00:45:58.585327: step 84840, total loss = 0.59, predict loss = 0.13 (72.3 examples/sec; 0.055 sec/batch; 90h:52m:13s remains)
INFO - root - 2019-11-04 00:45:59.217213: step 84850, total loss = 0.59, predict loss = 0.13 (78.6 examples/sec; 0.051 sec/batch; 83h:36m:44s remains)
INFO - root - 2019-11-04 00:45:59.841726: step 84860, total loss = 0.59, predict loss = 0.13 (66.4 examples/sec; 0.060 sec/batch; 98h:57m:34s remains)
INFO - root - 2019-11-04 00:46:00.509519: step 84870, total loss = 0.65, predict loss = 0.15 (66.9 examples/sec; 0.060 sec/batch; 98h:16m:15s remains)
INFO - root - 2019-11-04 00:46:01.183863: step 84880, total loss = 0.61, predict loss = 0.14 (64.6 examples/sec; 0.062 sec/batch; 101h:42m:38s remains)
INFO - root - 2019-11-04 00:46:01.839227: step 84890, total loss = 0.48, predict loss = 0.11 (64.8 examples/sec; 0.062 sec/batch; 101h:23m:50s remains)
INFO - root - 2019-11-04 00:46:02.517260: step 84900, total loss = 0.54, predict loss = 0.12 (65.1 examples/sec; 0.061 sec/batch; 100h:55m:27s remains)
INFO - root - 2019-11-04 00:46:03.189779: step 84910, total loss = 0.58, predict loss = 0.14 (62.4 examples/sec; 0.064 sec/batch; 105h:14m:42s remains)
INFO - root - 2019-11-04 00:46:03.797753: step 84920, total loss = 0.44, predict loss = 0.10 (73.1 examples/sec; 0.055 sec/batch; 89h:56m:25s remains)
INFO - root - 2019-11-04 00:46:04.452677: step 84930, total loss = 0.52, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 93h:32m:43s remains)
INFO - root - 2019-11-04 00:46:05.166681: step 84940, total loss = 0.42, predict loss = 0.10 (77.2 examples/sec; 0.052 sec/batch; 85h:09m:46s remains)
INFO - root - 2019-11-04 00:46:05.751207: step 84950, total loss = 0.56, predict loss = 0.13 (76.5 examples/sec; 0.052 sec/batch; 85h:55m:40s remains)
INFO - root - 2019-11-04 00:46:06.343023: step 84960, total loss = 0.51, predict loss = 0.11 (82.9 examples/sec; 0.048 sec/batch; 79h:17m:50s remains)
INFO - root - 2019-11-04 00:46:06.985078: step 84970, total loss = 0.60, predict loss = 0.14 (67.9 examples/sec; 0.059 sec/batch; 96h:49m:56s remains)
INFO - root - 2019-11-04 00:46:07.641572: step 84980, total loss = 0.49, predict loss = 0.11 (63.4 examples/sec; 0.063 sec/batch; 103h:35m:37s remains)
INFO - root - 2019-11-04 00:46:08.287688: step 84990, total loss = 0.47, predict loss = 0.11 (70.3 examples/sec; 0.057 sec/batch; 93h:27m:00s remains)
INFO - root - 2019-11-04 00:46:08.934400: step 85000, total loss = 0.48, predict loss = 0.11 (74.7 examples/sec; 0.054 sec/batch; 88h:00m:18s remains)
INFO - root - 2019-11-04 00:46:09.601685: step 85010, total loss = 0.52, predict loss = 0.13 (65.2 examples/sec; 0.061 sec/batch; 100h:43m:26s remains)
INFO - root - 2019-11-04 00:46:10.244876: step 85020, total loss = 0.49, predict loss = 0.12 (71.0 examples/sec; 0.056 sec/batch; 92h:34m:53s remains)
INFO - root - 2019-11-04 00:46:10.866733: step 85030, total loss = 0.74, predict loss = 0.19 (75.3 examples/sec; 0.053 sec/batch; 87h:13m:59s remains)
INFO - root - 2019-11-04 00:46:11.480681: step 85040, total loss = 0.28, predict loss = 0.06 (77.8 examples/sec; 0.051 sec/batch; 84h:28m:10s remains)
INFO - root - 2019-11-04 00:46:12.090237: step 85050, total loss = 0.62, predict loss = 0.14 (70.2 examples/sec; 0.057 sec/batch; 93h:35m:07s remains)
INFO - root - 2019-11-04 00:46:12.735352: step 85060, total loss = 0.45, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 98h:07m:30s remains)
INFO - root - 2019-11-04 00:46:13.348046: step 85070, total loss = 0.57, predict loss = 0.14 (64.0 examples/sec; 0.063 sec/batch; 102h:42m:08s remains)
INFO - root - 2019-11-04 00:46:14.011794: step 85080, total loss = 0.54, predict loss = 0.13 (70.4 examples/sec; 0.057 sec/batch; 93h:18m:30s remains)
INFO - root - 2019-11-04 00:46:14.697542: step 85090, total loss = 0.51, predict loss = 0.12 (77.1 examples/sec; 0.052 sec/batch; 85h:16m:31s remains)
INFO - root - 2019-11-04 00:46:15.338775: step 85100, total loss = 0.46, predict loss = 0.10 (66.4 examples/sec; 0.060 sec/batch; 99h:03m:05s remains)
INFO - root - 2019-11-04 00:46:16.007714: step 85110, total loss = 0.44, predict loss = 0.11 (70.0 examples/sec; 0.057 sec/batch; 93h:54m:50s remains)
INFO - root - 2019-11-04 00:46:16.622587: step 85120, total loss = 0.38, predict loss = 0.09 (74.1 examples/sec; 0.054 sec/batch; 88h:40m:49s remains)
INFO - root - 2019-11-04 00:46:17.238707: step 85130, total loss = 0.39, predict loss = 0.09 (80.8 examples/sec; 0.049 sec/batch; 81h:18m:50s remains)
INFO - root - 2019-11-04 00:46:17.848674: step 85140, total loss = 0.33, predict loss = 0.08 (74.7 examples/sec; 0.054 sec/batch; 87h:56m:21s remains)
INFO - root - 2019-11-04 00:46:18.495059: step 85150, total loss = 0.33, predict loss = 0.07 (58.4 examples/sec; 0.068 sec/batch; 112h:31m:32s remains)
INFO - root - 2019-11-04 00:46:19.157833: step 85160, total loss = 0.44, predict loss = 0.10 (78.8 examples/sec; 0.051 sec/batch; 83h:26m:52s remains)
INFO - root - 2019-11-04 00:46:19.812431: step 85170, total loss = 0.33, predict loss = 0.07 (63.1 examples/sec; 0.063 sec/batch; 104h:07m:21s remains)
INFO - root - 2019-11-04 00:46:20.446999: step 85180, total loss = 0.30, predict loss = 0.06 (75.7 examples/sec; 0.053 sec/batch; 86h:47m:25s remains)
INFO - root - 2019-11-04 00:46:21.063474: step 85190, total loss = 0.41, predict loss = 0.09 (68.1 examples/sec; 0.059 sec/batch; 96h:29m:10s remains)
INFO - root - 2019-11-04 00:46:21.695217: step 85200, total loss = 0.52, predict loss = 0.13 (75.8 examples/sec; 0.053 sec/batch; 86h:44m:32s remains)
INFO - root - 2019-11-04 00:46:22.330822: step 85210, total loss = 0.46, predict loss = 0.11 (65.2 examples/sec; 0.061 sec/batch; 100h:46m:29s remains)
INFO - root - 2019-11-04 00:46:23.016184: step 85220, total loss = 0.54, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 95h:32m:20s remains)
INFO - root - 2019-11-04 00:46:23.660119: step 85230, total loss = 0.51, predict loss = 0.12 (73.7 examples/sec; 0.054 sec/batch; 89h:11m:50s remains)
INFO - root - 2019-11-04 00:46:24.310088: step 85240, total loss = 0.54, predict loss = 0.13 (73.7 examples/sec; 0.054 sec/batch; 89h:11m:11s remains)
INFO - root - 2019-11-04 00:46:24.959754: step 85250, total loss = 0.49, predict loss = 0.11 (78.8 examples/sec; 0.051 sec/batch; 83h:21m:43s remains)
INFO - root - 2019-11-04 00:46:25.573907: step 85260, total loss = 0.66, predict loss = 0.16 (65.0 examples/sec; 0.062 sec/batch; 101h:04m:42s remains)
INFO - root - 2019-11-04 00:46:26.191806: step 85270, total loss = 0.38, predict loss = 0.08 (71.3 examples/sec; 0.056 sec/batch; 92h:08m:07s remains)
INFO - root - 2019-11-04 00:46:26.841818: step 85280, total loss = 0.58, predict loss = 0.14 (68.3 examples/sec; 0.059 sec/batch; 96h:13m:20s remains)
INFO - root - 2019-11-04 00:46:27.523820: step 85290, total loss = 0.41, predict loss = 0.09 (65.7 examples/sec; 0.061 sec/batch; 99h:57m:24s remains)
INFO - root - 2019-11-04 00:46:28.212536: step 85300, total loss = 0.47, predict loss = 0.10 (65.0 examples/sec; 0.062 sec/batch; 101h:09m:02s remains)
INFO - root - 2019-11-04 00:46:28.854648: step 85310, total loss = 0.65, predict loss = 0.15 (68.8 examples/sec; 0.058 sec/batch; 95h:32m:50s remains)
INFO - root - 2019-11-04 00:46:29.537600: step 85320, total loss = 0.66, predict loss = 0.16 (64.0 examples/sec; 0.063 sec/batch; 102h:45m:38s remains)
INFO - root - 2019-11-04 00:46:30.227099: step 85330, total loss = 0.54, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 102h:15m:08s remains)
INFO - root - 2019-11-04 00:46:30.899212: step 85340, total loss = 0.60, predict loss = 0.14 (66.7 examples/sec; 0.060 sec/batch; 98h:32m:06s remains)
INFO - root - 2019-11-04 00:46:31.552446: step 85350, total loss = 0.48, predict loss = 0.12 (81.1 examples/sec; 0.049 sec/batch; 81h:01m:39s remains)
INFO - root - 2019-11-04 00:46:32.173258: step 85360, total loss = 0.58, predict loss = 0.14 (65.2 examples/sec; 0.061 sec/batch; 100h:45m:46s remains)
INFO - root - 2019-11-04 00:46:32.780961: step 85370, total loss = 0.29, predict loss = 0.06 (74.1 examples/sec; 0.054 sec/batch; 88h:44m:50s remains)
INFO - root - 2019-11-04 00:46:33.395690: step 85380, total loss = 0.28, predict loss = 0.05 (71.9 examples/sec; 0.056 sec/batch; 91h:22m:49s remains)
INFO - root - 2019-11-04 00:46:34.039256: step 85390, total loss = 0.31, predict loss = 0.06 (72.4 examples/sec; 0.055 sec/batch; 90h:44m:27s remains)
INFO - root - 2019-11-04 00:46:34.686652: step 85400, total loss = 0.30, predict loss = 0.06 (70.2 examples/sec; 0.057 sec/batch; 93h:39m:05s remains)
INFO - root - 2019-11-04 00:46:35.341060: step 85410, total loss = 0.29, predict loss = 0.06 (78.7 examples/sec; 0.051 sec/batch; 83h:28m:39s remains)
INFO - root - 2019-11-04 00:46:35.935733: step 85420, total loss = 0.25, predict loss = 0.05 (71.0 examples/sec; 0.056 sec/batch; 92h:32m:42s remains)
INFO - root - 2019-11-04 00:46:36.604267: step 85430, total loss = 0.30, predict loss = 0.06 (59.5 examples/sec; 0.067 sec/batch; 110h:25m:09s remains)
INFO - root - 2019-11-04 00:46:37.272555: step 85440, total loss = 0.43, predict loss = 0.09 (75.8 examples/sec; 0.053 sec/batch; 86h:39m:59s remains)
INFO - root - 2019-11-04 00:46:37.897043: step 85450, total loss = 0.43, predict loss = 0.10 (67.5 examples/sec; 0.059 sec/batch; 97h:20m:06s remains)
INFO - root - 2019-11-04 00:46:38.550084: step 85460, total loss = 0.38, predict loss = 0.08 (75.2 examples/sec; 0.053 sec/batch; 87h:21m:41s remains)
INFO - root - 2019-11-04 00:46:39.211372: step 85470, total loss = 0.44, predict loss = 0.10 (62.9 examples/sec; 0.064 sec/batch; 104h:31m:13s remains)
INFO - root - 2019-11-04 00:46:39.826003: step 85480, total loss = 0.33, predict loss = 0.07 (84.9 examples/sec; 0.047 sec/batch; 77h:25m:57s remains)
INFO - root - 2019-11-04 00:46:40.434582: step 85490, total loss = 0.29, predict loss = 0.06 (74.6 examples/sec; 0.054 sec/batch; 88h:06m:47s remains)
INFO - root - 2019-11-04 00:46:41.078930: step 85500, total loss = 0.38, predict loss = 0.08 (66.1 examples/sec; 0.061 sec/batch; 99h:27m:35s remains)
INFO - root - 2019-11-04 00:46:41.701749: step 85510, total loss = 0.25, predict loss = 0.05 (78.4 examples/sec; 0.051 sec/batch; 83h:46m:38s remains)
INFO - root - 2019-11-04 00:46:42.312341: step 85520, total loss = 0.68, predict loss = 0.16 (72.0 examples/sec; 0.056 sec/batch; 91h:18m:19s remains)
INFO - root - 2019-11-04 00:46:42.953345: step 85530, total loss = 0.40, predict loss = 0.09 (62.2 examples/sec; 0.064 sec/batch; 105h:36m:29s remains)
INFO - root - 2019-11-04 00:46:43.702888: step 85540, total loss = 0.50, predict loss = 0.12 (62.5 examples/sec; 0.064 sec/batch; 105h:13m:45s remains)
INFO - root - 2019-11-04 00:46:44.429475: step 85550, total loss = 0.44, predict loss = 0.10 (65.6 examples/sec; 0.061 sec/batch; 100h:10m:43s remains)
INFO - root - 2019-11-04 00:46:45.064370: step 85560, total loss = 0.52, predict loss = 0.12 (79.1 examples/sec; 0.051 sec/batch; 83h:04m:50s remains)
INFO - root - 2019-11-04 00:46:45.667598: step 85570, total loss = 0.57, predict loss = 0.13 (78.7 examples/sec; 0.051 sec/batch; 83h:29m:54s remains)
INFO - root - 2019-11-04 00:46:46.275903: step 85580, total loss = 0.43, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 84h:40m:59s remains)
INFO - root - 2019-11-04 00:46:46.924253: step 85590, total loss = 0.37, predict loss = 0.09 (82.4 examples/sec; 0.049 sec/batch; 79h:47m:40s remains)
INFO - root - 2019-11-04 00:46:47.568565: step 85600, total loss = 0.51, predict loss = 0.13 (65.0 examples/sec; 0.062 sec/batch; 101h:07m:50s remains)
INFO - root - 2019-11-04 00:46:48.223883: step 85610, total loss = 0.34, predict loss = 0.08 (68.2 examples/sec; 0.059 sec/batch; 96h:17m:23s remains)
INFO - root - 2019-11-04 00:46:48.876788: step 85620, total loss = 0.25, predict loss = 0.05 (73.4 examples/sec; 0.054 sec/batch; 89h:31m:15s remains)
INFO - root - 2019-11-04 00:46:49.485490: step 85630, total loss = 0.34, predict loss = 0.08 (69.2 examples/sec; 0.058 sec/batch; 94h:59m:11s remains)
INFO - root - 2019-11-04 00:46:50.098137: step 85640, total loss = 0.31, predict loss = 0.06 (72.6 examples/sec; 0.055 sec/batch; 90h:31m:23s remains)
INFO - root - 2019-11-04 00:46:50.717985: step 85650, total loss = 0.35, predict loss = 0.08 (69.9 examples/sec; 0.057 sec/batch; 94h:03m:36s remains)
INFO - root - 2019-11-04 00:46:51.368649: step 85660, total loss = 0.42, predict loss = 0.10 (64.7 examples/sec; 0.062 sec/batch; 101h:32m:17s remains)
INFO - root - 2019-11-04 00:46:52.015366: step 85670, total loss = 0.55, predict loss = 0.13 (63.1 examples/sec; 0.063 sec/batch; 104h:08m:35s remains)
INFO - root - 2019-11-04 00:46:52.671500: step 85680, total loss = 0.61, predict loss = 0.14 (68.3 examples/sec; 0.059 sec/batch; 96h:12m:13s remains)
INFO - root - 2019-11-04 00:46:53.292364: step 85690, total loss = 0.38, predict loss = 0.09 (75.0 examples/sec; 0.053 sec/batch; 87h:38m:49s remains)
INFO - root - 2019-11-04 00:46:53.944241: step 85700, total loss = 0.33, predict loss = 0.07 (69.7 examples/sec; 0.057 sec/batch; 94h:14m:29s remains)
INFO - root - 2019-11-04 00:46:54.568465: step 85710, total loss = 0.49, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 87h:03m:43s remains)
INFO - root - 2019-11-04 00:46:55.218526: step 85720, total loss = 0.44, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 92h:27m:16s remains)
INFO - root - 2019-11-04 00:46:55.870557: step 85730, total loss = 0.48, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 97h:06m:53s remains)
INFO - root - 2019-11-04 00:46:56.517143: step 85740, total loss = 0.43, predict loss = 0.08 (77.6 examples/sec; 0.052 sec/batch; 84h:42m:13s remains)
INFO - root - 2019-11-04 00:46:57.137308: step 85750, total loss = 0.47, predict loss = 0.11 (69.4 examples/sec; 0.058 sec/batch; 94h:43m:46s remains)
INFO - root - 2019-11-04 00:46:57.782138: step 85760, total loss = 0.31, predict loss = 0.07 (66.6 examples/sec; 0.060 sec/batch; 98h:42m:20s remains)
INFO - root - 2019-11-04 00:46:58.465003: step 85770, total loss = 0.38, predict loss = 0.08 (59.6 examples/sec; 0.067 sec/batch; 110h:11m:06s remains)
INFO - root - 2019-11-04 00:46:59.135510: step 85780, total loss = 0.34, predict loss = 0.08 (69.7 examples/sec; 0.057 sec/batch; 94h:18m:48s remains)
INFO - root - 2019-11-04 00:46:59.825817: step 85790, total loss = 0.55, predict loss = 0.12 (60.7 examples/sec; 0.066 sec/batch; 108h:12m:16s remains)
INFO - root - 2019-11-04 00:47:00.471335: step 85800, total loss = 0.47, predict loss = 0.11 (74.3 examples/sec; 0.054 sec/batch; 88h:23m:48s remains)
INFO - root - 2019-11-04 00:47:01.077600: step 85810, total loss = 0.45, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 99h:45m:26s remains)
INFO - root - 2019-11-04 00:47:01.743679: step 85820, total loss = 0.61, predict loss = 0.15 (63.3 examples/sec; 0.063 sec/batch; 103h:52m:46s remains)
INFO - root - 2019-11-04 00:47:02.369768: step 85830, total loss = 0.40, predict loss = 0.10 (83.4 examples/sec; 0.048 sec/batch; 78h:45m:55s remains)
INFO - root - 2019-11-04 00:47:02.984046: step 85840, total loss = 0.36, predict loss = 0.07 (69.6 examples/sec; 0.057 sec/batch; 94h:26m:14s remains)
INFO - root - 2019-11-04 00:47:03.630844: step 85850, total loss = 0.39, predict loss = 0.08 (65.9 examples/sec; 0.061 sec/batch; 99h:38m:47s remains)
INFO - root - 2019-11-04 00:47:04.253009: step 85860, total loss = 0.53, predict loss = 0.12 (70.7 examples/sec; 0.057 sec/batch; 92h:54m:51s remains)
INFO - root - 2019-11-04 00:47:04.928743: step 85870, total loss = 0.66, predict loss = 0.15 (66.9 examples/sec; 0.060 sec/batch; 98h:09m:52s remains)
INFO - root - 2019-11-04 00:47:05.550815: step 85880, total loss = 0.63, predict loss = 0.14 (67.9 examples/sec; 0.059 sec/batch; 96h:47m:06s remains)
INFO - root - 2019-11-04 00:47:06.162369: step 85890, total loss = 0.41, predict loss = 0.09 (65.5 examples/sec; 0.061 sec/batch; 100h:16m:14s remains)
INFO - root - 2019-11-04 00:47:06.802708: step 85900, total loss = 0.34, predict loss = 0.07 (69.4 examples/sec; 0.058 sec/batch; 94h:43m:57s remains)
INFO - root - 2019-11-04 00:47:07.451704: step 85910, total loss = 0.28, predict loss = 0.07 (72.4 examples/sec; 0.055 sec/batch; 90h:44m:01s remains)
INFO - root - 2019-11-04 00:47:08.082808: step 85920, total loss = 0.26, predict loss = 0.05 (69.8 examples/sec; 0.057 sec/batch; 94h:06m:52s remains)
INFO - root - 2019-11-04 00:47:08.744832: step 85930, total loss = 0.53, predict loss = 0.11 (68.2 examples/sec; 0.059 sec/batch; 96h:19m:32s remains)
INFO - root - 2019-11-04 00:47:09.393176: step 85940, total loss = 0.43, predict loss = 0.10 (73.9 examples/sec; 0.054 sec/batch; 88h:57m:34s remains)
INFO - root - 2019-11-04 00:47:10.017466: step 85950, total loss = 0.55, predict loss = 0.13 (72.3 examples/sec; 0.055 sec/batch; 90h:49m:49s remains)
INFO - root - 2019-11-04 00:47:10.665865: step 85960, total loss = 0.50, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 93h:29m:00s remains)
INFO - root - 2019-11-04 00:47:11.342732: step 85970, total loss = 0.88, predict loss = 0.20 (71.4 examples/sec; 0.056 sec/batch; 92h:05m:19s remains)
INFO - root - 2019-11-04 00:47:11.981002: step 85980, total loss = 0.75, predict loss = 0.17 (75.1 examples/sec; 0.053 sec/batch; 87h:29m:22s remains)
INFO - root - 2019-11-04 00:47:12.607844: step 85990, total loss = 0.99, predict loss = 0.23 (69.1 examples/sec; 0.058 sec/batch; 95h:04m:21s remains)
INFO - root - 2019-11-04 00:47:13.232725: step 86000, total loss = 0.88, predict loss = 0.22 (66.6 examples/sec; 0.060 sec/batch; 98h:35m:56s remains)
INFO - root - 2019-11-04 00:47:13.908477: step 86010, total loss = 0.80, predict loss = 0.19 (65.4 examples/sec; 0.061 sec/batch; 100h:25m:52s remains)
INFO - root - 2019-11-04 00:47:14.573681: step 86020, total loss = 0.59, predict loss = 0.13 (71.3 examples/sec; 0.056 sec/batch; 92h:12m:52s remains)
INFO - root - 2019-11-04 00:47:15.238269: step 86030, total loss = 0.76, predict loss = 0.17 (69.8 examples/sec; 0.057 sec/batch; 94h:10m:04s remains)
INFO - root - 2019-11-04 00:47:15.887480: step 86040, total loss = 0.66, predict loss = 0.16 (65.0 examples/sec; 0.062 sec/batch; 101h:05m:04s remains)
INFO - root - 2019-11-04 00:47:16.548548: step 86050, total loss = 0.66, predict loss = 0.15 (70.3 examples/sec; 0.057 sec/batch; 93h:24m:49s remains)
INFO - root - 2019-11-04 00:47:17.228305: step 86060, total loss = 0.52, predict loss = 0.12 (62.0 examples/sec; 0.065 sec/batch; 106h:02m:28s remains)
INFO - root - 2019-11-04 00:47:17.922511: step 86070, total loss = 0.52, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 92h:06m:10s remains)
INFO - root - 2019-11-04 00:47:18.541088: step 86080, total loss = 0.53, predict loss = 0.11 (71.4 examples/sec; 0.056 sec/batch; 92h:05m:08s remains)
INFO - root - 2019-11-04 00:47:19.196613: step 86090, total loss = 0.75, predict loss = 0.18 (68.6 examples/sec; 0.058 sec/batch; 95h:50m:10s remains)
INFO - root - 2019-11-04 00:47:19.871969: step 86100, total loss = 0.53, predict loss = 0.12 (61.0 examples/sec; 0.066 sec/batch; 107h:40m:18s remains)
INFO - root - 2019-11-04 00:47:20.488712: step 86110, total loss = 0.50, predict loss = 0.11 (79.7 examples/sec; 0.050 sec/batch; 82h:28m:50s remains)
INFO - root - 2019-11-04 00:47:21.115644: step 86120, total loss = 0.52, predict loss = 0.12 (77.6 examples/sec; 0.052 sec/batch; 84h:39m:05s remains)
INFO - root - 2019-11-04 00:47:21.825174: step 86130, total loss = 0.54, predict loss = 0.13 (67.1 examples/sec; 0.060 sec/batch; 97h:51m:59s remains)
INFO - root - 2019-11-04 00:47:22.510589: step 86140, total loss = 0.58, predict loss = 0.14 (75.3 examples/sec; 0.053 sec/batch; 87h:18m:32s remains)
INFO - root - 2019-11-04 00:47:23.180132: step 86150, total loss = 0.58, predict loss = 0.13 (71.4 examples/sec; 0.056 sec/batch; 92h:02m:02s remains)
INFO - root - 2019-11-04 00:47:23.843036: step 86160, total loss = 0.49, predict loss = 0.11 (70.9 examples/sec; 0.056 sec/batch; 92h:38m:52s remains)
INFO - root - 2019-11-04 00:47:24.451596: step 86170, total loss = 0.54, predict loss = 0.12 (75.6 examples/sec; 0.053 sec/batch; 86h:58m:22s remains)
INFO - root - 2019-11-04 00:47:25.111453: step 86180, total loss = 0.53, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 98h:09m:52s remains)
INFO - root - 2019-11-04 00:47:25.801479: step 86190, total loss = 0.53, predict loss = 0.12 (56.2 examples/sec; 0.071 sec/batch; 116h:56m:23s remains)
INFO - root - 2019-11-04 00:47:26.529335: step 86200, total loss = 0.52, predict loss = 0.12 (54.3 examples/sec; 0.074 sec/batch; 121h:03m:31s remains)
INFO - root - 2019-11-04 00:47:27.345827: step 86210, total loss = 0.49, predict loss = 0.11 (57.3 examples/sec; 0.070 sec/batch; 114h:37m:36s remains)
INFO - root - 2019-11-04 00:47:28.128465: step 86220, total loss = 0.41, predict loss = 0.10 (59.6 examples/sec; 0.067 sec/batch; 110h:11m:18s remains)
INFO - root - 2019-11-04 00:47:28.733367: step 86230, total loss = 0.37, predict loss = 0.08 (72.3 examples/sec; 0.055 sec/batch; 90h:54m:00s remains)
INFO - root - 2019-11-04 00:47:29.356093: step 86240, total loss = 0.42, predict loss = 0.09 (77.2 examples/sec; 0.052 sec/batch; 85h:08m:03s remains)
INFO - root - 2019-11-04 00:47:30.004095: step 86250, total loss = 0.44, predict loss = 0.10 (73.1 examples/sec; 0.055 sec/batch; 89h:54m:29s remains)
INFO - root - 2019-11-04 00:47:30.668089: step 86260, total loss = 0.34, predict loss = 0.07 (73.7 examples/sec; 0.054 sec/batch; 89h:12m:09s remains)
INFO - root - 2019-11-04 00:47:31.351413: step 86270, total loss = 0.37, predict loss = 0.09 (65.9 examples/sec; 0.061 sec/batch; 99h:42m:52s remains)
INFO - root - 2019-11-04 00:47:32.059608: step 86280, total loss = 0.43, predict loss = 0.09 (66.4 examples/sec; 0.060 sec/batch; 98h:53m:45s remains)
INFO - root - 2019-11-04 00:47:32.671236: step 86290, total loss = 0.42, predict loss = 0.09 (71.0 examples/sec; 0.056 sec/batch; 92h:31m:43s remains)
INFO - root - 2019-11-04 00:47:33.328096: step 86300, total loss = 0.50, predict loss = 0.11 (73.3 examples/sec; 0.055 sec/batch; 89h:40m:41s remains)
INFO - root - 2019-11-04 00:47:33.974133: step 86310, total loss = 0.49, predict loss = 0.11 (65.8 examples/sec; 0.061 sec/batch; 99h:54m:39s remains)
INFO - root - 2019-11-04 00:47:34.613035: step 86320, total loss = 0.44, predict loss = 0.10 (71.0 examples/sec; 0.056 sec/batch; 92h:31m:19s remains)
INFO - root - 2019-11-04 00:47:35.335226: step 86330, total loss = 0.40, predict loss = 0.09 (70.7 examples/sec; 0.057 sec/batch; 92h:59m:02s remains)
INFO - root - 2019-11-04 00:47:36.003543: step 86340, total loss = 0.63, predict loss = 0.15 (62.3 examples/sec; 0.064 sec/batch; 105h:23m:55s remains)
INFO - root - 2019-11-04 00:47:36.688603: step 86350, total loss = 0.48, predict loss = 0.11 (63.4 examples/sec; 0.063 sec/batch; 103h:36m:53s remains)
INFO - root - 2019-11-04 00:47:37.352518: step 86360, total loss = 0.53, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 100h:36m:57s remains)
INFO - root - 2019-11-04 00:47:37.985190: step 86370, total loss = 0.45, predict loss = 0.11 (70.9 examples/sec; 0.056 sec/batch; 92h:43m:57s remains)
INFO - root - 2019-11-04 00:47:38.648840: step 86380, total loss = 0.51, predict loss = 0.12 (61.0 examples/sec; 0.066 sec/batch; 107h:43m:10s remains)
INFO - root - 2019-11-04 00:47:39.356640: step 86390, total loss = 0.54, predict loss = 0.13 (62.4 examples/sec; 0.064 sec/batch; 105h:16m:09s remains)
INFO - root - 2019-11-04 00:47:40.026475: step 86400, total loss = 0.46, predict loss = 0.11 (70.9 examples/sec; 0.056 sec/batch; 92h:43m:16s remains)
INFO - root - 2019-11-04 00:47:40.646104: step 86410, total loss = 0.62, predict loss = 0.15 (65.1 examples/sec; 0.061 sec/batch; 100h:59m:09s remains)
INFO - root - 2019-11-04 00:47:41.273843: step 86420, total loss = 0.49, predict loss = 0.11 (65.5 examples/sec; 0.061 sec/batch; 100h:19m:37s remains)
INFO - root - 2019-11-04 00:47:41.898053: step 86430, total loss = 0.60, predict loss = 0.14 (73.8 examples/sec; 0.054 sec/batch; 89h:05m:30s remains)
INFO - root - 2019-11-04 00:47:42.530400: step 86440, total loss = 0.69, predict loss = 0.17 (78.1 examples/sec; 0.051 sec/batch; 84h:09m:01s remains)
INFO - root - 2019-11-04 00:47:43.165449: step 86450, total loss = 0.74, predict loss = 0.17 (73.5 examples/sec; 0.054 sec/batch; 89h:26m:07s remains)
INFO - root - 2019-11-04 00:47:43.792081: step 86460, total loss = 0.55, predict loss = 0.12 (75.7 examples/sec; 0.053 sec/batch; 86h:45m:43s remains)
INFO - root - 2019-11-04 00:47:44.431978: step 86470, total loss = 0.65, predict loss = 0.15 (62.2 examples/sec; 0.064 sec/batch; 105h:35m:23s remains)
INFO - root - 2019-11-04 00:47:45.087678: step 86480, total loss = 0.59, predict loss = 0.14 (70.1 examples/sec; 0.057 sec/batch; 93h:46m:12s remains)
INFO - root - 2019-11-04 00:47:45.701526: step 86490, total loss = 0.58, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 93h:13m:26s remains)
INFO - root - 2019-11-04 00:47:46.312153: step 86500, total loss = 0.64, predict loss = 0.15 (72.5 examples/sec; 0.055 sec/batch; 90h:36m:23s remains)
INFO - root - 2019-11-04 00:47:46.944720: step 86510, total loss = 0.57, predict loss = 0.13 (63.7 examples/sec; 0.063 sec/batch; 103h:07m:57s remains)
INFO - root - 2019-11-04 00:47:47.549983: step 86520, total loss = 0.46, predict loss = 0.11 (73.4 examples/sec; 0.055 sec/batch; 89h:33m:29s remains)
INFO - root - 2019-11-04 00:47:48.195051: step 86530, total loss = 0.36, predict loss = 0.08 (63.0 examples/sec; 0.064 sec/batch; 104h:21m:56s remains)
INFO - root - 2019-11-04 00:47:48.835033: step 86540, total loss = 0.39, predict loss = 0.08 (71.1 examples/sec; 0.056 sec/batch; 92h:22m:05s remains)
INFO - root - 2019-11-04 00:47:49.468417: step 86550, total loss = 0.49, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 92h:24m:11s remains)
INFO - root - 2019-11-04 00:47:50.126431: step 86560, total loss = 0.26, predict loss = 0.05 (68.4 examples/sec; 0.058 sec/batch; 96h:00m:58s remains)
INFO - root - 2019-11-04 00:47:50.758376: step 86570, total loss = 0.51, predict loss = 0.12 (63.2 examples/sec; 0.063 sec/batch; 104h:00m:04s remains)
INFO - root - 2019-11-04 00:47:51.385828: step 86580, total loss = 0.47, predict loss = 0.11 (76.9 examples/sec; 0.052 sec/batch; 85h:24m:11s remains)
INFO - root - 2019-11-04 00:47:52.038580: step 86590, total loss = 0.42, predict loss = 0.09 (66.9 examples/sec; 0.060 sec/batch; 98h:13m:13s remains)
INFO - root - 2019-11-04 00:47:52.679829: step 86600, total loss = 0.47, predict loss = 0.11 (64.5 examples/sec; 0.062 sec/batch; 101h:52m:09s remains)
INFO - root - 2019-11-04 00:47:53.323588: step 86610, total loss = 0.56, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 92h:32m:05s remains)
INFO - root - 2019-11-04 00:47:53.972525: step 86620, total loss = 0.69, predict loss = 0.17 (82.6 examples/sec; 0.048 sec/batch; 79h:34m:18s remains)
INFO - root - 2019-11-04 00:47:54.591849: step 86630, total loss = 0.55, predict loss = 0.13 (71.1 examples/sec; 0.056 sec/batch; 92h:23m:39s remains)
INFO - root - 2019-11-04 00:47:55.210461: step 86640, total loss = 0.53, predict loss = 0.12 (74.9 examples/sec; 0.053 sec/batch; 87h:42m:01s remains)
INFO - root - 2019-11-04 00:47:55.884111: step 86650, total loss = 0.58, predict loss = 0.14 (72.7 examples/sec; 0.055 sec/batch; 90h:19m:12s remains)
INFO - root - 2019-11-04 00:47:56.517478: step 86660, total loss = 0.73, predict loss = 0.18 (74.3 examples/sec; 0.054 sec/batch; 88h:24m:22s remains)
INFO - root - 2019-11-04 00:47:57.196849: step 86670, total loss = 0.50, predict loss = 0.12 (76.0 examples/sec; 0.053 sec/batch; 86h:27m:15s remains)
INFO - root - 2019-11-04 00:47:57.901683: step 86680, total loss = 0.58, predict loss = 0.15 (61.4 examples/sec; 0.065 sec/batch; 107h:04m:55s remains)
INFO - root - 2019-11-04 00:47:58.644480: step 86690, total loss = 0.58, predict loss = 0.15 (68.2 examples/sec; 0.059 sec/batch; 96h:23m:17s remains)
INFO - root - 2019-11-04 00:47:59.433774: step 86700, total loss = 0.63, predict loss = 0.16 (54.9 examples/sec; 0.073 sec/batch; 119h:37m:08s remains)
INFO - root - 2019-11-04 00:48:00.259826: step 86710, total loss = 0.62, predict loss = 0.15 (56.6 examples/sec; 0.071 sec/batch; 116h:03m:38s remains)
INFO - root - 2019-11-04 00:48:00.924758: step 86720, total loss = 0.46, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 98h:48m:27s remains)
INFO - root - 2019-11-04 00:48:01.570405: step 86730, total loss = 0.51, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 98h:47m:07s remains)
INFO - root - 2019-11-04 00:48:02.236533: step 86740, total loss = 0.55, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 103h:23m:50s remains)
INFO - root - 2019-11-04 00:48:02.876964: step 86750, total loss = 0.54, predict loss = 0.12 (71.0 examples/sec; 0.056 sec/batch; 92h:34m:57s remains)
INFO - root - 2019-11-04 00:48:03.538387: step 86760, total loss = 0.53, predict loss = 0.13 (73.7 examples/sec; 0.054 sec/batch; 89h:11m:46s remains)
INFO - root - 2019-11-04 00:48:04.154574: step 86770, total loss = 0.49, predict loss = 0.11 (78.0 examples/sec; 0.051 sec/batch; 84h:12m:17s remains)
INFO - root - 2019-11-04 00:48:04.769003: step 86780, total loss = 0.49, predict loss = 0.12 (70.7 examples/sec; 0.057 sec/batch; 92h:57m:47s remains)
INFO - root - 2019-11-04 00:48:05.455156: step 86790, total loss = 0.49, predict loss = 0.11 (65.0 examples/sec; 0.062 sec/batch; 101h:04m:39s remains)
INFO - root - 2019-11-04 00:48:06.085403: step 86800, total loss = 0.48, predict loss = 0.11 (69.0 examples/sec; 0.058 sec/batch; 95h:14m:47s remains)
INFO - root - 2019-11-04 00:48:06.692251: step 86810, total loss = 0.42, predict loss = 0.10 (71.1 examples/sec; 0.056 sec/batch; 92h:28m:09s remains)
INFO - root - 2019-11-04 00:48:07.326606: step 86820, total loss = 0.42, predict loss = 0.09 (72.1 examples/sec; 0.055 sec/batch; 91h:09m:07s remains)
INFO - root - 2019-11-04 00:48:07.965655: step 86830, total loss = 0.45, predict loss = 0.10 (62.9 examples/sec; 0.064 sec/batch; 104h:30m:59s remains)
INFO - root - 2019-11-04 00:48:08.589796: step 86840, total loss = 0.41, predict loss = 0.09 (67.3 examples/sec; 0.059 sec/batch; 97h:35m:46s remains)
INFO - root - 2019-11-04 00:48:09.236151: step 86850, total loss = 0.36, predict loss = 0.08 (69.8 examples/sec; 0.057 sec/batch; 94h:09m:54s remains)
INFO - root - 2019-11-04 00:48:09.867588: step 86860, total loss = 0.51, predict loss = 0.12 (77.7 examples/sec; 0.051 sec/batch; 84h:30m:24s remains)
INFO - root - 2019-11-04 00:48:10.553512: step 86870, total loss = 0.41, predict loss = 0.09 (62.1 examples/sec; 0.064 sec/batch; 105h:45m:17s remains)
INFO - root - 2019-11-04 00:48:11.192666: step 86880, total loss = 0.49, predict loss = 0.11 (67.3 examples/sec; 0.059 sec/batch; 97h:39m:32s remains)
INFO - root - 2019-11-04 00:48:11.839718: step 86890, total loss = 0.43, predict loss = 0.09 (72.2 examples/sec; 0.055 sec/batch; 90h:59m:11s remains)
INFO - root - 2019-11-04 00:48:12.450222: step 86900, total loss = 0.50, predict loss = 0.11 (74.4 examples/sec; 0.054 sec/batch; 88h:18m:01s remains)
INFO - root - 2019-11-04 00:48:13.082583: step 86910, total loss = 0.61, predict loss = 0.14 (75.6 examples/sec; 0.053 sec/batch; 86h:51m:52s remains)
INFO - root - 2019-11-04 00:48:13.733670: step 86920, total loss = 0.44, predict loss = 0.10 (69.5 examples/sec; 0.058 sec/batch; 94h:34m:37s remains)
INFO - root - 2019-11-04 00:48:14.377244: step 86930, total loss = 0.45, predict loss = 0.10 (71.5 examples/sec; 0.056 sec/batch; 91h:52m:12s remains)
INFO - root - 2019-11-04 00:48:15.046030: step 86940, total loss = 0.47, predict loss = 0.11 (71.4 examples/sec; 0.056 sec/batch; 91h:58m:49s remains)
INFO - root - 2019-11-04 00:48:15.728887: step 86950, total loss = 0.48, predict loss = 0.11 (60.9 examples/sec; 0.066 sec/batch; 107h:57m:43s remains)
INFO - root - 2019-11-04 00:48:16.333271: step 86960, total loss = 0.53, predict loss = 0.13 (79.6 examples/sec; 0.050 sec/batch; 82h:29m:17s remains)
INFO - root - 2019-11-04 00:48:16.935576: step 86970, total loss = 0.67, predict loss = 0.16 (64.9 examples/sec; 0.062 sec/batch; 101h:17m:15s remains)
INFO - root - 2019-11-04 00:48:17.582850: step 86980, total loss = 0.54, predict loss = 0.12 (84.5 examples/sec; 0.047 sec/batch; 77h:47m:01s remains)
INFO - root - 2019-11-04 00:48:18.224237: step 86990, total loss = 0.64, predict loss = 0.15 (69.8 examples/sec; 0.057 sec/batch; 94h:05m:17s remains)
INFO - root - 2019-11-04 00:48:18.899193: step 87000, total loss = 0.51, predict loss = 0.13 (65.0 examples/sec; 0.062 sec/batch; 101h:04m:12s remains)
INFO - root - 2019-11-04 00:48:19.570662: step 87010, total loss = 0.66, predict loss = 0.16 (67.0 examples/sec; 0.060 sec/batch; 98h:07m:43s remains)
INFO - root - 2019-11-04 00:48:20.204005: step 87020, total loss = 0.67, predict loss = 0.16 (69.1 examples/sec; 0.058 sec/batch; 95h:03m:29s remains)
INFO - root - 2019-11-04 00:48:20.776480: step 87030, total loss = 0.65, predict loss = 0.16 (73.7 examples/sec; 0.054 sec/batch; 89h:06m:56s remains)
INFO - root - 2019-11-04 00:48:21.426179: step 87040, total loss = 0.44, predict loss = 0.10 (59.2 examples/sec; 0.068 sec/batch; 110h:55m:15s remains)
INFO - root - 2019-11-04 00:48:22.126069: step 87050, total loss = 0.70, predict loss = 0.18 (54.3 examples/sec; 0.074 sec/batch; 120h:57m:11s remains)
INFO - root - 2019-11-04 00:48:22.805517: step 87060, total loss = 0.57, predict loss = 0.13 (65.2 examples/sec; 0.061 sec/batch; 100h:47m:56s remains)
INFO - root - 2019-11-04 00:48:23.462307: step 87070, total loss = 0.71, predict loss = 0.17 (67.7 examples/sec; 0.059 sec/batch; 97h:03m:08s remains)
INFO - root - 2019-11-04 00:48:24.178720: step 87080, total loss = 0.59, predict loss = 0.14 (76.2 examples/sec; 0.053 sec/batch; 86h:16m:10s remains)
INFO - root - 2019-11-04 00:48:24.781413: step 87090, total loss = 0.62, predict loss = 0.14 (70.4 examples/sec; 0.057 sec/batch; 93h:15m:22s remains)
INFO - root - 2019-11-04 00:48:25.410280: step 87100, total loss = 0.79, predict loss = 0.20 (78.5 examples/sec; 0.051 sec/batch; 83h:43m:13s remains)
INFO - root - 2019-11-04 00:48:26.040845: step 87110, total loss = 0.48, predict loss = 0.11 (66.1 examples/sec; 0.060 sec/batch; 99h:21m:25s remains)
INFO - root - 2019-11-04 00:48:26.732762: step 87120, total loss = 0.54, predict loss = 0.12 (61.9 examples/sec; 0.065 sec/batch; 106h:12m:59s remains)
INFO - root - 2019-11-04 00:48:27.461418: step 87130, total loss = 0.48, predict loss = 0.10 (61.8 examples/sec; 0.065 sec/batch; 106h:16m:37s remains)
INFO - root - 2019-11-04 00:48:28.165459: step 87140, total loss = 0.54, predict loss = 0.13 (61.4 examples/sec; 0.065 sec/batch; 107h:05m:10s remains)
INFO - root - 2019-11-04 00:48:28.809122: step 87150, total loss = 0.36, predict loss = 0.08 (66.0 examples/sec; 0.061 sec/batch; 99h:36m:09s remains)
INFO - root - 2019-11-04 00:48:29.463078: step 87160, total loss = 0.36, predict loss = 0.08 (73.6 examples/sec; 0.054 sec/batch; 89h:17m:35s remains)
INFO - root - 2019-11-04 00:48:30.187743: step 87170, total loss = 0.38, predict loss = 0.09 (76.4 examples/sec; 0.052 sec/batch; 86h:01m:16s remains)
INFO - root - 2019-11-04 00:48:30.800015: step 87180, total loss = 0.53, predict loss = 0.12 (64.4 examples/sec; 0.062 sec/batch; 101h:59m:06s remains)
INFO - root - 2019-11-04 00:48:31.463942: step 87190, total loss = 0.54, predict loss = 0.13 (69.0 examples/sec; 0.058 sec/batch; 95h:14m:07s remains)
INFO - root - 2019-11-04 00:48:32.101426: step 87200, total loss = 0.59, predict loss = 0.14 (70.9 examples/sec; 0.056 sec/batch; 92h:41m:56s remains)
INFO - root - 2019-11-04 00:48:32.728264: step 87210, total loss = 0.53, predict loss = 0.12 (74.2 examples/sec; 0.054 sec/batch; 88h:33m:50s remains)
INFO - root - 2019-11-04 00:48:33.347268: step 87220, total loss = 0.53, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 86h:34m:06s remains)
INFO - root - 2019-11-04 00:48:34.005332: step 87230, total loss = 0.59, predict loss = 0.14 (62.5 examples/sec; 0.064 sec/batch; 105h:04m:17s remains)
INFO - root - 2019-11-04 00:48:34.672484: step 87240, total loss = 0.65, predict loss = 0.15 (72.5 examples/sec; 0.055 sec/batch; 90h:39m:42s remains)
INFO - root - 2019-11-04 00:48:35.313649: step 87250, total loss = 0.58, predict loss = 0.14 (57.8 examples/sec; 0.069 sec/batch; 113h:40m:44s remains)
INFO - root - 2019-11-04 00:48:35.870863: step 87260, total loss = 0.37, predict loss = 0.08 (86.8 examples/sec; 0.046 sec/batch; 75h:41m:53s remains)
INFO - root - 2019-11-04 00:48:36.339487: step 87270, total loss = 0.47, predict loss = 0.11 (90.8 examples/sec; 0.044 sec/batch; 72h:22m:11s remains)
INFO - root - 2019-11-04 00:48:37.414019: step 87280, total loss = 0.36, predict loss = 0.08 (72.3 examples/sec; 0.055 sec/batch; 90h:52m:04s remains)
INFO - root - 2019-11-04 00:48:38.048238: step 87290, total loss = 0.58, predict loss = 0.14 (59.5 examples/sec; 0.067 sec/batch; 110h:22m:47s remains)
INFO - root - 2019-11-04 00:48:38.767584: step 87300, total loss = 0.56, predict loss = 0.12 (68.5 examples/sec; 0.058 sec/batch; 95h:53m:46s remains)
INFO - root - 2019-11-04 00:48:39.409477: step 87310, total loss = 0.51, predict loss = 0.11 (66.3 examples/sec; 0.060 sec/batch; 99h:02m:35s remains)
INFO - root - 2019-11-04 00:48:40.049780: step 87320, total loss = 0.49, predict loss = 0.11 (72.5 examples/sec; 0.055 sec/batch; 90h:35m:12s remains)
INFO - root - 2019-11-04 00:48:40.677057: step 87330, total loss = 0.31, predict loss = 0.06 (74.0 examples/sec; 0.054 sec/batch; 88h:49m:45s remains)
INFO - root - 2019-11-04 00:48:41.366959: step 87340, total loss = 0.40, predict loss = 0.09 (58.5 examples/sec; 0.068 sec/batch; 112h:22m:44s remains)
INFO - root - 2019-11-04 00:48:42.000325: step 87350, total loss = 0.59, predict loss = 0.13 (77.7 examples/sec; 0.051 sec/batch; 84h:35m:00s remains)
INFO - root - 2019-11-04 00:48:42.612692: step 87360, total loss = 0.65, predict loss = 0.15 (80.3 examples/sec; 0.050 sec/batch; 81h:48m:47s remains)
INFO - root - 2019-11-04 00:48:43.335030: step 87370, total loss = 0.69, predict loss = 0.16 (64.4 examples/sec; 0.062 sec/batch; 102h:01m:53s remains)
INFO - root - 2019-11-04 00:48:44.082243: step 87380, total loss = 0.64, predict loss = 0.16 (61.4 examples/sec; 0.065 sec/batch; 106h:58m:06s remains)
INFO - root - 2019-11-04 00:48:44.888204: step 87390, total loss = 0.63, predict loss = 0.14 (47.1 examples/sec; 0.085 sec/batch; 139h:25m:03s remains)
INFO - root - 2019-11-04 00:48:45.685466: step 87400, total loss = 0.50, predict loss = 0.11 (61.8 examples/sec; 0.065 sec/batch; 106h:20m:05s remains)
INFO - root - 2019-11-04 00:48:46.310324: step 87410, total loss = 0.52, predict loss = 0.12 (79.1 examples/sec; 0.051 sec/batch; 83h:02m:07s remains)
INFO - root - 2019-11-04 00:48:46.937448: step 87420, total loss = 0.63, predict loss = 0.14 (77.2 examples/sec; 0.052 sec/batch; 85h:05m:35s remains)
INFO - root - 2019-11-04 00:48:47.579788: step 87430, total loss = 0.50, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 95h:16m:17s remains)
INFO - root - 2019-11-04 00:48:48.227818: step 87440, total loss = 0.45, predict loss = 0.10 (65.0 examples/sec; 0.062 sec/batch; 101h:06m:33s remains)
INFO - root - 2019-11-04 00:48:48.929012: step 87450, total loss = 0.46, predict loss = 0.10 (68.4 examples/sec; 0.058 sec/batch; 95h:58m:32s remains)
INFO - root - 2019-11-04 00:48:49.611779: step 87460, total loss = 0.50, predict loss = 0.12 (63.4 examples/sec; 0.063 sec/batch; 103h:41m:23s remains)
INFO - root - 2019-11-04 00:48:50.315801: step 87470, total loss = 0.39, predict loss = 0.09 (66.4 examples/sec; 0.060 sec/batch; 98h:56m:49s remains)
INFO - root - 2019-11-04 00:48:50.944173: step 87480, total loss = 0.37, predict loss = 0.08 (76.8 examples/sec; 0.052 sec/batch; 85h:31m:55s remains)
INFO - root - 2019-11-04 00:48:51.619462: step 87490, total loss = 0.41, predict loss = 0.08 (69.8 examples/sec; 0.057 sec/batch; 94h:03m:28s remains)
INFO - root - 2019-11-04 00:48:52.227981: step 87500, total loss = 0.65, predict loss = 0.15 (73.1 examples/sec; 0.055 sec/batch; 89h:54m:12s remains)
INFO - root - 2019-11-04 00:48:52.886144: step 87510, total loss = 0.54, predict loss = 0.12 (76.0 examples/sec; 0.053 sec/batch; 86h:24m:38s remains)
INFO - root - 2019-11-04 00:48:53.516839: step 87520, total loss = 0.56, predict loss = 0.12 (72.2 examples/sec; 0.055 sec/batch; 91h:02m:23s remains)
INFO - root - 2019-11-04 00:48:54.140130: step 87530, total loss = 0.53, predict loss = 0.13 (76.1 examples/sec; 0.053 sec/batch; 86h:18m:50s remains)
INFO - root - 2019-11-04 00:48:54.771850: step 87540, total loss = 0.67, predict loss = 0.16 (68.9 examples/sec; 0.058 sec/batch; 95h:24m:10s remains)
INFO - root - 2019-11-04 00:48:55.406083: step 87550, total loss = 0.63, predict loss = 0.14 (63.6 examples/sec; 0.063 sec/batch; 103h:22m:04s remains)
INFO - root - 2019-11-04 00:48:56.040616: step 87560, total loss = 0.56, predict loss = 0.13 (74.2 examples/sec; 0.054 sec/batch; 88h:35m:14s remains)
INFO - root - 2019-11-04 00:48:56.711635: step 87570, total loss = 0.58, predict loss = 0.13 (83.1 examples/sec; 0.048 sec/batch; 79h:02m:51s remains)
INFO - root - 2019-11-04 00:48:57.328435: step 87580, total loss = 0.65, predict loss = 0.15 (66.9 examples/sec; 0.060 sec/batch; 98h:12m:32s remains)
INFO - root - 2019-11-04 00:48:57.965535: step 87590, total loss = 0.80, predict loss = 0.19 (72.7 examples/sec; 0.055 sec/batch; 90h:23m:49s remains)
INFO - root - 2019-11-04 00:48:58.600376: step 87600, total loss = 0.57, predict loss = 0.12 (81.9 examples/sec; 0.049 sec/batch; 80h:11m:27s remains)
INFO - root - 2019-11-04 00:48:59.235358: step 87610, total loss = 0.54, predict loss = 0.13 (69.6 examples/sec; 0.057 sec/batch; 94h:23m:15s remains)
INFO - root - 2019-11-04 00:48:59.869727: step 87620, total loss = 0.58, predict loss = 0.13 (70.6 examples/sec; 0.057 sec/batch; 93h:04m:34s remains)
INFO - root - 2019-11-04 00:49:00.483729: step 87630, total loss = 0.48, predict loss = 0.11 (75.1 examples/sec; 0.053 sec/batch; 87h:28m:34s remains)
INFO - root - 2019-11-04 00:49:01.067505: step 87640, total loss = 0.50, predict loss = 0.11 (76.5 examples/sec; 0.052 sec/batch; 85h:52m:46s remains)
INFO - root - 2019-11-04 00:49:01.740261: step 87650, total loss = 0.54, predict loss = 0.12 (60.4 examples/sec; 0.066 sec/batch; 108h:44m:40s remains)
INFO - root - 2019-11-04 00:49:02.494905: step 87660, total loss = 0.53, predict loss = 0.12 (63.9 examples/sec; 0.063 sec/batch; 102h:43m:30s remains)
INFO - root - 2019-11-04 00:49:03.301779: step 87670, total loss = 0.53, predict loss = 0.12 (58.7 examples/sec; 0.068 sec/batch; 111h:58m:58s remains)
INFO - root - 2019-11-04 00:49:04.070561: step 87680, total loss = 0.47, predict loss = 0.11 (59.3 examples/sec; 0.067 sec/batch; 110h:47m:25s remains)
INFO - root - 2019-11-04 00:49:04.893698: step 87690, total loss = 0.56, predict loss = 0.13 (61.0 examples/sec; 0.066 sec/batch; 107h:40m:44s remains)
INFO - root - 2019-11-04 00:49:05.557132: step 87700, total loss = 0.56, predict loss = 0.13 (75.1 examples/sec; 0.053 sec/batch; 87h:28m:39s remains)
INFO - root - 2019-11-04 00:49:06.146679: step 87710, total loss = 0.56, predict loss = 0.13 (74.8 examples/sec; 0.053 sec/batch; 87h:47m:27s remains)
INFO - root - 2019-11-04 00:49:06.801764: step 87720, total loss = 0.31, predict loss = 0.06 (66.2 examples/sec; 0.060 sec/batch; 99h:11m:07s remains)
INFO - root - 2019-11-04 00:49:07.430330: step 87730, total loss = 0.55, predict loss = 0.13 (81.7 examples/sec; 0.049 sec/batch; 80h:26m:49s remains)
INFO - root - 2019-11-04 00:49:08.095011: step 87740, total loss = 0.38, predict loss = 0.08 (54.8 examples/sec; 0.073 sec/batch; 119h:51m:40s remains)
INFO - root - 2019-11-04 00:49:08.731615: step 87750, total loss = 0.40, predict loss = 0.10 (78.1 examples/sec; 0.051 sec/batch; 84h:06m:31s remains)
INFO - root - 2019-11-04 00:49:09.354519: step 87760, total loss = 0.66, predict loss = 0.17 (72.8 examples/sec; 0.055 sec/batch; 90h:12m:19s remains)
INFO - root - 2019-11-04 00:49:09.968290: step 87770, total loss = 0.58, predict loss = 0.14 (70.0 examples/sec; 0.057 sec/batch; 93h:50m:49s remains)
INFO - root - 2019-11-04 00:49:10.577181: step 87780, total loss = 0.49, predict loss = 0.12 (75.5 examples/sec; 0.053 sec/batch; 87h:00m:34s remains)
INFO - root - 2019-11-04 00:49:11.193621: step 87790, total loss = 0.56, predict loss = 0.12 (69.1 examples/sec; 0.058 sec/batch; 95h:03m:18s remains)
INFO - root - 2019-11-04 00:49:11.840696: step 87800, total loss = 0.58, predict loss = 0.14 (73.3 examples/sec; 0.055 sec/batch; 89h:39m:47s remains)
INFO - root - 2019-11-04 00:49:12.477611: step 87810, total loss = 0.45, predict loss = 0.10 (63.1 examples/sec; 0.063 sec/batch; 104h:03m:17s remains)
INFO - root - 2019-11-04 00:49:13.124401: step 87820, total loss = 0.56, predict loss = 0.13 (70.1 examples/sec; 0.057 sec/batch; 93h:43m:50s remains)
INFO - root - 2019-11-04 00:49:13.774874: step 87830, total loss = 0.40, predict loss = 0.09 (66.9 examples/sec; 0.060 sec/batch; 98h:07m:24s remains)
INFO - root - 2019-11-04 00:49:14.372309: step 87840, total loss = 0.40, predict loss = 0.10 (78.1 examples/sec; 0.051 sec/batch; 84h:08m:25s remains)
INFO - root - 2019-11-04 00:49:14.998014: step 87850, total loss = 0.37, predict loss = 0.09 (67.6 examples/sec; 0.059 sec/batch; 97h:12m:51s remains)
INFO - root - 2019-11-04 00:49:15.691179: step 87860, total loss = 0.36, predict loss = 0.09 (73.9 examples/sec; 0.054 sec/batch; 88h:56m:29s remains)
INFO - root - 2019-11-04 00:49:16.345587: step 87870, total loss = 0.27, predict loss = 0.06 (77.0 examples/sec; 0.052 sec/batch; 85h:20m:18s remains)
INFO - root - 2019-11-04 00:49:16.975482: step 87880, total loss = 0.36, predict loss = 0.08 (65.0 examples/sec; 0.062 sec/batch; 101h:03m:00s remains)
INFO - root - 2019-11-04 00:49:17.658748: step 87890, total loss = 0.39, predict loss = 0.09 (65.6 examples/sec; 0.061 sec/batch; 100h:06m:25s remains)
INFO - root - 2019-11-04 00:49:18.336734: step 87900, total loss = 0.45, predict loss = 0.10 (67.2 examples/sec; 0.059 sec/batch; 97h:41m:57s remains)
INFO - root - 2019-11-04 00:49:18.973574: step 87910, total loss = 0.32, predict loss = 0.07 (75.3 examples/sec; 0.053 sec/batch; 87h:13m:31s remains)
INFO - root - 2019-11-04 00:49:19.594011: step 87920, total loss = 0.40, predict loss = 0.08 (80.1 examples/sec; 0.050 sec/batch; 81h:57m:57s remains)
INFO - root - 2019-11-04 00:49:20.269342: step 87930, total loss = 0.35, predict loss = 0.08 (63.6 examples/sec; 0.063 sec/batch; 103h:15m:32s remains)
INFO - root - 2019-11-04 00:49:20.938672: step 87940, total loss = 0.58, predict loss = 0.13 (69.9 examples/sec; 0.057 sec/batch; 93h:54m:49s remains)
INFO - root - 2019-11-04 00:49:21.606872: step 87950, total loss = 0.54, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 92h:08m:38s remains)
INFO - root - 2019-11-04 00:49:22.256520: step 87960, total loss = 0.60, predict loss = 0.14 (72.7 examples/sec; 0.055 sec/batch; 90h:23m:02s remains)
INFO - root - 2019-11-04 00:49:22.875447: step 87970, total loss = 0.45, predict loss = 0.10 (71.1 examples/sec; 0.056 sec/batch; 92h:25m:56s remains)
INFO - root - 2019-11-04 00:49:23.507666: step 87980, total loss = 0.68, predict loss = 0.16 (66.2 examples/sec; 0.060 sec/batch; 99h:12m:29s remains)
INFO - root - 2019-11-04 00:49:24.125765: step 87990, total loss = 0.70, predict loss = 0.16 (77.7 examples/sec; 0.052 sec/batch; 84h:34m:37s remains)
INFO - root - 2019-11-04 00:49:24.776001: step 88000, total loss = 0.52, predict loss = 0.12 (63.6 examples/sec; 0.063 sec/batch; 103h:16m:31s remains)
INFO - root - 2019-11-04 00:49:25.430593: step 88010, total loss = 0.48, predict loss = 0.11 (65.0 examples/sec; 0.062 sec/batch; 101h:04m:43s remains)
INFO - root - 2019-11-04 00:49:26.057956: step 88020, total loss = 0.58, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 90h:57m:53s remains)
INFO - root - 2019-11-04 00:49:26.647055: step 88030, total loss = 0.53, predict loss = 0.11 (79.5 examples/sec; 0.050 sec/batch; 82h:38m:19s remains)
INFO - root - 2019-11-04 00:49:27.323361: step 88040, total loss = 0.46, predict loss = 0.12 (58.1 examples/sec; 0.069 sec/batch; 113h:08m:13s remains)
INFO - root - 2019-11-04 00:49:27.951226: step 88050, total loss = 0.39, predict loss = 0.09 (68.0 examples/sec; 0.059 sec/batch; 96h:36m:27s remains)
INFO - root - 2019-11-04 00:49:28.607176: step 88060, total loss = 0.52, predict loss = 0.12 (64.0 examples/sec; 0.062 sec/batch; 102h:35m:41s remains)
INFO - root - 2019-11-04 00:49:29.309106: step 88070, total loss = 0.65, predict loss = 0.16 (65.0 examples/sec; 0.061 sec/batch; 100h:59m:07s remains)
INFO - root - 2019-11-04 00:49:30.022674: step 88080, total loss = 0.49, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 103h:25m:42s remains)
INFO - root - 2019-11-04 00:49:30.663995: step 88090, total loss = 0.54, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 100h:51m:43s remains)
INFO - root - 2019-11-04 00:49:31.277332: step 88100, total loss = 0.22, predict loss = 0.04 (68.8 examples/sec; 0.058 sec/batch; 95h:29m:40s remains)
INFO - root - 2019-11-04 00:49:31.879437: step 88110, total loss = 0.25, predict loss = 0.05 (74.3 examples/sec; 0.054 sec/batch; 88h:21m:25s remains)
INFO - root - 2019-11-04 00:49:32.520200: step 88120, total loss = 0.25, predict loss = 0.05 (78.3 examples/sec; 0.051 sec/batch; 83h:55m:38s remains)
INFO - root - 2019-11-04 00:49:33.163999: step 88130, total loss = 0.29, predict loss = 0.06 (77.5 examples/sec; 0.052 sec/batch; 84h:45m:28s remains)
INFO - root - 2019-11-04 00:49:33.784950: step 88140, total loss = 0.36, predict loss = 0.08 (77.7 examples/sec; 0.051 sec/batch; 84h:29m:53s remains)
INFO - root - 2019-11-04 00:49:34.416884: step 88150, total loss = 0.35, predict loss = 0.08 (59.5 examples/sec; 0.067 sec/batch; 110h:19m:48s remains)
INFO - root - 2019-11-04 00:49:35.075589: step 88160, total loss = 0.27, predict loss = 0.06 (71.4 examples/sec; 0.056 sec/batch; 92h:03m:41s remains)
INFO - root - 2019-11-04 00:49:35.693225: step 88170, total loss = 0.38, predict loss = 0.08 (71.7 examples/sec; 0.056 sec/batch; 91h:35m:16s remains)
INFO - root - 2019-11-04 00:49:36.356930: step 88180, total loss = 0.47, predict loss = 0.11 (65.1 examples/sec; 0.061 sec/batch; 100h:58m:33s remains)
INFO - root - 2019-11-04 00:49:36.995868: step 88190, total loss = 0.39, predict loss = 0.09 (67.2 examples/sec; 0.060 sec/batch; 97h:46m:40s remains)
INFO - root - 2019-11-04 00:49:37.614223: step 88200, total loss = 0.37, predict loss = 0.08 (73.0 examples/sec; 0.055 sec/batch; 89h:58m:00s remains)
INFO - root - 2019-11-04 00:49:38.238349: step 88210, total loss = 0.34, predict loss = 0.07 (71.5 examples/sec; 0.056 sec/batch; 91h:51m:50s remains)
INFO - root - 2019-11-04 00:49:38.860576: step 88220, total loss = 0.39, predict loss = 0.08 (70.1 examples/sec; 0.057 sec/batch; 93h:39m:07s remains)
INFO - root - 2019-11-04 00:49:39.470880: step 88230, total loss = 0.70, predict loss = 0.18 (75.5 examples/sec; 0.053 sec/batch; 86h:58m:50s remains)
INFO - root - 2019-11-04 00:49:40.127705: step 88240, total loss = 0.57, predict loss = 0.14 (67.2 examples/sec; 0.059 sec/batch; 97h:42m:08s remains)
INFO - root - 2019-11-04 00:49:40.760917: step 88250, total loss = 0.31, predict loss = 0.07 (67.8 examples/sec; 0.059 sec/batch; 96h:51m:47s remains)
INFO - root - 2019-11-04 00:49:41.389859: step 88260, total loss = 0.33, predict loss = 0.07 (66.8 examples/sec; 0.060 sec/batch; 98h:23m:33s remains)
INFO - root - 2019-11-04 00:49:42.023985: step 88270, total loss = 0.53, predict loss = 0.12 (75.5 examples/sec; 0.053 sec/batch; 87h:02m:24s remains)
INFO - root - 2019-11-04 00:49:42.659555: step 88280, total loss = 0.48, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 89h:57m:50s remains)
INFO - root - 2019-11-04 00:49:43.280043: step 88290, total loss = 0.63, predict loss = 0.16 (80.7 examples/sec; 0.050 sec/batch; 81h:20m:45s remains)
INFO - root - 2019-11-04 00:49:43.908493: step 88300, total loss = 0.39, predict loss = 0.09 (70.2 examples/sec; 0.057 sec/batch; 93h:37m:21s remains)
INFO - root - 2019-11-04 00:49:44.556044: step 88310, total loss = 0.37, predict loss = 0.07 (69.8 examples/sec; 0.057 sec/batch; 94h:08m:40s remains)
INFO - root - 2019-11-04 00:49:45.233503: step 88320, total loss = 0.43, predict loss = 0.09 (67.0 examples/sec; 0.060 sec/batch; 98h:02m:14s remains)
INFO - root - 2019-11-04 00:49:45.874339: step 88330, total loss = 0.36, predict loss = 0.09 (73.4 examples/sec; 0.055 sec/batch; 89h:32m:49s remains)
INFO - root - 2019-11-04 00:49:46.515933: step 88340, total loss = 0.41, predict loss = 0.10 (78.1 examples/sec; 0.051 sec/batch; 84h:03m:00s remains)
INFO - root - 2019-11-04 00:49:47.186624: step 88350, total loss = 0.34, predict loss = 0.08 (66.5 examples/sec; 0.060 sec/batch; 98h:47m:01s remains)
INFO - root - 2019-11-04 00:49:47.845461: step 88360, total loss = 0.44, predict loss = 0.10 (68.2 examples/sec; 0.059 sec/batch; 96h:16m:18s remains)
INFO - root - 2019-11-04 00:49:48.504227: step 88370, total loss = 0.20, predict loss = 0.04 (63.6 examples/sec; 0.063 sec/batch; 103h:14m:45s remains)
INFO - root - 2019-11-04 00:49:49.138356: step 88380, total loss = 0.40, predict loss = 0.09 (75.0 examples/sec; 0.053 sec/batch; 87h:38m:04s remains)
INFO - root - 2019-11-04 00:49:49.796787: step 88390, total loss = 0.30, predict loss = 0.07 (66.1 examples/sec; 0.061 sec/batch; 99h:25m:12s remains)
INFO - root - 2019-11-04 00:49:50.436212: step 88400, total loss = 0.41, predict loss = 0.09 (66.1 examples/sec; 0.061 sec/batch; 99h:26m:19s remains)
INFO - root - 2019-11-04 00:49:51.066033: step 88410, total loss = 0.43, predict loss = 0.11 (76.8 examples/sec; 0.052 sec/batch; 85h:32m:59s remains)
INFO - root - 2019-11-04 00:49:51.668518: step 88420, total loss = 0.41, predict loss = 0.09 (67.8 examples/sec; 0.059 sec/batch; 96h:56m:00s remains)
INFO - root - 2019-11-04 00:49:52.294099: step 88430, total loss = 0.48, predict loss = 0.11 (64.3 examples/sec; 0.062 sec/batch; 102h:10m:39s remains)
INFO - root - 2019-11-04 00:49:52.985120: step 88440, total loss = 0.59, predict loss = 0.14 (64.8 examples/sec; 0.062 sec/batch; 101h:24m:40s remains)
INFO - root - 2019-11-04 00:49:53.647831: step 88450, total loss = 0.36, predict loss = 0.08 (76.4 examples/sec; 0.052 sec/batch; 85h:59m:52s remains)
INFO - root - 2019-11-04 00:49:54.256527: step 88460, total loss = 0.42, predict loss = 0.10 (83.2 examples/sec; 0.048 sec/batch; 78h:56m:30s remains)
INFO - root - 2019-11-04 00:49:54.899752: step 88470, total loss = 0.44, predict loss = 0.11 (65.9 examples/sec; 0.061 sec/batch; 99h:37m:43s remains)
INFO - root - 2019-11-04 00:49:55.558411: step 88480, total loss = 0.42, predict loss = 0.10 (67.9 examples/sec; 0.059 sec/batch; 96h:42m:21s remains)
INFO - root - 2019-11-04 00:49:56.248616: step 88490, total loss = 0.32, predict loss = 0.07 (60.4 examples/sec; 0.066 sec/batch; 108h:43m:15s remains)
INFO - root - 2019-11-04 00:49:56.888120: step 88500, total loss = 0.48, predict loss = 0.11 (73.1 examples/sec; 0.055 sec/batch; 89h:52m:01s remains)
INFO - root - 2019-11-04 00:49:57.525153: step 88510, total loss = 0.24, predict loss = 0.05 (65.4 examples/sec; 0.061 sec/batch; 100h:28m:22s remains)
INFO - root - 2019-11-04 00:49:58.147640: step 88520, total loss = 0.50, predict loss = 0.11 (65.6 examples/sec; 0.061 sec/batch; 100h:05m:31s remains)
INFO - root - 2019-11-04 00:49:58.769538: step 88530, total loss = 0.39, predict loss = 0.09 (82.1 examples/sec; 0.049 sec/batch; 79h:57m:57s remains)
INFO - root - 2019-11-04 00:49:59.420073: step 88540, total loss = 0.55, predict loss = 0.13 (65.7 examples/sec; 0.061 sec/batch; 99h:56m:29s remains)
INFO - root - 2019-11-04 00:50:00.095978: step 88550, total loss = 0.39, predict loss = 0.09 (71.5 examples/sec; 0.056 sec/batch; 91h:49m:06s remains)
INFO - root - 2019-11-04 00:50:00.690140: step 88560, total loss = 0.42, predict loss = 0.10 (81.5 examples/sec; 0.049 sec/batch; 80h:35m:18s remains)
INFO - root - 2019-11-04 00:50:01.320848: step 88570, total loss = 0.64, predict loss = 0.15 (64.3 examples/sec; 0.062 sec/batch; 102h:08m:19s remains)
INFO - root - 2019-11-04 00:50:01.980951: step 88580, total loss = 0.41, predict loss = 0.08 (65.4 examples/sec; 0.061 sec/batch; 100h:26m:09s remains)
INFO - root - 2019-11-04 00:50:02.666616: step 88590, total loss = 0.51, predict loss = 0.12 (59.3 examples/sec; 0.067 sec/batch; 110h:41m:29s remains)
INFO - root - 2019-11-04 00:50:03.311488: step 88600, total loss = 0.60, predict loss = 0.14 (65.7 examples/sec; 0.061 sec/batch; 99h:59m:40s remains)
INFO - root - 2019-11-04 00:50:03.968057: step 88610, total loss = 0.66, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 95h:40m:19s remains)
INFO - root - 2019-11-04 00:50:04.661244: step 88620, total loss = 0.18, predict loss = 0.03 (62.8 examples/sec; 0.064 sec/batch; 104h:32m:48s remains)
INFO - root - 2019-11-04 00:50:05.268414: step 88630, total loss = 0.45, predict loss = 0.11 (76.1 examples/sec; 0.053 sec/batch; 86h:21m:26s remains)
INFO - root - 2019-11-04 00:50:05.931451: step 88640, total loss = 0.35, predict loss = 0.07 (66.0 examples/sec; 0.061 sec/batch; 99h:29m:02s remains)
INFO - root - 2019-11-04 00:50:06.628903: step 88650, total loss = 0.48, predict loss = 0.11 (66.2 examples/sec; 0.060 sec/batch; 99h:16m:57s remains)
INFO - root - 2019-11-04 00:50:07.321053: step 88660, total loss = 0.44, predict loss = 0.10 (62.1 examples/sec; 0.064 sec/batch; 105h:41m:34s remains)
INFO - root - 2019-11-04 00:50:08.054760: step 88670, total loss = 0.54, predict loss = 0.13 (63.1 examples/sec; 0.063 sec/batch; 104h:00m:55s remains)
INFO - root - 2019-11-04 00:50:08.660031: step 88680, total loss = 0.62, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 90h:59m:35s remains)
INFO - root - 2019-11-04 00:50:09.257507: step 88690, total loss = 0.62, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 94h:43m:03s remains)
INFO - root - 2019-11-04 00:50:09.902262: step 88700, total loss = 0.75, predict loss = 0.18 (70.2 examples/sec; 0.057 sec/batch; 93h:37m:33s remains)
INFO - root - 2019-11-04 00:50:10.556933: step 88710, total loss = 0.66, predict loss = 0.16 (77.7 examples/sec; 0.051 sec/batch; 84h:29m:43s remains)
INFO - root - 2019-11-04 00:50:11.216307: step 88720, total loss = 0.83, predict loss = 0.20 (72.3 examples/sec; 0.055 sec/batch; 90h:54m:24s remains)
INFO - root - 2019-11-04 00:50:11.842227: step 88730, total loss = 0.79, predict loss = 0.19 (66.6 examples/sec; 0.060 sec/batch; 98h:38m:56s remains)
INFO - root - 2019-11-04 00:50:12.473337: step 88740, total loss = 0.63, predict loss = 0.16 (65.9 examples/sec; 0.061 sec/batch; 99h:37m:14s remains)
INFO - root - 2019-11-04 00:50:13.114704: step 88750, total loss = 0.71, predict loss = 0.17 (71.6 examples/sec; 0.056 sec/batch; 91h:42m:15s remains)
INFO - root - 2019-11-04 00:50:13.744630: step 88760, total loss = 0.65, predict loss = 0.15 (65.5 examples/sec; 0.061 sec/batch; 100h:19m:55s remains)
INFO - root - 2019-11-04 00:50:14.394645: step 88770, total loss = 0.59, predict loss = 0.14 (64.9 examples/sec; 0.062 sec/batch; 101h:12m:51s remains)
INFO - root - 2019-11-04 00:50:15.006904: step 88780, total loss = 0.58, predict loss = 0.14 (75.5 examples/sec; 0.053 sec/batch; 87h:02m:25s remains)
INFO - root - 2019-11-04 00:50:15.619458: step 88790, total loss = 0.59, predict loss = 0.14 (66.9 examples/sec; 0.060 sec/batch; 98h:14m:16s remains)
INFO - root - 2019-11-04 00:50:16.217951: step 88800, total loss = 0.56, predict loss = 0.13 (71.3 examples/sec; 0.056 sec/batch; 92h:08m:02s remains)
INFO - root - 2019-11-04 00:50:16.818646: step 88810, total loss = 0.54, predict loss = 0.12 (66.0 examples/sec; 0.061 sec/batch; 99h:31m:45s remains)
INFO - root - 2019-11-04 00:50:17.445120: step 88820, total loss = 0.45, predict loss = 0.10 (80.3 examples/sec; 0.050 sec/batch; 81h:47m:08s remains)
INFO - root - 2019-11-04 00:50:18.063305: step 88830, total loss = 0.51, predict loss = 0.12 (85.9 examples/sec; 0.047 sec/batch; 76h:26m:51s remains)
INFO - root - 2019-11-04 00:50:18.635099: step 88840, total loss = 0.53, predict loss = 0.12 (80.4 examples/sec; 0.050 sec/batch; 81h:40m:04s remains)
INFO - root - 2019-11-04 00:50:19.237667: step 88850, total loss = 0.55, predict loss = 0.12 (77.7 examples/sec; 0.052 sec/batch; 84h:34m:18s remains)
INFO - root - 2019-11-04 00:50:19.862699: step 88860, total loss = 0.60, predict loss = 0.14 (69.5 examples/sec; 0.058 sec/batch; 94h:27m:23s remains)
INFO - root - 2019-11-04 00:50:20.490499: step 88870, total loss = 0.63, predict loss = 0.14 (67.8 examples/sec; 0.059 sec/batch; 96h:50m:22s remains)
INFO - root - 2019-11-04 00:50:21.128458: step 88880, total loss = 0.46, predict loss = 0.10 (74.5 examples/sec; 0.054 sec/batch; 88h:08m:28s remains)
INFO - root - 2019-11-04 00:50:21.804613: step 88890, total loss = 0.45, predict loss = 0.10 (63.2 examples/sec; 0.063 sec/batch; 103h:52m:11s remains)
INFO - root - 2019-11-04 00:50:22.488204: step 88900, total loss = 0.52, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 95h:14m:42s remains)
INFO - root - 2019-11-04 00:50:23.179981: step 88910, total loss = 0.55, predict loss = 0.13 (60.4 examples/sec; 0.066 sec/batch; 108h:40m:28s remains)
INFO - root - 2019-11-04 00:50:23.849573: step 88920, total loss = 0.49, predict loss = 0.11 (72.1 examples/sec; 0.056 sec/batch; 91h:07m:59s remains)
INFO - root - 2019-11-04 00:50:24.510562: step 88930, total loss = 0.43, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 97h:39m:45s remains)
INFO - root - 2019-11-04 00:50:25.129295: step 88940, total loss = 0.41, predict loss = 0.09 (74.6 examples/sec; 0.054 sec/batch; 87h:59m:46s remains)
INFO - root - 2019-11-04 00:50:25.747559: step 88950, total loss = 0.48, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 88h:55m:40s remains)
INFO - root - 2019-11-04 00:50:26.396387: step 88960, total loss = 0.42, predict loss = 0.09 (66.1 examples/sec; 0.060 sec/batch; 99h:19m:25s remains)
INFO - root - 2019-11-04 00:50:27.068106: step 88970, total loss = 0.42, predict loss = 0.10 (73.2 examples/sec; 0.055 sec/batch; 89h:40m:04s remains)
INFO - root - 2019-11-04 00:50:27.724812: step 88980, total loss = 0.39, predict loss = 0.09 (63.0 examples/sec; 0.064 sec/batch; 104h:18m:39s remains)
INFO - root - 2019-11-04 00:50:28.371136: step 88990, total loss = 0.38, predict loss = 0.08 (65.6 examples/sec; 0.061 sec/batch; 100h:08m:16s remains)
INFO - root - 2019-11-04 00:50:29.024331: step 89000, total loss = 0.32, predict loss = 0.06 (62.7 examples/sec; 0.064 sec/batch; 104h:47m:32s remains)
INFO - root - 2019-11-04 00:50:29.675578: step 89010, total loss = 0.31, predict loss = 0.07 (82.8 examples/sec; 0.048 sec/batch; 79h:19m:41s remains)
INFO - root - 2019-11-04 00:50:30.313593: step 89020, total loss = 0.42, predict loss = 0.09 (71.4 examples/sec; 0.056 sec/batch; 91h:59m:35s remains)
INFO - root - 2019-11-04 00:50:30.961606: step 89030, total loss = 0.37, predict loss = 0.08 (65.1 examples/sec; 0.061 sec/batch; 100h:52m:55s remains)
INFO - root - 2019-11-04 00:50:32.103593: step 89040, total loss = 0.36, predict loss = 0.07 (75.3 examples/sec; 0.053 sec/batch; 87h:13m:02s remains)
INFO - root - 2019-11-04 00:50:32.736896: step 89050, total loss = 0.46, predict loss = 0.12 (66.0 examples/sec; 0.061 sec/batch; 99h:26m:52s remains)
INFO - root - 2019-11-04 00:50:33.378137: step 89060, total loss = 0.43, predict loss = 0.10 (72.6 examples/sec; 0.055 sec/batch; 90h:28m:08s remains)
INFO - root - 2019-11-04 00:50:34.017233: step 89070, total loss = 0.36, predict loss = 0.08 (74.2 examples/sec; 0.054 sec/batch; 88h:33m:00s remains)
INFO - root - 2019-11-04 00:50:34.710610: step 89080, total loss = 0.55, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 93h:12m:20s remains)
INFO - root - 2019-11-04 00:50:35.350160: step 89090, total loss = 0.52, predict loss = 0.13 (62.0 examples/sec; 0.065 sec/batch; 105h:56m:18s remains)
INFO - root - 2019-11-04 00:50:35.989508: step 89100, total loss = 0.50, predict loss = 0.11 (74.1 examples/sec; 0.054 sec/batch; 88h:39m:52s remains)
INFO - root - 2019-11-04 00:50:36.676105: step 89110, total loss = 0.56, predict loss = 0.13 (62.2 examples/sec; 0.064 sec/batch; 105h:38m:38s remains)
INFO - root - 2019-11-04 00:50:37.362669: step 89120, total loss = 0.74, predict loss = 0.17 (64.4 examples/sec; 0.062 sec/batch; 101h:55m:38s remains)
INFO - root - 2019-11-04 00:50:37.997425: step 89130, total loss = 0.62, predict loss = 0.14 (67.4 examples/sec; 0.059 sec/batch; 97h:23m:52s remains)
INFO - root - 2019-11-04 00:50:38.713635: step 89140, total loss = 0.68, predict loss = 0.16 (58.3 examples/sec; 0.069 sec/batch; 112h:34m:40s remains)
INFO - root - 2019-11-04 00:50:39.336375: step 89150, total loss = 0.53, predict loss = 0.13 (75.2 examples/sec; 0.053 sec/batch; 87h:17m:25s remains)
INFO - root - 2019-11-04 00:50:39.947331: step 89160, total loss = 0.74, predict loss = 0.18 (76.5 examples/sec; 0.052 sec/batch; 85h:54m:05s remains)
INFO - root - 2019-11-04 00:50:40.564515: step 89170, total loss = 0.75, predict loss = 0.19 (76.8 examples/sec; 0.052 sec/batch; 85h:33m:23s remains)
INFO - root - 2019-11-04 00:50:41.210817: step 89180, total loss = 0.67, predict loss = 0.16 (77.2 examples/sec; 0.052 sec/batch; 85h:04m:22s remains)
INFO - root - 2019-11-04 00:50:41.896587: step 89190, total loss = 0.56, predict loss = 0.14 (67.9 examples/sec; 0.059 sec/batch; 96h:41m:09s remains)
INFO - root - 2019-11-04 00:50:42.555388: step 89200, total loss = 0.71, predict loss = 0.17 (70.5 examples/sec; 0.057 sec/batch; 93h:07m:15s remains)
INFO - root - 2019-11-04 00:50:43.245321: step 89210, total loss = 0.51, predict loss = 0.11 (64.9 examples/sec; 0.062 sec/batch; 101h:10m:41s remains)
INFO - root - 2019-11-04 00:50:43.899378: step 89220, total loss = 0.50, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 103h:20m:49s remains)
INFO - root - 2019-11-04 00:50:44.538622: step 89230, total loss = 0.59, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 94h:45m:48s remains)
INFO - root - 2019-11-04 00:50:45.175789: step 89240, total loss = 0.43, predict loss = 0.10 (74.0 examples/sec; 0.054 sec/batch; 88h:45m:13s remains)
INFO - root - 2019-11-04 00:50:45.810331: step 89250, total loss = 0.31, predict loss = 0.07 (70.1 examples/sec; 0.057 sec/batch; 93h:42m:16s remains)
INFO - root - 2019-11-04 00:50:46.437063: step 89260, total loss = 0.50, predict loss = 0.12 (82.2 examples/sec; 0.049 sec/batch; 79h:54m:05s remains)
INFO - root - 2019-11-04 00:50:47.054808: step 89270, total loss = 0.43, predict loss = 0.09 (79.0 examples/sec; 0.051 sec/batch; 83h:09m:26s remains)
INFO - root - 2019-11-04 00:50:47.707383: step 89280, total loss = 0.49, predict loss = 0.11 (59.1 examples/sec; 0.068 sec/batch; 111h:05m:18s remains)
INFO - root - 2019-11-04 00:50:48.331398: step 89290, total loss = 0.51, predict loss = 0.12 (75.7 examples/sec; 0.053 sec/batch; 86h:48m:19s remains)
INFO - root - 2019-11-04 00:50:48.989642: step 89300, total loss = 0.45, predict loss = 0.10 (67.0 examples/sec; 0.060 sec/batch; 98h:00m:54s remains)
INFO - root - 2019-11-04 00:50:49.675412: step 89310, total loss = 0.49, predict loss = 0.12 (65.7 examples/sec; 0.061 sec/batch; 99h:53m:59s remains)
INFO - root - 2019-11-04 00:50:50.339016: step 89320, total loss = 0.40, predict loss = 0.09 (68.0 examples/sec; 0.059 sec/batch; 96h:32m:37s remains)
INFO - root - 2019-11-04 00:50:51.054970: step 89330, total loss = 0.40, predict loss = 0.09 (50.2 examples/sec; 0.080 sec/batch; 130h:44m:58s remains)
INFO - root - 2019-11-04 00:50:51.671603: step 89340, total loss = 0.55, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 92h:00m:47s remains)
INFO - root - 2019-11-04 00:50:52.307253: step 89350, total loss = 0.65, predict loss = 0.15 (69.2 examples/sec; 0.058 sec/batch; 94h:52m:13s remains)
INFO - root - 2019-11-04 00:50:52.950286: step 89360, total loss = 0.59, predict loss = 0.14 (62.8 examples/sec; 0.064 sec/batch; 104h:38m:33s remains)
INFO - root - 2019-11-04 00:50:53.626880: step 89370, total loss = 0.53, predict loss = 0.12 (63.2 examples/sec; 0.063 sec/batch; 103h:51m:18s remains)
INFO - root - 2019-11-04 00:50:54.302843: step 89380, total loss = 0.63, predict loss = 0.16 (66.6 examples/sec; 0.060 sec/batch; 98h:40m:10s remains)
INFO - root - 2019-11-04 00:50:54.950674: step 89390, total loss = 0.51, predict loss = 0.12 (80.2 examples/sec; 0.050 sec/batch; 81h:56m:13s remains)
INFO - root - 2019-11-04 00:50:55.619197: step 89400, total loss = 0.72, predict loss = 0.19 (58.5 examples/sec; 0.068 sec/batch; 112h:13m:18s remains)
INFO - root - 2019-11-04 00:50:56.293853: step 89410, total loss = 0.65, predict loss = 0.15 (68.1 examples/sec; 0.059 sec/batch; 96h:22m:37s remains)
INFO - root - 2019-11-04 00:50:56.970714: step 89420, total loss = 0.59, predict loss = 0.14 (64.5 examples/sec; 0.062 sec/batch; 101h:50m:05s remains)
INFO - root - 2019-11-04 00:50:57.641061: step 89430, total loss = 0.59, predict loss = 0.15 (63.5 examples/sec; 0.063 sec/batch; 103h:20m:31s remains)
INFO - root - 2019-11-04 00:50:58.321652: step 89440, total loss = 0.53, predict loss = 0.13 (59.7 examples/sec; 0.067 sec/batch; 109h:59m:33s remains)
INFO - root - 2019-11-04 00:50:59.017237: step 89450, total loss = 0.59, predict loss = 0.14 (69.2 examples/sec; 0.058 sec/batch; 94h:56m:47s remains)
INFO - root - 2019-11-04 00:50:59.659872: step 89460, total loss = 0.50, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 90h:15m:49s remains)
INFO - root - 2019-11-04 00:51:00.322381: step 89470, total loss = 0.56, predict loss = 0.13 (64.1 examples/sec; 0.062 sec/batch; 102h:24m:22s remains)
INFO - root - 2019-11-04 00:51:00.970039: step 89480, total loss = 0.51, predict loss = 0.12 (73.3 examples/sec; 0.055 sec/batch; 89h:37m:29s remains)
INFO - root - 2019-11-04 00:51:01.622649: step 89490, total loss = 0.51, predict loss = 0.12 (61.0 examples/sec; 0.066 sec/batch; 107h:39m:50s remains)
INFO - root - 2019-11-04 00:51:02.249068: step 89500, total loss = 0.55, predict loss = 0.12 (87.3 examples/sec; 0.046 sec/batch; 75h:15m:19s remains)
INFO - root - 2019-11-04 00:51:02.888750: step 89510, total loss = 0.51, predict loss = 0.12 (67.2 examples/sec; 0.060 sec/batch; 97h:46m:38s remains)
INFO - root - 2019-11-04 00:51:03.520317: step 89520, total loss = 0.59, predict loss = 0.14 (74.8 examples/sec; 0.053 sec/batch; 87h:47m:50s remains)
INFO - root - 2019-11-04 00:51:04.151936: step 89530, total loss = 0.42, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 99h:43m:12s remains)
INFO - root - 2019-11-04 00:51:04.830282: step 89540, total loss = 0.40, predict loss = 0.09 (71.5 examples/sec; 0.056 sec/batch; 91h:50m:50s remains)
INFO - root - 2019-11-04 00:51:05.434493: step 89550, total loss = 0.40, predict loss = 0.09 (70.5 examples/sec; 0.057 sec/batch; 93h:12m:15s remains)
INFO - root - 2019-11-04 00:51:06.046751: step 89560, total loss = 0.39, predict loss = 0.08 (67.4 examples/sec; 0.059 sec/batch; 97h:30m:08s remains)
INFO - root - 2019-11-04 00:51:06.643462: step 89570, total loss = 0.45, predict loss = 0.10 (73.3 examples/sec; 0.055 sec/batch; 89h:38m:09s remains)
INFO - root - 2019-11-04 00:51:07.318561: step 89580, total loss = 0.52, predict loss = 0.12 (65.2 examples/sec; 0.061 sec/batch; 100h:44m:39s remains)
INFO - root - 2019-11-04 00:51:07.939020: step 89590, total loss = 0.56, predict loss = 0.13 (69.6 examples/sec; 0.057 sec/batch; 94h:20m:09s remains)
INFO - root - 2019-11-04 00:51:08.572835: step 89600, total loss = 0.39, predict loss = 0.09 (66.1 examples/sec; 0.060 sec/batch; 99h:19m:38s remains)
INFO - root - 2019-11-04 00:51:09.209880: step 89610, total loss = 0.52, predict loss = 0.13 (74.2 examples/sec; 0.054 sec/batch; 88h:33m:40s remains)
INFO - root - 2019-11-04 00:51:09.850951: step 89620, total loss = 0.61, predict loss = 0.14 (65.3 examples/sec; 0.061 sec/batch; 100h:36m:38s remains)
INFO - root - 2019-11-04 00:51:10.499924: step 89630, total loss = 0.51, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 92h:06m:33s remains)
INFO - root - 2019-11-04 00:51:11.190486: step 89640, total loss = 0.48, predict loss = 0.10 (61.5 examples/sec; 0.065 sec/batch; 106h:43m:13s remains)
INFO - root - 2019-11-04 00:51:11.915632: step 89650, total loss = 0.51, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 98h:07m:09s remains)
INFO - root - 2019-11-04 00:51:12.535962: step 89660, total loss = 0.52, predict loss = 0.12 (67.7 examples/sec; 0.059 sec/batch; 97h:00m:59s remains)
INFO - root - 2019-11-04 00:51:13.183330: step 89670, total loss = 0.66, predict loss = 0.17 (66.4 examples/sec; 0.060 sec/batch; 98h:50m:21s remains)
INFO - root - 2019-11-04 00:51:13.833426: step 89680, total loss = 0.42, predict loss = 0.10 (71.2 examples/sec; 0.056 sec/batch; 92h:12m:17s remains)
INFO - root - 2019-11-04 00:51:14.486497: step 89690, total loss = 0.48, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:47m:17s remains)
INFO - root - 2019-11-04 00:51:15.153125: step 89700, total loss = 0.64, predict loss = 0.14 (63.4 examples/sec; 0.063 sec/batch; 103h:35m:20s remains)
INFO - root - 2019-11-04 00:51:15.882930: step 89710, total loss = 0.53, predict loss = 0.13 (66.9 examples/sec; 0.060 sec/batch; 98h:06m:08s remains)
INFO - root - 2019-11-04 00:51:16.504259: step 89720, total loss = 0.65, predict loss = 0.16 (76.3 examples/sec; 0.052 sec/batch; 86h:03m:39s remains)
INFO - root - 2019-11-04 00:51:17.095745: step 89730, total loss = 0.61, predict loss = 0.15 (77.7 examples/sec; 0.051 sec/batch; 84h:32m:55s remains)
INFO - root - 2019-11-04 00:51:17.715925: step 89740, total loss = 0.66, predict loss = 0.16 (67.0 examples/sec; 0.060 sec/batch; 98h:02m:57s remains)
INFO - root - 2019-11-04 00:51:18.369204: step 89750, total loss = 0.60, predict loss = 0.15 (71.6 examples/sec; 0.056 sec/batch; 91h:45m:45s remains)
INFO - root - 2019-11-04 00:51:19.010542: step 89760, total loss = 0.58, predict loss = 0.14 (66.6 examples/sec; 0.060 sec/batch; 98h:35m:57s remains)
INFO - root - 2019-11-04 00:51:19.648152: step 89770, total loss = 0.56, predict loss = 0.13 (65.5 examples/sec; 0.061 sec/batch; 100h:10m:57s remains)
INFO - root - 2019-11-04 00:51:20.304658: step 89780, total loss = 0.55, predict loss = 0.13 (74.2 examples/sec; 0.054 sec/batch; 88h:30m:08s remains)
INFO - root - 2019-11-04 00:51:20.940422: step 89790, total loss = 0.52, predict loss = 0.12 (76.4 examples/sec; 0.052 sec/batch; 85h:58m:25s remains)
INFO - root - 2019-11-04 00:51:21.646320: step 89800, total loss = 0.79, predict loss = 0.19 (60.3 examples/sec; 0.066 sec/batch; 108h:57m:42s remains)
INFO - root - 2019-11-04 00:51:22.308819: step 89810, total loss = 0.67, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 90h:57m:41s remains)
INFO - root - 2019-11-04 00:51:22.972828: step 89820, total loss = 0.66, predict loss = 0.16 (65.4 examples/sec; 0.061 sec/batch; 100h:21m:23s remains)
INFO - root - 2019-11-04 00:51:23.601978: step 89830, total loss = 0.37, predict loss = 0.09 (69.8 examples/sec; 0.057 sec/batch; 94h:01m:58s remains)
INFO - root - 2019-11-04 00:51:24.238272: step 89840, total loss = 0.55, predict loss = 0.12 (61.0 examples/sec; 0.066 sec/batch; 107h:39m:01s remains)
INFO - root - 2019-11-04 00:51:24.910027: step 89850, total loss = 0.48, predict loss = 0.11 (65.3 examples/sec; 0.061 sec/batch; 100h:31m:04s remains)
INFO - root - 2019-11-04 00:51:25.569485: step 89860, total loss = 0.51, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 102h:10m:56s remains)
INFO - root - 2019-11-04 00:51:26.215670: step 89870, total loss = 0.39, predict loss = 0.09 (73.3 examples/sec; 0.055 sec/batch; 89h:35m:55s remains)
INFO - root - 2019-11-04 00:51:26.837055: step 89880, total loss = 0.49, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 88h:51m:48s remains)
INFO - root - 2019-11-04 00:51:27.466908: step 89890, total loss = 0.41, predict loss = 0.09 (77.7 examples/sec; 0.051 sec/batch; 84h:31m:33s remains)
INFO - root - 2019-11-04 00:51:28.114055: step 89900, total loss = 0.52, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 85h:39m:56s remains)
INFO - root - 2019-11-04 00:51:28.755089: step 89910, total loss = 0.41, predict loss = 0.10 (69.0 examples/sec; 0.058 sec/batch; 95h:06m:41s remains)
INFO - root - 2019-11-04 00:51:29.419259: step 89920, total loss = 0.58, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 93h:28m:00s remains)
INFO - root - 2019-11-04 00:51:30.076462: step 89930, total loss = 0.66, predict loss = 0.16 (68.2 examples/sec; 0.059 sec/batch; 96h:15m:58s remains)
INFO - root - 2019-11-04 00:51:30.728559: step 89940, total loss = 0.52, predict loss = 0.13 (70.1 examples/sec; 0.057 sec/batch; 93h:38m:49s remains)
INFO - root - 2019-11-04 00:51:31.372045: step 89950, total loss = 0.73, predict loss = 0.17 (65.6 examples/sec; 0.061 sec/batch; 100h:07m:00s remains)
INFO - root - 2019-11-04 00:51:32.002468: step 89960, total loss = 0.50, predict loss = 0.11 (69.1 examples/sec; 0.058 sec/batch; 94h:59m:35s remains)
INFO - root - 2019-11-04 00:51:32.637034: step 89970, total loss = 0.56, predict loss = 0.12 (69.2 examples/sec; 0.058 sec/batch; 94h:57m:16s remains)
INFO - root - 2019-11-04 00:51:33.258152: step 89980, total loss = 0.50, predict loss = 0.12 (62.0 examples/sec; 0.064 sec/batch; 105h:51m:48s remains)
INFO - root - 2019-11-04 00:51:33.714578: step 89990, total loss = 0.46, predict loss = 0.11 (96.7 examples/sec; 0.041 sec/batch; 67h:55m:45s remains)
INFO - root - 2019-11-04 00:51:34.196777: step 90000, total loss = 0.63, predict loss = 0.15 (88.9 examples/sec; 0.045 sec/batch; 73h:50m:34s remains)
INFO - root - 2019-11-04 00:51:35.979922: step 90010, total loss = 0.38, predict loss = 0.08 (65.0 examples/sec; 0.062 sec/batch; 101h:03m:35s remains)
INFO - root - 2019-11-04 00:51:36.618668: step 90020, total loss = 0.36, predict loss = 0.08 (70.6 examples/sec; 0.057 sec/batch; 93h:03m:35s remains)
INFO - root - 2019-11-04 00:51:37.217004: step 90030, total loss = 0.28, predict loss = 0.07 (69.3 examples/sec; 0.058 sec/batch; 94h:42m:00s remains)
INFO - root - 2019-11-04 00:51:37.885154: step 90040, total loss = 0.57, predict loss = 0.14 (88.9 examples/sec; 0.045 sec/batch; 73h:49m:26s remains)
INFO - root - 2019-11-04 00:51:38.493669: step 90050, total loss = 0.58, predict loss = 0.13 (81.3 examples/sec; 0.049 sec/batch; 80h:44m:55s remains)
INFO - root - 2019-11-04 00:51:39.091044: step 90060, total loss = 0.46, predict loss = 0.11 (75.4 examples/sec; 0.053 sec/batch; 87h:06m:04s remains)
INFO - root - 2019-11-04 00:51:39.742141: step 90070, total loss = 0.36, predict loss = 0.09 (66.8 examples/sec; 0.060 sec/batch; 98h:20m:36s remains)
INFO - root - 2019-11-04 00:51:40.392279: step 90080, total loss = 0.50, predict loss = 0.11 (77.7 examples/sec; 0.051 sec/batch; 84h:30m:58s remains)
INFO - root - 2019-11-04 00:51:41.036946: step 90090, total loss = 0.68, predict loss = 0.16 (74.5 examples/sec; 0.054 sec/batch; 88h:09m:09s remains)
INFO - root - 2019-11-04 00:51:41.681827: step 90100, total loss = 0.56, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 98h:09m:05s remains)
INFO - root - 2019-11-04 00:51:42.306739: step 90110, total loss = 0.67, predict loss = 0.16 (65.4 examples/sec; 0.061 sec/batch; 100h:21m:54s remains)
INFO - root - 2019-11-04 00:51:42.944755: step 90120, total loss = 0.56, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 93h:25m:45s remains)
INFO - root - 2019-11-04 00:51:43.553286: step 90130, total loss = 0.38, predict loss = 0.08 (77.6 examples/sec; 0.052 sec/batch; 84h:36m:36s remains)
INFO - root - 2019-11-04 00:51:44.147720: step 90140, total loss = 0.40, predict loss = 0.09 (64.7 examples/sec; 0.062 sec/batch; 101h:25m:05s remains)
INFO - root - 2019-11-04 00:51:44.787478: step 90150, total loss = 0.51, predict loss = 0.12 (68.5 examples/sec; 0.058 sec/batch; 95h:48m:50s remains)
INFO - root - 2019-11-04 00:51:45.432456: step 90160, total loss = 0.45, predict loss = 0.09 (75.5 examples/sec; 0.053 sec/batch; 87h:01m:45s remains)
INFO - root - 2019-11-04 00:51:46.074845: step 90170, total loss = 0.45, predict loss = 0.10 (72.2 examples/sec; 0.055 sec/batch; 90h:53m:54s remains)
INFO - root - 2019-11-04 00:51:46.713341: step 90180, total loss = 0.52, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 92h:09m:32s remains)
INFO - root - 2019-11-04 00:51:47.367476: step 90190, total loss = 0.44, predict loss = 0.09 (59.5 examples/sec; 0.067 sec/batch; 110h:19m:35s remains)
INFO - root - 2019-11-04 00:51:48.066259: step 90200, total loss = 0.31, predict loss = 0.07 (62.2 examples/sec; 0.064 sec/batch; 105h:37m:03s remains)
INFO - root - 2019-11-04 00:51:48.743523: step 90210, total loss = 0.45, predict loss = 0.09 (63.8 examples/sec; 0.063 sec/batch; 102h:50m:43s remains)
INFO - root - 2019-11-04 00:51:49.431226: step 90220, total loss = 0.30, predict loss = 0.06 (62.3 examples/sec; 0.064 sec/batch; 105h:24m:04s remains)
INFO - root - 2019-11-04 00:51:50.062781: step 90230, total loss = 0.45, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 92h:24m:40s remains)
INFO - root - 2019-11-04 00:51:50.756640: step 90240, total loss = 0.53, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 97h:35m:36s remains)
INFO - root - 2019-11-04 00:51:51.379204: step 90250, total loss = 0.60, predict loss = 0.13 (67.9 examples/sec; 0.059 sec/batch; 96h:43m:05s remains)
INFO - root - 2019-11-04 00:51:51.994948: step 90260, total loss = 0.52, predict loss = 0.12 (65.3 examples/sec; 0.061 sec/batch; 100h:34m:32s remains)
INFO - root - 2019-11-04 00:51:52.619588: step 90270, total loss = 0.58, predict loss = 0.14 (78.0 examples/sec; 0.051 sec/batch; 84h:13m:31s remains)
INFO - root - 2019-11-04 00:51:53.226406: step 90280, total loss = 0.56, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 97h:35m:04s remains)
INFO - root - 2019-11-04 00:51:53.861162: step 90290, total loss = 0.55, predict loss = 0.13 (75.0 examples/sec; 0.053 sec/batch; 87h:35m:32s remains)
INFO - root - 2019-11-04 00:51:54.491205: step 90300, total loss = 0.57, predict loss = 0.13 (78.9 examples/sec; 0.051 sec/batch; 83h:11m:36s remains)
INFO - root - 2019-11-04 00:51:55.094884: step 90310, total loss = 0.60, predict loss = 0.14 (78.0 examples/sec; 0.051 sec/batch; 84h:10m:44s remains)
INFO - root - 2019-11-04 00:51:55.744196: step 90320, total loss = 0.59, predict loss = 0.14 (65.6 examples/sec; 0.061 sec/batch; 100h:03m:46s remains)
INFO - root - 2019-11-04 00:51:56.409682: step 90330, total loss = 0.58, predict loss = 0.14 (66.3 examples/sec; 0.060 sec/batch; 99h:01m:57s remains)
INFO - root - 2019-11-04 00:51:57.064951: step 90340, total loss = 0.51, predict loss = 0.12 (69.1 examples/sec; 0.058 sec/batch; 95h:03m:15s remains)
INFO - root - 2019-11-04 00:51:57.748067: step 90350, total loss = 0.68, predict loss = 0.16 (62.8 examples/sec; 0.064 sec/batch; 104h:35m:43s remains)
INFO - root - 2019-11-04 00:51:58.365768: step 90360, total loss = 0.61, predict loss = 0.15 (74.5 examples/sec; 0.054 sec/batch; 88h:04m:56s remains)
INFO - root - 2019-11-04 00:51:59.014614: step 90370, total loss = 0.56, predict loss = 0.13 (67.5 examples/sec; 0.059 sec/batch; 97h:20m:08s remains)
INFO - root - 2019-11-04 00:51:59.634623: step 90380, total loss = 0.52, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 94h:36m:03s remains)
INFO - root - 2019-11-04 00:52:00.282557: step 90390, total loss = 0.46, predict loss = 0.10 (72.5 examples/sec; 0.055 sec/batch; 90h:34m:16s remains)
INFO - root - 2019-11-04 00:52:00.929893: step 90400, total loss = 0.43, predict loss = 0.10 (73.5 examples/sec; 0.054 sec/batch; 89h:18m:23s remains)
INFO - root - 2019-11-04 00:52:01.550440: step 90410, total loss = 0.55, predict loss = 0.12 (66.4 examples/sec; 0.060 sec/batch; 98h:52m:16s remains)
INFO - root - 2019-11-04 00:52:02.265911: step 90420, total loss = 0.65, predict loss = 0.16 (61.1 examples/sec; 0.065 sec/batch; 107h:30m:12s remains)
INFO - root - 2019-11-04 00:52:02.914890: step 90430, total loss = 0.65, predict loss = 0.15 (77.2 examples/sec; 0.052 sec/batch; 85h:01m:06s remains)
INFO - root - 2019-11-04 00:52:03.580227: step 90440, total loss = 0.56, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 96h:48m:41s remains)
INFO - root - 2019-11-04 00:52:04.216600: step 90450, total loss = 0.51, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 92h:59m:38s remains)
INFO - root - 2019-11-04 00:52:04.906179: step 90460, total loss = 0.27, predict loss = 0.06 (70.8 examples/sec; 0.057 sec/batch; 92h:47m:53s remains)
INFO - root - 2019-11-04 00:52:05.521230: step 90470, total loss = 0.44, predict loss = 0.10 (70.3 examples/sec; 0.057 sec/batch; 93h:26m:23s remains)
INFO - root - 2019-11-04 00:52:06.181467: step 90480, total loss = 0.45, predict loss = 0.11 (63.8 examples/sec; 0.063 sec/batch; 102h:57m:25s remains)
INFO - root - 2019-11-04 00:52:06.831992: step 90490, total loss = 0.33, predict loss = 0.07 (71.1 examples/sec; 0.056 sec/batch; 92h:22m:59s remains)
INFO - root - 2019-11-04 00:52:07.543016: step 90500, total loss = 0.55, predict loss = 0.14 (66.9 examples/sec; 0.060 sec/batch; 98h:10m:17s remains)
INFO - root - 2019-11-04 00:52:08.174016: step 90510, total loss = 0.55, predict loss = 0.13 (76.0 examples/sec; 0.053 sec/batch; 86h:21m:12s remains)
INFO - root - 2019-11-04 00:52:08.833792: step 90520, total loss = 0.58, predict loss = 0.14 (70.9 examples/sec; 0.056 sec/batch; 92h:35m:07s remains)
INFO - root - 2019-11-04 00:52:09.462572: step 90530, total loss = 0.69, predict loss = 0.18 (65.5 examples/sec; 0.061 sec/batch; 100h:16m:33s remains)
INFO - root - 2019-11-04 00:52:10.127561: step 90540, total loss = 0.35, predict loss = 0.08 (68.6 examples/sec; 0.058 sec/batch; 95h:43m:40s remains)
INFO - root - 2019-11-04 00:52:10.794049: step 90550, total loss = 0.37, predict loss = 0.07 (61.8 examples/sec; 0.065 sec/batch; 106h:16m:20s remains)
INFO - root - 2019-11-04 00:52:11.418364: step 90560, total loss = 0.34, predict loss = 0.08 (77.9 examples/sec; 0.051 sec/batch; 84h:14m:03s remains)
INFO - root - 2019-11-04 00:52:12.061448: step 90570, total loss = 0.52, predict loss = 0.11 (71.0 examples/sec; 0.056 sec/batch; 92h:32m:00s remains)
INFO - root - 2019-11-04 00:52:12.745466: step 90580, total loss = 0.34, predict loss = 0.07 (71.2 examples/sec; 0.056 sec/batch; 92h:10m:44s remains)
INFO - root - 2019-11-04 00:52:13.384914: step 90590, total loss = 0.36, predict loss = 0.08 (73.4 examples/sec; 0.054 sec/batch; 89h:25m:26s remains)
INFO - root - 2019-11-04 00:52:14.053304: step 90600, total loss = 0.34, predict loss = 0.08 (60.0 examples/sec; 0.067 sec/batch; 109h:21m:10s remains)
INFO - root - 2019-11-04 00:52:14.755110: step 90610, total loss = 0.51, predict loss = 0.12 (57.5 examples/sec; 0.070 sec/batch; 114h:08m:12s remains)
INFO - root - 2019-11-04 00:52:15.410183: step 90620, total loss = 0.23, predict loss = 0.04 (69.2 examples/sec; 0.058 sec/batch; 94h:56m:56s remains)
INFO - root - 2019-11-04 00:52:16.068195: step 90630, total loss = 0.27, predict loss = 0.05 (63.1 examples/sec; 0.063 sec/batch; 104h:06m:36s remains)
INFO - root - 2019-11-04 00:52:16.712041: step 90640, total loss = 0.38, predict loss = 0.09 (77.9 examples/sec; 0.051 sec/batch; 84h:16m:31s remains)
INFO - root - 2019-11-04 00:52:17.382329: step 90650, total loss = 0.39, predict loss = 0.09 (63.1 examples/sec; 0.063 sec/batch; 104h:08m:01s remains)
INFO - root - 2019-11-04 00:52:17.989945: step 90660, total loss = 0.57, predict loss = 0.14 (72.5 examples/sec; 0.055 sec/batch; 90h:33m:47s remains)
INFO - root - 2019-11-04 00:52:18.613862: step 90670, total loss = 0.52, predict loss = 0.13 (64.3 examples/sec; 0.062 sec/batch; 102h:10m:55s remains)
INFO - root - 2019-11-04 00:52:19.288579: step 90680, total loss = 0.33, predict loss = 0.07 (60.5 examples/sec; 0.066 sec/batch; 108h:28m:46s remains)
INFO - root - 2019-11-04 00:52:19.970748: step 90690, total loss = 0.56, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 100h:48m:55s remains)
INFO - root - 2019-11-04 00:52:20.658813: step 90700, total loss = 0.55, predict loss = 0.13 (56.8 examples/sec; 0.070 sec/batch; 115h:32m:20s remains)
INFO - root - 2019-11-04 00:52:21.304842: step 90710, total loss = 0.55, predict loss = 0.12 (71.2 examples/sec; 0.056 sec/batch; 92h:10m:50s remains)
INFO - root - 2019-11-04 00:52:21.989817: step 90720, total loss = 0.48, predict loss = 0.11 (63.5 examples/sec; 0.063 sec/batch; 103h:20m:42s remains)
INFO - root - 2019-11-04 00:52:22.659891: step 90730, total loss = 0.45, predict loss = 0.10 (64.9 examples/sec; 0.062 sec/batch; 101h:10m:39s remains)
INFO - root - 2019-11-04 00:52:23.341960: step 90740, total loss = 0.51, predict loss = 0.12 (72.8 examples/sec; 0.055 sec/batch; 90h:14m:23s remains)
INFO - root - 2019-11-04 00:52:23.974777: step 90750, total loss = 0.47, predict loss = 0.11 (70.3 examples/sec; 0.057 sec/batch; 93h:22m:02s remains)
INFO - root - 2019-11-04 00:52:24.615206: step 90760, total loss = 0.41, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 92h:38m:01s remains)
INFO - root - 2019-11-04 00:52:25.287283: step 90770, total loss = 0.45, predict loss = 0.12 (74.9 examples/sec; 0.053 sec/batch; 87h:37m:21s remains)
INFO - root - 2019-11-04 00:52:25.932239: step 90780, total loss = 0.51, predict loss = 0.12 (67.0 examples/sec; 0.060 sec/batch; 98h:01m:31s remains)
INFO - root - 2019-11-04 00:52:26.605837: step 90790, total loss = 0.49, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 102h:09m:48s remains)
INFO - root - 2019-11-04 00:52:27.272752: step 90800, total loss = 0.51, predict loss = 0.12 (60.4 examples/sec; 0.066 sec/batch; 108h:38m:55s remains)
INFO - root - 2019-11-04 00:52:27.906055: step 90810, total loss = 0.36, predict loss = 0.09 (71.4 examples/sec; 0.056 sec/batch; 91h:58m:34s remains)
INFO - root - 2019-11-04 00:52:28.586773: step 90820, total loss = 0.22, predict loss = 0.04 (67.3 examples/sec; 0.059 sec/batch; 97h:35m:42s remains)
INFO - root - 2019-11-04 00:52:29.237835: step 90830, total loss = 0.30, predict loss = 0.06 (79.7 examples/sec; 0.050 sec/batch; 82h:24m:43s remains)
INFO - root - 2019-11-04 00:52:29.909100: step 90840, total loss = 0.29, predict loss = 0.06 (64.7 examples/sec; 0.062 sec/batch; 101h:26m:28s remains)
INFO - root - 2019-11-04 00:52:30.557225: step 90850, total loss = 0.39, predict loss = 0.09 (71.8 examples/sec; 0.056 sec/batch; 91h:25m:07s remains)
INFO - root - 2019-11-04 00:52:31.181564: step 90860, total loss = 0.26, predict loss = 0.06 (74.2 examples/sec; 0.054 sec/batch; 88h:28m:26s remains)
INFO - root - 2019-11-04 00:52:31.828151: step 90870, total loss = 0.37, predict loss = 0.09 (74.8 examples/sec; 0.053 sec/batch; 87h:47m:29s remains)
INFO - root - 2019-11-04 00:52:32.471904: step 90880, total loss = 0.35, predict loss = 0.07 (75.7 examples/sec; 0.053 sec/batch; 86h:46m:15s remains)
INFO - root - 2019-11-04 00:52:33.124785: step 90890, total loss = 0.44, predict loss = 0.10 (76.5 examples/sec; 0.052 sec/batch; 85h:47m:34s remains)
INFO - root - 2019-11-04 00:52:33.757417: step 90900, total loss = 0.27, predict loss = 0.06 (67.2 examples/sec; 0.060 sec/batch; 97h:42m:19s remains)
INFO - root - 2019-11-04 00:52:34.423637: step 90910, total loss = 0.52, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 95h:58m:42s remains)
INFO - root - 2019-11-04 00:52:35.077843: step 90920, total loss = 0.46, predict loss = 0.11 (60.5 examples/sec; 0.066 sec/batch; 108h:27m:53s remains)
INFO - root - 2019-11-04 00:52:35.707266: step 90930, total loss = 0.44, predict loss = 0.10 (70.1 examples/sec; 0.057 sec/batch; 93h:36m:59s remains)
INFO - root - 2019-11-04 00:52:36.296199: step 90940, total loss = 0.47, predict loss = 0.10 (76.8 examples/sec; 0.052 sec/batch; 85h:30m:13s remains)
INFO - root - 2019-11-04 00:52:36.905428: step 90950, total loss = 0.27, predict loss = 0.06 (80.6 examples/sec; 0.050 sec/batch; 81h:26m:52s remains)
INFO - root - 2019-11-04 00:52:37.531634: step 90960, total loss = 0.34, predict loss = 0.08 (68.1 examples/sec; 0.059 sec/batch; 96h:24m:33s remains)
INFO - root - 2019-11-04 00:52:38.140866: step 90970, total loss = 0.35, predict loss = 0.09 (69.6 examples/sec; 0.057 sec/batch; 94h:19m:56s remains)
INFO - root - 2019-11-04 00:52:38.747864: step 90980, total loss = 0.39, predict loss = 0.08 (72.6 examples/sec; 0.055 sec/batch; 90h:23m:34s remains)
INFO - root - 2019-11-04 00:52:39.340968: step 90990, total loss = 0.36, predict loss = 0.08 (71.5 examples/sec; 0.056 sec/batch; 91h:48m:13s remains)
INFO - root - 2019-11-04 00:52:39.994009: step 91000, total loss = 0.54, predict loss = 0.13 (68.2 examples/sec; 0.059 sec/batch; 96h:20m:22s remains)
INFO - root - 2019-11-04 00:52:40.625544: step 91010, total loss = 0.52, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 99h:34m:26s remains)
INFO - root - 2019-11-04 00:52:41.276590: step 91020, total loss = 0.59, predict loss = 0.14 (66.4 examples/sec; 0.060 sec/batch; 98h:53m:01s remains)
INFO - root - 2019-11-04 00:52:41.921802: step 91030, total loss = 0.50, predict loss = 0.13 (71.4 examples/sec; 0.056 sec/batch; 91h:56m:47s remains)
INFO - root - 2019-11-04 00:52:42.610138: step 91040, total loss = 0.39, predict loss = 0.09 (59.8 examples/sec; 0.067 sec/batch; 109h:48m:43s remains)
INFO - root - 2019-11-04 00:52:43.231420: step 91050, total loss = 0.30, predict loss = 0.07 (77.4 examples/sec; 0.052 sec/batch; 84h:49m:56s remains)
INFO - root - 2019-11-04 00:52:43.859740: step 91060, total loss = 0.47, predict loss = 0.12 (63.8 examples/sec; 0.063 sec/batch; 102h:52m:14s remains)
INFO - root - 2019-11-04 00:52:44.529447: step 91070, total loss = 0.48, predict loss = 0.12 (65.7 examples/sec; 0.061 sec/batch; 100h:00m:01s remains)
INFO - root - 2019-11-04 00:52:45.192062: step 91080, total loss = 0.32, predict loss = 0.07 (70.5 examples/sec; 0.057 sec/batch; 93h:07m:01s remains)
INFO - root - 2019-11-04 00:52:45.813877: step 91090, total loss = 0.36, predict loss = 0.09 (72.8 examples/sec; 0.055 sec/batch; 90h:08m:44s remains)
INFO - root - 2019-11-04 00:52:46.439572: step 91100, total loss = 0.31, predict loss = 0.06 (74.0 examples/sec; 0.054 sec/batch; 88h:41m:10s remains)
INFO - root - 2019-11-04 00:52:47.050718: step 91110, total loss = 0.29, predict loss = 0.06 (83.1 examples/sec; 0.048 sec/batch; 79h:00m:38s remains)
INFO - root - 2019-11-04 00:52:47.650894: step 91120, total loss = 0.42, predict loss = 0.10 (73.5 examples/sec; 0.054 sec/batch; 89h:21m:02s remains)
INFO - root - 2019-11-04 00:52:48.283225: step 91130, total loss = 0.33, predict loss = 0.07 (73.7 examples/sec; 0.054 sec/batch; 89h:04m:35s remains)
INFO - root - 2019-11-04 00:52:48.912806: step 91140, total loss = 0.29, predict loss = 0.06 (72.5 examples/sec; 0.055 sec/batch; 90h:31m:05s remains)
INFO - root - 2019-11-04 00:52:49.538729: step 91150, total loss = 0.40, predict loss = 0.09 (65.8 examples/sec; 0.061 sec/batch; 99h:43m:44s remains)
INFO - root - 2019-11-04 00:52:50.164116: step 91160, total loss = 0.44, predict loss = 0.10 (83.4 examples/sec; 0.048 sec/batch; 78h:40m:50s remains)
INFO - root - 2019-11-04 00:52:50.806756: step 91170, total loss = 0.39, predict loss = 0.09 (73.9 examples/sec; 0.054 sec/batch; 88h:51m:53s remains)
INFO - root - 2019-11-04 00:52:51.451030: step 91180, total loss = 0.37, predict loss = 0.08 (69.0 examples/sec; 0.058 sec/batch; 95h:06m:04s remains)
INFO - root - 2019-11-04 00:52:52.084850: step 91190, total loss = 0.31, predict loss = 0.07 (65.5 examples/sec; 0.061 sec/batch; 100h:10m:53s remains)
INFO - root - 2019-11-04 00:52:52.693424: step 91200, total loss = 0.37, predict loss = 0.09 (74.2 examples/sec; 0.054 sec/batch; 88h:28m:28s remains)
INFO - root - 2019-11-04 00:52:53.337094: step 91210, total loss = 0.36, predict loss = 0.09 (74.6 examples/sec; 0.054 sec/batch; 88h:01m:34s remains)
INFO - root - 2019-11-04 00:52:53.987766: step 91220, total loss = 0.51, predict loss = 0.13 (73.4 examples/sec; 0.054 sec/batch; 89h:26m:52s remains)
INFO - root - 2019-11-04 00:52:54.624908: step 91230, total loss = 0.32, predict loss = 0.07 (75.8 examples/sec; 0.053 sec/batch; 86h:37m:13s remains)
INFO - root - 2019-11-04 00:52:55.249673: step 91240, total loss = 0.50, predict loss = 0.12 (72.8 examples/sec; 0.055 sec/batch; 90h:12m:01s remains)
INFO - root - 2019-11-04 00:52:55.878647: step 91250, total loss = 0.63, predict loss = 0.14 (72.8 examples/sec; 0.055 sec/batch; 90h:14m:10s remains)
INFO - root - 2019-11-04 00:52:56.503028: step 91260, total loss = 0.43, predict loss = 0.10 (70.2 examples/sec; 0.057 sec/batch; 93h:32m:10s remains)
INFO - root - 2019-11-04 00:52:57.131826: step 91270, total loss = 0.47, predict loss = 0.10 (76.3 examples/sec; 0.052 sec/batch; 86h:03m:38s remains)
INFO - root - 2019-11-04 00:52:57.741864: step 91280, total loss = 0.52, predict loss = 0.12 (74.2 examples/sec; 0.054 sec/batch; 88h:30m:39s remains)
INFO - root - 2019-11-04 00:52:58.370323: step 91290, total loss = 0.37, predict loss = 0.08 (70.4 examples/sec; 0.057 sec/batch; 93h:17m:13s remains)
INFO - root - 2019-11-04 00:52:59.030629: step 91300, total loss = 0.47, predict loss = 0.11 (76.6 examples/sec; 0.052 sec/batch; 85h:44m:42s remains)
INFO - root - 2019-11-04 00:52:59.657198: step 91310, total loss = 0.40, predict loss = 0.09 (68.5 examples/sec; 0.058 sec/batch; 95h:49m:52s remains)
INFO - root - 2019-11-04 00:53:00.344799: step 91320, total loss = 0.62, predict loss = 0.15 (59.0 examples/sec; 0.068 sec/batch; 111h:14m:06s remains)
INFO - root - 2019-11-04 00:53:01.070247: step 91330, total loss = 0.46, predict loss = 0.10 (60.9 examples/sec; 0.066 sec/batch; 107h:46m:37s remains)
INFO - root - 2019-11-04 00:53:01.695806: step 91340, total loss = 0.57, predict loss = 0.13 (79.3 examples/sec; 0.050 sec/batch; 82h:44m:53s remains)
INFO - root - 2019-11-04 00:53:02.360936: step 91350, total loss = 0.28, predict loss = 0.06 (61.8 examples/sec; 0.065 sec/batch; 106h:17m:47s remains)
INFO - root - 2019-11-04 00:53:03.065271: step 91360, total loss = 0.33, predict loss = 0.07 (53.8 examples/sec; 0.074 sec/batch; 121h:57m:04s remains)
INFO - root - 2019-11-04 00:53:03.783082: step 91370, total loss = 0.48, predict loss = 0.11 (63.7 examples/sec; 0.063 sec/batch; 103h:00m:33s remains)
INFO - root - 2019-11-04 00:53:04.419067: step 91380, total loss = 0.41, predict loss = 0.09 (64.5 examples/sec; 0.062 sec/batch; 101h:50m:48s remains)
INFO - root - 2019-11-04 00:53:05.075786: step 91390, total loss = 0.43, predict loss = 0.10 (75.5 examples/sec; 0.053 sec/batch; 86h:54m:28s remains)
INFO - root - 2019-11-04 00:53:05.704087: step 91400, total loss = 0.55, predict loss = 0.13 (75.7 examples/sec; 0.053 sec/batch; 86h:42m:46s remains)
INFO - root - 2019-11-04 00:53:06.367760: step 91410, total loss = 0.70, predict loss = 0.17 (71.7 examples/sec; 0.056 sec/batch; 91h:33m:09s remains)
INFO - root - 2019-11-04 00:53:06.967210: step 91420, total loss = 0.45, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 95h:56m:58s remains)
INFO - root - 2019-11-04 00:53:07.598915: step 91430, total loss = 0.58, predict loss = 0.14 (69.9 examples/sec; 0.057 sec/batch; 93h:57m:13s remains)
INFO - root - 2019-11-04 00:53:08.202166: step 91440, total loss = 0.84, predict loss = 0.20 (74.0 examples/sec; 0.054 sec/batch; 88h:40m:01s remains)
INFO - root - 2019-11-04 00:53:08.827016: step 91450, total loss = 0.92, predict loss = 0.22 (71.5 examples/sec; 0.056 sec/batch; 91h:45m:57s remains)
INFO - root - 2019-11-04 00:53:09.472675: step 91460, total loss = 0.74, predict loss = 0.17 (66.5 examples/sec; 0.060 sec/batch; 98h:42m:07s remains)
INFO - root - 2019-11-04 00:53:10.085860: step 91470, total loss = 0.82, predict loss = 0.20 (73.1 examples/sec; 0.055 sec/batch; 89h:52m:07s remains)
INFO - root - 2019-11-04 00:53:10.735500: step 91480, total loss = 0.78, predict loss = 0.18 (66.2 examples/sec; 0.060 sec/batch; 99h:13m:17s remains)
INFO - root - 2019-11-04 00:53:11.444593: step 91490, total loss = 0.74, predict loss = 0.18 (63.3 examples/sec; 0.063 sec/batch; 103h:42m:55s remains)
INFO - root - 2019-11-04 00:53:12.044359: step 91500, total loss = 0.72, predict loss = 0.17 (75.4 examples/sec; 0.053 sec/batch; 87h:00m:44s remains)
INFO - root - 2019-11-04 00:53:12.667225: step 91510, total loss = 0.61, predict loss = 0.15 (68.5 examples/sec; 0.058 sec/batch; 95h:53m:21s remains)
INFO - root - 2019-11-04 00:53:13.326770: step 91520, total loss = 0.53, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 92h:57m:03s remains)
INFO - root - 2019-11-04 00:53:13.958914: step 91530, total loss = 0.69, predict loss = 0.16 (65.4 examples/sec; 0.061 sec/batch; 100h:25m:10s remains)
INFO - root - 2019-11-04 00:53:14.640316: step 91540, total loss = 0.55, predict loss = 0.13 (70.2 examples/sec; 0.057 sec/batch; 93h:33m:49s remains)
INFO - root - 2019-11-04 00:53:15.277400: step 91550, total loss = 0.57, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 93h:22m:06s remains)
INFO - root - 2019-11-04 00:53:15.910244: step 91560, total loss = 0.50, predict loss = 0.11 (68.3 examples/sec; 0.059 sec/batch; 96h:06m:20s remains)
INFO - root - 2019-11-04 00:53:16.544979: step 91570, total loss = 0.40, predict loss = 0.09 (67.9 examples/sec; 0.059 sec/batch; 96h:38m:21s remains)
INFO - root - 2019-11-04 00:53:17.190542: step 91580, total loss = 0.47, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 98h:44m:17s remains)
INFO - root - 2019-11-04 00:53:17.867054: step 91590, total loss = 0.55, predict loss = 0.13 (67.9 examples/sec; 0.059 sec/batch; 96h:45m:06s remains)
INFO - root - 2019-11-04 00:53:18.530080: step 91600, total loss = 0.44, predict loss = 0.10 (70.8 examples/sec; 0.057 sec/batch; 92h:46m:24s remains)
INFO - root - 2019-11-04 00:53:19.192451: step 91610, total loss = 0.53, predict loss = 0.13 (58.9 examples/sec; 0.068 sec/batch; 111h:32m:39s remains)
INFO - root - 2019-11-04 00:53:19.860781: step 91620, total loss = 0.33, predict loss = 0.06 (63.9 examples/sec; 0.063 sec/batch; 102h:40m:14s remains)
INFO - root - 2019-11-04 00:53:20.493003: step 91630, total loss = 0.49, predict loss = 0.11 (65.8 examples/sec; 0.061 sec/batch; 99h:49m:18s remains)
INFO - root - 2019-11-04 00:53:21.129105: step 91640, total loss = 0.55, predict loss = 0.13 (73.2 examples/sec; 0.055 sec/batch; 89h:40m:23s remains)
INFO - root - 2019-11-04 00:53:21.773593: step 91650, total loss = 0.36, predict loss = 0.08 (75.0 examples/sec; 0.053 sec/batch; 87h:35m:01s remains)
INFO - root - 2019-11-04 00:53:22.402024: step 91660, total loss = 0.55, predict loss = 0.13 (62.6 examples/sec; 0.064 sec/batch; 104h:55m:44s remains)
INFO - root - 2019-11-04 00:53:23.032777: step 91670, total loss = 0.46, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 92h:21m:19s remains)
INFO - root - 2019-11-04 00:53:23.660603: step 91680, total loss = 0.43, predict loss = 0.10 (62.7 examples/sec; 0.064 sec/batch; 104h:41m:36s remains)
INFO - root - 2019-11-04 00:53:24.324708: step 91690, total loss = 0.46, predict loss = 0.11 (59.1 examples/sec; 0.068 sec/batch; 110h:59m:53s remains)
INFO - root - 2019-11-04 00:53:24.959517: step 91700, total loss = 0.38, predict loss = 0.09 (75.8 examples/sec; 0.053 sec/batch; 86h:36m:17s remains)
INFO - root - 2019-11-04 00:53:25.626526: step 91710, total loss = 0.37, predict loss = 0.08 (66.6 examples/sec; 0.060 sec/batch; 98h:38m:07s remains)
INFO - root - 2019-11-04 00:53:26.300199: step 91720, total loss = 0.42, predict loss = 0.10 (70.7 examples/sec; 0.057 sec/batch; 92h:51m:39s remains)
INFO - root - 2019-11-04 00:53:26.888417: step 91730, total loss = 0.44, predict loss = 0.10 (84.5 examples/sec; 0.047 sec/batch; 77h:38m:40s remains)
INFO - root - 2019-11-04 00:53:27.509589: step 91740, total loss = 0.47, predict loss = 0.11 (67.9 examples/sec; 0.059 sec/batch; 96h:44m:03s remains)
INFO - root - 2019-11-04 00:53:28.199967: step 91750, total loss = 0.47, predict loss = 0.11 (69.4 examples/sec; 0.058 sec/batch; 94h:38m:32s remains)
INFO - root - 2019-11-04 00:53:28.854414: step 91760, total loss = 0.41, predict loss = 0.10 (74.3 examples/sec; 0.054 sec/batch; 88h:17m:50s remains)
INFO - root - 2019-11-04 00:53:29.484396: step 91770, total loss = 0.41, predict loss = 0.10 (88.5 examples/sec; 0.045 sec/batch; 74h:11m:43s remains)
INFO - root - 2019-11-04 00:53:30.100079: step 91780, total loss = 0.49, predict loss = 0.11 (71.7 examples/sec; 0.056 sec/batch; 91h:32m:51s remains)
INFO - root - 2019-11-04 00:53:30.751383: step 91790, total loss = 0.50, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 90h:11m:49s remains)
INFO - root - 2019-11-04 00:53:31.400083: step 91800, total loss = 0.55, predict loss = 0.13 (62.8 examples/sec; 0.064 sec/batch; 104h:28m:29s remains)
INFO - root - 2019-11-04 00:53:32.027395: step 91810, total loss = 0.50, predict loss = 0.11 (66.2 examples/sec; 0.060 sec/batch; 99h:05m:26s remains)
INFO - root - 2019-11-04 00:53:32.662339: step 91820, total loss = 0.50, predict loss = 0.11 (76.1 examples/sec; 0.053 sec/batch; 86h:15m:09s remains)
INFO - root - 2019-11-04 00:53:33.317359: step 91830, total loss = 0.62, predict loss = 0.14 (67.0 examples/sec; 0.060 sec/batch; 97h:54m:55s remains)
INFO - root - 2019-11-04 00:53:33.972249: step 91840, total loss = 0.64, predict loss = 0.15 (73.4 examples/sec; 0.055 sec/batch; 89h:29m:32s remains)
INFO - root - 2019-11-04 00:53:34.655604: step 91850, total loss = 0.61, predict loss = 0.14 (55.5 examples/sec; 0.072 sec/batch; 118h:12m:20s remains)
INFO - root - 2019-11-04 00:53:35.238785: step 91860, total loss = 0.57, predict loss = 0.13 (69.7 examples/sec; 0.057 sec/batch; 94h:07m:59s remains)
INFO - root - 2019-11-04 00:53:35.834781: step 91870, total loss = 0.66, predict loss = 0.16 (71.6 examples/sec; 0.056 sec/batch; 91h:43m:47s remains)
INFO - root - 2019-11-04 00:53:36.436782: step 91880, total loss = 0.68, predict loss = 0.16 (70.4 examples/sec; 0.057 sec/batch; 93h:16m:03s remains)
INFO - root - 2019-11-04 00:53:37.069694: step 91890, total loss = 0.63, predict loss = 0.15 (71.0 examples/sec; 0.056 sec/batch; 92h:28m:01s remains)
INFO - root - 2019-11-04 00:53:37.697242: step 91900, total loss = 0.66, predict loss = 0.15 (69.9 examples/sec; 0.057 sec/batch; 93h:52m:25s remains)
INFO - root - 2019-11-04 00:53:38.358633: step 91910, total loss = 0.60, predict loss = 0.14 (63.6 examples/sec; 0.063 sec/batch; 103h:14m:24s remains)
INFO - root - 2019-11-04 00:53:38.978485: step 91920, total loss = 0.67, predict loss = 0.16 (68.4 examples/sec; 0.058 sec/batch; 95h:56m:12s remains)
INFO - root - 2019-11-04 00:53:39.594008: step 91930, total loss = 0.50, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 97h:55m:11s remains)
INFO - root - 2019-11-04 00:53:40.223765: step 91940, total loss = 0.53, predict loss = 0.12 (72.5 examples/sec; 0.055 sec/batch; 90h:31m:07s remains)
INFO - root - 2019-11-04 00:53:40.890924: step 91950, total loss = 0.46, predict loss = 0.11 (65.1 examples/sec; 0.061 sec/batch; 100h:54m:50s remains)
INFO - root - 2019-11-04 00:53:41.542569: step 91960, total loss = 0.42, predict loss = 0.10 (69.4 examples/sec; 0.058 sec/batch; 94h:33m:42s remains)
INFO - root - 2019-11-04 00:53:42.196057: step 91970, total loss = 0.48, predict loss = 0.11 (69.4 examples/sec; 0.058 sec/batch; 94h:33m:43s remains)
INFO - root - 2019-11-04 00:53:42.907574: step 91980, total loss = 0.37, predict loss = 0.08 (65.6 examples/sec; 0.061 sec/batch; 100h:05m:29s remains)
INFO - root - 2019-11-04 00:53:43.536333: step 91990, total loss = 0.46, predict loss = 0.10 (70.5 examples/sec; 0.057 sec/batch; 93h:06m:48s remains)
INFO - root - 2019-11-04 00:53:44.169446: step 92000, total loss = 0.43, predict loss = 0.09 (83.1 examples/sec; 0.048 sec/batch; 78h:59m:48s remains)
INFO - root - 2019-11-04 00:53:44.827133: step 92010, total loss = 0.47, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 98h:41m:17s remains)
INFO - root - 2019-11-04 00:53:45.495855: step 92020, total loss = 0.45, predict loss = 0.10 (64.7 examples/sec; 0.062 sec/batch; 101h:28m:52s remains)
INFO - root - 2019-11-04 00:53:46.131396: step 92030, total loss = 0.47, predict loss = 0.11 (72.6 examples/sec; 0.055 sec/batch; 90h:24m:00s remains)
INFO - root - 2019-11-04 00:53:46.776219: step 92040, total loss = 0.53, predict loss = 0.12 (72.5 examples/sec; 0.055 sec/batch; 90h:36m:10s remains)
INFO - root - 2019-11-04 00:53:47.426613: step 92050, total loss = 0.63, predict loss = 0.15 (64.1 examples/sec; 0.062 sec/batch; 102h:24m:57s remains)
INFO - root - 2019-11-04 00:53:48.090842: step 92060, total loss = 0.51, predict loss = 0.12 (70.4 examples/sec; 0.057 sec/batch; 93h:14m:28s remains)
INFO - root - 2019-11-04 00:53:48.759209: step 92070, total loss = 0.56, predict loss = 0.13 (70.7 examples/sec; 0.057 sec/batch; 92h:47m:46s remains)
INFO - root - 2019-11-04 00:53:49.411939: step 92080, total loss = 0.50, predict loss = 0.11 (69.4 examples/sec; 0.058 sec/batch; 94h:32m:04s remains)
INFO - root - 2019-11-04 00:53:50.106297: step 92090, total loss = 0.56, predict loss = 0.13 (63.4 examples/sec; 0.063 sec/batch; 103h:36m:18s remains)
INFO - root - 2019-11-04 00:53:50.742954: step 92100, total loss = 0.66, predict loss = 0.16 (79.0 examples/sec; 0.051 sec/batch; 83h:06m:09s remains)
INFO - root - 2019-11-04 00:53:51.351539: step 92110, total loss = 0.67, predict loss = 0.17 (68.3 examples/sec; 0.059 sec/batch; 96h:03m:37s remains)
INFO - root - 2019-11-04 00:53:51.979598: step 92120, total loss = 0.51, predict loss = 0.12 (68.6 examples/sec; 0.058 sec/batch; 95h:37m:34s remains)
INFO - root - 2019-11-04 00:53:52.638301: step 92130, total loss = 0.58, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 100h:48m:47s remains)
INFO - root - 2019-11-04 00:53:53.275780: step 92140, total loss = 0.59, predict loss = 0.14 (62.4 examples/sec; 0.064 sec/batch; 105h:13m:50s remains)
INFO - root - 2019-11-04 00:53:53.940545: step 92150, total loss = 0.53, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 93h:19m:28s remains)
INFO - root - 2019-11-04 00:53:54.622459: step 92160, total loss = 0.52, predict loss = 0.12 (69.5 examples/sec; 0.058 sec/batch; 94h:29m:46s remains)
INFO - root - 2019-11-04 00:53:55.266002: step 92170, total loss = 0.44, predict loss = 0.10 (74.8 examples/sec; 0.053 sec/batch; 87h:45m:10s remains)
INFO - root - 2019-11-04 00:53:55.896616: step 92180, total loss = 0.47, predict loss = 0.11 (61.5 examples/sec; 0.065 sec/batch; 106h:41m:26s remains)
INFO - root - 2019-11-04 00:53:56.506974: step 92190, total loss = 0.52, predict loss = 0.12 (74.4 examples/sec; 0.054 sec/batch; 88h:17m:05s remains)
INFO - root - 2019-11-04 00:53:57.178273: step 92200, total loss = 0.56, predict loss = 0.13 (60.1 examples/sec; 0.067 sec/batch; 109h:08m:36s remains)
INFO - root - 2019-11-04 00:53:57.832306: step 92210, total loss = 0.48, predict loss = 0.11 (81.8 examples/sec; 0.049 sec/batch; 80h:16m:31s remains)
INFO - root - 2019-11-04 00:53:58.504573: step 92220, total loss = 0.46, predict loss = 0.10 (76.2 examples/sec; 0.052 sec/batch; 86h:05m:38s remains)
INFO - root - 2019-11-04 00:53:59.134795: step 92230, total loss = 0.59, predict loss = 0.14 (70.0 examples/sec; 0.057 sec/batch; 93h:43m:00s remains)
INFO - root - 2019-11-04 00:53:59.798021: step 92240, total loss = 0.57, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 94h:17m:09s remains)
INFO - root - 2019-11-04 00:54:00.394670: step 92250, total loss = 0.60, predict loss = 0.15 (78.4 examples/sec; 0.051 sec/batch; 83h:42m:33s remains)
INFO - root - 2019-11-04 00:54:01.028842: step 92260, total loss = 0.44, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 97h:24m:11s remains)
INFO - root - 2019-11-04 00:54:01.658068: step 92270, total loss = 0.48, predict loss = 0.11 (77.1 examples/sec; 0.052 sec/batch; 85h:06m:09s remains)
INFO - root - 2019-11-04 00:54:02.288842: step 92280, total loss = 0.50, predict loss = 0.12 (79.7 examples/sec; 0.050 sec/batch; 82h:23m:11s remains)
INFO - root - 2019-11-04 00:54:02.916723: step 92290, total loss = 0.33, predict loss = 0.07 (66.2 examples/sec; 0.060 sec/batch; 99h:06m:07s remains)
INFO - root - 2019-11-04 00:54:03.549302: step 92300, total loss = 0.42, predict loss = 0.09 (70.3 examples/sec; 0.057 sec/batch; 93h:19m:47s remains)
INFO - root - 2019-11-04 00:54:04.228923: step 92310, total loss = 0.51, predict loss = 0.12 (61.5 examples/sec; 0.065 sec/batch; 106h:41m:44s remains)
INFO - root - 2019-11-04 00:54:04.901424: step 92320, total loss = 0.46, predict loss = 0.10 (79.0 examples/sec; 0.051 sec/batch; 83h:02m:53s remains)
INFO - root - 2019-11-04 00:54:05.480230: step 92330, total loss = 0.57, predict loss = 0.13 (69.6 examples/sec; 0.057 sec/batch; 94h:18m:17s remains)
INFO - root - 2019-11-04 00:54:06.089350: step 92340, total loss = 0.48, predict loss = 0.11 (67.2 examples/sec; 0.060 sec/batch; 97h:42m:36s remains)
INFO - root - 2019-11-04 00:54:06.711309: step 92350, total loss = 0.53, predict loss = 0.12 (68.6 examples/sec; 0.058 sec/batch; 95h:42m:56s remains)
INFO - root - 2019-11-04 00:54:07.348296: step 92360, total loss = 0.44, predict loss = 0.10 (60.7 examples/sec; 0.066 sec/batch; 108h:05m:47s remains)
INFO - root - 2019-11-04 00:54:07.977113: step 92370, total loss = 0.40, predict loss = 0.09 (67.9 examples/sec; 0.059 sec/batch; 96h:43m:58s remains)
INFO - root - 2019-11-04 00:54:08.589592: step 92380, total loss = 0.51, predict loss = 0.12 (68.1 examples/sec; 0.059 sec/batch; 96h:23m:35s remains)
INFO - root - 2019-11-04 00:54:09.250369: step 92390, total loss = 0.58, predict loss = 0.14 (72.0 examples/sec; 0.056 sec/batch; 91h:08m:39s remains)
INFO - root - 2019-11-04 00:54:09.886738: step 92400, total loss = 0.47, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 93h:02m:23s remains)
INFO - root - 2019-11-04 00:54:10.508722: step 92410, total loss = 0.38, predict loss = 0.08 (74.8 examples/sec; 0.053 sec/batch; 87h:46m:22s remains)
INFO - root - 2019-11-04 00:54:11.145448: step 92420, total loss = 0.57, predict loss = 0.14 (65.4 examples/sec; 0.061 sec/batch; 100h:25m:46s remains)
INFO - root - 2019-11-04 00:54:11.800310: step 92430, total loss = 0.61, predict loss = 0.14 (70.0 examples/sec; 0.057 sec/batch; 93h:48m:14s remains)
INFO - root - 2019-11-04 00:54:12.464371: step 92440, total loss = 0.53, predict loss = 0.13 (69.9 examples/sec; 0.057 sec/batch; 93h:57m:09s remains)
INFO - root - 2019-11-04 00:54:13.143343: step 92450, total loss = 0.39, predict loss = 0.09 (61.2 examples/sec; 0.065 sec/batch; 107h:12m:04s remains)
INFO - root - 2019-11-04 00:54:13.839534: step 92460, total loss = 0.63, predict loss = 0.15 (70.8 examples/sec; 0.056 sec/batch; 92h:39m:14s remains)
INFO - root - 2019-11-04 00:54:14.491393: step 92470, total loss = 0.61, predict loss = 0.15 (62.2 examples/sec; 0.064 sec/batch; 105h:29m:06s remains)
INFO - root - 2019-11-04 00:54:15.172358: step 92480, total loss = 0.65, predict loss = 0.15 (70.5 examples/sec; 0.057 sec/batch; 93h:04m:40s remains)
INFO - root - 2019-11-04 00:54:15.827658: step 92490, total loss = 0.75, predict loss = 0.17 (65.2 examples/sec; 0.061 sec/batch; 100h:44m:08s remains)
INFO - root - 2019-11-04 00:54:16.517495: step 92500, total loss = 0.52, predict loss = 0.12 (64.9 examples/sec; 0.062 sec/batch; 101h:08m:19s remains)
INFO - root - 2019-11-04 00:54:17.188702: step 92510, total loss = 0.51, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 89h:56m:01s remains)
INFO - root - 2019-11-04 00:54:17.853968: step 92520, total loss = 0.64, predict loss = 0.15 (71.0 examples/sec; 0.056 sec/batch; 92h:23m:23s remains)
INFO - root - 2019-11-04 00:54:18.546910: step 92530, total loss = 0.64, predict loss = 0.15 (62.2 examples/sec; 0.064 sec/batch; 105h:34m:02s remains)
INFO - root - 2019-11-04 00:54:19.176428: step 92540, total loss = 0.65, predict loss = 0.16 (73.3 examples/sec; 0.055 sec/batch; 89h:33m:15s remains)
INFO - root - 2019-11-04 00:54:19.810726: step 92550, total loss = 0.50, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 95h:31m:47s remains)
INFO - root - 2019-11-04 00:54:20.418330: step 92560, total loss = 0.50, predict loss = 0.12 (69.6 examples/sec; 0.057 sec/batch; 94h:19m:27s remains)
INFO - root - 2019-11-04 00:54:21.043069: step 92570, total loss = 0.49, predict loss = 0.11 (68.9 examples/sec; 0.058 sec/batch; 95h:18m:28s remains)
INFO - root - 2019-11-04 00:54:21.715480: step 92580, total loss = 0.53, predict loss = 0.12 (64.6 examples/sec; 0.062 sec/batch; 101h:34m:52s remains)
INFO - root - 2019-11-04 00:54:22.396384: step 92590, total loss = 0.34, predict loss = 0.08 (64.1 examples/sec; 0.062 sec/batch; 102h:20m:53s remains)
INFO - root - 2019-11-04 00:54:23.058489: step 92600, total loss = 0.42, predict loss = 0.09 (72.7 examples/sec; 0.055 sec/batch; 90h:13m:47s remains)
INFO - root - 2019-11-04 00:54:23.729791: step 92610, total loss = 0.41, predict loss = 0.09 (66.8 examples/sec; 0.060 sec/batch; 98h:15m:19s remains)
INFO - root - 2019-11-04 00:54:24.370928: step 92620, total loss = 0.49, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 98h:48m:58s remains)
INFO - root - 2019-11-04 00:54:25.020902: step 92630, total loss = 0.54, predict loss = 0.13 (66.8 examples/sec; 0.060 sec/batch; 98h:13m:54s remains)
INFO - root - 2019-11-04 00:54:25.680934: step 92640, total loss = 0.74, predict loss = 0.17 (69.0 examples/sec; 0.058 sec/batch; 95h:07m:03s remains)
INFO - root - 2019-11-04 00:54:26.351599: step 92650, total loss = 0.63, predict loss = 0.15 (65.3 examples/sec; 0.061 sec/batch; 100h:29m:21s remains)
INFO - root - 2019-11-04 00:54:27.003868: step 92660, total loss = 0.43, predict loss = 0.10 (67.8 examples/sec; 0.059 sec/batch; 96h:51m:32s remains)
INFO - root - 2019-11-04 00:54:27.667694: step 92670, total loss = 0.51, predict loss = 0.12 (66.7 examples/sec; 0.060 sec/batch; 98h:25m:07s remains)
INFO - root - 2019-11-04 00:54:28.312857: step 92680, total loss = 0.49, predict loss = 0.10 (71.0 examples/sec; 0.056 sec/batch; 92h:24m:19s remains)
INFO - root - 2019-11-04 00:54:28.976471: step 92690, total loss = 0.62, predict loss = 0.14 (67.0 examples/sec; 0.060 sec/batch; 97h:58m:48s remains)
INFO - root - 2019-11-04 00:54:29.604152: step 92700, total loss = 0.64, predict loss = 0.15 (68.8 examples/sec; 0.058 sec/batch; 95h:27m:41s remains)
INFO - root - 2019-11-04 00:54:30.210818: step 92710, total loss = 0.33, predict loss = 0.08 (89.8 examples/sec; 0.045 sec/batch; 73h:07m:09s remains)
INFO - root - 2019-11-04 00:54:30.677120: step 92720, total loss = 0.53, predict loss = 0.12 (96.4 examples/sec; 0.041 sec/batch; 68h:04m:28s remains)
INFO - root - 2019-11-04 00:54:31.663723: step 92730, total loss = 0.32, predict loss = 0.07 (7.0 examples/sec; 0.571 sec/batch; 936h:13m:08s remains)
INFO - root - 2019-11-04 00:54:32.273232: step 92740, total loss = 0.54, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 91h:56m:33s remains)
INFO - root - 2019-11-04 00:54:32.911013: step 92750, total loss = 0.60, predict loss = 0.14 (71.4 examples/sec; 0.056 sec/batch; 91h:52m:15s remains)
INFO - root - 2019-11-04 00:54:33.526841: step 92760, total loss = 0.53, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 98h:02m:16s remains)
INFO - root - 2019-11-04 00:54:34.204799: step 92770, total loss = 0.42, predict loss = 0.09 (62.8 examples/sec; 0.064 sec/batch; 104h:28m:03s remains)
INFO - root - 2019-11-04 00:54:34.901737: step 92780, total loss = 0.36, predict loss = 0.08 (70.9 examples/sec; 0.056 sec/batch; 92h:36m:20s remains)
INFO - root - 2019-11-04 00:54:35.566328: step 92790, total loss = 0.60, predict loss = 0.14 (65.0 examples/sec; 0.062 sec/batch; 101h:00m:34s remains)
INFO - root - 2019-11-04 00:54:36.185773: step 92800, total loss = 0.46, predict loss = 0.11 (72.2 examples/sec; 0.055 sec/batch; 90h:57m:01s remains)
INFO - root - 2019-11-04 00:54:36.823674: step 92810, total loss = 0.43, predict loss = 0.10 (74.7 examples/sec; 0.054 sec/batch; 87h:49m:05s remains)
INFO - root - 2019-11-04 00:54:37.488411: step 92820, total loss = 0.50, predict loss = 0.11 (66.7 examples/sec; 0.060 sec/batch; 98h:22m:11s remains)
INFO - root - 2019-11-04 00:54:38.132442: step 92830, total loss = 0.75, predict loss = 0.18 (64.1 examples/sec; 0.062 sec/batch; 102h:25m:06s remains)
INFO - root - 2019-11-04 00:54:38.776214: step 92840, total loss = 0.70, predict loss = 0.16 (69.0 examples/sec; 0.058 sec/batch; 95h:08m:56s remains)
INFO - root - 2019-11-04 00:54:39.421115: step 92850, total loss = 0.59, predict loss = 0.14 (68.7 examples/sec; 0.058 sec/batch; 95h:32m:14s remains)
INFO - root - 2019-11-04 00:54:40.063260: step 92860, total loss = 0.50, predict loss = 0.11 (62.8 examples/sec; 0.064 sec/batch; 104h:31m:18s remains)
INFO - root - 2019-11-04 00:54:40.756873: step 92870, total loss = 0.50, predict loss = 0.11 (61.7 examples/sec; 0.065 sec/batch; 106h:21m:05s remains)
INFO - root - 2019-11-04 00:54:41.349081: step 92880, total loss = 0.54, predict loss = 0.12 (78.8 examples/sec; 0.051 sec/batch; 83h:19m:19s remains)
INFO - root - 2019-11-04 00:54:41.938912: step 92890, total loss = 0.52, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 94h:03m:09s remains)
INFO - root - 2019-11-04 00:54:42.585105: step 92900, total loss = 0.49, predict loss = 0.11 (63.8 examples/sec; 0.063 sec/batch; 102h:49m:10s remains)
INFO - root - 2019-11-04 00:54:43.288123: step 92910, total loss = 0.56, predict loss = 0.13 (59.6 examples/sec; 0.067 sec/batch; 110h:09m:18s remains)
INFO - root - 2019-11-04 00:54:43.979793: step 92920, total loss = 0.49, predict loss = 0.11 (71.8 examples/sec; 0.056 sec/batch; 91h:26m:56s remains)
INFO - root - 2019-11-04 00:54:44.612500: step 92930, total loss = 0.54, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 96h:42m:12s remains)
INFO - root - 2019-11-04 00:54:45.258455: step 92940, total loss = 0.40, predict loss = 0.08 (68.2 examples/sec; 0.059 sec/batch; 96h:10m:51s remains)
INFO - root - 2019-11-04 00:54:45.883203: step 92950, total loss = 0.41, predict loss = 0.09 (66.2 examples/sec; 0.060 sec/batch; 99h:09m:31s remains)
INFO - root - 2019-11-04 00:54:46.514587: step 92960, total loss = 0.50, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 92h:15m:06s remains)
INFO - root - 2019-11-04 00:54:47.122441: step 92970, total loss = 0.39, predict loss = 0.09 (85.6 examples/sec; 0.047 sec/batch; 76h:40m:45s remains)
INFO - root - 2019-11-04 00:54:47.741411: step 92980, total loss = 0.54, predict loss = 0.12 (69.3 examples/sec; 0.058 sec/batch; 94h:45m:43s remains)
INFO - root - 2019-11-04 00:54:48.380124: step 92990, total loss = 0.46, predict loss = 0.11 (77.3 examples/sec; 0.052 sec/batch; 84h:55m:39s remains)
INFO - root - 2019-11-04 00:54:49.002899: step 93000, total loss = 0.55, predict loss = 0.12 (69.9 examples/sec; 0.057 sec/batch; 93h:51m:56s remains)
INFO - root - 2019-11-04 00:54:49.648312: step 93010, total loss = 0.58, predict loss = 0.14 (67.4 examples/sec; 0.059 sec/batch; 97h:21m:53s remains)
INFO - root - 2019-11-04 00:54:50.295887: step 93020, total loss = 0.54, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 96h:53m:51s remains)
INFO - root - 2019-11-04 00:54:50.974832: step 93030, total loss = 0.48, predict loss = 0.11 (68.4 examples/sec; 0.059 sec/batch; 96h:00m:20s remains)
INFO - root - 2019-11-04 00:54:51.605480: step 93040, total loss = 0.66, predict loss = 0.16 (64.8 examples/sec; 0.062 sec/batch; 101h:19m:52s remains)
INFO - root - 2019-11-04 00:54:52.246112: step 93050, total loss = 0.60, predict loss = 0.14 (70.1 examples/sec; 0.057 sec/batch; 93h:34m:50s remains)
INFO - root - 2019-11-04 00:54:52.856105: step 93060, total loss = 0.55, predict loss = 0.13 (66.4 examples/sec; 0.060 sec/batch; 98h:48m:24s remains)
INFO - root - 2019-11-04 00:54:53.495563: step 93070, total loss = 0.47, predict loss = 0.10 (56.2 examples/sec; 0.071 sec/batch; 116h:46m:18s remains)
INFO - root - 2019-11-04 00:54:54.139230: step 93080, total loss = 0.49, predict loss = 0.11 (80.3 examples/sec; 0.050 sec/batch; 81h:42m:32s remains)
INFO - root - 2019-11-04 00:54:54.773876: step 93090, total loss = 0.49, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 93h:38m:48s remains)
INFO - root - 2019-11-04 00:54:55.450636: step 93100, total loss = 0.57, predict loss = 0.13 (58.1 examples/sec; 0.069 sec/batch; 112h:58m:08s remains)
INFO - root - 2019-11-04 00:54:56.118102: step 93110, total loss = 0.50, predict loss = 0.11 (60.2 examples/sec; 0.066 sec/batch; 109h:04m:38s remains)
INFO - root - 2019-11-04 00:54:56.783245: step 93120, total loss = 0.43, predict loss = 0.09 (65.7 examples/sec; 0.061 sec/batch; 99h:55m:31s remains)
INFO - root - 2019-11-04 00:54:57.456383: step 93130, total loss = 0.52, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 98h:41m:05s remains)
INFO - root - 2019-11-04 00:54:58.103569: step 93140, total loss = 0.55, predict loss = 0.13 (66.8 examples/sec; 0.060 sec/batch; 98h:11m:37s remains)
INFO - root - 2019-11-04 00:54:58.743534: step 93150, total loss = 0.62, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 95h:31m:39s remains)
INFO - root - 2019-11-04 00:54:59.389244: step 93160, total loss = 0.66, predict loss = 0.15 (62.9 examples/sec; 0.064 sec/batch; 104h:22m:27s remains)
INFO - root - 2019-11-04 00:55:00.008273: step 93170, total loss = 0.51, predict loss = 0.11 (65.4 examples/sec; 0.061 sec/batch; 100h:20m:59s remains)
INFO - root - 2019-11-04 00:55:00.607854: step 93180, total loss = 0.48, predict loss = 0.11 (87.8 examples/sec; 0.046 sec/batch; 74h:45m:52s remains)
INFO - root - 2019-11-04 00:55:01.234346: step 93190, total loss = 0.65, predict loss = 0.16 (68.9 examples/sec; 0.058 sec/batch; 95h:14m:32s remains)
INFO - root - 2019-11-04 00:55:01.855451: step 93200, total loss = 0.47, predict loss = 0.11 (74.4 examples/sec; 0.054 sec/batch; 88h:12m:07s remains)
INFO - root - 2019-11-04 00:55:02.466480: step 93210, total loss = 0.45, predict loss = 0.10 (65.4 examples/sec; 0.061 sec/batch; 100h:23m:05s remains)
INFO - root - 2019-11-04 00:55:03.070178: step 93220, total loss = 0.45, predict loss = 0.09 (72.3 examples/sec; 0.055 sec/batch; 90h:48m:46s remains)
INFO - root - 2019-11-04 00:55:03.711900: step 93230, total loss = 0.56, predict loss = 0.14 (74.8 examples/sec; 0.054 sec/batch; 87h:47m:45s remains)
INFO - root - 2019-11-04 00:55:04.321097: step 93240, total loss = 0.45, predict loss = 0.11 (75.9 examples/sec; 0.053 sec/batch; 86h:30m:40s remains)
INFO - root - 2019-11-04 00:55:04.981413: step 93250, total loss = 0.68, predict loss = 0.17 (86.2 examples/sec; 0.046 sec/batch; 76h:08m:07s remains)
INFO - root - 2019-11-04 00:55:05.564295: step 93260, total loss = 0.52, predict loss = 0.12 (81.1 examples/sec; 0.049 sec/batch; 80h:54m:35s remains)
INFO - root - 2019-11-04 00:55:06.135861: step 93270, total loss = 0.57, predict loss = 0.14 (81.6 examples/sec; 0.049 sec/batch; 80h:27m:52s remains)
INFO - root - 2019-11-04 00:55:06.704390: step 93280, total loss = 0.41, predict loss = 0.10 (78.8 examples/sec; 0.051 sec/batch; 83h:15m:36s remains)
INFO - root - 2019-11-04 00:55:07.322301: step 93290, total loss = 0.47, predict loss = 0.11 (65.1 examples/sec; 0.061 sec/batch; 100h:49m:16s remains)
INFO - root - 2019-11-04 00:55:07.974447: step 93300, total loss = 0.36, predict loss = 0.09 (73.9 examples/sec; 0.054 sec/batch; 88h:49m:25s remains)
INFO - root - 2019-11-04 00:55:08.647737: step 93310, total loss = 0.34, predict loss = 0.07 (75.1 examples/sec; 0.053 sec/batch; 87h:24m:47s remains)
INFO - root - 2019-11-04 00:55:09.315755: step 93320, total loss = 0.31, predict loss = 0.07 (81.9 examples/sec; 0.049 sec/batch; 80h:08m:37s remains)
INFO - root - 2019-11-04 00:55:09.953384: step 93330, total loss = 0.31, predict loss = 0.07 (72.2 examples/sec; 0.055 sec/batch; 90h:56m:42s remains)
INFO - root - 2019-11-04 00:55:10.585141: step 93340, total loss = 0.35, predict loss = 0.08 (77.5 examples/sec; 0.052 sec/batch; 84h:38m:09s remains)
INFO - root - 2019-11-04 00:55:11.218548: step 93350, total loss = 0.43, predict loss = 0.10 (63.3 examples/sec; 0.063 sec/batch; 103h:38m:16s remains)
INFO - root - 2019-11-04 00:55:11.838630: step 93360, total loss = 0.45, predict loss = 0.10 (67.9 examples/sec; 0.059 sec/batch; 96h:41m:29s remains)
INFO - root - 2019-11-04 00:55:12.452223: step 93370, total loss = 0.59, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 90h:26m:44s remains)
INFO - root - 2019-11-04 00:55:13.089401: step 93380, total loss = 0.49, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 99h:06m:19s remains)
INFO - root - 2019-11-04 00:55:13.770945: step 93390, total loss = 0.50, predict loss = 0.11 (63.8 examples/sec; 0.063 sec/batch; 102h:52m:17s remains)
INFO - root - 2019-11-04 00:55:14.429502: step 93400, total loss = 0.50, predict loss = 0.11 (69.0 examples/sec; 0.058 sec/batch; 95h:03m:01s remains)
INFO - root - 2019-11-04 00:55:15.062352: step 93410, total loss = 0.40, predict loss = 0.09 (65.7 examples/sec; 0.061 sec/batch; 99h:56m:01s remains)
INFO - root - 2019-11-04 00:55:15.736184: step 93420, total loss = 0.57, predict loss = 0.13 (72.8 examples/sec; 0.055 sec/batch; 90h:08m:34s remains)
INFO - root - 2019-11-04 00:55:16.418055: step 93430, total loss = 0.72, predict loss = 0.18 (58.9 examples/sec; 0.068 sec/batch; 111h:19m:58s remains)
INFO - root - 2019-11-04 00:55:17.104651: step 93440, total loss = 0.49, predict loss = 0.11 (67.5 examples/sec; 0.059 sec/batch; 97h:10m:51s remains)
INFO - root - 2019-11-04 00:55:17.769324: step 93450, total loss = 0.58, predict loss = 0.14 (67.3 examples/sec; 0.059 sec/batch; 97h:32m:12s remains)
INFO - root - 2019-11-04 00:55:18.451251: step 93460, total loss = 0.50, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 98h:04m:14s remains)
INFO - root - 2019-11-04 00:55:19.087971: step 93470, total loss = 0.60, predict loss = 0.15 (73.0 examples/sec; 0.055 sec/batch; 89h:54m:59s remains)
INFO - root - 2019-11-04 00:55:19.754685: step 93480, total loss = 0.52, predict loss = 0.12 (66.1 examples/sec; 0.061 sec/batch; 99h:16m:45s remains)
INFO - root - 2019-11-04 00:55:20.370918: step 93490, total loss = 0.59, predict loss = 0.14 (63.4 examples/sec; 0.063 sec/batch; 103h:26m:11s remains)
INFO - root - 2019-11-04 00:55:20.996976: step 93500, total loss = 0.54, predict loss = 0.13 (75.4 examples/sec; 0.053 sec/batch; 87h:02m:28s remains)
INFO - root - 2019-11-04 00:55:21.630036: step 93510, total loss = 0.44, predict loss = 0.10 (69.8 examples/sec; 0.057 sec/batch; 93h:59m:20s remains)
INFO - root - 2019-11-04 00:55:22.287517: step 93520, total loss = 0.55, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 89h:46m:50s remains)
INFO - root - 2019-11-04 00:55:22.926013: step 93530, total loss = 0.55, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 86h:30m:58s remains)
INFO - root - 2019-11-04 00:55:23.544248: step 93540, total loss = 0.56, predict loss = 0.14 (71.8 examples/sec; 0.056 sec/batch; 91h:26m:35s remains)
INFO - root - 2019-11-04 00:55:24.174295: step 93550, total loss = 0.43, predict loss = 0.10 (66.8 examples/sec; 0.060 sec/batch; 98h:10m:36s remains)
INFO - root - 2019-11-04 00:55:24.822984: step 93560, total loss = 0.28, predict loss = 0.06 (64.9 examples/sec; 0.062 sec/batch; 101h:06m:24s remains)
INFO - root - 2019-11-04 00:55:25.454133: step 93570, total loss = 0.29, predict loss = 0.07 (69.4 examples/sec; 0.058 sec/batch; 94h:30m:58s remains)
INFO - root - 2019-11-04 00:55:26.568337: step 93580, total loss = 0.44, predict loss = 0.10 (77.8 examples/sec; 0.051 sec/batch; 84h:22m:49s remains)
INFO - root - 2019-11-04 00:55:27.166038: step 93590, total loss = 0.33, predict loss = 0.07 (80.2 examples/sec; 0.050 sec/batch; 81h:49m:39s remains)
INFO - root - 2019-11-04 00:55:27.799278: step 93600, total loss = 0.18, predict loss = 0.04 (63.7 examples/sec; 0.063 sec/batch; 103h:00m:10s remains)
INFO - root - 2019-11-04 00:55:28.440541: step 93610, total loss = 0.28, predict loss = 0.06 (79.1 examples/sec; 0.051 sec/batch; 82h:59m:53s remains)
INFO - root - 2019-11-04 00:55:29.068624: step 93620, total loss = 0.38, predict loss = 0.09 (81.3 examples/sec; 0.049 sec/batch; 80h:45m:46s remains)
INFO - root - 2019-11-04 00:55:29.697312: step 93630, total loss = 0.33, predict loss = 0.06 (76.2 examples/sec; 0.053 sec/batch; 86h:10m:33s remains)
INFO - root - 2019-11-04 00:55:30.311435: step 93640, total loss = 0.38, predict loss = 0.08 (74.1 examples/sec; 0.054 sec/batch; 88h:33m:38s remains)
INFO - root - 2019-11-04 00:55:30.962826: step 93650, total loss = 0.50, predict loss = 0.12 (59.4 examples/sec; 0.067 sec/batch; 110h:23m:55s remains)
INFO - root - 2019-11-04 00:55:31.601537: step 93660, total loss = 0.34, predict loss = 0.08 (66.0 examples/sec; 0.061 sec/batch; 99h:25m:38s remains)
INFO - root - 2019-11-04 00:55:32.268164: step 93670, total loss = 0.28, predict loss = 0.06 (65.8 examples/sec; 0.061 sec/batch; 99h:43m:47s remains)
INFO - root - 2019-11-04 00:55:32.875452: step 93680, total loss = 0.74, predict loss = 0.19 (67.3 examples/sec; 0.059 sec/batch; 97h:29m:31s remains)
INFO - root - 2019-11-04 00:55:33.495149: step 93690, total loss = 0.53, predict loss = 0.13 (73.8 examples/sec; 0.054 sec/batch; 88h:55m:39s remains)
INFO - root - 2019-11-04 00:55:34.103830: step 93700, total loss = 0.29, predict loss = 0.06 (75.6 examples/sec; 0.053 sec/batch; 86h:51m:34s remains)
INFO - root - 2019-11-04 00:55:34.787413: step 93710, total loss = 0.44, predict loss = 0.10 (70.0 examples/sec; 0.057 sec/batch; 93h:48m:55s remains)
INFO - root - 2019-11-04 00:55:35.409259: step 93720, total loss = 0.37, predict loss = 0.08 (78.2 examples/sec; 0.051 sec/batch; 83h:57m:40s remains)
INFO - root - 2019-11-04 00:55:36.067221: step 93730, total loss = 0.48, predict loss = 0.12 (69.7 examples/sec; 0.057 sec/batch; 94h:07m:57s remains)
INFO - root - 2019-11-04 00:55:36.734732: step 93740, total loss = 0.40, predict loss = 0.09 (77.8 examples/sec; 0.051 sec/batch; 84h:23m:01s remains)
INFO - root - 2019-11-04 00:55:37.398578: step 93750, total loss = 0.61, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 95h:35m:06s remains)
INFO - root - 2019-11-04 00:55:38.057730: step 93760, total loss = 0.51, predict loss = 0.12 (77.9 examples/sec; 0.051 sec/batch; 84h:17m:26s remains)
INFO - root - 2019-11-04 00:55:38.664538: step 93770, total loss = 0.69, predict loss = 0.17 (71.4 examples/sec; 0.056 sec/batch; 91h:53m:15s remains)
INFO - root - 2019-11-04 00:55:39.304465: step 93780, total loss = 0.44, predict loss = 0.11 (69.2 examples/sec; 0.058 sec/batch; 94h:50m:43s remains)
INFO - root - 2019-11-04 00:55:39.951137: step 93790, total loss = 0.29, predict loss = 0.07 (78.4 examples/sec; 0.051 sec/batch; 83h:39m:46s remains)
INFO - root - 2019-11-04 00:55:40.612283: step 93800, total loss = 0.42, predict loss = 0.10 (68.8 examples/sec; 0.058 sec/batch; 95h:19m:22s remains)
INFO - root - 2019-11-04 00:55:41.250246: step 93810, total loss = 0.41, predict loss = 0.09 (73.3 examples/sec; 0.055 sec/batch; 89h:32m:35s remains)
INFO - root - 2019-11-04 00:55:41.865890: step 93820, total loss = 0.37, predict loss = 0.08 (70.2 examples/sec; 0.057 sec/batch; 93h:25m:18s remains)
INFO - root - 2019-11-04 00:55:42.460922: step 93830, total loss = 0.27, predict loss = 0.05 (73.9 examples/sec; 0.054 sec/batch; 88h:47m:06s remains)
INFO - root - 2019-11-04 00:55:43.107624: step 93840, total loss = 0.32, predict loss = 0.08 (67.4 examples/sec; 0.059 sec/batch; 97h:23m:09s remains)
INFO - root - 2019-11-04 00:55:43.744439: step 93850, total loss = 0.35, predict loss = 0.08 (71.1 examples/sec; 0.056 sec/batch; 92h:18m:30s remains)
INFO - root - 2019-11-04 00:55:44.391391: step 93860, total loss = 0.38, predict loss = 0.08 (65.6 examples/sec; 0.061 sec/batch; 100h:00m:04s remains)
INFO - root - 2019-11-04 00:55:45.007575: step 93870, total loss = 0.44, predict loss = 0.10 (76.8 examples/sec; 0.052 sec/batch; 85h:24m:55s remains)
INFO - root - 2019-11-04 00:55:45.610145: step 93880, total loss = 0.45, predict loss = 0.10 (82.0 examples/sec; 0.049 sec/batch; 80h:04m:34s remains)
INFO - root - 2019-11-04 00:55:46.256160: step 93890, total loss = 0.57, predict loss = 0.12 (67.2 examples/sec; 0.060 sec/batch; 97h:37m:06s remains)
INFO - root - 2019-11-04 00:55:46.898324: step 93900, total loss = 0.48, predict loss = 0.10 (67.9 examples/sec; 0.059 sec/batch; 96h:35m:43s remains)
INFO - root - 2019-11-04 00:55:47.541833: step 93910, total loss = 0.38, predict loss = 0.09 (65.4 examples/sec; 0.061 sec/batch; 100h:20m:28s remains)
INFO - root - 2019-11-04 00:55:48.173325: step 93920, total loss = 0.47, predict loss = 0.10 (79.1 examples/sec; 0.051 sec/batch; 82h:57m:53s remains)
INFO - root - 2019-11-04 00:55:48.818149: step 93930, total loss = 0.39, predict loss = 0.09 (69.6 examples/sec; 0.057 sec/batch; 94h:18m:42s remains)
INFO - root - 2019-11-04 00:55:49.448140: step 93940, total loss = 0.33, predict loss = 0.07 (70.2 examples/sec; 0.057 sec/batch; 93h:26m:34s remains)
INFO - root - 2019-11-04 00:55:50.086782: step 93950, total loss = 0.45, predict loss = 0.11 (71.2 examples/sec; 0.056 sec/batch; 92h:11m:54s remains)
INFO - root - 2019-11-04 00:55:50.723214: step 93960, total loss = 0.56, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 94h:02m:02s remains)
INFO - root - 2019-11-04 00:55:51.345683: step 93970, total loss = 0.59, predict loss = 0.14 (68.4 examples/sec; 0.058 sec/batch; 95h:53m:13s remains)
INFO - root - 2019-11-04 00:55:52.004558: step 93980, total loss = 0.42, predict loss = 0.10 (70.2 examples/sec; 0.057 sec/batch; 93h:30m:55s remains)
INFO - root - 2019-11-04 00:55:52.621402: step 93990, total loss = 0.66, predict loss = 0.16 (67.3 examples/sec; 0.059 sec/batch; 97h:29m:14s remains)
INFO - root - 2019-11-04 00:55:53.302342: step 94000, total loss = 0.59, predict loss = 0.14 (71.0 examples/sec; 0.056 sec/batch; 92h:23m:07s remains)
INFO - root - 2019-11-04 00:55:53.965261: step 94010, total loss = 0.44, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 99h:37m:02s remains)
INFO - root - 2019-11-04 00:55:54.617708: step 94020, total loss = 0.46, predict loss = 0.10 (74.9 examples/sec; 0.053 sec/batch; 87h:35m:54s remains)
INFO - root - 2019-11-04 00:55:55.255231: step 94030, total loss = 0.56, predict loss = 0.13 (63.5 examples/sec; 0.063 sec/batch; 103h:22m:34s remains)
INFO - root - 2019-11-04 00:55:55.869887: step 94040, total loss = 0.58, predict loss = 0.13 (75.4 examples/sec; 0.053 sec/batch; 87h:03m:52s remains)
INFO - root - 2019-11-04 00:55:56.493303: step 94050, total loss = 0.45, predict loss = 0.10 (72.7 examples/sec; 0.055 sec/batch; 90h:16m:59s remains)
INFO - root - 2019-11-04 00:55:57.107958: step 94060, total loss = 0.44, predict loss = 0.10 (71.9 examples/sec; 0.056 sec/batch; 91h:16m:15s remains)
INFO - root - 2019-11-04 00:55:57.769526: step 94070, total loss = 0.63, predict loss = 0.16 (67.6 examples/sec; 0.059 sec/batch; 97h:07m:15s remains)
INFO - root - 2019-11-04 00:55:58.412767: step 94080, total loss = 0.26, predict loss = 0.05 (69.0 examples/sec; 0.058 sec/batch; 95h:04m:36s remains)
INFO - root - 2019-11-04 00:55:59.078452: step 94090, total loss = 0.44, predict loss = 0.10 (67.5 examples/sec; 0.059 sec/batch; 97h:16m:55s remains)
INFO - root - 2019-11-04 00:55:59.725512: step 94100, total loss = 0.57, predict loss = 0.13 (71.8 examples/sec; 0.056 sec/batch; 91h:23m:21s remains)
INFO - root - 2019-11-04 00:56:00.369509: step 94110, total loss = 0.62, predict loss = 0.15 (71.6 examples/sec; 0.056 sec/batch; 91h:36m:58s remains)
INFO - root - 2019-11-04 00:56:01.038515: step 94120, total loss = 0.39, predict loss = 0.09 (57.0 examples/sec; 0.070 sec/batch; 115h:05m:37s remains)
INFO - root - 2019-11-04 00:56:01.682366: step 94130, total loss = 0.46, predict loss = 0.10 (70.7 examples/sec; 0.057 sec/batch; 92h:49m:30s remains)
INFO - root - 2019-11-04 00:56:02.297214: step 94140, total loss = 0.45, predict loss = 0.10 (65.4 examples/sec; 0.061 sec/batch; 100h:16m:37s remains)
INFO - root - 2019-11-04 00:56:02.940763: step 94150, total loss = 0.61, predict loss = 0.15 (68.5 examples/sec; 0.058 sec/batch; 95h:44m:16s remains)
INFO - root - 2019-11-04 00:56:03.630881: step 94160, total loss = 0.66, predict loss = 0.16 (56.4 examples/sec; 0.071 sec/batch; 116h:26m:08s remains)
INFO - root - 2019-11-04 00:56:04.285909: step 94170, total loss = 0.75, predict loss = 0.17 (71.2 examples/sec; 0.056 sec/batch; 92h:13m:16s remains)
INFO - root - 2019-11-04 00:56:04.984488: step 94180, total loss = 0.73, predict loss = 0.18 (71.1 examples/sec; 0.056 sec/batch; 92h:15m:38s remains)
INFO - root - 2019-11-04 00:56:05.599976: step 94190, total loss = 0.60, predict loss = 0.14 (68.8 examples/sec; 0.058 sec/batch; 95h:21m:22s remains)
INFO - root - 2019-11-04 00:56:06.323995: step 94200, total loss = 0.82, predict loss = 0.19 (62.7 examples/sec; 0.064 sec/batch; 104h:39m:26s remains)
INFO - root - 2019-11-04 00:56:06.979096: step 94210, total loss = 0.62, predict loss = 0.15 (62.5 examples/sec; 0.064 sec/batch; 104h:58m:13s remains)
INFO - root - 2019-11-04 00:56:07.585586: step 94220, total loss = 0.68, predict loss = 0.17 (72.5 examples/sec; 0.055 sec/batch; 90h:28m:52s remains)
INFO - root - 2019-11-04 00:56:08.199977: step 94230, total loss = 0.79, predict loss = 0.19 (72.3 examples/sec; 0.055 sec/batch; 90h:47m:26s remains)
INFO - root - 2019-11-04 00:56:08.878919: step 94240, total loss = 0.64, predict loss = 0.15 (65.1 examples/sec; 0.061 sec/batch; 100h:49m:29s remains)
INFO - root - 2019-11-04 00:56:09.571573: step 94250, total loss = 0.57, predict loss = 0.14 (61.3 examples/sec; 0.065 sec/batch; 107h:05m:11s remains)
INFO - root - 2019-11-04 00:56:10.183977: step 94260, total loss = 0.55, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 89h:47m:20s remains)
INFO - root - 2019-11-04 00:56:10.795883: step 94270, total loss = 0.62, predict loss = 0.14 (72.4 examples/sec; 0.055 sec/batch; 90h:34m:55s remains)
INFO - root - 2019-11-04 00:56:11.407019: step 94280, total loss = 0.46, predict loss = 0.10 (74.0 examples/sec; 0.054 sec/batch; 88h:39m:31s remains)
INFO - root - 2019-11-04 00:56:12.020806: step 94290, total loss = 0.45, predict loss = 0.10 (71.3 examples/sec; 0.056 sec/batch; 92h:01m:55s remains)
INFO - root - 2019-11-04 00:56:12.650706: step 94300, total loss = 0.58, predict loss = 0.14 (63.3 examples/sec; 0.063 sec/batch; 103h:37m:24s remains)
INFO - root - 2019-11-04 00:56:13.276490: step 94310, total loss = 0.46, predict loss = 0.10 (69.9 examples/sec; 0.057 sec/batch; 93h:50m:38s remains)
INFO - root - 2019-11-04 00:56:13.906949: step 94320, total loss = 0.49, predict loss = 0.11 (80.8 examples/sec; 0.049 sec/batch; 81h:10m:48s remains)
INFO - root - 2019-11-04 00:56:14.533775: step 94330, total loss = 0.53, predict loss = 0.13 (81.2 examples/sec; 0.049 sec/batch; 80h:50m:01s remains)
INFO - root - 2019-11-04 00:56:15.173427: step 94340, total loss = 0.51, predict loss = 0.12 (69.2 examples/sec; 0.058 sec/batch; 94h:49m:17s remains)
INFO - root - 2019-11-04 00:56:15.839695: step 94350, total loss = 0.48, predict loss = 0.11 (70.8 examples/sec; 0.056 sec/batch; 92h:40m:38s remains)
INFO - root - 2019-11-04 00:56:16.505331: step 94360, total loss = 0.62, predict loss = 0.14 (66.0 examples/sec; 0.061 sec/batch; 99h:29m:36s remains)
INFO - root - 2019-11-04 00:56:17.159035: step 94370, total loss = 0.41, predict loss = 0.09 (69.2 examples/sec; 0.058 sec/batch; 94h:50m:23s remains)
INFO - root - 2019-11-04 00:56:17.780044: step 94380, total loss = 0.54, predict loss = 0.12 (72.9 examples/sec; 0.055 sec/batch; 89h:57m:33s remains)
INFO - root - 2019-11-04 00:56:18.454909: step 94390, total loss = 0.62, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 95h:29m:01s remains)
INFO - root - 2019-11-04 00:56:19.149872: step 94400, total loss = 0.40, predict loss = 0.09 (64.9 examples/sec; 0.062 sec/batch; 101h:09m:13s remains)
INFO - root - 2019-11-04 00:56:19.798819: step 94410, total loss = 0.50, predict loss = 0.12 (69.2 examples/sec; 0.058 sec/batch; 94h:52m:54s remains)
INFO - root - 2019-11-04 00:56:20.467189: step 94420, total loss = 0.41, predict loss = 0.09 (71.7 examples/sec; 0.056 sec/batch; 91h:34m:02s remains)
INFO - root - 2019-11-04 00:56:21.115275: step 94430, total loss = 0.42, predict loss = 0.10 (63.6 examples/sec; 0.063 sec/batch; 103h:06m:40s remains)
INFO - root - 2019-11-04 00:56:21.754702: step 94440, total loss = 0.44, predict loss = 0.10 (72.3 examples/sec; 0.055 sec/batch; 90h:42m:26s remains)
INFO - root - 2019-11-04 00:56:22.366550: step 94450, total loss = 0.37, predict loss = 0.08 (66.5 examples/sec; 0.060 sec/batch; 98h:36m:52s remains)
INFO - root - 2019-11-04 00:56:22.996400: step 94460, total loss = 0.47, predict loss = 0.11 (69.5 examples/sec; 0.058 sec/batch; 94h:26m:27s remains)
INFO - root - 2019-11-04 00:56:23.602331: step 94470, total loss = 0.54, predict loss = 0.12 (71.0 examples/sec; 0.056 sec/batch; 92h:27m:26s remains)
INFO - root - 2019-11-04 00:56:24.244220: step 94480, total loss = 0.45, predict loss = 0.10 (66.6 examples/sec; 0.060 sec/batch; 98h:33m:36s remains)
INFO - root - 2019-11-04 00:56:25.001808: step 94490, total loss = 0.39, predict loss = 0.08 (61.1 examples/sec; 0.065 sec/batch; 107h:24m:09s remains)
INFO - root - 2019-11-04 00:56:25.685352: step 94500, total loss = 0.50, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 99h:33m:17s remains)
INFO - root - 2019-11-04 00:56:26.324501: step 94510, total loss = 0.42, predict loss = 0.10 (66.0 examples/sec; 0.061 sec/batch; 99h:24m:31s remains)
INFO - root - 2019-11-04 00:56:26.950782: step 94520, total loss = 0.54, predict loss = 0.12 (75.2 examples/sec; 0.053 sec/batch; 87h:18m:04s remains)
INFO - root - 2019-11-04 00:56:27.595168: step 94530, total loss = 0.42, predict loss = 0.10 (70.9 examples/sec; 0.056 sec/batch; 92h:29m:24s remains)
INFO - root - 2019-11-04 00:56:28.232428: step 94540, total loss = 0.48, predict loss = 0.11 (65.4 examples/sec; 0.061 sec/batch; 100h:21m:32s remains)
INFO - root - 2019-11-04 00:56:28.879172: step 94550, total loss = 0.52, predict loss = 0.12 (65.2 examples/sec; 0.061 sec/batch; 100h:37m:30s remains)
INFO - root - 2019-11-04 00:56:29.474830: step 94560, total loss = 0.56, predict loss = 0.12 (77.5 examples/sec; 0.052 sec/batch; 84h:38m:27s remains)
INFO - root - 2019-11-04 00:56:30.112962: step 94570, total loss = 0.56, predict loss = 0.13 (65.6 examples/sec; 0.061 sec/batch; 100h:03m:50s remains)
INFO - root - 2019-11-04 00:56:30.763540: step 94580, total loss = 0.61, predict loss = 0.15 (65.5 examples/sec; 0.061 sec/batch; 100h:06m:43s remains)
INFO - root - 2019-11-04 00:56:31.399064: step 94590, total loss = 0.55, predict loss = 0.12 (72.5 examples/sec; 0.055 sec/batch; 90h:33m:07s remains)
INFO - root - 2019-11-04 00:56:32.040752: step 94600, total loss = 0.45, predict loss = 0.10 (63.7 examples/sec; 0.063 sec/batch; 103h:05m:12s remains)
INFO - root - 2019-11-04 00:56:32.693529: step 94610, total loss = 0.64, predict loss = 0.15 (67.1 examples/sec; 0.060 sec/batch; 97h:50m:06s remains)
INFO - root - 2019-11-04 00:56:33.372489: step 94620, total loss = 0.69, predict loss = 0.17 (65.7 examples/sec; 0.061 sec/batch; 99h:53m:27s remains)
INFO - root - 2019-11-04 00:56:34.016744: step 94630, total loss = 0.71, predict loss = 0.18 (64.3 examples/sec; 0.062 sec/batch; 102h:04m:18s remains)
INFO - root - 2019-11-04 00:56:34.718969: step 94640, total loss = 0.66, predict loss = 0.16 (73.3 examples/sec; 0.055 sec/batch; 89h:29m:18s remains)
INFO - root - 2019-11-04 00:56:35.292597: step 94650, total loss = 0.68, predict loss = 0.16 (85.9 examples/sec; 0.047 sec/batch; 76h:21m:35s remains)
INFO - root - 2019-11-04 00:56:35.914400: step 94660, total loss = 0.52, predict loss = 0.11 (67.9 examples/sec; 0.059 sec/batch; 96h:39m:40s remains)
INFO - root - 2019-11-04 00:56:36.571803: step 94670, total loss = 0.56, predict loss = 0.12 (65.1 examples/sec; 0.061 sec/batch; 100h:42m:54s remains)
INFO - root - 2019-11-04 00:56:37.220775: step 94680, total loss = 0.51, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 95h:20m:51s remains)
INFO - root - 2019-11-04 00:56:37.840638: step 94690, total loss = 0.31, predict loss = 0.07 (66.3 examples/sec; 0.060 sec/batch; 98h:53m:44s remains)
INFO - root - 2019-11-04 00:56:38.515789: step 94700, total loss = 0.43, predict loss = 0.09 (66.0 examples/sec; 0.061 sec/batch; 99h:26m:36s remains)
INFO - root - 2019-11-04 00:56:39.142540: step 94710, total loss = 0.36, predict loss = 0.08 (74.2 examples/sec; 0.054 sec/batch; 88h:28m:41s remains)
INFO - root - 2019-11-04 00:56:39.743928: step 94720, total loss = 0.41, predict loss = 0.09 (70.2 examples/sec; 0.057 sec/batch; 93h:26m:15s remains)
INFO - root - 2019-11-04 00:56:40.364844: step 94730, total loss = 0.43, predict loss = 0.10 (73.2 examples/sec; 0.055 sec/batch; 89h:34m:41s remains)
INFO - root - 2019-11-04 00:56:41.058206: step 94740, total loss = 0.42, predict loss = 0.09 (67.8 examples/sec; 0.059 sec/batch; 96h:45m:42s remains)
INFO - root - 2019-11-04 00:56:41.696083: step 94750, total loss = 0.51, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 98h:37m:04s remains)
INFO - root - 2019-11-04 00:56:42.309862: step 94760, total loss = 0.49, predict loss = 0.11 (83.9 examples/sec; 0.048 sec/batch; 78h:12m:58s remains)
INFO - root - 2019-11-04 00:56:42.951186: step 94770, total loss = 0.53, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 89h:52m:48s remains)
INFO - root - 2019-11-04 00:56:43.582279: step 94780, total loss = 0.54, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 97h:30m:29s remains)
INFO - root - 2019-11-04 00:56:44.230749: step 94790, total loss = 0.50, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 98h:52m:15s remains)
INFO - root - 2019-11-04 00:56:44.874229: step 94800, total loss = 0.48, predict loss = 0.11 (69.4 examples/sec; 0.058 sec/batch; 94h:33m:34s remains)
INFO - root - 2019-11-04 00:56:45.514646: step 94810, total loss = 0.72, predict loss = 0.17 (66.9 examples/sec; 0.060 sec/batch; 98h:04m:08s remains)
INFO - root - 2019-11-04 00:56:46.148566: step 94820, total loss = 0.57, predict loss = 0.13 (67.1 examples/sec; 0.060 sec/batch; 97h:46m:48s remains)
INFO - root - 2019-11-04 00:56:46.786052: step 94830, total loss = 0.52, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 90h:21m:55s remains)
INFO - root - 2019-11-04 00:56:47.424675: step 94840, total loss = 0.63, predict loss = 0.15 (64.4 examples/sec; 0.062 sec/batch; 101h:48m:17s remains)
INFO - root - 2019-11-04 00:56:48.093222: step 94850, total loss = 0.63, predict loss = 0.15 (59.5 examples/sec; 0.067 sec/batch; 110h:12m:38s remains)
INFO - root - 2019-11-04 00:56:48.713045: step 94860, total loss = 0.61, predict loss = 0.16 (75.8 examples/sec; 0.053 sec/batch; 86h:35m:54s remains)
INFO - root - 2019-11-04 00:56:49.325149: step 94870, total loss = 0.54, predict loss = 0.13 (72.7 examples/sec; 0.055 sec/batch; 90h:13m:10s remains)
INFO - root - 2019-11-04 00:56:49.946767: step 94880, total loss = 0.62, predict loss = 0.15 (64.1 examples/sec; 0.062 sec/batch; 102h:23m:42s remains)
INFO - root - 2019-11-04 00:56:50.607752: step 94890, total loss = 0.57, predict loss = 0.13 (66.3 examples/sec; 0.060 sec/batch; 98h:59m:24s remains)
INFO - root - 2019-11-04 00:56:51.249903: step 94900, total loss = 0.47, predict loss = 0.11 (77.9 examples/sec; 0.051 sec/batch; 84h:12m:20s remains)
INFO - root - 2019-11-04 00:56:51.854084: step 94910, total loss = 0.56, predict loss = 0.13 (84.3 examples/sec; 0.047 sec/batch; 77h:48m:54s remains)
INFO - root - 2019-11-04 00:56:52.459107: step 94920, total loss = 0.50, predict loss = 0.11 (64.8 examples/sec; 0.062 sec/batch; 101h:17m:32s remains)
INFO - root - 2019-11-04 00:56:53.062934: step 94930, total loss = 0.45, predict loss = 0.10 (67.4 examples/sec; 0.059 sec/batch; 97h:24m:27s remains)
INFO - root - 2019-11-04 00:56:53.751567: step 94940, total loss = 0.55, predict loss = 0.13 (61.0 examples/sec; 0.066 sec/batch; 107h:29m:54s remains)
INFO - root - 2019-11-04 00:56:54.465969: step 94950, total loss = 0.61, predict loss = 0.15 (77.9 examples/sec; 0.051 sec/batch; 84h:15m:19s remains)
INFO - root - 2019-11-04 00:56:55.100918: step 94960, total loss = 0.54, predict loss = 0.13 (73.5 examples/sec; 0.054 sec/batch; 89h:16m:06s remains)
INFO - root - 2019-11-04 00:56:55.730096: step 94970, total loss = 0.41, predict loss = 0.10 (78.9 examples/sec; 0.051 sec/batch; 83h:12m:19s remains)
INFO - root - 2019-11-04 00:56:56.372288: step 94980, total loss = 0.48, predict loss = 0.11 (73.5 examples/sec; 0.054 sec/batch; 89h:18m:45s remains)
INFO - root - 2019-11-04 00:56:56.955923: step 94990, total loss = 0.47, predict loss = 0.11 (79.6 examples/sec; 0.050 sec/batch; 82h:24m:51s remains)
INFO - root - 2019-11-04 00:56:57.563196: step 95000, total loss = 0.57, predict loss = 0.14 (71.7 examples/sec; 0.056 sec/batch; 91h:27m:30s remains)
INFO - root - 2019-11-04 00:56:58.188522: step 95010, total loss = 0.51, predict loss = 0.12 (64.0 examples/sec; 0.062 sec/batch; 102h:28m:27s remains)
INFO - root - 2019-11-04 00:56:58.815268: step 95020, total loss = 0.36, predict loss = 0.08 (73.2 examples/sec; 0.055 sec/batch; 89h:38m:51s remains)
INFO - root - 2019-11-04 00:56:59.464529: step 95030, total loss = 0.51, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 95h:27m:51s remains)
INFO - root - 2019-11-04 00:57:00.071546: step 95040, total loss = 0.43, predict loss = 0.10 (66.6 examples/sec; 0.060 sec/batch; 98h:33m:17s remains)
INFO - root - 2019-11-04 00:57:00.763496: step 95050, total loss = 0.43, predict loss = 0.10 (63.4 examples/sec; 0.063 sec/batch; 103h:30m:26s remains)
INFO - root - 2019-11-04 00:57:01.446128: step 95060, total loss = 0.59, predict loss = 0.13 (63.2 examples/sec; 0.063 sec/batch; 103h:47m:56s remains)
INFO - root - 2019-11-04 00:57:02.076484: step 95070, total loss = 0.40, predict loss = 0.09 (67.9 examples/sec; 0.059 sec/batch; 96h:38m:02s remains)
INFO - root - 2019-11-04 00:57:02.741398: step 95080, total loss = 0.54, predict loss = 0.12 (64.5 examples/sec; 0.062 sec/batch; 101h:41m:12s remains)
INFO - root - 2019-11-04 00:57:03.455654: step 95090, total loss = 0.52, predict loss = 0.13 (62.7 examples/sec; 0.064 sec/batch; 104h:36m:54s remains)
INFO - root - 2019-11-04 00:57:04.123769: step 95100, total loss = 0.56, predict loss = 0.13 (65.4 examples/sec; 0.061 sec/batch; 100h:21m:37s remains)
INFO - root - 2019-11-04 00:57:04.818902: step 95110, total loss = 0.53, predict loss = 0.12 (73.4 examples/sec; 0.054 sec/batch; 89h:21m:09s remains)
INFO - root - 2019-11-04 00:57:05.404891: step 95120, total loss = 0.48, predict loss = 0.11 (77.5 examples/sec; 0.052 sec/batch; 84h:38m:25s remains)
INFO - root - 2019-11-04 00:57:05.999010: step 95130, total loss = 0.51, predict loss = 0.12 (77.0 examples/sec; 0.052 sec/batch; 85h:09m:43s remains)
INFO - root - 2019-11-04 00:57:06.633951: step 95140, total loss = 0.41, predict loss = 0.08 (65.0 examples/sec; 0.062 sec/batch; 100h:58m:21s remains)
INFO - root - 2019-11-04 00:57:07.261938: step 95150, total loss = 0.49, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 87h:27m:48s remains)
INFO - root - 2019-11-04 00:57:07.939169: step 95160, total loss = 0.56, predict loss = 0.13 (72.0 examples/sec; 0.056 sec/batch; 91h:06m:38s remains)
INFO - root - 2019-11-04 00:57:08.619261: step 95170, total loss = 0.65, predict loss = 0.16 (81.6 examples/sec; 0.049 sec/batch; 80h:23m:59s remains)
INFO - root - 2019-11-04 00:57:09.282042: step 95180, total loss = 0.46, predict loss = 0.12 (66.6 examples/sec; 0.060 sec/batch; 98h:34m:38s remains)
INFO - root - 2019-11-04 00:57:09.947159: step 95190, total loss = 0.59, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 100h:48m:15s remains)
INFO - root - 2019-11-04 00:57:10.573591: step 95200, total loss = 0.63, predict loss = 0.14 (70.7 examples/sec; 0.057 sec/batch; 92h:46m:04s remains)
INFO - root - 2019-11-04 00:57:11.208452: step 95210, total loss = 0.73, predict loss = 0.17 (71.2 examples/sec; 0.056 sec/batch; 92h:11m:14s remains)
INFO - root - 2019-11-04 00:57:11.860147: step 95220, total loss = 0.67, predict loss = 0.16 (65.4 examples/sec; 0.061 sec/batch; 100h:17m:47s remains)
INFO - root - 2019-11-04 00:57:12.498494: step 95230, total loss = 0.69, predict loss = 0.17 (75.9 examples/sec; 0.053 sec/batch; 86h:29m:24s remains)
INFO - root - 2019-11-04 00:57:13.181989: step 95240, total loss = 0.77, predict loss = 0.18 (63.8 examples/sec; 0.063 sec/batch; 102h:51m:19s remains)
INFO - root - 2019-11-04 00:57:13.887679: step 95250, total loss = 0.61, predict loss = 0.14 (61.6 examples/sec; 0.065 sec/batch; 106h:31m:25s remains)
INFO - root - 2019-11-04 00:57:14.514810: step 95260, total loss = 0.71, predict loss = 0.17 (76.2 examples/sec; 0.053 sec/batch; 86h:08m:11s remains)
INFO - root - 2019-11-04 00:57:15.170242: step 95270, total loss = 0.65, predict loss = 0.15 (79.7 examples/sec; 0.050 sec/batch; 82h:20m:59s remains)
INFO - root - 2019-11-04 00:57:15.812789: step 95280, total loss = 0.71, predict loss = 0.17 (71.4 examples/sec; 0.056 sec/batch; 91h:50m:13s remains)
INFO - root - 2019-11-04 00:57:16.474106: step 95290, total loss = 0.65, predict loss = 0.15 (66.0 examples/sec; 0.061 sec/batch; 99h:28m:27s remains)
INFO - root - 2019-11-04 00:57:17.129704: step 95300, total loss = 0.55, predict loss = 0.13 (76.2 examples/sec; 0.053 sec/batch; 86h:08m:28s remains)
INFO - root - 2019-11-04 00:57:17.797646: step 95310, total loss = 0.58, predict loss = 0.14 (67.6 examples/sec; 0.059 sec/batch; 97h:04m:19s remains)
INFO - root - 2019-11-04 00:57:18.443095: step 95320, total loss = 0.38, predict loss = 0.09 (66.2 examples/sec; 0.060 sec/batch; 99h:02m:24s remains)
INFO - root - 2019-11-04 00:57:19.099356: step 95330, total loss = 0.41, predict loss = 0.09 (73.0 examples/sec; 0.055 sec/batch; 89h:51m:33s remains)
INFO - root - 2019-11-04 00:57:19.748666: step 95340, total loss = 0.69, predict loss = 0.16 (78.5 examples/sec; 0.051 sec/batch; 83h:35m:59s remains)
INFO - root - 2019-11-04 00:57:20.399558: step 95350, total loss = 0.47, predict loss = 0.10 (68.1 examples/sec; 0.059 sec/batch; 96h:17m:46s remains)
INFO - root - 2019-11-04 00:57:21.020689: step 95360, total loss = 0.45, predict loss = 0.10 (66.8 examples/sec; 0.060 sec/batch; 98h:15m:47s remains)
INFO - root - 2019-11-04 00:57:21.658572: step 95370, total loss = 0.56, predict loss = 0.13 (64.2 examples/sec; 0.062 sec/batch; 102h:08m:16s remains)
INFO - root - 2019-11-04 00:57:22.322621: step 95380, total loss = 0.61, predict loss = 0.15 (64.3 examples/sec; 0.062 sec/batch; 102h:01m:26s remains)
INFO - root - 2019-11-04 00:57:22.961169: step 95390, total loss = 0.56, predict loss = 0.13 (78.3 examples/sec; 0.051 sec/batch; 83h:45m:21s remains)
INFO - root - 2019-11-04 00:57:23.620067: step 95400, total loss = 0.56, predict loss = 0.13 (79.0 examples/sec; 0.051 sec/batch; 83h:05m:36s remains)
INFO - root - 2019-11-04 00:57:24.276407: step 95410, total loss = 0.61, predict loss = 0.15 (71.5 examples/sec; 0.056 sec/batch; 91h:46m:42s remains)
INFO - root - 2019-11-04 00:57:24.906849: step 95420, total loss = 0.35, predict loss = 0.07 (64.4 examples/sec; 0.062 sec/batch; 101h:54m:08s remains)
INFO - root - 2019-11-04 00:57:25.534128: step 95430, total loss = 0.48, predict loss = 0.11 (73.4 examples/sec; 0.054 sec/batch; 89h:22m:06s remains)
INFO - root - 2019-11-04 00:57:26.099805: step 95440, total loss = 0.25, predict loss = 0.05 (92.3 examples/sec; 0.043 sec/batch; 71h:02m:58s remains)
INFO - root - 2019-11-04 00:57:26.570560: step 95450, total loss = 0.60, predict loss = 0.13 (86.4 examples/sec; 0.046 sec/batch; 75h:53m:25s remains)
INFO - root - 2019-11-04 00:57:27.577871: step 95460, total loss = 0.47, predict loss = 0.10 (78.5 examples/sec; 0.051 sec/batch; 83h:33m:58s remains)
INFO - root - 2019-11-04 00:57:28.184129: step 95470, total loss = 0.45, predict loss = 0.10 (82.0 examples/sec; 0.049 sec/batch; 79h:58m:42s remains)
INFO - root - 2019-11-04 00:57:28.841989: step 95480, total loss = 0.56, predict loss = 0.13 (67.9 examples/sec; 0.059 sec/batch; 96h:35m:26s remains)
INFO - root - 2019-11-04 00:57:29.500910: step 95490, total loss = 0.50, predict loss = 0.12 (66.4 examples/sec; 0.060 sec/batch; 98h:49m:50s remains)
INFO - root - 2019-11-04 00:57:30.142964: step 95500, total loss = 0.43, predict loss = 0.10 (58.1 examples/sec; 0.069 sec/batch; 112h:52m:37s remains)
INFO - root - 2019-11-04 00:57:30.775839: step 95510, total loss = 0.43, predict loss = 0.10 (82.6 examples/sec; 0.048 sec/batch; 79h:26m:01s remains)
INFO - root - 2019-11-04 00:57:31.402570: step 95520, total loss = 0.38, predict loss = 0.09 (66.8 examples/sec; 0.060 sec/batch; 98h:13m:49s remains)
INFO - root - 2019-11-04 00:57:32.050818: step 95530, total loss = 0.57, predict loss = 0.13 (77.6 examples/sec; 0.052 sec/batch; 84h:33m:16s remains)
INFO - root - 2019-11-04 00:57:32.691536: step 95540, total loss = 0.51, predict loss = 0.11 (65.4 examples/sec; 0.061 sec/batch; 100h:20m:58s remains)
INFO - root - 2019-11-04 00:57:33.340394: step 95550, total loss = 0.66, predict loss = 0.16 (77.1 examples/sec; 0.052 sec/batch; 85h:07m:17s remains)
INFO - root - 2019-11-04 00:57:33.913081: step 95560, total loss = 0.60, predict loss = 0.14 (74.6 examples/sec; 0.054 sec/batch; 87h:57m:59s remains)
INFO - root - 2019-11-04 00:57:34.592644: step 95570, total loss = 0.50, predict loss = 0.12 (56.3 examples/sec; 0.071 sec/batch; 116h:37m:45s remains)
INFO - root - 2019-11-04 00:57:35.237384: step 95580, total loss = 0.54, predict loss = 0.12 (66.3 examples/sec; 0.060 sec/batch; 98h:54m:30s remains)
INFO - root - 2019-11-04 00:57:35.872373: step 95590, total loss = 0.31, predict loss = 0.06 (70.2 examples/sec; 0.057 sec/batch; 93h:27m:11s remains)
INFO - root - 2019-11-04 00:57:36.510403: step 95600, total loss = 0.62, predict loss = 0.14 (69.5 examples/sec; 0.058 sec/batch; 94h:23m:33s remains)
INFO - root - 2019-11-04 00:57:37.168887: step 95610, total loss = 0.56, predict loss = 0.13 (62.1 examples/sec; 0.064 sec/batch; 105h:35m:27s remains)
INFO - root - 2019-11-04 00:57:37.836635: step 95620, total loss = 0.49, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 95h:11m:26s remains)
INFO - root - 2019-11-04 00:57:38.482490: step 95630, total loss = 0.56, predict loss = 0.13 (69.5 examples/sec; 0.058 sec/batch; 94h:24m:02s remains)
INFO - root - 2019-11-04 00:57:39.147186: step 95640, total loss = 0.43, predict loss = 0.09 (66.4 examples/sec; 0.060 sec/batch; 98h:46m:45s remains)
INFO - root - 2019-11-04 00:57:39.761647: step 95650, total loss = 0.38, predict loss = 0.08 (69.8 examples/sec; 0.057 sec/batch; 93h:57m:42s remains)
INFO - root - 2019-11-04 00:57:40.411476: step 95660, total loss = 0.34, predict loss = 0.08 (71.2 examples/sec; 0.056 sec/batch; 92h:08m:04s remains)
INFO - root - 2019-11-04 00:57:41.005461: step 95670, total loss = 0.50, predict loss = 0.11 (70.2 examples/sec; 0.057 sec/batch; 93h:29m:20s remains)
INFO - root - 2019-11-04 00:57:41.625792: step 95680, total loss = 0.42, predict loss = 0.10 (69.8 examples/sec; 0.057 sec/batch; 94h:00m:39s remains)
INFO - root - 2019-11-04 00:57:42.270782: step 95690, total loss = 0.54, predict loss = 0.13 (68.3 examples/sec; 0.059 sec/batch; 96h:02m:24s remains)
INFO - root - 2019-11-04 00:57:42.911235: step 95700, total loss = 0.48, predict loss = 0.10 (69.5 examples/sec; 0.058 sec/batch; 94h:22m:10s remains)
INFO - root - 2019-11-04 00:57:43.584239: step 95710, total loss = 0.63, predict loss = 0.15 (73.9 examples/sec; 0.054 sec/batch; 88h:46m:08s remains)
INFO - root - 2019-11-04 00:57:44.255015: step 95720, total loss = 0.46, predict loss = 0.10 (67.8 examples/sec; 0.059 sec/batch; 96h:48m:45s remains)
INFO - root - 2019-11-04 00:57:44.899757: step 95730, total loss = 0.63, predict loss = 0.14 (73.2 examples/sec; 0.055 sec/batch; 89h:38m:05s remains)
INFO - root - 2019-11-04 00:57:45.525122: step 95740, total loss = 0.61, predict loss = 0.14 (74.8 examples/sec; 0.053 sec/batch; 87h:40m:38s remains)
INFO - root - 2019-11-04 00:57:46.165982: step 95750, total loss = 0.55, predict loss = 0.14 (64.4 examples/sec; 0.062 sec/batch; 101h:53m:32s remains)
INFO - root - 2019-11-04 00:57:46.820298: step 95760, total loss = 0.67, predict loss = 0.16 (73.5 examples/sec; 0.054 sec/batch; 89h:12m:46s remains)
INFO - root - 2019-11-04 00:57:47.488196: step 95770, total loss = 0.56, predict loss = 0.13 (66.7 examples/sec; 0.060 sec/batch; 98h:20m:02s remains)
INFO - root - 2019-11-04 00:57:48.177097: step 95780, total loss = 0.63, predict loss = 0.15 (63.3 examples/sec; 0.063 sec/batch; 103h:33m:37s remains)
INFO - root - 2019-11-04 00:57:48.837826: step 95790, total loss = 0.49, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 92h:52m:30s remains)
INFO - root - 2019-11-04 00:57:49.514525: step 95800, total loss = 0.54, predict loss = 0.12 (61.9 examples/sec; 0.065 sec/batch; 106h:02m:00s remains)
INFO - root - 2019-11-04 00:57:50.179981: step 95810, total loss = 0.58, predict loss = 0.13 (68.7 examples/sec; 0.058 sec/batch; 95h:26m:53s remains)
INFO - root - 2019-11-04 00:57:50.863304: step 95820, total loss = 0.53, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 92h:14m:56s remains)
INFO - root - 2019-11-04 00:57:51.506832: step 95830, total loss = 0.44, predict loss = 0.10 (77.9 examples/sec; 0.051 sec/batch; 84h:10m:32s remains)
INFO - root - 2019-11-04 00:57:52.167266: step 95840, total loss = 0.43, predict loss = 0.09 (76.5 examples/sec; 0.052 sec/batch; 85h:46m:37s remains)
INFO - root - 2019-11-04 00:57:52.812964: step 95850, total loss = 0.59, predict loss = 0.14 (65.8 examples/sec; 0.061 sec/batch; 99h:39m:03s remains)
INFO - root - 2019-11-04 00:57:53.417162: step 95860, total loss = 0.59, predict loss = 0.14 (70.4 examples/sec; 0.057 sec/batch; 93h:07m:19s remains)
INFO - root - 2019-11-04 00:57:54.008577: step 95870, total loss = 0.48, predict loss = 0.12 (72.1 examples/sec; 0.055 sec/batch; 90h:59m:19s remains)
INFO - root - 2019-11-04 00:57:54.631126: step 95880, total loss = 0.42, predict loss = 0.09 (81.0 examples/sec; 0.049 sec/batch; 80h:57m:04s remains)
INFO - root - 2019-11-04 00:57:55.265596: step 95890, total loss = 0.65, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 90h:55m:17s remains)
INFO - root - 2019-11-04 00:57:55.904975: step 95900, total loss = 0.66, predict loss = 0.15 (70.7 examples/sec; 0.057 sec/batch; 92h:49m:45s remains)
INFO - root - 2019-11-04 00:57:56.555425: step 95910, total loss = 0.48, predict loss = 0.11 (64.1 examples/sec; 0.062 sec/batch; 102h:17m:15s remains)
INFO - root - 2019-11-04 00:57:57.163842: step 95920, total loss = 0.63, predict loss = 0.16 (73.9 examples/sec; 0.054 sec/batch; 88h:45m:37s remains)
INFO - root - 2019-11-04 00:57:57.782817: step 95930, total loss = 0.42, predict loss = 0.10 (69.0 examples/sec; 0.058 sec/batch; 95h:05m:08s remains)
INFO - root - 2019-11-04 00:57:58.407183: step 95940, total loss = 0.60, predict loss = 0.15 (65.6 examples/sec; 0.061 sec/batch; 99h:56m:30s remains)
INFO - root - 2019-11-04 00:57:59.055725: step 95950, total loss = 0.38, predict loss = 0.09 (82.9 examples/sec; 0.048 sec/batch; 79h:07m:00s remains)
INFO - root - 2019-11-04 00:57:59.719219: step 95960, total loss = 0.55, predict loss = 0.13 (66.8 examples/sec; 0.060 sec/batch; 98h:09m:58s remains)
INFO - root - 2019-11-04 00:58:00.347553: step 95970, total loss = 0.70, predict loss = 0.17 (78.3 examples/sec; 0.051 sec/batch; 83h:49m:57s remains)
INFO - root - 2019-11-04 00:58:00.992715: step 95980, total loss = 0.59, predict loss = 0.14 (70.7 examples/sec; 0.057 sec/batch; 92h:43m:38s remains)
INFO - root - 2019-11-04 00:58:01.631458: step 95990, total loss = 0.73, predict loss = 0.18 (66.7 examples/sec; 0.060 sec/batch; 98h:19m:42s remains)
INFO - root - 2019-11-04 00:58:02.274763: step 96000, total loss = 0.49, predict loss = 0.12 (65.7 examples/sec; 0.061 sec/batch; 99h:55m:05s remains)
INFO - root - 2019-11-04 00:58:02.882961: step 96010, total loss = 0.58, predict loss = 0.15 (71.5 examples/sec; 0.056 sec/batch; 91h:42m:54s remains)
INFO - root - 2019-11-04 00:58:03.497202: step 96020, total loss = 0.39, predict loss = 0.10 (72.3 examples/sec; 0.055 sec/batch; 90h:41m:18s remains)
INFO - root - 2019-11-04 00:58:04.145807: step 96030, total loss = 0.29, predict loss = 0.06 (66.6 examples/sec; 0.060 sec/batch; 98h:33m:36s remains)
INFO - root - 2019-11-04 00:58:04.862349: step 96040, total loss = 0.38, predict loss = 0.09 (70.7 examples/sec; 0.057 sec/batch; 92h:45m:56s remains)
INFO - root - 2019-11-04 00:58:05.510072: step 96050, total loss = 0.28, predict loss = 0.06 (63.9 examples/sec; 0.063 sec/batch; 102h:43m:45s remains)
INFO - root - 2019-11-04 00:58:06.185175: step 96060, total loss = 0.28, predict loss = 0.06 (61.9 examples/sec; 0.065 sec/batch; 105h:54m:10s remains)
INFO - root - 2019-11-04 00:58:06.906055: step 96070, total loss = 0.38, predict loss = 0.08 (60.2 examples/sec; 0.066 sec/batch; 109h:02m:50s remains)
INFO - root - 2019-11-04 00:58:07.524423: step 96080, total loss = 0.43, predict loss = 0.10 (70.9 examples/sec; 0.056 sec/batch; 92h:28m:22s remains)
INFO - root - 2019-11-04 00:58:08.159845: step 96090, total loss = 0.32, predict loss = 0.07 (64.2 examples/sec; 0.062 sec/batch; 102h:06m:43s remains)
INFO - root - 2019-11-04 00:58:08.802385: step 96100, total loss = 0.26, predict loss = 0.05 (69.4 examples/sec; 0.058 sec/batch; 94h:31m:22s remains)
INFO - root - 2019-11-04 00:58:09.449870: step 96110, total loss = 0.55, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 100h:30m:36s remains)
INFO - root - 2019-11-04 00:58:10.114321: step 96120, total loss = 0.44, predict loss = 0.10 (69.2 examples/sec; 0.058 sec/batch; 94h:44m:15s remains)
INFO - root - 2019-11-04 00:58:10.809920: step 96130, total loss = 0.52, predict loss = 0.12 (61.6 examples/sec; 0.065 sec/batch; 106h:28m:45s remains)
INFO - root - 2019-11-04 00:58:11.451752: step 96140, total loss = 0.49, predict loss = 0.12 (71.5 examples/sec; 0.056 sec/batch; 91h:42m:28s remains)
INFO - root - 2019-11-04 00:58:12.079804: step 96150, total loss = 0.64, predict loss = 0.15 (70.7 examples/sec; 0.057 sec/batch; 92h:43m:12s remains)
INFO - root - 2019-11-04 00:58:12.654783: step 96160, total loss = 0.69, predict loss = 0.16 (74.6 examples/sec; 0.054 sec/batch; 87h:57m:55s remains)
INFO - root - 2019-11-04 00:58:13.270852: step 96170, total loss = 0.67, predict loss = 0.16 (72.5 examples/sec; 0.055 sec/batch; 90h:26m:58s remains)
INFO - root - 2019-11-04 00:58:13.907132: step 96180, total loss = 0.50, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 85h:29m:03s remains)
INFO - root - 2019-11-04 00:58:14.556246: step 96190, total loss = 0.56, predict loss = 0.13 (72.7 examples/sec; 0.055 sec/batch; 90h:13m:43s remains)
INFO - root - 2019-11-04 00:58:15.175092: step 96200, total loss = 0.53, predict loss = 0.13 (69.9 examples/sec; 0.057 sec/batch; 93h:47m:05s remains)
INFO - root - 2019-11-04 00:58:15.808086: step 96210, total loss = 0.40, predict loss = 0.09 (64.1 examples/sec; 0.062 sec/batch; 102h:24m:48s remains)
INFO - root - 2019-11-04 00:58:16.466832: step 96220, total loss = 0.59, predict loss = 0.15 (72.6 examples/sec; 0.055 sec/batch; 90h:21m:18s remains)
INFO - root - 2019-11-04 00:58:17.115699: step 96230, total loss = 0.59, predict loss = 0.14 (64.3 examples/sec; 0.062 sec/batch; 102h:04m:09s remains)
INFO - root - 2019-11-04 00:58:17.794027: step 96240, total loss = 0.50, predict loss = 0.12 (68.2 examples/sec; 0.059 sec/batch; 96h:14m:40s remains)
INFO - root - 2019-11-04 00:58:18.422368: step 96250, total loss = 0.62, predict loss = 0.16 (69.2 examples/sec; 0.058 sec/batch; 94h:47m:57s remains)
INFO - root - 2019-11-04 00:58:19.056697: step 96260, total loss = 0.32, predict loss = 0.08 (71.8 examples/sec; 0.056 sec/batch; 91h:23m:13s remains)
INFO - root - 2019-11-04 00:58:19.695649: step 96270, total loss = 0.53, predict loss = 0.14 (65.8 examples/sec; 0.061 sec/batch; 99h:46m:01s remains)
INFO - root - 2019-11-04 00:58:20.375137: step 96280, total loss = 0.54, predict loss = 0.13 (77.9 examples/sec; 0.051 sec/batch; 84h:13m:21s remains)
INFO - root - 2019-11-04 00:58:21.061707: step 96290, total loss = 0.48, predict loss = 0.10 (57.3 examples/sec; 0.070 sec/batch; 114h:31m:56s remains)
INFO - root - 2019-11-04 00:58:21.743444: step 96300, total loss = 0.33, predict loss = 0.07 (76.8 examples/sec; 0.052 sec/batch; 85h:26m:40s remains)
INFO - root - 2019-11-04 00:58:22.374439: step 96310, total loss = 0.27, predict loss = 0.06 (79.3 examples/sec; 0.050 sec/batch; 82h:44m:08s remains)
INFO - root - 2019-11-04 00:58:23.034747: step 96320, total loss = 0.28, predict loss = 0.06 (66.4 examples/sec; 0.060 sec/batch; 98h:46m:17s remains)
INFO - root - 2019-11-04 00:58:23.649826: step 96330, total loss = 0.40, predict loss = 0.09 (74.0 examples/sec; 0.054 sec/batch; 88h:39m:16s remains)
INFO - root - 2019-11-04 00:58:24.282223: step 96340, total loss = 0.31, predict loss = 0.07 (69.3 examples/sec; 0.058 sec/batch; 94h:43m:19s remains)
INFO - root - 2019-11-04 00:58:24.947805: step 96350, total loss = 0.36, predict loss = 0.08 (75.5 examples/sec; 0.053 sec/batch; 86h:54m:26s remains)
INFO - root - 2019-11-04 00:58:25.631033: step 96360, total loss = 0.33, predict loss = 0.07 (59.8 examples/sec; 0.067 sec/batch; 109h:38m:44s remains)
INFO - root - 2019-11-04 00:58:26.237825: step 96370, total loss = 0.43, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 94h:26m:23s remains)
INFO - root - 2019-11-04 00:58:26.874091: step 96380, total loss = 0.48, predict loss = 0.11 (68.8 examples/sec; 0.058 sec/batch; 95h:24m:24s remains)
INFO - root - 2019-11-04 00:58:27.546616: step 96390, total loss = 0.45, predict loss = 0.10 (73.0 examples/sec; 0.055 sec/batch; 89h:49m:03s remains)
INFO - root - 2019-11-04 00:58:28.220818: step 96400, total loss = 0.42, predict loss = 0.09 (65.7 examples/sec; 0.061 sec/batch; 99h:46m:48s remains)
INFO - root - 2019-11-04 00:58:28.897552: step 96410, total loss = 0.38, predict loss = 0.09 (65.0 examples/sec; 0.062 sec/batch; 100h:56m:34s remains)
INFO - root - 2019-11-04 00:58:29.578133: step 96420, total loss = 0.34, predict loss = 0.07 (65.6 examples/sec; 0.061 sec/batch; 99h:55m:34s remains)
INFO - root - 2019-11-04 00:58:30.226724: step 96430, total loss = 0.38, predict loss = 0.09 (61.9 examples/sec; 0.065 sec/batch; 105h:55m:04s remains)
INFO - root - 2019-11-04 00:58:30.950556: step 96440, total loss = 0.38, predict loss = 0.08 (63.7 examples/sec; 0.063 sec/batch; 102h:58m:35s remains)
INFO - root - 2019-11-04 00:58:31.624121: step 96450, total loss = 0.26, predict loss = 0.05 (69.9 examples/sec; 0.057 sec/batch; 93h:46m:51s remains)
INFO - root - 2019-11-04 00:58:32.273015: step 96460, total loss = 0.50, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 91h:54m:15s remains)
INFO - root - 2019-11-04 00:58:32.960371: step 96470, total loss = 0.42, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 99h:28m:38s remains)
INFO - root - 2019-11-04 00:58:33.673111: step 96480, total loss = 0.50, predict loss = 0.12 (58.0 examples/sec; 0.069 sec/batch; 113h:00m:40s remains)
INFO - root - 2019-11-04 00:58:34.331038: step 96490, total loss = 0.62, predict loss = 0.15 (73.8 examples/sec; 0.054 sec/batch; 88h:55m:51s remains)
INFO - root - 2019-11-04 00:58:34.993232: step 96500, total loss = 0.37, predict loss = 0.09 (78.4 examples/sec; 0.051 sec/batch; 83h:42m:44s remains)
INFO - root - 2019-11-04 00:58:35.660362: step 96510, total loss = 0.34, predict loss = 0.08 (64.0 examples/sec; 0.062 sec/batch; 102h:27m:14s remains)
INFO - root - 2019-11-04 00:58:36.289285: step 96520, total loss = 0.35, predict loss = 0.07 (79.5 examples/sec; 0.050 sec/batch; 82h:31m:26s remains)
INFO - root - 2019-11-04 00:58:36.928141: step 96530, total loss = 0.28, predict loss = 0.06 (67.7 examples/sec; 0.059 sec/batch; 96h:52m:01s remains)
INFO - root - 2019-11-04 00:58:37.586507: step 96540, total loss = 0.24, predict loss = 0.05 (61.6 examples/sec; 0.065 sec/batch; 106h:28m:25s remains)
INFO - root - 2019-11-04 00:58:38.205710: step 96550, total loss = 0.14, predict loss = 0.03 (73.9 examples/sec; 0.054 sec/batch; 88h:47m:05s remains)
INFO - root - 2019-11-04 00:58:38.844534: step 96560, total loss = 0.44, predict loss = 0.10 (65.3 examples/sec; 0.061 sec/batch; 100h:26m:22s remains)
INFO - root - 2019-11-04 00:58:39.467553: step 96570, total loss = 0.31, predict loss = 0.07 (69.0 examples/sec; 0.058 sec/batch; 95h:06m:39s remains)
INFO - root - 2019-11-04 00:58:40.097802: step 96580, total loss = 0.40, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 92h:30m:12s remains)
INFO - root - 2019-11-04 00:58:40.706680: step 96590, total loss = 0.43, predict loss = 0.10 (75.4 examples/sec; 0.053 sec/batch; 86h:56m:48s remains)
INFO - root - 2019-11-04 00:58:41.321222: step 96600, total loss = 0.51, predict loss = 0.12 (74.4 examples/sec; 0.054 sec/batch; 88h:08m:47s remains)
INFO - root - 2019-11-04 00:58:41.917366: step 96610, total loss = 0.39, predict loss = 0.09 (74.6 examples/sec; 0.054 sec/batch; 87h:56m:52s remains)
INFO - root - 2019-11-04 00:58:42.544371: step 96620, total loss = 0.49, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 93h:29m:29s remains)
INFO - root - 2019-11-04 00:58:43.140224: step 96630, total loss = 0.41, predict loss = 0.10 (74.3 examples/sec; 0.054 sec/batch; 88h:16m:38s remains)
INFO - root - 2019-11-04 00:58:43.775320: step 96640, total loss = 0.37, predict loss = 0.09 (62.2 examples/sec; 0.064 sec/batch; 105h:31m:07s remains)
INFO - root - 2019-11-04 00:58:44.455686: step 96650, total loss = 0.34, predict loss = 0.07 (72.2 examples/sec; 0.055 sec/batch; 90h:53m:59s remains)
INFO - root - 2019-11-04 00:58:45.098838: step 96660, total loss = 0.47, predict loss = 0.11 (70.2 examples/sec; 0.057 sec/batch; 93h:24m:37s remains)
INFO - root - 2019-11-04 00:58:45.742348: step 96670, total loss = 0.45, predict loss = 0.10 (70.3 examples/sec; 0.057 sec/batch; 93h:19m:45s remains)
INFO - root - 2019-11-04 00:58:46.418616: step 96680, total loss = 0.40, predict loss = 0.09 (59.2 examples/sec; 0.068 sec/batch; 110h:46m:04s remains)
INFO - root - 2019-11-04 00:58:47.076249: step 96690, total loss = 0.33, predict loss = 0.07 (63.5 examples/sec; 0.063 sec/batch; 103h:13m:59s remains)
INFO - root - 2019-11-04 00:58:47.710689: step 96700, total loss = 0.40, predict loss = 0.09 (67.1 examples/sec; 0.060 sec/batch; 97h:48m:26s remains)
INFO - root - 2019-11-04 00:58:48.370183: step 96710, total loss = 0.47, predict loss = 0.10 (64.5 examples/sec; 0.062 sec/batch; 101h:43m:24s remains)
INFO - root - 2019-11-04 00:58:49.068724: step 96720, total loss = 0.38, predict loss = 0.09 (63.8 examples/sec; 0.063 sec/batch; 102h:44m:15s remains)
INFO - root - 2019-11-04 00:58:49.693018: step 96730, total loss = 0.64, predict loss = 0.16 (73.2 examples/sec; 0.055 sec/batch; 89h:33m:53s remains)
INFO - root - 2019-11-04 00:58:50.304487: step 96740, total loss = 0.56, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 100h:26m:04s remains)
INFO - root - 2019-11-04 00:58:50.891569: step 96750, total loss = 0.53, predict loss = 0.12 (79.1 examples/sec; 0.051 sec/batch; 82h:54m:45s remains)
INFO - root - 2019-11-04 00:58:51.530667: step 96760, total loss = 0.59, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 94h:48m:24s remains)
INFO - root - 2019-11-04 00:58:52.130332: step 96770, total loss = 0.57, predict loss = 0.12 (82.0 examples/sec; 0.049 sec/batch; 79h:58m:25s remains)
INFO - root - 2019-11-04 00:58:52.723241: step 96780, total loss = 0.50, predict loss = 0.11 (72.9 examples/sec; 0.055 sec/batch; 90h:01m:58s remains)
INFO - root - 2019-11-04 00:58:53.344705: step 96790, total loss = 0.30, predict loss = 0.06 (66.4 examples/sec; 0.060 sec/batch; 98h:49m:25s remains)
INFO - root - 2019-11-04 00:58:54.025383: step 96800, total loss = 0.32, predict loss = 0.07 (69.6 examples/sec; 0.057 sec/batch; 94h:12m:09s remains)
INFO - root - 2019-11-04 00:58:54.669024: step 96810, total loss = 0.29, predict loss = 0.06 (63.7 examples/sec; 0.063 sec/batch; 102h:55m:51s remains)
INFO - root - 2019-11-04 00:58:55.340711: step 96820, total loss = 0.25, predict loss = 0.05 (68.0 examples/sec; 0.059 sec/batch; 96h:25m:44s remains)
INFO - root - 2019-11-04 00:58:56.000456: step 96830, total loss = 0.46, predict loss = 0.11 (68.2 examples/sec; 0.059 sec/batch; 96h:07m:39s remains)
INFO - root - 2019-11-04 00:58:56.683527: step 96840, total loss = 0.52, predict loss = 0.12 (67.6 examples/sec; 0.059 sec/batch; 96h:58m:21s remains)
INFO - root - 2019-11-04 00:58:57.381129: step 96850, total loss = 0.61, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 100h:42m:35s remains)
INFO - root - 2019-11-04 00:58:58.027666: step 96860, total loss = 0.47, predict loss = 0.11 (70.8 examples/sec; 0.057 sec/batch; 92h:40m:10s remains)
INFO - root - 2019-11-04 00:58:58.637916: step 96870, total loss = 0.61, predict loss = 0.14 (78.6 examples/sec; 0.051 sec/batch; 83h:26m:38s remains)
INFO - root - 2019-11-04 00:58:59.234056: step 96880, total loss = 0.52, predict loss = 0.12 (86.0 examples/sec; 0.047 sec/batch; 76h:15m:45s remains)
INFO - root - 2019-11-04 00:58:59.855860: step 96890, total loss = 0.68, predict loss = 0.16 (80.9 examples/sec; 0.049 sec/batch; 81h:05m:51s remains)
INFO - root - 2019-11-04 00:59:00.503844: step 96900, total loss = 0.62, predict loss = 0.15 (63.3 examples/sec; 0.063 sec/batch; 103h:40m:41s remains)
INFO - root - 2019-11-04 00:59:01.149135: step 96910, total loss = 0.66, predict loss = 0.15 (69.7 examples/sec; 0.057 sec/batch; 94h:08m:02s remains)
INFO - root - 2019-11-04 00:59:01.805233: step 96920, total loss = 0.68, predict loss = 0.15 (65.8 examples/sec; 0.061 sec/batch; 99h:40m:00s remains)
INFO - root - 2019-11-04 00:59:02.482243: step 96930, total loss = 0.84, predict loss = 0.19 (64.9 examples/sec; 0.062 sec/batch; 101h:07m:18s remains)
INFO - root - 2019-11-04 00:59:03.181761: step 96940, total loss = 0.59, predict loss = 0.14 (76.4 examples/sec; 0.052 sec/batch; 85h:49m:10s remains)
INFO - root - 2019-11-04 00:59:03.847636: step 96950, total loss = 0.49, predict loss = 0.12 (69.5 examples/sec; 0.058 sec/batch; 94h:20m:41s remains)
INFO - root - 2019-11-04 00:59:04.546484: step 96960, total loss = 0.75, predict loss = 0.18 (53.3 examples/sec; 0.075 sec/batch; 122h:59m:22s remains)
INFO - root - 2019-11-04 00:59:05.208968: step 96970, total loss = 0.70, predict loss = 0.16 (65.0 examples/sec; 0.062 sec/batch; 100h:57m:51s remains)
INFO - root - 2019-11-04 00:59:05.897890: step 96980, total loss = 0.53, predict loss = 0.12 (77.9 examples/sec; 0.051 sec/batch; 84h:08m:50s remains)
INFO - root - 2019-11-04 00:59:06.528996: step 96990, total loss = 0.51, predict loss = 0.12 (79.6 examples/sec; 0.050 sec/batch; 82h:25m:05s remains)
INFO - root - 2019-11-04 00:59:07.143578: step 97000, total loss = 0.48, predict loss = 0.11 (79.6 examples/sec; 0.050 sec/batch; 82h:25m:17s remains)
INFO - root - 2019-11-04 00:59:07.779979: step 97010, total loss = 0.49, predict loss = 0.11 (64.6 examples/sec; 0.062 sec/batch; 101h:33m:48s remains)
INFO - root - 2019-11-04 00:59:08.449429: step 97020, total loss = 0.51, predict loss = 0.12 (64.2 examples/sec; 0.062 sec/batch; 102h:12m:47s remains)
INFO - root - 2019-11-04 00:59:09.104085: step 97030, total loss = 0.48, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 98h:35m:18s remains)
INFO - root - 2019-11-04 00:59:09.779046: step 97040, total loss = 0.53, predict loss = 0.12 (66.8 examples/sec; 0.060 sec/batch; 98h:15m:35s remains)
INFO - root - 2019-11-04 00:59:10.419388: step 97050, total loss = 0.46, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 92h:50m:39s remains)
INFO - root - 2019-11-04 00:59:11.100093: step 97060, total loss = 0.47, predict loss = 0.11 (62.3 examples/sec; 0.064 sec/batch; 105h:21m:09s remains)
INFO - root - 2019-11-04 00:59:11.736603: step 97070, total loss = 0.42, predict loss = 0.10 (73.4 examples/sec; 0.055 sec/batch; 89h:22m:56s remains)
INFO - root - 2019-11-04 00:59:12.377225: step 97080, total loss = 0.42, predict loss = 0.10 (69.6 examples/sec; 0.058 sec/batch; 94h:17m:35s remains)
INFO - root - 2019-11-04 00:59:13.040085: step 97090, total loss = 0.52, predict loss = 0.11 (64.5 examples/sec; 0.062 sec/batch; 101h:41m:23s remains)
INFO - root - 2019-11-04 00:59:13.666613: step 97100, total loss = 0.50, predict loss = 0.11 (72.0 examples/sec; 0.056 sec/batch; 91h:03m:56s remains)
INFO - root - 2019-11-04 00:59:14.296774: step 97110, total loss = 0.51, predict loss = 0.12 (82.9 examples/sec; 0.048 sec/batch; 79h:07m:12s remains)
INFO - root - 2019-11-04 00:59:14.931664: step 97120, total loss = 0.45, predict loss = 0.10 (72.5 examples/sec; 0.055 sec/batch; 90h:26m:27s remains)
INFO - root - 2019-11-04 00:59:15.547192: step 97130, total loss = 0.42, predict loss = 0.10 (75.9 examples/sec; 0.053 sec/batch; 86h:24m:48s remains)
INFO - root - 2019-11-04 00:59:16.185667: step 97140, total loss = 0.47, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 92h:18m:25s remains)
INFO - root - 2019-11-04 00:59:16.854687: step 97150, total loss = 0.41, predict loss = 0.09 (59.7 examples/sec; 0.067 sec/batch; 109h:49m:19s remains)
INFO - root - 2019-11-04 00:59:17.518506: step 97160, total loss = 0.43, predict loss = 0.10 (66.3 examples/sec; 0.060 sec/batch; 98h:53m:53s remains)
INFO - root - 2019-11-04 00:59:18.153215: step 97170, total loss = 0.30, predict loss = 0.07 (73.9 examples/sec; 0.054 sec/batch; 88h:44m:03s remains)
INFO - root - 2019-11-04 00:59:18.803077: step 97180, total loss = 0.44, predict loss = 0.10 (74.2 examples/sec; 0.054 sec/batch; 88h:24m:21s remains)
INFO - root - 2019-11-04 00:59:19.426152: step 97190, total loss = 0.44, predict loss = 0.10 (78.1 examples/sec; 0.051 sec/batch; 83h:56m:12s remains)
INFO - root - 2019-11-04 00:59:20.093152: step 97200, total loss = 0.50, predict loss = 0.12 (77.4 examples/sec; 0.052 sec/batch; 84h:43m:51s remains)
INFO - root - 2019-11-04 00:59:20.730435: step 97210, total loss = 0.48, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 96h:51m:28s remains)
INFO - root - 2019-11-04 00:59:21.373230: step 97220, total loss = 0.47, predict loss = 0.10 (69.2 examples/sec; 0.058 sec/batch; 94h:42m:45s remains)
INFO - root - 2019-11-04 00:59:21.967906: step 97230, total loss = 0.48, predict loss = 0.11 (79.2 examples/sec; 0.050 sec/batch; 82h:45m:40s remains)
INFO - root - 2019-11-04 00:59:22.612889: step 97240, total loss = 0.50, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 99h:03m:41s remains)
INFO - root - 2019-11-04 00:59:23.217992: step 97250, total loss = 0.50, predict loss = 0.11 (70.9 examples/sec; 0.056 sec/batch; 92h:27m:13s remains)
INFO - root - 2019-11-04 00:59:23.837162: step 97260, total loss = 0.46, predict loss = 0.10 (71.6 examples/sec; 0.056 sec/batch; 91h:36m:27s remains)
INFO - root - 2019-11-04 00:59:24.475372: step 97270, total loss = 0.60, predict loss = 0.13 (70.9 examples/sec; 0.056 sec/batch; 92h:33m:29s remains)
INFO - root - 2019-11-04 00:59:25.156220: step 97280, total loss = 0.55, predict loss = 0.14 (71.2 examples/sec; 0.056 sec/batch; 92h:07m:01s remains)
INFO - root - 2019-11-04 00:59:25.765138: step 97290, total loss = 0.51, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 93h:24m:04s remains)
INFO - root - 2019-11-04 00:59:26.390553: step 97300, total loss = 0.58, predict loss = 0.13 (82.6 examples/sec; 0.048 sec/batch; 79h:24m:26s remains)
INFO - root - 2019-11-04 00:59:27.019186: step 97310, total loss = 0.57, predict loss = 0.13 (61.4 examples/sec; 0.065 sec/batch; 106h:49m:16s remains)
INFO - root - 2019-11-04 00:59:27.654783: step 97320, total loss = 0.59, predict loss = 0.13 (64.2 examples/sec; 0.062 sec/batch; 102h:06m:12s remains)
INFO - root - 2019-11-04 00:59:28.312235: step 97330, total loss = 0.69, predict loss = 0.17 (67.7 examples/sec; 0.059 sec/batch; 96h:50m:47s remains)
INFO - root - 2019-11-04 00:59:28.941606: step 97340, total loss = 0.63, predict loss = 0.15 (66.7 examples/sec; 0.060 sec/batch; 98h:23m:22s remains)
INFO - root - 2019-11-04 00:59:29.531394: step 97350, total loss = 0.53, predict loss = 0.12 (79.4 examples/sec; 0.050 sec/batch; 82h:34m:03s remains)
INFO - root - 2019-11-04 00:59:30.166412: step 97360, total loss = 0.59, predict loss = 0.13 (68.7 examples/sec; 0.058 sec/batch; 95h:30m:59s remains)
INFO - root - 2019-11-04 00:59:30.797492: step 97370, total loss = 0.61, predict loss = 0.14 (67.8 examples/sec; 0.059 sec/batch; 96h:47m:04s remains)
INFO - root - 2019-11-04 00:59:31.462669: step 97380, total loss = 0.59, predict loss = 0.14 (77.4 examples/sec; 0.052 sec/batch; 84h:42m:44s remains)
INFO - root - 2019-11-04 00:59:32.097950: step 97390, total loss = 0.55, predict loss = 0.13 (73.8 examples/sec; 0.054 sec/batch; 88h:50m:21s remains)
INFO - root - 2019-11-04 00:59:32.762935: step 97400, total loss = 0.62, predict loss = 0.15 (69.0 examples/sec; 0.058 sec/batch; 95h:03m:12s remains)
INFO - root - 2019-11-04 00:59:33.430257: step 97410, total loss = 0.47, predict loss = 0.11 (61.2 examples/sec; 0.065 sec/batch; 107h:10m:09s remains)
INFO - root - 2019-11-04 00:59:34.054243: step 97420, total loss = 0.31, predict loss = 0.07 (68.6 examples/sec; 0.058 sec/batch; 95h:38m:50s remains)
INFO - root - 2019-11-04 00:59:34.750398: step 97430, total loss = 0.53, predict loss = 0.12 (73.3 examples/sec; 0.055 sec/batch; 89h:31m:55s remains)
INFO - root - 2019-11-04 00:59:35.346018: step 97440, total loss = 0.37, predict loss = 0.09 (76.3 examples/sec; 0.052 sec/batch; 85h:55m:38s remains)
INFO - root - 2019-11-04 00:59:35.953913: step 97450, total loss = 0.46, predict loss = 0.11 (65.8 examples/sec; 0.061 sec/batch; 99h:44m:09s remains)
INFO - root - 2019-11-04 00:59:36.565918: step 97460, total loss = 0.33, predict loss = 0.07 (71.1 examples/sec; 0.056 sec/batch; 92h:12m:25s remains)
INFO - root - 2019-11-04 00:59:37.174340: step 97470, total loss = 0.56, predict loss = 0.13 (71.9 examples/sec; 0.056 sec/batch; 91h:10m:16s remains)
INFO - root - 2019-11-04 00:59:37.782882: step 97480, total loss = 0.42, predict loss = 0.10 (70.8 examples/sec; 0.056 sec/batch; 92h:36m:22s remains)
INFO - root - 2019-11-04 00:59:38.396533: step 97490, total loss = 0.51, predict loss = 0.12 (62.5 examples/sec; 0.064 sec/batch; 104h:51m:11s remains)
INFO - root - 2019-11-04 00:59:39.091852: step 97500, total loss = 0.36, predict loss = 0.08 (62.1 examples/sec; 0.064 sec/batch; 105h:40m:58s remains)
INFO - root - 2019-11-04 00:59:39.732138: step 97510, total loss = 0.46, predict loss = 0.11 (64.6 examples/sec; 0.062 sec/batch; 101h:29m:43s remains)
INFO - root - 2019-11-04 00:59:40.381945: step 97520, total loss = 0.62, predict loss = 0.14 (66.6 examples/sec; 0.060 sec/batch; 98h:30m:53s remains)
INFO - root - 2019-11-04 00:59:41.019304: step 97530, total loss = 0.64, predict loss = 0.16 (73.0 examples/sec; 0.055 sec/batch; 89h:51m:06s remains)
INFO - root - 2019-11-04 00:59:41.673924: step 97540, total loss = 0.56, predict loss = 0.13 (63.8 examples/sec; 0.063 sec/batch; 102h:50m:08s remains)
INFO - root - 2019-11-04 00:59:42.324741: step 97550, total loss = 0.60, predict loss = 0.15 (74.7 examples/sec; 0.054 sec/batch; 87h:49m:19s remains)
INFO - root - 2019-11-04 00:59:42.943586: step 97560, total loss = 0.46, predict loss = 0.11 (74.8 examples/sec; 0.053 sec/batch; 87h:38m:53s remains)
INFO - root - 2019-11-04 00:59:43.615459: step 97570, total loss = 0.63, predict loss = 0.15 (64.8 examples/sec; 0.062 sec/batch; 101h:08m:34s remains)
INFO - root - 2019-11-04 00:59:44.261016: step 97580, total loss = 0.53, predict loss = 0.12 (74.7 examples/sec; 0.054 sec/batch; 87h:47m:43s remains)
INFO - root - 2019-11-04 00:59:44.906275: step 97590, total loss = 0.60, predict loss = 0.15 (69.7 examples/sec; 0.057 sec/batch; 94h:05m:26s remains)
INFO - root - 2019-11-04 00:59:45.507872: step 97600, total loss = 0.47, predict loss = 0.11 (68.4 examples/sec; 0.059 sec/batch; 95h:55m:44s remains)
INFO - root - 2019-11-04 00:59:46.130167: step 97610, total loss = 0.53, predict loss = 0.13 (64.4 examples/sec; 0.062 sec/batch; 101h:54m:34s remains)
INFO - root - 2019-11-04 00:59:46.766322: step 97620, total loss = 0.57, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 95h:27m:49s remains)
INFO - root - 2019-11-04 00:59:47.383781: step 97630, total loss = 0.64, predict loss = 0.16 (64.4 examples/sec; 0.062 sec/batch; 101h:52m:57s remains)
INFO - root - 2019-11-04 00:59:48.049919: step 97640, total loss = 0.46, predict loss = 0.11 (60.9 examples/sec; 0.066 sec/batch; 107h:37m:41s remains)
INFO - root - 2019-11-04 00:59:48.732967: step 97650, total loss = 0.47, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 96h:17m:37s remains)
INFO - root - 2019-11-04 00:59:49.412283: step 97660, total loss = 0.44, predict loss = 0.11 (70.9 examples/sec; 0.056 sec/batch; 92h:27m:01s remains)
INFO - root - 2019-11-04 00:59:50.048045: step 97670, total loss = 0.54, predict loss = 0.13 (69.0 examples/sec; 0.058 sec/batch; 94h:58m:41s remains)
INFO - root - 2019-11-04 00:59:50.742122: step 97680, total loss = 0.47, predict loss = 0.11 (59.5 examples/sec; 0.067 sec/batch; 110h:16m:10s remains)
INFO - root - 2019-11-04 00:59:51.359010: step 97690, total loss = 0.46, predict loss = 0.11 (81.7 examples/sec; 0.049 sec/batch; 80h:18m:44s remains)
INFO - root - 2019-11-04 00:59:51.996004: step 97700, total loss = 0.50, predict loss = 0.12 (67.5 examples/sec; 0.059 sec/batch; 97h:12m:28s remains)
INFO - root - 2019-11-04 00:59:52.655024: step 97710, total loss = 0.45, predict loss = 0.11 (78.1 examples/sec; 0.051 sec/batch; 83h:55m:47s remains)
INFO - root - 2019-11-04 00:59:53.291141: step 97720, total loss = 0.39, predict loss = 0.09 (72.3 examples/sec; 0.055 sec/batch; 90h:40m:31s remains)
INFO - root - 2019-11-04 00:59:53.887738: step 97730, total loss = 0.34, predict loss = 0.07 (78.7 examples/sec; 0.051 sec/batch; 83h:22m:05s remains)
INFO - root - 2019-11-04 00:59:54.506938: step 97740, total loss = 0.40, predict loss = 0.09 (68.7 examples/sec; 0.058 sec/batch; 95h:27m:54s remains)
INFO - root - 2019-11-04 00:59:55.150229: step 97750, total loss = 0.45, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 99h:34m:34s remains)
INFO - root - 2019-11-04 00:59:55.804252: step 97760, total loss = 0.45, predict loss = 0.10 (71.9 examples/sec; 0.056 sec/batch; 91h:13m:09s remains)
INFO - root - 2019-11-04 00:59:56.408293: step 97770, total loss = 0.50, predict loss = 0.11 (82.5 examples/sec; 0.048 sec/batch; 79h:28m:53s remains)
INFO - root - 2019-11-04 00:59:56.998136: step 97780, total loss = 0.39, predict loss = 0.08 (74.1 examples/sec; 0.054 sec/batch; 88h:30m:17s remains)
INFO - root - 2019-11-04 00:59:57.641632: step 97790, total loss = 0.58, predict loss = 0.13 (62.1 examples/sec; 0.064 sec/batch; 105h:39m:01s remains)
INFO - root - 2019-11-04 00:59:58.315179: step 97800, total loss = 0.50, predict loss = 0.11 (63.9 examples/sec; 0.063 sec/batch; 102h:34m:56s remains)
INFO - root - 2019-11-04 00:59:58.974614: step 97810, total loss = 0.47, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 94h:35m:43s remains)
INFO - root - 2019-11-04 00:59:59.638196: step 97820, total loss = 0.42, predict loss = 0.09 (69.0 examples/sec; 0.058 sec/batch; 95h:04m:16s remains)
INFO - root - 2019-11-04 01:00:00.312329: step 97830, total loss = 0.59, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 94h:09m:21s remains)
INFO - root - 2019-11-04 01:00:00.937978: step 97840, total loss = 0.46, predict loss = 0.10 (66.9 examples/sec; 0.060 sec/batch; 97h:57m:26s remains)
INFO - root - 2019-11-04 01:00:01.610106: step 97850, total loss = 0.42, predict loss = 0.09 (65.0 examples/sec; 0.062 sec/batch; 100h:53m:58s remains)
INFO - root - 2019-11-04 01:00:02.214893: step 97860, total loss = 0.50, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 92h:20m:54s remains)
INFO - root - 2019-11-04 01:00:02.811924: step 97870, total loss = 0.41, predict loss = 0.09 (74.4 examples/sec; 0.054 sec/batch; 88h:09m:01s remains)
INFO - root - 2019-11-04 01:00:03.435795: step 97880, total loss = 0.44, predict loss = 0.10 (69.8 examples/sec; 0.057 sec/batch; 94h:00m:46s remains)
INFO - root - 2019-11-04 01:00:04.054588: step 97890, total loss = 0.72, predict loss = 0.17 (67.7 examples/sec; 0.059 sec/batch; 96h:49m:53s remains)
INFO - root - 2019-11-04 01:00:04.726044: step 97900, total loss = 0.44, predict loss = 0.10 (82.5 examples/sec; 0.048 sec/batch; 79h:26m:50s remains)
INFO - root - 2019-11-04 01:00:05.319314: step 97910, total loss = 0.80, predict loss = 0.19 (77.1 examples/sec; 0.052 sec/batch; 85h:02m:06s remains)
INFO - root - 2019-11-04 01:00:05.895341: step 97920, total loss = 0.53, predict loss = 0.12 (80.1 examples/sec; 0.050 sec/batch; 81h:49m:21s remains)
INFO - root - 2019-11-04 01:00:06.484086: step 97930, total loss = 0.54, predict loss = 0.12 (76.0 examples/sec; 0.053 sec/batch; 86h:14m:50s remains)
INFO - root - 2019-11-04 01:00:07.061039: step 97940, total loss = 0.66, predict loss = 0.16 (72.6 examples/sec; 0.055 sec/batch; 90h:21m:09s remains)
INFO - root - 2019-11-04 01:00:07.668257: step 97950, total loss = 0.57, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 94h:51m:57s remains)
INFO - root - 2019-11-04 01:00:08.327320: step 97960, total loss = 0.55, predict loss = 0.13 (58.8 examples/sec; 0.068 sec/batch; 111h:32m:18s remains)
INFO - root - 2019-11-04 01:00:08.975570: step 97970, total loss = 0.45, predict loss = 0.10 (66.2 examples/sec; 0.060 sec/batch; 99h:05m:38s remains)
INFO - root - 2019-11-04 01:00:09.601915: step 97980, total loss = 0.67, predict loss = 0.16 (78.0 examples/sec; 0.051 sec/batch; 84h:04m:46s remains)
INFO - root - 2019-11-04 01:00:10.233353: step 97990, total loss = 0.68, predict loss = 0.16 (69.8 examples/sec; 0.057 sec/batch; 93h:59m:45s remains)
INFO - root - 2019-11-04 01:00:10.840742: step 98000, total loss = 0.59, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 92h:22m:19s remains)
INFO - root - 2019-11-04 01:00:11.431639: step 98010, total loss = 0.53, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 94h:55m:27s remains)
INFO - root - 2019-11-04 01:00:12.049840: step 98020, total loss = 0.66, predict loss = 0.16 (68.6 examples/sec; 0.058 sec/batch; 95h:35m:44s remains)
INFO - root - 2019-11-04 01:00:12.719149: step 98030, total loss = 0.54, predict loss = 0.12 (64.6 examples/sec; 0.062 sec/batch; 101h:29m:04s remains)
INFO - root - 2019-11-04 01:00:13.374185: step 98040, total loss = 0.46, predict loss = 0.10 (71.9 examples/sec; 0.056 sec/batch; 91h:14m:18s remains)
INFO - root - 2019-11-04 01:00:14.013487: step 98050, total loss = 0.36, predict loss = 0.07 (65.6 examples/sec; 0.061 sec/batch; 100h:01m:51s remains)
INFO - root - 2019-11-04 01:00:14.681955: step 98060, total loss = 0.47, predict loss = 0.11 (60.1 examples/sec; 0.067 sec/batch; 109h:07m:21s remains)
INFO - root - 2019-11-04 01:00:15.291848: step 98070, total loss = 0.44, predict loss = 0.10 (76.2 examples/sec; 0.052 sec/batch; 86h:03m:47s remains)
INFO - root - 2019-11-04 01:00:15.877176: step 98080, total loss = 0.34, predict loss = 0.07 (72.2 examples/sec; 0.055 sec/batch; 90h:50m:45s remains)
INFO - root - 2019-11-04 01:00:16.486346: step 98090, total loss = 0.51, predict loss = 0.12 (73.3 examples/sec; 0.055 sec/batch; 89h:24m:32s remains)
INFO - root - 2019-11-04 01:00:17.090251: step 98100, total loss = 0.47, predict loss = 0.11 (63.0 examples/sec; 0.064 sec/batch; 104h:10m:08s remains)
INFO - root - 2019-11-04 01:00:17.754777: step 98110, total loss = 0.41, predict loss = 0.09 (64.7 examples/sec; 0.062 sec/batch; 101h:22m:01s remains)
INFO - root - 2019-11-04 01:00:18.881281: step 98120, total loss = 0.51, predict loss = 0.12 (61.9 examples/sec; 0.065 sec/batch; 105h:56m:23s remains)
INFO - root - 2019-11-04 01:00:19.537071: step 98130, total loss = 0.40, predict loss = 0.10 (63.1 examples/sec; 0.063 sec/batch; 103h:54m:47s remains)
INFO - root - 2019-11-04 01:00:20.164059: step 98140, total loss = 0.38, predict loss = 0.09 (67.8 examples/sec; 0.059 sec/batch; 96h:40m:45s remains)
INFO - root - 2019-11-04 01:00:20.847361: step 98150, total loss = 0.50, predict loss = 0.11 (62.4 examples/sec; 0.064 sec/batch; 105h:06m:20s remains)
INFO - root - 2019-11-04 01:00:21.501199: step 98160, total loss = 0.48, predict loss = 0.11 (87.9 examples/sec; 0.045 sec/batch; 74h:35m:01s remains)
INFO - root - 2019-11-04 01:00:22.014078: step 98170, total loss = 0.53, predict loss = 0.12 (89.1 examples/sec; 0.045 sec/batch; 73h:38m:07s remains)
INFO - root - 2019-11-04 01:00:22.488530: step 98180, total loss = 0.49, predict loss = 0.11 (94.8 examples/sec; 0.042 sec/batch; 69h:08m:21s remains)
INFO - root - 2019-11-04 01:00:23.546121: step 98190, total loss = 0.31, predict loss = 0.07 (72.8 examples/sec; 0.055 sec/batch; 90h:05m:49s remains)
INFO - root - 2019-11-04 01:00:24.185328: step 98200, total loss = 0.34, predict loss = 0.08 (63.4 examples/sec; 0.063 sec/batch; 103h:22m:30s remains)
INFO - root - 2019-11-04 01:00:24.796529: step 98210, total loss = 0.49, predict loss = 0.12 (79.0 examples/sec; 0.051 sec/batch; 82h:59m:45s remains)
INFO - root - 2019-11-04 01:00:25.456352: step 98220, total loss = 0.75, predict loss = 0.19 (64.2 examples/sec; 0.062 sec/batch; 102h:04m:59s remains)
INFO - root - 2019-11-04 01:00:26.096034: step 98230, total loss = 0.50, predict loss = 0.12 (58.5 examples/sec; 0.068 sec/batch; 112h:07m:11s remains)
INFO - root - 2019-11-04 01:00:26.739467: step 98240, total loss = 0.55, predict loss = 0.13 (71.2 examples/sec; 0.056 sec/batch; 92h:08m:32s remains)
INFO - root - 2019-11-04 01:00:27.397387: step 98250, total loss = 0.44, predict loss = 0.10 (67.5 examples/sec; 0.059 sec/batch; 97h:12m:54s remains)
INFO - root - 2019-11-04 01:00:28.039401: step 98260, total loss = 0.64, predict loss = 0.14 (78.7 examples/sec; 0.051 sec/batch; 83h:21m:44s remains)
INFO - root - 2019-11-04 01:00:28.695930: step 98270, total loss = 0.57, predict loss = 0.13 (66.2 examples/sec; 0.060 sec/batch; 99h:06m:49s remains)
INFO - root - 2019-11-04 01:00:29.343359: step 98280, total loss = 0.46, predict loss = 0.10 (67.6 examples/sec; 0.059 sec/batch; 97h:00m:04s remains)
INFO - root - 2019-11-04 01:00:29.950187: step 98290, total loss = 0.65, predict loss = 0.15 (82.1 examples/sec; 0.049 sec/batch; 79h:54m:49s remains)
INFO - root - 2019-11-04 01:00:30.599699: step 98300, total loss = 0.63, predict loss = 0.14 (66.8 examples/sec; 0.060 sec/batch; 98h:09m:27s remains)
INFO - root - 2019-11-04 01:00:31.190887: step 98310, total loss = 0.56, predict loss = 0.12 (69.6 examples/sec; 0.057 sec/batch; 94h:13m:42s remains)
INFO - root - 2019-11-04 01:00:31.804298: step 98320, total loss = 0.48, predict loss = 0.11 (75.9 examples/sec; 0.053 sec/batch; 86h:22m:07s remains)
INFO - root - 2019-11-04 01:00:32.413859: step 98330, total loss = 0.59, predict loss = 0.14 (71.5 examples/sec; 0.056 sec/batch; 91h:45m:17s remains)
INFO - root - 2019-11-04 01:00:33.033124: step 98340, total loss = 0.44, predict loss = 0.09 (68.8 examples/sec; 0.058 sec/batch; 95h:20m:14s remains)
INFO - root - 2019-11-04 01:00:33.639024: step 98350, total loss = 0.52, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 93h:19m:00s remains)
INFO - root - 2019-11-04 01:00:34.283675: step 98360, total loss = 0.46, predict loss = 0.10 (61.5 examples/sec; 0.065 sec/batch; 106h:32m:17s remains)
INFO - root - 2019-11-04 01:00:35.006010: step 98370, total loss = 0.58, predict loss = 0.13 (73.3 examples/sec; 0.055 sec/batch; 89h:31m:01s remains)
INFO - root - 2019-11-04 01:00:35.706909: step 98380, total loss = 0.47, predict loss = 0.11 (60.4 examples/sec; 0.066 sec/batch; 108h:31m:02s remains)
INFO - root - 2019-11-04 01:00:36.330380: step 98390, total loss = 0.41, predict loss = 0.09 (75.6 examples/sec; 0.053 sec/batch; 86h:43m:36s remains)
INFO - root - 2019-11-04 01:00:36.951837: step 98400, total loss = 0.48, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 97h:28m:32s remains)
INFO - root - 2019-11-04 01:00:37.631957: step 98410, total loss = 0.46, predict loss = 0.11 (70.8 examples/sec; 0.056 sec/batch; 92h:35m:45s remains)
INFO - root - 2019-11-04 01:00:38.260666: step 98420, total loss = 0.33, predict loss = 0.07 (67.9 examples/sec; 0.059 sec/batch; 96h:35m:24s remains)
INFO - root - 2019-11-04 01:00:38.904679: step 98430, total loss = 0.44, predict loss = 0.10 (76.7 examples/sec; 0.052 sec/batch; 85h:32m:28s remains)
INFO - root - 2019-11-04 01:00:39.543543: step 98440, total loss = 0.55, predict loss = 0.13 (72.3 examples/sec; 0.055 sec/batch; 90h:42m:55s remains)
INFO - root - 2019-11-04 01:00:40.178368: step 98450, total loss = 0.66, predict loss = 0.15 (76.8 examples/sec; 0.052 sec/batch; 85h:19m:56s remains)
INFO - root - 2019-11-04 01:00:40.827134: step 98460, total loss = 0.61, predict loss = 0.14 (73.7 examples/sec; 0.054 sec/batch; 89h:00m:34s remains)
INFO - root - 2019-11-04 01:00:41.496992: step 98470, total loss = 0.47, predict loss = 0.11 (66.3 examples/sec; 0.060 sec/batch; 98h:50m:58s remains)
INFO - root - 2019-11-04 01:00:42.169021: step 98480, total loss = 0.59, predict loss = 0.14 (72.6 examples/sec; 0.055 sec/batch; 90h:21m:58s remains)
INFO - root - 2019-11-04 01:00:42.823737: step 98490, total loss = 0.68, predict loss = 0.16 (67.0 examples/sec; 0.060 sec/batch; 97h:53m:13s remains)
INFO - root - 2019-11-04 01:00:43.476060: step 98500, total loss = 0.46, predict loss = 0.10 (74.6 examples/sec; 0.054 sec/batch; 87h:51m:38s remains)
INFO - root - 2019-11-04 01:00:44.112059: step 98510, total loss = 0.66, predict loss = 0.14 (70.9 examples/sec; 0.056 sec/batch; 92h:25m:37s remains)
INFO - root - 2019-11-04 01:00:44.766220: step 98520, total loss = 0.65, predict loss = 0.15 (66.8 examples/sec; 0.060 sec/batch; 98h:13m:59s remains)
INFO - root - 2019-11-04 01:00:45.407158: step 98530, total loss = 0.55, predict loss = 0.13 (73.5 examples/sec; 0.054 sec/batch; 89h:16m:24s remains)
INFO - root - 2019-11-04 01:00:46.077319: step 98540, total loss = 0.51, predict loss = 0.11 (64.2 examples/sec; 0.062 sec/batch; 102h:04m:43s remains)
INFO - root - 2019-11-04 01:00:46.722605: step 98550, total loss = 0.47, predict loss = 0.11 (67.2 examples/sec; 0.060 sec/batch; 97h:33m:34s remains)
INFO - root - 2019-11-04 01:00:47.338033: step 98560, total loss = 0.50, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 91h:23m:40s remains)
INFO - root - 2019-11-04 01:00:48.013394: step 98570, total loss = 0.44, predict loss = 0.10 (73.0 examples/sec; 0.055 sec/batch; 89h:49m:02s remains)
INFO - root - 2019-11-04 01:00:48.670714: step 98580, total loss = 0.45, predict loss = 0.09 (67.9 examples/sec; 0.059 sec/batch; 96h:32m:02s remains)
INFO - root - 2019-11-04 01:00:49.265338: step 98590, total loss = 0.52, predict loss = 0.12 (85.4 examples/sec; 0.047 sec/batch; 76h:47m:39s remains)
INFO - root - 2019-11-04 01:00:49.862376: step 98600, total loss = 0.54, predict loss = 0.13 (77.7 examples/sec; 0.052 sec/batch; 84h:26m:09s remains)
INFO - root - 2019-11-04 01:00:50.482196: step 98610, total loss = 0.44, predict loss = 0.10 (63.0 examples/sec; 0.063 sec/batch; 104h:01m:49s remains)
INFO - root - 2019-11-04 01:00:51.244658: step 98620, total loss = 0.56, predict loss = 0.13 (71.5 examples/sec; 0.056 sec/batch; 91h:39m:23s remains)
INFO - root - 2019-11-04 01:00:51.862551: step 98630, total loss = 0.74, predict loss = 0.18 (73.0 examples/sec; 0.055 sec/batch; 89h:45m:49s remains)
INFO - root - 2019-11-04 01:00:52.475602: step 98640, total loss = 0.35, predict loss = 0.07 (73.4 examples/sec; 0.054 sec/batch; 89h:16m:53s remains)
INFO - root - 2019-11-04 01:00:53.132934: step 98650, total loss = 0.37, predict loss = 0.09 (68.4 examples/sec; 0.058 sec/batch; 95h:51m:59s remains)
INFO - root - 2019-11-04 01:00:53.774653: step 98660, total loss = 0.66, predict loss = 0.17 (68.4 examples/sec; 0.058 sec/batch; 95h:51m:50s remains)
INFO - root - 2019-11-04 01:00:54.392173: step 98670, total loss = 0.45, predict loss = 0.11 (68.3 examples/sec; 0.059 sec/batch; 95h:57m:55s remains)
INFO - root - 2019-11-04 01:00:55.067582: step 98680, total loss = 0.45, predict loss = 0.11 (78.2 examples/sec; 0.051 sec/batch; 83h:51m:36s remains)
INFO - root - 2019-11-04 01:00:55.710249: step 98690, total loss = 0.73, predict loss = 0.19 (65.8 examples/sec; 0.061 sec/batch; 99h:36m:13s remains)
INFO - root - 2019-11-04 01:00:56.324324: step 98700, total loss = 0.66, predict loss = 0.16 (73.7 examples/sec; 0.054 sec/batch; 88h:57m:50s remains)
INFO - root - 2019-11-04 01:00:56.954672: step 98710, total loss = 0.62, predict loss = 0.15 (68.6 examples/sec; 0.058 sec/batch; 95h:37m:34s remains)
INFO - root - 2019-11-04 01:00:57.570535: step 98720, total loss = 0.48, predict loss = 0.12 (79.3 examples/sec; 0.050 sec/batch; 82h:38m:52s remains)
INFO - root - 2019-11-04 01:00:58.197961: step 98730, total loss = 0.46, predict loss = 0.10 (68.9 examples/sec; 0.058 sec/batch; 95h:12m:49s remains)
INFO - root - 2019-11-04 01:00:58.844651: step 98740, total loss = 0.41, predict loss = 0.11 (64.7 examples/sec; 0.062 sec/batch; 101h:17m:01s remains)
INFO - root - 2019-11-04 01:00:59.495846: step 98750, total loss = 0.54, predict loss = 0.13 (72.8 examples/sec; 0.055 sec/batch; 90h:07m:45s remains)
INFO - root - 2019-11-04 01:01:00.136111: step 98760, total loss = 0.34, predict loss = 0.08 (69.2 examples/sec; 0.058 sec/batch; 94h:45m:20s remains)
INFO - root - 2019-11-04 01:01:00.785308: step 98770, total loss = 0.51, predict loss = 0.12 (72.9 examples/sec; 0.055 sec/batch; 89h:59m:18s remains)
INFO - root - 2019-11-04 01:01:01.452253: step 98780, total loss = 0.40, predict loss = 0.09 (68.5 examples/sec; 0.058 sec/batch; 95h:43m:26s remains)
INFO - root - 2019-11-04 01:01:02.068734: step 98790, total loss = 0.42, predict loss = 0.10 (79.2 examples/sec; 0.051 sec/batch; 82h:50m:07s remains)
INFO - root - 2019-11-04 01:01:02.729333: step 98800, total loss = 0.37, predict loss = 0.08 (68.1 examples/sec; 0.059 sec/batch; 96h:15m:28s remains)
INFO - root - 2019-11-04 01:01:03.381390: step 98810, total loss = 0.37, predict loss = 0.08 (65.3 examples/sec; 0.061 sec/batch; 100h:28m:40s remains)
INFO - root - 2019-11-04 01:01:04.025646: step 98820, total loss = 0.29, predict loss = 0.06 (75.9 examples/sec; 0.053 sec/batch; 86h:22m:11s remains)
INFO - root - 2019-11-04 01:01:04.659193: step 98830, total loss = 0.47, predict loss = 0.10 (65.7 examples/sec; 0.061 sec/batch; 99h:50m:12s remains)
INFO - root - 2019-11-04 01:01:05.252055: step 98840, total loss = 0.42, predict loss = 0.10 (75.3 examples/sec; 0.053 sec/batch; 87h:05m:37s remains)
INFO - root - 2019-11-04 01:01:05.864445: step 98850, total loss = 0.45, predict loss = 0.10 (71.5 examples/sec; 0.056 sec/batch; 91h:45m:39s remains)
INFO - root - 2019-11-04 01:01:06.473240: step 98860, total loss = 0.49, predict loss = 0.11 (84.0 examples/sec; 0.048 sec/batch; 78h:03m:31s remains)
INFO - root - 2019-11-04 01:01:07.083858: step 98870, total loss = 0.50, predict loss = 0.12 (70.8 examples/sec; 0.057 sec/batch; 92h:40m:19s remains)
INFO - root - 2019-11-04 01:01:07.714095: step 98880, total loss = 0.57, predict loss = 0.14 (74.5 examples/sec; 0.054 sec/batch; 88h:00m:29s remains)
INFO - root - 2019-11-04 01:01:08.340451: step 98890, total loss = 0.40, predict loss = 0.09 (76.2 examples/sec; 0.053 sec/batch; 86h:04m:20s remains)
INFO - root - 2019-11-04 01:01:08.984991: step 98900, total loss = 0.48, predict loss = 0.11 (74.4 examples/sec; 0.054 sec/batch; 88h:07m:52s remains)
INFO - root - 2019-11-04 01:01:09.616583: step 98910, total loss = 0.33, predict loss = 0.07 (65.0 examples/sec; 0.062 sec/batch; 100h:49m:46s remains)
INFO - root - 2019-11-04 01:01:10.273399: step 98920, total loss = 0.39, predict loss = 0.08 (65.0 examples/sec; 0.062 sec/batch; 100h:50m:08s remains)
INFO - root - 2019-11-04 01:01:10.942115: step 98930, total loss = 0.60, predict loss = 0.15 (66.3 examples/sec; 0.060 sec/batch; 98h:56m:07s remains)
INFO - root - 2019-11-04 01:01:11.598549: step 98940, total loss = 0.46, predict loss = 0.10 (65.7 examples/sec; 0.061 sec/batch; 99h:46m:25s remains)
INFO - root - 2019-11-04 01:01:12.263725: step 98950, total loss = 0.57, predict loss = 0.13 (57.6 examples/sec; 0.069 sec/batch; 113h:47m:25s remains)
INFO - root - 2019-11-04 01:01:12.967747: step 98960, total loss = 0.51, predict loss = 0.12 (62.1 examples/sec; 0.064 sec/batch; 105h:32m:39s remains)
INFO - root - 2019-11-04 01:01:13.696099: step 98970, total loss = 0.53, predict loss = 0.13 (60.6 examples/sec; 0.066 sec/batch; 108h:13m:29s remains)
INFO - root - 2019-11-04 01:01:14.371818: step 98980, total loss = 0.52, predict loss = 0.13 (73.7 examples/sec; 0.054 sec/batch; 89h:00m:24s remains)
INFO - root - 2019-11-04 01:01:14.988015: step 98990, total loss = 0.43, predict loss = 0.10 (81.6 examples/sec; 0.049 sec/batch; 80h:21m:30s remains)
INFO - root - 2019-11-04 01:01:15.632283: step 99000, total loss = 0.44, predict loss = 0.10 (67.9 examples/sec; 0.059 sec/batch; 96h:31m:56s remains)
INFO - root - 2019-11-04 01:01:16.263817: step 99010, total loss = 0.29, predict loss = 0.07 (71.6 examples/sec; 0.056 sec/batch; 91h:32m:06s remains)
INFO - root - 2019-11-04 01:01:16.874197: step 99020, total loss = 0.28, predict loss = 0.05 (79.6 examples/sec; 0.050 sec/batch; 82h:20m:45s remains)
INFO - root - 2019-11-04 01:01:17.485111: step 99030, total loss = 0.26, predict loss = 0.05 (73.5 examples/sec; 0.054 sec/batch; 89h:14m:15s remains)
INFO - root - 2019-11-04 01:01:18.129690: step 99040, total loss = 0.40, predict loss = 0.09 (73.9 examples/sec; 0.054 sec/batch; 88h:41m:48s remains)
INFO - root - 2019-11-04 01:01:18.788123: step 99050, total loss = 0.23, predict loss = 0.04 (74.1 examples/sec; 0.054 sec/batch; 88h:28m:34s remains)
INFO - root - 2019-11-04 01:01:19.449670: step 99060, total loss = 0.19, predict loss = 0.04 (70.1 examples/sec; 0.057 sec/batch; 93h:28m:47s remains)
INFO - root - 2019-11-04 01:01:20.030080: step 99070, total loss = 0.39, predict loss = 0.09 (67.6 examples/sec; 0.059 sec/batch; 96h:58m:35s remains)
INFO - root - 2019-11-04 01:01:20.655012: step 99080, total loss = 0.37, predict loss = 0.08 (78.7 examples/sec; 0.051 sec/batch; 83h:19m:56s remains)
INFO - root - 2019-11-04 01:01:21.304841: step 99090, total loss = 0.29, predict loss = 0.06 (65.1 examples/sec; 0.061 sec/batch; 100h:40m:13s remains)
INFO - root - 2019-11-04 01:01:21.966056: step 99100, total loss = 0.43, predict loss = 0.09 (69.0 examples/sec; 0.058 sec/batch; 94h:58m:33s remains)
INFO - root - 2019-11-04 01:01:22.621802: step 99110, total loss = 0.40, predict loss = 0.09 (69.7 examples/sec; 0.057 sec/batch; 94h:01m:44s remains)
INFO - root - 2019-11-04 01:01:23.314773: step 99120, total loss = 0.38, predict loss = 0.09 (62.3 examples/sec; 0.064 sec/batch; 105h:19m:16s remains)
INFO - root - 2019-11-04 01:01:23.964884: step 99130, total loss = 0.51, predict loss = 0.12 (77.1 examples/sec; 0.052 sec/batch; 84h:59m:55s remains)
INFO - root - 2019-11-04 01:01:24.593338: step 99140, total loss = 0.39, predict loss = 0.09 (71.2 examples/sec; 0.056 sec/batch; 92h:03m:58s remains)
INFO - root - 2019-11-04 01:01:25.203402: step 99150, total loss = 0.42, predict loss = 0.10 (70.3 examples/sec; 0.057 sec/batch; 93h:18m:19s remains)
INFO - root - 2019-11-04 01:01:25.826355: step 99160, total loss = 0.46, predict loss = 0.10 (81.6 examples/sec; 0.049 sec/batch; 80h:18m:43s remains)
INFO - root - 2019-11-04 01:01:26.481703: step 99170, total loss = 0.33, predict loss = 0.08 (74.3 examples/sec; 0.054 sec/batch; 88h:14m:39s remains)
INFO - root - 2019-11-04 01:01:27.195329: step 99180, total loss = 0.65, predict loss = 0.16 (61.9 examples/sec; 0.065 sec/batch; 105h:52m:06s remains)
INFO - root - 2019-11-04 01:01:27.819172: step 99190, total loss = 0.63, predict loss = 0.15 (77.2 examples/sec; 0.052 sec/batch; 84h:55m:00s remains)
INFO - root - 2019-11-04 01:01:28.433556: step 99200, total loss = 0.55, predict loss = 0.14 (67.0 examples/sec; 0.060 sec/batch; 97h:51m:17s remains)
INFO - root - 2019-11-04 01:01:29.105374: step 99210, total loss = 0.38, predict loss = 0.09 (68.9 examples/sec; 0.058 sec/batch; 95h:06m:21s remains)
INFO - root - 2019-11-04 01:01:29.743640: step 99220, total loss = 0.43, predict loss = 0.11 (78.2 examples/sec; 0.051 sec/batch; 83h:49m:57s remains)
INFO - root - 2019-11-04 01:01:30.384091: step 99230, total loss = 0.38, predict loss = 0.08 (65.2 examples/sec; 0.061 sec/batch; 100h:33m:20s remains)
INFO - root - 2019-11-04 01:01:31.052176: step 99240, total loss = 0.41, predict loss = 0.10 (66.7 examples/sec; 0.060 sec/batch; 98h:16m:29s remains)
INFO - root - 2019-11-04 01:01:31.669435: step 99250, total loss = 0.48, predict loss = 0.12 (88.1 examples/sec; 0.045 sec/batch; 74h:27m:26s remains)
INFO - root - 2019-11-04 01:01:32.280517: step 99260, total loss = 0.18, predict loss = 0.03 (67.7 examples/sec; 0.059 sec/batch; 96h:48m:46s remains)
INFO - root - 2019-11-04 01:01:32.906718: step 99270, total loss = 0.35, predict loss = 0.09 (65.3 examples/sec; 0.061 sec/batch; 100h:19m:54s remains)
INFO - root - 2019-11-04 01:01:33.510404: step 99280, total loss = 0.25, predict loss = 0.05 (76.4 examples/sec; 0.052 sec/batch; 85h:46m:34s remains)
INFO - root - 2019-11-04 01:01:34.146757: step 99290, total loss = 0.33, predict loss = 0.07 (68.5 examples/sec; 0.058 sec/batch; 95h:45m:17s remains)
INFO - root - 2019-11-04 01:01:34.829904: step 99300, total loss = 0.44, predict loss = 0.11 (73.4 examples/sec; 0.054 sec/batch; 89h:16m:27s remains)
INFO - root - 2019-11-04 01:01:35.435591: step 99310, total loss = 0.44, predict loss = 0.10 (66.9 examples/sec; 0.060 sec/batch; 97h:59m:15s remains)
INFO - root - 2019-11-04 01:01:36.042526: step 99320, total loss = 0.58, predict loss = 0.14 (71.3 examples/sec; 0.056 sec/batch; 91h:55m:41s remains)
INFO - root - 2019-11-04 01:01:36.638573: step 99330, total loss = 0.52, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 86h:25m:34s remains)
INFO - root - 2019-11-04 01:01:37.262422: step 99340, total loss = 0.49, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 92h:56m:38s remains)
INFO - root - 2019-11-04 01:01:37.893446: step 99350, total loss = 0.54, predict loss = 0.13 (68.9 examples/sec; 0.058 sec/batch; 95h:13m:12s remains)
INFO - root - 2019-11-04 01:01:38.502432: step 99360, total loss = 0.56, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 94h:50m:42s remains)
INFO - root - 2019-11-04 01:01:39.098697: step 99370, total loss = 0.38, predict loss = 0.09 (74.3 examples/sec; 0.054 sec/batch; 88h:12m:44s remains)
INFO - root - 2019-11-04 01:01:39.711647: step 99380, total loss = 0.39, predict loss = 0.09 (72.6 examples/sec; 0.055 sec/batch; 90h:20m:09s remains)
INFO - root - 2019-11-04 01:01:40.328413: step 99390, total loss = 0.28, predict loss = 0.06 (68.2 examples/sec; 0.059 sec/batch; 96h:07m:49s remains)
INFO - root - 2019-11-04 01:01:40.957620: step 99400, total loss = 0.45, predict loss = 0.11 (64.8 examples/sec; 0.062 sec/batch; 101h:11m:39s remains)
INFO - root - 2019-11-04 01:01:41.588085: step 99410, total loss = 0.48, predict loss = 0.11 (72.2 examples/sec; 0.055 sec/batch; 90h:47m:17s remains)
INFO - root - 2019-11-04 01:01:42.203833: step 99420, total loss = 0.31, predict loss = 0.07 (84.0 examples/sec; 0.048 sec/batch; 78h:03m:55s remains)
INFO - root - 2019-11-04 01:01:42.820885: step 99430, total loss = 0.39, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 94h:17m:25s remains)
INFO - root - 2019-11-04 01:01:43.445471: step 99440, total loss = 0.49, predict loss = 0.11 (73.5 examples/sec; 0.054 sec/batch; 89h:13m:38s remains)
INFO - root - 2019-11-04 01:01:44.076875: step 99450, total loss = 0.53, predict loss = 0.11 (78.4 examples/sec; 0.051 sec/batch; 83h:38m:11s remains)
INFO - root - 2019-11-04 01:01:44.717051: step 99460, total loss = 0.53, predict loss = 0.12 (65.6 examples/sec; 0.061 sec/batch; 99h:53m:56s remains)
INFO - root - 2019-11-04 01:01:45.382598: step 99470, total loss = 0.51, predict loss = 0.11 (65.2 examples/sec; 0.061 sec/batch; 100h:31m:05s remains)
INFO - root - 2019-11-04 01:01:46.002234: step 99480, total loss = 0.45, predict loss = 0.10 (72.0 examples/sec; 0.056 sec/batch; 91h:01m:14s remains)
INFO - root - 2019-11-04 01:01:46.624744: step 99490, total loss = 0.53, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 97h:27m:39s remains)
INFO - root - 2019-11-04 01:01:47.257292: step 99500, total loss = 0.49, predict loss = 0.11 (67.2 examples/sec; 0.059 sec/batch; 97h:29m:31s remains)
INFO - root - 2019-11-04 01:01:47.908113: step 99510, total loss = 0.30, predict loss = 0.06 (65.7 examples/sec; 0.061 sec/batch; 99h:47m:04s remains)
INFO - root - 2019-11-04 01:01:48.538391: step 99520, total loss = 0.44, predict loss = 0.09 (80.6 examples/sec; 0.050 sec/batch; 81h:19m:59s remains)
INFO - root - 2019-11-04 01:01:49.165265: step 99530, total loss = 0.57, predict loss = 0.13 (67.2 examples/sec; 0.059 sec/batch; 97h:29m:21s remains)
INFO - root - 2019-11-04 01:01:49.790271: step 99540, total loss = 0.35, predict loss = 0.08 (72.6 examples/sec; 0.055 sec/batch; 90h:17m:17s remains)
INFO - root - 2019-11-04 01:01:50.426002: step 99550, total loss = 0.23, predict loss = 0.04 (69.1 examples/sec; 0.058 sec/batch; 94h:54m:33s remains)
INFO - root - 2019-11-04 01:01:51.043583: step 99560, total loss = 0.66, predict loss = 0.16 (70.6 examples/sec; 0.057 sec/batch; 92h:54m:22s remains)
INFO - root - 2019-11-04 01:01:51.658018: step 99570, total loss = 0.26, predict loss = 0.05 (75.7 examples/sec; 0.053 sec/batch; 86h:37m:30s remains)
INFO - root - 2019-11-04 01:01:52.267775: step 99580, total loss = 0.32, predict loss = 0.07 (75.9 examples/sec; 0.053 sec/batch; 86h:21m:44s remains)
INFO - root - 2019-11-04 01:01:52.895063: step 99590, total loss = 0.57, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 100h:40m:01s remains)
INFO - root - 2019-11-04 01:01:53.502855: step 99600, total loss = 0.56, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 90h:46m:20s remains)
INFO - root - 2019-11-04 01:01:54.153985: step 99610, total loss = 0.66, predict loss = 0.15 (73.7 examples/sec; 0.054 sec/batch; 88h:57m:12s remains)
INFO - root - 2019-11-04 01:01:54.776718: step 99620, total loss = 0.75, predict loss = 0.16 (71.7 examples/sec; 0.056 sec/batch; 91h:25m:03s remains)
INFO - root - 2019-11-04 01:01:55.406360: step 99630, total loss = 0.73, predict loss = 0.18 (66.5 examples/sec; 0.060 sec/batch; 98h:35m:44s remains)
INFO - root - 2019-11-04 01:01:56.049010: step 99640, total loss = 0.83, predict loss = 0.21 (66.2 examples/sec; 0.060 sec/batch; 99h:04m:24s remains)
INFO - root - 2019-11-04 01:01:56.705039: step 99650, total loss = 0.68, predict loss = 0.16 (66.6 examples/sec; 0.060 sec/batch; 98h:23m:24s remains)
INFO - root - 2019-11-04 01:01:57.346247: step 99660, total loss = 0.75, predict loss = 0.18 (70.1 examples/sec; 0.057 sec/batch; 93h:34m:44s remains)
INFO - root - 2019-11-04 01:01:57.986760: step 99670, total loss = 0.56, predict loss = 0.13 (70.1 examples/sec; 0.057 sec/batch; 93h:33m:03s remains)
INFO - root - 2019-11-04 01:01:58.625975: step 99680, total loss = 0.52, predict loss = 0.12 (68.0 examples/sec; 0.059 sec/batch; 96h:28m:52s remains)
INFO - root - 2019-11-04 01:01:59.259162: step 99690, total loss = 0.45, predict loss = 0.10 (78.0 examples/sec; 0.051 sec/batch; 83h:59m:59s remains)
INFO - root - 2019-11-04 01:01:59.861333: step 99700, total loss = 0.64, predict loss = 0.15 (74.8 examples/sec; 0.053 sec/batch; 87h:37m:34s remains)
INFO - root - 2019-11-04 01:02:00.470415: step 99710, total loss = 0.44, predict loss = 0.10 (76.1 examples/sec; 0.053 sec/batch; 86h:11m:25s remains)
INFO - root - 2019-11-04 01:02:01.104079: step 99720, total loss = 0.51, predict loss = 0.12 (63.9 examples/sec; 0.063 sec/batch; 102h:32m:56s remains)
INFO - root - 2019-11-04 01:02:01.751493: step 99730, total loss = 0.50, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 93h:53m:32s remains)
INFO - root - 2019-11-04 01:02:02.447987: step 99740, total loss = 0.50, predict loss = 0.11 (62.3 examples/sec; 0.064 sec/batch; 105h:13m:09s remains)
INFO - root - 2019-11-04 01:02:03.148610: step 99750, total loss = 0.52, predict loss = 0.12 (60.4 examples/sec; 0.066 sec/batch; 108h:29m:46s remains)
INFO - root - 2019-11-04 01:02:03.820732: step 99760, total loss = 0.49, predict loss = 0.11 (62.0 examples/sec; 0.065 sec/batch; 105h:46m:56s remains)
INFO - root - 2019-11-04 01:02:04.501763: step 99770, total loss = 0.48, predict loss = 0.11 (62.6 examples/sec; 0.064 sec/batch; 104h:42m:45s remains)
INFO - root - 2019-11-04 01:02:05.154883: step 99780, total loss = 0.37, predict loss = 0.07 (81.4 examples/sec; 0.049 sec/batch; 80h:30m:39s remains)
INFO - root - 2019-11-04 01:02:05.753230: step 99790, total loss = 0.52, predict loss = 0.12 (80.7 examples/sec; 0.050 sec/batch; 81h:13m:53s remains)
INFO - root - 2019-11-04 01:02:06.346139: step 99800, total loss = 0.48, predict loss = 0.11 (72.1 examples/sec; 0.055 sec/batch; 90h:55m:40s remains)
INFO - root - 2019-11-04 01:02:06.973260: step 99810, total loss = 0.48, predict loss = 0.11 (76.1 examples/sec; 0.053 sec/batch; 86h:06m:57s remains)
INFO - root - 2019-11-04 01:02:07.587574: step 99820, total loss = 0.53, predict loss = 0.12 (78.0 examples/sec; 0.051 sec/batch; 84h:00m:23s remains)
INFO - root - 2019-11-04 01:02:08.214755: step 99830, total loss = 0.51, predict loss = 0.11 (75.6 examples/sec; 0.053 sec/batch; 86h:45m:19s remains)
INFO - root - 2019-11-04 01:02:08.818428: step 99840, total loss = 0.39, predict loss = 0.09 (69.6 examples/sec; 0.057 sec/batch; 94h:09m:03s remains)
INFO - root - 2019-11-04 01:02:09.431321: step 99850, total loss = 0.50, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 98h:00m:48s remains)
INFO - root - 2019-11-04 01:02:10.061638: step 99860, total loss = 0.40, predict loss = 0.09 (77.1 examples/sec; 0.052 sec/batch; 84h:59m:37s remains)
INFO - root - 2019-11-04 01:02:10.699600: step 99870, total loss = 0.48, predict loss = 0.11 (68.8 examples/sec; 0.058 sec/batch; 95h:20m:58s remains)
INFO - root - 2019-11-04 01:02:11.323617: step 99880, total loss = 0.37, predict loss = 0.08 (80.6 examples/sec; 0.050 sec/batch; 81h:18m:31s remains)
INFO - root - 2019-11-04 01:02:11.962822: step 99890, total loss = 0.34, predict loss = 0.07 (65.6 examples/sec; 0.061 sec/batch; 99h:52m:13s remains)
INFO - root - 2019-11-04 01:02:12.640056: step 99900, total loss = 0.36, predict loss = 0.08 (65.5 examples/sec; 0.061 sec/batch; 100h:01m:56s remains)
INFO - root - 2019-11-04 01:02:13.282118: step 99910, total loss = 0.43, predict loss = 0.10 (65.3 examples/sec; 0.061 sec/batch; 100h:20m:25s remains)
INFO - root - 2019-11-04 01:02:13.945941: step 99920, total loss = 0.45, predict loss = 0.10 (71.6 examples/sec; 0.056 sec/batch; 91h:30m:56s remains)
INFO - root - 2019-11-04 01:02:14.584873: step 99930, total loss = 0.42, predict loss = 0.09 (65.3 examples/sec; 0.061 sec/batch; 100h:20m:37s remains)
INFO - root - 2019-11-04 01:02:15.239613: step 99940, total loss = 0.44, predict loss = 0.10 (70.8 examples/sec; 0.056 sec/batch; 92h:34m:16s remains)
INFO - root - 2019-11-04 01:02:15.869117: step 99950, total loss = 0.42, predict loss = 0.09 (64.8 examples/sec; 0.062 sec/batch; 101h:12m:13s remains)
INFO - root - 2019-11-04 01:02:16.519005: step 99960, total loss = 0.40, predict loss = 0.08 (72.1 examples/sec; 0.055 sec/batch; 90h:51m:47s remains)
INFO - root - 2019-11-04 01:02:17.173066: step 99970, total loss = 0.48, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 99h:27m:00s remains)
INFO - root - 2019-11-04 01:02:17.829704: step 99980, total loss = 0.35, predict loss = 0.08 (72.3 examples/sec; 0.055 sec/batch; 90h:40m:03s remains)
INFO - root - 2019-11-04 01:02:18.494000: step 99990, total loss = 0.45, predict loss = 0.11 (65.8 examples/sec; 0.061 sec/batch; 99h:34m:19s remains)
INFO - root - 2019-11-04 01:02:19.158009: step 100000, total loss = 0.50, predict loss = 0.11 (63.2 examples/sec; 0.063 sec/batch; 103h:45m:04s remains)
INFO - root - 2019-11-04 01:02:19.775806: step 100010, total loss = 0.43, predict loss = 0.10 (61.2 examples/sec; 0.065 sec/batch; 107h:09m:35s remains)
INFO - root - 2019-11-04 01:02:20.385522: step 100020, total loss = 0.54, predict loss = 0.13 (73.4 examples/sec; 0.054 sec/batch; 89h:17m:53s remains)
INFO - root - 2019-11-04 01:02:21.008323: step 100030, total loss = 0.60, predict loss = 0.14 (69.0 examples/sec; 0.058 sec/batch; 95h:02m:15s remains)
INFO - root - 2019-11-04 01:02:21.651694: step 100040, total loss = 0.60, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 100h:43m:18s remains)
INFO - root - 2019-11-04 01:02:22.282759: step 100050, total loss = 0.52, predict loss = 0.12 (73.3 examples/sec; 0.055 sec/batch; 89h:22m:43s remains)
INFO - root - 2019-11-04 01:02:22.909004: step 100060, total loss = 0.67, predict loss = 0.17 (77.8 examples/sec; 0.051 sec/batch; 84h:16m:14s remains)
INFO - root - 2019-11-04 01:02:23.529572: step 100070, total loss = 0.66, predict loss = 0.16 (78.3 examples/sec; 0.051 sec/batch; 83h:42m:11s remains)
INFO - root - 2019-11-04 01:02:24.128662: step 100080, total loss = 0.63, predict loss = 0.15 (76.8 examples/sec; 0.052 sec/batch; 85h:18m:10s remains)
INFO - root - 2019-11-04 01:02:24.743946: step 100090, total loss = 0.58, predict loss = 0.13 (74.0 examples/sec; 0.054 sec/batch; 88h:31m:52s remains)
INFO - root - 2019-11-04 01:02:25.382435: step 100100, total loss = 0.48, predict loss = 0.11 (64.6 examples/sec; 0.062 sec/batch; 101h:24m:26s remains)
INFO - root - 2019-11-04 01:02:26.025377: step 100110, total loss = 0.54, predict loss = 0.11 (62.4 examples/sec; 0.064 sec/batch; 105h:02m:32s remains)
INFO - root - 2019-11-04 01:02:26.658545: step 100120, total loss = 0.48, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 93h:27m:24s remains)
INFO - root - 2019-11-04 01:02:27.354168: step 100130, total loss = 0.56, predict loss = 0.14 (61.8 examples/sec; 0.065 sec/batch; 106h:01m:42s remains)
INFO - root - 2019-11-04 01:02:28.076642: step 100140, total loss = 0.54, predict loss = 0.13 (63.3 examples/sec; 0.063 sec/batch; 103h:35m:44s remains)
INFO - root - 2019-11-04 01:02:28.675627: step 100150, total loss = 0.41, predict loss = 0.09 (87.7 examples/sec; 0.046 sec/batch; 74h:42m:22s remains)
INFO - root - 2019-11-04 01:02:29.314825: step 100160, total loss = 0.40, predict loss = 0.09 (71.3 examples/sec; 0.056 sec/batch; 91h:59m:59s remains)
INFO - root - 2019-11-04 01:02:29.983284: step 100170, total loss = 0.38, predict loss = 0.08 (73.0 examples/sec; 0.055 sec/batch; 89h:44m:48s remains)
INFO - root - 2019-11-04 01:02:30.610171: step 100180, total loss = 0.40, predict loss = 0.09 (75.6 examples/sec; 0.053 sec/batch; 86h:42m:16s remains)
INFO - root - 2019-11-04 01:02:31.254377: step 100190, total loss = 0.47, predict loss = 0.10 (70.2 examples/sec; 0.057 sec/batch; 93h:24m:54s remains)
INFO - root - 2019-11-04 01:02:31.919321: step 100200, total loss = 0.57, predict loss = 0.13 (61.1 examples/sec; 0.065 sec/batch; 107h:13m:02s remains)
INFO - root - 2019-11-04 01:02:32.539461: step 100210, total loss = 0.36, predict loss = 0.08 (71.7 examples/sec; 0.056 sec/batch; 91h:28m:57s remains)
INFO - root - 2019-11-04 01:02:33.179896: step 100220, total loss = 0.50, predict loss = 0.12 (75.2 examples/sec; 0.053 sec/batch; 87h:09m:39s remains)
INFO - root - 2019-11-04 01:02:33.846443: step 100230, total loss = 0.43, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 94h:02m:45s remains)
INFO - root - 2019-11-04 01:02:34.545461: step 100240, total loss = 0.54, predict loss = 0.12 (53.1 examples/sec; 0.075 sec/batch; 123h:29m:00s remains)
INFO - root - 2019-11-04 01:02:35.198129: step 100250, total loss = 0.63, predict loss = 0.15 (68.9 examples/sec; 0.058 sec/batch; 95h:12m:32s remains)
INFO - root - 2019-11-04 01:02:35.863937: step 100260, total loss = 0.51, predict loss = 0.12 (79.1 examples/sec; 0.051 sec/batch; 82h:49m:50s remains)
INFO - root - 2019-11-04 01:02:36.504179: step 100270, total loss = 0.65, predict loss = 0.15 (69.5 examples/sec; 0.058 sec/batch; 94h:19m:58s remains)
INFO - root - 2019-11-04 01:02:37.118054: step 100280, total loss = 0.61, predict loss = 0.15 (70.7 examples/sec; 0.057 sec/batch; 92h:43m:19s remains)
INFO - root - 2019-11-04 01:02:37.738581: step 100290, total loss = 0.49, predict loss = 0.11 (82.0 examples/sec; 0.049 sec/batch; 79h:55m:18s remains)
INFO - root - 2019-11-04 01:02:38.377054: step 100300, total loss = 0.59, predict loss = 0.14 (81.1 examples/sec; 0.049 sec/batch; 80h:46m:54s remains)
INFO - root - 2019-11-04 01:02:39.004685: step 100310, total loss = 0.50, predict loss = 0.12 (67.6 examples/sec; 0.059 sec/batch; 96h:55m:14s remains)
INFO - root - 2019-11-04 01:02:39.665950: step 100320, total loss = 0.59, predict loss = 0.14 (60.9 examples/sec; 0.066 sec/batch; 107h:38m:47s remains)
INFO - root - 2019-11-04 01:02:40.362125: step 100330, total loss = 0.56, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 100h:42m:49s remains)
INFO - root - 2019-11-04 01:02:40.947605: step 100340, total loss = 0.55, predict loss = 0.13 (81.7 examples/sec; 0.049 sec/batch; 80h:14m:46s remains)
INFO - root - 2019-11-04 01:02:41.568172: step 100350, total loss = 0.51, predict loss = 0.12 (69.7 examples/sec; 0.057 sec/batch; 93h:59m:43s remains)
INFO - root - 2019-11-04 01:02:42.194506: step 100360, total loss = 0.51, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 99h:29m:49s remains)
INFO - root - 2019-11-04 01:02:42.808754: step 100370, total loss = 0.47, predict loss = 0.10 (66.6 examples/sec; 0.060 sec/batch; 98h:28m:15s remains)
INFO - root - 2019-11-04 01:02:43.447455: step 100380, total loss = 0.46, predict loss = 0.10 (78.0 examples/sec; 0.051 sec/batch; 84h:01m:31s remains)
INFO - root - 2019-11-04 01:02:44.088048: step 100390, total loss = 0.47, predict loss = 0.11 (72.4 examples/sec; 0.055 sec/batch; 90h:33m:01s remains)
INFO - root - 2019-11-04 01:02:44.716985: step 100400, total loss = 0.46, predict loss = 0.10 (71.6 examples/sec; 0.056 sec/batch; 91h:34m:51s remains)
INFO - root - 2019-11-04 01:02:45.361935: step 100410, total loss = 0.38, predict loss = 0.08 (68.7 examples/sec; 0.058 sec/batch; 95h:21m:51s remains)
INFO - root - 2019-11-04 01:02:46.002826: step 100420, total loss = 0.39, predict loss = 0.09 (67.3 examples/sec; 0.059 sec/batch; 97h:28m:04s remains)
INFO - root - 2019-11-04 01:02:46.636648: step 100430, total loss = 0.45, predict loss = 0.10 (75.5 examples/sec; 0.053 sec/batch; 86h:51m:31s remains)
INFO - root - 2019-11-04 01:02:47.300461: step 100440, total loss = 0.37, predict loss = 0.09 (66.1 examples/sec; 0.060 sec/batch; 99h:06m:50s remains)
INFO - root - 2019-11-04 01:02:47.971030: step 100450, total loss = 0.41, predict loss = 0.09 (64.0 examples/sec; 0.062 sec/batch; 102h:22m:52s remains)
INFO - root - 2019-11-04 01:02:48.641551: step 100460, total loss = 0.43, predict loss = 0.10 (69.2 examples/sec; 0.058 sec/batch; 94h:47m:11s remains)
INFO - root - 2019-11-04 01:02:49.303351: step 100470, total loss = 0.41, predict loss = 0.09 (66.1 examples/sec; 0.061 sec/batch; 99h:11m:35s remains)
INFO - root - 2019-11-04 01:02:49.960832: step 100480, total loss = 0.38, predict loss = 0.09 (71.5 examples/sec; 0.056 sec/batch; 91h:43m:48s remains)
INFO - root - 2019-11-04 01:02:50.583409: step 100490, total loss = 0.51, predict loss = 0.12 (73.7 examples/sec; 0.054 sec/batch; 88h:54m:37s remains)
INFO - root - 2019-11-04 01:02:51.193051: step 100500, total loss = 0.46, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 94h:33m:15s remains)
INFO - root - 2019-11-04 01:02:51.823911: step 100510, total loss = 0.47, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 91h:31m:44s remains)
INFO - root - 2019-11-04 01:02:52.469069: step 100520, total loss = 0.49, predict loss = 0.12 (68.0 examples/sec; 0.059 sec/batch; 96h:20m:18s remains)
INFO - root - 2019-11-04 01:02:53.086626: step 100530, total loss = 0.59, predict loss = 0.14 (73.4 examples/sec; 0.054 sec/batch; 89h:15m:29s remains)
INFO - root - 2019-11-04 01:02:53.727012: step 100540, total loss = 0.56, predict loss = 0.13 (64.1 examples/sec; 0.062 sec/batch; 102h:17m:46s remains)
INFO - root - 2019-11-04 01:02:54.380657: step 100550, total loss = 0.56, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 93h:55m:24s remains)
INFO - root - 2019-11-04 01:02:55.054521: step 100560, total loss = 0.54, predict loss = 0.12 (61.3 examples/sec; 0.065 sec/batch; 106h:59m:08s remains)
INFO - root - 2019-11-04 01:02:55.663790: step 100570, total loss = 0.51, predict loss = 0.12 (78.3 examples/sec; 0.051 sec/batch; 83h:44m:32s remains)
INFO - root - 2019-11-04 01:02:56.321482: step 100580, total loss = 0.52, predict loss = 0.13 (62.4 examples/sec; 0.064 sec/batch; 105h:06m:34s remains)
INFO - root - 2019-11-04 01:02:56.917811: step 100590, total loss = 0.56, predict loss = 0.14 (77.7 examples/sec; 0.051 sec/batch; 84h:20m:07s remains)
INFO - root - 2019-11-04 01:02:57.544911: step 100600, total loss = 0.53, predict loss = 0.13 (72.4 examples/sec; 0.055 sec/batch; 90h:34m:35s remains)
INFO - root - 2019-11-04 01:02:58.245264: step 100610, total loss = 0.51, predict loss = 0.12 (62.5 examples/sec; 0.064 sec/batch; 104h:50m:04s remains)
INFO - root - 2019-11-04 01:02:58.935265: step 100620, total loss = 0.45, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 96h:14m:45s remains)
INFO - root - 2019-11-04 01:02:59.582777: step 100630, total loss = 0.60, predict loss = 0.15 (61.3 examples/sec; 0.065 sec/batch; 106h:54m:16s remains)
INFO - root - 2019-11-04 01:03:00.250239: step 100640, total loss = 0.50, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 94h:55m:58s remains)
INFO - root - 2019-11-04 01:03:00.901085: step 100650, total loss = 0.56, predict loss = 0.14 (62.5 examples/sec; 0.064 sec/batch; 104h:57m:24s remains)
INFO - root - 2019-11-04 01:03:01.533481: step 100660, total loss = 0.58, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 100h:42m:35s remains)
INFO - root - 2019-11-04 01:03:02.154575: step 100670, total loss = 0.62, predict loss = 0.15 (64.2 examples/sec; 0.062 sec/batch; 102h:06m:11s remains)
INFO - root - 2019-11-04 01:03:02.770358: step 100680, total loss = 0.60, predict loss = 0.14 (74.9 examples/sec; 0.053 sec/batch; 87h:34m:13s remains)
INFO - root - 2019-11-04 01:03:03.406002: step 100690, total loss = 0.50, predict loss = 0.11 (73.3 examples/sec; 0.055 sec/batch; 89h:26m:18s remains)
INFO - root - 2019-11-04 01:03:04.098247: step 100700, total loss = 0.46, predict loss = 0.11 (69.4 examples/sec; 0.058 sec/batch; 94h:29m:25s remains)
INFO - root - 2019-11-04 01:03:04.776873: step 100710, total loss = 0.71, predict loss = 0.17 (81.1 examples/sec; 0.049 sec/batch; 80h:48m:41s remains)
INFO - root - 2019-11-04 01:03:05.371105: step 100720, total loss = 0.67, predict loss = 0.16 (75.1 examples/sec; 0.053 sec/batch; 87h:16m:16s remains)
INFO - root - 2019-11-04 01:03:05.975550: step 100730, total loss = 0.39, predict loss = 0.08 (68.8 examples/sec; 0.058 sec/batch; 95h:17m:48s remains)
INFO - root - 2019-11-04 01:03:06.610740: step 100740, total loss = 0.52, predict loss = 0.12 (72.0 examples/sec; 0.056 sec/batch; 91h:03m:13s remains)
INFO - root - 2019-11-04 01:03:07.276454: step 100750, total loss = 0.51, predict loss = 0.11 (62.8 examples/sec; 0.064 sec/batch; 104h:17m:34s remains)
INFO - root - 2019-11-04 01:03:07.940862: step 100760, total loss = 0.46, predict loss = 0.10 (61.3 examples/sec; 0.065 sec/batch; 107h:00m:02s remains)
INFO - root - 2019-11-04 01:03:08.601324: step 100770, total loss = 0.56, predict loss = 0.13 (69.7 examples/sec; 0.057 sec/batch; 94h:00m:21s remains)
INFO - root - 2019-11-04 01:03:09.241841: step 100780, total loss = 0.45, predict loss = 0.10 (73.5 examples/sec; 0.054 sec/batch; 89h:09m:05s remains)
INFO - root - 2019-11-04 01:03:09.882505: step 100790, total loss = 0.52, predict loss = 0.12 (63.6 examples/sec; 0.063 sec/batch; 103h:00m:59s remains)
INFO - root - 2019-11-04 01:03:10.604261: step 100800, total loss = 0.40, predict loss = 0.08 (71.1 examples/sec; 0.056 sec/batch; 92h:09m:21s remains)
INFO - root - 2019-11-04 01:03:11.235476: step 100810, total loss = 0.45, predict loss = 0.10 (69.0 examples/sec; 0.058 sec/batch; 95h:02m:43s remains)
INFO - root - 2019-11-04 01:03:11.850683: step 100820, total loss = 0.40, predict loss = 0.09 (74.0 examples/sec; 0.054 sec/batch; 88h:35m:54s remains)
INFO - root - 2019-11-04 01:03:12.481393: step 100830, total loss = 0.47, predict loss = 0.11 (65.5 examples/sec; 0.061 sec/batch; 100h:07m:46s remains)
INFO - root - 2019-11-04 01:03:13.155688: step 100840, total loss = 0.65, predict loss = 0.15 (67.7 examples/sec; 0.059 sec/batch; 96h:51m:30s remains)
INFO - root - 2019-11-04 01:03:13.792792: step 100850, total loss = 0.59, predict loss = 0.14 (70.9 examples/sec; 0.056 sec/batch; 92h:24m:23s remains)
INFO - root - 2019-11-04 01:03:14.410445: step 100860, total loss = 0.51, predict loss = 0.11 (74.5 examples/sec; 0.054 sec/batch; 88h:00m:46s remains)
INFO - root - 2019-11-04 01:03:15.053151: step 100870, total loss = 0.41, predict loss = 0.09 (79.3 examples/sec; 0.050 sec/batch; 82h:39m:09s remains)
INFO - root - 2019-11-04 01:03:15.666465: step 100880, total loss = 0.42, predict loss = 0.09 (69.8 examples/sec; 0.057 sec/batch; 93h:52m:04s remains)
INFO - root - 2019-11-04 01:03:16.275503: step 100890, total loss = 0.53, predict loss = 0.12 (97.1 examples/sec; 0.041 sec/batch; 67h:29m:50s remains)
INFO - root - 2019-11-04 01:03:16.741640: step 100900, total loss = 0.49, predict loss = 0.12 (91.4 examples/sec; 0.044 sec/batch; 71h:41m:27s remains)
INFO - root - 2019-11-04 01:03:17.212852: step 100910, total loss = 0.47, predict loss = 0.10 (90.3 examples/sec; 0.044 sec/batch; 72h:34m:04s remains)
INFO - root - 2019-11-04 01:03:18.315933: step 100920, total loss = 0.26, predict loss = 0.05 (72.2 examples/sec; 0.055 sec/batch; 90h:44m:47s remains)
INFO - root - 2019-11-04 01:03:18.948004: step 100930, total loss = 0.37, predict loss = 0.08 (69.1 examples/sec; 0.058 sec/batch; 94h:49m:02s remains)
INFO - root - 2019-11-04 01:03:19.667970: step 100940, total loss = 0.50, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 98h:01m:14s remains)
INFO - root - 2019-11-04 01:03:20.297558: step 100950, total loss = 0.49, predict loss = 0.11 (71.8 examples/sec; 0.056 sec/batch; 91h:18m:04s remains)
INFO - root - 2019-11-04 01:03:20.924753: step 100960, total loss = 0.52, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 95h:22m:35s remains)
INFO - root - 2019-11-04 01:03:21.563256: step 100970, total loss = 0.58, predict loss = 0.13 (75.8 examples/sec; 0.053 sec/batch; 86h:25m:28s remains)
INFO - root - 2019-11-04 01:03:22.195739: step 100980, total loss = 0.36, predict loss = 0.08 (67.9 examples/sec; 0.059 sec/batch; 96h:32m:00s remains)
INFO - root - 2019-11-04 01:03:22.870362: step 100990, total loss = 0.59, predict loss = 0.13 (60.9 examples/sec; 0.066 sec/batch; 107h:36m:11s remains)
INFO - root - 2019-11-04 01:03:23.520952: step 101000, total loss = 0.68, predict loss = 0.17 (70.1 examples/sec; 0.057 sec/batch; 93h:29m:03s remains)
INFO - root - 2019-11-04 01:03:24.149170: step 101010, total loss = 0.73, predict loss = 0.16 (70.0 examples/sec; 0.057 sec/batch; 93h:34m:29s remains)
INFO - root - 2019-11-04 01:03:24.766660: step 101020, total loss = 0.51, predict loss = 0.12 (67.7 examples/sec; 0.059 sec/batch; 96h:47m:36s remains)
INFO - root - 2019-11-04 01:03:25.370040: step 101030, total loss = 0.51, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 93h:31m:05s remains)
INFO - root - 2019-11-04 01:03:26.006302: step 101040, total loss = 0.53, predict loss = 0.12 (72.1 examples/sec; 0.055 sec/batch; 90h:52m:38s remains)
INFO - root - 2019-11-04 01:03:26.654888: step 101050, total loss = 0.45, predict loss = 0.10 (79.3 examples/sec; 0.050 sec/batch; 82h:39m:44s remains)
INFO - root - 2019-11-04 01:03:27.341251: step 101060, total loss = 0.41, predict loss = 0.09 (75.5 examples/sec; 0.053 sec/batch; 86h:47m:39s remains)
INFO - root - 2019-11-04 01:03:27.988114: step 101070, total loss = 0.50, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 95h:08m:11s remains)
INFO - root - 2019-11-04 01:03:28.625647: step 101080, total loss = 0.47, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:40m:43s remains)
INFO - root - 2019-11-04 01:03:29.276217: step 101090, total loss = 0.51, predict loss = 0.12 (65.3 examples/sec; 0.061 sec/batch; 100h:22m:37s remains)
INFO - root - 2019-11-04 01:03:29.932484: step 101100, total loss = 0.48, predict loss = 0.10 (73.3 examples/sec; 0.055 sec/batch; 89h:27m:58s remains)
INFO - root - 2019-11-04 01:03:30.599982: step 101110, total loss = 0.38, predict loss = 0.08 (64.8 examples/sec; 0.062 sec/batch; 101h:04m:43s remains)
INFO - root - 2019-11-04 01:03:31.250231: step 101120, total loss = 0.53, predict loss = 0.13 (80.5 examples/sec; 0.050 sec/batch; 81h:25m:06s remains)
INFO - root - 2019-11-04 01:03:31.945510: step 101130, total loss = 0.48, predict loss = 0.11 (61.3 examples/sec; 0.065 sec/batch; 106h:55m:50s remains)
INFO - root - 2019-11-04 01:03:32.615098: step 101140, total loss = 0.43, predict loss = 0.09 (76.3 examples/sec; 0.052 sec/batch; 85h:56m:34s remains)
INFO - root - 2019-11-04 01:03:33.264988: step 101150, total loss = 0.50, predict loss = 0.11 (63.0 examples/sec; 0.064 sec/batch; 104h:06m:38s remains)
INFO - root - 2019-11-04 01:03:33.996695: step 101160, total loss = 0.49, predict loss = 0.12 (76.6 examples/sec; 0.052 sec/batch; 85h:32m:43s remains)
INFO - root - 2019-11-04 01:03:34.661329: step 101170, total loss = 0.56, predict loss = 0.13 (74.8 examples/sec; 0.053 sec/batch; 87h:36m:27s remains)
INFO - root - 2019-11-04 01:03:35.271153: step 101180, total loss = 0.64, predict loss = 0.15 (76.6 examples/sec; 0.052 sec/batch; 85h:35m:45s remains)
INFO - root - 2019-11-04 01:03:35.899005: step 101190, total loss = 0.48, predict loss = 0.09 (75.7 examples/sec; 0.053 sec/batch; 86h:35m:25s remains)
INFO - root - 2019-11-04 01:03:36.559547: step 101200, total loss = 0.57, predict loss = 0.13 (62.1 examples/sec; 0.064 sec/batch; 105h:36m:19s remains)
INFO - root - 2019-11-04 01:03:37.224444: step 101210, total loss = 0.50, predict loss = 0.11 (66.8 examples/sec; 0.060 sec/batch; 98h:06m:56s remains)
INFO - root - 2019-11-04 01:03:37.856846: step 101220, total loss = 0.57, predict loss = 0.13 (67.4 examples/sec; 0.059 sec/batch; 97h:14m:04s remains)
INFO - root - 2019-11-04 01:03:38.521276: step 101230, total loss = 0.65, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 90h:46m:27s remains)
INFO - root - 2019-11-04 01:03:39.159650: step 101240, total loss = 0.58, predict loss = 0.13 (76.3 examples/sec; 0.052 sec/batch; 85h:54m:10s remains)
INFO - root - 2019-11-04 01:03:39.813590: step 101250, total loss = 0.49, predict loss = 0.11 (65.9 examples/sec; 0.061 sec/batch; 99h:31m:48s remains)
INFO - root - 2019-11-04 01:03:40.443238: step 101260, total loss = 0.50, predict loss = 0.11 (83.5 examples/sec; 0.048 sec/batch; 78h:28m:13s remains)
INFO - root - 2019-11-04 01:03:41.061249: step 101270, total loss = 0.46, predict loss = 0.09 (77.0 examples/sec; 0.052 sec/batch; 85h:07m:06s remains)
INFO - root - 2019-11-04 01:03:41.701024: step 101280, total loss = 0.42, predict loss = 0.09 (71.0 examples/sec; 0.056 sec/batch; 92h:21m:07s remains)
INFO - root - 2019-11-04 01:03:42.321853: step 101290, total loss = 0.47, predict loss = 0.11 (82.3 examples/sec; 0.049 sec/batch; 79h:40m:52s remains)
INFO - root - 2019-11-04 01:03:42.957972: step 101300, total loss = 0.50, predict loss = 0.11 (72.6 examples/sec; 0.055 sec/batch; 90h:15m:46s remains)
INFO - root - 2019-11-04 01:03:43.629811: step 101310, total loss = 0.47, predict loss = 0.11 (61.1 examples/sec; 0.066 sec/batch; 107h:21m:14s remains)
INFO - root - 2019-11-04 01:03:44.277637: step 101320, total loss = 0.43, predict loss = 0.09 (77.2 examples/sec; 0.052 sec/batch; 84h:52m:04s remains)
INFO - root - 2019-11-04 01:03:44.914745: step 101330, total loss = 0.51, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 97h:27m:18s remains)
INFO - root - 2019-11-04 01:03:45.548763: step 101340, total loss = 0.44, predict loss = 0.10 (69.8 examples/sec; 0.057 sec/batch; 93h:57m:26s remains)
INFO - root - 2019-11-04 01:03:46.181990: step 101350, total loss = 0.50, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 95h:07m:46s remains)
INFO - root - 2019-11-04 01:03:46.813222: step 101360, total loss = 0.66, predict loss = 0.16 (71.0 examples/sec; 0.056 sec/batch; 92h:17m:20s remains)
INFO - root - 2019-11-04 01:03:47.428551: step 101370, total loss = 0.46, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 93h:55m:45s remains)
INFO - root - 2019-11-04 01:03:48.074531: step 101380, total loss = 0.26, predict loss = 0.06 (68.0 examples/sec; 0.059 sec/batch; 96h:25m:45s remains)
INFO - root - 2019-11-04 01:03:48.763358: step 101390, total loss = 0.42, predict loss = 0.10 (59.8 examples/sec; 0.067 sec/batch; 109h:36m:38s remains)
INFO - root - 2019-11-04 01:03:49.441559: step 101400, total loss = 0.84, predict loss = 0.22 (78.2 examples/sec; 0.051 sec/batch; 83h:45m:38s remains)
INFO - root - 2019-11-04 01:03:50.071842: step 101410, total loss = 0.57, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 93h:15m:07s remains)
INFO - root - 2019-11-04 01:03:50.724610: step 101420, total loss = 0.65, predict loss = 0.15 (64.1 examples/sec; 0.062 sec/batch; 102h:19m:14s remains)
INFO - root - 2019-11-04 01:03:51.373977: step 101430, total loss = 0.52, predict loss = 0.13 (68.2 examples/sec; 0.059 sec/batch; 96h:09m:58s remains)
INFO - root - 2019-11-04 01:03:51.984704: step 101440, total loss = 0.55, predict loss = 0.13 (77.2 examples/sec; 0.052 sec/batch; 84h:55m:07s remains)
INFO - root - 2019-11-04 01:03:52.628675: step 101450, total loss = 0.53, predict loss = 0.13 (60.8 examples/sec; 0.066 sec/batch; 107h:47m:25s remains)
INFO - root - 2019-11-04 01:03:53.261970: step 101460, total loss = 0.41, predict loss = 0.09 (87.4 examples/sec; 0.046 sec/batch; 75h:00m:15s remains)
INFO - root - 2019-11-04 01:03:53.870747: step 101470, total loss = 0.41, predict loss = 0.09 (67.7 examples/sec; 0.059 sec/batch; 96h:47m:18s remains)
INFO - root - 2019-11-04 01:03:54.500210: step 101480, total loss = 0.44, predict loss = 0.11 (65.8 examples/sec; 0.061 sec/batch; 99h:32m:26s remains)
INFO - root - 2019-11-04 01:03:55.163758: step 101490, total loss = 0.44, predict loss = 0.11 (64.8 examples/sec; 0.062 sec/batch; 101h:06m:39s remains)
INFO - root - 2019-11-04 01:03:55.784564: step 101500, total loss = 0.34, predict loss = 0.07 (67.7 examples/sec; 0.059 sec/batch; 96h:44m:37s remains)
INFO - root - 2019-11-04 01:03:56.434758: step 101510, total loss = 0.41, predict loss = 0.10 (64.5 examples/sec; 0.062 sec/batch; 101h:32m:29s remains)
INFO - root - 2019-11-04 01:03:57.087660: step 101520, total loss = 0.43, predict loss = 0.10 (67.0 examples/sec; 0.060 sec/batch; 97h:50m:01s remains)
INFO - root - 2019-11-04 01:03:57.755618: step 101530, total loss = 0.28, predict loss = 0.05 (67.8 examples/sec; 0.059 sec/batch; 96h:43m:02s remains)
INFO - root - 2019-11-04 01:03:58.430473: step 101540, total loss = 0.34, predict loss = 0.08 (76.0 examples/sec; 0.053 sec/batch; 86h:15m:14s remains)
INFO - root - 2019-11-04 01:03:59.040898: step 101550, total loss = 0.33, predict loss = 0.08 (83.7 examples/sec; 0.048 sec/batch; 78h:15m:43s remains)
INFO - root - 2019-11-04 01:03:59.699032: step 101560, total loss = 0.25, predict loss = 0.05 (63.0 examples/sec; 0.064 sec/batch; 104h:04m:13s remains)
INFO - root - 2019-11-04 01:04:00.345105: step 101570, total loss = 0.50, predict loss = 0.11 (62.2 examples/sec; 0.064 sec/batch; 105h:19m:49s remains)
INFO - root - 2019-11-04 01:04:00.974191: step 101580, total loss = 0.39, predict loss = 0.09 (61.6 examples/sec; 0.065 sec/batch; 106h:28m:29s remains)
INFO - root - 2019-11-04 01:04:01.607845: step 101590, total loss = 0.46, predict loss = 0.10 (68.7 examples/sec; 0.058 sec/batch; 95h:23m:17s remains)
INFO - root - 2019-11-04 01:04:02.206678: step 101600, total loss = 0.47, predict loss = 0.11 (66.7 examples/sec; 0.060 sec/batch; 98h:13m:21s remains)
INFO - root - 2019-11-04 01:04:02.815299: step 101610, total loss = 0.51, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 96h:29m:43s remains)
INFO - root - 2019-11-04 01:04:03.435602: step 101620, total loss = 0.77, predict loss = 0.18 (66.6 examples/sec; 0.060 sec/batch; 98h:20m:57s remains)
INFO - root - 2019-11-04 01:04:04.122430: step 101630, total loss = 0.50, predict loss = 0.12 (72.1 examples/sec; 0.056 sec/batch; 90h:56m:54s remains)
INFO - root - 2019-11-04 01:04:04.811573: step 101640, total loss = 0.56, predict loss = 0.13 (69.9 examples/sec; 0.057 sec/batch; 93h:46m:03s remains)
INFO - root - 2019-11-04 01:04:05.417586: step 101650, total loss = 0.44, predict loss = 0.10 (70.9 examples/sec; 0.056 sec/batch; 92h:23m:19s remains)
INFO - root - 2019-11-04 01:04:06.050071: step 101660, total loss = 0.54, predict loss = 0.13 (61.9 examples/sec; 0.065 sec/batch; 105h:56m:56s remains)
INFO - root - 2019-11-04 01:04:06.655873: step 101670, total loss = 0.41, predict loss = 0.10 (74.9 examples/sec; 0.053 sec/batch; 87h:27m:10s remains)
INFO - root - 2019-11-04 01:04:07.304491: step 101680, total loss = 0.60, predict loss = 0.15 (67.5 examples/sec; 0.059 sec/batch; 97h:04m:29s remains)
INFO - root - 2019-11-04 01:04:07.966670: step 101690, total loss = 0.43, predict loss = 0.10 (67.4 examples/sec; 0.059 sec/batch; 97h:10m:21s remains)
INFO - root - 2019-11-04 01:04:08.655530: step 101700, total loss = 0.49, predict loss = 0.12 (67.4 examples/sec; 0.059 sec/batch; 97h:14m:26s remains)
INFO - root - 2019-11-04 01:04:09.313738: step 101710, total loss = 0.57, predict loss = 0.14 (65.5 examples/sec; 0.061 sec/batch; 100h:02m:48s remains)
INFO - root - 2019-11-04 01:04:10.018144: step 101720, total loss = 0.59, predict loss = 0.14 (68.3 examples/sec; 0.059 sec/batch; 95h:56m:29s remains)
INFO - root - 2019-11-04 01:04:10.638336: step 101730, total loss = 0.37, predict loss = 0.08 (71.7 examples/sec; 0.056 sec/batch; 91h:23m:41s remains)
INFO - root - 2019-11-04 01:04:11.239807: step 101740, total loss = 0.24, predict loss = 0.05 (74.2 examples/sec; 0.054 sec/batch; 88h:22m:22s remains)
INFO - root - 2019-11-04 01:04:11.828004: step 101750, total loss = 0.30, predict loss = 0.07 (85.0 examples/sec; 0.047 sec/batch; 77h:04m:23s remains)
INFO - root - 2019-11-04 01:04:12.447026: step 101760, total loss = 0.26, predict loss = 0.05 (70.8 examples/sec; 0.057 sec/batch; 92h:35m:45s remains)
INFO - root - 2019-11-04 01:04:13.102064: step 101770, total loss = 0.40, predict loss = 0.09 (74.6 examples/sec; 0.054 sec/batch; 87h:53m:04s remains)
INFO - root - 2019-11-04 01:04:13.737523: step 101780, total loss = 0.40, predict loss = 0.09 (68.4 examples/sec; 0.058 sec/batch; 95h:46m:44s remains)
INFO - root - 2019-11-04 01:04:14.372869: step 101790, total loss = 0.30, predict loss = 0.07 (67.5 examples/sec; 0.059 sec/batch; 97h:03m:46s remains)
INFO - root - 2019-11-04 01:04:14.997762: step 101800, total loss = 0.29, predict loss = 0.06 (74.4 examples/sec; 0.054 sec/batch; 88h:04m:47s remains)
INFO - root - 2019-11-04 01:04:15.642120: step 101810, total loss = 0.30, predict loss = 0.06 (68.7 examples/sec; 0.058 sec/batch; 95h:23m:45s remains)
INFO - root - 2019-11-04 01:04:16.278635: step 101820, total loss = 0.40, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 92h:22m:43s remains)
INFO - root - 2019-11-04 01:04:16.922409: step 101830, total loss = 0.49, predict loss = 0.11 (80.3 examples/sec; 0.050 sec/batch; 81h:36m:55s remains)
INFO - root - 2019-11-04 01:04:17.571870: step 101840, total loss = 0.38, predict loss = 0.09 (67.2 examples/sec; 0.060 sec/batch; 97h:33m:13s remains)
INFO - root - 2019-11-04 01:04:18.205325: step 101850, total loss = 0.35, predict loss = 0.08 (79.5 examples/sec; 0.050 sec/batch; 82h:28m:25s remains)
INFO - root - 2019-11-04 01:04:18.838014: step 101860, total loss = 0.38, predict loss = 0.09 (66.7 examples/sec; 0.060 sec/batch; 98h:19m:35s remains)
INFO - root - 2019-11-04 01:04:19.461046: step 101870, total loss = 0.50, predict loss = 0.12 (63.7 examples/sec; 0.063 sec/batch; 102h:49m:38s remains)
INFO - root - 2019-11-04 01:04:20.100690: step 101880, total loss = 0.32, predict loss = 0.07 (71.0 examples/sec; 0.056 sec/batch; 92h:19m:58s remains)
INFO - root - 2019-11-04 01:04:20.767891: step 101890, total loss = 0.30, predict loss = 0.06 (69.7 examples/sec; 0.057 sec/batch; 94h:04m:04s remains)
INFO - root - 2019-11-04 01:04:21.449053: step 101900, total loss = 0.61, predict loss = 0.15 (66.5 examples/sec; 0.060 sec/batch; 98h:29m:51s remains)
INFO - root - 2019-11-04 01:04:22.112850: step 101910, total loss = 0.60, predict loss = 0.15 (59.6 examples/sec; 0.067 sec/batch; 110h:01m:49s remains)
INFO - root - 2019-11-04 01:04:22.788875: step 101920, total loss = 0.53, predict loss = 0.13 (69.4 examples/sec; 0.058 sec/batch; 94h:24m:49s remains)
INFO - root - 2019-11-04 01:04:23.411251: step 101930, total loss = 0.40, predict loss = 0.10 (85.9 examples/sec; 0.047 sec/batch; 76h:19m:40s remains)
INFO - root - 2019-11-04 01:04:24.044498: step 101940, total loss = 0.57, predict loss = 0.14 (65.6 examples/sec; 0.061 sec/batch; 99h:49m:59s remains)
INFO - root - 2019-11-04 01:04:24.743901: step 101950, total loss = 0.46, predict loss = 0.11 (62.9 examples/sec; 0.064 sec/batch; 104h:07m:03s remains)
INFO - root - 2019-11-04 01:04:25.386793: step 101960, total loss = 0.46, predict loss = 0.11 (68.7 examples/sec; 0.058 sec/batch; 95h:23m:35s remains)
INFO - root - 2019-11-04 01:04:26.021311: step 101970, total loss = 0.41, predict loss = 0.10 (73.7 examples/sec; 0.054 sec/batch; 88h:52m:34s remains)
INFO - root - 2019-11-04 01:04:26.669203: step 101980, total loss = 0.36, predict loss = 0.08 (60.4 examples/sec; 0.066 sec/batch; 108h:25m:09s remains)
INFO - root - 2019-11-04 01:04:27.336767: step 101990, total loss = 0.19, predict loss = 0.03 (77.8 examples/sec; 0.051 sec/batch; 84h:12m:55s remains)
INFO - root - 2019-11-04 01:04:27.970431: step 102000, total loss = 0.27, predict loss = 0.06 (76.3 examples/sec; 0.052 sec/batch; 85h:55m:13s remains)
INFO - root - 2019-11-04 01:04:28.596524: step 102010, total loss = 0.38, predict loss = 0.08 (75.2 examples/sec; 0.053 sec/batch; 87h:08m:35s remains)
INFO - root - 2019-11-04 01:04:29.227845: step 102020, total loss = 0.35, predict loss = 0.08 (72.5 examples/sec; 0.055 sec/batch; 90h:21m:05s remains)
INFO - root - 2019-11-04 01:04:29.870471: step 102030, total loss = 0.27, predict loss = 0.06 (73.6 examples/sec; 0.054 sec/batch; 89h:03m:42s remains)
INFO - root - 2019-11-04 01:04:30.577294: step 102040, total loss = 0.52, predict loss = 0.12 (63.6 examples/sec; 0.063 sec/batch; 103h:06m:14s remains)
INFO - root - 2019-11-04 01:04:31.258855: step 102050, total loss = 0.47, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 99h:17m:26s remains)
INFO - root - 2019-11-04 01:04:31.918722: step 102060, total loss = 0.41, predict loss = 0.10 (67.0 examples/sec; 0.060 sec/batch; 97h:52m:45s remains)
INFO - root - 2019-11-04 01:04:32.603563: step 102070, total loss = 0.38, predict loss = 0.08 (64.2 examples/sec; 0.062 sec/batch; 102h:01m:05s remains)
INFO - root - 2019-11-04 01:04:33.214631: step 102080, total loss = 0.47, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 93h:49m:14s remains)
INFO - root - 2019-11-04 01:04:33.860991: step 102090, total loss = 0.38, predict loss = 0.09 (72.9 examples/sec; 0.055 sec/batch; 89h:53m:41s remains)
INFO - root - 2019-11-04 01:04:34.529493: step 102100, total loss = 0.36, predict loss = 0.07 (65.3 examples/sec; 0.061 sec/batch; 100h:20m:32s remains)
INFO - root - 2019-11-04 01:04:35.164959: step 102110, total loss = 0.28, predict loss = 0.07 (70.8 examples/sec; 0.056 sec/batch; 92h:31m:24s remains)
INFO - root - 2019-11-04 01:04:35.779087: step 102120, total loss = 0.23, predict loss = 0.05 (74.3 examples/sec; 0.054 sec/batch; 88h:08m:46s remains)
INFO - root - 2019-11-04 01:04:36.391507: step 102130, total loss = 0.59, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 90h:45m:03s remains)
INFO - root - 2019-11-04 01:04:37.041317: step 102140, total loss = 0.42, predict loss = 0.09 (63.5 examples/sec; 0.063 sec/batch; 103h:12m:42s remains)
INFO - root - 2019-11-04 01:04:37.732157: step 102150, total loss = 0.54, predict loss = 0.12 (59.1 examples/sec; 0.068 sec/batch; 110h:49m:27s remains)
INFO - root - 2019-11-04 01:04:38.387344: step 102160, total loss = 0.55, predict loss = 0.13 (62.2 examples/sec; 0.064 sec/batch; 105h:20m:53s remains)
INFO - root - 2019-11-04 01:04:39.022637: step 102170, total loss = 0.55, predict loss = 0.13 (75.2 examples/sec; 0.053 sec/batch; 87h:11m:59s remains)
INFO - root - 2019-11-04 01:04:39.656275: step 102180, total loss = 0.64, predict loss = 0.15 (61.9 examples/sec; 0.065 sec/batch; 105h:50m:55s remains)
INFO - root - 2019-11-04 01:04:40.327540: step 102190, total loss = 0.50, predict loss = 0.11 (71.5 examples/sec; 0.056 sec/batch; 91h:37m:42s remains)
INFO - root - 2019-11-04 01:04:40.960093: step 102200, total loss = 0.47, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 88h:37m:52s remains)
INFO - root - 2019-11-04 01:04:41.594656: step 102210, total loss = 0.42, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 97h:20m:30s remains)
INFO - root - 2019-11-04 01:04:42.242256: step 102220, total loss = 0.57, predict loss = 0.13 (65.9 examples/sec; 0.061 sec/batch; 99h:23m:51s remains)
INFO - root - 2019-11-04 01:04:42.913451: step 102230, total loss = 0.42, predict loss = 0.09 (65.8 examples/sec; 0.061 sec/batch; 99h:31m:33s remains)
INFO - root - 2019-11-04 01:04:43.565611: step 102240, total loss = 0.30, predict loss = 0.06 (68.6 examples/sec; 0.058 sec/batch; 95h:29m:57s remains)
INFO - root - 2019-11-04 01:04:44.188280: step 102250, total loss = 0.26, predict loss = 0.06 (74.0 examples/sec; 0.054 sec/batch; 88h:33m:45s remains)
INFO - root - 2019-11-04 01:04:44.832487: step 102260, total loss = 0.45, predict loss = 0.11 (68.3 examples/sec; 0.059 sec/batch; 95h:59m:05s remains)
INFO - root - 2019-11-04 01:04:45.466042: step 102270, total loss = 0.37, predict loss = 0.09 (69.4 examples/sec; 0.058 sec/batch; 94h:27m:12s remains)
INFO - root - 2019-11-04 01:04:46.128449: step 102280, total loss = 0.12, predict loss = 0.03 (76.8 examples/sec; 0.052 sec/batch; 85h:20m:16s remains)
INFO - root - 2019-11-04 01:04:46.736605: step 102290, total loss = 0.63, predict loss = 0.15 (82.6 examples/sec; 0.048 sec/batch; 79h:19m:45s remains)
INFO - root - 2019-11-04 01:04:47.345543: step 102300, total loss = 0.52, predict loss = 0.12 (79.8 examples/sec; 0.050 sec/batch; 82h:07m:05s remains)
INFO - root - 2019-11-04 01:04:47.965475: step 102310, total loss = 0.53, predict loss = 0.12 (71.9 examples/sec; 0.056 sec/batch; 91h:06m:48s remains)
INFO - root - 2019-11-04 01:04:48.592831: step 102320, total loss = 0.51, predict loss = 0.12 (65.0 examples/sec; 0.062 sec/batch; 100h:50m:59s remains)
INFO - root - 2019-11-04 01:04:49.256848: step 102330, total loss = 0.81, predict loss = 0.20 (67.2 examples/sec; 0.060 sec/batch; 97h:29m:21s remains)
INFO - root - 2019-11-04 01:04:49.891513: step 102340, total loss = 0.65, predict loss = 0.15 (69.7 examples/sec; 0.057 sec/batch; 93h:58m:00s remains)
INFO - root - 2019-11-04 01:04:50.525460: step 102350, total loss = 0.66, predict loss = 0.15 (67.5 examples/sec; 0.059 sec/batch; 97h:04m:33s remains)
INFO - root - 2019-11-04 01:04:51.150371: step 102360, total loss = 0.89, predict loss = 0.21 (66.7 examples/sec; 0.060 sec/batch; 98h:12m:59s remains)
INFO - root - 2019-11-04 01:04:51.815119: step 102370, total loss = 0.52, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 101h:58m:53s remains)
INFO - root - 2019-11-04 01:04:52.463255: step 102380, total loss = 0.87, predict loss = 0.21 (70.3 examples/sec; 0.057 sec/batch; 93h:10m:04s remains)
INFO - root - 2019-11-04 01:04:53.120495: step 102390, total loss = 0.76, predict loss = 0.19 (63.3 examples/sec; 0.063 sec/batch; 103h:35m:58s remains)
INFO - root - 2019-11-04 01:04:53.820822: step 102400, total loss = 0.68, predict loss = 0.16 (63.8 examples/sec; 0.063 sec/batch; 102h:41m:00s remains)
INFO - root - 2019-11-04 01:04:54.455557: step 102410, total loss = 0.49, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 84h:27m:57s remains)
INFO - root - 2019-11-04 01:04:55.121431: step 102420, total loss = 0.57, predict loss = 0.13 (63.4 examples/sec; 0.063 sec/batch; 103h:26m:05s remains)
INFO - root - 2019-11-04 01:04:55.757726: step 102430, total loss = 0.57, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 93h:14m:30s remains)
INFO - root - 2019-11-04 01:04:56.413693: step 102440, total loss = 0.53, predict loss = 0.11 (69.6 examples/sec; 0.057 sec/batch; 94h:06m:41s remains)
INFO - root - 2019-11-04 01:04:57.036564: step 102450, total loss = 0.56, predict loss = 0.13 (72.5 examples/sec; 0.055 sec/batch; 90h:26m:28s remains)
INFO - root - 2019-11-04 01:04:57.687816: step 102460, total loss = 0.53, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 91h:21m:21s remains)
INFO - root - 2019-11-04 01:04:58.369083: step 102470, total loss = 0.49, predict loss = 0.11 (60.4 examples/sec; 0.066 sec/batch; 108h:28m:42s remains)
INFO - root - 2019-11-04 01:04:59.040431: step 102480, total loss = 0.58, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 100h:36m:44s remains)
INFO - root - 2019-11-04 01:04:59.700700: step 102490, total loss = 0.53, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 98h:29m:40s remains)
INFO - root - 2019-11-04 01:05:00.357851: step 102500, total loss = 0.52, predict loss = 0.12 (74.4 examples/sec; 0.054 sec/batch; 88h:06m:30s remains)
INFO - root - 2019-11-04 01:05:00.997675: step 102510, total loss = 0.43, predict loss = 0.10 (64.8 examples/sec; 0.062 sec/batch; 101h:07m:52s remains)
INFO - root - 2019-11-04 01:05:01.608124: step 102520, total loss = 0.52, predict loss = 0.12 (67.6 examples/sec; 0.059 sec/batch; 96h:54m:16s remains)
INFO - root - 2019-11-04 01:05:02.225316: step 102530, total loss = 0.41, predict loss = 0.10 (69.5 examples/sec; 0.058 sec/batch; 94h:16m:08s remains)
INFO - root - 2019-11-04 01:05:02.840930: step 102540, total loss = 0.62, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 100h:40m:52s remains)
INFO - root - 2019-11-04 01:05:03.467567: step 102550, total loss = 0.53, predict loss = 0.12 (68.6 examples/sec; 0.058 sec/batch; 95h:35m:20s remains)
INFO - root - 2019-11-04 01:05:04.118927: step 102560, total loss = 0.53, predict loss = 0.13 (68.3 examples/sec; 0.059 sec/batch; 95h:54m:34s remains)
INFO - root - 2019-11-04 01:05:04.791106: step 102570, total loss = 0.40, predict loss = 0.09 (78.6 examples/sec; 0.051 sec/batch; 83h:20m:38s remains)
INFO - root - 2019-11-04 01:05:05.399049: step 102580, total loss = 0.32, predict loss = 0.07 (63.7 examples/sec; 0.063 sec/batch; 102h:48m:03s remains)
INFO - root - 2019-11-04 01:05:06.098104: step 102590, total loss = 0.49, predict loss = 0.11 (59.4 examples/sec; 0.067 sec/batch; 110h:19m:45s remains)
INFO - root - 2019-11-04 01:05:06.722403: step 102600, total loss = 0.43, predict loss = 0.10 (75.5 examples/sec; 0.053 sec/batch; 86h:48m:37s remains)
INFO - root - 2019-11-04 01:05:07.345251: step 102610, total loss = 0.39, predict loss = 0.09 (68.6 examples/sec; 0.058 sec/batch; 95h:28m:01s remains)
INFO - root - 2019-11-04 01:05:07.985821: step 102620, total loss = 0.36, predict loss = 0.08 (63.3 examples/sec; 0.063 sec/batch; 103h:33m:38s remains)
INFO - root - 2019-11-04 01:05:08.691423: step 102630, total loss = 0.45, predict loss = 0.11 (67.6 examples/sec; 0.059 sec/batch; 96h:58m:30s remains)
INFO - root - 2019-11-04 01:05:09.438069: step 102640, total loss = 0.47, predict loss = 0.11 (65.4 examples/sec; 0.061 sec/batch; 100h:13m:39s remains)
INFO - root - 2019-11-04 01:05:10.118009: step 102650, total loss = 0.42, predict loss = 0.09 (62.0 examples/sec; 0.065 sec/batch; 105h:40m:23s remains)
INFO - root - 2019-11-04 01:05:11.272469: step 102660, total loss = 0.44, predict loss = 0.10 (64.9 examples/sec; 0.062 sec/batch; 100h:58m:57s remains)
INFO - root - 2019-11-04 01:05:11.945127: step 102670, total loss = 0.45, predict loss = 0.10 (75.5 examples/sec; 0.053 sec/batch; 86h:50m:10s remains)
INFO - root - 2019-11-04 01:05:12.622058: step 102680, total loss = 0.36, predict loss = 0.07 (70.0 examples/sec; 0.057 sec/batch; 93h:35m:14s remains)
INFO - root - 2019-11-04 01:05:13.296107: step 102690, total loss = 0.43, predict loss = 0.10 (64.2 examples/sec; 0.062 sec/batch; 102h:02m:09s remains)
INFO - root - 2019-11-04 01:05:13.961951: step 102700, total loss = 0.56, predict loss = 0.13 (65.4 examples/sec; 0.061 sec/batch; 100h:13m:20s remains)
INFO - root - 2019-11-04 01:05:14.635344: step 102710, total loss = 0.58, predict loss = 0.14 (74.5 examples/sec; 0.054 sec/batch; 87h:58m:01s remains)
INFO - root - 2019-11-04 01:05:15.332636: step 102720, total loss = 0.47, predict loss = 0.11 (57.1 examples/sec; 0.070 sec/batch; 114h:43m:31s remains)
INFO - root - 2019-11-04 01:05:15.975990: step 102730, total loss = 0.55, predict loss = 0.13 (74.6 examples/sec; 0.054 sec/batch; 87h:47m:53s remains)
INFO - root - 2019-11-04 01:05:16.641357: step 102740, total loss = 0.46, predict loss = 0.11 (68.0 examples/sec; 0.059 sec/batch; 96h:23m:27s remains)
INFO - root - 2019-11-04 01:05:17.291595: step 102750, total loss = 0.56, predict loss = 0.13 (75.7 examples/sec; 0.053 sec/batch; 86h:34m:45s remains)
INFO - root - 2019-11-04 01:05:17.911134: step 102760, total loss = 0.58, predict loss = 0.14 (69.2 examples/sec; 0.058 sec/batch; 94h:37m:32s remains)
INFO - root - 2019-11-04 01:05:18.589241: step 102770, total loss = 0.50, predict loss = 0.12 (58.6 examples/sec; 0.068 sec/batch; 111h:46m:00s remains)
INFO - root - 2019-11-04 01:05:19.200435: step 102780, total loss = 0.52, predict loss = 0.12 (77.3 examples/sec; 0.052 sec/batch; 84h:44m:01s remains)
INFO - root - 2019-11-04 01:05:19.837601: step 102790, total loss = 0.60, predict loss = 0.15 (70.7 examples/sec; 0.057 sec/batch; 92h:41m:41s remains)
INFO - root - 2019-11-04 01:05:20.474386: step 102800, total loss = 0.60, predict loss = 0.14 (68.8 examples/sec; 0.058 sec/batch; 95h:11m:57s remains)
INFO - root - 2019-11-04 01:05:21.115889: step 102810, total loss = 0.65, predict loss = 0.15 (62.9 examples/sec; 0.064 sec/batch; 104h:06m:33s remains)
INFO - root - 2019-11-04 01:05:21.798250: step 102820, total loss = 0.74, predict loss = 0.18 (66.2 examples/sec; 0.060 sec/batch; 98h:56m:33s remains)
INFO - root - 2019-11-04 01:05:22.460274: step 102830, total loss = 0.57, predict loss = 0.13 (66.1 examples/sec; 0.061 sec/batch; 99h:08m:26s remains)
INFO - root - 2019-11-04 01:05:23.085761: step 102840, total loss = 0.59, predict loss = 0.14 (71.5 examples/sec; 0.056 sec/batch; 91h:38m:27s remains)
INFO - root - 2019-11-04 01:05:23.746527: step 102850, total loss = 0.62, predict loss = 0.14 (71.5 examples/sec; 0.056 sec/batch; 91h:39m:34s remains)
INFO - root - 2019-11-04 01:05:24.383177: step 102860, total loss = 0.45, predict loss = 0.11 (79.2 examples/sec; 0.051 sec/batch; 82h:44m:05s remains)
INFO - root - 2019-11-04 01:05:25.047123: step 102870, total loss = 0.54, predict loss = 0.12 (66.7 examples/sec; 0.060 sec/batch; 98h:14m:24s remains)
INFO - root - 2019-11-04 01:05:25.739585: step 102880, total loss = 0.52, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 95h:15m:50s remains)
INFO - root - 2019-11-04 01:05:26.421954: step 102890, total loss = 0.46, predict loss = 0.11 (66.7 examples/sec; 0.060 sec/batch; 98h:17m:16s remains)
INFO - root - 2019-11-04 01:05:27.054238: step 102900, total loss = 0.50, predict loss = 0.11 (78.3 examples/sec; 0.051 sec/batch; 83h:38m:20s remains)
INFO - root - 2019-11-04 01:05:27.699779: step 102910, total loss = 0.41, predict loss = 0.09 (67.8 examples/sec; 0.059 sec/batch; 96h:36m:17s remains)
INFO - root - 2019-11-04 01:05:28.331512: step 102920, total loss = 0.28, predict loss = 0.06 (67.6 examples/sec; 0.059 sec/batch; 96h:56m:02s remains)
INFO - root - 2019-11-04 01:05:28.969912: step 102930, total loss = 0.51, predict loss = 0.12 (82.1 examples/sec; 0.049 sec/batch; 79h:47m:52s remains)
INFO - root - 2019-11-04 01:05:29.629370: step 102940, total loss = 0.48, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 85h:28m:38s remains)
INFO - root - 2019-11-04 01:05:30.285988: step 102950, total loss = 0.52, predict loss = 0.12 (76.1 examples/sec; 0.053 sec/batch; 86h:04m:25s remains)
INFO - root - 2019-11-04 01:05:30.921450: step 102960, total loss = 0.56, predict loss = 0.13 (70.4 examples/sec; 0.057 sec/batch; 93h:07m:58s remains)
INFO - root - 2019-11-04 01:05:31.534052: step 102970, total loss = 0.60, predict loss = 0.14 (67.4 examples/sec; 0.059 sec/batch; 97h:16m:00s remains)
INFO - root - 2019-11-04 01:05:32.223456: step 102980, total loss = 0.51, predict loss = 0.12 (63.9 examples/sec; 0.063 sec/batch; 102h:31m:29s remains)
INFO - root - 2019-11-04 01:05:32.898594: step 102990, total loss = 0.58, predict loss = 0.13 (68.6 examples/sec; 0.058 sec/batch; 95h:27m:05s remains)
INFO - root - 2019-11-04 01:05:33.554133: step 103000, total loss = 0.55, predict loss = 0.12 (67.7 examples/sec; 0.059 sec/batch; 96h:48m:20s remains)
INFO - root - 2019-11-04 01:05:34.174603: step 103010, total loss = 0.63, predict loss = 0.15 (73.3 examples/sec; 0.055 sec/batch; 89h:24m:31s remains)
INFO - root - 2019-11-04 01:05:34.839457: step 103020, total loss = 0.55, predict loss = 0.13 (76.5 examples/sec; 0.052 sec/batch; 85h:38m:58s remains)
INFO - root - 2019-11-04 01:05:35.456583: step 103030, total loss = 0.54, predict loss = 0.12 (82.9 examples/sec; 0.048 sec/batch; 79h:00m:48s remains)
INFO - root - 2019-11-04 01:05:36.087545: step 103040, total loss = 0.63, predict loss = 0.15 (70.5 examples/sec; 0.057 sec/batch; 93h:00m:04s remains)
INFO - root - 2019-11-04 01:05:36.693231: step 103050, total loss = 0.59, predict loss = 0.15 (74.9 examples/sec; 0.053 sec/batch; 87h:26m:53s remains)
INFO - root - 2019-11-04 01:05:37.332794: step 103060, total loss = 0.49, predict loss = 0.12 (65.0 examples/sec; 0.061 sec/batch; 100h:44m:20s remains)
INFO - root - 2019-11-04 01:05:37.994638: step 103070, total loss = 0.53, predict loss = 0.13 (66.7 examples/sec; 0.060 sec/batch; 98h:15m:08s remains)
INFO - root - 2019-11-04 01:05:38.643475: step 103080, total loss = 0.56, predict loss = 0.13 (74.5 examples/sec; 0.054 sec/batch; 87h:54m:19s remains)
INFO - root - 2019-11-04 01:05:39.320553: step 103090, total loss = 0.49, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 95h:48m:47s remains)
INFO - root - 2019-11-04 01:05:40.003711: step 103100, total loss = 0.50, predict loss = 0.12 (71.9 examples/sec; 0.056 sec/batch; 91h:08m:19s remains)
INFO - root - 2019-11-04 01:05:40.608198: step 103110, total loss = 0.48, predict loss = 0.11 (75.3 examples/sec; 0.053 sec/batch; 86h:58m:56s remains)
INFO - root - 2019-11-04 01:05:41.218984: step 103120, total loss = 0.50, predict loss = 0.12 (71.2 examples/sec; 0.056 sec/batch; 91h:58m:29s remains)
INFO - root - 2019-11-04 01:05:41.820982: step 103130, total loss = 0.60, predict loss = 0.14 (72.6 examples/sec; 0.055 sec/batch; 90h:17m:56s remains)
INFO - root - 2019-11-04 01:05:42.443411: step 103140, total loss = 0.37, predict loss = 0.08 (69.8 examples/sec; 0.057 sec/batch; 93h:48m:54s remains)
INFO - root - 2019-11-04 01:05:43.066887: step 103150, total loss = 0.51, predict loss = 0.12 (78.7 examples/sec; 0.051 sec/batch; 83h:13m:24s remains)
INFO - root - 2019-11-04 01:05:43.685478: step 103160, total loss = 0.47, predict loss = 0.11 (74.8 examples/sec; 0.053 sec/batch; 87h:35m:34s remains)
INFO - root - 2019-11-04 01:05:44.308128: step 103170, total loss = 0.37, predict loss = 0.08 (80.7 examples/sec; 0.050 sec/batch; 81h:12m:25s remains)
INFO - root - 2019-11-04 01:05:44.965951: step 103180, total loss = 0.39, predict loss = 0.09 (66.2 examples/sec; 0.060 sec/batch; 98h:59m:26s remains)
INFO - root - 2019-11-04 01:05:45.646413: step 103190, total loss = 0.42, predict loss = 0.09 (61.2 examples/sec; 0.065 sec/batch; 107h:02m:03s remains)
INFO - root - 2019-11-04 01:05:46.273120: step 103200, total loss = 0.41, predict loss = 0.09 (77.2 examples/sec; 0.052 sec/batch; 84h:49m:36s remains)
INFO - root - 2019-11-04 01:05:46.895093: step 103210, total loss = 0.42, predict loss = 0.09 (73.2 examples/sec; 0.055 sec/batch; 89h:33m:33s remains)
INFO - root - 2019-11-04 01:05:47.556559: step 103220, total loss = 0.41, predict loss = 0.10 (76.1 examples/sec; 0.053 sec/batch; 86h:03m:49s remains)
INFO - root - 2019-11-04 01:05:48.272589: step 103230, total loss = 0.47, predict loss = 0.11 (62.2 examples/sec; 0.064 sec/batch; 105h:21m:06s remains)
INFO - root - 2019-11-04 01:05:48.899825: step 103240, total loss = 0.40, predict loss = 0.09 (81.2 examples/sec; 0.049 sec/batch; 80h:39m:42s remains)
INFO - root - 2019-11-04 01:05:49.504584: step 103250, total loss = 0.47, predict loss = 0.11 (79.4 examples/sec; 0.050 sec/batch; 82h:32m:20s remains)
INFO - root - 2019-11-04 01:05:50.135431: step 103260, total loss = 0.46, predict loss = 0.11 (72.0 examples/sec; 0.056 sec/batch; 90h:57m:15s remains)
INFO - root - 2019-11-04 01:05:50.783745: step 103270, total loss = 0.55, predict loss = 0.13 (67.7 examples/sec; 0.059 sec/batch; 96h:44m:36s remains)
INFO - root - 2019-11-04 01:05:51.374392: step 103280, total loss = 0.53, predict loss = 0.12 (75.2 examples/sec; 0.053 sec/batch; 87h:05m:28s remains)
INFO - root - 2019-11-04 01:05:51.957396: step 103290, total loss = 0.46, predict loss = 0.11 (78.0 examples/sec; 0.051 sec/batch; 83h:58m:13s remains)
INFO - root - 2019-11-04 01:05:52.605780: step 103300, total loss = 0.48, predict loss = 0.11 (64.3 examples/sec; 0.062 sec/batch; 101h:52m:11s remains)
INFO - root - 2019-11-04 01:05:53.242954: step 103310, total loss = 0.48, predict loss = 0.11 (71.8 examples/sec; 0.056 sec/batch; 91h:11m:27s remains)
INFO - root - 2019-11-04 01:05:53.861560: step 103320, total loss = 0.48, predict loss = 0.11 (78.4 examples/sec; 0.051 sec/batch; 83h:36m:23s remains)
INFO - root - 2019-11-04 01:05:54.481779: step 103330, total loss = 0.52, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 94h:56m:05s remains)
INFO - root - 2019-11-04 01:05:55.061677: step 103340, total loss = 0.46, predict loss = 0.11 (71.2 examples/sec; 0.056 sec/batch; 92h:02m:31s remains)
INFO - root - 2019-11-04 01:05:55.667883: step 103350, total loss = 0.57, predict loss = 0.14 (66.7 examples/sec; 0.060 sec/batch; 98h:17m:18s remains)
INFO - root - 2019-11-04 01:05:56.296895: step 103360, total loss = 0.61, predict loss = 0.15 (66.1 examples/sec; 0.060 sec/batch; 99h:04m:20s remains)
INFO - root - 2019-11-04 01:05:56.935380: step 103370, total loss = 0.80, predict loss = 0.20 (64.4 examples/sec; 0.062 sec/batch; 101h:42m:33s remains)
INFO - root - 2019-11-04 01:05:57.576236: step 103380, total loss = 0.67, predict loss = 0.16 (71.9 examples/sec; 0.056 sec/batch; 91h:04m:20s remains)
INFO - root - 2019-11-04 01:05:58.191753: step 103390, total loss = 0.65, predict loss = 0.14 (66.1 examples/sec; 0.060 sec/batch; 99h:04m:41s remains)
INFO - root - 2019-11-04 01:05:58.848272: step 103400, total loss = 0.77, predict loss = 0.18 (70.7 examples/sec; 0.057 sec/batch; 92h:38m:17s remains)
INFO - root - 2019-11-04 01:05:59.531498: step 103410, total loss = 0.60, predict loss = 0.14 (61.8 examples/sec; 0.065 sec/batch; 105h:59m:43s remains)
INFO - root - 2019-11-04 01:06:00.239980: step 103420, total loss = 0.74, predict loss = 0.18 (60.1 examples/sec; 0.067 sec/batch; 108h:58m:46s remains)
INFO - root - 2019-11-04 01:06:00.895090: step 103430, total loss = 0.62, predict loss = 0.14 (74.1 examples/sec; 0.054 sec/batch; 88h:24m:58s remains)
INFO - root - 2019-11-04 01:06:01.520952: step 103440, total loss = 0.70, predict loss = 0.17 (71.9 examples/sec; 0.056 sec/batch; 91h:07m:53s remains)
INFO - root - 2019-11-04 01:06:02.134081: step 103450, total loss = 0.85, predict loss = 0.20 (80.8 examples/sec; 0.050 sec/batch; 81h:05m:07s remains)
INFO - root - 2019-11-04 01:06:02.761873: step 103460, total loss = 0.70, predict loss = 0.17 (73.4 examples/sec; 0.055 sec/batch; 89h:18m:02s remains)
INFO - root - 2019-11-04 01:06:03.377132: step 103470, total loss = 0.64, predict loss = 0.15 (73.7 examples/sec; 0.054 sec/batch; 88h:55m:49s remains)
INFO - root - 2019-11-04 01:06:04.016908: step 103480, total loss = 0.60, predict loss = 0.14 (72.7 examples/sec; 0.055 sec/batch; 90h:03m:34s remains)
INFO - root - 2019-11-04 01:06:04.679565: step 103490, total loss = 0.46, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 95h:28m:26s remains)
INFO - root - 2019-11-04 01:06:05.252542: step 103500, total loss = 0.54, predict loss = 0.13 (73.6 examples/sec; 0.054 sec/batch; 88h:57m:40s remains)
INFO - root - 2019-11-04 01:06:05.861263: step 103510, total loss = 0.41, predict loss = 0.09 (68.9 examples/sec; 0.058 sec/batch; 95h:06m:40s remains)
INFO - root - 2019-11-04 01:06:06.490035: step 103520, total loss = 0.50, predict loss = 0.11 (75.6 examples/sec; 0.053 sec/batch; 86h:40m:26s remains)
INFO - root - 2019-11-04 01:06:07.136561: step 103530, total loss = 0.62, predict loss = 0.14 (73.2 examples/sec; 0.055 sec/batch; 89h:31m:07s remains)
INFO - root - 2019-11-04 01:06:07.788715: step 103540, total loss = 0.45, predict loss = 0.11 (61.1 examples/sec; 0.065 sec/batch; 107h:12m:48s remains)
INFO - root - 2019-11-04 01:06:08.485106: step 103550, total loss = 0.71, predict loss = 0.17 (61.2 examples/sec; 0.065 sec/batch; 106h:58m:57s remains)
INFO - root - 2019-11-04 01:06:09.156472: step 103560, total loss = 0.46, predict loss = 0.11 (65.3 examples/sec; 0.061 sec/batch; 100h:16m:54s remains)
INFO - root - 2019-11-04 01:06:09.802060: step 103570, total loss = 0.53, predict loss = 0.12 (71.5 examples/sec; 0.056 sec/batch; 91h:36m:23s remains)
INFO - root - 2019-11-04 01:06:10.466221: step 103580, total loss = 0.53, predict loss = 0.12 (74.5 examples/sec; 0.054 sec/batch; 87h:52m:56s remains)
INFO - root - 2019-11-04 01:06:11.089184: step 103590, total loss = 0.50, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 91h:24m:11s remains)
INFO - root - 2019-11-04 01:06:11.710629: step 103600, total loss = 0.41, predict loss = 0.09 (81.4 examples/sec; 0.049 sec/batch; 80h:30m:22s remains)
INFO - root - 2019-11-04 01:06:12.340241: step 103610, total loss = 0.54, predict loss = 0.12 (72.3 examples/sec; 0.055 sec/batch; 90h:35m:26s remains)
INFO - root - 2019-11-04 01:06:12.916855: step 103620, total loss = 0.63, predict loss = 0.13 (75.8 examples/sec; 0.053 sec/batch; 86h:22m:40s remains)
INFO - root - 2019-11-04 01:06:13.402082: step 103630, total loss = 0.43, predict loss = 0.10 (80.2 examples/sec; 0.050 sec/batch; 81h:39m:05s remains)
INFO - root - 2019-11-04 01:06:14.415464: step 103640, total loss = 0.43, predict loss = 0.10 (74.0 examples/sec; 0.054 sec/batch; 88h:35m:30s remains)
INFO - root - 2019-11-04 01:06:15.022332: step 103650, total loss = 0.38, predict loss = 0.08 (67.8 examples/sec; 0.059 sec/batch; 96h:40m:36s remains)
INFO - root - 2019-11-04 01:06:15.644367: step 103660, total loss = 0.40, predict loss = 0.09 (74.1 examples/sec; 0.054 sec/batch; 88h:21m:31s remains)
INFO - root - 2019-11-04 01:06:16.323263: step 103670, total loss = 0.33, predict loss = 0.07 (63.0 examples/sec; 0.064 sec/batch; 104h:03m:15s remains)
INFO - root - 2019-11-04 01:06:17.005104: step 103680, total loss = 0.53, predict loss = 0.13 (69.7 examples/sec; 0.057 sec/batch; 93h:59m:10s remains)
INFO - root - 2019-11-04 01:06:17.697301: step 103690, total loss = 0.53, predict loss = 0.12 (56.6 examples/sec; 0.071 sec/batch; 115h:45m:36s remains)
INFO - root - 2019-11-04 01:06:18.340309: step 103700, total loss = 0.55, predict loss = 0.13 (68.2 examples/sec; 0.059 sec/batch; 96h:01m:28s remains)
INFO - root - 2019-11-04 01:06:19.012357: step 103710, total loss = 0.37, predict loss = 0.09 (58.1 examples/sec; 0.069 sec/batch; 112h:40m:06s remains)
INFO - root - 2019-11-04 01:06:19.642219: step 103720, total loss = 0.58, predict loss = 0.13 (82.8 examples/sec; 0.048 sec/batch; 79h:09m:07s remains)
INFO - root - 2019-11-04 01:06:20.260015: step 103730, total loss = 0.67, predict loss = 0.16 (79.3 examples/sec; 0.050 sec/batch; 82h:34m:06s remains)
INFO - root - 2019-11-04 01:06:20.871641: step 103740, total loss = 0.47, predict loss = 0.10 (72.5 examples/sec; 0.055 sec/batch; 90h:24m:08s remains)
INFO - root - 2019-11-04 01:06:21.497517: step 103750, total loss = 0.52, predict loss = 0.12 (75.7 examples/sec; 0.053 sec/batch; 86h:35m:16s remains)
INFO - root - 2019-11-04 01:06:22.109378: step 103760, total loss = 0.42, predict loss = 0.09 (65.3 examples/sec; 0.061 sec/batch; 100h:22m:50s remains)
INFO - root - 2019-11-04 01:06:22.719967: step 103770, total loss = 0.48, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 97h:54m:03s remains)
INFO - root - 2019-11-04 01:06:23.355990: step 103780, total loss = 0.45, predict loss = 0.10 (73.4 examples/sec; 0.054 sec/batch; 89h:12m:06s remains)
INFO - root - 2019-11-04 01:06:24.021422: step 103790, total loss = 0.51, predict loss = 0.12 (71.1 examples/sec; 0.056 sec/batch; 92h:08m:06s remains)
INFO - root - 2019-11-04 01:06:24.643812: step 103800, total loss = 0.35, predict loss = 0.07 (68.4 examples/sec; 0.059 sec/batch; 95h:49m:37s remains)
INFO - root - 2019-11-04 01:06:25.287739: step 103810, total loss = 0.48, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 97h:53m:03s remains)
INFO - root - 2019-11-04 01:06:25.922258: step 103820, total loss = 0.58, predict loss = 0.13 (77.1 examples/sec; 0.052 sec/batch; 84h:59m:58s remains)
INFO - root - 2019-11-04 01:06:26.610084: step 103830, total loss = 0.56, predict loss = 0.13 (66.0 examples/sec; 0.061 sec/batch; 99h:11m:35s remains)
INFO - root - 2019-11-04 01:06:27.223126: step 103840, total loss = 0.41, predict loss = 0.09 (87.2 examples/sec; 0.046 sec/batch; 75h:07m:47s remains)
INFO - root - 2019-11-04 01:06:27.834116: step 103850, total loss = 0.41, predict loss = 0.09 (70.1 examples/sec; 0.057 sec/batch; 93h:29m:57s remains)
INFO - root - 2019-11-04 01:06:28.476390: step 103860, total loss = 0.37, predict loss = 0.08 (74.6 examples/sec; 0.054 sec/batch; 87h:51m:18s remains)
INFO - root - 2019-11-04 01:06:29.134986: step 103870, total loss = 0.43, predict loss = 0.10 (65.0 examples/sec; 0.062 sec/batch; 100h:44m:18s remains)
INFO - root - 2019-11-04 01:06:29.760159: step 103880, total loss = 0.47, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 94h:30m:12s remains)
INFO - root - 2019-11-04 01:06:30.412206: step 103890, total loss = 0.64, predict loss = 0.16 (72.0 examples/sec; 0.056 sec/batch; 90h:59m:51s remains)
INFO - root - 2019-11-04 01:06:31.079778: step 103900, total loss = 0.45, predict loss = 0.10 (67.8 examples/sec; 0.059 sec/batch; 96h:36m:05s remains)
INFO - root - 2019-11-04 01:06:31.708327: step 103910, total loss = 0.51, predict loss = 0.12 (65.3 examples/sec; 0.061 sec/batch; 100h:16m:43s remains)
INFO - root - 2019-11-04 01:06:32.337566: step 103920, total loss = 0.54, predict loss = 0.12 (80.8 examples/sec; 0.049 sec/batch; 81h:02m:44s remains)
INFO - root - 2019-11-04 01:06:32.973930: step 103930, total loss = 0.62, predict loss = 0.14 (69.8 examples/sec; 0.057 sec/batch; 93h:53m:07s remains)
INFO - root - 2019-11-04 01:06:33.583345: step 103940, total loss = 0.72, predict loss = 0.17 (67.4 examples/sec; 0.059 sec/batch; 97h:09m:59s remains)
INFO - root - 2019-11-04 01:06:34.254690: step 103950, total loss = 0.59, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 90h:44m:37s remains)
INFO - root - 2019-11-04 01:06:34.916059: step 103960, total loss = 0.69, predict loss = 0.17 (77.2 examples/sec; 0.052 sec/batch; 84h:50m:41s remains)
INFO - root - 2019-11-04 01:06:35.523216: step 103970, total loss = 0.70, predict loss = 0.16 (62.8 examples/sec; 0.064 sec/batch; 104h:14m:14s remains)
INFO - root - 2019-11-04 01:06:36.149264: step 103980, total loss = 0.58, predict loss = 0.14 (74.4 examples/sec; 0.054 sec/batch; 88h:01m:25s remains)
INFO - root - 2019-11-04 01:06:36.790604: step 103990, total loss = 0.63, predict loss = 0.15 (67.6 examples/sec; 0.059 sec/batch; 96h:57m:43s remains)
INFO - root - 2019-11-04 01:06:37.482261: step 104000, total loss = 0.47, predict loss = 0.10 (64.2 examples/sec; 0.062 sec/batch; 101h:58m:40s remains)
INFO - root - 2019-11-04 01:06:38.124687: step 104010, total loss = 0.54, predict loss = 0.12 (72.7 examples/sec; 0.055 sec/batch; 90h:04m:06s remains)
INFO - root - 2019-11-04 01:06:38.741262: step 104020, total loss = 0.38, predict loss = 0.09 (74.8 examples/sec; 0.053 sec/batch; 87h:36m:09s remains)
INFO - root - 2019-11-04 01:06:39.396001: step 104030, total loss = 0.45, predict loss = 0.09 (68.5 examples/sec; 0.058 sec/batch; 95h:34m:24s remains)
INFO - root - 2019-11-04 01:06:40.058476: step 104040, total loss = 0.49, predict loss = 0.11 (63.1 examples/sec; 0.063 sec/batch; 103h:52m:47s remains)
INFO - root - 2019-11-04 01:06:40.709877: step 104050, total loss = 0.47, predict loss = 0.11 (79.2 examples/sec; 0.051 sec/batch; 82h:44m:42s remains)
INFO - root - 2019-11-04 01:06:41.348302: step 104060, total loss = 0.65, predict loss = 0.15 (66.1 examples/sec; 0.061 sec/batch; 99h:09m:21s remains)
INFO - root - 2019-11-04 01:06:41.957729: step 104070, total loss = 0.68, predict loss = 0.16 (71.7 examples/sec; 0.056 sec/batch; 91h:18m:16s remains)
INFO - root - 2019-11-04 01:06:42.607373: step 104080, total loss = 0.41, predict loss = 0.09 (65.7 examples/sec; 0.061 sec/batch; 99h:39m:16s remains)
INFO - root - 2019-11-04 01:06:43.266498: step 104090, total loss = 0.52, predict loss = 0.12 (65.2 examples/sec; 0.061 sec/batch; 100h:28m:38s remains)
INFO - root - 2019-11-04 01:06:43.882230: step 104100, total loss = 0.46, predict loss = 0.11 (70.7 examples/sec; 0.057 sec/batch; 92h:36m:54s remains)
INFO - root - 2019-11-04 01:06:44.477952: step 104110, total loss = 0.39, predict loss = 0.09 (72.8 examples/sec; 0.055 sec/batch; 89h:56m:32s remains)
INFO - root - 2019-11-04 01:06:45.092374: step 104120, total loss = 0.37, predict loss = 0.08 (69.9 examples/sec; 0.057 sec/batch; 93h:42m:01s remains)
INFO - root - 2019-11-04 01:06:45.732225: step 104130, total loss = 0.47, predict loss = 0.11 (65.7 examples/sec; 0.061 sec/batch; 99h:46m:26s remains)
INFO - root - 2019-11-04 01:06:46.393685: step 104140, total loss = 0.42, predict loss = 0.10 (67.8 examples/sec; 0.059 sec/batch; 96h:33m:05s remains)
INFO - root - 2019-11-04 01:06:47.064199: step 104150, total loss = 0.56, predict loss = 0.14 (59.1 examples/sec; 0.068 sec/batch; 110h:49m:42s remains)
INFO - root - 2019-11-04 01:06:47.747630: step 104160, total loss = 0.54, predict loss = 0.13 (68.0 examples/sec; 0.059 sec/batch; 96h:24m:20s remains)
INFO - root - 2019-11-04 01:06:48.398825: step 104170, total loss = 0.48, predict loss = 0.11 (67.6 examples/sec; 0.059 sec/batch; 96h:50m:45s remains)
INFO - root - 2019-11-04 01:06:49.073935: step 104180, total loss = 0.35, predict loss = 0.08 (71.1 examples/sec; 0.056 sec/batch; 92h:09m:48s remains)
INFO - root - 2019-11-04 01:06:49.750652: step 104190, total loss = 0.43, predict loss = 0.11 (71.2 examples/sec; 0.056 sec/batch; 92h:01m:49s remains)
INFO - root - 2019-11-04 01:06:50.404709: step 104200, total loss = 0.28, predict loss = 0.06 (67.2 examples/sec; 0.060 sec/batch; 97h:33m:00s remains)
INFO - root - 2019-11-04 01:06:51.060032: step 104210, total loss = 0.37, predict loss = 0.09 (71.1 examples/sec; 0.056 sec/batch; 92h:07m:48s remains)
INFO - root - 2019-11-04 01:06:51.716359: step 104220, total loss = 0.52, predict loss = 0.13 (73.9 examples/sec; 0.054 sec/batch; 88h:38m:53s remains)
INFO - root - 2019-11-04 01:06:52.372454: step 104230, total loss = 0.34, predict loss = 0.08 (68.6 examples/sec; 0.058 sec/batch; 95h:31m:42s remains)
INFO - root - 2019-11-04 01:06:53.038160: step 104240, total loss = 0.39, predict loss = 0.08 (72.8 examples/sec; 0.055 sec/batch; 89h:58m:59s remains)
INFO - root - 2019-11-04 01:06:53.675360: step 104250, total loss = 0.33, predict loss = 0.07 (70.2 examples/sec; 0.057 sec/batch; 93h:18m:32s remains)
INFO - root - 2019-11-04 01:06:54.283962: step 104260, total loss = 0.41, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 92h:49m:09s remains)
INFO - root - 2019-11-04 01:06:54.957375: step 104270, total loss = 0.34, predict loss = 0.08 (70.7 examples/sec; 0.057 sec/batch; 92h:40m:11s remains)
INFO - root - 2019-11-04 01:06:55.557253: step 104280, total loss = 0.36, predict loss = 0.08 (74.0 examples/sec; 0.054 sec/batch; 88h:31m:45s remains)
INFO - root - 2019-11-04 01:06:56.146148: step 104290, total loss = 0.37, predict loss = 0.08 (69.7 examples/sec; 0.057 sec/batch; 94h:00m:46s remains)
INFO - root - 2019-11-04 01:06:56.751376: step 104300, total loss = 0.49, predict loss = 0.12 (61.4 examples/sec; 0.065 sec/batch; 106h:44m:04s remains)
INFO - root - 2019-11-04 01:06:57.399684: step 104310, total loss = 0.57, predict loss = 0.14 (71.5 examples/sec; 0.056 sec/batch; 91h:39m:33s remains)
INFO - root - 2019-11-04 01:06:58.057646: step 104320, total loss = 0.39, predict loss = 0.08 (66.2 examples/sec; 0.060 sec/batch; 98h:55m:20s remains)
INFO - root - 2019-11-04 01:06:58.750945: step 104330, total loss = 0.52, predict loss = 0.11 (72.1 examples/sec; 0.055 sec/batch; 90h:50m:56s remains)
INFO - root - 2019-11-04 01:06:59.382493: step 104340, total loss = 0.63, predict loss = 0.15 (63.3 examples/sec; 0.063 sec/batch; 103h:30m:19s remains)
INFO - root - 2019-11-04 01:07:00.014746: step 104350, total loss = 0.69, predict loss = 0.16 (62.4 examples/sec; 0.064 sec/batch; 105h:01m:03s remains)
INFO - root - 2019-11-04 01:07:00.679529: step 104360, total loss = 0.61, predict loss = 0.15 (77.7 examples/sec; 0.051 sec/batch; 84h:16m:56s remains)
INFO - root - 2019-11-04 01:07:01.318265: step 104370, total loss = 0.42, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 99h:27m:39s remains)
INFO - root - 2019-11-04 01:07:02.003876: step 104380, total loss = 0.53, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 96h:38m:13s remains)
INFO - root - 2019-11-04 01:07:02.629172: step 104390, total loss = 0.48, predict loss = 0.10 (71.0 examples/sec; 0.056 sec/batch; 92h:13m:43s remains)
INFO - root - 2019-11-04 01:07:03.308493: step 104400, total loss = 0.37, predict loss = 0.08 (62.7 examples/sec; 0.064 sec/batch; 104h:27m:43s remains)
INFO - root - 2019-11-04 01:07:03.913658: step 104410, total loss = 0.46, predict loss = 0.11 (71.4 examples/sec; 0.056 sec/batch; 91h:43m:51s remains)
INFO - root - 2019-11-04 01:07:04.543787: step 104420, total loss = 0.53, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 93h:09m:21s remains)
INFO - root - 2019-11-04 01:07:05.166490: step 104430, total loss = 0.59, predict loss = 0.14 (78.9 examples/sec; 0.051 sec/batch; 82h:58m:45s remains)
INFO - root - 2019-11-04 01:07:05.798314: step 104440, total loss = 0.31, predict loss = 0.07 (75.4 examples/sec; 0.053 sec/batch; 86h:52m:32s remains)
INFO - root - 2019-11-04 01:07:06.449401: step 104450, total loss = 0.46, predict loss = 0.11 (66.3 examples/sec; 0.060 sec/batch; 98h:51m:44s remains)
INFO - root - 2019-11-04 01:07:07.145635: step 104460, total loss = 0.47, predict loss = 0.10 (64.8 examples/sec; 0.062 sec/batch; 101h:02m:07s remains)
INFO - root - 2019-11-04 01:07:07.748459: step 104470, total loss = 0.39, predict loss = 0.10 (79.4 examples/sec; 0.050 sec/batch; 82h:27m:19s remains)
INFO - root - 2019-11-04 01:07:08.366663: step 104480, total loss = 0.32, predict loss = 0.07 (77.6 examples/sec; 0.052 sec/batch; 84h:23m:39s remains)
INFO - root - 2019-11-04 01:07:08.994325: step 104490, total loss = 0.21, predict loss = 0.04 (82.6 examples/sec; 0.048 sec/batch; 79h:21m:04s remains)
INFO - root - 2019-11-04 01:07:09.660711: step 104500, total loss = 0.21, predict loss = 0.04 (61.0 examples/sec; 0.066 sec/batch; 107h:19m:19s remains)
INFO - root - 2019-11-04 01:07:10.297667: step 104510, total loss = 0.36, predict loss = 0.08 (72.8 examples/sec; 0.055 sec/batch; 89h:55m:19s remains)
INFO - root - 2019-11-04 01:07:10.972569: step 104520, total loss = 0.25, predict loss = 0.05 (77.0 examples/sec; 0.052 sec/batch; 85h:03m:31s remains)
INFO - root - 2019-11-04 01:07:11.635461: step 104530, total loss = 0.40, predict loss = 0.09 (78.3 examples/sec; 0.051 sec/batch; 83h:39m:51s remains)
INFO - root - 2019-11-04 01:07:12.286258: step 104540, total loss = 0.36, predict loss = 0.08 (72.7 examples/sec; 0.055 sec/batch; 90h:03m:36s remains)
INFO - root - 2019-11-04 01:07:12.902398: step 104550, total loss = 0.52, predict loss = 0.12 (77.6 examples/sec; 0.052 sec/batch; 84h:26m:04s remains)
INFO - root - 2019-11-04 01:07:13.553438: step 104560, total loss = 0.41, predict loss = 0.09 (65.1 examples/sec; 0.061 sec/batch; 100h:36m:55s remains)
INFO - root - 2019-11-04 01:07:14.179297: step 104570, total loss = 0.40, predict loss = 0.09 (81.1 examples/sec; 0.049 sec/batch; 80h:48m:38s remains)
INFO - root - 2019-11-04 01:07:14.850516: step 104580, total loss = 0.28, predict loss = 0.06 (69.2 examples/sec; 0.058 sec/batch; 94h:42m:12s remains)
INFO - root - 2019-11-04 01:07:15.506874: step 104590, total loss = 0.44, predict loss = 0.10 (71.9 examples/sec; 0.056 sec/batch; 91h:05m:23s remains)
INFO - root - 2019-11-04 01:07:16.108151: step 104600, total loss = 0.33, predict loss = 0.07 (77.1 examples/sec; 0.052 sec/batch; 84h:55m:00s remains)
INFO - root - 2019-11-04 01:07:16.714166: step 104610, total loss = 0.48, predict loss = 0.10 (73.2 examples/sec; 0.055 sec/batch; 89h:25m:33s remains)
INFO - root - 2019-11-04 01:07:17.328460: step 104620, total loss = 0.35, predict loss = 0.08 (71.4 examples/sec; 0.056 sec/batch; 91h:43m:21s remains)
INFO - root - 2019-11-04 01:07:17.929805: step 104630, total loss = 0.61, predict loss = 0.15 (90.8 examples/sec; 0.044 sec/batch; 72h:10m:06s remains)
INFO - root - 2019-11-04 01:07:18.527898: step 104640, total loss = 0.40, predict loss = 0.10 (69.1 examples/sec; 0.058 sec/batch; 94h:44m:18s remains)
INFO - root - 2019-11-04 01:07:19.208253: step 104650, total loss = 0.54, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 98h:55m:49s remains)
INFO - root - 2019-11-04 01:07:19.874348: step 104660, total loss = 0.36, predict loss = 0.08 (76.7 examples/sec; 0.052 sec/batch; 85h:25m:54s remains)
INFO - root - 2019-11-04 01:07:20.553379: step 104670, total loss = 0.36, predict loss = 0.08 (58.9 examples/sec; 0.068 sec/batch; 111h:13m:39s remains)
INFO - root - 2019-11-04 01:07:21.200523: step 104680, total loss = 0.72, predict loss = 0.18 (67.8 examples/sec; 0.059 sec/batch; 96h:39m:11s remains)
INFO - root - 2019-11-04 01:07:21.854665: step 104690, total loss = 0.47, predict loss = 0.11 (63.9 examples/sec; 0.063 sec/batch; 102h:31m:40s remains)
INFO - root - 2019-11-04 01:07:22.530294: step 104700, total loss = 0.35, predict loss = 0.08 (59.0 examples/sec; 0.068 sec/batch; 110h:58m:54s remains)
INFO - root - 2019-11-04 01:07:23.194717: step 104710, total loss = 0.30, predict loss = 0.06 (71.8 examples/sec; 0.056 sec/batch; 91h:15m:05s remains)
INFO - root - 2019-11-04 01:07:23.824266: step 104720, total loss = 0.26, predict loss = 0.06 (69.8 examples/sec; 0.057 sec/batch; 93h:49m:16s remains)
INFO - root - 2019-11-04 01:07:24.443663: step 104730, total loss = 0.26, predict loss = 0.05 (70.8 examples/sec; 0.057 sec/batch; 92h:33m:10s remains)
INFO - root - 2019-11-04 01:07:25.119714: step 104740, total loss = 0.25, predict loss = 0.05 (67.6 examples/sec; 0.059 sec/batch; 96h:53m:48s remains)
INFO - root - 2019-11-04 01:07:25.771820: step 104750, total loss = 0.47, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 96h:12m:49s remains)
INFO - root - 2019-11-04 01:07:26.394238: step 104760, total loss = 0.39, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 83h:48m:32s remains)
INFO - root - 2019-11-04 01:07:26.990929: step 104770, total loss = 0.50, predict loss = 0.12 (82.6 examples/sec; 0.048 sec/batch; 79h:17m:53s remains)
INFO - root - 2019-11-04 01:07:27.623030: step 104780, total loss = 0.32, predict loss = 0.07 (73.8 examples/sec; 0.054 sec/batch; 88h:42m:37s remains)
INFO - root - 2019-11-04 01:07:28.228170: step 104790, total loss = 0.48, predict loss = 0.10 (78.6 examples/sec; 0.051 sec/batch; 83h:20m:09s remains)
INFO - root - 2019-11-04 01:07:28.838785: step 104800, total loss = 0.42, predict loss = 0.09 (71.1 examples/sec; 0.056 sec/batch; 92h:05m:19s remains)
INFO - root - 2019-11-04 01:07:29.472060: step 104810, total loss = 0.49, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 89h:44m:10s remains)
INFO - root - 2019-11-04 01:07:30.116782: step 104820, total loss = 0.38, predict loss = 0.08 (69.4 examples/sec; 0.058 sec/batch; 94h:23m:39s remains)
INFO - root - 2019-11-04 01:07:30.739866: step 104830, total loss = 0.48, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 86h:46m:40s remains)
INFO - root - 2019-11-04 01:07:31.390835: step 104840, total loss = 0.32, predict loss = 0.06 (66.7 examples/sec; 0.060 sec/batch; 98h:11m:06s remains)
INFO - root - 2019-11-04 01:07:32.065784: step 104850, total loss = 0.44, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:35m:28s remains)
INFO - root - 2019-11-04 01:07:32.723788: step 104860, total loss = 0.29, predict loss = 0.06 (63.9 examples/sec; 0.063 sec/batch; 102h:29m:54s remains)
INFO - root - 2019-11-04 01:07:33.345900: step 104870, total loss = 0.32, predict loss = 0.07 (85.2 examples/sec; 0.047 sec/batch; 76h:52m:55s remains)
INFO - root - 2019-11-04 01:07:33.940170: step 104880, total loss = 0.58, predict loss = 0.14 (74.1 examples/sec; 0.054 sec/batch; 88h:24m:03s remains)
INFO - root - 2019-11-04 01:07:34.579589: step 104890, total loss = 0.53, predict loss = 0.12 (58.4 examples/sec; 0.068 sec/batch; 112h:10m:08s remains)
INFO - root - 2019-11-04 01:07:35.192741: step 104900, total loss = 0.49, predict loss = 0.12 (69.1 examples/sec; 0.058 sec/batch; 94h:46m:07s remains)
INFO - root - 2019-11-04 01:07:35.795725: step 104910, total loss = 0.50, predict loss = 0.12 (74.9 examples/sec; 0.053 sec/batch; 87h:26m:59s remains)
INFO - root - 2019-11-04 01:07:36.377883: step 104920, total loss = 0.60, predict loss = 0.14 (74.3 examples/sec; 0.054 sec/batch; 88h:07m:42s remains)
INFO - root - 2019-11-04 01:07:37.003246: step 104930, total loss = 0.63, predict loss = 0.15 (70.4 examples/sec; 0.057 sec/batch; 92h:59m:05s remains)
INFO - root - 2019-11-04 01:07:37.597555: step 104940, total loss = 0.66, predict loss = 0.15 (74.7 examples/sec; 0.054 sec/batch; 87h:42m:22s remains)
INFO - root - 2019-11-04 01:07:38.211326: step 104950, total loss = 0.43, predict loss = 0.09 (72.5 examples/sec; 0.055 sec/batch; 90h:19m:41s remains)
INFO - root - 2019-11-04 01:07:38.847921: step 104960, total loss = 0.65, predict loss = 0.14 (76.8 examples/sec; 0.052 sec/batch; 85h:15m:34s remains)
INFO - root - 2019-11-04 01:07:39.474419: step 104970, total loss = 0.48, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 97h:58m:37s remains)
INFO - root - 2019-11-04 01:07:40.090391: step 104980, total loss = 0.39, predict loss = 0.09 (72.9 examples/sec; 0.055 sec/batch; 89h:50m:14s remains)
INFO - root - 2019-11-04 01:07:40.708681: step 104990, total loss = 0.44, predict loss = 0.10 (75.7 examples/sec; 0.053 sec/batch; 86h:34m:50s remains)
INFO - root - 2019-11-04 01:07:41.346836: step 105000, total loss = 0.39, predict loss = 0.09 (73.0 examples/sec; 0.055 sec/batch; 89h:45m:51s remains)
INFO - root - 2019-11-04 01:07:42.629436: step 105010, total loss = 0.37, predict loss = 0.08 (67.9 examples/sec; 0.059 sec/batch; 96h:31m:02s remains)
INFO - root - 2019-11-04 01:07:43.265093: step 105020, total loss = 0.42, predict loss = 0.09 (67.0 examples/sec; 0.060 sec/batch; 97h:49m:56s remains)
INFO - root - 2019-11-04 01:07:43.890046: step 105030, total loss = 0.61, predict loss = 0.15 (66.9 examples/sec; 0.060 sec/batch; 97h:50m:30s remains)
INFO - root - 2019-11-04 01:07:44.495907: step 105040, total loss = 0.49, predict loss = 0.12 (78.4 examples/sec; 0.051 sec/batch; 83h:32m:51s remains)
INFO - root - 2019-11-04 01:07:45.106798: step 105050, total loss = 0.20, predict loss = 0.04 (83.3 examples/sec; 0.048 sec/batch; 78h:38m:01s remains)
INFO - root - 2019-11-04 01:07:45.761977: step 105060, total loss = 0.64, predict loss = 0.14 (61.9 examples/sec; 0.065 sec/batch; 105h:49m:58s remains)
INFO - root - 2019-11-04 01:07:46.437225: step 105070, total loss = 0.66, predict loss = 0.16 (60.0 examples/sec; 0.067 sec/batch; 109h:11m:05s remains)
INFO - root - 2019-11-04 01:07:47.112770: step 105080, total loss = 0.79, predict loss = 0.19 (71.2 examples/sec; 0.056 sec/batch; 91h:59m:00s remains)
INFO - root - 2019-11-04 01:07:47.756550: step 105090, total loss = 0.80, predict loss = 0.20 (60.7 examples/sec; 0.066 sec/batch; 107h:53m:51s remains)
INFO - root - 2019-11-04 01:07:48.420326: step 105100, total loss = 0.67, predict loss = 0.16 (75.6 examples/sec; 0.053 sec/batch; 86h:38m:54s remains)
INFO - root - 2019-11-04 01:07:49.119263: step 105110, total loss = 0.57, predict loss = 0.13 (70.2 examples/sec; 0.057 sec/batch; 93h:18m:22s remains)
INFO - root - 2019-11-04 01:07:49.745758: step 105120, total loss = 0.76, predict loss = 0.19 (71.3 examples/sec; 0.056 sec/batch; 91h:51m:08s remains)
INFO - root - 2019-11-04 01:07:50.397454: step 105130, total loss = 0.71, predict loss = 0.17 (75.9 examples/sec; 0.053 sec/batch; 86h:14m:42s remains)
INFO - root - 2019-11-04 01:07:51.077514: step 105140, total loss = 0.58, predict loss = 0.14 (61.4 examples/sec; 0.065 sec/batch; 106h:38m:57s remains)
INFO - root - 2019-11-04 01:07:51.699357: step 105150, total loss = 0.53, predict loss = 0.12 (73.5 examples/sec; 0.054 sec/batch; 89h:09m:57s remains)
INFO - root - 2019-11-04 01:07:52.308117: step 105160, total loss = 0.48, predict loss = 0.11 (71.8 examples/sec; 0.056 sec/batch; 91h:10m:02s remains)
INFO - root - 2019-11-04 01:07:52.979178: step 105170, total loss = 0.64, predict loss = 0.15 (65.3 examples/sec; 0.061 sec/batch; 100h:18m:41s remains)
INFO - root - 2019-11-04 01:07:53.626143: step 105180, total loss = 0.51, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 95h:20m:09s remains)
INFO - root - 2019-11-04 01:07:54.309521: step 105190, total loss = 0.56, predict loss = 0.14 (67.3 examples/sec; 0.059 sec/batch; 97h:22m:27s remains)
INFO - root - 2019-11-04 01:07:54.990801: step 105200, total loss = 0.49, predict loss = 0.11 (61.4 examples/sec; 0.065 sec/batch; 106h:40m:50s remains)
INFO - root - 2019-11-04 01:07:55.685556: step 105210, total loss = 0.44, predict loss = 0.10 (70.1 examples/sec; 0.057 sec/batch; 93h:24m:05s remains)
INFO - root - 2019-11-04 01:07:56.387183: step 105220, total loss = 0.42, predict loss = 0.09 (59.5 examples/sec; 0.067 sec/batch; 110h:00m:06s remains)
INFO - root - 2019-11-04 01:07:57.095077: step 105230, total loss = 0.52, predict loss = 0.12 (62.0 examples/sec; 0.065 sec/batch; 105h:39m:19s remains)
INFO - root - 2019-11-04 01:07:57.757232: step 105240, total loss = 0.48, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 89h:56m:29s remains)
INFO - root - 2019-11-04 01:07:58.388843: step 105250, total loss = 0.51, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 93h:18m:48s remains)
INFO - root - 2019-11-04 01:07:59.004060: step 105260, total loss = 0.49, predict loss = 0.11 (72.1 examples/sec; 0.055 sec/batch; 90h:47m:19s remains)
INFO - root - 2019-11-04 01:07:59.625199: step 105270, total loss = 0.52, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 96h:25m:31s remains)
INFO - root - 2019-11-04 01:08:00.245003: step 105280, total loss = 0.57, predict loss = 0.13 (78.6 examples/sec; 0.051 sec/batch; 83h:21m:03s remains)
INFO - root - 2019-11-04 01:08:00.899928: step 105290, total loss = 0.61, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 100h:39m:05s remains)
INFO - root - 2019-11-04 01:08:01.559840: step 105300, total loss = 0.31, predict loss = 0.07 (77.0 examples/sec; 0.052 sec/batch; 85h:06m:12s remains)
INFO - root - 2019-11-04 01:08:02.171156: step 105310, total loss = 0.63, predict loss = 0.14 (72.3 examples/sec; 0.055 sec/batch; 90h:33m:53s remains)
INFO - root - 2019-11-04 01:08:02.796262: step 105320, total loss = 0.35, predict loss = 0.08 (81.1 examples/sec; 0.049 sec/batch; 80h:46m:25s remains)
INFO - root - 2019-11-04 01:08:03.430623: step 105330, total loss = 0.36, predict loss = 0.08 (76.4 examples/sec; 0.052 sec/batch; 85h:45m:35s remains)
INFO - root - 2019-11-04 01:08:04.065245: step 105340, total loss = 0.44, predict loss = 0.10 (58.8 examples/sec; 0.068 sec/batch; 111h:18m:57s remains)
INFO - root - 2019-11-04 01:08:04.756912: step 105350, total loss = 0.38, predict loss = 0.09 (90.3 examples/sec; 0.044 sec/batch; 72h:33m:01s remains)
INFO - root - 2019-11-04 01:08:05.392360: step 105360, total loss = 0.46, predict loss = 0.10 (63.0 examples/sec; 0.063 sec/batch; 103h:56m:49s remains)
INFO - root - 2019-11-04 01:08:06.079533: step 105370, total loss = 0.46, predict loss = 0.10 (68.2 examples/sec; 0.059 sec/batch; 95h:58m:20s remains)
INFO - root - 2019-11-04 01:08:06.755277: step 105380, total loss = 0.48, predict loss = 0.11 (78.4 examples/sec; 0.051 sec/batch; 83h:30m:43s remains)
INFO - root - 2019-11-04 01:08:07.404065: step 105390, total loss = 0.33, predict loss = 0.07 (70.0 examples/sec; 0.057 sec/batch; 93h:31m:43s remains)
INFO - root - 2019-11-04 01:08:08.006822: step 105400, total loss = 0.39, predict loss = 0.09 (72.4 examples/sec; 0.055 sec/batch; 90h:28m:51s remains)
INFO - root - 2019-11-04 01:08:08.633030: step 105410, total loss = 0.36, predict loss = 0.08 (69.2 examples/sec; 0.058 sec/batch; 94h:39m:08s remains)
INFO - root - 2019-11-04 01:08:09.268878: step 105420, total loss = 0.38, predict loss = 0.08 (71.6 examples/sec; 0.056 sec/batch; 91h:26m:45s remains)
INFO - root - 2019-11-04 01:08:09.904739: step 105430, total loss = 0.35, predict loss = 0.07 (72.1 examples/sec; 0.055 sec/batch; 90h:47m:43s remains)
INFO - root - 2019-11-04 01:08:10.551843: step 105440, total loss = 0.55, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 94h:41m:00s remains)
INFO - root - 2019-11-04 01:08:11.217199: step 105450, total loss = 0.47, predict loss = 0.11 (72.2 examples/sec; 0.055 sec/batch; 90h:40m:07s remains)
INFO - root - 2019-11-04 01:08:11.863689: step 105460, total loss = 0.49, predict loss = 0.11 (68.0 examples/sec; 0.059 sec/batch; 96h:21m:25s remains)
INFO - root - 2019-11-04 01:08:12.515743: step 105470, total loss = 0.50, predict loss = 0.12 (66.8 examples/sec; 0.060 sec/batch; 98h:02m:10s remains)
INFO - root - 2019-11-04 01:08:13.145441: step 105480, total loss = 0.53, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 88h:35m:03s remains)
INFO - root - 2019-11-04 01:08:13.775569: step 105490, total loss = 0.51, predict loss = 0.12 (65.7 examples/sec; 0.061 sec/batch; 99h:43m:25s remains)
INFO - root - 2019-11-04 01:08:14.419808: step 105500, total loss = 0.60, predict loss = 0.14 (66.1 examples/sec; 0.061 sec/batch; 99h:04m:49s remains)
INFO - root - 2019-11-04 01:08:15.032287: step 105510, total loss = 0.61, predict loss = 0.15 (71.1 examples/sec; 0.056 sec/batch; 92h:05m:06s remains)
INFO - root - 2019-11-04 01:08:15.666999: step 105520, total loss = 0.72, predict loss = 0.16 (63.5 examples/sec; 0.063 sec/batch; 103h:08m:07s remains)
INFO - root - 2019-11-04 01:08:16.320301: step 105530, total loss = 0.63, predict loss = 0.16 (66.2 examples/sec; 0.060 sec/batch; 98h:56m:55s remains)
INFO - root - 2019-11-04 01:08:16.964532: step 105540, total loss = 0.63, predict loss = 0.15 (70.6 examples/sec; 0.057 sec/batch; 92h:48m:52s remains)
INFO - root - 2019-11-04 01:08:17.591116: step 105550, total loss = 0.56, predict loss = 0.13 (71.3 examples/sec; 0.056 sec/batch; 91h:52m:10s remains)
INFO - root - 2019-11-04 01:08:18.236794: step 105560, total loss = 0.67, predict loss = 0.16 (61.3 examples/sec; 0.065 sec/batch; 106h:54m:13s remains)
INFO - root - 2019-11-04 01:08:18.878520: step 105570, total loss = 0.50, predict loss = 0.11 (66.2 examples/sec; 0.060 sec/batch; 98h:56m:46s remains)
INFO - root - 2019-11-04 01:08:19.564392: step 105580, total loss = 0.51, predict loss = 0.12 (62.8 examples/sec; 0.064 sec/batch; 104h:20m:24s remains)
INFO - root - 2019-11-04 01:08:20.188006: step 105590, total loss = 0.49, predict loss = 0.11 (71.9 examples/sec; 0.056 sec/batch; 91h:02m:18s remains)
INFO - root - 2019-11-04 01:08:20.812165: step 105600, total loss = 0.57, predict loss = 0.13 (66.1 examples/sec; 0.060 sec/batch; 99h:03m:06s remains)
INFO - root - 2019-11-04 01:08:21.465006: step 105610, total loss = 0.47, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 93h:25m:08s remains)
INFO - root - 2019-11-04 01:08:22.109063: step 105620, total loss = 0.42, predict loss = 0.09 (66.7 examples/sec; 0.060 sec/batch; 98h:13m:54s remains)
INFO - root - 2019-11-04 01:08:22.781951: step 105630, total loss = 0.42, predict loss = 0.09 (70.8 examples/sec; 0.056 sec/batch; 92h:28m:35s remains)
INFO - root - 2019-11-04 01:08:23.422883: step 105640, total loss = 0.38, predict loss = 0.09 (65.5 examples/sec; 0.061 sec/batch; 99h:56m:31s remains)
INFO - root - 2019-11-04 01:08:24.042125: step 105650, total loss = 0.42, predict loss = 0.09 (76.2 examples/sec; 0.052 sec/batch; 85h:57m:15s remains)
INFO - root - 2019-11-04 01:08:24.658163: step 105660, total loss = 0.51, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 95h:09m:02s remains)
INFO - root - 2019-11-04 01:08:25.253565: step 105670, total loss = 0.48, predict loss = 0.11 (76.4 examples/sec; 0.052 sec/batch; 85h:42m:31s remains)
INFO - root - 2019-11-04 01:08:25.867412: step 105680, total loss = 0.47, predict loss = 0.10 (64.5 examples/sec; 0.062 sec/batch; 101h:33m:50s remains)
INFO - root - 2019-11-04 01:08:26.494903: step 105690, total loss = 0.43, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 95h:29m:33s remains)
INFO - root - 2019-11-04 01:08:27.114632: step 105700, total loss = 0.45, predict loss = 0.10 (72.4 examples/sec; 0.055 sec/batch; 90h:27m:04s remains)
INFO - root - 2019-11-04 01:08:27.719531: step 105710, total loss = 0.60, predict loss = 0.14 (79.1 examples/sec; 0.051 sec/batch; 82h:48m:12s remains)
INFO - root - 2019-11-04 01:08:28.343619: step 105720, total loss = 0.65, predict loss = 0.15 (64.4 examples/sec; 0.062 sec/batch; 101h:45m:46s remains)
INFO - root - 2019-11-04 01:08:28.994483: step 105730, total loss = 0.62, predict loss = 0.15 (70.7 examples/sec; 0.057 sec/batch; 92h:40m:19s remains)
INFO - root - 2019-11-04 01:08:29.652288: step 105740, total loss = 0.53, predict loss = 0.13 (67.7 examples/sec; 0.059 sec/batch; 96h:46m:43s remains)
INFO - root - 2019-11-04 01:08:30.270979: step 105750, total loss = 0.49, predict loss = 0.12 (74.6 examples/sec; 0.054 sec/batch; 87h:47m:58s remains)
INFO - root - 2019-11-04 01:08:30.893464: step 105760, total loss = 0.62, predict loss = 0.15 (60.5 examples/sec; 0.066 sec/batch; 108h:18m:36s remains)
INFO - root - 2019-11-04 01:08:31.548992: step 105770, total loss = 0.63, predict loss = 0.15 (72.0 examples/sec; 0.056 sec/batch; 90h:59m:19s remains)
INFO - root - 2019-11-04 01:08:32.170173: step 105780, total loss = 0.61, predict loss = 0.16 (68.8 examples/sec; 0.058 sec/batch; 95h:10m:22s remains)
INFO - root - 2019-11-04 01:08:32.798626: step 105790, total loss = 0.52, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 93h:21m:31s remains)
INFO - root - 2019-11-04 01:08:33.419205: step 105800, total loss = 0.50, predict loss = 0.12 (66.1 examples/sec; 0.061 sec/batch; 99h:07m:09s remains)
INFO - root - 2019-11-04 01:08:34.063081: step 105810, total loss = 0.51, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 91h:41m:41s remains)
INFO - root - 2019-11-04 01:08:34.772046: step 105820, total loss = 0.59, predict loss = 0.14 (72.4 examples/sec; 0.055 sec/batch; 90h:25m:47s remains)
INFO - root - 2019-11-04 01:08:35.412152: step 105830, total loss = 0.40, predict loss = 0.09 (74.9 examples/sec; 0.053 sec/batch; 87h:24m:17s remains)
INFO - root - 2019-11-04 01:08:36.077147: step 105840, total loss = 0.55, predict loss = 0.13 (68.9 examples/sec; 0.058 sec/batch; 94h:59m:30s remains)
INFO - root - 2019-11-04 01:08:36.741533: step 105850, total loss = 0.48, predict loss = 0.11 (67.3 examples/sec; 0.059 sec/batch; 97h:15m:15s remains)
INFO - root - 2019-11-04 01:08:37.387836: step 105860, total loss = 0.54, predict loss = 0.13 (80.2 examples/sec; 0.050 sec/batch; 81h:38m:36s remains)
INFO - root - 2019-11-04 01:08:37.987592: step 105870, total loss = 0.57, predict loss = 0.13 (84.1 examples/sec; 0.048 sec/batch; 77h:55m:00s remains)
INFO - root - 2019-11-04 01:08:38.610851: step 105880, total loss = 0.40, predict loss = 0.10 (80.3 examples/sec; 0.050 sec/batch; 81h:35m:38s remains)
INFO - root - 2019-11-04 01:08:39.276865: step 105890, total loss = 0.46, predict loss = 0.11 (67.6 examples/sec; 0.059 sec/batch; 96h:51m:48s remains)
INFO - root - 2019-11-04 01:08:39.920086: step 105900, total loss = 0.49, predict loss = 0.11 (75.2 examples/sec; 0.053 sec/batch; 87h:04m:33s remains)
INFO - root - 2019-11-04 01:08:40.555886: step 105910, total loss = 0.45, predict loss = 0.11 (70.3 examples/sec; 0.057 sec/batch; 93h:06m:18s remains)
INFO - root - 2019-11-04 01:08:41.190941: step 105920, total loss = 0.41, predict loss = 0.09 (79.3 examples/sec; 0.050 sec/batch; 82h:34m:31s remains)
INFO - root - 2019-11-04 01:08:41.817817: step 105930, total loss = 0.37, predict loss = 0.08 (70.2 examples/sec; 0.057 sec/batch; 93h:16m:20s remains)
INFO - root - 2019-11-04 01:08:42.444921: step 105940, total loss = 0.47, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 93h:57m:17s remains)
INFO - root - 2019-11-04 01:08:43.109055: step 105950, total loss = 0.52, predict loss = 0.12 (61.6 examples/sec; 0.065 sec/batch; 106h:20m:52s remains)
INFO - root - 2019-11-04 01:08:43.787325: step 105960, total loss = 0.47, predict loss = 0.11 (62.5 examples/sec; 0.064 sec/batch; 104h:48m:43s remains)
INFO - root - 2019-11-04 01:08:44.391544: step 105970, total loss = 0.48, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 85h:25m:08s remains)
INFO - root - 2019-11-04 01:08:44.994600: step 105980, total loss = 0.49, predict loss = 0.12 (75.5 examples/sec; 0.053 sec/batch; 86h:46m:06s remains)
INFO - root - 2019-11-04 01:08:45.614846: step 105990, total loss = 0.50, predict loss = 0.11 (66.3 examples/sec; 0.060 sec/batch; 98h:45m:10s remains)
INFO - root - 2019-11-04 01:08:46.274855: step 106000, total loss = 0.49, predict loss = 0.12 (68.5 examples/sec; 0.058 sec/batch; 95h:36m:22s remains)
INFO - root - 2019-11-04 01:08:46.879441: step 106010, total loss = 0.55, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 92h:51m:14s remains)
INFO - root - 2019-11-04 01:08:47.515722: step 106020, total loss = 0.48, predict loss = 0.11 (72.9 examples/sec; 0.055 sec/batch; 89h:49m:15s remains)
INFO - root - 2019-11-04 01:08:48.198769: step 106030, total loss = 0.47, predict loss = 0.11 (62.7 examples/sec; 0.064 sec/batch; 104h:27m:14s remains)
INFO - root - 2019-11-04 01:08:48.881420: step 106040, total loss = 0.50, predict loss = 0.11 (75.9 examples/sec; 0.053 sec/batch; 86h:15m:58s remains)
INFO - root - 2019-11-04 01:08:49.505141: step 106050, total loss = 0.45, predict loss = 0.11 (78.6 examples/sec; 0.051 sec/batch; 83h:19m:52s remains)
INFO - root - 2019-11-04 01:08:50.108592: step 106060, total loss = 0.47, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 98h:29m:53s remains)
INFO - root - 2019-11-04 01:08:50.742603: step 106070, total loss = 0.40, predict loss = 0.09 (68.2 examples/sec; 0.059 sec/batch; 95h:59m:09s remains)
INFO - root - 2019-11-04 01:08:51.393453: step 106080, total loss = 0.46, predict loss = 0.11 (62.9 examples/sec; 0.064 sec/batch; 104h:07m:57s remains)
INFO - root - 2019-11-04 01:08:52.039699: step 106090, total loss = 0.42, predict loss = 0.10 (73.2 examples/sec; 0.055 sec/batch; 89h:29m:25s remains)
INFO - root - 2019-11-04 01:08:52.664264: step 106100, total loss = 0.73, predict loss = 0.18 (72.0 examples/sec; 0.056 sec/batch; 91h:00m:11s remains)
INFO - root - 2019-11-04 01:08:53.290870: step 106110, total loss = 0.64, predict loss = 0.15 (64.0 examples/sec; 0.062 sec/batch; 102h:15m:33s remains)
INFO - root - 2019-11-04 01:08:53.923045: step 106120, total loss = 0.65, predict loss = 0.15 (63.6 examples/sec; 0.063 sec/batch; 103h:02m:44s remains)
INFO - root - 2019-11-04 01:08:54.548360: step 106130, total loss = 0.59, predict loss = 0.14 (66.8 examples/sec; 0.060 sec/batch; 97h:58m:46s remains)
INFO - root - 2019-11-04 01:08:55.173554: step 106140, total loss = 0.72, predict loss = 0.18 (70.7 examples/sec; 0.057 sec/batch; 92h:38m:30s remains)
INFO - root - 2019-11-04 01:08:55.812254: step 106150, total loss = 0.51, predict loss = 0.12 (81.5 examples/sec; 0.049 sec/batch; 80h:23m:44s remains)
INFO - root - 2019-11-04 01:08:56.450174: step 106160, total loss = 0.76, predict loss = 0.18 (73.7 examples/sec; 0.054 sec/batch; 88h:48m:24s remains)
INFO - root - 2019-11-04 01:08:57.074765: step 106170, total loss = 0.61, predict loss = 0.14 (77.1 examples/sec; 0.052 sec/batch; 84h:56m:08s remains)
INFO - root - 2019-11-04 01:08:57.726321: step 106180, total loss = 0.70, predict loss = 0.18 (64.2 examples/sec; 0.062 sec/batch; 102h:01m:18s remains)
INFO - root - 2019-11-04 01:08:58.370812: step 106190, total loss = 0.63, predict loss = 0.15 (61.3 examples/sec; 0.065 sec/batch; 106h:47m:31s remains)
INFO - root - 2019-11-04 01:08:58.979891: step 106200, total loss = 0.39, predict loss = 0.09 (72.7 examples/sec; 0.055 sec/batch; 90h:00m:58s remains)
INFO - root - 2019-11-04 01:08:59.591607: step 106210, total loss = 0.42, predict loss = 0.09 (70.2 examples/sec; 0.057 sec/batch; 93h:17m:21s remains)
INFO - root - 2019-11-04 01:09:00.242646: step 106220, total loss = 0.39, predict loss = 0.08 (62.5 examples/sec; 0.064 sec/batch; 104h:42m:59s remains)
INFO - root - 2019-11-04 01:09:00.904089: step 106230, total loss = 0.53, predict loss = 0.12 (63.3 examples/sec; 0.063 sec/batch; 103h:25m:03s remains)
INFO - root - 2019-11-04 01:09:01.563324: step 106240, total loss = 0.44, predict loss = 0.10 (72.0 examples/sec; 0.056 sec/batch; 90h:54m:38s remains)
INFO - root - 2019-11-04 01:09:02.246814: step 106250, total loss = 0.71, predict loss = 0.17 (66.7 examples/sec; 0.060 sec/batch; 98h:07m:40s remains)
INFO - root - 2019-11-04 01:09:02.897613: step 106260, total loss = 0.43, predict loss = 0.10 (77.6 examples/sec; 0.052 sec/batch; 84h:24m:07s remains)
INFO - root - 2019-11-04 01:09:03.556439: step 106270, total loss = 0.57, predict loss = 0.16 (59.5 examples/sec; 0.067 sec/batch; 110h:00m:11s remains)
INFO - root - 2019-11-04 01:09:04.175219: step 106280, total loss = 0.55, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 95h:10m:54s remains)
INFO - root - 2019-11-04 01:09:04.873142: step 106290, total loss = 0.67, predict loss = 0.16 (72.2 examples/sec; 0.055 sec/batch; 90h:40m:23s remains)
INFO - root - 2019-11-04 01:09:05.510440: step 106300, total loss = 0.48, predict loss = 0.11 (62.8 examples/sec; 0.064 sec/batch; 104h:19m:47s remains)
INFO - root - 2019-11-04 01:09:06.164740: step 106310, total loss = 0.44, predict loss = 0.10 (67.0 examples/sec; 0.060 sec/batch; 97h:41m:45s remains)
INFO - root - 2019-11-04 01:09:06.826093: step 106320, total loss = 0.45, predict loss = 0.10 (70.2 examples/sec; 0.057 sec/batch; 93h:19m:13s remains)
INFO - root - 2019-11-04 01:09:07.443911: step 106330, total loss = 0.57, predict loss = 0.14 (69.9 examples/sec; 0.057 sec/batch; 93h:43m:49s remains)
INFO - root - 2019-11-04 01:09:08.064272: step 106340, total loss = 0.45, predict loss = 0.10 (73.4 examples/sec; 0.055 sec/batch; 89h:14m:50s remains)
INFO - root - 2019-11-04 01:09:08.599710: step 106350, total loss = 0.56, predict loss = 0.13 (93.2 examples/sec; 0.043 sec/batch; 70h:14m:19s remains)
INFO - root - 2019-11-04 01:09:09.076721: step 106360, total loss = 0.46, predict loss = 0.11 (88.6 examples/sec; 0.045 sec/batch; 73h:56m:12s remains)
INFO - root - 2019-11-04 01:09:10.113236: step 106370, total loss = 0.44, predict loss = 0.10 (76.0 examples/sec; 0.053 sec/batch; 86h:10m:36s remains)
INFO - root - 2019-11-04 01:09:10.693095: step 106380, total loss = 0.48, predict loss = 0.11 (75.9 examples/sec; 0.053 sec/batch; 86h:14m:55s remains)
INFO - root - 2019-11-04 01:09:11.332132: step 106390, total loss = 0.26, predict loss = 0.06 (73.8 examples/sec; 0.054 sec/batch; 88h:41m:01s remains)
INFO - root - 2019-11-04 01:09:11.958326: step 106400, total loss = 0.46, predict loss = 0.11 (65.3 examples/sec; 0.061 sec/batch; 100h:21m:23s remains)
INFO - root - 2019-11-04 01:09:12.576439: step 106410, total loss = 0.51, predict loss = 0.12 (72.3 examples/sec; 0.055 sec/batch; 90h:34m:45s remains)
INFO - root - 2019-11-04 01:09:13.203206: step 106420, total loss = 0.42, predict loss = 0.09 (74.9 examples/sec; 0.053 sec/batch; 87h:24m:12s remains)
INFO - root - 2019-11-04 01:09:13.822806: step 106430, total loss = 0.39, predict loss = 0.08 (75.3 examples/sec; 0.053 sec/batch; 87h:00m:17s remains)
INFO - root - 2019-11-04 01:09:14.443274: step 106440, total loss = 0.49, predict loss = 0.11 (71.0 examples/sec; 0.056 sec/batch; 92h:10m:01s remains)
INFO - root - 2019-11-04 01:09:15.068881: step 106450, total loss = 0.57, predict loss = 0.14 (76.3 examples/sec; 0.052 sec/batch; 85h:50m:26s remains)
INFO - root - 2019-11-04 01:09:15.699641: step 106460, total loss = 0.68, predict loss = 0.16 (62.7 examples/sec; 0.064 sec/batch; 104h:28m:21s remains)
INFO - root - 2019-11-04 01:09:16.333803: step 106470, total loss = 0.54, predict loss = 0.12 (69.2 examples/sec; 0.058 sec/batch; 94h:40m:59s remains)
INFO - root - 2019-11-04 01:09:16.973298: step 106480, total loss = 0.53, predict loss = 0.12 (69.7 examples/sec; 0.057 sec/batch; 93h:55m:33s remains)
INFO - root - 2019-11-04 01:09:17.609196: step 106490, total loss = 0.48, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 96h:11m:42s remains)
INFO - root - 2019-11-04 01:09:18.275146: step 106500, total loss = 0.51, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 88h:38m:05s remains)
INFO - root - 2019-11-04 01:09:18.876625: step 106510, total loss = 0.51, predict loss = 0.11 (66.3 examples/sec; 0.060 sec/batch; 98h:45m:31s remains)
INFO - root - 2019-11-04 01:09:19.509685: step 106520, total loss = 0.47, predict loss = 0.10 (70.9 examples/sec; 0.056 sec/batch; 92h:19m:45s remains)
INFO - root - 2019-11-04 01:09:20.128581: step 106530, total loss = 0.46, predict loss = 0.10 (70.9 examples/sec; 0.056 sec/batch; 92h:23m:11s remains)
INFO - root - 2019-11-04 01:09:20.759271: step 106540, total loss = 0.42, predict loss = 0.08 (71.0 examples/sec; 0.056 sec/batch; 92h:17m:15s remains)
INFO - root - 2019-11-04 01:09:21.401318: step 106550, total loss = 0.47, predict loss = 0.10 (69.4 examples/sec; 0.058 sec/batch; 94h:24m:09s remains)
INFO - root - 2019-11-04 01:09:22.061261: step 106560, total loss = 0.39, predict loss = 0.08 (68.2 examples/sec; 0.059 sec/batch; 96h:01m:14s remains)
INFO - root - 2019-11-04 01:09:22.683868: step 106570, total loss = 0.22, predict loss = 0.05 (65.5 examples/sec; 0.061 sec/batch; 99h:54m:32s remains)
INFO - root - 2019-11-04 01:09:23.338142: step 106580, total loss = 0.46, predict loss = 0.10 (66.9 examples/sec; 0.060 sec/batch; 97h:49m:21s remains)
INFO - root - 2019-11-04 01:09:23.971762: step 106590, total loss = 0.37, predict loss = 0.08 (69.4 examples/sec; 0.058 sec/batch; 94h:19m:44s remains)
INFO - root - 2019-11-04 01:09:24.598255: step 106600, total loss = 0.32, predict loss = 0.07 (76.5 examples/sec; 0.052 sec/batch; 85h:33m:33s remains)
INFO - root - 2019-11-04 01:09:25.225008: step 106610, total loss = 0.48, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 92h:47m:10s remains)
INFO - root - 2019-11-04 01:09:25.859601: step 106620, total loss = 0.58, predict loss = 0.13 (69.4 examples/sec; 0.058 sec/batch; 94h:23m:37s remains)
INFO - root - 2019-11-04 01:09:26.482359: step 106630, total loss = 0.52, predict loss = 0.12 (73.2 examples/sec; 0.055 sec/batch; 89h:28m:46s remains)
INFO - root - 2019-11-04 01:09:27.133464: step 106640, total loss = 0.53, predict loss = 0.12 (64.4 examples/sec; 0.062 sec/batch; 101h:37m:12s remains)
INFO - root - 2019-11-04 01:09:27.771149: step 106650, total loss = 0.64, predict loss = 0.15 (78.4 examples/sec; 0.051 sec/batch; 83h:28m:21s remains)
INFO - root - 2019-11-04 01:09:28.369097: step 106660, total loss = 0.47, predict loss = 0.11 (77.0 examples/sec; 0.052 sec/batch; 85h:00m:53s remains)
INFO - root - 2019-11-04 01:09:28.958971: step 106670, total loss = 0.47, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 98h:26m:19s remains)
INFO - root - 2019-11-04 01:09:29.563209: step 106680, total loss = 0.50, predict loss = 0.11 (66.6 examples/sec; 0.060 sec/batch; 98h:18m:04s remains)
INFO - root - 2019-11-04 01:09:30.208830: step 106690, total loss = 0.72, predict loss = 0.18 (69.2 examples/sec; 0.058 sec/batch; 94h:40m:21s remains)
INFO - root - 2019-11-04 01:09:30.819833: step 106700, total loss = 0.47, predict loss = 0.11 (69.6 examples/sec; 0.057 sec/batch; 94h:05m:39s remains)
INFO - root - 2019-11-04 01:09:31.463392: step 106710, total loss = 0.65, predict loss = 0.15 (67.2 examples/sec; 0.059 sec/batch; 97h:23m:11s remains)
INFO - root - 2019-11-04 01:09:32.103449: step 106720, total loss = 0.70, predict loss = 0.15 (72.7 examples/sec; 0.055 sec/batch; 90h:05m:57s remains)
INFO - root - 2019-11-04 01:09:32.743631: step 106730, total loss = 0.59, predict loss = 0.13 (73.8 examples/sec; 0.054 sec/batch; 88h:41m:01s remains)
INFO - root - 2019-11-04 01:09:33.402380: step 106740, total loss = 0.51, predict loss = 0.12 (71.9 examples/sec; 0.056 sec/batch; 91h:02m:17s remains)
INFO - root - 2019-11-04 01:09:34.019810: step 106750, total loss = 0.56, predict loss = 0.13 (70.1 examples/sec; 0.057 sec/batch; 93h:23m:50s remains)
INFO - root - 2019-11-04 01:09:34.727559: step 106760, total loss = 0.45, predict loss = 0.09 (71.4 examples/sec; 0.056 sec/batch; 91h:38m:54s remains)
INFO - root - 2019-11-04 01:09:35.326144: step 106770, total loss = 0.46, predict loss = 0.11 (69.4 examples/sec; 0.058 sec/batch; 94h:17m:37s remains)
INFO - root - 2019-11-04 01:09:35.947488: step 106780, total loss = 0.42, predict loss = 0.09 (72.2 examples/sec; 0.055 sec/batch; 90h:40m:58s remains)
INFO - root - 2019-11-04 01:09:36.585323: step 106790, total loss = 0.64, predict loss = 0.15 (79.2 examples/sec; 0.051 sec/batch; 82h:43m:23s remains)
INFO - root - 2019-11-04 01:09:37.244038: step 106800, total loss = 0.48, predict loss = 0.11 (70.3 examples/sec; 0.057 sec/batch; 93h:10m:18s remains)
INFO - root - 2019-11-04 01:09:37.887864: step 106810, total loss = 0.42, predict loss = 0.09 (65.3 examples/sec; 0.061 sec/batch; 100h:18m:50s remains)
INFO - root - 2019-11-04 01:09:38.493714: step 106820, total loss = 0.38, predict loss = 0.08 (77.2 examples/sec; 0.052 sec/batch; 84h:49m:08s remains)
INFO - root - 2019-11-04 01:09:39.106599: step 106830, total loss = 0.51, predict loss = 0.12 (59.3 examples/sec; 0.067 sec/batch; 110h:19m:53s remains)
INFO - root - 2019-11-04 01:09:39.764289: step 106840, total loss = 0.34, predict loss = 0.08 (63.9 examples/sec; 0.063 sec/batch; 102h:25m:11s remains)
INFO - root - 2019-11-04 01:09:40.423178: step 106850, total loss = 0.36, predict loss = 0.09 (67.9 examples/sec; 0.059 sec/batch; 96h:30m:19s remains)
INFO - root - 2019-11-04 01:09:41.086079: step 106860, total loss = 0.56, predict loss = 0.14 (67.3 examples/sec; 0.059 sec/batch; 97h:15m:41s remains)
INFO - root - 2019-11-04 01:09:41.688047: step 106870, total loss = 0.52, predict loss = 0.12 (73.6 examples/sec; 0.054 sec/batch; 88h:55m:35s remains)
INFO - root - 2019-11-04 01:09:42.279488: step 106880, total loss = 0.50, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 99h:19m:21s remains)
INFO - root - 2019-11-04 01:09:42.903402: step 106890, total loss = 0.44, predict loss = 0.10 (74.4 examples/sec; 0.054 sec/batch; 87h:59m:05s remains)
INFO - root - 2019-11-04 01:09:43.522711: step 106900, total loss = 0.45, predict loss = 0.11 (71.8 examples/sec; 0.056 sec/batch; 91h:08m:39s remains)
INFO - root - 2019-11-04 01:09:44.135318: step 106910, total loss = 0.36, predict loss = 0.07 (80.4 examples/sec; 0.050 sec/batch; 81h:26m:31s remains)
INFO - root - 2019-11-04 01:09:44.756293: step 106920, total loss = 0.33, predict loss = 0.08 (61.6 examples/sec; 0.065 sec/batch; 106h:21m:21s remains)
INFO - root - 2019-11-04 01:09:45.354310: step 106930, total loss = 0.43, predict loss = 0.11 (71.5 examples/sec; 0.056 sec/batch; 91h:33m:32s remains)
INFO - root - 2019-11-04 01:09:45.972560: step 106940, total loss = 0.31, predict loss = 0.07 (66.7 examples/sec; 0.060 sec/batch; 98h:12m:30s remains)
INFO - root - 2019-11-04 01:09:46.636427: step 106950, total loss = 0.33, predict loss = 0.08 (62.8 examples/sec; 0.064 sec/batch; 104h:16m:32s remains)
INFO - root - 2019-11-04 01:09:47.302415: step 106960, total loss = 0.31, predict loss = 0.07 (65.5 examples/sec; 0.061 sec/batch; 99h:59m:57s remains)
INFO - root - 2019-11-04 01:09:47.946565: step 106970, total loss = 0.41, predict loss = 0.10 (72.8 examples/sec; 0.055 sec/batch; 89h:56m:18s remains)
INFO - root - 2019-11-04 01:09:48.576538: step 106980, total loss = 0.28, predict loss = 0.06 (65.2 examples/sec; 0.061 sec/batch; 100h:23m:01s remains)
INFO - root - 2019-11-04 01:09:49.246315: step 106990, total loss = 0.36, predict loss = 0.08 (63.9 examples/sec; 0.063 sec/batch; 102h:25m:59s remains)
INFO - root - 2019-11-04 01:09:49.895242: step 107000, total loss = 0.35, predict loss = 0.08 (77.6 examples/sec; 0.052 sec/batch; 84h:21m:01s remains)
INFO - root - 2019-11-04 01:09:50.561169: step 107010, total loss = 0.38, predict loss = 0.09 (73.3 examples/sec; 0.055 sec/batch; 89h:18m:44s remains)
INFO - root - 2019-11-04 01:09:51.212496: step 107020, total loss = 0.46, predict loss = 0.11 (62.1 examples/sec; 0.064 sec/batch; 105h:27m:50s remains)
INFO - root - 2019-11-04 01:09:51.837324: step 107030, total loss = 0.40, predict loss = 0.09 (67.5 examples/sec; 0.059 sec/batch; 97h:02m:18s remains)
INFO - root - 2019-11-04 01:09:52.459604: step 107040, total loss = 0.61, predict loss = 0.14 (65.3 examples/sec; 0.061 sec/batch; 100h:12m:21s remains)
INFO - root - 2019-11-04 01:09:53.075597: step 107050, total loss = 0.54, predict loss = 0.13 (70.0 examples/sec; 0.057 sec/batch; 93h:31m:24s remains)
INFO - root - 2019-11-04 01:09:53.739083: step 107060, total loss = 0.51, predict loss = 0.12 (64.6 examples/sec; 0.062 sec/batch; 101h:22m:39s remains)
INFO - root - 2019-11-04 01:09:54.398936: step 107070, total loss = 0.49, predict loss = 0.11 (69.9 examples/sec; 0.057 sec/batch; 93h:43m:05s remains)
INFO - root - 2019-11-04 01:09:55.050086: step 107080, total loss = 0.51, predict loss = 0.12 (63.9 examples/sec; 0.063 sec/batch; 102h:30m:52s remains)
INFO - root - 2019-11-04 01:09:55.657351: step 107090, total loss = 0.62, predict loss = 0.15 (73.3 examples/sec; 0.055 sec/batch; 89h:17m:08s remains)
INFO - root - 2019-11-04 01:09:56.287283: step 107100, total loss = 0.41, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 83h:46m:49s remains)
INFO - root - 2019-11-04 01:09:56.945165: step 107110, total loss = 0.60, predict loss = 0.15 (63.8 examples/sec; 0.063 sec/batch; 102h:41m:01s remains)
INFO - root - 2019-11-04 01:09:57.583494: step 107120, total loss = 0.49, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 93h:59m:32s remains)
INFO - root - 2019-11-04 01:09:58.232889: step 107130, total loss = 0.55, predict loss = 0.14 (68.6 examples/sec; 0.058 sec/batch; 95h:24m:13s remains)
INFO - root - 2019-11-04 01:09:58.912898: step 107140, total loss = 0.53, predict loss = 0.13 (60.9 examples/sec; 0.066 sec/batch; 107h:26m:58s remains)
INFO - root - 2019-11-04 01:09:59.591924: step 107150, total loss = 0.39, predict loss = 0.09 (64.9 examples/sec; 0.062 sec/batch; 100h:51m:40s remains)
INFO - root - 2019-11-04 01:10:00.256296: step 107160, total loss = 0.47, predict loss = 0.11 (66.8 examples/sec; 0.060 sec/batch; 98h:05m:02s remains)
INFO - root - 2019-11-04 01:10:00.939395: step 107170, total loss = 0.59, predict loss = 0.15 (65.6 examples/sec; 0.061 sec/batch; 99h:52m:34s remains)
INFO - root - 2019-11-04 01:10:01.575750: step 107180, total loss = 0.43, predict loss = 0.09 (66.3 examples/sec; 0.060 sec/batch; 98h:47m:54s remains)
INFO - root - 2019-11-04 01:10:02.237173: step 107190, total loss = 0.40, predict loss = 0.09 (67.8 examples/sec; 0.059 sec/batch; 96h:33m:04s remains)
INFO - root - 2019-11-04 01:10:03.374266: step 107200, total loss = 0.33, predict loss = 0.07 (73.2 examples/sec; 0.055 sec/batch; 89h:28m:08s remains)
INFO - root - 2019-11-04 01:10:03.980190: step 107210, total loss = 0.39, predict loss = 0.09 (74.6 examples/sec; 0.054 sec/batch; 87h:46m:11s remains)
INFO - root - 2019-11-04 01:10:04.627742: step 107220, total loss = 0.31, predict loss = 0.07 (61.0 examples/sec; 0.066 sec/batch; 107h:24m:38s remains)
INFO - root - 2019-11-04 01:10:05.229746: step 107230, total loss = 0.23, predict loss = 0.05 (74.4 examples/sec; 0.054 sec/batch; 88h:01m:46s remains)
INFO - root - 2019-11-04 01:10:05.841217: step 107240, total loss = 0.50, predict loss = 0.11 (77.4 examples/sec; 0.052 sec/batch; 84h:33m:16s remains)
INFO - root - 2019-11-04 01:10:06.486914: step 107250, total loss = 0.29, predict loss = 0.06 (71.8 examples/sec; 0.056 sec/batch; 91h:12m:27s remains)
INFO - root - 2019-11-04 01:10:07.129512: step 107260, total loss = 0.44, predict loss = 0.09 (72.6 examples/sec; 0.055 sec/batch; 90h:07m:50s remains)
INFO - root - 2019-11-04 01:10:07.757551: step 107270, total loss = 0.38, predict loss = 0.08 (68.8 examples/sec; 0.058 sec/batch; 95h:09m:42s remains)
INFO - root - 2019-11-04 01:10:08.370189: step 107280, total loss = 0.48, predict loss = 0.11 (67.9 examples/sec; 0.059 sec/batch; 96h:25m:40s remains)
INFO - root - 2019-11-04 01:10:09.001846: step 107290, total loss = 0.37, predict loss = 0.08 (66.6 examples/sec; 0.060 sec/batch; 98h:15m:53s remains)
INFO - root - 2019-11-04 01:10:09.651640: step 107300, total loss = 0.45, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 99h:17m:43s remains)
INFO - root - 2019-11-04 01:10:10.279103: step 107310, total loss = 0.47, predict loss = 0.10 (66.4 examples/sec; 0.060 sec/batch; 98h:34m:45s remains)
INFO - root - 2019-11-04 01:10:10.892790: step 107320, total loss = 0.37, predict loss = 0.09 (72.7 examples/sec; 0.055 sec/batch; 90h:01m:47s remains)
INFO - root - 2019-11-04 01:10:11.515315: step 107330, total loss = 0.38, predict loss = 0.08 (70.6 examples/sec; 0.057 sec/batch; 92h:46m:58s remains)
INFO - root - 2019-11-04 01:10:12.129812: step 107340, total loss = 0.34, predict loss = 0.08 (77.3 examples/sec; 0.052 sec/batch; 84h:43m:19s remains)
INFO - root - 2019-11-04 01:10:12.757424: step 107350, total loss = 0.39, predict loss = 0.09 (72.8 examples/sec; 0.055 sec/batch; 89h:55m:11s remains)
INFO - root - 2019-11-04 01:10:13.412895: step 107360, total loss = 0.56, predict loss = 0.14 (62.3 examples/sec; 0.064 sec/batch; 105h:05m:44s remains)
INFO - root - 2019-11-04 01:10:14.076660: step 107370, total loss = 0.55, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 94h:47m:33s remains)
INFO - root - 2019-11-04 01:10:14.717266: step 107380, total loss = 0.63, predict loss = 0.15 (82.1 examples/sec; 0.049 sec/batch; 79h:46m:06s remains)
INFO - root - 2019-11-04 01:10:15.385031: step 107390, total loss = 0.46, predict loss = 0.11 (58.0 examples/sec; 0.069 sec/batch; 112h:50m:29s remains)
INFO - root - 2019-11-04 01:10:16.000485: step 107400, total loss = 0.51, predict loss = 0.12 (74.3 examples/sec; 0.054 sec/batch; 88h:08m:27s remains)
INFO - root - 2019-11-04 01:10:16.619721: step 107410, total loss = 0.43, predict loss = 0.10 (73.3 examples/sec; 0.055 sec/batch; 89h:16m:21s remains)
INFO - root - 2019-11-04 01:10:17.239888: step 107420, total loss = 0.24, predict loss = 0.05 (72.9 examples/sec; 0.055 sec/batch; 89h:46m:57s remains)
INFO - root - 2019-11-04 01:10:17.894306: step 107430, total loss = 0.48, predict loss = 0.11 (55.2 examples/sec; 0.073 sec/batch; 118h:42m:06s remains)
INFO - root - 2019-11-04 01:10:18.520682: step 107440, total loss = 0.35, predict loss = 0.08 (67.2 examples/sec; 0.059 sec/batch; 97h:22m:35s remains)
INFO - root - 2019-11-04 01:10:19.178146: step 107450, total loss = 0.33, predict loss = 0.08 (66.9 examples/sec; 0.060 sec/batch; 97h:48m:25s remains)
INFO - root - 2019-11-04 01:10:19.833031: step 107460, total loss = 0.36, predict loss = 0.08 (76.2 examples/sec; 0.053 sec/batch; 85h:57m:53s remains)
INFO - root - 2019-11-04 01:10:20.475031: step 107470, total loss = 0.39, predict loss = 0.09 (63.0 examples/sec; 0.064 sec/batch; 103h:56m:47s remains)
INFO - root - 2019-11-04 01:10:21.136254: step 107480, total loss = 0.33, predict loss = 0.08 (66.5 examples/sec; 0.060 sec/batch; 98h:22m:51s remains)
INFO - root - 2019-11-04 01:10:21.813751: step 107490, total loss = 0.41, predict loss = 0.10 (68.5 examples/sec; 0.058 sec/batch; 95h:37m:29s remains)
INFO - root - 2019-11-04 01:10:22.467473: step 107500, total loss = 0.34, predict loss = 0.08 (69.7 examples/sec; 0.057 sec/batch; 93h:54m:58s remains)
INFO - root - 2019-11-04 01:10:23.114734: step 107510, total loss = 0.44, predict loss = 0.10 (64.1 examples/sec; 0.062 sec/batch; 102h:10m:57s remains)
INFO - root - 2019-11-04 01:10:23.779463: step 107520, total loss = 0.42, predict loss = 0.10 (67.1 examples/sec; 0.060 sec/batch; 97h:36m:47s remains)
INFO - root - 2019-11-04 01:10:24.471974: step 107530, total loss = 0.45, predict loss = 0.11 (63.3 examples/sec; 0.063 sec/batch; 103h:24m:37s remains)
INFO - root - 2019-11-04 01:10:25.127142: step 107540, total loss = 0.42, predict loss = 0.10 (63.5 examples/sec; 0.063 sec/batch; 103h:01m:44s remains)
INFO - root - 2019-11-04 01:10:25.785912: step 107550, total loss = 0.43, predict loss = 0.09 (65.2 examples/sec; 0.061 sec/batch; 100h:21m:07s remains)
INFO - root - 2019-11-04 01:10:26.422254: step 107560, total loss = 0.32, predict loss = 0.07 (69.2 examples/sec; 0.058 sec/batch; 94h:36m:40s remains)
INFO - root - 2019-11-04 01:10:27.018083: step 107570, total loss = 0.38, predict loss = 0.09 (77.4 examples/sec; 0.052 sec/batch; 84h:35m:18s remains)
INFO - root - 2019-11-04 01:10:27.603611: step 107580, total loss = 0.39, predict loss = 0.09 (81.8 examples/sec; 0.049 sec/batch; 80h:00m:40s remains)
INFO - root - 2019-11-04 01:10:28.210123: step 107590, total loss = 0.34, predict loss = 0.08 (75.7 examples/sec; 0.053 sec/batch; 86h:31m:50s remains)
INFO - root - 2019-11-04 01:10:28.902180: step 107600, total loss = 0.40, predict loss = 0.09 (60.6 examples/sec; 0.066 sec/batch; 108h:05m:45s remains)
INFO - root - 2019-11-04 01:10:29.584959: step 107610, total loss = 0.39, predict loss = 0.09 (64.5 examples/sec; 0.062 sec/batch; 101h:30m:49s remains)
INFO - root - 2019-11-04 01:10:30.281807: step 107620, total loss = 0.47, predict loss = 0.11 (69.6 examples/sec; 0.057 sec/batch; 94h:06m:18s remains)
INFO - root - 2019-11-04 01:10:30.914229: step 107630, total loss = 0.54, predict loss = 0.13 (67.4 examples/sec; 0.059 sec/batch; 97h:07m:23s remains)
INFO - root - 2019-11-04 01:10:31.546123: step 107640, total loss = 0.57, predict loss = 0.13 (70.7 examples/sec; 0.057 sec/batch; 92h:35m:53s remains)
INFO - root - 2019-11-04 01:10:32.187195: step 107650, total loss = 0.43, predict loss = 0.09 (82.1 examples/sec; 0.049 sec/batch; 79h:44m:43s remains)
INFO - root - 2019-11-04 01:10:32.833569: step 107660, total loss = 0.51, predict loss = 0.12 (64.8 examples/sec; 0.062 sec/batch; 101h:02m:09s remains)
INFO - root - 2019-11-04 01:10:33.508358: step 107670, total loss = 0.65, predict loss = 0.15 (75.2 examples/sec; 0.053 sec/batch; 87h:00m:21s remains)
INFO - root - 2019-11-04 01:10:34.144340: step 107680, total loss = 0.35, predict loss = 0.07 (75.0 examples/sec; 0.053 sec/batch; 87h:14m:48s remains)
INFO - root - 2019-11-04 01:10:34.828365: step 107690, total loss = 0.57, predict loss = 0.13 (81.7 examples/sec; 0.049 sec/batch; 80h:09m:55s remains)
INFO - root - 2019-11-04 01:10:35.423667: step 107700, total loss = 0.55, predict loss = 0.12 (70.1 examples/sec; 0.057 sec/batch; 93h:24m:53s remains)
INFO - root - 2019-11-04 01:10:36.099855: step 107710, total loss = 0.32, predict loss = 0.07 (67.0 examples/sec; 0.060 sec/batch; 97h:44m:07s remains)
INFO - root - 2019-11-04 01:10:36.716525: step 107720, total loss = 0.31, predict loss = 0.07 (77.6 examples/sec; 0.052 sec/batch; 84h:21m:00s remains)
INFO - root - 2019-11-04 01:10:37.382554: step 107730, total loss = 0.26, predict loss = 0.05 (63.3 examples/sec; 0.063 sec/batch; 103h:29m:27s remains)
INFO - root - 2019-11-04 01:10:38.085488: step 107740, total loss = 0.50, predict loss = 0.12 (62.0 examples/sec; 0.065 sec/batch; 105h:38m:22s remains)
INFO - root - 2019-11-04 01:10:38.712065: step 107750, total loss = 0.38, predict loss = 0.09 (73.2 examples/sec; 0.055 sec/batch; 89h:26m:44s remains)
INFO - root - 2019-11-04 01:10:39.352790: step 107760, total loss = 0.41, predict loss = 0.09 (69.1 examples/sec; 0.058 sec/batch; 94h:43m:59s remains)
INFO - root - 2019-11-04 01:10:40.015141: step 107770, total loss = 0.32, predict loss = 0.07 (68.9 examples/sec; 0.058 sec/batch; 95h:00m:54s remains)
INFO - root - 2019-11-04 01:10:40.661497: step 107780, total loss = 0.49, predict loss = 0.12 (74.5 examples/sec; 0.054 sec/batch; 87h:53m:02s remains)
INFO - root - 2019-11-04 01:10:41.299531: step 107790, total loss = 0.56, predict loss = 0.14 (71.1 examples/sec; 0.056 sec/batch; 92h:07m:02s remains)
INFO - root - 2019-11-04 01:10:41.952962: step 107800, total loss = 0.68, predict loss = 0.16 (73.4 examples/sec; 0.054 sec/batch; 89h:11m:55s remains)
INFO - root - 2019-11-04 01:10:42.588114: step 107810, total loss = 0.57, predict loss = 0.14 (68.7 examples/sec; 0.058 sec/batch; 95h:15m:40s remains)
INFO - root - 2019-11-04 01:10:43.207531: step 107820, total loss = 0.79, predict loss = 0.19 (65.3 examples/sec; 0.061 sec/batch; 100h:18m:33s remains)
INFO - root - 2019-11-04 01:10:43.827823: step 107830, total loss = 0.84, predict loss = 0.19 (73.2 examples/sec; 0.055 sec/batch; 89h:29m:24s remains)
INFO - root - 2019-11-04 01:10:44.484509: step 107840, total loss = 0.55, predict loss = 0.13 (66.7 examples/sec; 0.060 sec/batch; 98h:07m:04s remains)
INFO - root - 2019-11-04 01:10:45.106516: step 107850, total loss = 0.79, predict loss = 0.19 (69.3 examples/sec; 0.058 sec/batch; 94h:26m:29s remains)
INFO - root - 2019-11-04 01:10:45.748241: step 107860, total loss = 0.61, predict loss = 0.15 (71.8 examples/sec; 0.056 sec/batch; 91h:09m:11s remains)
INFO - root - 2019-11-04 01:10:46.387848: step 107870, total loss = 0.62, predict loss = 0.14 (69.9 examples/sec; 0.057 sec/batch; 93h:40m:19s remains)
INFO - root - 2019-11-04 01:10:47.042568: step 107880, total loss = 0.59, predict loss = 0.14 (68.7 examples/sec; 0.058 sec/batch; 95h:18m:02s remains)
INFO - root - 2019-11-04 01:10:47.697169: step 107890, total loss = 0.71, predict loss = 0.17 (66.8 examples/sec; 0.060 sec/batch; 97h:59m:58s remains)
INFO - root - 2019-11-04 01:10:48.326203: step 107900, total loss = 0.61, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 90h:39m:08s remains)
INFO - root - 2019-11-04 01:10:48.992680: step 107910, total loss = 0.52, predict loss = 0.11 (64.7 examples/sec; 0.062 sec/batch; 101h:14m:17s remains)
INFO - root - 2019-11-04 01:10:49.655480: step 107920, total loss = 0.57, predict loss = 0.14 (64.5 examples/sec; 0.062 sec/batch; 101h:32m:35s remains)
INFO - root - 2019-11-04 01:10:50.320947: step 107930, total loss = 0.58, predict loss = 0.13 (67.6 examples/sec; 0.059 sec/batch; 96h:54m:57s remains)
INFO - root - 2019-11-04 01:10:50.983248: step 107940, total loss = 0.59, predict loss = 0.14 (64.7 examples/sec; 0.062 sec/batch; 101h:09m:21s remains)
INFO - root - 2019-11-04 01:10:51.650768: step 107950, total loss = 0.53, predict loss = 0.12 (68.0 examples/sec; 0.059 sec/batch; 96h:20m:20s remains)
INFO - root - 2019-11-04 01:10:52.332084: step 107960, total loss = 0.34, predict loss = 0.08 (63.6 examples/sec; 0.063 sec/batch; 102h:51m:19s remains)
INFO - root - 2019-11-04 01:10:52.953227: step 107970, total loss = 0.46, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 96h:44m:09s remains)
INFO - root - 2019-11-04 01:10:53.579115: step 107980, total loss = 0.57, predict loss = 0.13 (75.3 examples/sec; 0.053 sec/batch; 86h:55m:46s remains)
INFO - root - 2019-11-04 01:10:54.198242: step 107990, total loss = 0.52, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 89h:57m:33s remains)
INFO - root - 2019-11-04 01:10:54.849037: step 108000, total loss = 0.43, predict loss = 0.10 (64.0 examples/sec; 0.062 sec/batch; 102h:15m:33s remains)
INFO - root - 2019-11-04 01:10:55.524783: step 108010, total loss = 0.49, predict loss = 0.11 (61.8 examples/sec; 0.065 sec/batch; 105h:53m:40s remains)
INFO - root - 2019-11-04 01:10:56.177849: step 108020, total loss = 0.52, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 93h:12m:47s remains)
INFO - root - 2019-11-04 01:10:56.799600: step 108030, total loss = 0.32, predict loss = 0.07 (74.7 examples/sec; 0.054 sec/batch; 87h:34m:57s remains)
INFO - root - 2019-11-04 01:10:57.434028: step 108040, total loss = 0.53, predict loss = 0.13 (66.4 examples/sec; 0.060 sec/batch; 98h:38m:10s remains)
INFO - root - 2019-11-04 01:10:58.071427: step 108050, total loss = 0.46, predict loss = 0.10 (67.2 examples/sec; 0.059 sec/batch; 97h:22m:22s remains)
INFO - root - 2019-11-04 01:10:58.699255: step 108060, total loss = 0.37, predict loss = 0.08 (64.0 examples/sec; 0.063 sec/batch; 102h:17m:30s remains)
INFO - root - 2019-11-04 01:10:59.374340: step 108070, total loss = 0.39, predict loss = 0.08 (66.2 examples/sec; 0.060 sec/batch; 98h:56m:07s remains)
INFO - root - 2019-11-04 01:11:00.058745: step 108080, total loss = 0.28, predict loss = 0.06 (63.0 examples/sec; 0.064 sec/batch; 103h:58m:15s remains)
INFO - root - 2019-11-04 01:11:00.758159: step 108090, total loss = 0.34, predict loss = 0.08 (60.0 examples/sec; 0.067 sec/batch; 109h:05m:54s remains)
INFO - root - 2019-11-04 01:11:01.455194: step 108100, total loss = 0.42, predict loss = 0.09 (61.9 examples/sec; 0.065 sec/batch; 105h:41m:03s remains)
INFO - root - 2019-11-04 01:11:02.131978: step 108110, total loss = 0.30, predict loss = 0.06 (63.5 examples/sec; 0.063 sec/batch; 103h:03m:45s remains)
INFO - root - 2019-11-04 01:11:02.831076: step 108120, total loss = 0.39, predict loss = 0.09 (72.1 examples/sec; 0.055 sec/batch; 90h:45m:59s remains)
INFO - root - 2019-11-04 01:11:03.518033: step 108130, total loss = 0.47, predict loss = 0.11 (62.5 examples/sec; 0.064 sec/batch; 104h:45m:54s remains)
INFO - root - 2019-11-04 01:11:04.165347: step 108140, total loss = 0.29, predict loss = 0.06 (61.2 examples/sec; 0.065 sec/batch; 107h:01m:41s remains)
INFO - root - 2019-11-04 01:11:04.829684: step 108150, total loss = 0.41, predict loss = 0.09 (72.7 examples/sec; 0.055 sec/batch; 90h:04m:07s remains)
INFO - root - 2019-11-04 01:11:05.409204: step 108160, total loss = 0.53, predict loss = 0.13 (78.2 examples/sec; 0.051 sec/batch; 83h:42m:03s remains)
INFO - root - 2019-11-04 01:11:05.991223: step 108170, total loss = 0.45, predict loss = 0.10 (70.8 examples/sec; 0.056 sec/batch; 92h:25m:37s remains)
INFO - root - 2019-11-04 01:11:06.621812: step 108180, total loss = 0.49, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 89h:58m:46s remains)
INFO - root - 2019-11-04 01:11:07.294437: step 108190, total loss = 0.52, predict loss = 0.12 (67.2 examples/sec; 0.060 sec/batch; 97h:24m:39s remains)
INFO - root - 2019-11-04 01:11:07.908359: step 108200, total loss = 0.57, predict loss = 0.14 (67.4 examples/sec; 0.059 sec/batch; 97h:10m:10s remains)
INFO - root - 2019-11-04 01:11:08.560366: step 108210, total loss = 0.56, predict loss = 0.13 (62.7 examples/sec; 0.064 sec/batch; 104h:23m:12s remains)
INFO - root - 2019-11-04 01:11:09.222262: step 108220, total loss = 0.47, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 91h:22m:56s remains)
INFO - root - 2019-11-04 01:11:09.839704: step 108230, total loss = 0.69, predict loss = 0.17 (67.6 examples/sec; 0.059 sec/batch; 96h:52m:36s remains)
INFO - root - 2019-11-04 01:11:10.478619: step 108240, total loss = 0.51, predict loss = 0.13 (71.7 examples/sec; 0.056 sec/batch; 91h:18m:36s remains)
INFO - root - 2019-11-04 01:11:11.134096: step 108250, total loss = 0.62, predict loss = 0.15 (64.8 examples/sec; 0.062 sec/batch; 101h:05m:21s remains)
INFO - root - 2019-11-04 01:11:11.791515: step 108260, total loss = 0.63, predict loss = 0.16 (62.1 examples/sec; 0.064 sec/batch; 105h:21m:42s remains)
INFO - root - 2019-11-04 01:11:12.413955: step 108270, total loss = 0.58, predict loss = 0.14 (80.2 examples/sec; 0.050 sec/batch; 81h:35m:43s remains)
INFO - root - 2019-11-04 01:11:13.016917: step 108280, total loss = 0.52, predict loss = 0.13 (82.6 examples/sec; 0.048 sec/batch; 79h:17m:02s remains)
INFO - root - 2019-11-04 01:11:13.621652: step 108290, total loss = 0.40, predict loss = 0.09 (73.8 examples/sec; 0.054 sec/batch; 88h:40m:47s remains)
INFO - root - 2019-11-04 01:11:14.291203: step 108300, total loss = 0.47, predict loss = 0.10 (65.4 examples/sec; 0.061 sec/batch; 100h:09m:49s remains)
INFO - root - 2019-11-04 01:11:14.962157: step 108310, total loss = 0.53, predict loss = 0.12 (69.2 examples/sec; 0.058 sec/batch; 94h:36m:22s remains)
INFO - root - 2019-11-04 01:11:15.567029: step 108320, total loss = 0.50, predict loss = 0.12 (74.0 examples/sec; 0.054 sec/batch; 88h:26m:51s remains)
INFO - root - 2019-11-04 01:11:16.185213: step 108330, total loss = 0.45, predict loss = 0.10 (69.4 examples/sec; 0.058 sec/batch; 94h:19m:39s remains)
INFO - root - 2019-11-04 01:11:16.802021: step 108340, total loss = 0.39, predict loss = 0.09 (70.1 examples/sec; 0.057 sec/batch; 93h:20m:13s remains)
INFO - root - 2019-11-04 01:11:17.425507: step 108350, total loss = 0.43, predict loss = 0.10 (70.0 examples/sec; 0.057 sec/batch; 93h:29m:49s remains)
INFO - root - 2019-11-04 01:11:18.039256: step 108360, total loss = 0.42, predict loss = 0.09 (75.1 examples/sec; 0.053 sec/batch; 87h:10m:06s remains)
INFO - root - 2019-11-04 01:11:18.700127: step 108370, total loss = 0.37, predict loss = 0.09 (69.9 examples/sec; 0.057 sec/batch; 93h:36m:07s remains)
INFO - root - 2019-11-04 01:11:19.349798: step 108380, total loss = 0.45, predict loss = 0.10 (66.8 examples/sec; 0.060 sec/batch; 97h:59m:13s remains)
INFO - root - 2019-11-04 01:11:19.939972: step 108390, total loss = 0.39, predict loss = 0.08 (80.3 examples/sec; 0.050 sec/batch; 81h:31m:02s remains)
INFO - root - 2019-11-04 01:11:20.578615: step 108400, total loss = 0.48, predict loss = 0.12 (62.9 examples/sec; 0.064 sec/batch; 104h:01m:32s remains)
INFO - root - 2019-11-04 01:11:21.223329: step 108410, total loss = 0.41, predict loss = 0.09 (64.2 examples/sec; 0.062 sec/batch; 101h:57m:10s remains)
INFO - root - 2019-11-04 01:11:21.888744: step 108420, total loss = 0.55, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 93h:49m:08s remains)
INFO - root - 2019-11-04 01:11:22.521353: step 108430, total loss = 0.61, predict loss = 0.15 (70.4 examples/sec; 0.057 sec/batch; 92h:57m:33s remains)
INFO - root - 2019-11-04 01:11:23.125427: step 108440, total loss = 0.58, predict loss = 0.13 (71.5 examples/sec; 0.056 sec/batch; 91h:30m:35s remains)
INFO - root - 2019-11-04 01:11:23.777527: step 108450, total loss = 0.56, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 89h:32m:48s remains)
INFO - root - 2019-11-04 01:11:24.408372: step 108460, total loss = 0.58, predict loss = 0.14 (70.9 examples/sec; 0.056 sec/batch; 92h:20m:13s remains)
INFO - root - 2019-11-04 01:11:25.067927: step 108470, total loss = 0.56, predict loss = 0.13 (69.3 examples/sec; 0.058 sec/batch; 94h:29m:35s remains)
INFO - root - 2019-11-04 01:11:25.704773: step 108480, total loss = 0.46, predict loss = 0.11 (77.7 examples/sec; 0.051 sec/batch; 84h:16m:03s remains)
INFO - root - 2019-11-04 01:11:26.350960: step 108490, total loss = 0.51, predict loss = 0.12 (74.5 examples/sec; 0.054 sec/batch; 87h:49m:11s remains)
INFO - root - 2019-11-04 01:11:27.012094: step 108500, total loss = 0.54, predict loss = 0.13 (60.4 examples/sec; 0.066 sec/batch; 108h:20m:03s remains)
INFO - root - 2019-11-04 01:11:27.677690: step 108510, total loss = 0.48, predict loss = 0.11 (69.0 examples/sec; 0.058 sec/batch; 94h:52m:28s remains)
INFO - root - 2019-11-04 01:11:28.355406: step 108520, total loss = 0.53, predict loss = 0.12 (63.1 examples/sec; 0.063 sec/batch; 103h:44m:54s remains)
INFO - root - 2019-11-04 01:11:29.051478: step 108530, total loss = 0.51, predict loss = 0.12 (60.9 examples/sec; 0.066 sec/batch; 107h:25m:45s remains)
INFO - root - 2019-11-04 01:11:29.759148: step 108540, total loss = 0.47, predict loss = 0.11 (62.5 examples/sec; 0.064 sec/batch; 104h:41m:57s remains)
INFO - root - 2019-11-04 01:11:30.402633: step 108550, total loss = 0.49, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 92h:05m:44s remains)
INFO - root - 2019-11-04 01:11:31.031372: step 108560, total loss = 0.52, predict loss = 0.12 (65.2 examples/sec; 0.061 sec/batch; 100h:22m:24s remains)
INFO - root - 2019-11-04 01:11:31.690822: step 108570, total loss = 0.50, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 91h:25m:36s remains)
INFO - root - 2019-11-04 01:11:32.324332: step 108580, total loss = 0.49, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 94h:30m:25s remains)
INFO - root - 2019-11-04 01:11:32.965244: step 108590, total loss = 0.48, predict loss = 0.11 (76.1 examples/sec; 0.053 sec/batch; 86h:03m:19s remains)
INFO - root - 2019-11-04 01:11:33.613057: step 108600, total loss = 0.50, predict loss = 0.12 (77.9 examples/sec; 0.051 sec/batch; 83h:59m:00s remains)
INFO - root - 2019-11-04 01:11:34.288668: step 108610, total loss = 0.43, predict loss = 0.09 (66.1 examples/sec; 0.061 sec/batch; 99h:05m:01s remains)
INFO - root - 2019-11-04 01:11:34.967404: step 108620, total loss = 0.37, predict loss = 0.09 (70.4 examples/sec; 0.057 sec/batch; 92h:55m:21s remains)
INFO - root - 2019-11-04 01:11:35.622461: step 108630, total loss = 0.52, predict loss = 0.13 (75.8 examples/sec; 0.053 sec/batch; 86h:23m:06s remains)
INFO - root - 2019-11-04 01:11:36.290330: step 108640, total loss = 0.32, predict loss = 0.07 (62.1 examples/sec; 0.064 sec/batch; 105h:26m:25s remains)
INFO - root - 2019-11-04 01:11:36.950454: step 108650, total loss = 0.42, predict loss = 0.09 (71.4 examples/sec; 0.056 sec/batch; 91h:40m:27s remains)
INFO - root - 2019-11-04 01:11:37.633150: step 108660, total loss = 0.46, predict loss = 0.11 (66.2 examples/sec; 0.060 sec/batch; 98h:57m:18s remains)
INFO - root - 2019-11-04 01:11:38.237531: step 108670, total loss = 0.39, predict loss = 0.09 (70.7 examples/sec; 0.057 sec/batch; 92h:38m:17s remains)
INFO - root - 2019-11-04 01:11:38.890266: step 108680, total loss = 0.33, predict loss = 0.07 (63.3 examples/sec; 0.063 sec/batch; 103h:21m:27s remains)
INFO - root - 2019-11-04 01:11:39.535144: step 108690, total loss = 0.46, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 89h:57m:32s remains)
INFO - root - 2019-11-04 01:11:40.233219: step 108700, total loss = 0.53, predict loss = 0.13 (62.0 examples/sec; 0.065 sec/batch; 105h:37m:08s remains)
INFO - root - 2019-11-04 01:11:40.927585: step 108710, total loss = 0.44, predict loss = 0.10 (59.7 examples/sec; 0.067 sec/batch; 109h:37m:35s remains)
INFO - root - 2019-11-04 01:11:41.619938: step 108720, total loss = 0.46, predict loss = 0.11 (58.6 examples/sec; 0.068 sec/batch; 111h:37m:29s remains)
INFO - root - 2019-11-04 01:11:42.263565: step 108730, total loss = 0.57, predict loss = 0.13 (77.1 examples/sec; 0.052 sec/batch; 84h:55m:11s remains)
INFO - root - 2019-11-04 01:11:42.913636: step 108740, total loss = 0.47, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 99h:13m:34s remains)
INFO - root - 2019-11-04 01:11:43.589086: step 108750, total loss = 0.52, predict loss = 0.13 (62.6 examples/sec; 0.064 sec/batch; 104h:36m:00s remains)
INFO - root - 2019-11-04 01:11:44.266465: step 108760, total loss = 0.53, predict loss = 0.13 (69.9 examples/sec; 0.057 sec/batch; 93h:39m:45s remains)
INFO - root - 2019-11-04 01:11:44.961189: step 108770, total loss = 0.53, predict loss = 0.13 (65.7 examples/sec; 0.061 sec/batch; 99h:34m:46s remains)
INFO - root - 2019-11-04 01:11:45.645878: step 108780, total loss = 0.45, predict loss = 0.10 (62.2 examples/sec; 0.064 sec/batch; 105h:09m:32s remains)
INFO - root - 2019-11-04 01:11:46.274278: step 108790, total loss = 0.52, predict loss = 0.12 (71.2 examples/sec; 0.056 sec/batch; 91h:53m:26s remains)
INFO - root - 2019-11-04 01:11:46.900085: step 108800, total loss = 0.60, predict loss = 0.15 (72.6 examples/sec; 0.055 sec/batch; 90h:08m:43s remains)
INFO - root - 2019-11-04 01:11:47.498130: step 108810, total loss = 0.72, predict loss = 0.17 (78.0 examples/sec; 0.051 sec/batch; 83h:53m:29s remains)
INFO - root - 2019-11-04 01:11:48.119734: step 108820, total loss = 0.54, predict loss = 0.13 (66.8 examples/sec; 0.060 sec/batch; 97h:57m:40s remains)
INFO - root - 2019-11-04 01:11:48.795298: step 108830, total loss = 0.46, predict loss = 0.11 (64.0 examples/sec; 0.062 sec/batch; 102h:13m:58s remains)
INFO - root - 2019-11-04 01:11:49.452842: step 108840, total loss = 0.62, predict loss = 0.15 (67.6 examples/sec; 0.059 sec/batch; 96h:46m:50s remains)
INFO - root - 2019-11-04 01:11:50.085336: step 108850, total loss = 0.62, predict loss = 0.14 (69.0 examples/sec; 0.058 sec/batch; 94h:48m:32s remains)
INFO - root - 2019-11-04 01:11:50.704879: step 108860, total loss = 0.52, predict loss = 0.13 (81.1 examples/sec; 0.049 sec/batch; 80h:40m:52s remains)
INFO - root - 2019-11-04 01:11:51.349684: step 108870, total loss = 0.58, predict loss = 0.14 (71.6 examples/sec; 0.056 sec/batch; 91h:26m:39s remains)
INFO - root - 2019-11-04 01:11:51.989942: step 108880, total loss = 0.53, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 90h:13m:14s remains)
INFO - root - 2019-11-04 01:11:52.654857: step 108890, total loss = 0.70, predict loss = 0.17 (66.9 examples/sec; 0.060 sec/batch; 97h:51m:36s remains)
INFO - root - 2019-11-04 01:11:53.322410: step 108900, total loss = 0.57, predict loss = 0.13 (67.0 examples/sec; 0.060 sec/batch; 97h:43m:08s remains)
INFO - root - 2019-11-04 01:11:54.007038: step 108910, total loss = 0.53, predict loss = 0.13 (69.3 examples/sec; 0.058 sec/batch; 94h:26m:11s remains)
INFO - root - 2019-11-04 01:11:54.734468: step 108920, total loss = 0.67, predict loss = 0.16 (59.2 examples/sec; 0.068 sec/batch; 110h:32m:12s remains)
INFO - root - 2019-11-04 01:11:55.502317: step 108930, total loss = 0.46, predict loss = 0.10 (57.7 examples/sec; 0.069 sec/batch; 113h:20m:58s remains)
INFO - root - 2019-11-04 01:11:56.183382: step 108940, total loss = 0.60, predict loss = 0.14 (65.9 examples/sec; 0.061 sec/batch; 99h:17m:34s remains)
INFO - root - 2019-11-04 01:11:56.883741: step 108950, total loss = 0.46, predict loss = 0.10 (64.2 examples/sec; 0.062 sec/batch; 101h:59m:00s remains)
INFO - root - 2019-11-04 01:11:57.561608: step 108960, total loss = 0.34, predict loss = 0.08 (67.4 examples/sec; 0.059 sec/batch; 97h:05m:26s remains)
INFO - root - 2019-11-04 01:11:58.203962: step 108970, total loss = 0.50, predict loss = 0.12 (72.5 examples/sec; 0.055 sec/batch; 90h:16m:24s remains)
INFO - root - 2019-11-04 01:11:58.838799: step 108980, total loss = 0.50, predict loss = 0.12 (71.9 examples/sec; 0.056 sec/batch; 91h:05m:36s remains)
INFO - root - 2019-11-04 01:11:59.498050: step 108990, total loss = 0.48, predict loss = 0.11 (66.8 examples/sec; 0.060 sec/batch; 97h:57m:59s remains)
INFO - root - 2019-11-04 01:12:00.200709: step 109000, total loss = 0.50, predict loss = 0.11 (65.8 examples/sec; 0.061 sec/batch; 99h:24m:22s remains)
INFO - root - 2019-11-04 01:12:00.837918: step 109010, total loss = 0.56, predict loss = 0.13 (71.2 examples/sec; 0.056 sec/batch; 91h:53m:11s remains)
INFO - root - 2019-11-04 01:12:01.464883: step 109020, total loss = 0.52, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 95h:50m:01s remains)
INFO - root - 2019-11-04 01:12:02.114217: step 109030, total loss = 0.54, predict loss = 0.13 (65.0 examples/sec; 0.062 sec/batch; 100h:45m:54s remains)
INFO - root - 2019-11-04 01:12:02.748552: step 109040, total loss = 0.52, predict loss = 0.12 (64.4 examples/sec; 0.062 sec/batch; 101h:34m:07s remains)
INFO - root - 2019-11-04 01:12:03.384406: step 109050, total loss = 0.46, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 93h:52m:25s remains)
INFO - root - 2019-11-04 01:12:04.005495: step 109060, total loss = 0.49, predict loss = 0.11 (76.8 examples/sec; 0.052 sec/batch; 85h:11m:11s remains)
INFO - root - 2019-11-04 01:12:04.607448: step 109070, total loss = 0.39, predict loss = 0.09 (77.1 examples/sec; 0.052 sec/batch; 84h:52m:15s remains)
INFO - root - 2019-11-04 01:12:05.087924: step 109080, total loss = 0.59, predict loss = 0.13 (97.4 examples/sec; 0.041 sec/batch; 67h:12m:59s remains)
INFO - root - 2019-11-04 01:12:05.569118: step 109090, total loss = 0.49, predict loss = 0.11 (94.8 examples/sec; 0.042 sec/batch; 69h:03m:53s remains)
INFO - root - 2019-11-04 01:12:06.633544: step 109100, total loss = 0.32, predict loss = 0.07 (77.7 examples/sec; 0.052 sec/batch; 84h:16m:35s remains)
INFO - root - 2019-11-04 01:12:07.271294: step 109110, total loss = 0.40, predict loss = 0.09 (61.9 examples/sec; 0.065 sec/batch; 105h:48m:40s remains)
INFO - root - 2019-11-04 01:12:07.999087: step 109120, total loss = 0.36, predict loss = 0.08 (61.7 examples/sec; 0.065 sec/batch; 106h:09m:51s remains)
INFO - root - 2019-11-04 01:12:08.724353: step 109130, total loss = 0.60, predict loss = 0.14 (62.0 examples/sec; 0.065 sec/batch; 105h:33m:20s remains)
INFO - root - 2019-11-04 01:12:09.375573: step 109140, total loss = 0.39, predict loss = 0.09 (78.1 examples/sec; 0.051 sec/batch; 83h:47m:49s remains)
INFO - root - 2019-11-04 01:12:10.033998: step 109150, total loss = 0.27, predict loss = 0.05 (71.9 examples/sec; 0.056 sec/batch; 91h:05m:03s remains)
INFO - root - 2019-11-04 01:12:10.704004: step 109160, total loss = 0.76, predict loss = 0.18 (67.9 examples/sec; 0.059 sec/batch; 96h:26m:54s remains)
INFO - root - 2019-11-04 01:12:11.377342: step 109170, total loss = 0.63, predict loss = 0.15 (65.0 examples/sec; 0.062 sec/batch; 100h:44m:22s remains)
INFO - root - 2019-11-04 01:12:12.079885: step 109180, total loss = 0.50, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 99h:14m:41s remains)
INFO - root - 2019-11-04 01:12:12.769073: step 109190, total loss = 0.57, predict loss = 0.13 (59.5 examples/sec; 0.067 sec/batch; 109h:59m:10s remains)
INFO - root - 2019-11-04 01:12:13.450598: step 109200, total loss = 0.58, predict loss = 0.14 (63.6 examples/sec; 0.063 sec/batch; 102h:52m:55s remains)
INFO - root - 2019-11-04 01:12:14.078078: step 109210, total loss = 0.68, predict loss = 0.15 (87.4 examples/sec; 0.046 sec/batch; 74h:53m:19s remains)
INFO - root - 2019-11-04 01:12:14.665986: step 109220, total loss = 0.48, predict loss = 0.10 (81.7 examples/sec; 0.049 sec/batch; 80h:07m:20s remains)
INFO - root - 2019-11-04 01:12:15.259867: step 109230, total loss = 0.60, predict loss = 0.14 (72.3 examples/sec; 0.055 sec/batch; 90h:29m:42s remains)
INFO - root - 2019-11-04 01:12:15.865365: step 109240, total loss = 0.45, predict loss = 0.10 (72.0 examples/sec; 0.056 sec/batch; 90h:54m:55s remains)
INFO - root - 2019-11-04 01:12:16.496280: step 109250, total loss = 0.50, predict loss = 0.11 (74.6 examples/sec; 0.054 sec/batch; 87h:41m:43s remains)
INFO - root - 2019-11-04 01:12:17.172798: step 109260, total loss = 0.37, predict loss = 0.08 (70.7 examples/sec; 0.057 sec/batch; 92h:31m:27s remains)
INFO - root - 2019-11-04 01:12:17.799145: step 109270, total loss = 0.50, predict loss = 0.11 (73.7 examples/sec; 0.054 sec/batch; 88h:51m:18s remains)
INFO - root - 2019-11-04 01:12:18.439722: step 109280, total loss = 0.34, predict loss = 0.07 (70.3 examples/sec; 0.057 sec/batch; 93h:09m:51s remains)
INFO - root - 2019-11-04 01:12:19.095926: step 109290, total loss = 0.47, predict loss = 0.11 (65.6 examples/sec; 0.061 sec/batch; 99h:50m:26s remains)
INFO - root - 2019-11-04 01:12:19.716636: step 109300, total loss = 0.42, predict loss = 0.09 (74.4 examples/sec; 0.054 sec/batch; 87h:55m:08s remains)
INFO - root - 2019-11-04 01:12:20.311512: step 109310, total loss = 0.39, predict loss = 0.09 (73.2 examples/sec; 0.055 sec/batch; 89h:23m:18s remains)
INFO - root - 2019-11-04 01:12:20.946966: step 109320, total loss = 0.48, predict loss = 0.11 (78.3 examples/sec; 0.051 sec/batch; 83h:38m:22s remains)
INFO - root - 2019-11-04 01:12:21.605136: step 109330, total loss = 0.55, predict loss = 0.13 (72.7 examples/sec; 0.055 sec/batch; 90h:03m:15s remains)
INFO - root - 2019-11-04 01:12:22.246400: step 109340, total loss = 0.36, predict loss = 0.08 (69.0 examples/sec; 0.058 sec/batch; 94h:49m:26s remains)
INFO - root - 2019-11-04 01:12:22.860535: step 109350, total loss = 0.50, predict loss = 0.11 (70.3 examples/sec; 0.057 sec/batch; 93h:07m:18s remains)
INFO - root - 2019-11-04 01:12:23.468046: step 109360, total loss = 0.55, predict loss = 0.13 (72.7 examples/sec; 0.055 sec/batch; 90h:01m:40s remains)
INFO - root - 2019-11-04 01:12:24.081878: step 109370, total loss = 0.49, predict loss = 0.11 (86.7 examples/sec; 0.046 sec/batch; 75h:31m:55s remains)
INFO - root - 2019-11-04 01:12:24.698036: step 109380, total loss = 0.55, predict loss = 0.13 (67.6 examples/sec; 0.059 sec/batch; 96h:49m:52s remains)
INFO - root - 2019-11-04 01:12:25.292201: step 109390, total loss = 0.69, predict loss = 0.17 (75.4 examples/sec; 0.053 sec/batch; 86h:48m:52s remains)
INFO - root - 2019-11-04 01:12:25.885080: step 109400, total loss = 0.79, predict loss = 0.20 (83.0 examples/sec; 0.048 sec/batch; 78h:51m:52s remains)
INFO - root - 2019-11-04 01:12:26.497973: step 109410, total loss = 0.42, predict loss = 0.10 (77.0 examples/sec; 0.052 sec/batch; 85h:02m:26s remains)
INFO - root - 2019-11-04 01:12:27.154380: step 109420, total loss = 0.66, predict loss = 0.14 (64.4 examples/sec; 0.062 sec/batch; 101h:37m:45s remains)
INFO - root - 2019-11-04 01:12:27.804869: step 109430, total loss = 0.50, predict loss = 0.11 (69.9 examples/sec; 0.057 sec/batch; 93h:41m:55s remains)
INFO - root - 2019-11-04 01:12:28.467992: step 109440, total loss = 0.63, predict loss = 0.15 (64.9 examples/sec; 0.062 sec/batch; 100h:53m:30s remains)
INFO - root - 2019-11-04 01:12:29.133219: step 109450, total loss = 0.63, predict loss = 0.15 (72.6 examples/sec; 0.055 sec/batch; 90h:08m:02s remains)
INFO - root - 2019-11-04 01:12:29.771068: step 109460, total loss = 0.58, predict loss = 0.14 (68.8 examples/sec; 0.058 sec/batch; 95h:09m:34s remains)
INFO - root - 2019-11-04 01:12:30.364364: step 109470, total loss = 0.47, predict loss = 0.11 (76.5 examples/sec; 0.052 sec/batch; 85h:34m:09s remains)
INFO - root - 2019-11-04 01:12:30.942857: step 109480, total loss = 0.51, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:33m:52s remains)
INFO - root - 2019-11-04 01:12:31.583096: step 109490, total loss = 0.54, predict loss = 0.13 (73.2 examples/sec; 0.055 sec/batch; 89h:28m:13s remains)
INFO - root - 2019-11-04 01:12:32.214171: step 109500, total loss = 0.52, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 95h:47m:39s remains)
INFO - root - 2019-11-04 01:12:32.870981: step 109510, total loss = 0.57, predict loss = 0.14 (65.5 examples/sec; 0.061 sec/batch; 99h:56m:04s remains)
INFO - root - 2019-11-04 01:12:33.527081: step 109520, total loss = 0.47, predict loss = 0.11 (64.4 examples/sec; 0.062 sec/batch; 101h:34m:46s remains)
INFO - root - 2019-11-04 01:12:34.232823: step 109530, total loss = 0.51, predict loss = 0.12 (67.2 examples/sec; 0.059 sec/batch; 97h:20m:30s remains)
INFO - root - 2019-11-04 01:12:34.900457: step 109540, total loss = 0.43, predict loss = 0.10 (75.7 examples/sec; 0.053 sec/batch; 86h:26m:18s remains)
INFO - root - 2019-11-04 01:12:35.490875: step 109550, total loss = 0.52, predict loss = 0.13 (70.4 examples/sec; 0.057 sec/batch; 93h:00m:36s remains)
INFO - root - 2019-11-04 01:12:36.106414: step 109560, total loss = 0.49, predict loss = 0.13 (67.4 examples/sec; 0.059 sec/batch; 97h:08m:48s remains)
INFO - root - 2019-11-04 01:12:36.707811: step 109570, total loss = 0.41, predict loss = 0.10 (72.4 examples/sec; 0.055 sec/batch; 90h:21m:52s remains)
INFO - root - 2019-11-04 01:12:37.337560: step 109580, total loss = 0.45, predict loss = 0.12 (67.7 examples/sec; 0.059 sec/batch; 96h:38m:17s remains)
INFO - root - 2019-11-04 01:12:38.017077: step 109590, total loss = 0.52, predict loss = 0.13 (65.7 examples/sec; 0.061 sec/batch; 99h:39m:58s remains)
INFO - root - 2019-11-04 01:12:38.705475: step 109600, total loss = 0.59, predict loss = 0.15 (60.8 examples/sec; 0.066 sec/batch; 107h:43m:27s remains)
INFO - root - 2019-11-04 01:12:39.315307: step 109610, total loss = 0.74, predict loss = 0.19 (74.2 examples/sec; 0.054 sec/batch; 88h:10m:33s remains)
INFO - root - 2019-11-04 01:12:39.941621: step 109620, total loss = 0.63, predict loss = 0.16 (68.2 examples/sec; 0.059 sec/batch; 95h:56m:08s remains)
INFO - root - 2019-11-04 01:12:40.578285: step 109630, total loss = 0.42, predict loss = 0.10 (71.3 examples/sec; 0.056 sec/batch; 91h:46m:42s remains)
INFO - root - 2019-11-04 01:12:41.224294: step 109640, total loss = 0.43, predict loss = 0.11 (63.4 examples/sec; 0.063 sec/batch; 103h:11m:55s remains)
INFO - root - 2019-11-04 01:12:41.902180: step 109650, total loss = 0.72, predict loss = 0.19 (59.6 examples/sec; 0.067 sec/batch; 109h:46m:22s remains)
INFO - root - 2019-11-04 01:12:42.593721: step 109660, total loss = 0.45, predict loss = 0.10 (66.0 examples/sec; 0.061 sec/batch; 99h:06m:29s remains)
INFO - root - 2019-11-04 01:12:43.261757: step 109670, total loss = 0.31, predict loss = 0.07 (66.4 examples/sec; 0.060 sec/batch; 98h:30m:39s remains)
INFO - root - 2019-11-04 01:12:43.918325: step 109680, total loss = 0.47, predict loss = 0.10 (67.2 examples/sec; 0.060 sec/batch; 97h:27m:03s remains)
INFO - root - 2019-11-04 01:12:44.567205: step 109690, total loss = 0.37, predict loss = 0.09 (65.8 examples/sec; 0.061 sec/batch; 99h:28m:31s remains)
INFO - root - 2019-11-04 01:12:45.293026: step 109700, total loss = 0.40, predict loss = 0.10 (72.4 examples/sec; 0.055 sec/batch; 90h:20m:33s remains)
INFO - root - 2019-11-04 01:12:45.976798: step 109710, total loss = 0.44, predict loss = 0.10 (64.2 examples/sec; 0.062 sec/batch; 101h:57m:22s remains)
INFO - root - 2019-11-04 01:12:46.648768: step 109720, total loss = 0.35, predict loss = 0.08 (63.8 examples/sec; 0.063 sec/batch; 102h:35m:09s remains)
INFO - root - 2019-11-04 01:12:47.303394: step 109730, total loss = 0.34, predict loss = 0.07 (79.6 examples/sec; 0.050 sec/batch; 82h:11m:02s remains)
INFO - root - 2019-11-04 01:12:47.953839: step 109740, total loss = 0.36, predict loss = 0.08 (65.6 examples/sec; 0.061 sec/batch; 99h:49m:46s remains)
INFO - root - 2019-11-04 01:12:48.587968: step 109750, total loss = 0.47, predict loss = 0.12 (71.0 examples/sec; 0.056 sec/batch; 92h:13m:47s remains)
INFO - root - 2019-11-04 01:12:49.193909: step 109760, total loss = 0.45, predict loss = 0.11 (68.8 examples/sec; 0.058 sec/batch; 95h:10m:09s remains)
INFO - root - 2019-11-04 01:12:49.863924: step 109770, total loss = 0.54, predict loss = 0.12 (63.7 examples/sec; 0.063 sec/batch; 102h:48m:31s remains)
INFO - root - 2019-11-04 01:12:50.577316: step 109780, total loss = 0.48, predict loss = 0.11 (63.2 examples/sec; 0.063 sec/batch; 103h:34m:03s remains)
INFO - root - 2019-11-04 01:12:51.278945: step 109790, total loss = 0.45, predict loss = 0.10 (60.9 examples/sec; 0.066 sec/batch; 107h:28m:00s remains)
INFO - root - 2019-11-04 01:12:51.959607: step 109800, total loss = 0.67, predict loss = 0.16 (68.9 examples/sec; 0.058 sec/batch; 95h:01m:50s remains)
INFO - root - 2019-11-04 01:12:52.590964: step 109810, total loss = 0.68, predict loss = 0.17 (83.6 examples/sec; 0.048 sec/batch; 78h:15m:15s remains)
INFO - root - 2019-11-04 01:12:53.233224: step 109820, total loss = 0.51, predict loss = 0.13 (61.7 examples/sec; 0.065 sec/batch; 106h:02m:27s remains)
INFO - root - 2019-11-04 01:12:53.904936: step 109830, total loss = 0.48, predict loss = 0.11 (68.6 examples/sec; 0.058 sec/batch; 95h:21m:24s remains)
INFO - root - 2019-11-04 01:12:54.574337: step 109840, total loss = 0.48, predict loss = 0.12 (67.5 examples/sec; 0.059 sec/batch; 96h:59m:43s remains)
INFO - root - 2019-11-04 01:12:55.204561: step 109850, total loss = 0.34, predict loss = 0.08 (70.1 examples/sec; 0.057 sec/batch; 93h:17m:57s remains)
INFO - root - 2019-11-04 01:12:55.844237: step 109860, total loss = 0.51, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 99h:11m:31s remains)
INFO - root - 2019-11-04 01:12:56.495856: step 109870, total loss = 0.46, predict loss = 0.10 (68.9 examples/sec; 0.058 sec/batch; 94h:55m:23s remains)
INFO - root - 2019-11-04 01:12:57.128346: step 109880, total loss = 0.59, predict loss = 0.15 (62.2 examples/sec; 0.064 sec/batch; 105h:10m:40s remains)
INFO - root - 2019-11-04 01:12:57.793822: step 109890, total loss = 0.52, predict loss = 0.13 (65.5 examples/sec; 0.061 sec/batch; 99h:55m:20s remains)
INFO - root - 2019-11-04 01:12:58.408431: step 109900, total loss = 0.47, predict loss = 0.10 (72.3 examples/sec; 0.055 sec/batch; 90h:34m:26s remains)
INFO - root - 2019-11-04 01:12:59.027855: step 109910, total loss = 0.45, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 93h:50m:21s remains)
INFO - root - 2019-11-04 01:12:59.698195: step 109920, total loss = 0.23, predict loss = 0.04 (66.4 examples/sec; 0.060 sec/batch; 98h:36m:20s remains)
INFO - root - 2019-11-04 01:13:00.383404: step 109930, total loss = 0.35, predict loss = 0.08 (68.4 examples/sec; 0.058 sec/batch; 95h:40m:27s remains)
INFO - root - 2019-11-04 01:13:01.012326: step 109940, total loss = 0.45, predict loss = 0.11 (75.3 examples/sec; 0.053 sec/batch; 86h:55m:21s remains)
INFO - root - 2019-11-04 01:13:01.625495: step 109950, total loss = 0.29, predict loss = 0.06 (69.7 examples/sec; 0.057 sec/batch; 93h:53m:17s remains)
INFO - root - 2019-11-04 01:13:02.244947: step 109960, total loss = 0.36, predict loss = 0.08 (64.5 examples/sec; 0.062 sec/batch; 101h:26m:50s remains)
INFO - root - 2019-11-04 01:13:02.896369: step 109970, total loss = 0.24, predict loss = 0.05 (72.6 examples/sec; 0.055 sec/batch; 90h:10m:03s remains)
INFO - root - 2019-11-04 01:13:03.505803: step 109980, total loss = 0.35, predict loss = 0.07 (67.0 examples/sec; 0.060 sec/batch; 97h:39m:00s remains)
INFO - root - 2019-11-04 01:13:04.158899: step 109990, total loss = 0.37, predict loss = 0.08 (69.7 examples/sec; 0.057 sec/batch; 93h:57m:08s remains)
INFO - root - 2019-11-04 01:13:04.857273: step 110000, total loss = 0.40, predict loss = 0.08 (83.9 examples/sec; 0.048 sec/batch; 77h:58m:22s remains)
INFO - root - 2019-11-04 01:13:05.462729: step 110010, total loss = 0.49, predict loss = 0.11 (69.4 examples/sec; 0.058 sec/batch; 94h:21m:48s remains)
INFO - root - 2019-11-04 01:13:06.094146: step 110020, total loss = 0.39, predict loss = 0.08 (75.5 examples/sec; 0.053 sec/batch; 86h:42m:31s remains)
INFO - root - 2019-11-04 01:13:06.711928: step 110030, total loss = 0.42, predict loss = 0.09 (61.5 examples/sec; 0.065 sec/batch; 106h:21m:12s remains)
INFO - root - 2019-11-04 01:13:07.364100: step 110040, total loss = 0.43, predict loss = 0.10 (62.5 examples/sec; 0.064 sec/batch; 104h:44m:17s remains)
INFO - root - 2019-11-04 01:13:08.099150: step 110050, total loss = 0.44, predict loss = 0.11 (60.2 examples/sec; 0.066 sec/batch; 108h:40m:17s remains)
INFO - root - 2019-11-04 01:13:08.800692: step 110060, total loss = 0.44, predict loss = 0.10 (70.2 examples/sec; 0.057 sec/batch; 93h:11m:37s remains)
INFO - root - 2019-11-04 01:13:09.548322: step 110070, total loss = 0.43, predict loss = 0.10 (58.3 examples/sec; 0.069 sec/batch; 112h:20m:47s remains)
INFO - root - 2019-11-04 01:13:10.200258: step 110080, total loss = 0.40, predict loss = 0.09 (66.1 examples/sec; 0.061 sec/batch; 99h:03m:10s remains)
INFO - root - 2019-11-04 01:13:10.855093: step 110090, total loss = 0.41, predict loss = 0.08 (62.6 examples/sec; 0.064 sec/batch; 104h:36m:18s remains)
INFO - root - 2019-11-04 01:13:11.455483: step 110100, total loss = 0.43, predict loss = 0.10 (67.5 examples/sec; 0.059 sec/batch; 96h:57m:59s remains)
INFO - root - 2019-11-04 01:13:12.088369: step 110110, total loss = 0.46, predict loss = 0.10 (61.9 examples/sec; 0.065 sec/batch; 105h:47m:03s remains)
INFO - root - 2019-11-04 01:13:12.772330: step 110120, total loss = 0.50, predict loss = 0.12 (67.7 examples/sec; 0.059 sec/batch; 96h:38m:06s remains)
INFO - root - 2019-11-04 01:13:13.443637: step 110130, total loss = 0.44, predict loss = 0.11 (68.7 examples/sec; 0.058 sec/batch; 95h:11m:56s remains)
INFO - root - 2019-11-04 01:13:14.118063: step 110140, total loss = 0.32, predict loss = 0.07 (68.6 examples/sec; 0.058 sec/batch; 95h:23m:17s remains)
INFO - root - 2019-11-04 01:13:14.784759: step 110150, total loss = 0.43, predict loss = 0.11 (67.6 examples/sec; 0.059 sec/batch; 96h:49m:06s remains)
INFO - root - 2019-11-04 01:13:15.399412: step 110160, total loss = 0.38, predict loss = 0.09 (81.3 examples/sec; 0.049 sec/batch; 80h:28m:31s remains)
INFO - root - 2019-11-04 01:13:16.050366: step 110170, total loss = 0.34, predict loss = 0.08 (64.8 examples/sec; 0.062 sec/batch; 100h:55m:46s remains)
INFO - root - 2019-11-04 01:13:16.692788: step 110180, total loss = 0.30, predict loss = 0.07 (73.4 examples/sec; 0.054 sec/batch; 89h:05m:54s remains)
INFO - root - 2019-11-04 01:13:17.320225: step 110190, total loss = 0.18, predict loss = 0.04 (65.6 examples/sec; 0.061 sec/batch; 99h:46m:27s remains)
INFO - root - 2019-11-04 01:13:17.953957: step 110200, total loss = 0.29, predict loss = 0.07 (59.8 examples/sec; 0.067 sec/batch; 109h:28m:41s remains)
INFO - root - 2019-11-04 01:13:18.646998: step 110210, total loss = 0.40, predict loss = 0.10 (69.1 examples/sec; 0.058 sec/batch; 94h:43m:44s remains)
INFO - root - 2019-11-04 01:13:19.304023: step 110220, total loss = 0.52, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 93h:07m:15s remains)
INFO - root - 2019-11-04 01:13:19.920689: step 110230, total loss = 0.52, predict loss = 0.13 (73.8 examples/sec; 0.054 sec/batch; 88h:42m:41s remains)
INFO - root - 2019-11-04 01:13:20.561110: step 110240, total loss = 0.43, predict loss = 0.10 (79.0 examples/sec; 0.051 sec/batch; 82h:50m:37s remains)
INFO - root - 2019-11-04 01:13:21.161761: step 110250, total loss = 0.43, predict loss = 0.09 (69.9 examples/sec; 0.057 sec/batch; 93h:35m:46s remains)
INFO - root - 2019-11-04 01:13:21.748599: step 110260, total loss = 0.56, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 94h:32m:11s remains)
INFO - root - 2019-11-04 01:13:22.351750: step 110270, total loss = 0.48, predict loss = 0.12 (75.9 examples/sec; 0.053 sec/batch; 86h:15m:26s remains)
INFO - root - 2019-11-04 01:13:22.943694: step 110280, total loss = 0.39, predict loss = 0.09 (68.6 examples/sec; 0.058 sec/batch; 95h:22m:17s remains)
INFO - root - 2019-11-04 01:13:23.570055: step 110290, total loss = 0.50, predict loss = 0.12 (73.2 examples/sec; 0.055 sec/batch; 89h:21m:20s remains)
INFO - root - 2019-11-04 01:13:24.164784: step 110300, total loss = 0.40, predict loss = 0.09 (76.3 examples/sec; 0.052 sec/batch; 85h:44m:42s remains)
INFO - root - 2019-11-04 01:13:24.802774: step 110310, total loss = 0.30, predict loss = 0.06 (65.1 examples/sec; 0.061 sec/batch; 100h:34m:23s remains)
INFO - root - 2019-11-04 01:13:25.444856: step 110320, total loss = 0.38, predict loss = 0.09 (76.4 examples/sec; 0.052 sec/batch; 85h:36m:55s remains)
INFO - root - 2019-11-04 01:13:26.098657: step 110330, total loss = 0.48, predict loss = 0.11 (65.7 examples/sec; 0.061 sec/batch; 99h:38m:37s remains)
INFO - root - 2019-11-04 01:13:26.720771: step 110340, total loss = 0.39, predict loss = 0.09 (76.4 examples/sec; 0.052 sec/batch; 85h:41m:19s remains)
INFO - root - 2019-11-04 01:13:27.378184: step 110350, total loss = 0.52, predict loss = 0.12 (65.1 examples/sec; 0.061 sec/batch; 100h:29m:55s remains)
INFO - root - 2019-11-04 01:13:28.058308: step 110360, total loss = 0.41, predict loss = 0.09 (64.6 examples/sec; 0.062 sec/batch; 101h:14m:59s remains)
INFO - root - 2019-11-04 01:13:28.693059: step 110370, total loss = 0.46, predict loss = 0.10 (73.6 examples/sec; 0.054 sec/batch; 88h:53m:26s remains)
INFO - root - 2019-11-04 01:13:29.307314: step 110380, total loss = 0.45, predict loss = 0.10 (73.2 examples/sec; 0.055 sec/batch; 89h:20m:36s remains)
INFO - root - 2019-11-04 01:13:29.963520: step 110390, total loss = 0.55, predict loss = 0.13 (74.1 examples/sec; 0.054 sec/batch; 88h:18m:30s remains)
INFO - root - 2019-11-04 01:13:30.611403: step 110400, total loss = 0.53, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 96h:06m:18s remains)
INFO - root - 2019-11-04 01:13:31.218941: step 110410, total loss = 0.20, predict loss = 0.04 (66.5 examples/sec; 0.060 sec/batch; 98h:28m:21s remains)
INFO - root - 2019-11-04 01:13:31.859955: step 110420, total loss = 0.44, predict loss = 0.10 (70.4 examples/sec; 0.057 sec/batch; 92h:57m:24s remains)
INFO - root - 2019-11-04 01:13:32.506026: step 110430, total loss = 0.54, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 99h:18m:05s remains)
INFO - root - 2019-11-04 01:13:33.174628: step 110440, total loss = 0.24, predict loss = 0.05 (66.2 examples/sec; 0.060 sec/batch; 98h:47m:56s remains)
INFO - root - 2019-11-04 01:13:33.813797: step 110450, total loss = 0.23, predict loss = 0.05 (78.3 examples/sec; 0.051 sec/batch; 83h:32m:38s remains)
INFO - root - 2019-11-04 01:13:34.490361: step 110460, total loss = 0.34, predict loss = 0.08 (60.3 examples/sec; 0.066 sec/batch; 108h:27m:02s remains)
INFO - root - 2019-11-04 01:13:35.169100: step 110470, total loss = 0.59, predict loss = 0.14 (81.7 examples/sec; 0.049 sec/batch; 80h:04m:36s remains)
INFO - root - 2019-11-04 01:13:35.789139: step 110480, total loss = 0.47, predict loss = 0.11 (74.2 examples/sec; 0.054 sec/batch; 88h:13m:29s remains)
INFO - root - 2019-11-04 01:13:36.404228: step 110490, total loss = 0.47, predict loss = 0.11 (76.4 examples/sec; 0.052 sec/batch; 85h:38m:20s remains)
INFO - root - 2019-11-04 01:13:37.073524: step 110500, total loss = 0.54, predict loss = 0.13 (63.2 examples/sec; 0.063 sec/batch; 103h:28m:34s remains)
INFO - root - 2019-11-04 01:13:37.725165: step 110510, total loss = 0.59, predict loss = 0.14 (82.1 examples/sec; 0.049 sec/batch; 79h:42m:55s remains)
INFO - root - 2019-11-04 01:13:38.393671: step 110520, total loss = 0.67, predict loss = 0.17 (74.1 examples/sec; 0.054 sec/batch; 88h:20m:18s remains)
INFO - root - 2019-11-04 01:13:39.048147: step 110530, total loss = 0.83, predict loss = 0.20 (66.3 examples/sec; 0.060 sec/batch; 98h:46m:05s remains)
INFO - root - 2019-11-04 01:13:39.697918: step 110540, total loss = 0.67, predict loss = 0.16 (77.7 examples/sec; 0.051 sec/batch; 84h:15m:02s remains)
INFO - root - 2019-11-04 01:13:40.334308: step 110550, total loss = 0.77, predict loss = 0.17 (66.9 examples/sec; 0.060 sec/batch; 97h:47m:30s remains)
INFO - root - 2019-11-04 01:13:40.971460: step 110560, total loss = 0.79, predict loss = 0.18 (67.9 examples/sec; 0.059 sec/batch; 96h:19m:02s remains)
INFO - root - 2019-11-04 01:13:41.612752: step 110570, total loss = 0.85, predict loss = 0.21 (67.3 examples/sec; 0.059 sec/batch; 97h:12m:34s remains)
INFO - root - 2019-11-04 01:13:42.238206: step 110580, total loss = 0.78, predict loss = 0.19 (78.0 examples/sec; 0.051 sec/batch; 83h:51m:03s remains)
INFO - root - 2019-11-04 01:13:42.880210: step 110590, total loss = 0.54, predict loss = 0.13 (70.0 examples/sec; 0.057 sec/batch; 93h:29m:04s remains)
INFO - root - 2019-11-04 01:13:43.501052: step 110600, total loss = 0.65, predict loss = 0.16 (67.5 examples/sec; 0.059 sec/batch; 96h:57m:48s remains)
INFO - root - 2019-11-04 01:13:44.145319: step 110610, total loss = 0.57, predict loss = 0.13 (74.1 examples/sec; 0.054 sec/batch; 88h:21m:26s remains)
INFO - root - 2019-11-04 01:13:44.766749: step 110620, total loss = 0.54, predict loss = 0.12 (72.4 examples/sec; 0.055 sec/batch; 90h:19m:54s remains)
INFO - root - 2019-11-04 01:13:45.397424: step 110630, total loss = 0.47, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 95h:39m:59s remains)
INFO - root - 2019-11-04 01:13:46.025817: step 110640, total loss = 0.49, predict loss = 0.11 (80.1 examples/sec; 0.050 sec/batch; 81h:44m:28s remains)
INFO - root - 2019-11-04 01:13:46.667426: step 110650, total loss = 0.56, predict loss = 0.14 (73.2 examples/sec; 0.055 sec/batch; 89h:26m:49s remains)
INFO - root - 2019-11-04 01:13:47.282926: step 110660, total loss = 0.53, predict loss = 0.12 (79.2 examples/sec; 0.051 sec/batch; 82h:37m:38s remains)
INFO - root - 2019-11-04 01:13:47.915290: step 110670, total loss = 0.48, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 91h:25m:11s remains)
INFO - root - 2019-11-04 01:13:48.545225: step 110680, total loss = 0.46, predict loss = 0.10 (74.7 examples/sec; 0.054 sec/batch; 87h:35m:12s remains)
INFO - root - 2019-11-04 01:13:49.208294: step 110690, total loss = 0.51, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 91h:48m:55s remains)
INFO - root - 2019-11-04 01:13:49.904280: step 110700, total loss = 0.50, predict loss = 0.12 (62.5 examples/sec; 0.064 sec/batch; 104h:38m:35s remains)
INFO - root - 2019-11-04 01:13:50.561885: step 110710, total loss = 0.43, predict loss = 0.10 (71.4 examples/sec; 0.056 sec/batch; 91h:41m:37s remains)
INFO - root - 2019-11-04 01:13:51.225501: step 110720, total loss = 0.52, predict loss = 0.12 (71.5 examples/sec; 0.056 sec/batch; 91h:34m:40s remains)
INFO - root - 2019-11-04 01:13:51.912362: step 110730, total loss = 0.53, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 98h:23m:32s remains)
INFO - root - 2019-11-04 01:13:52.528482: step 110740, total loss = 0.59, predict loss = 0.13 (81.0 examples/sec; 0.049 sec/batch; 80h:50m:05s remains)
INFO - root - 2019-11-04 01:13:53.132712: step 110750, total loss = 0.43, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 92h:37m:24s remains)
INFO - root - 2019-11-04 01:13:53.767403: step 110760, total loss = 0.52, predict loss = 0.12 (63.8 examples/sec; 0.063 sec/batch; 102h:36m:23s remains)
INFO - root - 2019-11-04 01:13:54.400942: step 110770, total loss = 0.46, predict loss = 0.11 (73.1 examples/sec; 0.055 sec/batch; 89h:29m:46s remains)
INFO - root - 2019-11-04 01:13:55.015987: step 110780, total loss = 0.37, predict loss = 0.09 (69.6 examples/sec; 0.057 sec/batch; 93h:58m:56s remains)
INFO - root - 2019-11-04 01:13:55.676851: step 110790, total loss = 0.40, predict loss = 0.10 (65.8 examples/sec; 0.061 sec/batch; 99h:27m:44s remains)
INFO - root - 2019-11-04 01:13:56.330928: step 110800, total loss = 0.42, predict loss = 0.10 (74.2 examples/sec; 0.054 sec/batch; 88h:14m:19s remains)
INFO - root - 2019-11-04 01:13:57.046989: step 110810, total loss = 0.41, predict loss = 0.09 (61.7 examples/sec; 0.065 sec/batch; 105h:58m:06s remains)
INFO - root - 2019-11-04 01:13:57.712991: step 110820, total loss = 0.35, predict loss = 0.08 (78.2 examples/sec; 0.051 sec/batch; 83h:40m:46s remains)
INFO - root - 2019-11-04 01:13:58.331752: step 110830, total loss = 0.43, predict loss = 0.10 (67.6 examples/sec; 0.059 sec/batch; 96h:47m:47s remains)
INFO - root - 2019-11-04 01:13:58.960357: step 110840, total loss = 0.47, predict loss = 0.10 (74.6 examples/sec; 0.054 sec/batch; 87h:39m:56s remains)
INFO - root - 2019-11-04 01:13:59.598447: step 110850, total loss = 0.42, predict loss = 0.10 (74.5 examples/sec; 0.054 sec/batch; 87h:49m:36s remains)
INFO - root - 2019-11-04 01:14:00.238387: step 110860, total loss = 0.36, predict loss = 0.07 (67.9 examples/sec; 0.059 sec/batch; 96h:18m:52s remains)
INFO - root - 2019-11-04 01:14:00.873292: step 110870, total loss = 0.45, predict loss = 0.10 (72.1 examples/sec; 0.055 sec/batch; 90h:47m:18s remains)
INFO - root - 2019-11-04 01:14:01.503303: step 110880, total loss = 0.42, predict loss = 0.09 (75.6 examples/sec; 0.053 sec/batch; 86h:30m:46s remains)
INFO - root - 2019-11-04 01:14:02.153891: step 110890, total loss = 0.47, predict loss = 0.11 (75.4 examples/sec; 0.053 sec/batch; 86h:44m:17s remains)
INFO - root - 2019-11-04 01:14:02.799829: step 110900, total loss = 0.49, predict loss = 0.10 (70.5 examples/sec; 0.057 sec/batch; 92h:46m:45s remains)
INFO - root - 2019-11-04 01:14:03.414745: step 110910, total loss = 0.50, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 97h:03m:25s remains)
INFO - root - 2019-11-04 01:14:04.070531: step 110920, total loss = 0.51, predict loss = 0.12 (63.6 examples/sec; 0.063 sec/batch; 102h:51m:27s remains)
INFO - root - 2019-11-04 01:14:04.729134: step 110930, total loss = 0.61, predict loss = 0.14 (67.4 examples/sec; 0.059 sec/batch; 97h:03m:26s remains)
INFO - root - 2019-11-04 01:14:05.354404: step 110940, total loss = 0.57, predict loss = 0.14 (66.5 examples/sec; 0.060 sec/batch; 98h:22m:05s remains)
INFO - root - 2019-11-04 01:14:05.990837: step 110950, total loss = 0.57, predict loss = 0.14 (67.0 examples/sec; 0.060 sec/batch; 97h:40m:21s remains)
INFO - root - 2019-11-04 01:14:06.643191: step 110960, total loss = 0.49, predict loss = 0.10 (73.5 examples/sec; 0.054 sec/batch; 88h:59m:40s remains)
INFO - root - 2019-11-04 01:14:07.344127: step 110970, total loss = 0.56, predict loss = 0.13 (71.2 examples/sec; 0.056 sec/batch; 91h:55m:46s remains)
INFO - root - 2019-11-04 01:14:08.040620: step 110980, total loss = 0.73, predict loss = 0.17 (55.2 examples/sec; 0.072 sec/batch; 118h:30m:52s remains)
INFO - root - 2019-11-04 01:14:08.709510: step 110990, total loss = 0.78, predict loss = 0.19 (68.3 examples/sec; 0.059 sec/batch; 95h:47m:42s remains)
INFO - root - 2019-11-04 01:14:09.350402: step 111000, total loss = 0.59, predict loss = 0.14 (67.9 examples/sec; 0.059 sec/batch; 96h:23m:03s remains)
INFO - root - 2019-11-04 01:14:10.007668: step 111010, total loss = 0.53, predict loss = 0.12 (69.8 examples/sec; 0.057 sec/batch; 93h:42m:56s remains)
INFO - root - 2019-11-04 01:14:10.614763: step 111020, total loss = 0.53, predict loss = 0.12 (65.7 examples/sec; 0.061 sec/batch; 99h:37m:13s remains)
INFO - root - 2019-11-04 01:14:11.262167: step 111030, total loss = 0.58, predict loss = 0.14 (68.8 examples/sec; 0.058 sec/batch; 95h:02m:48s remains)
INFO - root - 2019-11-04 01:14:11.923607: step 111040, total loss = 0.47, predict loss = 0.11 (65.2 examples/sec; 0.061 sec/batch; 100h:24m:30s remains)
INFO - root - 2019-11-04 01:14:12.568430: step 111050, total loss = 0.46, predict loss = 0.11 (78.5 examples/sec; 0.051 sec/batch; 83h:21m:06s remains)
INFO - root - 2019-11-04 01:14:13.245323: step 111060, total loss = 0.36, predict loss = 0.09 (63.5 examples/sec; 0.063 sec/batch; 103h:05m:40s remains)
INFO - root - 2019-11-04 01:14:13.922692: step 111070, total loss = 0.43, predict loss = 0.10 (62.5 examples/sec; 0.064 sec/batch; 104h:46m:28s remains)
INFO - root - 2019-11-04 01:14:14.602047: step 111080, total loss = 0.34, predict loss = 0.07 (67.8 examples/sec; 0.059 sec/batch; 96h:31m:48s remains)
INFO - root - 2019-11-04 01:14:15.323703: step 111090, total loss = 0.32, predict loss = 0.07 (67.3 examples/sec; 0.059 sec/batch; 97h:15m:10s remains)
INFO - root - 2019-11-04 01:14:16.013036: step 111100, total loss = 0.37, predict loss = 0.08 (59.8 examples/sec; 0.067 sec/batch; 109h:27m:42s remains)
INFO - root - 2019-11-04 01:14:16.703527: step 111110, total loss = 0.35, predict loss = 0.08 (66.5 examples/sec; 0.060 sec/batch; 98h:20m:30s remains)
INFO - root - 2019-11-04 01:14:17.408187: step 111120, total loss = 0.53, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 101h:47m:35s remains)
INFO - root - 2019-11-04 01:14:18.084638: step 111130, total loss = 0.40, predict loss = 0.09 (66.1 examples/sec; 0.060 sec/batch; 98h:56m:48s remains)
INFO - root - 2019-11-04 01:14:18.693094: step 111140, total loss = 0.42, predict loss = 0.09 (68.4 examples/sec; 0.058 sec/batch; 95h:39m:22s remains)
INFO - root - 2019-11-04 01:14:19.330783: step 111150, total loss = 0.56, predict loss = 0.13 (76.7 examples/sec; 0.052 sec/batch; 85h:20m:28s remains)
INFO - root - 2019-11-04 01:14:19.962537: step 111160, total loss = 0.69, predict loss = 0.16 (82.9 examples/sec; 0.048 sec/batch; 78h:53m:29s remains)
INFO - root - 2019-11-04 01:14:20.566184: step 111170, total loss = 0.56, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 86h:09m:20s remains)
INFO - root - 2019-11-04 01:14:21.213983: step 111180, total loss = 0.46, predict loss = 0.10 (76.7 examples/sec; 0.052 sec/batch; 85h:19m:54s remains)
INFO - root - 2019-11-04 01:14:21.854856: step 111190, total loss = 0.61, predict loss = 0.15 (71.9 examples/sec; 0.056 sec/batch; 91h:02m:59s remains)
INFO - root - 2019-11-04 01:14:22.504594: step 111200, total loss = 0.58, predict loss = 0.14 (66.2 examples/sec; 0.060 sec/batch; 98h:54m:01s remains)
INFO - root - 2019-11-04 01:14:23.132068: step 111210, total loss = 0.55, predict loss = 0.14 (66.9 examples/sec; 0.060 sec/batch; 97h:52m:35s remains)
INFO - root - 2019-11-04 01:14:23.781267: step 111220, total loss = 0.63, predict loss = 0.16 (66.6 examples/sec; 0.060 sec/batch; 98h:17m:10s remains)
INFO - root - 2019-11-04 01:14:24.435860: step 111230, total loss = 0.54, predict loss = 0.13 (78.9 examples/sec; 0.051 sec/batch; 82h:57m:13s remains)
INFO - root - 2019-11-04 01:14:25.089508: step 111240, total loss = 0.50, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 101h:41m:03s remains)
INFO - root - 2019-11-04 01:14:25.729019: step 111250, total loss = 0.60, predict loss = 0.14 (69.1 examples/sec; 0.058 sec/batch; 94h:43m:31s remains)
INFO - root - 2019-11-04 01:14:26.403558: step 111260, total loss = 0.55, predict loss = 0.13 (65.9 examples/sec; 0.061 sec/batch; 99h:20m:11s remains)
INFO - root - 2019-11-04 01:14:27.037410: step 111270, total loss = 0.46, predict loss = 0.11 (72.1 examples/sec; 0.055 sec/batch; 90h:42m:58s remains)
INFO - root - 2019-11-04 01:14:27.672086: step 111280, total loss = 0.46, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 97h:07m:11s remains)
INFO - root - 2019-11-04 01:14:28.306875: step 111290, total loss = 0.54, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 97h:16m:58s remains)
INFO - root - 2019-11-04 01:14:28.949129: step 111300, total loss = 0.53, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 96h:31m:21s remains)
INFO - root - 2019-11-04 01:14:29.576641: step 111310, total loss = 0.55, predict loss = 0.13 (72.7 examples/sec; 0.055 sec/batch; 90h:01m:23s remains)
INFO - root - 2019-11-04 01:14:30.218795: step 111320, total loss = 0.49, predict loss = 0.12 (77.7 examples/sec; 0.051 sec/batch; 84h:09m:46s remains)
INFO - root - 2019-11-04 01:14:30.858356: step 111330, total loss = 0.46, predict loss = 0.10 (64.4 examples/sec; 0.062 sec/batch; 101h:33m:04s remains)
INFO - root - 2019-11-04 01:14:31.555663: step 111340, total loss = 0.47, predict loss = 0.11 (61.3 examples/sec; 0.065 sec/batch; 106h:44m:18s remains)
INFO - root - 2019-11-04 01:14:32.254641: step 111350, total loss = 0.49, predict loss = 0.12 (61.1 examples/sec; 0.065 sec/batch; 107h:04m:54s remains)
INFO - root - 2019-11-04 01:14:32.923378: step 111360, total loss = 0.41, predict loss = 0.09 (65.3 examples/sec; 0.061 sec/batch; 100h:14m:04s remains)
INFO - root - 2019-11-04 01:14:33.599306: step 111370, total loss = 0.40, predict loss = 0.09 (67.8 examples/sec; 0.059 sec/batch; 96h:31m:01s remains)
INFO - root - 2019-11-04 01:14:34.247072: step 111380, total loss = 0.36, predict loss = 0.08 (67.9 examples/sec; 0.059 sec/batch; 96h:21m:21s remains)
INFO - root - 2019-11-04 01:14:34.933937: step 111390, total loss = 0.31, predict loss = 0.06 (68.5 examples/sec; 0.058 sec/batch; 95h:29m:27s remains)
INFO - root - 2019-11-04 01:14:35.577548: step 111400, total loss = 0.50, predict loss = 0.11 (63.3 examples/sec; 0.063 sec/batch; 103h:22m:45s remains)
INFO - root - 2019-11-04 01:14:36.207115: step 111410, total loss = 0.37, predict loss = 0.08 (65.6 examples/sec; 0.061 sec/batch; 99h:42m:48s remains)
INFO - root - 2019-11-04 01:14:36.823141: step 111420, total loss = 0.43, predict loss = 0.10 (71.6 examples/sec; 0.056 sec/batch; 91h:21m:55s remains)
INFO - root - 2019-11-04 01:14:37.458272: step 111430, total loss = 0.43, predict loss = 0.09 (64.2 examples/sec; 0.062 sec/batch; 101h:52m:33s remains)
INFO - root - 2019-11-04 01:14:38.116393: step 111440, total loss = 0.51, predict loss = 0.11 (74.5 examples/sec; 0.054 sec/batch; 87h:47m:20s remains)
INFO - root - 2019-11-04 01:14:38.737205: step 111450, total loss = 0.51, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 92h:43m:50s remains)
INFO - root - 2019-11-04 01:14:39.353206: step 111460, total loss = 0.54, predict loss = 0.13 (65.9 examples/sec; 0.061 sec/batch; 99h:18m:22s remains)
INFO - root - 2019-11-04 01:14:40.023276: step 111470, total loss = 0.48, predict loss = 0.11 (65.5 examples/sec; 0.061 sec/batch; 99h:53m:46s remains)
INFO - root - 2019-11-04 01:14:40.697042: step 111480, total loss = 0.40, predict loss = 0.10 (66.1 examples/sec; 0.061 sec/batch; 99h:01m:44s remains)
INFO - root - 2019-11-04 01:14:41.348895: step 111490, total loss = 0.43, predict loss = 0.10 (72.2 examples/sec; 0.055 sec/batch; 90h:37m:19s remains)
INFO - root - 2019-11-04 01:14:42.016879: step 111500, total loss = 0.52, predict loss = 0.12 (59.9 examples/sec; 0.067 sec/batch; 109h:08m:59s remains)
INFO - root - 2019-11-04 01:14:42.707002: step 111510, total loss = 0.50, predict loss = 0.13 (64.7 examples/sec; 0.062 sec/batch; 101h:04m:58s remains)
INFO - root - 2019-11-04 01:14:43.302517: step 111520, total loss = 0.42, predict loss = 0.10 (79.3 examples/sec; 0.050 sec/batch; 82h:29m:24s remains)
INFO - root - 2019-11-04 01:14:43.902487: step 111530, total loss = 0.35, predict loss = 0.08 (78.8 examples/sec; 0.051 sec/batch; 83h:04m:10s remains)
INFO - root - 2019-11-04 01:14:44.505510: step 111540, total loss = 0.66, predict loss = 0.16 (78.8 examples/sec; 0.051 sec/batch; 83h:00m:28s remains)
INFO - root - 2019-11-04 01:14:45.135858: step 111550, total loss = 0.47, predict loss = 0.11 (75.3 examples/sec; 0.053 sec/batch; 86h:54m:31s remains)
INFO - root - 2019-11-04 01:14:45.753216: step 111560, total loss = 0.49, predict loss = 0.12 (75.4 examples/sec; 0.053 sec/batch; 86h:45m:06s remains)
INFO - root - 2019-11-04 01:14:46.377413: step 111570, total loss = 0.52, predict loss = 0.12 (64.5 examples/sec; 0.062 sec/batch; 101h:21m:41s remains)
INFO - root - 2019-11-04 01:14:47.035052: step 111580, total loss = 0.68, predict loss = 0.16 (76.5 examples/sec; 0.052 sec/batch; 85h:29m:44s remains)
INFO - root - 2019-11-04 01:14:47.650833: step 111590, total loss = 0.56, predict loss = 0.13 (75.1 examples/sec; 0.053 sec/batch; 87h:06m:17s remains)
INFO - root - 2019-11-04 01:14:48.278758: step 111600, total loss = 0.53, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 94h:16m:32s remains)
INFO - root - 2019-11-04 01:14:48.892112: step 111610, total loss = 0.54, predict loss = 0.12 (74.6 examples/sec; 0.054 sec/batch; 87h:45m:19s remains)
INFO - root - 2019-11-04 01:14:49.524672: step 111620, total loss = 0.77, predict loss = 0.18 (70.7 examples/sec; 0.057 sec/batch; 92h:34m:59s remains)
INFO - root - 2019-11-04 01:14:50.181754: step 111630, total loss = 0.59, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 93h:02m:27s remains)
INFO - root - 2019-11-04 01:14:50.851856: step 111640, total loss = 0.54, predict loss = 0.14 (56.3 examples/sec; 0.071 sec/batch; 116h:12m:16s remains)
INFO - root - 2019-11-04 01:14:51.460738: step 111650, total loss = 0.63, predict loss = 0.15 (69.6 examples/sec; 0.057 sec/batch; 94h:01m:00s remains)
INFO - root - 2019-11-04 01:14:52.096114: step 111660, total loss = 0.61, predict loss = 0.15 (64.5 examples/sec; 0.062 sec/batch; 101h:27m:25s remains)
INFO - root - 2019-11-04 01:14:52.724069: step 111670, total loss = 0.61, predict loss = 0.15 (74.2 examples/sec; 0.054 sec/batch; 88h:11m:10s remains)
INFO - root - 2019-11-04 01:14:53.364741: step 111680, total loss = 0.33, predict loss = 0.07 (66.7 examples/sec; 0.060 sec/batch; 98h:07m:39s remains)
INFO - root - 2019-11-04 01:14:54.008125: step 111690, total loss = 0.54, predict loss = 0.13 (71.5 examples/sec; 0.056 sec/batch; 91h:32m:00s remains)
INFO - root - 2019-11-04 01:14:54.657809: step 111700, total loss = 0.45, predict loss = 0.10 (69.2 examples/sec; 0.058 sec/batch; 94h:32m:19s remains)
INFO - root - 2019-11-04 01:14:55.314761: step 111710, total loss = 0.39, predict loss = 0.09 (64.6 examples/sec; 0.062 sec/batch; 101h:19m:43s remains)
INFO - root - 2019-11-04 01:14:55.967886: step 111720, total loss = 0.50, predict loss = 0.12 (65.1 examples/sec; 0.061 sec/batch; 100h:30m:16s remains)
INFO - root - 2019-11-04 01:14:56.589749: step 111730, total loss = 0.52, predict loss = 0.12 (77.9 examples/sec; 0.051 sec/batch; 83h:56m:37s remains)
INFO - root - 2019-11-04 01:14:57.717352: step 111740, total loss = 0.62, predict loss = 0.16 (68.3 examples/sec; 0.059 sec/batch; 95h:51m:25s remains)
INFO - root - 2019-11-04 01:14:58.346298: step 111750, total loss = 0.55, predict loss = 0.12 (63.3 examples/sec; 0.063 sec/batch; 103h:17m:14s remains)
INFO - root - 2019-11-04 01:14:58.999240: step 111760, total loss = 0.62, predict loss = 0.14 (62.6 examples/sec; 0.064 sec/batch; 104h:32m:36s remains)
INFO - root - 2019-11-04 01:14:59.666838: step 111770, total loss = 0.54, predict loss = 0.13 (70.8 examples/sec; 0.056 sec/batch; 92h:24m:40s remains)
INFO - root - 2019-11-04 01:15:00.328955: step 111780, total loss = 0.57, predict loss = 0.13 (69.5 examples/sec; 0.058 sec/batch; 94h:06m:43s remains)
INFO - root - 2019-11-04 01:15:00.943520: step 111790, total loss = 0.52, predict loss = 0.12 (83.2 examples/sec; 0.048 sec/batch; 78h:37m:20s remains)
INFO - root - 2019-11-04 01:15:01.525931: step 111800, total loss = 0.50, predict loss = 0.11 (91.0 examples/sec; 0.044 sec/batch; 71h:55m:53s remains)
INFO - root - 2019-11-04 01:15:01.991539: step 111810, total loss = 0.54, predict loss = 0.12 (96.1 examples/sec; 0.042 sec/batch; 68h:05m:53s remains)
INFO - root - 2019-11-04 01:15:02.458925: step 111820, total loss = 0.55, predict loss = 0.13 (98.3 examples/sec; 0.041 sec/batch; 66h:32m:05s remains)
INFO - root - 2019-11-04 01:15:03.594417: step 111830, total loss = 0.34, predict loss = 0.07 (69.7 examples/sec; 0.057 sec/batch; 93h:54m:02s remains)
INFO - root - 2019-11-04 01:15:04.244179: step 111840, total loss = 0.46, predict loss = 0.10 (65.8 examples/sec; 0.061 sec/batch; 99h:24m:49s remains)
INFO - root - 2019-11-04 01:15:04.920362: step 111850, total loss = 0.38, predict loss = 0.09 (80.0 examples/sec; 0.050 sec/batch; 81h:47m:25s remains)
INFO - root - 2019-11-04 01:15:05.509226: step 111860, total loss = 0.52, predict loss = 0.12 (75.3 examples/sec; 0.053 sec/batch; 86h:52m:05s remains)
INFO - root - 2019-11-04 01:15:06.082439: step 111870, total loss = 0.60, predict loss = 0.14 (73.4 examples/sec; 0.054 sec/batch; 89h:04m:23s remains)
INFO - root - 2019-11-04 01:15:06.724254: step 111880, total loss = 0.27, predict loss = 0.06 (65.5 examples/sec; 0.061 sec/batch; 99h:53m:29s remains)
INFO - root - 2019-11-04 01:15:07.371193: step 111890, total loss = 0.53, predict loss = 0.13 (67.1 examples/sec; 0.060 sec/batch; 97h:27m:02s remains)
INFO - root - 2019-11-04 01:15:07.984476: step 111900, total loss = 0.58, predict loss = 0.13 (85.2 examples/sec; 0.047 sec/batch; 76h:46m:57s remains)
INFO - root - 2019-11-04 01:15:08.589167: step 111910, total loss = 0.61, predict loss = 0.14 (67.1 examples/sec; 0.060 sec/batch; 97h:27m:22s remains)
INFO - root - 2019-11-04 01:15:09.214231: step 111920, total loss = 0.65, predict loss = 0.15 (65.1 examples/sec; 0.061 sec/batch; 100h:29m:51s remains)
INFO - root - 2019-11-04 01:15:09.881886: step 111930, total loss = 0.59, predict loss = 0.13 (67.5 examples/sec; 0.059 sec/batch; 96h:58m:09s remains)
INFO - root - 2019-11-04 01:15:10.540770: step 111940, total loss = 0.49, predict loss = 0.11 (74.6 examples/sec; 0.054 sec/batch; 87h:41m:33s remains)
INFO - root - 2019-11-04 01:15:11.190858: step 111950, total loss = 0.55, predict loss = 0.12 (74.2 examples/sec; 0.054 sec/batch; 88h:11m:44s remains)
INFO - root - 2019-11-04 01:15:11.807841: step 111960, total loss = 0.32, predict loss = 0.07 (71.8 examples/sec; 0.056 sec/batch; 91h:06m:47s remains)
INFO - root - 2019-11-04 01:15:12.443866: step 111970, total loss = 0.39, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 94h:07m:26s remains)
INFO - root - 2019-11-04 01:15:13.068888: step 111980, total loss = 0.48, predict loss = 0.11 (82.8 examples/sec; 0.048 sec/batch; 78h:59m:26s remains)
INFO - root - 2019-11-04 01:15:13.696062: step 111990, total loss = 0.45, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 93h:51m:55s remains)
INFO - root - 2019-11-04 01:15:14.350364: step 112000, total loss = 0.51, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 90h:08m:15s remains)
INFO - root - 2019-11-04 01:15:14.971738: step 112010, total loss = 0.51, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 92h:37m:02s remains)
INFO - root - 2019-11-04 01:15:15.593637: step 112020, total loss = 0.56, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 93h:58m:29s remains)
INFO - root - 2019-11-04 01:15:16.217620: step 112030, total loss = 0.49, predict loss = 0.11 (74.6 examples/sec; 0.054 sec/batch; 87h:44m:43s remains)
INFO - root - 2019-11-04 01:15:16.830466: step 112040, total loss = 0.48, predict loss = 0.11 (71.9 examples/sec; 0.056 sec/batch; 90h:58m:29s remains)
INFO - root - 2019-11-04 01:15:17.432301: step 112050, total loss = 0.50, predict loss = 0.11 (74.7 examples/sec; 0.054 sec/batch; 87h:34m:55s remains)
INFO - root - 2019-11-04 01:15:18.051349: step 112060, total loss = 0.60, predict loss = 0.14 (68.1 examples/sec; 0.059 sec/batch; 96h:04m:49s remains)
INFO - root - 2019-11-04 01:15:18.731107: step 112070, total loss = 0.58, predict loss = 0.14 (66.8 examples/sec; 0.060 sec/batch; 97h:51m:59s remains)
INFO - root - 2019-11-04 01:15:19.403206: step 112080, total loss = 0.57, predict loss = 0.14 (65.5 examples/sec; 0.061 sec/batch; 99h:54m:31s remains)
INFO - root - 2019-11-04 01:15:20.017381: step 112090, total loss = 0.57, predict loss = 0.13 (78.5 examples/sec; 0.051 sec/batch; 83h:21m:27s remains)
INFO - root - 2019-11-04 01:15:20.694203: step 112100, total loss = 0.45, predict loss = 0.10 (65.8 examples/sec; 0.061 sec/batch; 99h:22m:10s remains)
INFO - root - 2019-11-04 01:15:21.367747: step 112110, total loss = 0.57, predict loss = 0.14 (61.4 examples/sec; 0.065 sec/batch; 106h:31m:49s remains)
INFO - root - 2019-11-04 01:15:22.017126: step 112120, total loss = 0.56, predict loss = 0.13 (66.0 examples/sec; 0.061 sec/batch; 99h:06m:59s remains)
INFO - root - 2019-11-04 01:15:22.656084: step 112130, total loss = 0.74, predict loss = 0.17 (77.4 examples/sec; 0.052 sec/batch; 84h:30m:57s remains)
INFO - root - 2019-11-04 01:15:23.343077: step 112140, total loss = 0.49, predict loss = 0.11 (65.5 examples/sec; 0.061 sec/batch; 99h:55m:46s remains)
INFO - root - 2019-11-04 01:15:23.969347: step 112150, total loss = 0.60, predict loss = 0.14 (79.5 examples/sec; 0.050 sec/batch; 82h:16m:37s remains)
INFO - root - 2019-11-04 01:15:24.624061: step 112160, total loss = 0.55, predict loss = 0.13 (64.0 examples/sec; 0.062 sec/batch; 102h:12m:05s remains)
INFO - root - 2019-11-04 01:15:25.289885: step 112170, total loss = 0.66, predict loss = 0.15 (73.4 examples/sec; 0.054 sec/batch; 89h:05m:32s remains)
INFO - root - 2019-11-04 01:15:25.919933: step 112180, total loss = 0.58, predict loss = 0.14 (66.8 examples/sec; 0.060 sec/batch; 97h:57m:53s remains)
INFO - root - 2019-11-04 01:15:26.632851: step 112190, total loss = 0.57, predict loss = 0.14 (61.2 examples/sec; 0.065 sec/batch; 106h:54m:18s remains)
INFO - root - 2019-11-04 01:15:27.283067: step 112200, total loss = 0.42, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 92h:40m:47s remains)
INFO - root - 2019-11-04 01:15:27.941635: step 112210, total loss = 0.54, predict loss = 0.13 (74.1 examples/sec; 0.054 sec/batch; 88h:18m:55s remains)
INFO - root - 2019-11-04 01:15:28.563345: step 112220, total loss = 0.37, predict loss = 0.07 (74.4 examples/sec; 0.054 sec/batch; 87h:53m:08s remains)
INFO - root - 2019-11-04 01:15:29.213784: step 112230, total loss = 0.43, predict loss = 0.10 (74.7 examples/sec; 0.054 sec/batch; 87h:36m:05s remains)
INFO - root - 2019-11-04 01:15:29.893752: step 112240, total loss = 0.48, predict loss = 0.11 (64.3 examples/sec; 0.062 sec/batch; 101h:42m:55s remains)
INFO - root - 2019-11-04 01:15:30.536948: step 112250, total loss = 0.49, predict loss = 0.12 (62.2 examples/sec; 0.064 sec/batch; 105h:12m:42s remains)
INFO - root - 2019-11-04 01:15:31.175195: step 112260, total loss = 0.54, predict loss = 0.14 (72.8 examples/sec; 0.055 sec/batch; 89h:51m:19s remains)
INFO - root - 2019-11-04 01:15:31.803476: step 112270, total loss = 0.56, predict loss = 0.13 (68.5 examples/sec; 0.058 sec/batch; 95h:32m:38s remains)
INFO - root - 2019-11-04 01:15:32.415747: step 112280, total loss = 0.61, predict loss = 0.13 (73.4 examples/sec; 0.054 sec/batch; 89h:04m:44s remains)
INFO - root - 2019-11-04 01:15:33.030600: step 112290, total loss = 0.60, predict loss = 0.15 (75.5 examples/sec; 0.053 sec/batch; 86h:38m:22s remains)
INFO - root - 2019-11-04 01:15:33.661864: step 112300, total loss = 0.41, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 95h:17m:53s remains)
INFO - root - 2019-11-04 01:15:34.275060: step 112310, total loss = 0.38, predict loss = 0.09 (68.7 examples/sec; 0.058 sec/batch; 95h:10m:31s remains)
INFO - root - 2019-11-04 01:15:34.936339: step 112320, total loss = 0.52, predict loss = 0.13 (75.1 examples/sec; 0.053 sec/batch; 87h:08m:14s remains)
INFO - root - 2019-11-04 01:15:35.533279: step 112330, total loss = 0.52, predict loss = 0.13 (81.8 examples/sec; 0.049 sec/batch; 79h:55m:40s remains)
INFO - root - 2019-11-04 01:15:36.142672: step 112340, total loss = 0.47, predict loss = 0.11 (73.2 examples/sec; 0.055 sec/batch; 89h:25m:08s remains)
INFO - root - 2019-11-04 01:15:36.778857: step 112350, total loss = 0.56, predict loss = 0.14 (64.5 examples/sec; 0.062 sec/batch; 101h:21m:04s remains)
INFO - root - 2019-11-04 01:15:37.445474: step 112360, total loss = 0.51, predict loss = 0.12 (63.1 examples/sec; 0.063 sec/batch; 103h:41m:16s remains)
INFO - root - 2019-11-04 01:15:38.156304: step 112370, total loss = 0.48, predict loss = 0.12 (58.2 examples/sec; 0.069 sec/batch; 112h:20m:10s remains)
INFO - root - 2019-11-04 01:15:38.793223: step 112380, total loss = 0.32, predict loss = 0.08 (66.5 examples/sec; 0.060 sec/batch; 98h:26m:11s remains)
INFO - root - 2019-11-04 01:15:39.432926: step 112390, total loss = 0.48, predict loss = 0.12 (68.0 examples/sec; 0.059 sec/batch; 96h:13m:19s remains)
INFO - root - 2019-11-04 01:15:40.065202: step 112400, total loss = 0.40, predict loss = 0.09 (72.5 examples/sec; 0.055 sec/batch; 90h:12m:12s remains)
INFO - root - 2019-11-04 01:15:40.683528: step 112410, total loss = 0.29, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 82h:53m:55s remains)
INFO - root - 2019-11-04 01:15:41.291333: step 112420, total loss = 0.26, predict loss = 0.06 (75.0 examples/sec; 0.053 sec/batch; 87h:12m:25s remains)
INFO - root - 2019-11-04 01:15:41.882958: step 112430, total loss = 0.40, predict loss = 0.09 (78.1 examples/sec; 0.051 sec/batch; 83h:48m:08s remains)
INFO - root - 2019-11-04 01:15:42.493953: step 112440, total loss = 0.35, predict loss = 0.08 (71.1 examples/sec; 0.056 sec/batch; 92h:01m:30s remains)
INFO - root - 2019-11-04 01:15:43.130192: step 112450, total loss = 0.56, predict loss = 0.12 (85.0 examples/sec; 0.047 sec/batch; 76h:56m:15s remains)
INFO - root - 2019-11-04 01:15:43.795228: step 112460, total loss = 0.34, predict loss = 0.07 (70.0 examples/sec; 0.057 sec/batch; 93h:27m:18s remains)
INFO - root - 2019-11-04 01:15:44.493871: step 112470, total loss = 0.40, predict loss = 0.10 (69.5 examples/sec; 0.058 sec/batch; 94h:07m:28s remains)
INFO - root - 2019-11-04 01:15:45.211070: step 112480, total loss = 0.29, predict loss = 0.06 (60.2 examples/sec; 0.066 sec/batch; 108h:43m:03s remains)
INFO - root - 2019-11-04 01:15:45.869466: step 112490, total loss = 0.49, predict loss = 0.12 (66.6 examples/sec; 0.060 sec/batch; 98h:13m:53s remains)
INFO - root - 2019-11-04 01:15:46.494726: step 112500, total loss = 0.58, predict loss = 0.14 (74.7 examples/sec; 0.054 sec/batch; 87h:32m:19s remains)
INFO - root - 2019-11-04 01:15:47.174072: step 112510, total loss = 0.52, predict loss = 0.12 (59.6 examples/sec; 0.067 sec/batch; 109h:48m:19s remains)
INFO - root - 2019-11-04 01:15:47.824151: step 112520, total loss = 0.49, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 98h:51m:47s remains)
INFO - root - 2019-11-04 01:15:48.475023: step 112530, total loss = 0.46, predict loss = 0.10 (71.4 examples/sec; 0.056 sec/batch; 91h:40m:32s remains)
INFO - root - 2019-11-04 01:15:49.137801: step 112540, total loss = 0.47, predict loss = 0.11 (63.5 examples/sec; 0.063 sec/batch; 102h:59m:32s remains)
INFO - root - 2019-11-04 01:15:49.847161: step 112550, total loss = 0.56, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 94h:33m:13s remains)
INFO - root - 2019-11-04 01:15:50.547531: step 112560, total loss = 0.48, predict loss = 0.11 (73.7 examples/sec; 0.054 sec/batch; 88h:44m:56s remains)
INFO - root - 2019-11-04 01:15:51.176611: step 112570, total loss = 0.49, predict loss = 0.11 (72.9 examples/sec; 0.055 sec/batch; 89h:46m:45s remains)
INFO - root - 2019-11-04 01:15:51.811721: step 112580, total loss = 0.53, predict loss = 0.13 (68.7 examples/sec; 0.058 sec/batch; 95h:15m:11s remains)
INFO - root - 2019-11-04 01:15:52.427483: step 112590, total loss = 0.51, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 88h:31m:30s remains)
INFO - root - 2019-11-04 01:15:53.039250: step 112600, total loss = 0.46, predict loss = 0.11 (78.1 examples/sec; 0.051 sec/batch; 83h:42m:49s remains)
INFO - root - 2019-11-04 01:15:53.707958: step 112610, total loss = 0.67, predict loss = 0.17 (73.6 examples/sec; 0.054 sec/batch; 88h:51m:10s remains)
INFO - root - 2019-11-04 01:15:54.374219: step 112620, total loss = 0.72, predict loss = 0.19 (69.5 examples/sec; 0.058 sec/batch; 94h:11m:19s remains)
INFO - root - 2019-11-04 01:15:55.035431: step 112630, total loss = 0.55, predict loss = 0.13 (67.7 examples/sec; 0.059 sec/batch; 96h:36m:01s remains)
INFO - root - 2019-11-04 01:15:55.713059: step 112640, total loss = 0.31, predict loss = 0.06 (61.4 examples/sec; 0.065 sec/batch; 106h:28m:52s remains)
INFO - root - 2019-11-04 01:15:56.327597: step 112650, total loss = 0.43, predict loss = 0.10 (77.3 examples/sec; 0.052 sec/batch; 84h:36m:00s remains)
INFO - root - 2019-11-04 01:15:56.996807: step 112660, total loss = 0.29, predict loss = 0.06 (60.9 examples/sec; 0.066 sec/batch; 107h:22m:52s remains)
INFO - root - 2019-11-04 01:15:57.601701: step 112670, total loss = 0.30, predict loss = 0.06 (80.9 examples/sec; 0.049 sec/batch; 80h:52m:29s remains)
INFO - root - 2019-11-04 01:15:58.240984: step 112680, total loss = 0.24, predict loss = 0.05 (68.4 examples/sec; 0.058 sec/batch; 95h:38m:58s remains)
INFO - root - 2019-11-04 01:15:58.863727: step 112690, total loss = 0.44, predict loss = 0.10 (66.1 examples/sec; 0.061 sec/batch; 98h:57m:30s remains)
INFO - root - 2019-11-04 01:15:59.472137: step 112700, total loss = 0.27, predict loss = 0.06 (85.6 examples/sec; 0.047 sec/batch; 76h:26m:53s remains)
INFO - root - 2019-11-04 01:16:00.154088: step 112710, total loss = 0.55, predict loss = 0.12 (59.4 examples/sec; 0.067 sec/batch; 110h:08m:13s remains)
INFO - root - 2019-11-04 01:16:00.821260: step 112720, total loss = 0.23, predict loss = 0.05 (65.5 examples/sec; 0.061 sec/batch; 99h:50m:06s remains)
INFO - root - 2019-11-04 01:16:01.525188: step 112730, total loss = 0.41, predict loss = 0.09 (64.9 examples/sec; 0.062 sec/batch; 100h:52m:10s remains)
INFO - root - 2019-11-04 01:16:02.184348: step 112740, total loss = 0.38, predict loss = 0.09 (72.7 examples/sec; 0.055 sec/batch; 89h:55m:56s remains)
INFO - root - 2019-11-04 01:16:02.801760: step 112750, total loss = 0.41, predict loss = 0.09 (68.0 examples/sec; 0.059 sec/batch; 96h:13m:04s remains)
INFO - root - 2019-11-04 01:16:03.401232: step 112760, total loss = 0.43, predict loss = 0.10 (70.2 examples/sec; 0.057 sec/batch; 93h:11m:06s remains)
INFO - root - 2019-11-04 01:16:04.008156: step 112770, total loss = 0.35, predict loss = 0.08 (68.5 examples/sec; 0.058 sec/batch; 95h:29m:21s remains)
INFO - root - 2019-11-04 01:16:04.667246: step 112780, total loss = 0.39, predict loss = 0.09 (69.8 examples/sec; 0.057 sec/batch; 93h:43m:19s remains)
INFO - root - 2019-11-04 01:16:05.268452: step 112790, total loss = 0.40, predict loss = 0.10 (68.8 examples/sec; 0.058 sec/batch; 95h:02m:48s remains)
INFO - root - 2019-11-04 01:16:05.886403: step 112800, total loss = 0.40, predict loss = 0.09 (75.3 examples/sec; 0.053 sec/batch; 86h:48m:46s remains)
INFO - root - 2019-11-04 01:16:06.533159: step 112810, total loss = 0.37, predict loss = 0.08 (73.5 examples/sec; 0.054 sec/batch; 89h:02m:09s remains)
INFO - root - 2019-11-04 01:16:07.188510: step 112820, total loss = 0.66, predict loss = 0.16 (70.8 examples/sec; 0.057 sec/batch; 92h:25m:19s remains)
INFO - root - 2019-11-04 01:16:07.816569: step 112830, total loss = 0.59, predict loss = 0.14 (72.5 examples/sec; 0.055 sec/batch; 90h:15m:56s remains)
INFO - root - 2019-11-04 01:16:08.420997: step 112840, total loss = 0.63, predict loss = 0.15 (78.9 examples/sec; 0.051 sec/batch; 82h:54m:37s remains)
INFO - root - 2019-11-04 01:16:09.042819: step 112850, total loss = 0.55, predict loss = 0.14 (72.8 examples/sec; 0.055 sec/batch; 89h:53m:00s remains)
INFO - root - 2019-11-04 01:16:09.651403: step 112860, total loss = 0.47, predict loss = 0.11 (75.4 examples/sec; 0.053 sec/batch; 86h:47m:22s remains)
INFO - root - 2019-11-04 01:16:10.263913: step 112870, total loss = 0.31, predict loss = 0.07 (67.2 examples/sec; 0.060 sec/batch; 97h:18m:56s remains)
INFO - root - 2019-11-04 01:16:10.858296: step 112880, total loss = 0.32, predict loss = 0.08 (71.2 examples/sec; 0.056 sec/batch; 91h:48m:46s remains)
INFO - root - 2019-11-04 01:16:11.501025: step 112890, total loss = 0.26, predict loss = 0.06 (75.0 examples/sec; 0.053 sec/batch; 87h:14m:53s remains)
INFO - root - 2019-11-04 01:16:12.113362: step 112900, total loss = 0.38, predict loss = 0.09 (81.0 examples/sec; 0.049 sec/batch; 80h:44m:20s remains)
INFO - root - 2019-11-04 01:16:12.833473: step 112910, total loss = 0.28, predict loss = 0.06 (47.5 examples/sec; 0.084 sec/batch; 137h:49m:56s remains)
INFO - root - 2019-11-04 01:16:13.512502: step 112920, total loss = 0.31, predict loss = 0.07 (68.6 examples/sec; 0.058 sec/batch; 95h:24m:43s remains)
INFO - root - 2019-11-04 01:16:14.149320: step 112930, total loss = 0.40, predict loss = 0.09 (84.3 examples/sec; 0.047 sec/batch; 77h:35m:05s remains)
INFO - root - 2019-11-04 01:16:14.761612: step 112940, total loss = 0.39, predict loss = 0.09 (68.6 examples/sec; 0.058 sec/batch; 95h:20m:40s remains)
INFO - root - 2019-11-04 01:16:15.420354: step 112950, total loss = 0.32, predict loss = 0.07 (60.4 examples/sec; 0.066 sec/batch; 108h:20m:04s remains)
INFO - root - 2019-11-04 01:16:16.081667: step 112960, total loss = 0.50, predict loss = 0.12 (65.2 examples/sec; 0.061 sec/batch; 100h:23m:30s remains)
INFO - root - 2019-11-04 01:16:16.770436: step 112970, total loss = 0.40, predict loss = 0.09 (66.0 examples/sec; 0.061 sec/batch; 99h:03m:50s remains)
INFO - root - 2019-11-04 01:16:17.510077: step 112980, total loss = 0.54, predict loss = 0.12 (60.3 examples/sec; 0.066 sec/batch; 108h:31m:27s remains)
INFO - root - 2019-11-04 01:16:18.220727: step 112990, total loss = 0.42, predict loss = 0.09 (75.4 examples/sec; 0.053 sec/batch; 86h:46m:49s remains)
INFO - root - 2019-11-04 01:16:18.902642: step 113000, total loss = 0.51, predict loss = 0.12 (78.9 examples/sec; 0.051 sec/batch; 82h:54m:50s remains)
INFO - root - 2019-11-04 01:16:19.511198: step 113010, total loss = 0.33, predict loss = 0.07 (72.3 examples/sec; 0.055 sec/batch; 90h:25m:51s remains)
INFO - root - 2019-11-04 01:16:20.155744: step 113020, total loss = 0.49, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 88h:34m:02s remains)
INFO - root - 2019-11-04 01:16:20.800673: step 113030, total loss = 0.28, predict loss = 0.06 (75.5 examples/sec; 0.053 sec/batch; 86h:38m:07s remains)
INFO - root - 2019-11-04 01:16:21.383588: step 113040, total loss = 0.44, predict loss = 0.10 (67.0 examples/sec; 0.060 sec/batch; 97h:41m:44s remains)
INFO - root - 2019-11-04 01:16:22.108413: step 113050, total loss = 0.35, predict loss = 0.08 (62.6 examples/sec; 0.064 sec/batch; 104h:33m:48s remains)
INFO - root - 2019-11-04 01:16:22.834835: step 113060, total loss = 0.38, predict loss = 0.08 (51.5 examples/sec; 0.078 sec/batch; 126h:54m:27s remains)
INFO - root - 2019-11-04 01:16:23.577499: step 113070, total loss = 0.43, predict loss = 0.10 (51.1 examples/sec; 0.078 sec/batch; 128h:06m:35s remains)
INFO - root - 2019-11-04 01:16:24.388058: step 113080, total loss = 0.63, predict loss = 0.15 (55.7 examples/sec; 0.072 sec/batch; 117h:26m:09s remains)
INFO - root - 2019-11-04 01:16:25.187135: step 113090, total loss = 0.47, predict loss = 0.11 (62.4 examples/sec; 0.064 sec/batch; 104h:49m:39s remains)
INFO - root - 2019-11-04 01:16:25.828186: step 113100, total loss = 0.50, predict loss = 0.11 (60.1 examples/sec; 0.067 sec/batch; 108h:47m:00s remains)
INFO - root - 2019-11-04 01:16:26.448371: step 113110, total loss = 0.37, predict loss = 0.08 (74.9 examples/sec; 0.053 sec/batch; 87h:17m:15s remains)
INFO - root - 2019-11-04 01:16:27.103697: step 113120, total loss = 0.48, predict loss = 0.11 (67.5 examples/sec; 0.059 sec/batch; 96h:56m:19s remains)
INFO - root - 2019-11-04 01:16:27.743368: step 113130, total loss = 0.57, predict loss = 0.12 (70.0 examples/sec; 0.057 sec/batch; 93h:25m:20s remains)
INFO - root - 2019-11-04 01:16:28.375462: step 113140, total loss = 0.47, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 97h:01m:32s remains)
INFO - root - 2019-11-04 01:16:29.076728: step 113150, total loss = 0.34, predict loss = 0.08 (62.4 examples/sec; 0.064 sec/batch; 104h:48m:12s remains)
INFO - root - 2019-11-04 01:16:29.795329: step 113160, total loss = 0.35, predict loss = 0.08 (68.9 examples/sec; 0.058 sec/batch; 94h:57m:29s remains)
INFO - root - 2019-11-04 01:16:30.427324: step 113170, total loss = 0.38, predict loss = 0.08 (80.3 examples/sec; 0.050 sec/batch; 81h:25m:38s remains)
INFO - root - 2019-11-04 01:16:31.080277: step 113180, total loss = 0.24, predict loss = 0.05 (67.4 examples/sec; 0.059 sec/batch; 97h:06m:10s remains)
INFO - root - 2019-11-04 01:16:31.717341: step 113190, total loss = 0.48, predict loss = 0.12 (74.0 examples/sec; 0.054 sec/batch; 88h:21m:26s remains)
INFO - root - 2019-11-04 01:16:32.334466: step 113200, total loss = 0.46, predict loss = 0.11 (76.0 examples/sec; 0.053 sec/batch; 86h:01m:55s remains)
INFO - root - 2019-11-04 01:16:33.046515: step 113210, total loss = 0.50, predict loss = 0.11 (61.8 examples/sec; 0.065 sec/batch; 105h:47m:13s remains)
INFO - root - 2019-11-04 01:16:33.645532: step 113220, total loss = 0.44, predict loss = 0.10 (79.9 examples/sec; 0.050 sec/batch; 81h:53m:02s remains)
INFO - root - 2019-11-04 01:16:34.278647: step 113230, total loss = 0.48, predict loss = 0.11 (65.0 examples/sec; 0.062 sec/batch; 100h:37m:18s remains)
INFO - root - 2019-11-04 01:16:34.957186: step 113240, total loss = 0.51, predict loss = 0.12 (64.7 examples/sec; 0.062 sec/batch; 101h:03m:33s remains)
INFO - root - 2019-11-04 01:16:35.688335: step 113250, total loss = 0.66, predict loss = 0.15 (61.8 examples/sec; 0.065 sec/batch; 105h:45m:35s remains)
INFO - root - 2019-11-04 01:16:36.414585: step 113260, total loss = 0.72, predict loss = 0.18 (74.3 examples/sec; 0.054 sec/batch; 88h:05m:00s remains)
INFO - root - 2019-11-04 01:16:37.029158: step 113270, total loss = 0.77, predict loss = 0.18 (71.9 examples/sec; 0.056 sec/batch; 90h:58m:28s remains)
INFO - root - 2019-11-04 01:16:37.663761: step 113280, total loss = 0.83, predict loss = 0.20 (76.1 examples/sec; 0.053 sec/batch; 85h:57m:31s remains)
INFO - root - 2019-11-04 01:16:38.277677: step 113290, total loss = 0.62, predict loss = 0.15 (73.3 examples/sec; 0.055 sec/batch; 89h:12m:16s remains)
INFO - root - 2019-11-04 01:16:38.980433: step 113300, total loss = 0.85, predict loss = 0.21 (55.3 examples/sec; 0.072 sec/batch; 118h:20m:55s remains)
INFO - root - 2019-11-04 01:16:39.617623: step 113310, total loss = 0.55, predict loss = 0.13 (81.1 examples/sec; 0.049 sec/batch; 80h:39m:51s remains)
INFO - root - 2019-11-04 01:16:40.231134: step 113320, total loss = 0.65, predict loss = 0.15 (82.6 examples/sec; 0.048 sec/batch; 79h:08m:54s remains)
INFO - root - 2019-11-04 01:16:40.880324: step 113330, total loss = 0.52, predict loss = 0.12 (50.5 examples/sec; 0.079 sec/batch; 129h:37m:22s remains)
INFO - root - 2019-11-04 01:16:41.597219: step 113340, total loss = 0.52, predict loss = 0.12 (63.4 examples/sec; 0.063 sec/batch; 103h:09m:30s remains)
INFO - root - 2019-11-04 01:16:42.320734: step 113350, total loss = 0.57, predict loss = 0.13 (62.8 examples/sec; 0.064 sec/batch; 104h:10m:31s remains)
INFO - root - 2019-11-04 01:16:43.003369: step 113360, total loss = 0.49, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:29m:01s remains)
INFO - root - 2019-11-04 01:16:43.631424: step 113370, total loss = 0.49, predict loss = 0.10 (71.0 examples/sec; 0.056 sec/batch; 92h:03m:29s remains)
INFO - root - 2019-11-04 01:16:44.291894: step 113380, total loss = 0.56, predict loss = 0.13 (78.8 examples/sec; 0.051 sec/batch; 83h:02m:49s remains)
INFO - root - 2019-11-04 01:16:44.888691: step 113390, total loss = 0.47, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 86h:35m:52s remains)
INFO - root - 2019-11-04 01:16:45.520434: step 113400, total loss = 0.59, predict loss = 0.14 (76.4 examples/sec; 0.052 sec/batch; 85h:35m:58s remains)
INFO - root - 2019-11-04 01:16:46.142262: step 113410, total loss = 0.46, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 93h:54m:08s remains)
INFO - root - 2019-11-04 01:16:46.809490: step 113420, total loss = 0.41, predict loss = 0.09 (68.1 examples/sec; 0.059 sec/batch; 95h:58m:29s remains)
INFO - root - 2019-11-04 01:16:47.453737: step 113430, total loss = 0.46, predict loss = 0.11 (67.6 examples/sec; 0.059 sec/batch; 96h:42m:25s remains)
INFO - root - 2019-11-04 01:16:48.105756: step 113440, total loss = 0.58, predict loss = 0.13 (67.1 examples/sec; 0.060 sec/batch; 97h:32m:19s remains)
INFO - root - 2019-11-04 01:16:48.797650: step 113450, total loss = 0.40, predict loss = 0.09 (57.7 examples/sec; 0.069 sec/batch; 113h:18m:50s remains)
INFO - root - 2019-11-04 01:16:49.483017: step 113460, total loss = 0.58, predict loss = 0.14 (64.0 examples/sec; 0.063 sec/batch; 102h:15m:48s remains)
INFO - root - 2019-11-04 01:16:50.160711: step 113470, total loss = 0.47, predict loss = 0.11 (61.7 examples/sec; 0.065 sec/batch; 106h:01m:58s remains)
INFO - root - 2019-11-04 01:16:50.956250: step 113480, total loss = 0.59, predict loss = 0.14 (63.9 examples/sec; 0.063 sec/batch; 102h:18m:24s remains)
INFO - root - 2019-11-04 01:16:51.630803: step 113490, total loss = 0.38, predict loss = 0.08 (71.9 examples/sec; 0.056 sec/batch; 90h:58m:08s remains)
INFO - root - 2019-11-04 01:16:52.245045: step 113500, total loss = 0.40, predict loss = 0.09 (75.5 examples/sec; 0.053 sec/batch; 86h:36m:57s remains)
INFO - root - 2019-11-04 01:16:52.872069: step 113510, total loss = 0.43, predict loss = 0.10 (73.1 examples/sec; 0.055 sec/batch; 89h:26m:27s remains)
INFO - root - 2019-11-04 01:16:53.521551: step 113520, total loss = 0.38, predict loss = 0.09 (69.6 examples/sec; 0.057 sec/batch; 93h:56m:48s remains)
INFO - root - 2019-11-04 01:16:54.232981: step 113530, total loss = 0.38, predict loss = 0.09 (68.1 examples/sec; 0.059 sec/batch; 96h:06m:36s remains)
INFO - root - 2019-11-04 01:16:54.929135: step 113540, total loss = 0.37, predict loss = 0.08 (75.0 examples/sec; 0.053 sec/batch; 87h:13m:35s remains)
INFO - root - 2019-11-04 01:16:55.541546: step 113550, total loss = 0.47, predict loss = 0.11 (76.8 examples/sec; 0.052 sec/batch; 85h:10m:35s remains)
INFO - root - 2019-11-04 01:16:56.218936: step 113560, total loss = 0.45, predict loss = 0.11 (60.7 examples/sec; 0.066 sec/batch; 107h:41m:59s remains)
INFO - root - 2019-11-04 01:16:56.883215: step 113570, total loss = 0.41, predict loss = 0.09 (65.7 examples/sec; 0.061 sec/batch; 99h:35m:57s remains)
INFO - root - 2019-11-04 01:16:57.566243: step 113580, total loss = 0.37, predict loss = 0.08 (69.5 examples/sec; 0.058 sec/batch; 94h:07m:41s remains)
INFO - root - 2019-11-04 01:16:58.166897: step 113590, total loss = 0.50, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 91h:12m:57s remains)
INFO - root - 2019-11-04 01:16:58.794818: step 113600, total loss = 0.44, predict loss = 0.10 (72.9 examples/sec; 0.055 sec/batch; 89h:41m:23s remains)
INFO - root - 2019-11-04 01:16:59.420490: step 113610, total loss = 0.35, predict loss = 0.07 (74.2 examples/sec; 0.054 sec/batch; 88h:06m:53s remains)
INFO - root - 2019-11-04 01:17:00.049039: step 113620, total loss = 0.46, predict loss = 0.10 (70.7 examples/sec; 0.057 sec/batch; 92h:29m:07s remains)
INFO - root - 2019-11-04 01:17:00.683983: step 113630, total loss = 0.52, predict loss = 0.12 (70.8 examples/sec; 0.057 sec/batch; 92h:25m:04s remains)
INFO - root - 2019-11-04 01:17:01.290785: step 113640, total loss = 0.41, predict loss = 0.09 (66.9 examples/sec; 0.060 sec/batch; 97h:43m:45s remains)
INFO - root - 2019-11-04 01:17:01.914312: step 113650, total loss = 0.51, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 89h:37m:02s remains)
INFO - root - 2019-11-04 01:17:02.581735: step 113660, total loss = 0.57, predict loss = 0.13 (65.2 examples/sec; 0.061 sec/batch; 100h:23m:12s remains)
INFO - root - 2019-11-04 01:17:03.255150: step 113670, total loss = 0.62, predict loss = 0.15 (67.0 examples/sec; 0.060 sec/batch; 97h:39m:49s remains)
INFO - root - 2019-11-04 01:17:03.895300: step 113680, total loss = 0.42, predict loss = 0.10 (68.4 examples/sec; 0.058 sec/batch; 95h:38m:36s remains)
INFO - root - 2019-11-04 01:17:04.518751: step 113690, total loss = 0.71, predict loss = 0.17 (74.9 examples/sec; 0.053 sec/batch; 87h:19m:18s remains)
INFO - root - 2019-11-04 01:17:05.183407: step 113700, total loss = 0.62, predict loss = 0.15 (67.2 examples/sec; 0.060 sec/batch; 97h:21m:51s remains)
INFO - root - 2019-11-04 01:17:05.862112: step 113710, total loss = 0.69, predict loss = 0.17 (66.2 examples/sec; 0.060 sec/batch; 98h:43m:55s remains)
INFO - root - 2019-11-04 01:17:06.577793: step 113720, total loss = 0.84, predict loss = 0.20 (64.7 examples/sec; 0.062 sec/batch; 101h:06m:28s remains)
INFO - root - 2019-11-04 01:17:07.305599: step 113730, total loss = 0.43, predict loss = 0.10 (64.9 examples/sec; 0.062 sec/batch; 100h:43m:53s remains)
INFO - root - 2019-11-04 01:17:07.965111: step 113740, total loss = 0.72, predict loss = 0.18 (67.9 examples/sec; 0.059 sec/batch; 96h:18m:34s remains)
INFO - root - 2019-11-04 01:17:08.621249: step 113750, total loss = 0.55, predict loss = 0.13 (69.9 examples/sec; 0.057 sec/batch; 93h:34m:23s remains)
INFO - root - 2019-11-04 01:17:09.256527: step 113760, total loss = 0.54, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 91h:46m:34s remains)
INFO - root - 2019-11-04 01:17:09.940945: step 113770, total loss = 0.46, predict loss = 0.11 (72.2 examples/sec; 0.055 sec/batch; 90h:34m:56s remains)
INFO - root - 2019-11-04 01:17:10.623936: step 113780, total loss = 0.46, predict loss = 0.11 (60.7 examples/sec; 0.066 sec/batch; 107h:45m:52s remains)
INFO - root - 2019-11-04 01:17:11.254392: step 113790, total loss = 0.54, predict loss = 0.13 (74.5 examples/sec; 0.054 sec/batch; 87h:47m:30s remains)
INFO - root - 2019-11-04 01:17:11.858028: step 113800, total loss = 0.35, predict loss = 0.08 (67.3 examples/sec; 0.059 sec/batch; 97h:09m:43s remains)
INFO - root - 2019-11-04 01:17:12.498045: step 113810, total loss = 0.39, predict loss = 0.09 (67.2 examples/sec; 0.060 sec/batch; 97h:23m:06s remains)
INFO - root - 2019-11-04 01:17:13.178847: step 113820, total loss = 0.42, predict loss = 0.09 (72.5 examples/sec; 0.055 sec/batch; 90h:12m:27s remains)
INFO - root - 2019-11-04 01:17:13.806951: step 113830, total loss = 0.35, predict loss = 0.08 (67.0 examples/sec; 0.060 sec/batch; 97h:39m:58s remains)
INFO - root - 2019-11-04 01:17:14.476028: step 113840, total loss = 0.40, predict loss = 0.09 (64.5 examples/sec; 0.062 sec/batch; 101h:24m:28s remains)
INFO - root - 2019-11-04 01:17:15.130309: step 113850, total loss = 0.45, predict loss = 0.11 (71.9 examples/sec; 0.056 sec/batch; 90h:57m:44s remains)
INFO - root - 2019-11-04 01:17:15.798767: step 113860, total loss = 0.59, predict loss = 0.14 (81.8 examples/sec; 0.049 sec/batch; 79h:58m:41s remains)
INFO - root - 2019-11-04 01:17:16.476999: step 113870, total loss = 0.44, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 98h:20m:48s remains)
INFO - root - 2019-11-04 01:17:17.185286: step 113880, total loss = 0.45, predict loss = 0.10 (55.3 examples/sec; 0.072 sec/batch; 118h:10m:06s remains)
INFO - root - 2019-11-04 01:17:17.865597: step 113890, total loss = 0.66, predict loss = 0.16 (69.8 examples/sec; 0.057 sec/batch; 93h:43m:40s remains)
INFO - root - 2019-11-04 01:17:18.535215: step 113900, total loss = 0.50, predict loss = 0.12 (59.7 examples/sec; 0.067 sec/batch; 109h:34m:22s remains)
INFO - root - 2019-11-04 01:17:19.240885: step 113910, total loss = 0.42, predict loss = 0.10 (58.2 examples/sec; 0.069 sec/batch; 112h:17m:53s remains)
INFO - root - 2019-11-04 01:17:19.865305: step 113920, total loss = 0.52, predict loss = 0.13 (75.7 examples/sec; 0.053 sec/batch; 86h:26m:38s remains)
INFO - root - 2019-11-04 01:17:20.517049: step 113930, total loss = 0.68, predict loss = 0.17 (71.6 examples/sec; 0.056 sec/batch; 91h:21m:41s remains)
INFO - root - 2019-11-04 01:17:21.145726: step 113940, total loss = 0.58, predict loss = 0.14 (62.9 examples/sec; 0.064 sec/batch; 103h:59m:01s remains)
INFO - root - 2019-11-04 01:17:21.804706: step 113950, total loss = 0.50, predict loss = 0.13 (58.6 examples/sec; 0.068 sec/batch; 111h:34m:09s remains)
INFO - root - 2019-11-04 01:17:22.477152: step 113960, total loss = 0.45, predict loss = 0.11 (63.7 examples/sec; 0.063 sec/batch; 102h:40m:00s remains)
INFO - root - 2019-11-04 01:17:23.125507: step 113970, total loss = 0.53, predict loss = 0.13 (63.3 examples/sec; 0.063 sec/batch; 103h:19m:21s remains)
INFO - root - 2019-11-04 01:17:23.764812: step 113980, total loss = 0.58, predict loss = 0.14 (64.7 examples/sec; 0.062 sec/batch; 101h:06m:54s remains)
INFO - root - 2019-11-04 01:17:24.427231: step 113990, total loss = 0.48, predict loss = 0.11 (63.8 examples/sec; 0.063 sec/batch; 102h:35m:15s remains)
INFO - root - 2019-11-04 01:17:25.152989: step 114000, total loss = 0.50, predict loss = 0.11 (60.9 examples/sec; 0.066 sec/batch; 107h:19m:30s remains)
INFO - root - 2019-11-04 01:17:25.805081: step 114010, total loss = 0.49, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 89h:38m:57s remains)
INFO - root - 2019-11-04 01:17:26.421137: step 114020, total loss = 0.39, predict loss = 0.08 (65.8 examples/sec; 0.061 sec/batch; 99h:21m:57s remains)
INFO - root - 2019-11-04 01:17:27.068492: step 114030, total loss = 0.52, predict loss = 0.13 (74.3 examples/sec; 0.054 sec/batch; 88h:02m:27s remains)
INFO - root - 2019-11-04 01:17:27.701660: step 114040, total loss = 0.47, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 84h:19m:50s remains)
INFO - root - 2019-11-04 01:17:28.345611: step 114050, total loss = 0.55, predict loss = 0.12 (83.4 examples/sec; 0.048 sec/batch; 78h:25m:52s remains)
INFO - root - 2019-11-04 01:17:28.975459: step 114060, total loss = 0.52, predict loss = 0.12 (84.9 examples/sec; 0.047 sec/batch; 76h:59m:36s remains)
INFO - root - 2019-11-04 01:17:29.564556: step 114070, total loss = 0.48, predict loss = 0.11 (81.5 examples/sec; 0.049 sec/batch; 80h:12m:21s remains)
INFO - root - 2019-11-04 01:17:30.193574: step 114080, total loss = 0.37, predict loss = 0.08 (59.6 examples/sec; 0.067 sec/batch; 109h:40m:36s remains)
INFO - root - 2019-11-04 01:17:30.917218: step 114090, total loss = 0.36, predict loss = 0.08 (65.6 examples/sec; 0.061 sec/batch; 99h:42m:08s remains)
INFO - root - 2019-11-04 01:17:31.675021: step 114100, total loss = 0.34, predict loss = 0.08 (62.7 examples/sec; 0.064 sec/batch; 104h:16m:29s remains)
INFO - root - 2019-11-04 01:17:32.524795: step 114110, total loss = 0.46, predict loss = 0.11 (46.7 examples/sec; 0.086 sec/batch; 139h:56m:51s remains)
INFO - root - 2019-11-04 01:17:33.321317: step 114120, total loss = 0.42, predict loss = 0.09 (67.1 examples/sec; 0.060 sec/batch; 97h:24m:18s remains)
INFO - root - 2019-11-04 01:17:33.951316: step 114130, total loss = 0.44, predict loss = 0.10 (82.1 examples/sec; 0.049 sec/batch; 79h:38m:43s remains)
INFO - root - 2019-11-04 01:17:34.694355: step 114140, total loss = 0.45, predict loss = 0.11 (59.8 examples/sec; 0.067 sec/batch; 109h:22m:28s remains)
INFO - root - 2019-11-04 01:17:35.305444: step 114150, total loss = 0.46, predict loss = 0.10 (74.0 examples/sec; 0.054 sec/batch; 88h:24m:19s remains)
INFO - root - 2019-11-04 01:17:35.953093: step 114160, total loss = 0.55, predict loss = 0.13 (68.9 examples/sec; 0.058 sec/batch; 94h:51m:52s remains)
INFO - root - 2019-11-04 01:17:36.635872: step 114170, total loss = 0.55, predict loss = 0.13 (61.1 examples/sec; 0.065 sec/batch; 106h:57m:08s remains)
INFO - root - 2019-11-04 01:17:37.341987: step 114180, total loss = 0.45, predict loss = 0.10 (60.3 examples/sec; 0.066 sec/batch; 108h:30m:29s remains)
INFO - root - 2019-11-04 01:17:38.012305: step 114190, total loss = 0.52, predict loss = 0.13 (73.8 examples/sec; 0.054 sec/batch; 88h:38m:14s remains)
INFO - root - 2019-11-04 01:17:38.610052: step 114200, total loss = 0.40, predict loss = 0.09 (74.0 examples/sec; 0.054 sec/batch; 88h:19m:25s remains)
INFO - root - 2019-11-04 01:17:39.235474: step 114210, total loss = 0.41, predict loss = 0.09 (70.3 examples/sec; 0.057 sec/batch; 93h:01m:19s remains)
INFO - root - 2019-11-04 01:17:39.838416: step 114220, total loss = 0.42, predict loss = 0.10 (81.0 examples/sec; 0.049 sec/batch; 80h:47m:02s remains)
INFO - root - 2019-11-04 01:17:40.455141: step 114230, total loss = 0.44, predict loss = 0.10 (81.3 examples/sec; 0.049 sec/batch; 80h:25m:50s remains)
INFO - root - 2019-11-04 01:17:41.119890: step 114240, total loss = 0.48, predict loss = 0.11 (64.1 examples/sec; 0.062 sec/batch; 101h:58m:44s remains)
INFO - root - 2019-11-04 01:17:41.821017: step 114250, total loss = 0.45, predict loss = 0.11 (55.2 examples/sec; 0.072 sec/batch; 118h:22m:06s remains)
INFO - root - 2019-11-04 01:17:42.564093: step 114260, total loss = 0.48, predict loss = 0.11 (69.9 examples/sec; 0.057 sec/batch; 93h:33m:14s remains)
INFO - root - 2019-11-04 01:17:43.190279: step 114270, total loss = 0.59, predict loss = 0.15 (72.3 examples/sec; 0.055 sec/batch; 90h:30m:05s remains)
INFO - root - 2019-11-04 01:17:43.828024: step 114280, total loss = 0.67, predict loss = 0.15 (67.5 examples/sec; 0.059 sec/batch; 96h:51m:28s remains)
INFO - root - 2019-11-04 01:17:44.426622: step 114290, total loss = 0.74, predict loss = 0.18 (84.5 examples/sec; 0.047 sec/batch; 77h:24m:12s remains)
INFO - root - 2019-11-04 01:17:45.057457: step 114300, total loss = 0.61, predict loss = 0.15 (66.5 examples/sec; 0.060 sec/batch; 98h:23m:15s remains)
INFO - root - 2019-11-04 01:17:45.719322: step 114310, total loss = 0.58, predict loss = 0.14 (74.8 examples/sec; 0.053 sec/batch; 87h:24m:22s remains)
INFO - root - 2019-11-04 01:17:46.445718: step 114320, total loss = 0.68, predict loss = 0.16 (61.3 examples/sec; 0.065 sec/batch; 106h:35m:56s remains)
INFO - root - 2019-11-04 01:17:47.117124: step 114330, total loss = 0.49, predict loss = 0.12 (77.7 examples/sec; 0.051 sec/batch; 84h:08m:17s remains)
INFO - root - 2019-11-04 01:17:47.796647: step 114340, total loss = 0.63, predict loss = 0.14 (62.1 examples/sec; 0.064 sec/batch; 105h:13m:33s remains)
INFO - root - 2019-11-04 01:17:48.444125: step 114350, total loss = 0.66, predict loss = 0.16 (65.0 examples/sec; 0.062 sec/batch; 100h:37m:20s remains)
INFO - root - 2019-11-04 01:17:49.083265: step 114360, total loss = 0.52, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 92h:46m:08s remains)
INFO - root - 2019-11-04 01:17:49.737872: step 114370, total loss = 0.68, predict loss = 0.17 (66.8 examples/sec; 0.060 sec/batch; 97h:56m:28s remains)
INFO - root - 2019-11-04 01:17:50.394833: step 114380, total loss = 0.68, predict loss = 0.16 (66.4 examples/sec; 0.060 sec/batch; 98h:27m:41s remains)
INFO - root - 2019-11-04 01:17:51.036360: step 114390, total loss = 0.53, predict loss = 0.12 (75.0 examples/sec; 0.053 sec/batch; 87h:11m:20s remains)
INFO - root - 2019-11-04 01:17:51.688168: step 114400, total loss = 0.42, predict loss = 0.10 (65.3 examples/sec; 0.061 sec/batch; 100h:11m:26s remains)
INFO - root - 2019-11-04 01:17:52.350723: step 114410, total loss = 0.39, predict loss = 0.08 (70.8 examples/sec; 0.056 sec/batch; 92h:19m:59s remains)
INFO - root - 2019-11-04 01:17:52.993695: step 114420, total loss = 0.40, predict loss = 0.09 (65.3 examples/sec; 0.061 sec/batch; 100h:04m:35s remains)
INFO - root - 2019-11-04 01:17:53.628558: step 114430, total loss = 0.66, predict loss = 0.16 (86.6 examples/sec; 0.046 sec/batch; 75h:31m:36s remains)
INFO - root - 2019-11-04 01:17:54.238294: step 114440, total loss = 0.44, predict loss = 0.10 (73.8 examples/sec; 0.054 sec/batch; 88h:35m:04s remains)
INFO - root - 2019-11-04 01:17:54.876412: step 114450, total loss = 0.66, predict loss = 0.16 (74.6 examples/sec; 0.054 sec/batch; 87h:42m:54s remains)
INFO - root - 2019-11-04 01:17:55.495130: step 114460, total loss = 0.48, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 93h:45m:43s remains)
INFO - root - 2019-11-04 01:17:56.122962: step 114470, total loss = 0.48, predict loss = 0.11 (76.8 examples/sec; 0.052 sec/batch; 85h:10m:23s remains)
INFO - root - 2019-11-04 01:17:56.735432: step 114480, total loss = 0.67, predict loss = 0.15 (67.8 examples/sec; 0.059 sec/batch; 96h:29m:26s remains)
INFO - root - 2019-11-04 01:17:57.375820: step 114490, total loss = 0.61, predict loss = 0.14 (74.0 examples/sec; 0.054 sec/batch; 88h:20m:09s remains)
INFO - root - 2019-11-04 01:17:58.048941: step 114500, total loss = 0.60, predict loss = 0.13 (74.8 examples/sec; 0.053 sec/batch; 87h:23m:07s remains)
INFO - root - 2019-11-04 01:17:58.689941: step 114510, total loss = 0.62, predict loss = 0.14 (66.0 examples/sec; 0.061 sec/batch; 99h:07m:35s remains)
INFO - root - 2019-11-04 01:17:59.314854: step 114520, total loss = 0.48, predict loss = 0.11 (69.2 examples/sec; 0.058 sec/batch; 94h:30m:04s remains)
INFO - root - 2019-11-04 01:17:59.848689: step 114530, total loss = 0.48, predict loss = 0.11 (90.5 examples/sec; 0.044 sec/batch; 72h:16m:22s remains)
INFO - root - 2019-11-04 01:18:00.322969: step 114540, total loss = 0.48, predict loss = 0.10 (88.1 examples/sec; 0.045 sec/batch; 74h:12m:14s remains)
INFO - root - 2019-11-04 01:18:01.341659: step 114550, total loss = 0.45, predict loss = 0.11 (73.1 examples/sec; 0.055 sec/batch; 89h:24m:41s remains)
INFO - root - 2019-11-04 01:18:01.937843: step 114560, total loss = 0.48, predict loss = 0.11 (76.9 examples/sec; 0.052 sec/batch; 85h:03m:10s remains)
INFO - root - 2019-11-04 01:18:02.616220: step 114570, total loss = 0.47, predict loss = 0.11 (56.6 examples/sec; 0.071 sec/batch; 115h:26m:45s remains)
INFO - root - 2019-11-04 01:18:03.246341: step 114580, total loss = 0.41, predict loss = 0.09 (69.2 examples/sec; 0.058 sec/batch; 94h:30m:14s remains)
INFO - root - 2019-11-04 01:18:03.879150: step 114590, total loss = 0.49, predict loss = 0.11 (67.3 examples/sec; 0.059 sec/batch; 97h:12m:10s remains)
INFO - root - 2019-11-04 01:18:04.539660: step 114600, total loss = 0.44, predict loss = 0.11 (65.7 examples/sec; 0.061 sec/batch; 99h:34m:49s remains)
INFO - root - 2019-11-04 01:18:05.157906: step 114610, total loss = 0.40, predict loss = 0.09 (74.9 examples/sec; 0.053 sec/batch; 87h:18m:19s remains)
INFO - root - 2019-11-04 01:18:05.779988: step 114620, total loss = 0.64, predict loss = 0.15 (60.8 examples/sec; 0.066 sec/batch; 107h:34m:38s remains)
INFO - root - 2019-11-04 01:18:06.394667: step 114630, total loss = 0.58, predict loss = 0.13 (68.7 examples/sec; 0.058 sec/batch; 95h:13m:25s remains)
INFO - root - 2019-11-04 01:18:07.047972: step 114640, total loss = 0.38, predict loss = 0.08 (73.7 examples/sec; 0.054 sec/batch; 88h:45m:51s remains)
INFO - root - 2019-11-04 01:18:07.801689: step 114650, total loss = 0.64, predict loss = 0.16 (43.6 examples/sec; 0.092 sec/batch; 150h:04m:39s remains)
INFO - root - 2019-11-04 01:18:08.436185: step 114660, total loss = 0.51, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 86h:33m:55s remains)
INFO - root - 2019-11-04 01:18:09.094965: step 114670, total loss = 0.48, predict loss = 0.10 (82.9 examples/sec; 0.048 sec/batch; 78h:55m:04s remains)
INFO - root - 2019-11-04 01:18:09.756698: step 114680, total loss = 0.42, predict loss = 0.09 (68.1 examples/sec; 0.059 sec/batch; 96h:00m:04s remains)
INFO - root - 2019-11-04 01:18:10.384365: step 114690, total loss = 0.41, predict loss = 0.09 (67.6 examples/sec; 0.059 sec/batch; 96h:42m:43s remains)
INFO - root - 2019-11-04 01:18:11.041740: step 114700, total loss = 0.49, predict loss = 0.11 (76.1 examples/sec; 0.053 sec/batch; 85h:53m:17s remains)
INFO - root - 2019-11-04 01:18:11.708761: step 114710, total loss = 0.48, predict loss = 0.11 (64.4 examples/sec; 0.062 sec/batch; 101h:33m:18s remains)
INFO - root - 2019-11-04 01:18:12.377224: step 114720, total loss = 0.50, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 91h:10m:08s remains)
