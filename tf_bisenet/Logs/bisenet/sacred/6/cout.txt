INFO - bisenet-v2 - Running command 'main'
INFO - bisenet-v2 - Started run with ID "6"
INFO - root - nvidia-ml-py is not installed, automatically select gpu is disabled!
WARNING:tensorflow:From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - tensorflow - From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:53: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:53: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:65: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:65: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
INFO - root - preproces -- augment
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:73: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:73: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:74: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:74: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:131: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:131: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:164: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:164: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070a41f860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070a41f860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070a41f860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070a41f860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070aa980b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070aa980b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070aa980b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070aa980b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070b403518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070b403518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070b403518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070b403518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b403a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b403a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b403a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b403a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070b4036d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070b4036d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070b4036d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070b4036d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a4ac3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a4ac3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a4ac3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a4ac3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070a4ac7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070a4ac7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070a4ac7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070a4ac7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b375780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b375780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b375780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b375780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070aeb7198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070aeb7198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070aeb7198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070aeb7198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b7e98d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b7e98d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b7e98d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b7e98d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f070b3c1a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f070b3c1a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f070b3c1a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f070b3c1a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b375d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b375d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b375d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b375d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bd2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bd2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bd2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bd2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b3361d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b3361d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b3361d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b3361d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bc0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bc0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bc0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bc0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b3656a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b3656a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b3656a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b3656a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bd358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bd358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bd358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bd358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2bd358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2bd358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2bd358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2bd358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bc208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bc208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bc208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2bc208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b1f71d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b1f71d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b1f71d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b1f71d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b1466a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b1466a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b1466a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b1466a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2bda20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2bda20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2bda20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2bda20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b23d0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b23d0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b23d0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b23d0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b1466a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b1466a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b1466a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b1466a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b09ec18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b09ec18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b09ec18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b09ec18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b23d390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b23d390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b23d390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b23d390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b336588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b336588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b336588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b336588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b0b8be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b0b8be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b0b8be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b0b8be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070af48e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070af48e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070af48e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070af48e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b0d7b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b0d7b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b0d7b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b0d7b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ae293c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ae293c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ae293c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ae293c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b0d7710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b0d7710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b0d7710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b0d7710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ad7eef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ad7eef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ad7eef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ad7eef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ae8c828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ae8c828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ae8c828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ae8c828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ad7e7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ad7e7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ad7e7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ad7e7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ad3abe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ad3abe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ad3abe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ad3abe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ac6cc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ac6cc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ac6cc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ac6cc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070acecfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070acecfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070acecfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070acecfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b146438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b146438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b146438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b146438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ac3bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ac3bc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ac3bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ac3bc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070af48588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070af48588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070af48588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070af48588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ad99278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ad99278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ad99278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ad99278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b840ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b840ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b840ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b840ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b146438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b146438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b146438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b146438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ac84cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ac84cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ac84cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ac84cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070aae7518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070aae7518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070aae7518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070aae7518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a9f62e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a9f62e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a9f62e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a9f62e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b7e97f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b7e97f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b7e97f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b7e97f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a972f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a972f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a972f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a972f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b7e9da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b7e9da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b7e9da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b7e9da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ad09278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ad09278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ad09278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ad09278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f1d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f1d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f1d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f1d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a9f6390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a9f6390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a9f6390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a9f6390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f1898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f1898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f1898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f1898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ac84cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ac84cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ac84cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070ac84cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f6390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f6390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f6390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f6390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070abff048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070abff048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070abff048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070abff048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ac84d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ac84d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ac84d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070ac84d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a7e5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a7e5048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a7e5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a7e5048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e8eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e8eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e8eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e8eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a8529e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a8529e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a8529e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a8529e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e8d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e8d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e8d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e8d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a8529e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a8529e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a8529e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a8529e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a6df668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a6df668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a6df668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a6df668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a52b4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a52b4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a52b4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a52b4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e5f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e5f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e5f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e5f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a55c748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a55c748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a55c748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a55c748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e5ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e5ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e5ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a7e5ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a55ce48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a55ce48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a55ce48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a55ce48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f6780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f6780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f6780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f6780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a3c0438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a3c0438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a3c0438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a3c0438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a700cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a700cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a700cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a700cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a2d2a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a2d2a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a2d2a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a2d2a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a52b470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a52b470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a52b470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a52b470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a6b1b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a6b1b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a6b1b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a6b1b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a560860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a560860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a560860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a560860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a14c198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a14c198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a14c198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a14c198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f6470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f6470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f6470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f6470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a14cf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a14cf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a14cf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a14cf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f69b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f69b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f69b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a9f69b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a1ca320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a1ca320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a1ca320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a1ca320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2bca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2bca90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2bca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2bca90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b411588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b411588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b411588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b411588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a65c198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a65c198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a65c198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a65c198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a0c1940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a0c1940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a0c1940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a0c1940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a65c128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a65c128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a65c128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a65c128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a0c1ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a0c1ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a0c1ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070a0c1ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a2117f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a2117f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a2117f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a2117f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709f2e940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709f2e940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709f2e940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709f2e940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b840d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b840d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b840d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b840d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709ec4da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709ec4da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709ec4da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709ec4da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b840908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b840908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b840908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b840908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709ed3c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709ed3c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709ed3c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709ed3c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ec4a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ec4a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ec4a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ec4a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b403e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b403e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b403e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b403e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a0c12e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a0c12e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a0c12e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070a0c12e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709cb1470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709cb1470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709cb1470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709cb1470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709e35828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709e35828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709e35828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709e35828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709d73898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709d73898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709d73898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709d73898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ec4da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ec4da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ec4da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ec4da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709d73b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709d73b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709d73b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709d73b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ca2b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ca2b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ca2b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ca2b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709b20dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709b20dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709b20dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709b20dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ed35c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ed35c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ed35c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709ed35c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709b209b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709b209b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709b209b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709b209b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709afe0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709afe0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709afe0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709afe0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709a9f908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709a9f908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709a9f908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709a9f908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709c8ae10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709c8ae10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709c8ae10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709c8ae10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709a05780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709a05780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709a05780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709a05780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709a9f320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709a9f320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709a9f320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709a9f320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709e25eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709e25eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709e25eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709e25eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709a9f908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709a9f908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709a9f908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709a9f908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070992eef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070992eef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070992eef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070992eef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070a92dc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070a92dc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070a92dc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070a92dc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070980fba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070980fba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070980fba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070980fba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:186: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:186: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070984b9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070984b9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070984b9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070984b9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070980f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070980f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070980f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070980f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070980f860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070980f860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070980f860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f070980f860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07097d4f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07097d4f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07097d4f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07097d4f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07098043c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07098043c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07098043c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07098043c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709756828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709756828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709756828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709756828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07097566d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07097566d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07097566d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07097566d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b7e9278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b7e9278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b7e9278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b7e9278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07096516a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07096516a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07096516a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07096516a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070969ff98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070969ff98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070969ff98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070969ff98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07095e1da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07095e1da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07095e1da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07095e1da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07095f8390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07095f8390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07095f8390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07095f8390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709573240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709573240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709573240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709573240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070958ccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070958ccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070958ccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070958ccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07095f8748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07095f8748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07095f8748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07095f8748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709564c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709564c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709564c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709564c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07095e1e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07095e1e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07095e1e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07095e1e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07095070b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07095070b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07095070b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07095070b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709549358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709549358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709549358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709549358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07095078d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07095078d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07095078d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07095078d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07094fe780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07094fe780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07094fe780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07094fe780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07094fefd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07094fefd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07094fefd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07094fefd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07093ef2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07093ef2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07093ef2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07093ef2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07093ef940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07093ef940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07093ef940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07093ef940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07093ef208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07093ef208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07093ef208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07093ef208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07094fec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07094fec50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07094fec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f07094fec50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709290898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709290898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709290898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0709290898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:224: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:224: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:228: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:228: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:231: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:231: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:243: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:243: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:247: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:247: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - root - img_mean is not explicitly specified, using default value: None
INFO - root - preproces -- None
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e86d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e86d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e86d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e86d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e86c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e86c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e86c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e86c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e86ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e86ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e86ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e86ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e86c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903f588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903f588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903f588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903f588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e867f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e867f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e867f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708e867f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709053208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709053208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709053208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709053208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f07092900b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f07092900b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f07092900b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f07092900b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709053048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709053048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709053048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0709053048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903feb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903feb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903feb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903feb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903f7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903f7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903f7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903f7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e9f0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e9f0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e9f0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e9f0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e96208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e96208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e96208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e96208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e96048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e96048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e96048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e96048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f404a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f404a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f404a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f404a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070904a2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070904a2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070904a2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070904a2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f40b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f40b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f40b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f40b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070904a048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070904a048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070904a048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070904a048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903ff98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903ff98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903ff98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070903ff98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f40240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f40240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f40240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f40240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708ff2d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708ff2d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708ff2d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708ff2d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070903f780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbcef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbcef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbcef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbcef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbc550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbc550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbc550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbc550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2dba58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2dba58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2dba58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070b2dba58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2db940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2db940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2db940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2db940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070900bac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070900bac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070900bac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070900bac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2db940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2db940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2db940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f070b2db940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fe9208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fe9208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fe9208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fe9208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708ff2208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708ff2208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708ff2208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708ff2208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07090412e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07090412e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07090412e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07090412e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fe9a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fe9a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fe9a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fe9a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709041f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709041f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709041f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709041f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fe9940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fe9940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fe9940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fe9940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709041748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709041748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709041748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709041748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbc358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbc358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbc358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbc358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070900b978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070900b978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070900b978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f070900b978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e9f780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e9f780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e9f780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e9f780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709061240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709061240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709061240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0709061240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f07090414e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f07090414e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f07090414e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f07090414e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fda198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fda198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fda198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fda198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbd4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbd4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbd4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbd4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f75be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f75be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f75be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f75be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fda780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fda780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fda780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fda780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbe3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbe3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbe3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbe3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f75be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f75be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f75be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f75be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbe240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbe240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbe240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbe240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbe240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbe240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbe240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbe240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f916d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f916d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f916d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f916d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbe6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbe6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbe6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbe6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708de8e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708de8e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708de8e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708de8e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f9c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f9c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f9c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f9c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708de8b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708de8b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708de8b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708de8b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f9c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f9c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f9c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f9c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e64278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e64278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e64278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e64278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f7e860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f7e860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f7e860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f7e860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e64b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e64b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e64b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e64b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708de81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708de81d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708de81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708de81d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e0e780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e0e780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e0e780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e0e780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e64cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e64cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e64cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e64cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e62be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e62be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e62be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e62be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708df8ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708df8ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708df8ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708df8ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbe390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbe390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbe390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708fbe390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbe668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbe668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbe668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708fbe668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708dae0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708dae0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708dae0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708dae0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e622e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e622e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e622e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e622e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708dae0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708dae0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708dae0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708dae0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708daeb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708daeb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708daeb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708daeb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e0e240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e0e240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e0e240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e0e240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708daeb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708daeb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708daeb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708daeb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708df30b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708df30b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708df30b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708df30b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e0ef28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e0ef28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e0ef28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e0ef28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d84ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d84ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d84ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d84ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e1eda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e1eda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e1eda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708e1eda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d84400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d84400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d84400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d84400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708df32b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708df32b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708df32b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708df32b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f9c240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f9c240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f9c240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708f9c240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f9c240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f9c240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f9c240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f9c240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e12b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e12b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e12b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e12b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f91550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f91550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f91550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f91550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e0ea58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e0ea58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e0ea58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708e0ea58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f91438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f91438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f91438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f91438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708dc2860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708dc2860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708dc2860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708dc2860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f916d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f916d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f916d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708f916d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d7cf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d7cf28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d7cf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d7cf28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708dbcbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708dbcbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708dbcbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708dbcbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cec2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cec2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cec2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cec2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d7c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d7c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d7c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d7c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cec908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cec908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cec908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cec908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708dc2cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708dc2cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708dc2cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708dc2cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cecf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cecf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cecf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cecf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708dc2eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708dc2eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708dc2eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708dc2eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d93be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d93be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d93be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d93be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708cec4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708cec4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708cec4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708cec4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d54f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d54f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d54f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d54f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d93e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d93e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d93e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d93e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d542e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d542e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d542e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d542e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d93e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d93e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d93e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d93e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d54f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d54f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d54f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d54f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d54e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d54e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d54e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d54e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d1eba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d1eba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d1eba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708d1eba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d54b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d54b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d54b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d54b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cd6dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cd6dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cd6dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cd6dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d5f2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d5f2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d5f2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d5f2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cd9e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cd9e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cd9e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cd9e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d5fba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d5fba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d5fba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f0708d5fba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cdf4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cdf4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cdf4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cdf4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cabcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cabcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cabcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cabcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c274a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c274a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c274a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c274a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708d1e940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708d1e940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708d1e940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708d1e940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c27978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c27978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c27978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c27978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cdf8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cdf8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cdf8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cdf8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c904a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c904a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c904a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c904a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c90c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c90c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c90c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c90c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cdf8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cdf8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cdf8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708cdf8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cdfba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cdfba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cdfba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cdfba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c29080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c29080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c29080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c29080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cb83c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cb83c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cb83c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cb83c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c274e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c274e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c274e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c274e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c59f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c59f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c59f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c59f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c599e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c599e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c599e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c599e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c596a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c596a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c596a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c596a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bef4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bef4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bef4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bef4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708befb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708befb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708befb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708befb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cd6a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cd6a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cd6a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708cd6a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c29b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c29b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c29b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c29b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bb8780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bb8780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bb8780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bb8780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708bb8ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708bb8ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708bb8ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708bb8ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c5f3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c5f3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c5f3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708c5f3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c5f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c5f1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c5f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708c5f1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bafbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bafbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bafbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bafbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708bdf358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708bdf358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708bdf358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708bdf358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bdf9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bdf9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bdf9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0708bdf9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708bdf898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708bdf898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708bdf898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708bdf898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708b0acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708b0acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708b0acf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708b0acf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708b0ad68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708b0ad68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708b0ad68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f0708b0ad68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From train.py:66: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From train.py:66: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING - tensorflow - From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING:tensorflow:From train.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING - tensorflow - From train.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING:tensorflow:From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING - tensorflow - From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING - tensorflow - From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING - tensorflow - From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

2019-11-04 01:58:12.053682: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-04 01:58:12.058471: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-11-04 01:58:12.143183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-04 01:58:12.143646: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556fce24c230 executing computations on platform CUDA. Devices:
2019-11-04 01:58:12.143662: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-11-04 01:58:12.161834: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz
2019-11-04 01:58:12.162166: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556fce255a60 executing computations on platform Host. Devices:
2019-11-04 01:58:12.162181: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-11-04 01:58:12.162308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-04 01:58:12.162681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-11-04 01:58:12.163006: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-04 01:58:12.164416: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-11-04 01:58:12.165271: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-11-04 01:58:12.166155: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-11-04 01:58:12.167636: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-11-04 01:58:12.168613: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-11-04 01:58:12.170658: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-11-04 01:58:12.170744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-04 01:58:12.171164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-04 01:58:12.171548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-11-04 01:58:12.171575: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-04 01:58:12.172194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-04 01:58:12.172206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-11-04 01:58:12.172211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-11-04 01:58:12.172337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-04 01:58:12.172726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-04 01:58:12.173097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6819 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
INFO - root - Restore from last checkpoint: Logs/bisenet/checkpoints/bisenet-v2/model.ckpt-105000
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from Logs/bisenet/checkpoints/bisenet-v2/model.ckpt-105000
INFO - tensorflow - Restoring parameters from Logs/bisenet/checkpoints/bisenet-v2/model.ckpt-105000
WARNING:tensorflow:From train.py:161: The name tf.train.global_step is deprecated. Please use tf.compat.v1.train.global_step instead.

WARNING - tensorflow - From train.py:161: The name tf.train.global_step is deprecated. Please use tf.compat.v1.train.global_step instead.

INFO - root - Train for 6000000 steps
2019-11-04 01:58:18.549311: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
INFO - root - 2019-11-04 01:58:20.578268: step 105010, total loss = 0.28, predict loss = 0.06 (77.0 examples/sec; 0.052 sec/batch; 85h:04m:27s remains)
2019-11-04 01:58:22.095525: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
INFO - root - 2019-11-04 01:58:23.577552: step 105020, total loss = 0.36, predict loss = 0.08 (80.7 examples/sec; 0.050 sec/batch; 81h:08m:20s remains)
INFO - root - 2019-11-04 01:58:24.154016: step 105030, total loss = 0.36, predict loss = 0.08 (74.7 examples/sec; 0.054 sec/batch; 87h:41m:59s remains)
INFO - root - 2019-11-04 01:58:24.740265: step 105040, total loss = 0.46, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 97h:57m:16s remains)
INFO - root - 2019-11-04 01:58:25.350262: step 105050, total loss = 0.51, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 95h:02m:27s remains)
INFO - root - 2019-11-04 01:58:25.962417: step 105060, total loss = 0.61, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 90h:45m:57s remains)
INFO - root - 2019-11-04 01:58:26.599664: step 105070, total loss = 0.48, predict loss = 0.11 (62.0 examples/sec; 0.065 sec/batch; 105h:39m:24s remains)
INFO - root - 2019-11-04 01:58:27.275916: step 105080, total loss = 0.53, predict loss = 0.12 (75.8 examples/sec; 0.053 sec/batch; 86h:25m:47s remains)
INFO - root - 2019-11-04 01:58:27.935497: step 105090, total loss = 0.32, predict loss = 0.06 (67.1 examples/sec; 0.060 sec/batch; 97h:33m:08s remains)
INFO - root - 2019-11-04 01:58:28.564068: step 105100, total loss = 0.66, predict loss = 0.15 (77.4 examples/sec; 0.052 sec/batch; 84h:36m:25s remains)
INFO - root - 2019-11-04 01:58:29.212564: step 105110, total loss = 0.47, predict loss = 0.10 (65.8 examples/sec; 0.061 sec/batch; 99h:29m:08s remains)
INFO - root - 2019-11-04 01:58:29.815169: step 105120, total loss = 0.61, predict loss = 0.14 (67.6 examples/sec; 0.059 sec/batch; 96h:52m:59s remains)
INFO - root - 2019-11-04 01:58:30.454309: step 105130, total loss = 0.37, predict loss = 0.08 (68.1 examples/sec; 0.059 sec/batch; 96h:08m:37s remains)
INFO - root - 2019-11-04 01:58:31.063789: step 105140, total loss = 0.40, predict loss = 0.09 (71.5 examples/sec; 0.056 sec/batch; 91h:36m:31s remains)
INFO - root - 2019-11-04 01:58:31.687085: step 105150, total loss = 0.46, predict loss = 0.10 (62.0 examples/sec; 0.065 sec/batch; 105h:37m:33s remains)
INFO - root - 2019-11-04 01:58:32.336041: step 105160, total loss = 0.44, predict loss = 0.10 (62.1 examples/sec; 0.064 sec/batch; 105h:27m:00s remains)
INFO - root - 2019-11-04 01:58:32.948649: step 105170, total loss = 0.40, predict loss = 0.09 (81.0 examples/sec; 0.049 sec/batch; 80h:53m:04s remains)
INFO - root - 2019-11-04 01:58:33.570652: step 105180, total loss = 0.48, predict loss = 0.11 (64.9 examples/sec; 0.062 sec/batch; 100h:51m:52s remains)
INFO - root - 2019-11-04 01:58:34.201696: step 105190, total loss = 0.48, predict loss = 0.11 (66.1 examples/sec; 0.060 sec/batch; 99h:01m:51s remains)
INFO - root - 2019-11-04 01:58:34.837732: step 105200, total loss = 0.40, predict loss = 0.08 (70.7 examples/sec; 0.057 sec/batch; 92h:35m:55s remains)
INFO - root - 2019-11-04 01:58:35.498310: step 105210, total loss = 0.53, predict loss = 0.12 (72.3 examples/sec; 0.055 sec/batch; 90h:36m:53s remains)
INFO - root - 2019-11-04 01:58:36.134583: step 105220, total loss = 0.47, predict loss = 0.11 (73.8 examples/sec; 0.054 sec/batch; 88h:43m:14s remains)
INFO - root - 2019-11-04 01:58:36.771643: step 105230, total loss = 0.33, predict loss = 0.07 (73.1 examples/sec; 0.055 sec/batch; 89h:34m:31s remains)
INFO - root - 2019-11-04 01:58:37.449538: step 105240, total loss = 0.54, predict loss = 0.12 (68.2 examples/sec; 0.059 sec/batch; 96h:00m:50s remains)
INFO - root - 2019-11-04 01:58:38.073467: step 105250, total loss = 0.39, predict loss = 0.08 (77.8 examples/sec; 0.051 sec/batch; 84h:10m:53s remains)
INFO - root - 2019-11-04 01:58:38.717569: step 105260, total loss = 0.49, predict loss = 0.11 (65.6 examples/sec; 0.061 sec/batch; 99h:50m:39s remains)
INFO - root - 2019-11-04 01:58:39.346351: step 105270, total loss = 0.51, predict loss = 0.11 (66.6 examples/sec; 0.060 sec/batch; 98h:22m:44s remains)
INFO - root - 2019-11-04 01:58:40.041565: step 105280, total loss = 0.56, predict loss = 0.13 (60.6 examples/sec; 0.066 sec/batch; 108h:04m:43s remains)
INFO - root - 2019-11-04 01:58:40.689888: step 105290, total loss = 0.65, predict loss = 0.16 (65.5 examples/sec; 0.061 sec/batch; 100h:03m:20s remains)
INFO - root - 2019-11-04 01:58:41.296463: step 105300, total loss = 0.45, predict loss = 0.10 (72.0 examples/sec; 0.056 sec/batch; 91h:00m:43s remains)
INFO - root - 2019-11-04 01:58:41.893284: step 105310, total loss = 0.52, predict loss = 0.12 (78.8 examples/sec; 0.051 sec/batch; 83h:07m:04s remains)
INFO - root - 2019-11-04 01:58:42.496477: step 105320, total loss = 0.60, predict loss = 0.14 (75.4 examples/sec; 0.053 sec/batch; 86h:53m:20s remains)
INFO - root - 2019-11-04 01:58:43.101999: step 105330, total loss = 0.53, predict loss = 0.12 (68.1 examples/sec; 0.059 sec/batch; 96h:12m:25s remains)
INFO - root - 2019-11-04 01:58:43.756568: step 105340, total loss = 0.54, predict loss = 0.12 (69.2 examples/sec; 0.058 sec/batch; 94h:39m:03s remains)
INFO - root - 2019-11-04 01:58:44.394985: step 105350, total loss = 0.61, predict loss = 0.14 (59.7 examples/sec; 0.067 sec/batch; 109h:37m:35s remains)
INFO - root - 2019-11-04 01:58:45.100999: step 105360, total loss = 0.53, predict loss = 0.12 (63.1 examples/sec; 0.063 sec/batch; 103h:48m:13s remains)
INFO - root - 2019-11-04 01:58:45.755125: step 105370, total loss = 0.40, predict loss = 0.09 (70.1 examples/sec; 0.057 sec/batch; 93h:25m:54s remains)
INFO - root - 2019-11-04 01:58:46.394529: step 105380, total loss = 0.48, predict loss = 0.11 (64.8 examples/sec; 0.062 sec/batch; 101h:03m:38s remains)
INFO - root - 2019-11-04 01:58:47.013749: step 105390, total loss = 0.49, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 97h:52m:00s remains)
INFO - root - 2019-11-04 01:58:47.655556: step 105400, total loss = 0.45, predict loss = 0.10 (68.1 examples/sec; 0.059 sec/batch; 96h:10m:59s remains)
INFO - root - 2019-11-04 01:58:48.298485: step 105410, total loss = 0.42, predict loss = 0.09 (72.6 examples/sec; 0.055 sec/batch; 90h:11m:51s remains)
INFO - root - 2019-11-04 01:58:48.889874: step 105420, total loss = 0.56, predict loss = 0.13 (70.6 examples/sec; 0.057 sec/batch; 92h:42m:34s remains)
INFO - root - 2019-11-04 01:58:49.520026: step 105430, total loss = 0.53, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 99h:26m:52s remains)
INFO - root - 2019-11-04 01:58:50.170494: step 105440, total loss = 0.62, predict loss = 0.15 (67.0 examples/sec; 0.060 sec/batch; 97h:42m:29s remains)
INFO - root - 2019-11-04 01:58:50.790012: step 105450, total loss = 0.39, predict loss = 0.09 (83.2 examples/sec; 0.048 sec/batch; 78h:45m:50s remains)
INFO - root - 2019-11-04 01:58:51.393942: step 105460, total loss = 0.51, predict loss = 0.13 (80.5 examples/sec; 0.050 sec/batch; 81h:24m:35s remains)
INFO - root - 2019-11-04 01:58:51.978650: step 105470, total loss = 0.35, predict loss = 0.08 (81.9 examples/sec; 0.049 sec/batch; 79h:57m:59s remains)
INFO - root - 2019-11-04 01:58:52.586767: step 105480, total loss = 0.40, predict loss = 0.09 (69.3 examples/sec; 0.058 sec/batch; 94h:29m:21s remains)
INFO - root - 2019-11-04 01:58:53.255566: step 105490, total loss = 0.51, predict loss = 0.13 (69.4 examples/sec; 0.058 sec/batch; 94h:23m:37s remains)
INFO - root - 2019-11-04 01:58:53.876520: step 105500, total loss = 0.57, predict loss = 0.13 (78.0 examples/sec; 0.051 sec/batch; 83h:56m:59s remains)
INFO - root - 2019-11-04 01:58:54.505936: step 105510, total loss = 0.57, predict loss = 0.14 (69.9 examples/sec; 0.057 sec/batch; 93h:39m:00s remains)
INFO - root - 2019-11-04 01:58:55.127177: step 105520, total loss = 0.59, predict loss = 0.14 (75.7 examples/sec; 0.053 sec/batch; 86h:31m:43s remains)
INFO - root - 2019-11-04 01:58:55.768416: step 105530, total loss = 0.54, predict loss = 0.14 (73.7 examples/sec; 0.054 sec/batch; 88h:51m:13s remains)
INFO - root - 2019-11-04 01:58:56.381678: step 105540, total loss = 0.48, predict loss = 0.12 (66.8 examples/sec; 0.060 sec/batch; 97h:58m:40s remains)
INFO - root - 2019-11-04 01:58:56.961472: step 105550, total loss = 0.31, predict loss = 0.06 (81.2 examples/sec; 0.049 sec/batch; 80h:40m:49s remains)
INFO - root - 2019-11-04 01:58:57.575232: step 105560, total loss = 0.32, predict loss = 0.08 (72.6 examples/sec; 0.055 sec/batch; 90h:13m:14s remains)
INFO - root - 2019-11-04 01:58:58.214455: step 105570, total loss = 0.26, predict loss = 0.06 (74.4 examples/sec; 0.054 sec/batch; 88h:04m:47s remains)
INFO - root - 2019-11-04 01:58:58.821332: step 105580, total loss = 0.38, predict loss = 0.09 (79.8 examples/sec; 0.050 sec/batch; 82h:06m:41s remains)
INFO - root - 2019-11-04 01:58:59.463206: step 105590, total loss = 0.75, predict loss = 0.23 (67.3 examples/sec; 0.059 sec/batch; 97h:15m:00s remains)
INFO - root - 2019-11-04 01:59:00.110273: step 105600, total loss = 0.33, predict loss = 0.08 (73.3 examples/sec; 0.055 sec/batch; 89h:20m:13s remains)
INFO - root - 2019-11-04 01:59:00.772440: step 105610, total loss = 0.34, predict loss = 0.07 (68.6 examples/sec; 0.058 sec/batch; 95h:32m:09s remains)
INFO - root - 2019-11-04 01:59:01.414682: step 105620, total loss = 0.35, predict loss = 0.07 (72.7 examples/sec; 0.055 sec/batch; 90h:06m:15s remains)
INFO - root - 2019-11-04 01:59:02.030480: step 105630, total loss = 0.39, predict loss = 0.08 (77.8 examples/sec; 0.051 sec/batch; 84h:10m:40s remains)
INFO - root - 2019-11-04 01:59:02.637066: step 105640, total loss = 0.50, predict loss = 0.12 (82.4 examples/sec; 0.049 sec/batch; 79h:30m:46s remains)
INFO - root - 2019-11-04 01:59:03.261771: step 105650, total loss = 0.45, predict loss = 0.11 (65.5 examples/sec; 0.061 sec/batch; 99h:58m:04s remains)
INFO - root - 2019-11-04 01:59:03.912418: step 105660, total loss = 0.51, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 93h:46m:42s remains)
INFO - root - 2019-11-04 01:59:04.544919: step 105670, total loss = 0.47, predict loss = 0.11 (75.3 examples/sec; 0.053 sec/batch; 86h:58m:57s remains)
INFO - root - 2019-11-04 01:59:05.150191: step 105680, total loss = 0.51, predict loss = 0.11 (79.5 examples/sec; 0.050 sec/batch; 82h:22m:25s remains)
INFO - root - 2019-11-04 01:59:05.776529: step 105690, total loss = 0.40, predict loss = 0.09 (64.2 examples/sec; 0.062 sec/batch; 102h:00m:25s remains)
INFO - root - 2019-11-04 01:59:06.403912: step 105700, total loss = 0.50, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 98h:28m:20s remains)
INFO - root - 2019-11-04 01:59:07.016287: step 105710, total loss = 0.43, predict loss = 0.10 (73.2 examples/sec; 0.055 sec/batch; 89h:28m:18s remains)
INFO - root - 2019-11-04 01:59:07.633350: step 105720, total loss = 0.50, predict loss = 0.11 (68.6 examples/sec; 0.058 sec/batch; 95h:28m:31s remains)
INFO - root - 2019-11-04 01:59:08.266759: step 105730, total loss = 0.53, predict loss = 0.13 (71.2 examples/sec; 0.056 sec/batch; 91h:56m:42s remains)
INFO - root - 2019-11-04 01:59:08.945270: step 105740, total loss = 0.50, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 97h:55m:18s remains)
INFO - root - 2019-11-04 01:59:09.598827: step 105750, total loss = 0.32, predict loss = 0.07 (72.6 examples/sec; 0.055 sec/batch; 90h:15m:23s remains)
INFO - root - 2019-11-04 01:59:10.223072: step 105760, total loss = 0.54, predict loss = 0.12 (76.4 examples/sec; 0.052 sec/batch; 85h:43m:37s remains)
INFO - root - 2019-11-04 01:59:10.876768: step 105770, total loss = 0.32, predict loss = 0.07 (75.6 examples/sec; 0.053 sec/batch; 86h:38m:33s remains)
INFO - root - 2019-11-04 01:59:11.542234: step 105780, total loss = 0.41, predict loss = 0.09 (73.2 examples/sec; 0.055 sec/batch; 89h:28m:22s remains)
INFO - root - 2019-11-04 01:59:12.167625: step 105790, total loss = 0.45, predict loss = 0.10 (73.3 examples/sec; 0.055 sec/batch; 89h:21m:50s remains)
INFO - root - 2019-11-04 01:59:12.810911: step 105800, total loss = 0.55, predict loss = 0.13 (70.0 examples/sec; 0.057 sec/batch; 93h:34m:39s remains)
INFO - root - 2019-11-04 01:59:13.430471: step 105810, total loss = 0.39, predict loss = 0.09 (77.0 examples/sec; 0.052 sec/batch; 85h:04m:06s remains)
INFO - root - 2019-11-04 01:59:14.041166: step 105820, total loss = 0.34, predict loss = 0.07 (72.3 examples/sec; 0.055 sec/batch; 90h:34m:19s remains)
INFO - root - 2019-11-04 01:59:14.696961: step 105830, total loss = 0.34, predict loss = 0.08 (71.1 examples/sec; 0.056 sec/batch; 92h:04m:07s remains)
INFO - root - 2019-11-04 01:59:15.332044: step 105840, total loss = 0.33, predict loss = 0.07 (73.3 examples/sec; 0.055 sec/batch; 89h:23m:50s remains)
INFO - root - 2019-11-04 01:59:16.001673: step 105850, total loss = 0.30, predict loss = 0.06 (71.3 examples/sec; 0.056 sec/batch; 91h:54m:58s remains)
INFO - root - 2019-11-04 01:59:16.630604: step 105860, total loss = 0.33, predict loss = 0.07 (79.3 examples/sec; 0.050 sec/batch; 82h:35m:50s remains)
INFO - root - 2019-11-04 01:59:17.277899: step 105870, total loss = 0.27, predict loss = 0.06 (67.3 examples/sec; 0.059 sec/batch; 97h:17m:33s remains)
INFO - root - 2019-11-04 01:59:17.914470: step 105880, total loss = 0.24, predict loss = 0.05 (64.7 examples/sec; 0.062 sec/batch; 101h:17m:17s remains)
INFO - root - 2019-11-04 01:59:18.538324: step 105890, total loss = 0.46, predict loss = 0.10 (69.1 examples/sec; 0.058 sec/batch; 94h:49m:36s remains)
INFO - root - 2019-11-04 01:59:19.173347: step 105900, total loss = 0.46, predict loss = 0.11 (76.9 examples/sec; 0.052 sec/batch; 85h:07m:14s remains)
INFO - root - 2019-11-04 01:59:19.818255: step 105910, total loss = 0.46, predict loss = 0.10 (67.7 examples/sec; 0.059 sec/batch; 96h:40m:40s remains)
INFO - root - 2019-11-04 01:59:20.448788: step 105920, total loss = 0.55, predict loss = 0.12 (71.0 examples/sec; 0.056 sec/batch; 92h:10m:48s remains)
INFO - root - 2019-11-04 01:59:21.058439: step 105930, total loss = 0.24, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 83h:52m:17s remains)
INFO - root - 2019-11-04 01:59:21.711627: step 105940, total loss = 0.46, predict loss = 0.11 (65.7 examples/sec; 0.061 sec/batch; 99h:37m:32s remains)
INFO - root - 2019-11-04 01:59:22.363210: step 105950, total loss = 0.38, predict loss = 0.08 (76.6 examples/sec; 0.052 sec/batch; 85h:32m:36s remains)
INFO - root - 2019-11-04 01:59:23.021316: step 105960, total loss = 0.36, predict loss = 0.08 (67.1 examples/sec; 0.060 sec/batch; 97h:32m:47s remains)
INFO - root - 2019-11-04 01:59:23.676239: step 105970, total loss = 0.40, predict loss = 0.09 (70.2 examples/sec; 0.057 sec/batch; 93h:17m:42s remains)
INFO - root - 2019-11-04 01:59:24.301696: step 105980, total loss = 0.30, predict loss = 0.07 (67.0 examples/sec; 0.060 sec/batch; 97h:41m:31s remains)
INFO - root - 2019-11-04 01:59:24.949322: step 105990, total loss = 0.42, predict loss = 0.09 (66.6 examples/sec; 0.060 sec/batch; 98h:15m:34s remains)
INFO - root - 2019-11-04 01:59:25.617078: step 106000, total loss = 0.39, predict loss = 0.09 (66.1 examples/sec; 0.061 sec/batch; 99h:05m:07s remains)
INFO - root - 2019-11-04 01:59:26.246239: step 106010, total loss = 0.46, predict loss = 0.11 (68.8 examples/sec; 0.058 sec/batch; 95h:07m:30s remains)
INFO - root - 2019-11-04 01:59:26.879397: step 106020, total loss = 0.66, predict loss = 0.16 (70.8 examples/sec; 0.056 sec/batch; 92h:26m:20s remains)
INFO - root - 2019-11-04 01:59:27.529802: step 106030, total loss = 0.47, predict loss = 0.11 (69.2 examples/sec; 0.058 sec/batch; 94h:37m:04s remains)
INFO - root - 2019-11-04 01:59:28.137426: step 106040, total loss = 0.49, predict loss = 0.11 (71.2 examples/sec; 0.056 sec/batch; 92h:00m:53s remains)
INFO - root - 2019-11-04 01:59:28.749505: step 106050, total loss = 0.40, predict loss = 0.09 (65.7 examples/sec; 0.061 sec/batch; 99h:38m:04s remains)
INFO - root - 2019-11-04 01:59:29.383739: step 106060, total loss = 0.57, predict loss = 0.14 (80.7 examples/sec; 0.050 sec/batch; 81h:10m:05s remains)
INFO - root - 2019-11-04 01:59:30.003619: step 106070, total loss = 0.35, predict loss = 0.08 (73.8 examples/sec; 0.054 sec/batch; 88h:40m:44s remains)
INFO - root - 2019-11-04 01:59:30.613744: step 106080, total loss = 0.42, predict loss = 0.09 (73.3 examples/sec; 0.055 sec/batch; 89h:21m:56s remains)
INFO - root - 2019-11-04 01:59:31.299904: step 106090, total loss = 0.29, predict loss = 0.06 (68.8 examples/sec; 0.058 sec/batch; 95h:09m:05s remains)
INFO - root - 2019-11-04 01:59:31.963189: step 106100, total loss = 0.20, predict loss = 0.04 (71.9 examples/sec; 0.056 sec/batch; 91h:05m:56s remains)
INFO - root - 2019-11-04 01:59:32.608376: step 106110, total loss = 0.21, predict loss = 0.04 (83.9 examples/sec; 0.048 sec/batch; 78h:03m:59s remains)
INFO - root - 2019-11-04 01:59:33.216001: step 106120, total loss = 0.24, predict loss = 0.05 (68.9 examples/sec; 0.058 sec/batch; 95h:01m:14s remains)
INFO - root - 2019-11-04 01:59:33.808789: step 106130, total loss = 0.33, predict loss = 0.07 (73.5 examples/sec; 0.054 sec/batch; 89h:03m:01s remains)
INFO - root - 2019-11-04 01:59:34.418548: step 106140, total loss = 0.58, predict loss = 0.14 (77.7 examples/sec; 0.051 sec/batch; 84h:17m:56s remains)
INFO - root - 2019-11-04 01:59:35.091337: step 106150, total loss = 0.45, predict loss = 0.10 (65.8 examples/sec; 0.061 sec/batch; 99h:29m:04s remains)
INFO - root - 2019-11-04 01:59:35.731857: step 106160, total loss = 0.56, predict loss = 0.13 (75.0 examples/sec; 0.053 sec/batch; 87h:17m:36s remains)
INFO - root - 2019-11-04 01:59:36.364518: step 106170, total loss = 0.57, predict loss = 0.13 (76.9 examples/sec; 0.052 sec/batch; 85h:08m:30s remains)
INFO - root - 2019-11-04 01:59:37.002020: step 106180, total loss = 0.29, predict loss = 0.07 (66.3 examples/sec; 0.060 sec/batch; 98h:48m:58s remains)
INFO - root - 2019-11-04 01:59:37.596313: step 106190, total loss = 0.32, predict loss = 0.07 (76.8 examples/sec; 0.052 sec/batch; 85h:17m:13s remains)
INFO - root - 2019-11-04 01:59:38.176468: step 106200, total loss = 0.33, predict loss = 0.07 (80.5 examples/sec; 0.050 sec/batch; 81h:22m:35s remains)
INFO - root - 2019-11-04 01:59:38.798373: step 106210, total loss = 0.38, predict loss = 0.09 (74.0 examples/sec; 0.054 sec/batch; 88h:26m:44s remains)
INFO - root - 2019-11-04 01:59:39.456298: step 106220, total loss = 0.42, predict loss = 0.10 (66.3 examples/sec; 0.060 sec/batch; 98h:46m:54s remains)
INFO - root - 2019-11-04 01:59:40.101889: step 106230, total loss = 0.46, predict loss = 0.10 (75.6 examples/sec; 0.053 sec/batch; 86h:40m:25s remains)
INFO - root - 2019-11-04 01:59:40.732000: step 106240, total loss = 0.33, predict loss = 0.07 (78.6 examples/sec; 0.051 sec/batch; 83h:16m:28s remains)
INFO - root - 2019-11-04 01:59:41.354414: step 106250, total loss = 0.51, predict loss = 0.12 (67.4 examples/sec; 0.059 sec/batch; 97h:09m:19s remains)
INFO - root - 2019-11-04 01:59:41.970198: step 106260, total loss = 0.33, predict loss = 0.07 (67.3 examples/sec; 0.059 sec/batch; 97h:19m:53s remains)
INFO - root - 2019-11-04 01:59:42.588039: step 106270, total loss = 0.47, predict loss = 0.11 (78.0 examples/sec; 0.051 sec/batch; 83h:59m:10s remains)
INFO - root - 2019-11-04 01:59:43.246417: step 106280, total loss = 0.46, predict loss = 0.11 (65.5 examples/sec; 0.061 sec/batch; 99h:57m:31s remains)
INFO - root - 2019-11-04 01:59:43.905047: step 106290, total loss = 0.36, predict loss = 0.08 (66.0 examples/sec; 0.061 sec/batch; 99h:15m:00s remains)
INFO - root - 2019-11-04 01:59:44.562939: step 106300, total loss = 0.46, predict loss = 0.10 (63.9 examples/sec; 0.063 sec/batch; 102h:25m:31s remains)
INFO - root - 2019-11-04 01:59:45.211793: step 106310, total loss = 0.43, predict loss = 0.09 (83.0 examples/sec; 0.048 sec/batch; 78h:55m:30s remains)
INFO - root - 2019-11-04 01:59:45.820499: step 106320, total loss = 0.46, predict loss = 0.10 (73.2 examples/sec; 0.055 sec/batch; 89h:28m:29s remains)
INFO - root - 2019-11-04 01:59:46.427885: step 106330, total loss = 0.46, predict loss = 0.10 (81.1 examples/sec; 0.049 sec/batch; 80h:43m:21s remains)
INFO - root - 2019-11-04 01:59:47.061801: step 106340, total loss = 0.18, predict loss = 0.03 (67.7 examples/sec; 0.059 sec/batch; 96h:44m:57s remains)
INFO - root - 2019-11-04 01:59:47.667122: step 106350, total loss = 0.38, predict loss = 0.08 (74.2 examples/sec; 0.054 sec/batch; 88h:13m:13s remains)
INFO - root - 2019-11-04 01:59:48.262996: step 106360, total loss = 0.31, predict loss = 0.07 (81.5 examples/sec; 0.049 sec/batch; 80h:22m:44s remains)
INFO - root - 2019-11-04 01:59:48.864621: step 106370, total loss = 0.46, predict loss = 0.11 (87.9 examples/sec; 0.046 sec/batch; 74h:31m:25s remains)
INFO - root - 2019-11-04 01:59:49.474585: step 106380, total loss = 0.45, predict loss = 0.10 (85.6 examples/sec; 0.047 sec/batch; 76h:29m:45s remains)
INFO - root - 2019-11-04 01:59:50.086749: step 106390, total loss = 0.45, predict loss = 0.10 (60.5 examples/sec; 0.066 sec/batch; 108h:10m:54s remains)
INFO - root - 2019-11-04 01:59:50.727941: step 106400, total loss = 0.48, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 89h:43m:33s remains)
INFO - root - 2019-11-04 01:59:51.335145: step 106410, total loss = 0.63, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 93h:07m:04s remains)
INFO - root - 2019-11-04 01:59:51.991851: step 106420, total loss = 0.54, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 98h:25m:47s remains)
INFO - root - 2019-11-04 01:59:52.639758: step 106430, total loss = 0.63, predict loss = 0.16 (66.7 examples/sec; 0.060 sec/batch; 98h:08m:21s remains)
INFO - root - 2019-11-04 01:59:53.297769: step 106440, total loss = 0.63, predict loss = 0.15 (69.3 examples/sec; 0.058 sec/batch; 94h:30m:36s remains)
INFO - root - 2019-11-04 01:59:53.897568: step 106450, total loss = 0.87, predict loss = 0.21 (81.1 examples/sec; 0.049 sec/batch; 80h:42m:41s remains)
INFO - root - 2019-11-04 01:59:54.484006: step 106460, total loss = 0.93, predict loss = 0.22 (81.1 examples/sec; 0.049 sec/batch; 80h:45m:25s remains)
INFO - root - 2019-11-04 01:59:55.143391: step 106470, total loss = 0.63, predict loss = 0.15 (78.4 examples/sec; 0.051 sec/batch; 83h:33m:25s remains)
INFO - root - 2019-11-04 01:59:55.778704: step 106480, total loss = 0.78, predict loss = 0.19 (71.9 examples/sec; 0.056 sec/batch; 91h:02m:19s remains)
INFO - root - 2019-11-04 01:59:56.420570: step 106490, total loss = 0.70, predict loss = 0.17 (61.7 examples/sec; 0.065 sec/batch; 106h:08m:09s remains)
INFO - root - 2019-11-04 01:59:57.041523: step 106500, total loss = 0.64, predict loss = 0.15 (68.4 examples/sec; 0.058 sec/batch; 95h:42m:35s remains)
INFO - root - 2019-11-04 01:59:57.670100: step 106510, total loss = 0.65, predict loss = 0.15 (73.1 examples/sec; 0.055 sec/batch; 89h:36m:35s remains)
INFO - root - 2019-11-04 01:59:58.288350: step 106520, total loss = 0.67, predict loss = 0.16 (74.6 examples/sec; 0.054 sec/batch; 87h:49m:08s remains)
INFO - root - 2019-11-04 01:59:58.892906: step 106530, total loss = 0.49, predict loss = 0.10 (74.0 examples/sec; 0.054 sec/batch; 88h:29m:32s remains)
INFO - root - 2019-11-04 01:59:59.496483: step 106540, total loss = 0.58, predict loss = 0.13 (71.8 examples/sec; 0.056 sec/batch; 91h:12m:36s remains)
INFO - root - 2019-11-04 02:00:00.120947: step 106550, total loss = 0.53, predict loss = 0.12 (72.5 examples/sec; 0.055 sec/batch; 90h:18m:01s remains)
INFO - root - 2019-11-04 02:00:00.766663: step 106560, total loss = 0.57, predict loss = 0.13 (64.1 examples/sec; 0.062 sec/batch; 102h:14m:07s remains)
INFO - root - 2019-11-04 02:00:01.440209: step 106570, total loss = 0.49, predict loss = 0.11 (69.5 examples/sec; 0.058 sec/batch; 94h:14m:46s remains)
INFO - root - 2019-11-04 02:00:02.165191: step 106580, total loss = 0.42, predict loss = 0.10 (63.9 examples/sec; 0.063 sec/batch; 102h:29m:36s remains)
INFO - root - 2019-11-04 02:00:02.833373: step 106590, total loss = 0.54, predict loss = 0.13 (74.5 examples/sec; 0.054 sec/batch; 87h:56m:11s remains)
INFO - root - 2019-11-04 02:00:03.425604: step 106600, total loss = 0.49, predict loss = 0.12 (71.0 examples/sec; 0.056 sec/batch; 92h:14m:19s remains)
INFO - root - 2019-11-04 02:00:04.049095: step 106610, total loss = 0.43, predict loss = 0.10 (77.8 examples/sec; 0.051 sec/batch; 84h:10m:39s remains)
INFO - root - 2019-11-04 02:00:04.690329: step 106620, total loss = 0.51, predict loss = 0.12 (61.5 examples/sec; 0.065 sec/batch; 106h:29m:54s remains)
INFO - root - 2019-11-04 02:00:05.304093: step 106630, total loss = 0.53, predict loss = 0.12 (77.5 examples/sec; 0.052 sec/batch; 84h:30m:45s remains)
INFO - root - 2019-11-04 02:00:05.922522: step 106640, total loss = 0.51, predict loss = 0.12 (77.0 examples/sec; 0.052 sec/batch; 85h:04m:08s remains)
INFO - root - 2019-11-04 02:00:06.580687: step 106650, total loss = 0.57, predict loss = 0.13 (74.0 examples/sec; 0.054 sec/batch; 88h:30m:59s remains)
INFO - root - 2019-11-04 02:00:07.191349: step 106660, total loss = 0.56, predict loss = 0.13 (76.3 examples/sec; 0.052 sec/batch; 85h:50m:21s remains)
INFO - root - 2019-11-04 02:00:07.792646: step 106670, total loss = 0.37, predict loss = 0.08 (78.9 examples/sec; 0.051 sec/batch; 83h:00m:32s remains)
INFO - root - 2019-11-04 02:00:08.401480: step 106680, total loss = 0.47, predict loss = 0.11 (67.8 examples/sec; 0.059 sec/batch; 96h:31m:00s remains)
INFO - root - 2019-11-04 02:00:08.990275: step 106690, total loss = 0.40, predict loss = 0.09 (75.4 examples/sec; 0.053 sec/batch; 86h:50m:13s remains)
INFO - root - 2019-11-04 02:00:09.601990: step 106700, total loss = 0.35, predict loss = 0.08 (69.5 examples/sec; 0.058 sec/batch; 94h:14m:25s remains)
INFO - root - 2019-11-04 02:00:10.232816: step 106710, total loss = 0.38, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 92h:19m:29s remains)
INFO - root - 2019-11-04 02:00:10.857007: step 106720, total loss = 0.42, predict loss = 0.10 (70.2 examples/sec; 0.057 sec/batch; 93h:12m:50s remains)
INFO - root - 2019-11-04 02:00:11.473664: step 106730, total loss = 0.35, predict loss = 0.08 (75.8 examples/sec; 0.053 sec/batch; 86h:25m:06s remains)
INFO - root - 2019-11-04 02:00:12.133343: step 106740, total loss = 0.51, predict loss = 0.12 (67.5 examples/sec; 0.059 sec/batch; 97h:03m:45s remains)
INFO - root - 2019-11-04 02:00:12.753870: step 106750, total loss = 0.42, predict loss = 0.09 (74.0 examples/sec; 0.054 sec/batch; 88h:29m:16s remains)
INFO - root - 2019-11-04 02:00:13.359512: step 106760, total loss = 0.41, predict loss = 0.09 (70.7 examples/sec; 0.057 sec/batch; 92h:40m:52s remains)
INFO - root - 2019-11-04 02:00:14.025341: step 106770, total loss = 0.38, predict loss = 0.09 (69.6 examples/sec; 0.058 sec/batch; 94h:07m:52s remains)
INFO - root - 2019-11-04 02:00:14.660841: step 106780, total loss = 0.36, predict loss = 0.08 (69.6 examples/sec; 0.057 sec/batch; 94h:07m:00s remains)
INFO - root - 2019-11-04 02:00:15.314976: step 106790, total loss = 0.41, predict loss = 0.09 (64.4 examples/sec; 0.062 sec/batch; 101h:38m:48s remains)
INFO - root - 2019-11-04 02:00:15.991107: step 106800, total loss = 0.36, predict loss = 0.08 (65.7 examples/sec; 0.061 sec/batch; 99h:42m:15s remains)
INFO - root - 2019-11-04 02:00:16.661238: step 106810, total loss = 0.44, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 98h:26m:13s remains)
INFO - root - 2019-11-04 02:00:17.281258: step 106820, total loss = 0.56, predict loss = 0.12 (83.8 examples/sec; 0.048 sec/batch; 78h:07m:33s remains)
INFO - root - 2019-11-04 02:00:17.909887: step 106830, total loss = 0.53, predict loss = 0.12 (81.6 examples/sec; 0.049 sec/batch; 80h:13m:50s remains)
INFO - root - 2019-11-04 02:00:18.533012: step 106840, total loss = 0.53, predict loss = 0.12 (61.1 examples/sec; 0.065 sec/batch; 107h:10m:15s remains)
INFO - root - 2019-11-04 02:00:19.168460: step 106850, total loss = 0.39, predict loss = 0.09 (76.0 examples/sec; 0.053 sec/batch; 86h:10m:27s remains)
INFO - root - 2019-11-04 02:00:19.818267: step 106860, total loss = 0.60, predict loss = 0.14 (70.5 examples/sec; 0.057 sec/batch; 92h:56m:03s remains)
INFO - root - 2019-11-04 02:00:20.440074: step 106870, total loss = 0.67, predict loss = 0.17 (69.5 examples/sec; 0.058 sec/batch; 94h:12m:27s remains)
INFO - root - 2019-11-04 02:00:21.074104: step 106880, total loss = 0.54, predict loss = 0.13 (61.0 examples/sec; 0.066 sec/batch; 107h:20m:25s remains)
INFO - root - 2019-11-04 02:00:21.716435: step 106890, total loss = 0.55, predict loss = 0.13 (74.1 examples/sec; 0.054 sec/batch; 88h:25m:08s remains)
INFO - root - 2019-11-04 02:00:22.332615: step 106900, total loss = 0.58, predict loss = 0.14 (70.8 examples/sec; 0.057 sec/batch; 92h:31m:27s remains)
INFO - root - 2019-11-04 02:00:22.955148: step 106910, total loss = 0.70, predict loss = 0.16 (80.1 examples/sec; 0.050 sec/batch; 81h:43m:55s remains)
INFO - root - 2019-11-04 02:00:23.592213: step 106920, total loss = 0.52, predict loss = 0.12 (69.8 examples/sec; 0.057 sec/batch; 93h:51m:45s remains)
INFO - root - 2019-11-04 02:00:24.215489: step 106930, total loss = 0.65, predict loss = 0.15 (79.2 examples/sec; 0.050 sec/batch; 82h:39m:15s remains)
INFO - root - 2019-11-04 02:00:24.859690: step 106940, total loss = 0.58, predict loss = 0.13 (64.0 examples/sec; 0.062 sec/batch; 102h:15m:51s remains)
INFO - root - 2019-11-04 02:00:25.513876: step 106950, total loss = 0.43, predict loss = 0.10 (76.5 examples/sec; 0.052 sec/batch; 85h:36m:50s remains)
INFO - root - 2019-11-04 02:00:26.143549: step 106960, total loss = 0.34, predict loss = 0.08 (74.3 examples/sec; 0.054 sec/batch; 88h:09m:51s remains)
INFO - root - 2019-11-04 02:00:26.814413: step 106970, total loss = 0.37, predict loss = 0.09 (61.2 examples/sec; 0.065 sec/batch; 106h:56m:02s remains)
INFO - root - 2019-11-04 02:00:27.488751: step 106980, total loss = 0.49, predict loss = 0.11 (71.9 examples/sec; 0.056 sec/batch; 91h:05m:28s remains)
INFO - root - 2019-11-04 02:00:28.177994: step 106990, total loss = 0.21, predict loss = 0.04 (59.8 examples/sec; 0.067 sec/batch; 109h:35m:06s remains)
INFO - root - 2019-11-04 02:00:28.833314: step 107000, total loss = 0.46, predict loss = 0.11 (60.6 examples/sec; 0.066 sec/batch; 108h:07m:32s remains)
INFO - root - 2019-11-04 02:00:29.459992: step 107010, total loss = 0.42, predict loss = 0.09 (78.5 examples/sec; 0.051 sec/batch; 83h:25m:08s remains)
INFO - root - 2019-11-04 02:00:30.081169: step 107020, total loss = 0.46, predict loss = 0.10 (70.1 examples/sec; 0.057 sec/batch; 93h:27m:11s remains)
INFO - root - 2019-11-04 02:00:30.692173: step 107030, total loss = 0.46, predict loss = 0.10 (70.2 examples/sec; 0.057 sec/batch; 93h:14m:25s remains)
INFO - root - 2019-11-04 02:00:31.326781: step 107040, total loss = 0.34, predict loss = 0.07 (66.7 examples/sec; 0.060 sec/batch; 98h:07m:24s remains)
INFO - root - 2019-11-04 02:00:31.950149: step 107050, total loss = 0.55, predict loss = 0.13 (83.4 examples/sec; 0.048 sec/batch; 78h:30m:06s remains)
INFO - root - 2019-11-04 02:00:32.573468: step 107060, total loss = 0.52, predict loss = 0.11 (76.5 examples/sec; 0.052 sec/batch; 85h:38m:11s remains)
INFO - root - 2019-11-04 02:00:33.199297: step 107070, total loss = 0.54, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 97h:16m:05s remains)
INFO - root - 2019-11-04 02:00:33.836078: step 107080, total loss = 0.46, predict loss = 0.11 (76.8 examples/sec; 0.052 sec/batch; 85h:13m:35s remains)
INFO - root - 2019-11-04 02:00:34.485656: step 107090, total loss = 0.65, predict loss = 0.16 (68.6 examples/sec; 0.058 sec/batch; 95h:24m:05s remains)
INFO - root - 2019-11-04 02:00:35.183950: step 107100, total loss = 0.47, predict loss = 0.11 (70.8 examples/sec; 0.056 sec/batch; 92h:25m:04s remains)
INFO - root - 2019-11-04 02:00:35.891107: step 107110, total loss = 0.67, predict loss = 0.16 (56.4 examples/sec; 0.071 sec/batch; 116h:05m:48s remains)
INFO - root - 2019-11-04 02:00:36.522018: step 107120, total loss = 0.45, predict loss = 0.11 (80.2 examples/sec; 0.050 sec/batch; 81h:40m:29s remains)
INFO - root - 2019-11-04 02:00:37.126057: step 107130, total loss = 0.64, predict loss = 0.16 (73.8 examples/sec; 0.054 sec/batch; 88h:45m:49s remains)
INFO - root - 2019-11-04 02:00:37.764843: step 107140, total loss = 0.58, predict loss = 0.14 (70.7 examples/sec; 0.057 sec/batch; 92h:35m:46s remains)
INFO - root - 2019-11-04 02:00:38.401975: step 107150, total loss = 0.60, predict loss = 0.15 (73.7 examples/sec; 0.054 sec/batch; 88h:53m:23s remains)
INFO - root - 2019-11-04 02:00:39.041487: step 107160, total loss = 0.51, predict loss = 0.12 (80.2 examples/sec; 0.050 sec/batch; 81h:40m:06s remains)
INFO - root - 2019-11-04 02:00:39.660128: step 107170, total loss = 0.55, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 97h:13m:05s remains)
INFO - root - 2019-11-04 02:00:40.309107: step 107180, total loss = 0.42, predict loss = 0.09 (65.3 examples/sec; 0.061 sec/batch; 100h:12m:14s remains)
INFO - root - 2019-11-04 02:00:40.976483: step 107190, total loss = 0.46, predict loss = 0.10 (62.1 examples/sec; 0.064 sec/batch; 105h:28m:14s remains)
INFO - root - 2019-11-04 02:00:41.605592: step 107200, total loss = 0.47, predict loss = 0.11 (71.4 examples/sec; 0.056 sec/batch; 91h:40m:03s remains)
INFO - root - 2019-11-04 02:00:42.236901: step 107210, total loss = 0.39, predict loss = 0.08 (72.7 examples/sec; 0.055 sec/batch; 90h:02m:20s remains)
INFO - root - 2019-11-04 02:00:42.852794: step 107220, total loss = 0.51, predict loss = 0.12 (82.0 examples/sec; 0.049 sec/batch; 79h:49m:26s remains)
INFO - root - 2019-11-04 02:00:43.465469: step 107230, total loss = 0.49, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 98h:40m:09s remains)
INFO - root - 2019-11-04 02:00:44.129590: step 107240, total loss = 0.52, predict loss = 0.13 (63.0 examples/sec; 0.063 sec/batch; 103h:54m:48s remains)
INFO - root - 2019-11-04 02:00:44.800653: step 107250, total loss = 0.45, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:35m:27s remains)
INFO - root - 2019-11-04 02:00:45.463422: step 107260, total loss = 0.50, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 97h:38m:32s remains)
INFO - root - 2019-11-04 02:00:46.092203: step 107270, total loss = 0.48, predict loss = 0.11 (71.4 examples/sec; 0.056 sec/batch; 91h:44m:19s remains)
INFO - root - 2019-11-04 02:00:46.715592: step 107280, total loss = 0.46, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 97h:08m:55s remains)
INFO - root - 2019-11-04 02:00:47.393790: step 107290, total loss = 0.43, predict loss = 0.10 (62.0 examples/sec; 0.064 sec/batch; 105h:32m:01s remains)
INFO - root - 2019-11-04 02:00:47.993423: step 107300, total loss = 0.43, predict loss = 0.10 (72.1 examples/sec; 0.055 sec/batch; 90h:48m:09s remains)
INFO - root - 2019-11-04 02:00:48.620413: step 107310, total loss = 0.50, predict loss = 0.12 (69.1 examples/sec; 0.058 sec/batch; 94h:47m:38s remains)
INFO - root - 2019-11-04 02:00:49.279240: step 107320, total loss = 0.32, predict loss = 0.06 (68.4 examples/sec; 0.058 sec/batch; 95h:42m:13s remains)
INFO - root - 2019-11-04 02:00:49.932561: step 107330, total loss = 0.46, predict loss = 0.11 (82.0 examples/sec; 0.049 sec/batch; 79h:52m:49s remains)
INFO - root - 2019-11-04 02:00:50.585470: step 107340, total loss = 0.47, predict loss = 0.10 (67.8 examples/sec; 0.059 sec/batch; 96h:37m:17s remains)
INFO - root - 2019-11-04 02:00:51.200557: step 107350, total loss = 0.55, predict loss = 0.13 (71.5 examples/sec; 0.056 sec/batch; 91h:33m:31s remains)
INFO - root - 2019-11-04 02:00:51.845630: step 107360, total loss = 0.44, predict loss = 0.10 (66.7 examples/sec; 0.060 sec/batch; 98h:12m:08s remains)
INFO - root - 2019-11-04 02:00:52.506717: step 107370, total loss = 0.49, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 85h:19m:56s remains)
INFO - root - 2019-11-04 02:00:53.115664: step 107380, total loss = 0.49, predict loss = 0.11 (70.7 examples/sec; 0.057 sec/batch; 92h:33m:25s remains)
INFO - root - 2019-11-04 02:00:53.727169: step 107390, total loss = 0.62, predict loss = 0.15 (68.6 examples/sec; 0.058 sec/batch; 95h:23m:39s remains)
INFO - root - 2019-11-04 02:00:54.352803: step 107400, total loss = 0.44, predict loss = 0.09 (68.1 examples/sec; 0.059 sec/batch; 96h:08m:21s remains)
INFO - root - 2019-11-04 02:00:55.007324: step 107410, total loss = 0.54, predict loss = 0.14 (73.2 examples/sec; 0.055 sec/batch; 89h:25m:34s remains)
INFO - root - 2019-11-04 02:00:55.628336: step 107420, total loss = 0.46, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 95h:28m:48s remains)
INFO - root - 2019-11-04 02:00:56.274097: step 107430, total loss = 0.50, predict loss = 0.12 (63.4 examples/sec; 0.063 sec/batch; 103h:15m:31s remains)
INFO - root - 2019-11-04 02:00:56.891885: step 107440, total loss = 0.61, predict loss = 0.15 (61.8 examples/sec; 0.065 sec/batch; 105h:53m:25s remains)
INFO - root - 2019-11-04 02:00:57.522068: step 107450, total loss = 0.60, predict loss = 0.15 (85.4 examples/sec; 0.047 sec/batch; 76h:39m:35s remains)
INFO - root - 2019-11-04 02:00:58.138610: step 107460, total loss = 0.70, predict loss = 0.17 (76.3 examples/sec; 0.052 sec/batch; 85h:48m:47s remains)
INFO - root - 2019-11-04 02:00:58.780754: step 107470, total loss = 0.66, predict loss = 0.15 (67.3 examples/sec; 0.059 sec/batch; 97h:16m:26s remains)
INFO - root - 2019-11-04 02:00:59.416116: step 107480, total loss = 0.61, predict loss = 0.14 (75.7 examples/sec; 0.053 sec/batch; 86h:32m:27s remains)
INFO - root - 2019-11-04 02:01:00.042011: step 107490, total loss = 0.64, predict loss = 0.15 (65.4 examples/sec; 0.061 sec/batch; 100h:07m:43s remains)
INFO - root - 2019-11-04 02:01:00.701745: step 107500, total loss = 0.54, predict loss = 0.13 (69.7 examples/sec; 0.057 sec/batch; 93h:59m:43s remains)
INFO - root - 2019-11-04 02:01:01.321638: step 107510, total loss = 0.69, predict loss = 0.16 (67.9 examples/sec; 0.059 sec/batch; 96h:26m:31s remains)
INFO - root - 2019-11-04 02:01:01.952581: step 107520, total loss = 0.74, predict loss = 0.17 (77.4 examples/sec; 0.052 sec/batch; 84h:34m:57s remains)
INFO - root - 2019-11-04 02:01:02.591898: step 107530, total loss = 0.81, predict loss = 0.20 (72.7 examples/sec; 0.055 sec/batch; 90h:04m:42s remains)
INFO - root - 2019-11-04 02:01:03.204055: step 107540, total loss = 0.55, predict loss = 0.12 (76.9 examples/sec; 0.052 sec/batch; 85h:10m:54s remains)
INFO - root - 2019-11-04 02:01:03.807987: step 107550, total loss = 0.48, predict loss = 0.11 (78.8 examples/sec; 0.051 sec/batch; 83h:08m:10s remains)
INFO - root - 2019-11-04 02:01:04.467863: step 107560, total loss = 0.63, predict loss = 0.15 (75.5 examples/sec; 0.053 sec/batch; 86h:46m:09s remains)
INFO - root - 2019-11-04 02:01:05.077846: step 107570, total loss = 0.52, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 90h:41m:10s remains)
INFO - root - 2019-11-04 02:01:05.747687: step 107580, total loss = 0.42, predict loss = 0.09 (68.7 examples/sec; 0.058 sec/batch; 95h:17m:57s remains)
INFO - root - 2019-11-04 02:01:06.400807: step 107590, total loss = 0.68, predict loss = 0.16 (69.1 examples/sec; 0.058 sec/batch; 94h:47m:44s remains)
INFO - root - 2019-11-04 02:01:07.020691: step 107600, total loss = 0.39, predict loss = 0.09 (63.1 examples/sec; 0.063 sec/batch; 103h:45m:17s remains)
INFO - root - 2019-11-04 02:01:07.607387: step 107610, total loss = 0.49, predict loss = 0.11 (74.0 examples/sec; 0.054 sec/batch; 88h:31m:35s remains)
INFO - root - 2019-11-04 02:01:08.214526: step 107620, total loss = 0.56, predict loss = 0.13 (73.6 examples/sec; 0.054 sec/batch; 88h:58m:09s remains)
INFO - root - 2019-11-04 02:01:08.825895: step 107630, total loss = 0.45, predict loss = 0.10 (78.0 examples/sec; 0.051 sec/batch; 83h:56m:54s remains)
INFO - root - 2019-11-04 02:01:09.436538: step 107640, total loss = 0.55, predict loss = 0.12 (74.3 examples/sec; 0.054 sec/batch; 88h:04m:06s remains)
INFO - root - 2019-11-04 02:01:10.097115: step 107650, total loss = 0.56, predict loss = 0.13 (69.4 examples/sec; 0.058 sec/batch; 94h:17m:56s remains)
INFO - root - 2019-11-04 02:01:10.766234: step 107660, total loss = 0.55, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 97h:31m:01s remains)
INFO - root - 2019-11-04 02:01:11.413500: step 107670, total loss = 0.49, predict loss = 0.12 (66.1 examples/sec; 0.060 sec/batch; 99h:00m:34s remains)
INFO - root - 2019-11-04 02:01:12.013278: step 107680, total loss = 0.51, predict loss = 0.11 (68.2 examples/sec; 0.059 sec/batch; 95h:58m:56s remains)
INFO - root - 2019-11-04 02:01:12.664496: step 107690, total loss = 0.53, predict loss = 0.11 (66.6 examples/sec; 0.060 sec/batch; 98h:14m:05s remains)
INFO - root - 2019-11-04 02:01:13.297921: step 107700, total loss = 0.51, predict loss = 0.12 (62.8 examples/sec; 0.064 sec/batch; 104h:15m:36s remains)
INFO - root - 2019-11-04 02:01:13.887866: step 107710, total loss = 0.49, predict loss = 0.11 (99.0 examples/sec; 0.040 sec/batch; 66h:07m:37s remains)
INFO - root - 2019-11-04 02:01:14.356059: step 107720, total loss = 0.52, predict loss = 0.11 (92.5 examples/sec; 0.043 sec/batch; 70h:47m:51s remains)
INFO - root - 2019-11-04 02:01:15.390941: step 107730, total loss = 0.37, predict loss = 0.07 (65.3 examples/sec; 0.061 sec/batch; 100h:18m:46s remains)
INFO - root - 2019-11-04 02:01:16.000145: step 107740, total loss = 0.26, predict loss = 0.05 (73.3 examples/sec; 0.055 sec/batch; 89h:19m:42s remains)
INFO - root - 2019-11-04 02:01:16.613874: step 107750, total loss = 0.56, predict loss = 0.13 (70.9 examples/sec; 0.056 sec/batch; 92h:17m:47s remains)
INFO - root - 2019-11-04 02:01:17.267950: step 107760, total loss = 0.47, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 97h:04m:15s remains)
INFO - root - 2019-11-04 02:01:17.918359: step 107770, total loss = 0.46, predict loss = 0.10 (70.1 examples/sec; 0.057 sec/batch; 93h:20m:44s remains)
INFO - root - 2019-11-04 02:01:18.569570: step 107780, total loss = 0.53, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 94h:35m:27s remains)
INFO - root - 2019-11-04 02:01:19.262198: step 107790, total loss = 0.51, predict loss = 0.12 (64.2 examples/sec; 0.062 sec/batch; 101h:55m:22s remains)
INFO - root - 2019-11-04 02:01:19.940136: step 107800, total loss = 0.65, predict loss = 0.16 (64.5 examples/sec; 0.062 sec/batch; 101h:25m:45s remains)
INFO - root - 2019-11-04 02:01:20.542487: step 107810, total loss = 0.50, predict loss = 0.11 (74.8 examples/sec; 0.053 sec/batch; 87h:33m:48s remains)
INFO - root - 2019-11-04 02:01:21.183785: step 107820, total loss = 0.48, predict loss = 0.11 (69.6 examples/sec; 0.057 sec/batch; 94h:02m:38s remains)
INFO - root - 2019-11-04 02:01:21.834727: step 107830, total loss = 0.64, predict loss = 0.14 (70.1 examples/sec; 0.057 sec/batch; 93h:24m:41s remains)
INFO - root - 2019-11-04 02:01:22.458351: step 107840, total loss = 0.49, predict loss = 0.11 (79.4 examples/sec; 0.050 sec/batch; 82h:26m:53s remains)
INFO - root - 2019-11-04 02:01:23.118224: step 107850, total loss = 0.47, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 93h:58m:43s remains)
INFO - root - 2019-11-04 02:01:23.774221: step 107860, total loss = 0.54, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 93h:13m:56s remains)
INFO - root - 2019-11-04 02:01:24.413827: step 107870, total loss = 0.46, predict loss = 0.10 (75.0 examples/sec; 0.053 sec/batch; 87h:16m:57s remains)
INFO - root - 2019-11-04 02:01:25.061255: step 107880, total loss = 0.41, predict loss = 0.09 (67.0 examples/sec; 0.060 sec/batch; 97h:42m:18s remains)
INFO - root - 2019-11-04 02:01:25.714872: step 107890, total loss = 0.60, predict loss = 0.14 (70.0 examples/sec; 0.057 sec/batch; 93h:30m:08s remains)
INFO - root - 2019-11-04 02:01:26.310637: step 107900, total loss = 0.49, predict loss = 0.11 (75.8 examples/sec; 0.053 sec/batch; 86h:24m:06s remains)
INFO - root - 2019-11-04 02:01:26.940577: step 107910, total loss = 0.41, predict loss = 0.09 (67.9 examples/sec; 0.059 sec/batch; 96h:22m:09s remains)
INFO - root - 2019-11-04 02:01:27.554702: step 107920, total loss = 0.49, predict loss = 0.11 (83.0 examples/sec; 0.048 sec/batch; 78h:54m:47s remains)
INFO - root - 2019-11-04 02:01:28.172604: step 107930, total loss = 0.43, predict loss = 0.09 (77.5 examples/sec; 0.052 sec/batch; 84h:27m:12s remains)
INFO - root - 2019-11-04 02:01:28.793519: step 107940, total loss = 0.47, predict loss = 0.10 (60.8 examples/sec; 0.066 sec/batch; 107h:35m:54s remains)
INFO - root - 2019-11-04 02:01:29.457501: step 107950, total loss = 0.52, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 91h:18m:03s remains)
INFO - root - 2019-11-04 02:01:30.093413: step 107960, total loss = 0.30, predict loss = 0.07 (79.0 examples/sec; 0.051 sec/batch; 82h:50m:58s remains)
INFO - root - 2019-11-04 02:01:30.699478: step 107970, total loss = 0.40, predict loss = 0.09 (69.6 examples/sec; 0.057 sec/batch; 94h:02m:50s remains)
INFO - root - 2019-11-04 02:01:31.365009: step 107980, total loss = 0.44, predict loss = 0.10 (74.9 examples/sec; 0.053 sec/batch; 87h:21m:11s remains)
INFO - root - 2019-11-04 02:01:31.971224: step 107990, total loss = 0.51, predict loss = 0.11 (81.2 examples/sec; 0.049 sec/batch; 80h:37m:46s remains)
INFO - root - 2019-11-04 02:01:32.577667: step 108000, total loss = 0.48, predict loss = 0.11 (67.8 examples/sec; 0.059 sec/batch; 96h:31m:40s remains)
INFO - root - 2019-11-04 02:01:33.263085: step 108010, total loss = 0.57, predict loss = 0.13 (63.8 examples/sec; 0.063 sec/batch; 102h:35m:54s remains)
INFO - root - 2019-11-04 02:01:33.935585: step 108020, total loss = 0.55, predict loss = 0.13 (62.8 examples/sec; 0.064 sec/batch; 104h:16m:50s remains)
INFO - root - 2019-11-04 02:01:34.560874: step 108030, total loss = 0.69, predict loss = 0.17 (75.1 examples/sec; 0.053 sec/batch; 87h:08m:52s remains)
INFO - root - 2019-11-04 02:01:35.176002: step 108040, total loss = 0.48, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 87h:17m:59s remains)
INFO - root - 2019-11-04 02:01:35.803544: step 108050, total loss = 0.55, predict loss = 0.12 (77.0 examples/sec; 0.052 sec/batch; 84h:59m:35s remains)
INFO - root - 2019-11-04 02:01:36.510998: step 108060, total loss = 0.57, predict loss = 0.13 (52.3 examples/sec; 0.077 sec/batch; 125h:16m:52s remains)
INFO - root - 2019-11-04 02:01:37.222837: step 108070, total loss = 0.56, predict loss = 0.14 (70.4 examples/sec; 0.057 sec/batch; 92h:58m:55s remains)
INFO - root - 2019-11-04 02:01:37.925873: step 108080, total loss = 0.63, predict loss = 0.14 (70.7 examples/sec; 0.057 sec/batch; 92h:36m:10s remains)
INFO - root - 2019-11-04 02:01:38.604088: step 108090, total loss = 0.38, predict loss = 0.07 (65.0 examples/sec; 0.062 sec/batch; 100h:39m:32s remains)
INFO - root - 2019-11-04 02:01:39.253958: step 108100, total loss = 0.49, predict loss = 0.11 (61.4 examples/sec; 0.065 sec/batch; 106h:34m:33s remains)
INFO - root - 2019-11-04 02:01:39.892873: step 108110, total loss = 0.35, predict loss = 0.07 (63.0 examples/sec; 0.064 sec/batch; 103h:56m:37s remains)
INFO - root - 2019-11-04 02:01:40.517700: step 108120, total loss = 0.53, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 95h:14m:20s remains)
INFO - root - 2019-11-04 02:01:41.133104: step 108130, total loss = 0.49, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:33m:08s remains)
INFO - root - 2019-11-04 02:01:41.742507: step 108140, total loss = 0.62, predict loss = 0.15 (73.5 examples/sec; 0.054 sec/batch; 89h:05m:52s remains)
INFO - root - 2019-11-04 02:01:42.407004: step 108150, total loss = 0.50, predict loss = 0.11 (61.2 examples/sec; 0.065 sec/batch; 106h:55m:09s remains)
INFO - root - 2019-11-04 02:01:43.067153: step 108160, total loss = 0.52, predict loss = 0.13 (67.6 examples/sec; 0.059 sec/batch; 96h:48m:50s remains)
INFO - root - 2019-11-04 02:01:43.700785: step 108170, total loss = 0.57, predict loss = 0.13 (70.4 examples/sec; 0.057 sec/batch; 93h:01m:11s remains)
INFO - root - 2019-11-04 02:01:44.323467: step 108180, total loss = 0.42, predict loss = 0.10 (72.1 examples/sec; 0.055 sec/batch; 90h:49m:12s remains)
INFO - root - 2019-11-04 02:01:44.943012: step 108190, total loss = 0.31, predict loss = 0.07 (65.0 examples/sec; 0.062 sec/batch; 100h:46m:46s remains)
INFO - root - 2019-11-04 02:01:45.531460: step 108200, total loss = 0.48, predict loss = 0.12 (78.3 examples/sec; 0.051 sec/batch; 83h:34m:17s remains)
INFO - root - 2019-11-04 02:01:46.135105: step 108210, total loss = 0.51, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 95h:07m:47s remains)
INFO - root - 2019-11-04 02:01:46.755702: step 108220, total loss = 0.61, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 92h:46m:57s remains)
INFO - root - 2019-11-04 02:01:47.365925: step 108230, total loss = 0.48, predict loss = 0.12 (75.7 examples/sec; 0.053 sec/batch; 86h:28m:00s remains)
INFO - root - 2019-11-04 02:01:47.967354: step 108240, total loss = 0.47, predict loss = 0.11 (78.4 examples/sec; 0.051 sec/batch; 83h:32m:25s remains)
INFO - root - 2019-11-04 02:01:48.591491: step 108250, total loss = 0.54, predict loss = 0.13 (74.7 examples/sec; 0.054 sec/batch; 87h:41m:37s remains)
INFO - root - 2019-11-04 02:01:49.242038: step 108260, total loss = 0.64, predict loss = 0.15 (61.7 examples/sec; 0.065 sec/batch; 106h:05m:05s remains)
INFO - root - 2019-11-04 02:01:49.846588: step 108270, total loss = 0.40, predict loss = 0.10 (80.1 examples/sec; 0.050 sec/batch; 81h:45m:15s remains)
INFO - root - 2019-11-04 02:01:50.463028: step 108280, total loss = 0.55, predict loss = 0.13 (69.6 examples/sec; 0.057 sec/batch; 94h:03m:24s remains)
INFO - root - 2019-11-04 02:01:51.097045: step 108290, total loss = 0.46, predict loss = 0.12 (68.4 examples/sec; 0.058 sec/batch; 95h:42m:31s remains)
INFO - root - 2019-11-04 02:01:51.707709: step 108300, total loss = 0.57, predict loss = 0.14 (69.0 examples/sec; 0.058 sec/batch; 94h:48m:51s remains)
INFO - root - 2019-11-04 02:01:52.341802: step 108310, total loss = 0.40, predict loss = 0.10 (65.4 examples/sec; 0.061 sec/batch; 100h:04m:37s remains)
INFO - root - 2019-11-04 02:01:52.998948: step 108320, total loss = 0.39, predict loss = 0.09 (65.1 examples/sec; 0.061 sec/batch; 100h:37m:34s remains)
INFO - root - 2019-11-04 02:01:53.632068: step 108330, total loss = 0.24, predict loss = 0.05 (64.5 examples/sec; 0.062 sec/batch; 101h:26m:28s remains)
INFO - root - 2019-11-04 02:01:54.250603: step 108340, total loss = 0.33, predict loss = 0.07 (71.2 examples/sec; 0.056 sec/batch; 91h:58m:21s remains)
INFO - root - 2019-11-04 02:01:54.910638: step 108350, total loss = 0.23, predict loss = 0.04 (66.9 examples/sec; 0.060 sec/batch; 97h:50m:54s remains)
INFO - root - 2019-11-04 02:01:55.501802: step 108360, total loss = 0.35, predict loss = 0.08 (78.7 examples/sec; 0.051 sec/batch; 83h:12m:23s remains)
INFO - root - 2019-11-04 02:01:56.111312: step 108370, total loss = 0.43, predict loss = 0.10 (64.9 examples/sec; 0.062 sec/batch; 100h:48m:42s remains)
INFO - root - 2019-11-04 02:01:56.700316: step 108380, total loss = 0.37, predict loss = 0.08 (78.4 examples/sec; 0.051 sec/batch; 83h:30m:44s remains)
INFO - root - 2019-11-04 02:01:57.296138: step 108390, total loss = 0.47, predict loss = 0.11 (78.7 examples/sec; 0.051 sec/batch; 83h:10m:06s remains)
INFO - root - 2019-11-04 02:01:57.908714: step 108400, total loss = 0.44, predict loss = 0.10 (68.1 examples/sec; 0.059 sec/batch; 96h:10m:30s remains)
INFO - root - 2019-11-04 02:01:58.564497: step 108410, total loss = 0.44, predict loss = 0.09 (78.3 examples/sec; 0.051 sec/batch; 83h:33m:34s remains)
INFO - root - 2019-11-04 02:01:59.177412: step 108420, total loss = 0.66, predict loss = 0.16 (73.8 examples/sec; 0.054 sec/batch; 88h:40m:54s remains)
INFO - root - 2019-11-04 02:01:59.762028: step 108430, total loss = 0.35, predict loss = 0.08 (68.5 examples/sec; 0.058 sec/batch; 95h:34m:28s remains)
INFO - root - 2019-11-04 02:02:00.379057: step 108440, total loss = 0.64, predict loss = 0.16 (74.2 examples/sec; 0.054 sec/batch; 88h:14m:00s remains)
INFO - root - 2019-11-04 02:02:01.040468: step 108450, total loss = 0.69, predict loss = 0.16 (65.4 examples/sec; 0.061 sec/batch; 100h:06m:56s remains)
INFO - root - 2019-11-04 02:02:01.672719: step 108460, total loss = 0.66, predict loss = 0.15 (84.3 examples/sec; 0.047 sec/batch; 77h:38m:20s remains)
INFO - root - 2019-11-04 02:02:02.278436: step 108470, total loss = 0.51, predict loss = 0.11 (74.1 examples/sec; 0.054 sec/batch; 88h:17m:39s remains)
INFO - root - 2019-11-04 02:02:02.871270: step 108480, total loss = 0.48, predict loss = 0.11 (64.3 examples/sec; 0.062 sec/batch; 101h:46m:08s remains)
INFO - root - 2019-11-04 02:02:03.484650: step 108490, total loss = 0.43, predict loss = 0.10 (68.4 examples/sec; 0.058 sec/batch; 95h:43m:00s remains)
INFO - root - 2019-11-04 02:02:04.111516: step 108500, total loss = 0.43, predict loss = 0.10 (71.7 examples/sec; 0.056 sec/batch; 91h:17m:10s remains)
INFO - root - 2019-11-04 02:02:04.745366: step 108510, total loss = 0.49, predict loss = 0.11 (74.1 examples/sec; 0.054 sec/batch; 88h:23m:27s remains)
INFO - root - 2019-11-04 02:02:05.412908: step 108520, total loss = 0.48, predict loss = 0.12 (68.6 examples/sec; 0.058 sec/batch; 95h:22m:31s remains)
INFO - root - 2019-11-04 02:02:06.010637: step 108530, total loss = 0.54, predict loss = 0.14 (76.0 examples/sec; 0.053 sec/batch; 86h:08m:55s remains)
INFO - root - 2019-11-04 02:02:06.619268: step 108540, total loss = 0.53, predict loss = 0.13 (67.4 examples/sec; 0.059 sec/batch; 97h:06m:12s remains)
INFO - root - 2019-11-04 02:02:07.256789: step 108550, total loss = 0.30, predict loss = 0.06 (78.5 examples/sec; 0.051 sec/batch; 83h:20m:10s remains)
INFO - root - 2019-11-04 02:02:07.885838: step 108560, total loss = 0.37, predict loss = 0.09 (71.8 examples/sec; 0.056 sec/batch; 91h:12m:47s remains)
INFO - root - 2019-11-04 02:02:08.544154: step 108570, total loss = 0.34, predict loss = 0.07 (77.8 examples/sec; 0.051 sec/batch; 84h:06m:24s remains)
INFO - root - 2019-11-04 02:02:09.147615: step 108580, total loss = 0.20, predict loss = 0.04 (68.6 examples/sec; 0.058 sec/batch; 95h:23m:21s remains)
INFO - root - 2019-11-04 02:02:09.721078: step 108590, total loss = 0.22, predict loss = 0.05 (75.3 examples/sec; 0.053 sec/batch; 86h:58m:10s remains)
INFO - root - 2019-11-04 02:02:10.307158: step 108600, total loss = 0.26, predict loss = 0.05 (75.1 examples/sec; 0.053 sec/batch; 87h:07m:25s remains)
INFO - root - 2019-11-04 02:02:10.922517: step 108610, total loss = 0.30, predict loss = 0.07 (68.5 examples/sec; 0.058 sec/batch; 95h:34m:41s remains)
INFO - root - 2019-11-04 02:02:11.542895: step 108620, total loss = 0.39, predict loss = 0.09 (71.7 examples/sec; 0.056 sec/batch; 91h:16m:35s remains)
INFO - root - 2019-11-04 02:02:12.189000: step 108630, total loss = 0.37, predict loss = 0.08 (60.6 examples/sec; 0.066 sec/batch; 107h:57m:51s remains)
INFO - root - 2019-11-04 02:02:12.807327: step 108640, total loss = 0.46, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 85h:21m:09s remains)
INFO - root - 2019-11-04 02:02:13.429994: step 108650, total loss = 0.36, predict loss = 0.08 (81.2 examples/sec; 0.049 sec/batch; 80h:37m:15s remains)
INFO - root - 2019-11-04 02:02:14.050090: step 108660, total loss = 0.29, predict loss = 0.06 (70.4 examples/sec; 0.057 sec/batch; 92h:58m:30s remains)
INFO - root - 2019-11-04 02:02:14.657113: step 108670, total loss = 0.34, predict loss = 0.08 (69.7 examples/sec; 0.057 sec/batch; 93h:56m:48s remains)
INFO - root - 2019-11-04 02:02:15.255818: step 108680, total loss = 0.32, predict loss = 0.07 (76.4 examples/sec; 0.052 sec/batch; 85h:41m:22s remains)
INFO - root - 2019-11-04 02:02:15.875390: step 108690, total loss = 0.28, predict loss = 0.06 (71.8 examples/sec; 0.056 sec/batch; 91h:10m:34s remains)
INFO - root - 2019-11-04 02:02:16.480512: step 108700, total loss = 0.41, predict loss = 0.09 (72.3 examples/sec; 0.055 sec/batch; 90h:33m:57s remains)
INFO - root - 2019-11-04 02:02:17.106338: step 108710, total loss = 0.34, predict loss = 0.08 (79.4 examples/sec; 0.050 sec/batch; 82h:25m:09s remains)
INFO - root - 2019-11-04 02:02:17.754350: step 108720, total loss = 0.41, predict loss = 0.10 (66.9 examples/sec; 0.060 sec/batch; 97h:52m:31s remains)
INFO - root - 2019-11-04 02:02:18.384334: step 108730, total loss = 0.44, predict loss = 0.10 (76.9 examples/sec; 0.052 sec/batch; 85h:05m:33s remains)
INFO - root - 2019-11-04 02:02:19.047866: step 108740, total loss = 0.52, predict loss = 0.12 (61.1 examples/sec; 0.066 sec/batch; 107h:11m:35s remains)
INFO - root - 2019-11-04 02:02:19.697351: step 108750, total loss = 0.63, predict loss = 0.16 (75.2 examples/sec; 0.053 sec/batch; 87h:05m:33s remains)
INFO - root - 2019-11-04 02:02:20.352576: step 108760, total loss = 0.55, predict loss = 0.13 (73.4 examples/sec; 0.054 sec/batch; 89h:10m:05s remains)
INFO - root - 2019-11-04 02:02:21.001601: step 108770, total loss = 0.42, predict loss = 0.10 (65.7 examples/sec; 0.061 sec/batch; 99h:40m:20s remains)
INFO - root - 2019-11-04 02:02:21.626892: step 108780, total loss = 0.40, predict loss = 0.10 (70.9 examples/sec; 0.056 sec/batch; 92h:23m:09s remains)
INFO - root - 2019-11-04 02:02:22.248312: step 108790, total loss = 0.39, predict loss = 0.10 (72.1 examples/sec; 0.055 sec/batch; 90h:49m:20s remains)
INFO - root - 2019-11-04 02:02:22.883772: step 108800, total loss = 0.33, predict loss = 0.08 (76.0 examples/sec; 0.053 sec/batch; 86h:05m:42s remains)
INFO - root - 2019-11-04 02:02:23.516453: step 108810, total loss = 0.37, predict loss = 0.08 (64.8 examples/sec; 0.062 sec/batch; 101h:04m:24s remains)
INFO - root - 2019-11-04 02:02:24.099059: step 108820, total loss = 0.33, predict loss = 0.07 (79.5 examples/sec; 0.050 sec/batch; 82h:21m:14s remains)
INFO - root - 2019-11-04 02:02:24.699845: step 108830, total loss = 0.26, predict loss = 0.05 (68.9 examples/sec; 0.058 sec/batch; 95h:01m:16s remains)
INFO - root - 2019-11-04 02:02:25.330610: step 108840, total loss = 0.30, predict loss = 0.07 (61.3 examples/sec; 0.065 sec/batch; 106h:43m:16s remains)
INFO - root - 2019-11-04 02:02:25.988597: step 108850, total loss = 0.36, predict loss = 0.08 (62.5 examples/sec; 0.064 sec/batch; 104h:40m:15s remains)
INFO - root - 2019-11-04 02:02:26.622766: step 108860, total loss = 0.46, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 97h:43m:09s remains)
INFO - root - 2019-11-04 02:02:27.261362: step 108870, total loss = 0.44, predict loss = 0.10 (74.4 examples/sec; 0.054 sec/batch; 88h:01m:48s remains)
INFO - root - 2019-11-04 02:02:27.904606: step 108880, total loss = 0.46, predict loss = 0.10 (73.1 examples/sec; 0.055 sec/batch; 89h:32m:50s remains)
INFO - root - 2019-11-04 02:02:28.533542: step 108890, total loss = 0.44, predict loss = 0.09 (71.2 examples/sec; 0.056 sec/batch; 91h:57m:24s remains)
INFO - root - 2019-11-04 02:02:29.164867: step 108900, total loss = 0.46, predict loss = 0.11 (74.9 examples/sec; 0.053 sec/batch; 87h:23m:30s remains)
INFO - root - 2019-11-04 02:02:29.785928: step 108910, total loss = 0.49, predict loss = 0.11 (63.6 examples/sec; 0.063 sec/batch; 102h:58m:32s remains)
INFO - root - 2019-11-04 02:02:30.407721: step 108920, total loss = 0.37, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 83h:34m:59s remains)
INFO - root - 2019-11-04 02:02:31.035855: step 108930, total loss = 0.30, predict loss = 0.07 (66.1 examples/sec; 0.060 sec/batch; 98h:58m:09s remains)
INFO - root - 2019-11-04 02:02:31.628071: step 108940, total loss = 0.40, predict loss = 0.09 (68.9 examples/sec; 0.058 sec/batch; 94h:56m:27s remains)
INFO - root - 2019-11-04 02:02:32.253951: step 108950, total loss = 0.42, predict loss = 0.10 (72.6 examples/sec; 0.055 sec/batch; 90h:07m:55s remains)
INFO - root - 2019-11-04 02:02:32.928419: step 108960, total loss = 0.22, predict loss = 0.05 (67.2 examples/sec; 0.060 sec/batch; 97h:26m:46s remains)
INFO - root - 2019-11-04 02:02:33.591348: step 108970, total loss = 0.54, predict loss = 0.13 (65.7 examples/sec; 0.061 sec/batch; 99h:33m:11s remains)
INFO - root - 2019-11-04 02:02:34.225611: step 108980, total loss = 0.45, predict loss = 0.10 (73.9 examples/sec; 0.054 sec/batch; 88h:34m:58s remains)
INFO - root - 2019-11-04 02:02:34.854217: step 108990, total loss = 0.59, predict loss = 0.14 (72.4 examples/sec; 0.055 sec/batch; 90h:24m:30s remains)
INFO - root - 2019-11-04 02:02:35.535234: step 109000, total loss = 0.61, predict loss = 0.15 (65.5 examples/sec; 0.061 sec/batch; 99h:58m:30s remains)
INFO - root - 2019-11-04 02:02:36.158824: step 109010, total loss = 0.45, predict loss = 0.11 (78.7 examples/sec; 0.051 sec/batch; 83h:09m:35s remains)
INFO - root - 2019-11-04 02:02:36.839544: step 109020, total loss = 0.54, predict loss = 0.13 (67.1 examples/sec; 0.060 sec/batch; 97h:35m:24s remains)
INFO - root - 2019-11-04 02:02:37.465257: step 109030, total loss = 0.51, predict loss = 0.12 (76.4 examples/sec; 0.052 sec/batch; 85h:39m:49s remains)
INFO - root - 2019-11-04 02:02:38.087519: step 109040, total loss = 0.42, predict loss = 0.09 (72.3 examples/sec; 0.055 sec/batch; 90h:29m:11s remains)
INFO - root - 2019-11-04 02:02:38.709686: step 109050, total loss = 0.38, predict loss = 0.09 (71.6 examples/sec; 0.056 sec/batch; 91h:21m:23s remains)
INFO - root - 2019-11-04 02:02:39.343616: step 109060, total loss = 0.41, predict loss = 0.09 (76.8 examples/sec; 0.052 sec/batch; 85h:11m:25s remains)
INFO - root - 2019-11-04 02:02:39.979384: step 109070, total loss = 0.42, predict loss = 0.10 (63.8 examples/sec; 0.063 sec/batch; 102h:33m:43s remains)
INFO - root - 2019-11-04 02:02:40.631924: step 109080, total loss = 0.44, predict loss = 0.10 (60.8 examples/sec; 0.066 sec/batch; 107h:40m:22s remains)
INFO - root - 2019-11-04 02:02:41.262326: step 109090, total loss = 0.38, predict loss = 0.09 (70.5 examples/sec; 0.057 sec/batch; 92h:51m:50s remains)
INFO - root - 2019-11-04 02:02:41.887670: step 109100, total loss = 0.48, predict loss = 0.11 (77.7 examples/sec; 0.052 sec/batch; 84h:16m:33s remains)
INFO - root - 2019-11-04 02:02:42.499599: step 109110, total loss = 0.53, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 94h:15m:58s remains)
INFO - root - 2019-11-04 02:02:43.099256: step 109120, total loss = 0.50, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 85h:17m:46s remains)
INFO - root - 2019-11-04 02:02:43.732164: step 109130, total loss = 0.41, predict loss = 0.09 (68.1 examples/sec; 0.059 sec/batch; 96h:06m:45s remains)
INFO - root - 2019-11-04 02:02:44.365827: step 109140, total loss = 0.48, predict loss = 0.11 (72.4 examples/sec; 0.055 sec/batch; 90h:24m:39s remains)
INFO - root - 2019-11-04 02:02:44.997497: step 109150, total loss = 0.63, predict loss = 0.15 (72.1 examples/sec; 0.056 sec/batch; 90h:49m:41s remains)
INFO - root - 2019-11-04 02:02:45.640722: step 109160, total loss = 0.66, predict loss = 0.16 (71.5 examples/sec; 0.056 sec/batch; 91h:34m:30s remains)
INFO - root - 2019-11-04 02:02:46.227636: step 109170, total loss = 0.50, predict loss = 0.11 (76.0 examples/sec; 0.053 sec/batch; 86h:09m:23s remains)
INFO - root - 2019-11-04 02:02:46.876556: step 109180, total loss = 0.64, predict loss = 0.15 (61.6 examples/sec; 0.065 sec/batch; 106h:12m:00s remains)
INFO - root - 2019-11-04 02:02:47.575286: step 109190, total loss = 0.64, predict loss = 0.14 (65.0 examples/sec; 0.062 sec/batch; 100h:40m:45s remains)
INFO - root - 2019-11-04 02:02:48.248240: step 109200, total loss = 0.53, predict loss = 0.12 (69.9 examples/sec; 0.057 sec/batch; 93h:36m:41s remains)
INFO - root - 2019-11-04 02:02:48.908575: step 109210, total loss = 0.70, predict loss = 0.17 (72.6 examples/sec; 0.055 sec/batch; 90h:12m:18s remains)
INFO - root - 2019-11-04 02:02:49.551000: step 109220, total loss = 0.58, predict loss = 0.14 (68.8 examples/sec; 0.058 sec/batch; 95h:04m:43s remains)
INFO - root - 2019-11-04 02:02:50.155154: step 109230, total loss = 0.67, predict loss = 0.16 (67.3 examples/sec; 0.059 sec/batch; 97h:12m:38s remains)
INFO - root - 2019-11-04 02:02:50.750552: step 109240, total loss = 0.56, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 97h:17m:38s remains)
INFO - root - 2019-11-04 02:02:51.350578: step 109250, total loss = 0.46, predict loss = 0.10 (68.1 examples/sec; 0.059 sec/batch; 96h:07m:48s remains)
INFO - root - 2019-11-04 02:02:51.952517: step 109260, total loss = 0.48, predict loss = 0.11 (80.8 examples/sec; 0.050 sec/batch; 81h:00m:25s remains)
INFO - root - 2019-11-04 02:02:52.588750: step 109270, total loss = 0.50, predict loss = 0.12 (77.5 examples/sec; 0.052 sec/batch; 84h:27m:06s remains)
INFO - root - 2019-11-04 02:02:53.206435: step 109280, total loss = 0.50, predict loss = 0.12 (65.3 examples/sec; 0.061 sec/batch; 100h:15m:25s remains)
INFO - root - 2019-11-04 02:02:53.815086: step 109290, total loss = 0.48, predict loss = 0.11 (78.9 examples/sec; 0.051 sec/batch; 82h:58m:43s remains)
INFO - root - 2019-11-04 02:02:54.446967: step 109300, total loss = 0.47, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:35m:27s remains)
INFO - root - 2019-11-04 02:02:55.068893: step 109310, total loss = 0.44, predict loss = 0.10 (77.1 examples/sec; 0.052 sec/batch; 84h:52m:54s remains)
INFO - root - 2019-11-04 02:02:55.733714: step 109320, total loss = 0.53, predict loss = 0.12 (62.5 examples/sec; 0.064 sec/batch; 104h:41m:58s remains)
INFO - root - 2019-11-04 02:02:56.388265: step 109330, total loss = 0.54, predict loss = 0.12 (72.5 examples/sec; 0.055 sec/batch; 90h:18m:13s remains)
INFO - root - 2019-11-04 02:02:57.028143: step 109340, total loss = 0.43, predict loss = 0.10 (71.1 examples/sec; 0.056 sec/batch; 92h:03m:48s remains)
INFO - root - 2019-11-04 02:02:57.667281: step 109350, total loss = 0.49, predict loss = 0.12 (76.7 examples/sec; 0.052 sec/batch; 85h:21m:24s remains)
INFO - root - 2019-11-04 02:02:58.300541: step 109360, total loss = 0.49, predict loss = 0.12 (65.7 examples/sec; 0.061 sec/batch; 99h:35m:54s remains)
INFO - root - 2019-11-04 02:02:58.949148: step 109370, total loss = 0.53, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 96h:30m:53s remains)
INFO - root - 2019-11-04 02:02:59.604425: step 109380, total loss = 0.44, predict loss = 0.10 (67.5 examples/sec; 0.059 sec/batch; 96h:56m:02s remains)
INFO - root - 2019-11-04 02:03:00.263548: step 109390, total loss = 0.30, predict loss = 0.06 (68.1 examples/sec; 0.059 sec/batch; 96h:09m:08s remains)
INFO - root - 2019-11-04 02:03:00.925705: step 109400, total loss = 0.38, predict loss = 0.09 (66.5 examples/sec; 0.060 sec/batch; 98h:28m:49s remains)
INFO - root - 2019-11-04 02:03:01.603407: step 109410, total loss = 0.39, predict loss = 0.09 (57.6 examples/sec; 0.069 sec/batch; 113h:32m:41s remains)
INFO - root - 2019-11-04 02:03:02.281811: step 109420, total loss = 0.39, predict loss = 0.08 (69.2 examples/sec; 0.058 sec/batch; 94h:31m:55s remains)
INFO - root - 2019-11-04 02:03:02.913472: step 109430, total loss = 0.31, predict loss = 0.06 (77.0 examples/sec; 0.052 sec/batch; 84h:58m:24s remains)
INFO - root - 2019-11-04 02:03:03.529861: step 109440, total loss = 0.34, predict loss = 0.07 (68.1 examples/sec; 0.059 sec/batch; 96h:08m:15s remains)
INFO - root - 2019-11-04 02:03:04.153544: step 109450, total loss = 0.44, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 93h:55m:16s remains)
INFO - root - 2019-11-04 02:03:04.779819: step 109460, total loss = 0.31, predict loss = 0.06 (70.4 examples/sec; 0.057 sec/batch; 92h:56m:44s remains)
INFO - root - 2019-11-04 02:03:05.395070: step 109470, total loss = 0.44, predict loss = 0.10 (76.0 examples/sec; 0.053 sec/batch; 86h:09m:52s remains)
INFO - root - 2019-11-04 02:03:06.024581: step 109480, total loss = 0.42, predict loss = 0.10 (82.2 examples/sec; 0.049 sec/batch; 79h:38m:49s remains)
INFO - root - 2019-11-04 02:03:06.660642: step 109490, total loss = 0.35, predict loss = 0.08 (62.4 examples/sec; 0.064 sec/batch; 104h:53m:17s remains)
INFO - root - 2019-11-04 02:03:07.286196: step 109500, total loss = 0.61, predict loss = 0.15 (65.9 examples/sec; 0.061 sec/batch; 99h:20m:24s remains)
INFO - root - 2019-11-04 02:03:07.904295: step 109510, total loss = 0.42, predict loss = 0.09 (71.2 examples/sec; 0.056 sec/batch; 91h:53m:33s remains)
INFO - root - 2019-11-04 02:03:08.538588: step 109520, total loss = 0.60, predict loss = 0.14 (77.0 examples/sec; 0.052 sec/batch; 84h:56m:49s remains)
INFO - root - 2019-11-04 02:03:09.214201: step 109530, total loss = 0.54, predict loss = 0.13 (65.0 examples/sec; 0.062 sec/batch; 100h:38m:05s remains)
INFO - root - 2019-11-04 02:03:09.886571: step 109540, total loss = 0.54, predict loss = 0.13 (76.3 examples/sec; 0.052 sec/batch; 85h:47m:24s remains)
INFO - root - 2019-11-04 02:03:10.627198: step 109550, total loss = 0.52, predict loss = 0.12 (69.8 examples/sec; 0.057 sec/batch; 93h:42m:34s remains)
INFO - root - 2019-11-04 02:03:11.731873: step 109560, total loss = 0.51, predict loss = 0.12 (67.2 examples/sec; 0.060 sec/batch; 97h:24m:25s remains)
INFO - root - 2019-11-04 02:03:12.335722: step 109570, total loss = 0.58, predict loss = 0.14 (71.3 examples/sec; 0.056 sec/batch; 91h:44m:01s remains)
INFO - root - 2019-11-04 02:03:12.926065: step 109580, total loss = 0.61, predict loss = 0.14 (64.1 examples/sec; 0.062 sec/batch; 102h:09m:27s remains)
INFO - root - 2019-11-04 02:03:13.548826: step 109590, total loss = 0.50, predict loss = 0.11 (72.2 examples/sec; 0.055 sec/batch; 90h:35m:14s remains)
INFO - root - 2019-11-04 02:03:14.153906: step 109600, total loss = 0.61, predict loss = 0.15 (71.5 examples/sec; 0.056 sec/batch; 91h:33m:56s remains)
INFO - root - 2019-11-04 02:03:14.791248: step 109610, total loss = 0.63, predict loss = 0.15 (74.1 examples/sec; 0.054 sec/batch; 88h:21m:23s remains)
INFO - root - 2019-11-04 02:03:15.413314: step 109620, total loss = 0.64, predict loss = 0.15 (81.4 examples/sec; 0.049 sec/batch; 80h:22m:58s remains)
INFO - root - 2019-11-04 02:03:16.044308: step 109630, total loss = 0.61, predict loss = 0.14 (66.1 examples/sec; 0.061 sec/batch; 99h:04m:00s remains)
INFO - root - 2019-11-04 02:03:16.677233: step 109640, total loss = 0.64, predict loss = 0.15 (70.9 examples/sec; 0.056 sec/batch; 92h:15m:19s remains)
INFO - root - 2019-11-04 02:03:17.349865: step 109650, total loss = 0.60, predict loss = 0.15 (64.1 examples/sec; 0.062 sec/batch; 102h:08m:09s remains)
INFO - root - 2019-11-04 02:03:17.980128: step 109660, total loss = 0.43, predict loss = 0.10 (66.0 examples/sec; 0.061 sec/batch; 99h:10m:13s remains)
INFO - root - 2019-11-04 02:03:18.590330: step 109670, total loss = 0.64, predict loss = 0.15 (74.6 examples/sec; 0.054 sec/batch; 87h:43m:03s remains)
INFO - root - 2019-11-04 02:03:19.235828: step 109680, total loss = 0.54, predict loss = 0.12 (63.2 examples/sec; 0.063 sec/batch; 103h:29m:13s remains)
INFO - root - 2019-11-04 02:03:19.887761: step 109690, total loss = 0.33, predict loss = 0.07 (81.4 examples/sec; 0.049 sec/batch; 80h:24m:51s remains)
INFO - root - 2019-11-04 02:03:20.496746: step 109700, total loss = 0.49, predict loss = 0.11 (75.2 examples/sec; 0.053 sec/batch; 87h:02m:17s remains)
INFO - root - 2019-11-04 02:03:21.133452: step 109710, total loss = 0.42, predict loss = 0.09 (71.8 examples/sec; 0.056 sec/batch; 91h:08m:48s remains)
INFO - root - 2019-11-04 02:03:21.759485: step 109720, total loss = 0.45, predict loss = 0.11 (64.0 examples/sec; 0.062 sec/batch; 102h:14m:16s remains)
INFO - root - 2019-11-04 02:03:22.420132: step 109730, total loss = 0.35, predict loss = 0.07 (59.8 examples/sec; 0.067 sec/batch; 109h:26m:55s remains)
INFO - root - 2019-11-04 02:03:23.163588: step 109740, total loss = 0.37, predict loss = 0.08 (54.4 examples/sec; 0.073 sec/batch; 120h:12m:17s remains)
INFO - root - 2019-11-04 02:03:23.821788: step 109750, total loss = 0.44, predict loss = 0.11 (72.1 examples/sec; 0.055 sec/batch; 90h:44m:28s remains)
INFO - root - 2019-11-04 02:03:24.470320: step 109760, total loss = 0.49, predict loss = 0.11 (80.3 examples/sec; 0.050 sec/batch; 81h:31m:15s remains)
INFO - root - 2019-11-04 02:03:25.077618: step 109770, total loss = 0.60, predict loss = 0.15 (75.3 examples/sec; 0.053 sec/batch; 86h:55m:48s remains)
INFO - root - 2019-11-04 02:03:25.715036: step 109780, total loss = 0.50, predict loss = 0.11 (61.7 examples/sec; 0.065 sec/batch; 105h:59m:28s remains)
INFO - root - 2019-11-04 02:03:26.348184: step 109790, total loss = 0.48, predict loss = 0.11 (82.6 examples/sec; 0.048 sec/batch; 79h:14m:15s remains)
INFO - root - 2019-11-04 02:03:26.969299: step 109800, total loss = 0.48, predict loss = 0.11 (69.5 examples/sec; 0.058 sec/batch; 94h:10m:25s remains)
INFO - root - 2019-11-04 02:03:27.620578: step 109810, total loss = 0.56, predict loss = 0.13 (58.1 examples/sec; 0.069 sec/batch; 112h:44m:14s remains)
INFO - root - 2019-11-04 02:03:28.318306: step 109820, total loss = 0.72, predict loss = 0.17 (68.3 examples/sec; 0.059 sec/batch; 95h:53m:16s remains)
INFO - root - 2019-11-04 02:03:28.975803: step 109830, total loss = 0.66, predict loss = 0.17 (69.0 examples/sec; 0.058 sec/batch; 94h:51m:58s remains)
INFO - root - 2019-11-04 02:03:29.584059: step 109840, total loss = 0.55, predict loss = 0.13 (77.5 examples/sec; 0.052 sec/batch; 84h:27m:25s remains)
INFO - root - 2019-11-04 02:03:30.159255: step 109850, total loss = 0.65, predict loss = 0.16 (72.1 examples/sec; 0.055 sec/batch; 90h:42m:50s remains)
INFO - root - 2019-11-04 02:03:30.769526: step 109860, total loss = 0.48, predict loss = 0.12 (74.3 examples/sec; 0.054 sec/batch; 88h:06m:34s remains)
INFO - root - 2019-11-04 02:03:31.437807: step 109870, total loss = 0.44, predict loss = 0.10 (62.1 examples/sec; 0.064 sec/batch; 105h:20m:13s remains)
INFO - root - 2019-11-04 02:03:32.108096: step 109880, total loss = 0.52, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 99h:15m:36s remains)
INFO - root - 2019-11-04 02:03:32.752300: step 109890, total loss = 0.53, predict loss = 0.13 (70.0 examples/sec; 0.057 sec/batch; 93h:26m:17s remains)
INFO - root - 2019-11-04 02:03:33.404934: step 109900, total loss = 0.53, predict loss = 0.12 (64.9 examples/sec; 0.062 sec/batch; 100h:47m:53s remains)
INFO - root - 2019-11-04 02:03:34.092963: step 109910, total loss = 0.64, predict loss = 0.14 (59.0 examples/sec; 0.068 sec/batch; 110h:55m:51s remains)
INFO - root - 2019-11-04 02:03:34.715384: step 109920, total loss = 0.48, predict loss = 0.11 (66.3 examples/sec; 0.060 sec/batch; 98h:40m:44s remains)
INFO - root - 2019-11-04 02:03:35.339435: step 109930, total loss = 0.54, predict loss = 0.12 (67.0 examples/sec; 0.060 sec/batch; 97h:40m:10s remains)
INFO - root - 2019-11-04 02:03:35.982102: step 109940, total loss = 0.50, predict loss = 0.12 (67.2 examples/sec; 0.059 sec/batch; 97h:20m:16s remains)
INFO - root - 2019-11-04 02:03:36.639869: step 109950, total loss = 0.51, predict loss = 0.12 (69.1 examples/sec; 0.058 sec/batch; 94h:39m:46s remains)
INFO - root - 2019-11-04 02:03:37.250761: step 109960, total loss = 0.38, predict loss = 0.09 (78.4 examples/sec; 0.051 sec/batch; 83h:25m:46s remains)
INFO - root - 2019-11-04 02:03:37.858819: step 109970, total loss = 0.53, predict loss = 0.13 (78.7 examples/sec; 0.051 sec/batch; 83h:08m:25s remains)
INFO - root - 2019-11-04 02:03:38.469007: step 109980, total loss = 0.38, predict loss = 0.09 (67.0 examples/sec; 0.060 sec/batch; 97h:39m:54s remains)
INFO - root - 2019-11-04 02:03:39.088233: step 109990, total loss = 0.53, predict loss = 0.13 (75.8 examples/sec; 0.053 sec/batch; 86h:19m:24s remains)
INFO - root - 2019-11-04 02:03:39.705038: step 110000, total loss = 0.45, predict loss = 0.10 (69.3 examples/sec; 0.058 sec/batch; 94h:26m:00s remains)
INFO - root - 2019-11-04 02:03:40.296219: step 110010, total loss = 0.39, predict loss = 0.09 (79.8 examples/sec; 0.050 sec/batch; 82h:03m:01s remains)
INFO - root - 2019-11-04 02:03:40.901609: step 110020, total loss = 0.48, predict loss = 0.11 (62.0 examples/sec; 0.064 sec/batch; 105h:28m:23s remains)
INFO - root - 2019-11-04 02:03:41.554918: step 110030, total loss = 0.45, predict loss = 0.10 (76.1 examples/sec; 0.053 sec/batch; 85h:58m:58s remains)
INFO - root - 2019-11-04 02:03:42.222382: step 110040, total loss = 0.46, predict loss = 0.10 (60.8 examples/sec; 0.066 sec/batch; 107h:33m:43s remains)
INFO - root - 2019-11-04 02:03:42.895652: step 110050, total loss = 0.35, predict loss = 0.08 (67.8 examples/sec; 0.059 sec/batch; 96h:30m:50s remains)
INFO - root - 2019-11-04 02:03:43.586071: step 110060, total loss = 0.52, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 97h:13m:54s remains)
INFO - root - 2019-11-04 02:03:44.295376: step 110070, total loss = 0.56, predict loss = 0.13 (67.9 examples/sec; 0.059 sec/batch; 96h:19m:45s remains)
INFO - root - 2019-11-04 02:03:44.977848: step 110080, total loss = 0.49, predict loss = 0.12 (64.6 examples/sec; 0.062 sec/batch; 101h:19m:04s remains)
INFO - root - 2019-11-04 02:03:45.656455: step 110090, total loss = 0.49, predict loss = 0.11 (61.3 examples/sec; 0.065 sec/batch; 106h:44m:34s remains)
INFO - root - 2019-11-04 02:03:46.317263: step 110100, total loss = 0.52, predict loss = 0.12 (70.7 examples/sec; 0.057 sec/batch; 92h:32m:08s remains)
INFO - root - 2019-11-04 02:03:46.995820: step 110110, total loss = 0.48, predict loss = 0.11 (67.8 examples/sec; 0.059 sec/batch; 96h:32m:32s remains)
INFO - root - 2019-11-04 02:03:47.626509: step 110120, total loss = 0.62, predict loss = 0.16 (71.8 examples/sec; 0.056 sec/batch; 91h:05m:12s remains)
INFO - root - 2019-11-04 02:03:48.288456: step 110130, total loss = 0.50, predict loss = 0.11 (67.2 examples/sec; 0.060 sec/batch; 97h:22m:38s remains)
INFO - root - 2019-11-04 02:03:48.919741: step 110140, total loss = 0.40, predict loss = 0.09 (71.8 examples/sec; 0.056 sec/batch; 91h:07m:21s remains)
INFO - root - 2019-11-04 02:03:49.606517: step 110150, total loss = 0.44, predict loss = 0.11 (61.5 examples/sec; 0.065 sec/batch; 106h:25m:05s remains)
INFO - root - 2019-11-04 02:03:50.217436: step 110160, total loss = 0.49, predict loss = 0.12 (81.3 examples/sec; 0.049 sec/batch; 80h:27m:59s remains)
INFO - root - 2019-11-04 02:03:50.825693: step 110170, total loss = 0.66, predict loss = 0.16 (78.3 examples/sec; 0.051 sec/batch; 83h:33m:49s remains)
INFO - root - 2019-11-04 02:03:51.461169: step 110180, total loss = 0.59, predict loss = 0.15 (66.9 examples/sec; 0.060 sec/batch; 97h:51m:12s remains)
INFO - root - 2019-11-04 02:03:52.133640: step 110190, total loss = 0.60, predict loss = 0.14 (62.0 examples/sec; 0.065 sec/batch; 105h:34m:10s remains)
INFO - root - 2019-11-04 02:03:52.736156: step 110200, total loss = 0.71, predict loss = 0.17 (78.6 examples/sec; 0.051 sec/batch; 83h:14m:03s remains)
INFO - root - 2019-11-04 02:03:53.323831: step 110210, total loss = 0.58, predict loss = 0.14 (76.0 examples/sec; 0.053 sec/batch; 86h:04m:56s remains)
INFO - root - 2019-11-04 02:03:53.962739: step 110220, total loss = 0.60, predict loss = 0.14 (59.7 examples/sec; 0.067 sec/batch; 109h:34m:20s remains)
INFO - root - 2019-11-04 02:03:54.589556: step 110230, total loss = 0.47, predict loss = 0.11 (72.4 examples/sec; 0.055 sec/batch; 90h:25m:09s remains)
INFO - root - 2019-11-04 02:03:55.198772: step 110240, total loss = 0.66, predict loss = 0.16 (74.7 examples/sec; 0.054 sec/batch; 87h:38m:09s remains)
INFO - root - 2019-11-04 02:03:55.822960: step 110250, total loss = 0.62, predict loss = 0.15 (84.3 examples/sec; 0.047 sec/batch; 77h:40m:20s remains)
INFO - root - 2019-11-04 02:03:56.414026: step 110260, total loss = 0.70, predict loss = 0.16 (80.6 examples/sec; 0.050 sec/batch; 81h:09m:15s remains)
INFO - root - 2019-11-04 02:03:57.037069: step 110270, total loss = 0.54, predict loss = 0.13 (78.8 examples/sec; 0.051 sec/batch; 83h:01m:55s remains)
INFO - root - 2019-11-04 02:03:57.678008: step 110280, total loss = 0.42, predict loss = 0.10 (61.8 examples/sec; 0.065 sec/batch; 105h:48m:59s remains)
INFO - root - 2019-11-04 02:03:58.355544: step 110290, total loss = 0.42, predict loss = 0.10 (69.3 examples/sec; 0.058 sec/batch; 94h:22m:31s remains)
INFO - root - 2019-11-04 02:03:58.972805: step 110300, total loss = 0.38, predict loss = 0.08 (67.1 examples/sec; 0.060 sec/batch; 97h:28m:27s remains)
INFO - root - 2019-11-04 02:03:59.612083: step 110310, total loss = 0.55, predict loss = 0.13 (79.8 examples/sec; 0.050 sec/batch; 82h:00m:43s remains)
INFO - root - 2019-11-04 02:04:00.247104: step 110320, total loss = 0.49, predict loss = 0.11 (67.6 examples/sec; 0.059 sec/batch; 96h:48m:57s remains)
INFO - root - 2019-11-04 02:04:00.868069: step 110330, total loss = 0.46, predict loss = 0.10 (67.9 examples/sec; 0.059 sec/batch; 96h:23m:23s remains)
INFO - root - 2019-11-04 02:04:01.519472: step 110340, total loss = 0.62, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 95h:30m:27s remains)
INFO - root - 2019-11-04 02:04:02.123933: step 110350, total loss = 0.31, predict loss = 0.06 (74.8 examples/sec; 0.053 sec/batch; 87h:28m:40s remains)
INFO - root - 2019-11-04 02:04:02.748724: step 110360, total loss = 0.52, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 96h:19m:25s remains)
INFO - root - 2019-11-04 02:04:03.365174: step 110370, total loss = 0.58, predict loss = 0.14 (79.6 examples/sec; 0.050 sec/batch; 82h:15m:29s remains)
INFO - root - 2019-11-04 02:04:04.007382: step 110380, total loss = 0.59, predict loss = 0.13 (67.2 examples/sec; 0.060 sec/batch; 97h:23m:22s remains)
INFO - root - 2019-11-04 02:04:04.599445: step 110390, total loss = 0.65, predict loss = 0.15 (88.6 examples/sec; 0.045 sec/batch; 73h:51m:05s remains)
INFO - root - 2019-11-04 02:04:05.178925: step 110400, total loss = 0.53, predict loss = 0.12 (83.0 examples/sec; 0.048 sec/batch; 78h:50m:57s remains)
INFO - root - 2019-11-04 02:04:05.792223: step 110410, total loss = 0.46, predict loss = 0.10 (65.8 examples/sec; 0.061 sec/batch; 99h:23m:47s remains)
INFO - root - 2019-11-04 02:04:06.413875: step 110420, total loss = 0.59, predict loss = 0.14 (78.6 examples/sec; 0.051 sec/batch; 83h:18m:14s remains)
INFO - root - 2019-11-04 02:04:07.052171: step 110430, total loss = 0.37, predict loss = 0.08 (73.5 examples/sec; 0.054 sec/batch; 89h:01m:41s remains)
INFO - root - 2019-11-04 02:04:07.554602: step 110440, total loss = 0.46, predict loss = 0.10 (100.4 examples/sec; 0.040 sec/batch; 65h:09m:45s remains)
INFO - root - 2019-11-04 02:04:08.010255: step 110450, total loss = 0.45, predict loss = 0.10 (97.9 examples/sec; 0.041 sec/batch; 66h:50m:01s remains)
INFO - root - 2019-11-04 02:04:09.057325: step 110460, total loss = 0.21, predict loss = 0.04 (79.9 examples/sec; 0.050 sec/batch; 81h:53m:02s remains)
INFO - root - 2019-11-04 02:04:09.651430: step 110470, total loss = 0.37, predict loss = 0.08 (65.7 examples/sec; 0.061 sec/batch; 99h:40m:11s remains)
INFO - root - 2019-11-04 02:04:10.301210: step 110480, total loss = 0.50, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 98h:21m:21s remains)
INFO - root - 2019-11-04 02:04:10.925628: step 110490, total loss = 0.41, predict loss = 0.10 (76.6 examples/sec; 0.052 sec/batch; 85h:28m:54s remains)
INFO - root - 2019-11-04 02:04:11.551701: step 110500, total loss = 0.42, predict loss = 0.10 (71.4 examples/sec; 0.056 sec/batch; 91h:35m:50s remains)
INFO - root - 2019-11-04 02:04:12.157860: step 110510, total loss = 0.50, predict loss = 0.12 (73.8 examples/sec; 0.054 sec/batch; 88h:43m:29s remains)
INFO - root - 2019-11-04 02:04:12.771589: step 110520, total loss = 0.41, predict loss = 0.10 (79.8 examples/sec; 0.050 sec/batch; 82h:00m:22s remains)
INFO - root - 2019-11-04 02:04:13.425729: step 110530, total loss = 0.46, predict loss = 0.11 (56.9 examples/sec; 0.070 sec/batch; 114h:55m:21s remains)
INFO - root - 2019-11-04 02:04:14.037574: step 110540, total loss = 0.47, predict loss = 0.11 (84.1 examples/sec; 0.048 sec/batch; 77h:51m:19s remains)
INFO - root - 2019-11-04 02:04:14.636963: step 110550, total loss = 0.63, predict loss = 0.15 (80.8 examples/sec; 0.049 sec/batch; 80h:56m:41s remains)
INFO - root - 2019-11-04 02:04:15.237441: step 110560, total loss = 0.55, predict loss = 0.12 (73.1 examples/sec; 0.055 sec/batch; 89h:31m:42s remains)
INFO - root - 2019-11-04 02:04:15.866095: step 110570, total loss = 0.52, predict loss = 0.12 (69.5 examples/sec; 0.058 sec/batch; 94h:08m:36s remains)
INFO - root - 2019-11-04 02:04:16.457915: step 110580, total loss = 0.57, predict loss = 0.13 (64.9 examples/sec; 0.062 sec/batch; 100h:52m:55s remains)
INFO - root - 2019-11-04 02:04:17.118567: step 110590, total loss = 0.46, predict loss = 0.10 (61.4 examples/sec; 0.065 sec/batch; 106h:35m:20s remains)
INFO - root - 2019-11-04 02:04:17.743560: step 110600, total loss = 0.35, predict loss = 0.07 (67.2 examples/sec; 0.060 sec/batch; 97h:23m:03s remains)
INFO - root - 2019-11-04 02:04:18.391070: step 110610, total loss = 0.51, predict loss = 0.11 (72.7 examples/sec; 0.055 sec/batch; 90h:03m:45s remains)
INFO - root - 2019-11-04 02:04:19.001841: step 110620, total loss = 0.51, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 85h:20m:56s remains)
INFO - root - 2019-11-04 02:04:19.572702: step 110630, total loss = 0.46, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 91h:27m:13s remains)
INFO - root - 2019-11-04 02:04:20.165245: step 110640, total loss = 0.48, predict loss = 0.11 (71.0 examples/sec; 0.056 sec/batch; 92h:11m:58s remains)
INFO - root - 2019-11-04 02:04:20.780046: step 110650, total loss = 0.42, predict loss = 0.10 (79.0 examples/sec; 0.051 sec/batch; 82h:48m:33s remains)
INFO - root - 2019-11-04 02:04:21.399147: step 110660, total loss = 0.46, predict loss = 0.10 (66.9 examples/sec; 0.060 sec/batch; 97h:46m:01s remains)
INFO - root - 2019-11-04 02:04:22.023317: step 110670, total loss = 0.58, predict loss = 0.14 (65.7 examples/sec; 0.061 sec/batch; 99h:39m:14s remains)
INFO - root - 2019-11-04 02:04:22.667269: step 110680, total loss = 0.50, predict loss = 0.11 (74.6 examples/sec; 0.054 sec/batch; 87h:45m:20s remains)
INFO - root - 2019-11-04 02:04:23.262662: step 110690, total loss = 0.34, predict loss = 0.07 (73.7 examples/sec; 0.054 sec/batch; 88h:46m:36s remains)
INFO - root - 2019-11-04 02:04:23.880331: step 110700, total loss = 0.59, predict loss = 0.13 (73.2 examples/sec; 0.055 sec/batch; 89h:22m:56s remains)
INFO - root - 2019-11-04 02:04:24.495299: step 110710, total loss = 0.58, predict loss = 0.13 (79.8 examples/sec; 0.050 sec/batch; 81h:58m:27s remains)
INFO - root - 2019-11-04 02:04:25.104858: step 110720, total loss = 0.56, predict loss = 0.13 (70.0 examples/sec; 0.057 sec/batch; 93h:32m:27s remains)
INFO - root - 2019-11-04 02:04:25.717785: step 110730, total loss = 0.35, predict loss = 0.08 (74.4 examples/sec; 0.054 sec/batch; 87h:55m:24s remains)
INFO - root - 2019-11-04 02:04:26.359698: step 110740, total loss = 0.54, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 93h:49m:26s remains)
INFO - root - 2019-11-04 02:04:26.987484: step 110750, total loss = 0.54, predict loss = 0.13 (64.8 examples/sec; 0.062 sec/batch; 100h:55m:37s remains)
INFO - root - 2019-11-04 02:04:27.634301: step 110760, total loss = 0.55, predict loss = 0.13 (74.6 examples/sec; 0.054 sec/batch; 87h:39m:43s remains)
INFO - root - 2019-11-04 02:04:28.274311: step 110770, total loss = 0.54, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 98h:24m:07s remains)
INFO - root - 2019-11-04 02:04:28.889527: step 110780, total loss = 0.51, predict loss = 0.12 (85.2 examples/sec; 0.047 sec/batch; 76h:45m:35s remains)
INFO - root - 2019-11-04 02:04:29.500203: step 110790, total loss = 0.55, predict loss = 0.12 (80.7 examples/sec; 0.050 sec/batch; 81h:04m:42s remains)
INFO - root - 2019-11-04 02:04:30.146462: step 110800, total loss = 0.52, predict loss = 0.13 (72.4 examples/sec; 0.055 sec/batch; 90h:24m:01s remains)
INFO - root - 2019-11-04 02:04:30.747164: step 110810, total loss = 0.47, predict loss = 0.11 (70.8 examples/sec; 0.056 sec/batch; 92h:22m:54s remains)
INFO - root - 2019-11-04 02:04:31.352459: step 110820, total loss = 0.43, predict loss = 0.09 (76.2 examples/sec; 0.053 sec/batch; 85h:53m:51s remains)
INFO - root - 2019-11-04 02:04:31.955019: step 110830, total loss = 0.56, predict loss = 0.13 (80.2 examples/sec; 0.050 sec/batch; 81h:35m:08s remains)
INFO - root - 2019-11-04 02:04:32.548216: step 110840, total loss = 0.51, predict loss = 0.11 (70.0 examples/sec; 0.057 sec/batch; 93h:26m:01s remains)
INFO - root - 2019-11-04 02:04:33.160863: step 110850, total loss = 0.48, predict loss = 0.11 (74.2 examples/sec; 0.054 sec/batch; 88h:10m:13s remains)
INFO - root - 2019-11-04 02:04:33.819602: step 110860, total loss = 0.44, predict loss = 0.09 (63.8 examples/sec; 0.063 sec/batch; 102h:32m:35s remains)
INFO - root - 2019-11-04 02:04:34.490856: step 110870, total loss = 0.53, predict loss = 0.11 (71.4 examples/sec; 0.056 sec/batch; 91h:39m:19s remains)
INFO - root - 2019-11-04 02:04:35.106813: step 110880, total loss = 0.65, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 90h:38m:41s remains)
INFO - root - 2019-11-04 02:04:35.751606: step 110890, total loss = 0.62, predict loss = 0.15 (74.5 examples/sec; 0.054 sec/batch; 87h:51m:06s remains)
INFO - root - 2019-11-04 02:04:36.373696: step 110900, total loss = 0.46, predict loss = 0.11 (76.0 examples/sec; 0.053 sec/batch; 86h:07m:14s remains)
INFO - root - 2019-11-04 02:04:36.966897: step 110910, total loss = 0.41, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 92h:40m:33s remains)
INFO - root - 2019-11-04 02:04:37.562576: step 110920, total loss = 0.47, predict loss = 0.10 (67.4 examples/sec; 0.059 sec/batch; 97h:08m:41s remains)
INFO - root - 2019-11-04 02:04:38.197304: step 110930, total loss = 0.51, predict loss = 0.12 (66.1 examples/sec; 0.061 sec/batch; 99h:02m:09s remains)
INFO - root - 2019-11-04 02:04:38.860112: step 110940, total loss = 0.53, predict loss = 0.12 (63.7 examples/sec; 0.063 sec/batch; 102h:43m:00s remains)
INFO - root - 2019-11-04 02:04:39.466746: step 110950, total loss = 0.41, predict loss = 0.10 (80.1 examples/sec; 0.050 sec/batch; 81h:44m:03s remains)
INFO - root - 2019-11-04 02:04:40.114795: step 110960, total loss = 0.49, predict loss = 0.13 (67.2 examples/sec; 0.060 sec/batch; 97h:20m:01s remains)
INFO - root - 2019-11-04 02:04:40.733866: step 110970, total loss = 0.53, predict loss = 0.13 (81.9 examples/sec; 0.049 sec/batch; 79h:56m:23s remains)
INFO - root - 2019-11-04 02:04:41.320282: step 110980, total loss = 0.53, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 96h:06m:51s remains)
INFO - root - 2019-11-04 02:04:41.965976: step 110990, total loss = 0.62, predict loss = 0.16 (68.6 examples/sec; 0.058 sec/batch; 95h:23m:12s remains)
INFO - root - 2019-11-04 02:04:42.598890: step 111000, total loss = 0.30, predict loss = 0.07 (68.6 examples/sec; 0.058 sec/batch; 95h:20m:33s remains)
INFO - root - 2019-11-04 02:04:43.188493: step 111010, total loss = 0.36, predict loss = 0.09 (72.3 examples/sec; 0.055 sec/batch; 90h:33m:29s remains)
INFO - root - 2019-11-04 02:04:43.804206: step 111020, total loss = 0.50, predict loss = 0.12 (66.7 examples/sec; 0.060 sec/batch; 98h:05m:37s remains)
INFO - root - 2019-11-04 02:04:44.427054: step 111030, total loss = 0.28, predict loss = 0.06 (77.8 examples/sec; 0.051 sec/batch; 84h:03m:13s remains)
INFO - root - 2019-11-04 02:04:45.047440: step 111040, total loss = 0.45, predict loss = 0.11 (79.8 examples/sec; 0.050 sec/batch; 82h:00m:21s remains)
INFO - root - 2019-11-04 02:04:45.663470: step 111050, total loss = 0.38, predict loss = 0.09 (69.8 examples/sec; 0.057 sec/batch; 93h:44m:10s remains)
INFO - root - 2019-11-04 02:04:46.265128: step 111060, total loss = 0.52, predict loss = 0.13 (70.6 examples/sec; 0.057 sec/batch; 92h:44m:26s remains)
INFO - root - 2019-11-04 02:04:46.857246: step 111070, total loss = 0.56, predict loss = 0.13 (80.7 examples/sec; 0.050 sec/batch; 81h:03m:40s remains)
INFO - root - 2019-11-04 02:04:47.480194: step 111080, total loss = 0.50, predict loss = 0.12 (74.5 examples/sec; 0.054 sec/batch; 87h:52m:31s remains)
INFO - root - 2019-11-04 02:04:48.123676: step 111090, total loss = 0.44, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 92h:44m:00s remains)
INFO - root - 2019-11-04 02:04:48.726250: step 111100, total loss = 0.22, predict loss = 0.04 (76.0 examples/sec; 0.053 sec/batch; 86h:03m:37s remains)
INFO - root - 2019-11-04 02:04:49.312990: step 111110, total loss = 0.33, predict loss = 0.07 (84.7 examples/sec; 0.047 sec/batch; 77h:17m:21s remains)
INFO - root - 2019-11-04 02:04:49.916162: step 111120, total loss = 0.38, predict loss = 0.08 (71.6 examples/sec; 0.056 sec/batch; 91h:23m:57s remains)
INFO - root - 2019-11-04 02:04:50.529626: step 111130, total loss = 0.44, predict loss = 0.10 (73.4 examples/sec; 0.054 sec/batch; 89h:07m:34s remains)
INFO - root - 2019-11-04 02:04:51.128571: step 111140, total loss = 0.62, predict loss = 0.15 (76.1 examples/sec; 0.053 sec/batch; 86h:02m:06s remains)
INFO - root - 2019-11-04 02:04:51.729020: step 111150, total loss = 0.47, predict loss = 0.12 (80.3 examples/sec; 0.050 sec/batch; 81h:27m:56s remains)
INFO - root - 2019-11-04 02:04:52.355156: step 111160, total loss = 0.53, predict loss = 0.12 (64.8 examples/sec; 0.062 sec/batch; 100h:55m:06s remains)
INFO - root - 2019-11-04 02:04:52.928854: step 111170, total loss = 0.69, predict loss = 0.17 (74.2 examples/sec; 0.054 sec/batch; 88h:12m:22s remains)
INFO - root - 2019-11-04 02:04:53.534496: step 111180, total loss = 0.55, predict loss = 0.13 (78.1 examples/sec; 0.051 sec/batch; 83h:47m:41s remains)
INFO - root - 2019-11-04 02:04:54.159929: step 111190, total loss = 0.53, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 91h:48m:29s remains)
INFO - root - 2019-11-04 02:04:54.762240: step 111200, total loss = 0.51, predict loss = 0.12 (72.5 examples/sec; 0.055 sec/batch; 90h:13m:01s remains)
INFO - root - 2019-11-04 02:04:55.344144: step 111210, total loss = 0.59, predict loss = 0.13 (77.0 examples/sec; 0.052 sec/batch; 85h:01m:28s remains)
INFO - root - 2019-11-04 02:04:55.917954: step 111220, total loss = 0.34, predict loss = 0.08 (78.0 examples/sec; 0.051 sec/batch; 83h:52m:35s remains)
INFO - root - 2019-11-04 02:04:56.487628: step 111230, total loss = 0.51, predict loss = 0.12 (76.7 examples/sec; 0.052 sec/batch; 85h:15m:33s remains)
INFO - root - 2019-11-04 02:04:57.097694: step 111240, total loss = 0.41, predict loss = 0.09 (83.7 examples/sec; 0.048 sec/batch; 78h:08m:29s remains)
INFO - root - 2019-11-04 02:04:57.732963: step 111250, total loss = 0.49, predict loss = 0.12 (65.2 examples/sec; 0.061 sec/batch; 100h:20m:44s remains)
INFO - root - 2019-11-04 02:04:58.387967: step 111260, total loss = 0.40, predict loss = 0.09 (71.7 examples/sec; 0.056 sec/batch; 91h:18m:04s remains)
INFO - root - 2019-11-04 02:04:58.979196: step 111270, total loss = 0.47, predict loss = 0.11 (72.6 examples/sec; 0.055 sec/batch; 90h:09m:20s remains)
INFO - root - 2019-11-04 02:04:59.601556: step 111280, total loss = 0.40, predict loss = 0.09 (66.1 examples/sec; 0.060 sec/batch; 98h:57m:06s remains)
INFO - root - 2019-11-04 02:05:00.234371: step 111290, total loss = 0.23, predict loss = 0.05 (75.6 examples/sec; 0.053 sec/batch; 86h:31m:36s remains)
INFO - root - 2019-11-04 02:05:00.846883: step 111300, total loss = 0.23, predict loss = 0.05 (69.3 examples/sec; 0.058 sec/batch; 94h:25m:51s remains)
INFO - root - 2019-11-04 02:05:01.473264: step 111310, total loss = 0.22, predict loss = 0.04 (73.3 examples/sec; 0.055 sec/batch; 89h:12m:50s remains)
INFO - root - 2019-11-04 02:05:02.159211: step 111320, total loss = 0.20, predict loss = 0.04 (72.4 examples/sec; 0.055 sec/batch; 90h:22m:23s remains)
INFO - root - 2019-11-04 02:05:02.852605: step 111330, total loss = 0.31, predict loss = 0.07 (73.9 examples/sec; 0.054 sec/batch; 88h:34m:18s remains)
INFO - root - 2019-11-04 02:05:03.543294: step 111340, total loss = 0.29, predict loss = 0.06 (70.2 examples/sec; 0.057 sec/batch; 93h:08m:41s remains)
INFO - root - 2019-11-04 02:05:04.199551: step 111350, total loss = 0.35, predict loss = 0.08 (74.5 examples/sec; 0.054 sec/batch; 87h:47m:48s remains)
INFO - root - 2019-11-04 02:05:04.863585: step 111360, total loss = 0.45, predict loss = 0.10 (63.2 examples/sec; 0.063 sec/batch; 103h:35m:59s remains)
INFO - root - 2019-11-04 02:05:05.479593: step 111370, total loss = 0.51, predict loss = 0.12 (72.2 examples/sec; 0.055 sec/batch; 90h:34m:13s remains)
INFO - root - 2019-11-04 02:05:06.128611: step 111380, total loss = 0.29, predict loss = 0.06 (67.0 examples/sec; 0.060 sec/batch; 97h:40m:50s remains)
INFO - root - 2019-11-04 02:05:06.762519: step 111390, total loss = 0.31, predict loss = 0.07 (73.0 examples/sec; 0.055 sec/batch; 89h:38m:10s remains)
INFO - root - 2019-11-04 02:05:07.363816: step 111400, total loss = 0.46, predict loss = 0.11 (72.6 examples/sec; 0.055 sec/batch; 90h:09m:03s remains)
INFO - root - 2019-11-04 02:05:07.968928: step 111410, total loss = 0.36, predict loss = 0.08 (77.8 examples/sec; 0.051 sec/batch; 84h:04m:13s remains)
INFO - root - 2019-11-04 02:05:08.598356: step 111420, total loss = 0.48, predict loss = 0.11 (66.7 examples/sec; 0.060 sec/batch; 98h:05m:54s remains)
INFO - root - 2019-11-04 02:05:09.224667: step 111430, total loss = 0.44, predict loss = 0.10 (64.9 examples/sec; 0.062 sec/batch; 100h:49m:20s remains)
INFO - root - 2019-11-04 02:05:09.869513: step 111440, total loss = 0.40, predict loss = 0.09 (66.1 examples/sec; 0.061 sec/batch; 99h:01m:54s remains)
INFO - root - 2019-11-04 02:05:10.462098: step 111450, total loss = 0.45, predict loss = 0.10 (72.2 examples/sec; 0.055 sec/batch; 90h:37m:07s remains)
INFO - root - 2019-11-04 02:05:11.077373: step 111460, total loss = 0.58, predict loss = 0.14 (74.3 examples/sec; 0.054 sec/batch; 88h:00m:26s remains)
INFO - root - 2019-11-04 02:05:11.691459: step 111470, total loss = 0.50, predict loss = 0.12 (72.5 examples/sec; 0.055 sec/batch; 90h:14m:20s remains)
INFO - root - 2019-11-04 02:05:12.307599: step 111480, total loss = 0.29, predict loss = 0.06 (70.5 examples/sec; 0.057 sec/batch; 92h:50m:13s remains)
INFO - root - 2019-11-04 02:05:12.935495: step 111490, total loss = 0.59, predict loss = 0.14 (76.0 examples/sec; 0.053 sec/batch; 86h:02m:05s remains)
INFO - root - 2019-11-04 02:05:13.595921: step 111500, total loss = 0.39, predict loss = 0.10 (60.6 examples/sec; 0.066 sec/batch; 107h:57m:29s remains)
INFO - root - 2019-11-04 02:05:14.211722: step 111510, total loss = 0.37, predict loss = 0.09 (73.4 examples/sec; 0.054 sec/batch; 89h:06m:32s remains)
INFO - root - 2019-11-04 02:05:14.892091: step 111520, total loss = 0.44, predict loss = 0.10 (64.3 examples/sec; 0.062 sec/batch; 101h:41m:08s remains)
INFO - root - 2019-11-04 02:05:15.545965: step 111530, total loss = 0.35, predict loss = 0.08 (72.3 examples/sec; 0.055 sec/batch; 90h:27m:19s remains)
INFO - root - 2019-11-04 02:05:16.214415: step 111540, total loss = 0.45, predict loss = 0.10 (73.9 examples/sec; 0.054 sec/batch; 88h:34m:58s remains)
INFO - root - 2019-11-04 02:05:16.857023: step 111550, total loss = 0.21, predict loss = 0.04 (58.8 examples/sec; 0.068 sec/batch; 111h:14m:03s remains)
INFO - root - 2019-11-04 02:05:17.514601: step 111560, total loss = 0.37, predict loss = 0.09 (64.6 examples/sec; 0.062 sec/batch; 101h:15m:31s remains)
INFO - root - 2019-11-04 02:05:18.164865: step 111570, total loss = 0.25, predict loss = 0.06 (70.3 examples/sec; 0.057 sec/batch; 93h:04m:10s remains)
INFO - root - 2019-11-04 02:05:18.815921: step 111580, total loss = 0.50, predict loss = 0.12 (71.2 examples/sec; 0.056 sec/batch; 91h:55m:53s remains)
INFO - root - 2019-11-04 02:05:19.485226: step 111590, total loss = 0.40, predict loss = 0.09 (76.1 examples/sec; 0.053 sec/batch; 85h:55m:33s remains)
INFO - root - 2019-11-04 02:05:20.118316: step 111600, total loss = 0.52, predict loss = 0.12 (74.9 examples/sec; 0.053 sec/batch; 87h:22m:37s remains)
INFO - root - 2019-11-04 02:05:20.701245: step 111610, total loss = 0.35, predict loss = 0.07 (81.6 examples/sec; 0.049 sec/batch; 80h:08m:24s remains)
INFO - root - 2019-11-04 02:05:21.312711: step 111620, total loss = 0.37, predict loss = 0.07 (75.8 examples/sec; 0.053 sec/batch; 86h:19m:18s remains)
INFO - root - 2019-11-04 02:05:21.917338: step 111630, total loss = 0.47, predict loss = 0.11 (76.0 examples/sec; 0.053 sec/batch; 86h:06m:51s remains)
INFO - root - 2019-11-04 02:05:22.506597: step 111640, total loss = 0.44, predict loss = 0.11 (65.5 examples/sec; 0.061 sec/batch; 99h:49m:19s remains)
INFO - root - 2019-11-04 02:05:23.135914: step 111650, total loss = 0.31, predict loss = 0.07 (75.7 examples/sec; 0.053 sec/batch; 86h:25m:38s remains)
INFO - root - 2019-11-04 02:05:23.760245: step 111660, total loss = 0.40, predict loss = 0.10 (69.9 examples/sec; 0.057 sec/batch; 93h:34m:47s remains)
INFO - root - 2019-11-04 02:05:24.395019: step 111670, total loss = 0.43, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 92h:42m:44s remains)
INFO - root - 2019-11-04 02:05:25.013548: step 111680, total loss = 0.27, predict loss = 0.06 (69.0 examples/sec; 0.058 sec/batch; 94h:53m:06s remains)
INFO - root - 2019-11-04 02:05:25.635067: step 111690, total loss = 0.44, predict loss = 0.10 (65.3 examples/sec; 0.061 sec/batch; 100h:07m:31s remains)
INFO - root - 2019-11-04 02:05:26.277494: step 111700, total loss = 0.56, predict loss = 0.13 (75.0 examples/sec; 0.053 sec/batch; 87h:14m:29s remains)
INFO - root - 2019-11-04 02:05:26.898072: step 111710, total loss = 0.51, predict loss = 0.12 (69.1 examples/sec; 0.058 sec/batch; 94h:42m:15s remains)
INFO - root - 2019-11-04 02:05:27.523217: step 111720, total loss = 0.43, predict loss = 0.09 (75.9 examples/sec; 0.053 sec/batch; 86h:11m:54s remains)
INFO - root - 2019-11-04 02:05:28.180796: step 111730, total loss = 0.43, predict loss = 0.10 (70.7 examples/sec; 0.057 sec/batch; 92h:31m:31s remains)
INFO - root - 2019-11-04 02:05:28.789128: step 111740, total loss = 0.30, predict loss = 0.06 (72.7 examples/sec; 0.055 sec/batch; 90h:02m:01s remains)
INFO - root - 2019-11-04 02:05:29.409330: step 111750, total loss = 0.38, predict loss = 0.08 (80.8 examples/sec; 0.050 sec/batch; 81h:00m:31s remains)
INFO - root - 2019-11-04 02:05:30.027348: step 111760, total loss = 0.38, predict loss = 0.08 (79.9 examples/sec; 0.050 sec/batch; 81h:51m:32s remains)
INFO - root - 2019-11-04 02:05:30.651032: step 111770, total loss = 0.36, predict loss = 0.08 (69.0 examples/sec; 0.058 sec/batch; 94h:52m:45s remains)
INFO - root - 2019-11-04 02:05:31.271278: step 111780, total loss = 0.36, predict loss = 0.08 (78.0 examples/sec; 0.051 sec/batch; 83h:50m:41s remains)
INFO - root - 2019-11-04 02:05:31.897420: step 111790, total loss = 0.62, predict loss = 0.15 (66.2 examples/sec; 0.060 sec/batch; 98h:51m:27s remains)
INFO - root - 2019-11-04 02:05:32.550404: step 111800, total loss = 0.38, predict loss = 0.09 (74.7 examples/sec; 0.054 sec/batch; 87h:37m:34s remains)
INFO - root - 2019-11-04 02:05:33.248696: step 111810, total loss = 0.45, predict loss = 0.10 (65.8 examples/sec; 0.061 sec/batch; 99h:23m:00s remains)
INFO - root - 2019-11-04 02:05:33.895059: step 111820, total loss = 0.20, predict loss = 0.04 (72.1 examples/sec; 0.055 sec/batch; 90h:44m:53s remains)
INFO - root - 2019-11-04 02:05:34.544263: step 111830, total loss = 0.48, predict loss = 0.11 (71.9 examples/sec; 0.056 sec/batch; 91h:00m:20s remains)
INFO - root - 2019-11-04 02:05:35.180153: step 111840, total loss = 0.36, predict loss = 0.08 (68.3 examples/sec; 0.059 sec/batch; 95h:48m:49s remains)
INFO - root - 2019-11-04 02:05:35.773602: step 111850, total loss = 0.36, predict loss = 0.08 (81.7 examples/sec; 0.049 sec/batch; 80h:02m:37s remains)
INFO - root - 2019-11-04 02:05:36.391022: step 111860, total loss = 0.60, predict loss = 0.14 (72.7 examples/sec; 0.055 sec/batch; 90h:00m:34s remains)
INFO - root - 2019-11-04 02:05:36.982294: step 111870, total loss = 0.52, predict loss = 0.12 (81.7 examples/sec; 0.049 sec/batch; 80h:05m:03s remains)
INFO - root - 2019-11-04 02:05:37.569208: step 111880, total loss = 0.66, predict loss = 0.16 (75.6 examples/sec; 0.053 sec/batch; 86h:30m:27s remains)
INFO - root - 2019-11-04 02:05:38.173780: step 111890, total loss = 0.69, predict loss = 0.17 (70.3 examples/sec; 0.057 sec/batch; 93h:04m:30s remains)
INFO - root - 2019-11-04 02:05:38.784328: step 111900, total loss = 0.62, predict loss = 0.14 (67.6 examples/sec; 0.059 sec/batch; 96h:49m:38s remains)
INFO - root - 2019-11-04 02:05:39.422642: step 111910, total loss = 0.89, predict loss = 0.21 (66.3 examples/sec; 0.060 sec/batch; 98h:41m:43s remains)
INFO - root - 2019-11-04 02:05:40.092656: step 111920, total loss = 0.68, predict loss = 0.15 (73.5 examples/sec; 0.054 sec/batch; 89h:01m:52s remains)
INFO - root - 2019-11-04 02:05:40.731346: step 111930, total loss = 0.71, predict loss = 0.16 (68.1 examples/sec; 0.059 sec/batch; 96h:07m:31s remains)
INFO - root - 2019-11-04 02:05:41.353178: step 111940, total loss = 0.78, predict loss = 0.18 (67.3 examples/sec; 0.059 sec/batch; 97h:16m:18s remains)
INFO - root - 2019-11-04 02:05:41.966699: step 111950, total loss = 0.51, predict loss = 0.12 (78.3 examples/sec; 0.051 sec/batch; 83h:34m:15s remains)
INFO - root - 2019-11-04 02:05:42.586438: step 111960, total loss = 0.62, predict loss = 0.14 (66.0 examples/sec; 0.061 sec/batch; 99h:10m:10s remains)
INFO - root - 2019-11-04 02:05:43.190784: step 111970, total loss = 0.53, predict loss = 0.12 (65.8 examples/sec; 0.061 sec/batch; 99h:27m:06s remains)
INFO - root - 2019-11-04 02:05:43.830474: step 111980, total loss = 0.58, predict loss = 0.14 (65.9 examples/sec; 0.061 sec/batch; 99h:15m:57s remains)
INFO - root - 2019-11-04 02:05:44.491170: step 111990, total loss = 0.46, predict loss = 0.11 (64.7 examples/sec; 0.062 sec/batch; 101h:10m:58s remains)
INFO - root - 2019-11-04 02:05:45.148458: step 112000, total loss = 0.58, predict loss = 0.14 (66.5 examples/sec; 0.060 sec/batch; 98h:24m:29s remains)
INFO - root - 2019-11-04 02:05:45.794521: step 112010, total loss = 0.60, predict loss = 0.14 (68.0 examples/sec; 0.059 sec/batch; 96h:10m:05s remains)
INFO - root - 2019-11-04 02:05:46.452148: step 112020, total loss = 0.55, predict loss = 0.13 (77.7 examples/sec; 0.051 sec/batch; 84h:08m:39s remains)
INFO - root - 2019-11-04 02:05:47.065868: step 112030, total loss = 0.50, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 97h:37m:49s remains)
INFO - root - 2019-11-04 02:05:47.663845: step 112040, total loss = 0.51, predict loss = 0.12 (73.7 examples/sec; 0.054 sec/batch; 88h:47m:12s remains)
INFO - root - 2019-11-04 02:05:48.308781: step 112050, total loss = 0.54, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 96h:07m:45s remains)
INFO - root - 2019-11-04 02:05:48.913445: step 112060, total loss = 0.43, predict loss = 0.10 (83.4 examples/sec; 0.048 sec/batch; 78h:27m:40s remains)
INFO - root - 2019-11-04 02:05:49.530709: step 112070, total loss = 0.45, predict loss = 0.11 (74.0 examples/sec; 0.054 sec/batch; 88h:21m:13s remains)
INFO - root - 2019-11-04 02:05:50.149762: step 112080, total loss = 0.54, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 88h:29m:45s remains)
INFO - root - 2019-11-04 02:05:50.769119: step 112090, total loss = 0.44, predict loss = 0.09 (75.2 examples/sec; 0.053 sec/batch; 87h:01m:25s remains)
INFO - root - 2019-11-04 02:05:51.400156: step 112100, total loss = 0.39, predict loss = 0.09 (77.1 examples/sec; 0.052 sec/batch; 84h:52m:58s remains)
INFO - root - 2019-11-04 02:05:51.991809: step 112110, total loss = 0.54, predict loss = 0.12 (82.5 examples/sec; 0.048 sec/batch; 79h:18m:56s remains)
INFO - root - 2019-11-04 02:05:52.592609: step 112120, total loss = 0.63, predict loss = 0.14 (75.4 examples/sec; 0.053 sec/batch; 86h:46m:31s remains)
INFO - root - 2019-11-04 02:05:53.216005: step 112130, total loss = 0.39, predict loss = 0.09 (69.1 examples/sec; 0.058 sec/batch; 94h:44m:16s remains)
INFO - root - 2019-11-04 02:05:53.863980: step 112140, total loss = 0.40, predict loss = 0.09 (67.5 examples/sec; 0.059 sec/batch; 96h:51m:11s remains)
INFO - root - 2019-11-04 02:05:54.531309: step 112150, total loss = 0.40, predict loss = 0.09 (67.2 examples/sec; 0.060 sec/batch; 97h:22m:35s remains)
INFO - root - 2019-11-04 02:05:55.176274: step 112160, total loss = 0.36, predict loss = 0.08 (62.0 examples/sec; 0.065 sec/batch; 105h:31m:29s remains)
INFO - root - 2019-11-04 02:05:55.816452: step 112170, total loss = 0.28, predict loss = 0.06 (79.9 examples/sec; 0.050 sec/batch; 81h:53m:19s remains)
INFO - root - 2019-11-04 02:05:56.488906: step 112180, total loss = 0.39, predict loss = 0.09 (68.8 examples/sec; 0.058 sec/batch; 95h:05m:54s remains)
INFO - root - 2019-11-04 02:05:57.144200: step 112190, total loss = 0.34, predict loss = 0.07 (73.0 examples/sec; 0.055 sec/batch; 89h:36m:09s remains)
INFO - root - 2019-11-04 02:05:57.765717: step 112200, total loss = 0.40, predict loss = 0.09 (62.9 examples/sec; 0.064 sec/batch; 104h:00m:25s remains)
INFO - root - 2019-11-04 02:05:58.468714: step 112210, total loss = 0.42, predict loss = 0.10 (62.7 examples/sec; 0.064 sec/batch; 104h:20m:45s remains)
INFO - root - 2019-11-04 02:05:59.173440: step 112220, total loss = 0.41, predict loss = 0.08 (75.1 examples/sec; 0.053 sec/batch; 87h:05m:23s remains)
INFO - root - 2019-11-04 02:05:59.787922: step 112230, total loss = 0.49, predict loss = 0.12 (70.7 examples/sec; 0.057 sec/batch; 92h:31m:45s remains)
INFO - root - 2019-11-04 02:06:00.400875: step 112240, total loss = 0.43, predict loss = 0.10 (77.0 examples/sec; 0.052 sec/batch; 84h:55m:25s remains)
INFO - root - 2019-11-04 02:06:01.020085: step 112250, total loss = 0.48, predict loss = 0.11 (71.5 examples/sec; 0.056 sec/batch; 91h:28m:17s remains)
INFO - root - 2019-11-04 02:06:01.655168: step 112260, total loss = 0.43, predict loss = 0.10 (74.0 examples/sec; 0.054 sec/batch; 88h:23m:24s remains)
INFO - root - 2019-11-04 02:06:02.263951: step 112270, total loss = 0.57, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 95h:05m:04s remains)
INFO - root - 2019-11-04 02:06:02.890784: step 112280, total loss = 0.48, predict loss = 0.11 (69.9 examples/sec; 0.057 sec/batch; 93h:38m:39s remains)
INFO - root - 2019-11-04 02:06:03.514658: step 112290, total loss = 0.63, predict loss = 0.15 (70.5 examples/sec; 0.057 sec/batch; 92h:48m:46s remains)
INFO - root - 2019-11-04 02:06:04.137813: step 112300, total loss = 0.50, predict loss = 0.12 (63.4 examples/sec; 0.063 sec/batch; 103h:14m:44s remains)
INFO - root - 2019-11-04 02:06:04.777765: step 112310, total loss = 0.60, predict loss = 0.14 (72.1 examples/sec; 0.056 sec/batch; 90h:47m:29s remains)
INFO - root - 2019-11-04 02:06:05.383258: step 112320, total loss = 0.69, predict loss = 0.17 (78.7 examples/sec; 0.051 sec/batch; 83h:06m:30s remains)
INFO - root - 2019-11-04 02:06:06.002747: step 112330, total loss = 0.53, predict loss = 0.13 (78.1 examples/sec; 0.051 sec/batch; 83h:47m:44s remains)
INFO - root - 2019-11-04 02:06:06.597068: step 112340, total loss = 0.75, predict loss = 0.19 (70.1 examples/sec; 0.057 sec/batch; 93h:19m:22s remains)
INFO - root - 2019-11-04 02:06:07.176212: step 112350, total loss = 0.57, predict loss = 0.14 (76.8 examples/sec; 0.052 sec/batch; 85h:09m:20s remains)
INFO - root - 2019-11-04 02:06:07.799041: step 112360, total loss = 0.65, predict loss = 0.15 (66.0 examples/sec; 0.061 sec/batch; 99h:10m:25s remains)
INFO - root - 2019-11-04 02:06:08.451401: step 112370, total loss = 0.62, predict loss = 0.16 (62.2 examples/sec; 0.064 sec/batch; 105h:08m:16s remains)
INFO - root - 2019-11-04 02:06:09.103458: step 112380, total loss = 0.50, predict loss = 0.12 (76.9 examples/sec; 0.052 sec/batch; 85h:06m:01s remains)
INFO - root - 2019-11-04 02:06:09.702524: step 112390, total loss = 0.56, predict loss = 0.13 (76.5 examples/sec; 0.052 sec/batch; 85h:29m:07s remains)
INFO - root - 2019-11-04 02:06:10.351689: step 112400, total loss = 0.51, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 97h:43m:03s remains)
INFO - root - 2019-11-04 02:06:11.020055: step 112410, total loss = 0.42, predict loss = 0.10 (59.9 examples/sec; 0.067 sec/batch; 109h:09m:58s remains)
INFO - root - 2019-11-04 02:06:11.691748: step 112420, total loss = 0.47, predict loss = 0.11 (67.9 examples/sec; 0.059 sec/batch; 96h:21m:52s remains)
INFO - root - 2019-11-04 02:06:12.330456: step 112430, total loss = 0.43, predict loss = 0.10 (74.1 examples/sec; 0.054 sec/batch; 88h:18m:41s remains)
INFO - root - 2019-11-04 02:06:12.940502: step 112440, total loss = 0.48, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 98h:27m:37s remains)
INFO - root - 2019-11-04 02:06:13.588617: step 112450, total loss = 0.41, predict loss = 0.09 (65.7 examples/sec; 0.061 sec/batch; 99h:33m:13s remains)
INFO - root - 2019-11-04 02:06:14.277546: step 112460, total loss = 0.36, predict loss = 0.08 (63.6 examples/sec; 0.063 sec/batch; 102h:53m:40s remains)
INFO - root - 2019-11-04 02:06:14.903499: step 112470, total loss = 0.41, predict loss = 0.09 (74.3 examples/sec; 0.054 sec/batch; 88h:01m:20s remains)
INFO - root - 2019-11-04 02:06:15.499369: step 112480, total loss = 0.44, predict loss = 0.10 (70.5 examples/sec; 0.057 sec/batch; 92h:50m:02s remains)
INFO - root - 2019-11-04 02:06:16.161854: step 112490, total loss = 0.29, predict loss = 0.06 (70.9 examples/sec; 0.056 sec/batch; 92h:15m:09s remains)
INFO - root - 2019-11-04 02:06:16.789599: step 112500, total loss = 0.40, predict loss = 0.09 (76.6 examples/sec; 0.052 sec/batch; 85h:24m:49s remains)
INFO - root - 2019-11-04 02:06:17.408505: step 112510, total loss = 0.54, predict loss = 0.13 (65.4 examples/sec; 0.061 sec/batch; 100h:04m:58s remains)
INFO - root - 2019-11-04 02:06:18.055421: step 112520, total loss = 0.62, predict loss = 0.15 (74.0 examples/sec; 0.054 sec/batch; 88h:25m:02s remains)
INFO - root - 2019-11-04 02:06:18.731170: step 112530, total loss = 0.50, predict loss = 0.12 (63.3 examples/sec; 0.063 sec/batch; 103h:18m:48s remains)
INFO - root - 2019-11-04 02:06:19.377565: step 112540, total loss = 0.58, predict loss = 0.13 (69.0 examples/sec; 0.058 sec/batch; 94h:50m:17s remains)
INFO - root - 2019-11-04 02:06:19.999380: step 112550, total loss = 0.55, predict loss = 0.14 (75.4 examples/sec; 0.053 sec/batch; 86h:48m:57s remains)
INFO - root - 2019-11-04 02:06:20.626160: step 112560, total loss = 0.56, predict loss = 0.13 (81.2 examples/sec; 0.049 sec/batch; 80h:34m:35s remains)
INFO - root - 2019-11-04 02:06:21.226527: step 112570, total loss = 0.58, predict loss = 0.14 (82.0 examples/sec; 0.049 sec/batch; 79h:47m:13s remains)
INFO - root - 2019-11-04 02:06:21.847428: step 112580, total loss = 0.51, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 96h:27m:52s remains)
INFO - root - 2019-11-04 02:06:22.457433: step 112590, total loss = 0.44, predict loss = 0.10 (69.0 examples/sec; 0.058 sec/batch; 94h:45m:08s remains)
INFO - root - 2019-11-04 02:06:23.095818: step 112600, total loss = 0.60, predict loss = 0.14 (68.4 examples/sec; 0.058 sec/batch; 95h:38m:40s remains)
INFO - root - 2019-11-04 02:06:23.738472: step 112610, total loss = 0.52, predict loss = 0.13 (68.4 examples/sec; 0.058 sec/batch; 95h:38m:54s remains)
INFO - root - 2019-11-04 02:06:24.363949: step 112620, total loss = 0.48, predict loss = 0.11 (71.5 examples/sec; 0.056 sec/batch; 91h:31m:12s remains)
INFO - root - 2019-11-04 02:06:25.002570: step 112630, total loss = 0.43, predict loss = 0.10 (67.0 examples/sec; 0.060 sec/batch; 97h:35m:49s remains)
INFO - root - 2019-11-04 02:06:25.649101: step 112640, total loss = 0.59, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 96h:02m:42s remains)
INFO - root - 2019-11-04 02:06:26.277970: step 112650, total loss = 0.50, predict loss = 0.11 (69.6 examples/sec; 0.058 sec/batch; 94h:03m:00s remains)
INFO - root - 2019-11-04 02:06:26.909772: step 112660, total loss = 0.48, predict loss = 0.12 (63.5 examples/sec; 0.063 sec/batch; 103h:01m:22s remains)
INFO - root - 2019-11-04 02:06:27.528818: step 112670, total loss = 0.46, predict loss = 0.11 (67.1 examples/sec; 0.060 sec/batch; 97h:24m:57s remains)
INFO - root - 2019-11-04 02:06:28.131822: step 112680, total loss = 0.49, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 85h:19m:55s remains)
INFO - root - 2019-11-04 02:06:28.751932: step 112690, total loss = 0.43, predict loss = 0.09 (79.0 examples/sec; 0.051 sec/batch; 82h:49m:22s remains)
INFO - root - 2019-11-04 02:06:29.408144: step 112700, total loss = 0.39, predict loss = 0.09 (64.1 examples/sec; 0.062 sec/batch; 102h:07m:20s remains)
INFO - root - 2019-11-04 02:06:30.032943: step 112710, total loss = 0.46, predict loss = 0.10 (77.6 examples/sec; 0.052 sec/batch; 84h:17m:21s remains)
INFO - root - 2019-11-04 02:06:30.659831: step 112720, total loss = 0.45, predict loss = 0.11 (74.7 examples/sec; 0.054 sec/batch; 87h:35m:29s remains)
INFO - root - 2019-11-04 02:06:31.276896: step 112730, total loss = 0.48, predict loss = 0.12 (73.7 examples/sec; 0.054 sec/batch; 88h:44m:32s remains)
INFO - root - 2019-11-04 02:06:31.893234: step 112740, total loss = 0.41, predict loss = 0.09 (74.6 examples/sec; 0.054 sec/batch; 87h:38m:04s remains)
INFO - root - 2019-11-04 02:06:32.520623: step 112750, total loss = 0.44, predict loss = 0.11 (78.1 examples/sec; 0.051 sec/batch; 83h:45m:24s remains)
INFO - root - 2019-11-04 02:06:33.136301: step 112760, total loss = 0.50, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 94h:24m:35s remains)
INFO - root - 2019-11-04 02:06:33.755495: step 112770, total loss = 0.38, predict loss = 0.08 (71.0 examples/sec; 0.056 sec/batch; 92h:04m:56s remains)
INFO - root - 2019-11-04 02:06:34.393275: step 112780, total loss = 0.50, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 98h:47m:27s remains)
INFO - root - 2019-11-04 02:06:35.021072: step 112790, total loss = 0.44, predict loss = 0.10 (72.0 examples/sec; 0.056 sec/batch; 90h:48m:58s remains)
INFO - root - 2019-11-04 02:06:35.636855: step 112800, total loss = 0.40, predict loss = 0.09 (73.0 examples/sec; 0.055 sec/batch; 89h:35m:58s remains)
INFO - root - 2019-11-04 02:06:36.244569: step 112810, total loss = 0.50, predict loss = 0.11 (72.0 examples/sec; 0.056 sec/batch; 90h:54m:48s remains)
INFO - root - 2019-11-04 02:06:36.857764: step 112820, total loss = 0.38, predict loss = 0.09 (86.9 examples/sec; 0.046 sec/batch; 75h:16m:42s remains)
INFO - root - 2019-11-04 02:06:37.480939: step 112830, total loss = 0.61, predict loss = 0.15 (62.8 examples/sec; 0.064 sec/batch; 104h:05m:05s remains)
INFO - root - 2019-11-04 02:06:38.100413: step 112840, total loss = 0.44, predict loss = 0.10 (68.3 examples/sec; 0.059 sec/batch; 95h:49m:58s remains)
INFO - root - 2019-11-04 02:06:38.730556: step 112850, total loss = 0.47, predict loss = 0.11 (72.6 examples/sec; 0.055 sec/batch; 90h:02m:21s remains)
INFO - root - 2019-11-04 02:06:39.361683: step 112860, total loss = 0.36, predict loss = 0.08 (71.2 examples/sec; 0.056 sec/batch; 91h:54m:51s remains)
INFO - root - 2019-11-04 02:06:39.985287: step 112870, total loss = 0.46, predict loss = 0.11 (76.3 examples/sec; 0.052 sec/batch; 85h:41m:57s remains)
INFO - root - 2019-11-04 02:06:40.629119: step 112880, total loss = 0.56, predict loss = 0.14 (72.7 examples/sec; 0.055 sec/batch; 89h:57m:32s remains)
INFO - root - 2019-11-04 02:06:41.257547: step 112890, total loss = 0.56, predict loss = 0.14 (74.6 examples/sec; 0.054 sec/batch; 87h:39m:01s remains)
INFO - root - 2019-11-04 02:06:41.896565: step 112900, total loss = 0.66, predict loss = 0.16 (75.4 examples/sec; 0.053 sec/batch; 86h:43m:25s remains)
INFO - root - 2019-11-04 02:06:42.553764: step 112910, total loss = 0.54, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 100h:06m:48s remains)
INFO - root - 2019-11-04 02:06:43.194863: step 112920, total loss = 0.66, predict loss = 0.16 (80.1 examples/sec; 0.050 sec/batch; 81h:42m:10s remains)
INFO - root - 2019-11-04 02:06:43.816091: step 112930, total loss = 0.62, predict loss = 0.16 (68.5 examples/sec; 0.058 sec/batch; 95h:26m:56s remains)
INFO - root - 2019-11-04 02:06:44.460163: step 112940, total loss = 0.56, predict loss = 0.13 (68.7 examples/sec; 0.058 sec/batch; 95h:09m:13s remains)
INFO - root - 2019-11-04 02:06:45.110462: step 112950, total loss = 0.39, predict loss = 0.09 (69.9 examples/sec; 0.057 sec/batch; 93h:33m:48s remains)
INFO - root - 2019-11-04 02:06:45.746876: step 112960, total loss = 0.61, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 92h:38m:02s remains)
INFO - root - 2019-11-04 02:06:46.366194: step 112970, total loss = 0.65, predict loss = 0.15 (76.3 examples/sec; 0.052 sec/batch; 85h:46m:46s remains)
INFO - root - 2019-11-04 02:06:46.971365: step 112980, total loss = 0.72, predict loss = 0.17 (67.1 examples/sec; 0.060 sec/batch; 97h:29m:11s remains)
INFO - root - 2019-11-04 02:06:47.606099: step 112990, total loss = 0.71, predict loss = 0.16 (67.2 examples/sec; 0.060 sec/batch; 97h:18m:50s remains)
INFO - root - 2019-11-04 02:06:48.225899: step 113000, total loss = 0.41, predict loss = 0.10 (77.7 examples/sec; 0.051 sec/batch; 84h:10m:09s remains)
INFO - root - 2019-11-04 02:06:48.840944: step 113010, total loss = 0.55, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 99h:11m:21s remains)
INFO - root - 2019-11-04 02:06:49.458046: step 113020, total loss = 0.58, predict loss = 0.13 (80.1 examples/sec; 0.050 sec/batch; 81h:41m:27s remains)
INFO - root - 2019-11-04 02:06:50.137855: step 113030, total loss = 0.47, predict loss = 0.10 (76.3 examples/sec; 0.052 sec/batch; 85h:41m:58s remains)
INFO - root - 2019-11-04 02:06:50.739959: step 113040, total loss = 0.47, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 85h:14m:56s remains)
INFO - root - 2019-11-04 02:06:51.389906: step 113050, total loss = 0.39, predict loss = 0.09 (63.9 examples/sec; 0.063 sec/batch; 102h:23m:47s remains)
INFO - root - 2019-11-04 02:06:52.026576: step 113060, total loss = 0.49, predict loss = 0.11 (74.4 examples/sec; 0.054 sec/batch; 87h:58m:25s remains)
INFO - root - 2019-11-04 02:06:52.647800: step 113070, total loss = 0.58, predict loss = 0.14 (73.2 examples/sec; 0.055 sec/batch; 89h:19m:54s remains)
INFO - root - 2019-11-04 02:06:53.259206: step 113080, total loss = 0.39, predict loss = 0.08 (73.9 examples/sec; 0.054 sec/batch; 88h:31m:51s remains)
INFO - root - 2019-11-04 02:06:53.879812: step 113090, total loss = 0.62, predict loss = 0.15 (74.6 examples/sec; 0.054 sec/batch; 87h:43m:31s remains)
INFO - root - 2019-11-04 02:06:54.534657: step 113100, total loss = 0.58, predict loss = 0.13 (76.0 examples/sec; 0.053 sec/batch; 86h:05m:18s remains)
INFO - root - 2019-11-04 02:06:55.173915: step 113110, total loss = 0.47, predict loss = 0.11 (66.7 examples/sec; 0.060 sec/batch; 98h:05m:33s remains)
INFO - root - 2019-11-04 02:06:55.815576: step 113120, total loss = 0.58, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 95h:30m:32s remains)
INFO - root - 2019-11-04 02:06:56.474951: step 113130, total loss = 0.65, predict loss = 0.15 (63.6 examples/sec; 0.063 sec/batch; 102h:50m:48s remains)
INFO - root - 2019-11-04 02:06:57.088321: step 113140, total loss = 0.41, predict loss = 0.09 (76.9 examples/sec; 0.052 sec/batch; 85h:04m:00s remains)
INFO - root - 2019-11-04 02:06:57.683565: step 113150, total loss = 0.44, predict loss = 0.10 (76.9 examples/sec; 0.052 sec/batch; 85h:00m:09s remains)
INFO - root - 2019-11-04 02:06:58.253249: step 113160, total loss = 0.55, predict loss = 0.12 (94.8 examples/sec; 0.042 sec/batch; 69h:00m:12s remains)
INFO - root - 2019-11-04 02:06:58.797348: step 113170, total loss = 0.59, predict loss = 0.13 (87.8 examples/sec; 0.046 sec/batch; 74h:31m:52s remains)
INFO - root - 2019-11-04 02:06:59.254733: step 113180, total loss = 0.48, predict loss = 0.12 (99.8 examples/sec; 0.040 sec/batch; 65h:31m:58s remains)
INFO - root - 2019-11-04 02:07:00.367632: step 113190, total loss = 0.19, predict loss = 0.03 (64.6 examples/sec; 0.062 sec/batch; 101h:19m:30s remains)
INFO - root - 2019-11-04 02:07:00.981083: step 113200, total loss = 0.50, predict loss = 0.11 (68.0 examples/sec; 0.059 sec/batch; 96h:08m:56s remains)
INFO - root - 2019-11-04 02:07:01.609599: step 113210, total loss = 0.33, predict loss = 0.07 (77.9 examples/sec; 0.051 sec/batch; 84h:00m:05s remains)
INFO - root - 2019-11-04 02:07:02.213725: step 113220, total loss = 0.54, predict loss = 0.13 (72.5 examples/sec; 0.055 sec/batch; 90h:12m:25s remains)
INFO - root - 2019-11-04 02:07:02.834271: step 113230, total loss = 0.50, predict loss = 0.11 (68.5 examples/sec; 0.058 sec/batch; 95h:27m:02s remains)
INFO - root - 2019-11-04 02:07:03.503828: step 113240, total loss = 0.59, predict loss = 0.14 (74.4 examples/sec; 0.054 sec/batch; 87h:53m:44s remains)
INFO - root - 2019-11-04 02:07:04.156314: step 113250, total loss = 0.42, predict loss = 0.09 (66.0 examples/sec; 0.061 sec/batch; 99h:03m:25s remains)
INFO - root - 2019-11-04 02:07:04.760233: step 113260, total loss = 0.28, predict loss = 0.06 (74.9 examples/sec; 0.053 sec/batch; 87h:21m:36s remains)
INFO - root - 2019-11-04 02:07:05.368174: step 113270, total loss = 0.42, predict loss = 0.09 (68.8 examples/sec; 0.058 sec/batch; 95h:02m:21s remains)
INFO - root - 2019-11-04 02:07:06.045779: step 113280, total loss = 0.56, predict loss = 0.12 (63.6 examples/sec; 0.063 sec/batch; 102h:52m:21s remains)
INFO - root - 2019-11-04 02:07:06.694023: step 113290, total loss = 0.60, predict loss = 0.13 (71.5 examples/sec; 0.056 sec/batch; 91h:31m:39s remains)
INFO - root - 2019-11-04 02:07:07.387606: step 113300, total loss = 0.45, predict loss = 0.10 (64.0 examples/sec; 0.063 sec/batch; 102h:14m:47s remains)
INFO - root - 2019-11-04 02:07:08.051064: step 113310, total loss = 0.58, predict loss = 0.13 (70.6 examples/sec; 0.057 sec/batch; 92h:35m:21s remains)
INFO - root - 2019-11-04 02:07:08.705482: step 113320, total loss = 0.50, predict loss = 0.11 (77.2 examples/sec; 0.052 sec/batch; 84h:40m:31s remains)
INFO - root - 2019-11-04 02:07:09.365081: step 113330, total loss = 0.47, predict loss = 0.10 (74.2 examples/sec; 0.054 sec/batch; 88h:05m:30s remains)
INFO - root - 2019-11-04 02:07:10.024832: step 113340, total loss = 0.51, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:29m:11s remains)
INFO - root - 2019-11-04 02:07:10.668951: step 113350, total loss = 0.56, predict loss = 0.13 (73.3 examples/sec; 0.055 sec/batch; 89h:17m:06s remains)
INFO - root - 2019-11-04 02:07:11.274807: step 113360, total loss = 0.46, predict loss = 0.10 (69.3 examples/sec; 0.058 sec/batch; 94h:25m:57s remains)
INFO - root - 2019-11-04 02:07:11.907947: step 113370, total loss = 0.50, predict loss = 0.11 (71.0 examples/sec; 0.056 sec/batch; 92h:07m:05s remains)
INFO - root - 2019-11-04 02:07:12.558335: step 113380, total loss = 0.47, predict loss = 0.11 (71.4 examples/sec; 0.056 sec/batch; 91h:36m:39s remains)
INFO - root - 2019-11-04 02:07:13.202143: step 113390, total loss = 0.52, predict loss = 0.09 (64.4 examples/sec; 0.062 sec/batch; 101h:34m:00s remains)
INFO - root - 2019-11-04 02:07:13.802181: step 113400, total loss = 0.47, predict loss = 0.08 (81.0 examples/sec; 0.049 sec/batch; 80h:45m:42s remains)
INFO - root - 2019-11-04 02:07:14.403248: step 113410, total loss = 0.58, predict loss = 0.13 (74.9 examples/sec; 0.053 sec/batch; 87h:20m:42s remains)
INFO - root - 2019-11-04 02:07:14.999462: step 113420, total loss = 0.62, predict loss = 0.15 (75.1 examples/sec; 0.053 sec/batch; 87h:04m:03s remains)
INFO - root - 2019-11-04 02:07:15.625306: step 113430, total loss = 0.66, predict loss = 0.16 (62.8 examples/sec; 0.064 sec/batch; 104h:13m:31s remains)
INFO - root - 2019-11-04 02:07:16.285359: step 113440, total loss = 0.76, predict loss = 0.17 (65.7 examples/sec; 0.061 sec/batch; 99h:31m:05s remains)
INFO - root - 2019-11-04 02:07:16.933275: step 113450, total loss = 0.55, predict loss = 0.12 (71.1 examples/sec; 0.056 sec/batch; 92h:01m:06s remains)
INFO - root - 2019-11-04 02:07:17.560271: step 113460, total loss = 0.70, predict loss = 0.16 (82.2 examples/sec; 0.049 sec/batch; 79h:36m:44s remains)
INFO - root - 2019-11-04 02:07:18.246899: step 113470, total loss = 0.62, predict loss = 0.14 (64.0 examples/sec; 0.062 sec/batch; 102h:09m:48s remains)
INFO - root - 2019-11-04 02:07:18.901364: step 113480, total loss = 0.52, predict loss = 0.11 (63.4 examples/sec; 0.063 sec/batch; 103h:05m:20s remains)
INFO - root - 2019-11-04 02:07:19.577560: step 113490, total loss = 0.84, predict loss = 0.20 (66.5 examples/sec; 0.060 sec/batch; 98h:23m:24s remains)
INFO - root - 2019-11-04 02:07:20.179127: step 113500, total loss = 0.59, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 93h:41m:32s remains)
INFO - root - 2019-11-04 02:07:20.807723: step 113510, total loss = 0.62, predict loss = 0.14 (73.2 examples/sec; 0.055 sec/batch; 89h:17m:53s remains)
INFO - root - 2019-11-04 02:07:21.431311: step 113520, total loss = 0.55, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 93h:04m:41s remains)
INFO - root - 2019-11-04 02:07:22.058789: step 113530, total loss = 0.47, predict loss = 0.11 (71.4 examples/sec; 0.056 sec/batch; 91h:37m:00s remains)
INFO - root - 2019-11-04 02:07:22.732124: step 113540, total loss = 0.61, predict loss = 0.14 (74.2 examples/sec; 0.054 sec/batch; 88h:08m:46s remains)
INFO - root - 2019-11-04 02:07:23.341973: step 113550, total loss = 0.51, predict loss = 0.11 (75.4 examples/sec; 0.053 sec/batch; 86h:45m:00s remains)
INFO - root - 2019-11-04 02:07:23.974710: step 113560, total loss = 0.50, predict loss = 0.11 (62.5 examples/sec; 0.064 sec/batch; 104h:36m:04s remains)
INFO - root - 2019-11-04 02:07:24.611156: step 113570, total loss = 0.50, predict loss = 0.11 (69.1 examples/sec; 0.058 sec/batch; 94h:37m:08s remains)
INFO - root - 2019-11-04 02:07:25.225448: step 113580, total loss = 0.48, predict loss = 0.10 (67.6 examples/sec; 0.059 sec/batch; 96h:46m:16s remains)
INFO - root - 2019-11-04 02:07:25.870367: step 113590, total loss = 0.66, predict loss = 0.15 (60.8 examples/sec; 0.066 sec/batch; 107h:33m:57s remains)
INFO - root - 2019-11-04 02:07:26.501160: step 113600, total loss = 0.55, predict loss = 0.12 (80.8 examples/sec; 0.050 sec/batch; 80h:56m:51s remains)
INFO - root - 2019-11-04 02:07:27.110602: step 113610, total loss = 0.56, predict loss = 0.13 (80.8 examples/sec; 0.049 sec/batch; 80h:53m:54s remains)
INFO - root - 2019-11-04 02:07:27.754303: step 113620, total loss = 0.78, predict loss = 0.19 (75.7 examples/sec; 0.053 sec/batch; 86h:20m:58s remains)
INFO - root - 2019-11-04 02:07:28.356611: step 113630, total loss = 0.39, predict loss = 0.09 (71.9 examples/sec; 0.056 sec/batch; 90h:58m:33s remains)
INFO - root - 2019-11-04 02:07:28.960033: step 113640, total loss = 0.39, predict loss = 0.09 (73.5 examples/sec; 0.054 sec/batch; 88h:56m:28s remains)
INFO - root - 2019-11-04 02:07:29.576709: step 113650, total loss = 0.38, predict loss = 0.08 (70.3 examples/sec; 0.057 sec/batch; 92h:59m:47s remains)
INFO - root - 2019-11-04 02:07:30.217469: step 113660, total loss = 0.43, predict loss = 0.10 (66.0 examples/sec; 0.061 sec/batch; 99h:07m:21s remains)
INFO - root - 2019-11-04 02:07:30.933833: step 113670, total loss = 0.67, predict loss = 0.16 (64.8 examples/sec; 0.062 sec/batch; 100h:51m:38s remains)
INFO - root - 2019-11-04 02:07:31.527214: step 113680, total loss = 0.51, predict loss = 0.13 (75.1 examples/sec; 0.053 sec/batch; 87h:05m:18s remains)
INFO - root - 2019-11-04 02:07:32.156629: step 113690, total loss = 0.33, predict loss = 0.07 (76.5 examples/sec; 0.052 sec/batch; 85h:27m:58s remains)
INFO - root - 2019-11-04 02:07:32.770764: step 113700, total loss = 0.61, predict loss = 0.14 (67.8 examples/sec; 0.059 sec/batch; 96h:29m:16s remains)
INFO - root - 2019-11-04 02:07:33.394731: step 113710, total loss = 0.61, predict loss = 0.15 (67.0 examples/sec; 0.060 sec/batch; 97h:39m:59s remains)
INFO - root - 2019-11-04 02:07:34.012310: step 113720, total loss = 0.63, predict loss = 0.16 (63.2 examples/sec; 0.063 sec/batch; 103h:29m:44s remains)
INFO - root - 2019-11-04 02:07:34.630867: step 113730, total loss = 0.49, predict loss = 0.12 (65.0 examples/sec; 0.061 sec/batch; 100h:33m:17s remains)
INFO - root - 2019-11-04 02:07:35.231363: step 113740, total loss = 0.62, predict loss = 0.15 (78.2 examples/sec; 0.051 sec/batch; 83h:35m:32s remains)
INFO - root - 2019-11-04 02:07:35.840883: step 113750, total loss = 0.38, predict loss = 0.09 (67.8 examples/sec; 0.059 sec/batch; 96h:29m:59s remains)
INFO - root - 2019-11-04 02:07:36.463095: step 113760, total loss = 0.33, predict loss = 0.07 (77.5 examples/sec; 0.052 sec/batch; 84h:20m:56s remains)
INFO - root - 2019-11-04 02:07:37.089238: step 113770, total loss = 0.28, predict loss = 0.06 (61.7 examples/sec; 0.065 sec/batch; 105h:58m:54s remains)
INFO - root - 2019-11-04 02:07:37.729352: step 113780, total loss = 0.52, predict loss = 0.12 (64.1 examples/sec; 0.062 sec/batch; 102h:00m:19s remains)
INFO - root - 2019-11-04 02:07:38.381948: step 113790, total loss = 0.46, predict loss = 0.11 (74.3 examples/sec; 0.054 sec/batch; 87h:59m:29s remains)
INFO - root - 2019-11-04 02:07:39.024303: step 113800, total loss = 0.25, predict loss = 0.05 (74.4 examples/sec; 0.054 sec/batch; 87h:54m:44s remains)
INFO - root - 2019-11-04 02:07:39.659862: step 113810, total loss = 0.31, predict loss = 0.07 (72.0 examples/sec; 0.056 sec/batch; 90h:48m:11s remains)
INFO - root - 2019-11-04 02:07:40.371444: step 113820, total loss = 0.51, predict loss = 0.11 (68.0 examples/sec; 0.059 sec/batch; 96h:10m:42s remains)
INFO - root - 2019-11-04 02:07:41.035247: step 113830, total loss = 0.45, predict loss = 0.10 (64.4 examples/sec; 0.062 sec/batch; 101h:31m:21s remains)
INFO - root - 2019-11-04 02:07:41.684832: step 113840, total loss = 0.34, predict loss = 0.07 (70.0 examples/sec; 0.057 sec/batch; 93h:26m:31s remains)
INFO - root - 2019-11-04 02:07:42.300775: step 113850, total loss = 0.42, predict loss = 0.09 (79.5 examples/sec; 0.050 sec/batch; 82h:17m:56s remains)
INFO - root - 2019-11-04 02:07:42.912583: step 113860, total loss = 0.56, predict loss = 0.13 (70.9 examples/sec; 0.056 sec/batch; 92h:15m:23s remains)
INFO - root - 2019-11-04 02:07:43.498063: step 113870, total loss = 0.50, predict loss = 0.13 (75.3 examples/sec; 0.053 sec/batch; 86h:51m:31s remains)
INFO - root - 2019-11-04 02:07:44.075046: step 113880, total loss = 0.55, predict loss = 0.13 (80.5 examples/sec; 0.050 sec/batch; 81h:16m:44s remains)
INFO - root - 2019-11-04 02:07:44.679240: step 113890, total loss = 0.54, predict loss = 0.13 (58.9 examples/sec; 0.068 sec/batch; 110h:56m:49s remains)
INFO - root - 2019-11-04 02:07:45.348902: step 113900, total loss = 0.70, predict loss = 0.19 (67.8 examples/sec; 0.059 sec/batch; 96h:28m:43s remains)
INFO - root - 2019-11-04 02:07:46.005862: step 113910, total loss = 0.46, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 95h:37m:44s remains)
INFO - root - 2019-11-04 02:07:46.679918: step 113920, total loss = 0.48, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 96h:57m:55s remains)
INFO - root - 2019-11-04 02:07:47.346684: step 113930, total loss = 0.52, predict loss = 0.12 (65.5 examples/sec; 0.061 sec/batch; 99h:47m:24s remains)
INFO - root - 2019-11-04 02:07:47.992100: step 113940, total loss = 0.63, predict loss = 0.15 (71.3 examples/sec; 0.056 sec/batch; 91h:41m:53s remains)
INFO - root - 2019-11-04 02:07:48.617396: step 113950, total loss = 0.55, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 94h:59m:53s remains)
INFO - root - 2019-11-04 02:07:49.215178: step 113960, total loss = 0.44, predict loss = 0.10 (73.6 examples/sec; 0.054 sec/batch; 88h:50m:00s remains)
INFO - root - 2019-11-04 02:07:49.808866: step 113970, total loss = 0.66, predict loss = 0.18 (76.7 examples/sec; 0.052 sec/batch; 85h:16m:31s remains)
INFO - root - 2019-11-04 02:07:50.454293: step 113980, total loss = 0.40, predict loss = 0.09 (76.6 examples/sec; 0.052 sec/batch; 85h:25m:17s remains)
INFO - root - 2019-11-04 02:07:51.072470: step 113990, total loss = 0.67, predict loss = 0.16 (72.1 examples/sec; 0.056 sec/batch; 90h:44m:43s remains)
INFO - root - 2019-11-04 02:07:51.710401: step 114000, total loss = 0.52, predict loss = 0.12 (76.1 examples/sec; 0.053 sec/batch; 85h:56m:17s remains)
INFO - root - 2019-11-04 02:07:52.377549: step 114010, total loss = 0.25, predict loss = 0.05 (59.6 examples/sec; 0.067 sec/batch; 109h:43m:09s remains)
INFO - root - 2019-11-04 02:07:53.044845: step 114020, total loss = 0.24, predict loss = 0.05 (60.3 examples/sec; 0.066 sec/batch; 108h:22m:17s remains)
INFO - root - 2019-11-04 02:07:53.693862: step 114030, total loss = 0.33, predict loss = 0.07 (68.9 examples/sec; 0.058 sec/batch; 94h:52m:32s remains)
INFO - root - 2019-11-04 02:07:54.351152: step 114040, total loss = 0.33, predict loss = 0.07 (70.1 examples/sec; 0.057 sec/batch; 93h:20m:29s remains)
INFO - root - 2019-11-04 02:07:54.987628: step 114050, total loss = 0.27, predict loss = 0.06 (61.9 examples/sec; 0.065 sec/batch; 105h:35m:32s remains)
INFO - root - 2019-11-04 02:07:55.629739: step 114060, total loss = 0.17, predict loss = 0.03 (65.1 examples/sec; 0.061 sec/batch; 100h:29m:02s remains)
INFO - root - 2019-11-04 02:07:56.267950: step 114070, total loss = 0.28, predict loss = 0.06 (72.7 examples/sec; 0.055 sec/batch; 89h:54m:19s remains)
INFO - root - 2019-11-04 02:07:56.919802: step 114080, total loss = 0.39, predict loss = 0.08 (67.1 examples/sec; 0.060 sec/batch; 97h:29m:07s remains)
INFO - root - 2019-11-04 02:07:57.549351: step 114090, total loss = 0.52, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 97h:13m:09s remains)
INFO - root - 2019-11-04 02:07:58.657170: step 114100, total loss = 0.56, predict loss = 0.13 (83.7 examples/sec; 0.048 sec/batch; 78h:05m:26s remains)
INFO - root - 2019-11-04 02:07:59.260514: step 114110, total loss = 0.51, predict loss = 0.12 (72.7 examples/sec; 0.055 sec/batch; 89h:58m:11s remains)
INFO - root - 2019-11-04 02:07:59.877199: step 114120, total loss = 0.38, predict loss = 0.08 (63.9 examples/sec; 0.063 sec/batch; 102h:24m:14s remains)
INFO - root - 2019-11-04 02:08:00.539186: step 114130, total loss = 0.51, predict loss = 0.12 (70.0 examples/sec; 0.057 sec/batch; 93h:26m:30s remains)
INFO - root - 2019-11-04 02:08:01.217113: step 114140, total loss = 0.31, predict loss = 0.07 (64.7 examples/sec; 0.062 sec/batch; 101h:07m:27s remains)
INFO - root - 2019-11-04 02:08:01.849239: step 114150, total loss = 0.30, predict loss = 0.06 (71.4 examples/sec; 0.056 sec/batch; 91h:38m:57s remains)
INFO - root - 2019-11-04 02:08:02.455485: step 114160, total loss = 0.34, predict loss = 0.08 (85.1 examples/sec; 0.047 sec/batch; 76h:52m:16s remains)
INFO - root - 2019-11-04 02:08:03.070429: step 114170, total loss = 0.41, predict loss = 0.09 (65.1 examples/sec; 0.061 sec/batch; 100h:28m:40s remains)
INFO - root - 2019-11-04 02:08:03.675289: step 114180, total loss = 0.38, predict loss = 0.09 (73.3 examples/sec; 0.055 sec/batch; 89h:14m:55s remains)
INFO - root - 2019-11-04 02:08:04.280998: step 114190, total loss = 0.62, predict loss = 0.14 (66.2 examples/sec; 0.060 sec/batch; 98h:44m:32s remains)
INFO - root - 2019-11-04 02:08:04.907821: step 114200, total loss = 0.51, predict loss = 0.12 (72.2 examples/sec; 0.055 sec/batch; 90h:31m:41s remains)
INFO - root - 2019-11-04 02:08:05.591019: step 114210, total loss = 0.54, predict loss = 0.13 (66.0 examples/sec; 0.061 sec/batch; 99h:04m:15s remains)
INFO - root - 2019-11-04 02:08:06.228400: step 114220, total loss = 0.46, predict loss = 0.11 (81.1 examples/sec; 0.049 sec/batch; 80h:38m:33s remains)
INFO - root - 2019-11-04 02:08:06.844606: step 114230, total loss = 0.40, predict loss = 0.09 (66.9 examples/sec; 0.060 sec/batch; 97h:41m:06s remains)
INFO - root - 2019-11-04 02:08:07.481663: step 114240, total loss = 0.40, predict loss = 0.10 (71.3 examples/sec; 0.056 sec/batch; 91h:41m:14s remains)
INFO - root - 2019-11-04 02:08:08.098062: step 114250, total loss = 0.39, predict loss = 0.08 (68.7 examples/sec; 0.058 sec/batch; 95h:09m:15s remains)
INFO - root - 2019-11-04 02:08:08.730614: step 114260, total loss = 0.35, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 83h:34m:15s remains)
INFO - root - 2019-11-04 02:08:09.331375: step 114270, total loss = 0.23, predict loss = 0.05 (76.2 examples/sec; 0.052 sec/batch; 85h:47m:11s remains)
INFO - root - 2019-11-04 02:08:09.927041: step 114280, total loss = 0.29, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 83h:10m:11s remains)
INFO - root - 2019-11-04 02:08:10.543248: step 114290, total loss = 0.35, predict loss = 0.08 (73.3 examples/sec; 0.055 sec/batch; 89h:15m:12s remains)
INFO - root - 2019-11-04 02:08:11.158270: step 114300, total loss = 0.32, predict loss = 0.07 (67.5 examples/sec; 0.059 sec/batch; 96h:52m:56s remains)
INFO - root - 2019-11-04 02:08:11.807614: step 114310, total loss = 0.27, predict loss = 0.06 (69.0 examples/sec; 0.058 sec/batch; 94h:47m:59s remains)
INFO - root - 2019-11-04 02:08:12.489411: step 114320, total loss = 0.36, predict loss = 0.08 (58.0 examples/sec; 0.069 sec/batch; 112h:42m:08s remains)
INFO - root - 2019-11-04 02:08:13.086401: step 114330, total loss = 0.38, predict loss = 0.09 (76.2 examples/sec; 0.052 sec/batch; 85h:48m:47s remains)
INFO - root - 2019-11-04 02:08:13.701502: step 114340, total loss = 0.44, predict loss = 0.10 (68.5 examples/sec; 0.058 sec/batch; 95h:29m:57s remains)
INFO - root - 2019-11-04 02:08:14.330975: step 114350, total loss = 0.52, predict loss = 0.11 (65.0 examples/sec; 0.062 sec/batch; 100h:38m:38s remains)
INFO - root - 2019-11-04 02:08:14.996901: step 114360, total loss = 0.65, predict loss = 0.16 (69.5 examples/sec; 0.058 sec/batch; 94h:03m:03s remains)
INFO - root - 2019-11-04 02:08:15.629335: step 114370, total loss = 0.34, predict loss = 0.07 (69.0 examples/sec; 0.058 sec/batch; 94h:48m:21s remains)
INFO - root - 2019-11-04 02:08:16.258938: step 114380, total loss = 0.41, predict loss = 0.10 (76.0 examples/sec; 0.053 sec/batch; 86h:01m:39s remains)
INFO - root - 2019-11-04 02:08:16.868069: step 114390, total loss = 0.34, predict loss = 0.08 (75.9 examples/sec; 0.053 sec/batch; 86h:11m:43s remains)
INFO - root - 2019-11-04 02:08:17.509605: step 114400, total loss = 0.33, predict loss = 0.08 (63.1 examples/sec; 0.063 sec/batch; 103h:33m:55s remains)
INFO - root - 2019-11-04 02:08:18.200812: step 114410, total loss = 0.34, predict loss = 0.07 (64.5 examples/sec; 0.062 sec/batch; 101h:24m:04s remains)
INFO - root - 2019-11-04 02:08:18.892792: step 114420, total loss = 0.37, predict loss = 0.08 (69.6 examples/sec; 0.057 sec/batch; 94h:00m:18s remains)
INFO - root - 2019-11-04 02:08:19.539544: step 114430, total loss = 0.39, predict loss = 0.09 (67.8 examples/sec; 0.059 sec/batch; 96h:24m:26s remains)
INFO - root - 2019-11-04 02:08:20.161895: step 114440, total loss = 0.59, predict loss = 0.14 (74.7 examples/sec; 0.054 sec/batch; 87h:31m:23s remains)
INFO - root - 2019-11-04 02:08:20.804120: step 114450, total loss = 0.39, predict loss = 0.08 (68.8 examples/sec; 0.058 sec/batch; 95h:01m:58s remains)
INFO - root - 2019-11-04 02:08:21.402491: step 114460, total loss = 0.47, predict loss = 0.11 (77.8 examples/sec; 0.051 sec/batch; 84h:05m:59s remains)
INFO - root - 2019-11-04 02:08:22.024169: step 114470, total loss = 0.39, predict loss = 0.09 (68.7 examples/sec; 0.058 sec/batch; 95h:10m:39s remains)
INFO - root - 2019-11-04 02:08:22.626046: step 114480, total loss = 0.52, predict loss = 0.11 (68.5 examples/sec; 0.058 sec/batch; 95h:29m:53s remains)
INFO - root - 2019-11-04 02:08:23.254268: step 114490, total loss = 0.39, predict loss = 0.08 (71.1 examples/sec; 0.056 sec/batch; 92h:01m:50s remains)
INFO - root - 2019-11-04 02:08:23.956574: step 114500, total loss = 0.45, predict loss = 0.09 (72.7 examples/sec; 0.055 sec/batch; 89h:56m:11s remains)
INFO - root - 2019-11-04 02:08:24.596651: step 114510, total loss = 0.30, predict loss = 0.06 (69.4 examples/sec; 0.058 sec/batch; 94h:12m:05s remains)
INFO - root - 2019-11-04 02:08:25.222346: step 114520, total loss = 0.46, predict loss = 0.10 (68.2 examples/sec; 0.059 sec/batch; 95h:56m:04s remains)
INFO - root - 2019-11-04 02:08:25.870017: step 114530, total loss = 0.28, predict loss = 0.06 (63.3 examples/sec; 0.063 sec/batch; 103h:15m:18s remains)
INFO - root - 2019-11-04 02:08:26.474911: step 114540, total loss = 0.36, predict loss = 0.08 (69.5 examples/sec; 0.058 sec/batch; 94h:03m:54s remains)
INFO - root - 2019-11-04 02:08:27.184967: step 114550, total loss = 0.41, predict loss = 0.09 (65.4 examples/sec; 0.061 sec/batch; 99h:58m:31s remains)
INFO - root - 2019-11-04 02:08:27.833571: step 114560, total loss = 0.34, predict loss = 0.08 (70.5 examples/sec; 0.057 sec/batch; 92h:42m:26s remains)
INFO - root - 2019-11-04 02:08:28.453344: step 114570, total loss = 0.51, predict loss = 0.12 (64.8 examples/sec; 0.062 sec/batch; 100h:52m:31s remains)
INFO - root - 2019-11-04 02:08:29.088779: step 114580, total loss = 0.48, predict loss = 0.11 (70.7 examples/sec; 0.057 sec/batch; 92h:28m:23s remains)
INFO - root - 2019-11-04 02:08:29.758785: step 114590, total loss = 0.50, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 96h:29m:36s remains)
INFO - root - 2019-11-04 02:08:30.420349: step 114600, total loss = 0.37, predict loss = 0.09 (65.1 examples/sec; 0.061 sec/batch; 100h:23m:30s remains)
INFO - root - 2019-11-04 02:08:31.068808: step 114610, total loss = 0.59, predict loss = 0.13 (63.9 examples/sec; 0.063 sec/batch; 102h:18m:45s remains)
INFO - root - 2019-11-04 02:08:31.727747: step 114620, total loss = 0.64, predict loss = 0.15 (71.1 examples/sec; 0.056 sec/batch; 92h:01m:40s remains)
INFO - root - 2019-11-04 02:08:32.392553: step 114630, total loss = 0.68, predict loss = 0.16 (66.8 examples/sec; 0.060 sec/batch; 97h:57m:10s remains)
INFO - root - 2019-11-04 02:08:33.006065: step 114640, total loss = 0.77, predict loss = 0.18 (75.7 examples/sec; 0.053 sec/batch; 86h:26m:00s remains)
INFO - root - 2019-11-04 02:08:33.629705: step 114650, total loss = 0.79, predict loss = 0.20 (64.9 examples/sec; 0.062 sec/batch; 100h:43m:13s remains)
INFO - root - 2019-11-04 02:08:34.295310: step 114660, total loss = 0.83, predict loss = 0.19 (66.6 examples/sec; 0.060 sec/batch; 98h:15m:17s remains)
INFO - root - 2019-11-04 02:08:34.925989: step 114670, total loss = 0.55, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 86h:09m:03s remains)
INFO - root - 2019-11-04 02:08:35.530173: step 114680, total loss = 0.68, predict loss = 0.16 (65.8 examples/sec; 0.061 sec/batch; 99h:26m:37s remains)
INFO - root - 2019-11-04 02:08:36.157115: step 114690, total loss = 0.69, predict loss = 0.16 (69.9 examples/sec; 0.057 sec/batch; 93h:31m:46s remains)
INFO - root - 2019-11-04 02:08:36.771382: step 114700, total loss = 0.48, predict loss = 0.11 (67.1 examples/sec; 0.060 sec/batch; 97h:27m:44s remains)
INFO - root - 2019-11-04 02:08:37.380062: step 114710, total loss = 0.52, predict loss = 0.11 (79.2 examples/sec; 0.050 sec/batch; 82h:32m:31s remains)
INFO - root - 2019-11-04 02:08:38.051384: step 114720, total loss = 0.44, predict loss = 0.10 (70.2 examples/sec; 0.057 sec/batch; 93h:05m:27s remains)
INFO - root - 2019-11-04 02:08:38.673981: step 114730, total loss = 0.53, predict loss = 0.13 (68.7 examples/sec; 0.058 sec/batch; 95h:12m:39s remains)
INFO - root - 2019-11-04 02:08:39.320667: step 114740, total loss = 0.57, predict loss = 0.13 (65.4 examples/sec; 0.061 sec/batch; 100h:02m:20s remains)
INFO - root - 2019-11-04 02:08:39.986241: step 114750, total loss = 0.64, predict loss = 0.15 (63.0 examples/sec; 0.064 sec/batch; 103h:50m:13s remains)
INFO - root - 2019-11-04 02:08:40.608525: step 114760, total loss = 0.61, predict loss = 0.14 (67.8 examples/sec; 0.059 sec/batch; 96h:24m:47s remains)
INFO - root - 2019-11-04 02:08:41.252967: step 114770, total loss = 0.46, predict loss = 0.11 (69.2 examples/sec; 0.058 sec/batch; 94h:32m:57s remains)
INFO - root - 2019-11-04 02:08:41.909594: step 114780, total loss = 0.44, predict loss = 0.10 (66.0 examples/sec; 0.061 sec/batch; 99h:07m:56s remains)
INFO - root - 2019-11-04 02:08:42.570851: step 114790, total loss = 0.57, predict loss = 0.13 (70.8 examples/sec; 0.057 sec/batch; 92h:22m:53s remains)
INFO - root - 2019-11-04 02:08:43.229419: step 114800, total loss = 0.44, predict loss = 0.10 (70.9 examples/sec; 0.056 sec/batch; 92h:16m:15s remains)
INFO - root - 2019-11-04 02:08:43.847032: step 114810, total loss = 0.49, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 93h:41m:37s remains)
INFO - root - 2019-11-04 02:08:44.455773: step 114820, total loss = 0.51, predict loss = 0.11 (79.9 examples/sec; 0.050 sec/batch; 81h:51m:54s remains)
INFO - root - 2019-11-04 02:08:45.079302: step 114830, total loss = 0.49, predict loss = 0.11 (61.3 examples/sec; 0.065 sec/batch; 106h:37m:42s remains)
INFO - root - 2019-11-04 02:08:45.757841: step 114840, total loss = 0.51, predict loss = 0.11 (66.3 examples/sec; 0.060 sec/batch; 98h:37m:22s remains)
INFO - root - 2019-11-04 02:08:46.432606: step 114850, total loss = 0.45, predict loss = 0.11 (77.1 examples/sec; 0.052 sec/batch; 84h:51m:08s remains)
INFO - root - 2019-11-04 02:08:47.094983: step 114860, total loss = 0.43, predict loss = 0.10 (59.4 examples/sec; 0.067 sec/batch; 110h:07m:02s remains)
INFO - root - 2019-11-04 02:08:47.770867: step 114870, total loss = 0.34, predict loss = 0.08 (68.0 examples/sec; 0.059 sec/batch; 96h:11m:10s remains)
INFO - root - 2019-11-04 02:08:48.361867: step 114880, total loss = 0.32, predict loss = 0.07 (70.8 examples/sec; 0.057 sec/batch; 92h:25m:15s remains)
INFO - root - 2019-11-04 02:08:48.965074: step 114890, total loss = 0.48, predict loss = 0.11 (71.7 examples/sec; 0.056 sec/batch; 91h:13m:32s remains)
INFO - root - 2019-11-04 02:08:49.576991: step 114900, total loss = 0.40, predict loss = 0.09 (70.1 examples/sec; 0.057 sec/batch; 93h:20m:43s remains)
INFO - root - 2019-11-04 02:08:50.200165: step 114910, total loss = 0.51, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 91h:04m:07s remains)
INFO - root - 2019-11-04 02:08:50.831312: step 114920, total loss = 0.45, predict loss = 0.10 (66.8 examples/sec; 0.060 sec/batch; 97h:54m:10s remains)
INFO - root - 2019-11-04 02:08:51.474882: step 114930, total loss = 0.38, predict loss = 0.08 (68.6 examples/sec; 0.058 sec/batch; 95h:17m:48s remains)
INFO - root - 2019-11-04 02:08:52.071440: step 114940, total loss = 0.42, predict loss = 0.10 (73.3 examples/sec; 0.055 sec/batch; 89h:14m:43s remains)
INFO - root - 2019-11-04 02:08:52.699790: step 114950, total loss = 0.40, predict loss = 0.09 (70.5 examples/sec; 0.057 sec/batch; 92h:46m:39s remains)
INFO - root - 2019-11-04 02:08:53.291507: step 114960, total loss = 0.39, predict loss = 0.09 (79.7 examples/sec; 0.050 sec/batch; 82h:02m:58s remains)
INFO - root - 2019-11-04 02:08:53.904984: step 114970, total loss = 0.52, predict loss = 0.12 (63.6 examples/sec; 0.063 sec/batch; 102h:49m:31s remains)
INFO - root - 2019-11-04 02:08:54.545362: step 114980, total loss = 0.45, predict loss = 0.11 (74.1 examples/sec; 0.054 sec/batch; 88h:13m:29s remains)
INFO - root - 2019-11-04 02:08:55.211091: step 114990, total loss = 0.40, predict loss = 0.09 (67.4 examples/sec; 0.059 sec/batch; 96h:58m:01s remains)
INFO - root - 2019-11-04 02:08:55.843127: step 115000, total loss = 0.55, predict loss = 0.13 (76.8 examples/sec; 0.052 sec/batch; 85h:11m:22s remains)
INFO - root - 2019-11-04 02:08:56.463059: step 115010, total loss = 0.47, predict loss = 0.11 (71.0 examples/sec; 0.056 sec/batch; 92h:02m:46s remains)
INFO - root - 2019-11-04 02:08:57.072433: step 115020, total loss = 0.60, predict loss = 0.14 (78.0 examples/sec; 0.051 sec/batch; 83h:46m:50s remains)
INFO - root - 2019-11-04 02:08:57.711788: step 115030, total loss = 0.60, predict loss = 0.14 (82.8 examples/sec; 0.048 sec/batch; 78h:55m:50s remains)
INFO - root - 2019-11-04 02:08:58.355674: step 115040, total loss = 0.54, predict loss = 0.12 (64.6 examples/sec; 0.062 sec/batch; 101h:15m:45s remains)
INFO - root - 2019-11-04 02:08:58.971023: step 115050, total loss = 0.59, predict loss = 0.14 (77.8 examples/sec; 0.051 sec/batch; 84h:00m:25s remains)
INFO - root - 2019-11-04 02:08:59.663960: step 115060, total loss = 0.59, predict loss = 0.14 (60.8 examples/sec; 0.066 sec/batch; 107h:31m:48s remains)
INFO - root - 2019-11-04 02:09:00.363356: step 115070, total loss = 0.59, predict loss = 0.14 (63.6 examples/sec; 0.063 sec/batch; 102h:45m:07s remains)
INFO - root - 2019-11-04 02:09:00.973120: step 115080, total loss = 0.67, predict loss = 0.16 (70.1 examples/sec; 0.057 sec/batch; 93h:12m:46s remains)
INFO - root - 2019-11-04 02:09:01.619324: step 115090, total loss = 0.63, predict loss = 0.15 (78.5 examples/sec; 0.051 sec/batch; 83h:17m:50s remains)
INFO - root - 2019-11-04 02:09:02.309290: step 115100, total loss = 0.51, predict loss = 0.12 (61.1 examples/sec; 0.065 sec/batch; 107h:01m:39s remains)
INFO - root - 2019-11-04 02:09:03.014313: step 115110, total loss = 0.60, predict loss = 0.14 (66.6 examples/sec; 0.060 sec/batch; 98h:14m:15s remains)
INFO - root - 2019-11-04 02:09:03.651400: step 115120, total loss = 0.44, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 93h:45m:10s remains)
INFO - root - 2019-11-04 02:09:04.268124: step 115130, total loss = 0.68, predict loss = 0.15 (67.6 examples/sec; 0.059 sec/batch; 96h:43m:13s remains)
INFO - root - 2019-11-04 02:09:04.924442: step 115140, total loss = 0.45, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 99h:15m:59s remains)
INFO - root - 2019-11-04 02:09:05.558840: step 115150, total loss = 0.44, predict loss = 0.10 (72.8 examples/sec; 0.055 sec/batch; 89h:49m:46s remains)
INFO - root - 2019-11-04 02:09:06.230497: step 115160, total loss = 0.45, predict loss = 0.10 (66.1 examples/sec; 0.061 sec/batch; 98h:57m:05s remains)
INFO - root - 2019-11-04 02:09:06.866652: step 115170, total loss = 0.35, predict loss = 0.08 (61.7 examples/sec; 0.065 sec/batch; 105h:56m:07s remains)
INFO - root - 2019-11-04 02:09:07.540743: step 115180, total loss = 0.55, predict loss = 0.13 (75.1 examples/sec; 0.053 sec/batch; 87h:05m:53s remains)
INFO - root - 2019-11-04 02:09:08.228251: step 115190, total loss = 0.47, predict loss = 0.11 (71.8 examples/sec; 0.056 sec/batch; 91h:06m:44s remains)
INFO - root - 2019-11-04 02:09:08.882789: step 115200, total loss = 0.39, predict loss = 0.09 (66.4 examples/sec; 0.060 sec/batch; 98h:24m:41s remains)
INFO - root - 2019-11-04 02:09:09.516709: step 115210, total loss = 0.54, predict loss = 0.12 (68.6 examples/sec; 0.058 sec/batch; 95h:20m:50s remains)
INFO - root - 2019-11-04 02:09:10.175877: step 115220, total loss = 0.44, predict loss = 0.10 (72.9 examples/sec; 0.055 sec/batch; 89h:43m:40s remains)
INFO - root - 2019-11-04 02:09:10.784648: step 115230, total loss = 0.66, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 90h:37m:28s remains)
INFO - root - 2019-11-04 02:09:11.393383: step 115240, total loss = 0.64, predict loss = 0.15 (73.3 examples/sec; 0.055 sec/batch; 89h:13m:38s remains)
INFO - root - 2019-11-04 02:09:11.982066: step 115250, total loss = 0.48, predict loss = 0.11 (77.3 examples/sec; 0.052 sec/batch; 84h:35m:10s remains)
INFO - root - 2019-11-04 02:09:12.552785: step 115260, total loss = 0.48, predict loss = 0.11 (82.6 examples/sec; 0.048 sec/batch; 79h:06m:45s remains)
INFO - root - 2019-11-04 02:09:13.157997: step 115270, total loss = 0.69, predict loss = 0.17 (76.3 examples/sec; 0.052 sec/batch; 85h:40m:57s remains)
INFO - root - 2019-11-04 02:09:13.810505: step 115280, total loss = 0.51, predict loss = 0.12 (65.8 examples/sec; 0.061 sec/batch; 99h:23m:10s remains)
INFO - root - 2019-11-04 02:09:14.469097: step 115290, total loss = 0.62, predict loss = 0.15 (70.0 examples/sec; 0.057 sec/batch; 93h:24m:53s remains)
INFO - root - 2019-11-04 02:09:15.137902: step 115300, total loss = 0.56, predict loss = 0.13 (75.6 examples/sec; 0.053 sec/batch; 86h:27m:09s remains)
INFO - root - 2019-11-04 02:09:15.771564: step 115310, total loss = 0.48, predict loss = 0.11 (73.4 examples/sec; 0.054 sec/batch; 89h:04m:28s remains)
INFO - root - 2019-11-04 02:09:16.421242: step 115320, total loss = 0.58, predict loss = 0.15 (79.4 examples/sec; 0.050 sec/batch; 82h:22m:21s remains)
INFO - root - 2019-11-04 02:09:17.022351: step 115330, total loss = 0.48, predict loss = 0.11 (76.6 examples/sec; 0.052 sec/batch; 85h:18m:28s remains)
INFO - root - 2019-11-04 02:09:17.613405: step 115340, total loss = 0.52, predict loss = 0.13 (60.8 examples/sec; 0.066 sec/batch; 107h:31m:12s remains)
INFO - root - 2019-11-04 02:09:18.262421: step 115350, total loss = 0.51, predict loss = 0.12 (73.8 examples/sec; 0.054 sec/batch; 88h:38m:06s remains)
INFO - root - 2019-11-04 02:09:18.896925: step 115360, total loss = 0.47, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:25m:55s remains)
INFO - root - 2019-11-04 02:09:19.490694: step 115370, total loss = 0.43, predict loss = 0.09 (82.2 examples/sec; 0.049 sec/batch; 79h:31m:14s remains)
INFO - root - 2019-11-04 02:09:20.081885: step 115380, total loss = 0.55, predict loss = 0.13 (75.6 examples/sec; 0.053 sec/batch; 86h:25m:59s remains)
INFO - root - 2019-11-04 02:09:20.686306: step 115390, total loss = 0.50, predict loss = 0.12 (64.7 examples/sec; 0.062 sec/batch; 101h:06m:35s remains)
INFO - root - 2019-11-04 02:09:21.297831: step 115400, total loss = 0.52, predict loss = 0.12 (70.4 examples/sec; 0.057 sec/batch; 92h:51m:58s remains)
INFO - root - 2019-11-04 02:09:21.936752: step 115410, total loss = 0.53, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 91h:44m:46s remains)
INFO - root - 2019-11-04 02:09:22.589722: step 115420, total loss = 0.48, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 97h:43m:38s remains)
INFO - root - 2019-11-04 02:09:23.257016: step 115430, total loss = 0.45, predict loss = 0.11 (63.7 examples/sec; 0.063 sec/batch; 102h:36m:57s remains)
INFO - root - 2019-11-04 02:09:23.902410: step 115440, total loss = 0.39, predict loss = 0.09 (64.2 examples/sec; 0.062 sec/batch; 101h:46m:40s remains)
INFO - root - 2019-11-04 02:09:24.490769: step 115450, total loss = 0.35, predict loss = 0.08 (75.7 examples/sec; 0.053 sec/batch; 86h:22m:36s remains)
INFO - root - 2019-11-04 02:09:25.084186: step 115460, total loss = 0.45, predict loss = 0.11 (79.6 examples/sec; 0.050 sec/batch; 82h:09m:06s remains)
INFO - root - 2019-11-04 02:09:25.705164: step 115470, total loss = 0.40, predict loss = 0.10 (74.6 examples/sec; 0.054 sec/batch; 87h:41m:41s remains)
INFO - root - 2019-11-04 02:09:26.339922: step 115480, total loss = 0.33, predict loss = 0.07 (66.6 examples/sec; 0.060 sec/batch; 98h:06m:31s remains)
INFO - root - 2019-11-04 02:09:26.927652: step 115490, total loss = 0.48, predict loss = 0.11 (70.0 examples/sec; 0.057 sec/batch; 93h:22m:42s remains)
INFO - root - 2019-11-04 02:09:27.535461: step 115500, total loss = 0.47, predict loss = 0.11 (64.6 examples/sec; 0.062 sec/batch; 101h:12m:14s remains)
INFO - root - 2019-11-04 02:09:28.144670: step 115510, total loss = 0.50, predict loss = 0.12 (76.6 examples/sec; 0.052 sec/batch; 85h:24m:01s remains)
INFO - root - 2019-11-04 02:09:28.753800: step 115520, total loss = 0.49, predict loss = 0.11 (74.4 examples/sec; 0.054 sec/batch; 87h:52m:48s remains)
INFO - root - 2019-11-04 02:09:29.355459: step 115530, total loss = 0.46, predict loss = 0.10 (70.9 examples/sec; 0.056 sec/batch; 92h:13m:48s remains)
INFO - root - 2019-11-04 02:09:29.986713: step 115540, total loss = 0.54, predict loss = 0.13 (72.3 examples/sec; 0.055 sec/batch; 90h:23m:26s remains)
INFO - root - 2019-11-04 02:09:30.635201: step 115550, total loss = 0.52, predict loss = 0.12 (69.5 examples/sec; 0.058 sec/batch; 94h:03m:03s remains)
INFO - root - 2019-11-04 02:09:31.306613: step 115560, total loss = 0.40, predict loss = 0.09 (67.5 examples/sec; 0.059 sec/batch; 96h:50m:00s remains)
INFO - root - 2019-11-04 02:09:31.985371: step 115570, total loss = 0.45, predict loss = 0.10 (75.8 examples/sec; 0.053 sec/batch; 86h:18m:28s remains)
INFO - root - 2019-11-04 02:09:32.645621: step 115580, total loss = 0.42, predict loss = 0.09 (73.5 examples/sec; 0.054 sec/batch; 88h:54m:56s remains)
INFO - root - 2019-11-04 02:09:33.366223: step 115590, total loss = 0.51, predict loss = 0.12 (65.5 examples/sec; 0.061 sec/batch; 99h:50m:09s remains)
INFO - root - 2019-11-04 02:09:33.934623: step 115600, total loss = 0.41, predict loss = 0.10 (70.2 examples/sec; 0.057 sec/batch; 93h:06m:44s remains)
INFO - root - 2019-11-04 02:09:34.553907: step 115610, total loss = 0.59, predict loss = 0.14 (75.3 examples/sec; 0.053 sec/batch; 86h:46m:24s remains)
INFO - root - 2019-11-04 02:09:35.186578: step 115620, total loss = 0.63, predict loss = 0.15 (71.1 examples/sec; 0.056 sec/batch; 91h:54m:27s remains)
INFO - root - 2019-11-04 02:09:35.776503: step 115630, total loss = 0.68, predict loss = 0.16 (76.1 examples/sec; 0.053 sec/batch; 85h:57m:53s remains)
INFO - root - 2019-11-04 02:09:36.367103: step 115640, total loss = 0.62, predict loss = 0.15 (73.9 examples/sec; 0.054 sec/batch; 88h:29m:28s remains)
INFO - root - 2019-11-04 02:09:36.995513: step 115650, total loss = 0.48, predict loss = 0.11 (73.3 examples/sec; 0.055 sec/batch; 89h:08m:28s remains)
INFO - root - 2019-11-04 02:09:37.614637: step 115660, total loss = 0.69, predict loss = 0.16 (69.7 examples/sec; 0.057 sec/batch; 93h:48m:09s remains)
INFO - root - 2019-11-04 02:09:38.247027: step 115670, total loss = 0.68, predict loss = 0.17 (72.0 examples/sec; 0.056 sec/batch; 90h:44m:48s remains)
INFO - root - 2019-11-04 02:09:38.868786: step 115680, total loss = 0.64, predict loss = 0.15 (62.5 examples/sec; 0.064 sec/batch; 104h:39m:12s remains)
INFO - root - 2019-11-04 02:09:39.496311: step 115690, total loss = 0.79, predict loss = 0.20 (73.5 examples/sec; 0.054 sec/batch; 88h:58m:56s remains)
INFO - root - 2019-11-04 02:09:40.088390: step 115700, total loss = 0.61, predict loss = 0.15 (74.8 examples/sec; 0.053 sec/batch; 87h:25m:03s remains)
INFO - root - 2019-11-04 02:09:40.701073: step 115710, total loss = 0.55, predict loss = 0.13 (74.7 examples/sec; 0.054 sec/batch; 87h:34m:54s remains)
INFO - root - 2019-11-04 02:09:41.313222: step 115720, total loss = 0.75, predict loss = 0.17 (70.6 examples/sec; 0.057 sec/batch; 92h:34m:05s remains)
INFO - root - 2019-11-04 02:09:41.975237: step 115730, total loss = 0.49, predict loss = 0.11 (77.1 examples/sec; 0.052 sec/batch; 84h:49m:27s remains)
INFO - root - 2019-11-04 02:09:42.639431: step 115740, total loss = 0.39, predict loss = 0.08 (60.8 examples/sec; 0.066 sec/batch; 107h:33m:36s remains)
INFO - root - 2019-11-04 02:09:43.312192: step 115750, total loss = 0.65, predict loss = 0.16 (65.1 examples/sec; 0.061 sec/batch; 100h:24m:41s remains)
INFO - root - 2019-11-04 02:09:43.943657: step 115760, total loss = 0.60, predict loss = 0.14 (67.8 examples/sec; 0.059 sec/batch; 96h:29m:32s remains)
INFO - root - 2019-11-04 02:09:44.567826: step 115770, total loss = 0.45, predict loss = 0.10 (69.6 examples/sec; 0.057 sec/batch; 93h:58m:05s remains)
INFO - root - 2019-11-04 02:09:45.234092: step 115780, total loss = 0.61, predict loss = 0.14 (65.2 examples/sec; 0.061 sec/batch; 100h:12m:12s remains)
INFO - root - 2019-11-04 02:09:45.846369: step 115790, total loss = 0.54, predict loss = 0.13 (67.4 examples/sec; 0.059 sec/batch; 96h:58m:34s remains)
INFO - root - 2019-11-04 02:09:46.490403: step 115800, total loss = 0.54, predict loss = 0.13 (78.7 examples/sec; 0.051 sec/batch; 83h:04m:04s remains)
INFO - root - 2019-11-04 02:09:47.081965: step 115810, total loss = 0.47, predict loss = 0.11 (84.3 examples/sec; 0.047 sec/batch; 77h:33m:35s remains)
INFO - root - 2019-11-04 02:09:47.667506: step 115820, total loss = 0.56, predict loss = 0.13 (77.0 examples/sec; 0.052 sec/batch; 84h:56m:15s remains)
INFO - root - 2019-11-04 02:09:48.243349: step 115830, total loss = 0.51, predict loss = 0.12 (68.4 examples/sec; 0.059 sec/batch; 95h:38m:14s remains)
INFO - root - 2019-11-04 02:09:48.829861: step 115840, total loss = 0.55, predict loss = 0.13 (84.1 examples/sec; 0.048 sec/batch; 77h:46m:33s remains)
INFO - root - 2019-11-04 02:09:49.425095: step 115850, total loss = 0.59, predict loss = 0.13 (69.9 examples/sec; 0.057 sec/batch; 93h:28m:32s remains)
INFO - root - 2019-11-04 02:09:50.037698: step 115860, total loss = 0.51, predict loss = 0.12 (73.3 examples/sec; 0.055 sec/batch; 89h:09m:46s remains)
INFO - root - 2019-11-04 02:09:50.654142: step 115870, total loss = 0.62, predict loss = 0.14 (73.1 examples/sec; 0.055 sec/batch; 89h:24m:14s remains)
INFO - root - 2019-11-04 02:09:51.260074: step 115880, total loss = 0.49, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 91h:54m:18s remains)
INFO - root - 2019-11-04 02:09:51.831577: step 115890, total loss = 0.46, predict loss = 0.10 (88.0 examples/sec; 0.045 sec/batch; 74h:17m:09s remains)
INFO - root - 2019-11-04 02:09:52.292627: step 115900, total loss = 0.61, predict loss = 0.15 (96.7 examples/sec; 0.041 sec/batch; 67h:34m:47s remains)
INFO - root - 2019-11-04 02:09:52.742413: step 115910, total loss = 0.53, predict loss = 0.12 (100.0 examples/sec; 0.040 sec/batch; 65h:20m:53s remains)
INFO - root - 2019-11-04 02:09:53.852311: step 115920, total loss = 0.42, predict loss = 0.10 (73.5 examples/sec; 0.054 sec/batch; 88h:59m:24s remains)
INFO - root - 2019-11-04 02:09:54.506665: step 115930, total loss = 0.42, predict loss = 0.09 (60.6 examples/sec; 0.066 sec/batch; 107h:55m:08s remains)
INFO - root - 2019-11-04 02:09:55.132868: step 115940, total loss = 0.42, predict loss = 0.10 (67.9 examples/sec; 0.059 sec/batch; 96h:14m:01s remains)
INFO - root - 2019-11-04 02:09:55.771794: step 115950, total loss = 0.51, predict loss = 0.13 (62.2 examples/sec; 0.064 sec/batch; 105h:10m:21s remains)
INFO - root - 2019-11-04 02:09:56.436931: step 115960, total loss = 0.33, predict loss = 0.07 (72.5 examples/sec; 0.055 sec/batch; 90h:09m:12s remains)
INFO - root - 2019-11-04 02:09:57.116494: step 115970, total loss = 0.51, predict loss = 0.11 (70.4 examples/sec; 0.057 sec/batch; 92h:49m:32s remains)
INFO - root - 2019-11-04 02:09:57.769195: step 115980, total loss = 0.52, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 90h:03m:46s remains)
INFO - root - 2019-11-04 02:09:58.394698: step 115990, total loss = 0.69, predict loss = 0.16 (62.7 examples/sec; 0.064 sec/batch; 104h:19m:11s remains)
INFO - root - 2019-11-04 02:09:59.012773: step 116000, total loss = 0.73, predict loss = 0.17 (67.7 examples/sec; 0.059 sec/batch; 96h:32m:52s remains)
INFO - root - 2019-11-04 02:09:59.633075: step 116010, total loss = 0.93, predict loss = 0.21 (69.7 examples/sec; 0.057 sec/batch; 93h:49m:54s remains)
INFO - root - 2019-11-04 02:10:00.256013: step 116020, total loss = 0.38, predict loss = 0.08 (78.8 examples/sec; 0.051 sec/batch; 82h:55m:41s remains)
INFO - root - 2019-11-04 02:10:00.921450: step 116030, total loss = 0.56, predict loss = 0.13 (66.7 examples/sec; 0.060 sec/batch; 98h:01m:26s remains)
INFO - root - 2019-11-04 02:10:01.596604: step 116040, total loss = 0.53, predict loss = 0.13 (71.6 examples/sec; 0.056 sec/batch; 91h:18m:57s remains)
INFO - root - 2019-11-04 02:10:02.224014: step 116050, total loss = 0.63, predict loss = 0.15 (65.6 examples/sec; 0.061 sec/batch; 99h:36m:54s remains)
INFO - root - 2019-11-04 02:10:02.891428: step 116060, total loss = 0.40, predict loss = 0.08 (73.9 examples/sec; 0.054 sec/batch; 88h:27m:59s remains)
INFO - root - 2019-11-04 02:10:03.524340: step 116070, total loss = 0.47, predict loss = 0.11 (70.8 examples/sec; 0.056 sec/batch; 92h:16m:54s remains)
INFO - root - 2019-11-04 02:10:04.144389: step 116080, total loss = 0.45, predict loss = 0.09 (79.1 examples/sec; 0.051 sec/batch; 82h:38m:20s remains)
INFO - root - 2019-11-04 02:10:04.798611: step 116090, total loss = 0.43, predict loss = 0.10 (72.5 examples/sec; 0.055 sec/batch; 90h:11m:11s remains)
INFO - root - 2019-11-04 02:10:05.403275: step 116100, total loss = 0.31, predict loss = 0.06 (75.1 examples/sec; 0.053 sec/batch; 87h:04m:00s remains)
INFO - root - 2019-11-04 02:10:06.016784: step 116110, total loss = 0.43, predict loss = 0.09 (77.1 examples/sec; 0.052 sec/batch; 84h:49m:38s remains)
INFO - root - 2019-11-04 02:10:06.613395: step 116120, total loss = 0.40, predict loss = 0.09 (80.9 examples/sec; 0.049 sec/batch; 80h:49m:55s remains)
INFO - root - 2019-11-04 02:10:07.225172: step 116130, total loss = 0.51, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 95h:12m:58s remains)
INFO - root - 2019-11-04 02:10:07.870285: step 116140, total loss = 0.46, predict loss = 0.10 (66.7 examples/sec; 0.060 sec/batch; 98h:02m:34s remains)
INFO - root - 2019-11-04 02:10:08.478292: step 116150, total loss = 0.58, predict loss = 0.14 (77.5 examples/sec; 0.052 sec/batch; 84h:21m:01s remains)
INFO - root - 2019-11-04 02:10:09.104019: step 116160, total loss = 0.53, predict loss = 0.12 (71.6 examples/sec; 0.056 sec/batch; 91h:17m:17s remains)
INFO - root - 2019-11-04 02:10:09.715856: step 116170, total loss = 0.50, predict loss = 0.11 (79.7 examples/sec; 0.050 sec/batch; 82h:01m:53s remains)
INFO - root - 2019-11-04 02:10:10.321022: step 116180, total loss = 0.63, predict loss = 0.15 (70.0 examples/sec; 0.057 sec/batch; 93h:23m:10s remains)
INFO - root - 2019-11-04 02:10:10.925556: step 116190, total loss = 0.46, predict loss = 0.10 (65.8 examples/sec; 0.061 sec/batch; 99h:20m:54s remains)
INFO - root - 2019-11-04 02:10:11.514208: step 116200, total loss = 0.66, predict loss = 0.16 (81.4 examples/sec; 0.049 sec/batch; 80h:21m:31s remains)
INFO - root - 2019-11-04 02:10:12.105839: step 116210, total loss = 0.44, predict loss = 0.10 (73.4 examples/sec; 0.054 sec/batch; 89h:01m:21s remains)
INFO - root - 2019-11-04 02:10:12.770684: step 116220, total loss = 0.59, predict loss = 0.14 (69.0 examples/sec; 0.058 sec/batch; 94h:40m:44s remains)
INFO - root - 2019-11-04 02:10:13.395372: step 116230, total loss = 0.53, predict loss = 0.12 (75.8 examples/sec; 0.053 sec/batch; 86h:17m:13s remains)
INFO - root - 2019-11-04 02:10:14.037497: step 116240, total loss = 0.58, predict loss = 0.14 (76.2 examples/sec; 0.053 sec/batch; 85h:49m:12s remains)
INFO - root - 2019-11-04 02:10:14.702465: step 116250, total loss = 0.50, predict loss = 0.11 (78.2 examples/sec; 0.051 sec/batch; 83h:38m:46s remains)
INFO - root - 2019-11-04 02:10:15.339616: step 116260, total loss = 0.59, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 92h:57m:48s remains)
INFO - root - 2019-11-04 02:10:15.991356: step 116270, total loss = 0.58, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 92h:32m:58s remains)
INFO - root - 2019-11-04 02:10:16.638923: step 116280, total loss = 0.51, predict loss = 0.12 (70.1 examples/sec; 0.057 sec/batch; 93h:13m:53s remains)
INFO - root - 2019-11-04 02:10:17.225856: step 116290, total loss = 0.61, predict loss = 0.15 (74.5 examples/sec; 0.054 sec/batch; 87h:42m:23s remains)
INFO - root - 2019-11-04 02:10:17.831668: step 116300, total loss = 0.47, predict loss = 0.11 (76.1 examples/sec; 0.053 sec/batch; 85h:56m:30s remains)
INFO - root - 2019-11-04 02:10:18.448635: step 116310, total loss = 0.43, predict loss = 0.10 (77.7 examples/sec; 0.051 sec/batch; 84h:07m:32s remains)
INFO - root - 2019-11-04 02:10:19.049785: step 116320, total loss = 0.61, predict loss = 0.15 (72.3 examples/sec; 0.055 sec/batch; 90h:22m:58s remains)
INFO - root - 2019-11-04 02:10:19.684136: step 116330, total loss = 0.48, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 97h:33m:58s remains)
INFO - root - 2019-11-04 02:10:20.305504: step 116340, total loss = 0.52, predict loss = 0.12 (78.1 examples/sec; 0.051 sec/batch; 83h:41m:29s remains)
INFO - root - 2019-11-04 02:10:20.939693: step 116350, total loss = 0.51, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 93h:07m:53s remains)
INFO - root - 2019-11-04 02:10:21.539245: step 116360, total loss = 0.49, predict loss = 0.11 (75.1 examples/sec; 0.053 sec/batch; 87h:04m:48s remains)
INFO - root - 2019-11-04 02:10:22.143788: step 116370, total loss = 0.39, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 92h:10m:11s remains)
INFO - root - 2019-11-04 02:10:22.754762: step 116380, total loss = 0.30, predict loss = 0.07 (71.2 examples/sec; 0.056 sec/batch; 91h:51m:41s remains)
INFO - root - 2019-11-04 02:10:23.353485: step 116390, total loss = 0.45, predict loss = 0.10 (69.4 examples/sec; 0.058 sec/batch; 94h:07m:55s remains)
INFO - root - 2019-11-04 02:10:23.979282: step 116400, total loss = 0.38, predict loss = 0.09 (71.1 examples/sec; 0.056 sec/batch; 91h:56m:01s remains)
INFO - root - 2019-11-04 02:10:24.596454: step 116410, total loss = 0.42, predict loss = 0.10 (78.2 examples/sec; 0.051 sec/batch; 83h:33m:51s remains)
INFO - root - 2019-11-04 02:10:25.218817: step 116420, total loss = 0.57, predict loss = 0.14 (65.3 examples/sec; 0.061 sec/batch; 100h:04m:39s remains)
INFO - root - 2019-11-04 02:10:25.818305: step 116430, total loss = 0.33, predict loss = 0.07 (78.5 examples/sec; 0.051 sec/batch; 83h:15m:59s remains)
INFO - root - 2019-11-04 02:10:26.442760: step 116440, total loss = 0.67, predict loss = 0.16 (68.1 examples/sec; 0.059 sec/batch; 95h:55m:34s remains)
INFO - root - 2019-11-04 02:10:27.068059: step 116450, total loss = 0.46, predict loss = 0.11 (81.8 examples/sec; 0.049 sec/batch; 79h:52m:55s remains)
INFO - root - 2019-11-04 02:10:27.680944: step 116460, total loss = 0.46, predict loss = 0.11 (75.7 examples/sec; 0.053 sec/batch; 86h:22m:30s remains)
INFO - root - 2019-11-04 02:10:28.300436: step 116470, total loss = 0.37, predict loss = 0.09 (71.5 examples/sec; 0.056 sec/batch; 91h:25m:56s remains)
INFO - root - 2019-11-04 02:10:28.902370: step 116480, total loss = 0.40, predict loss = 0.09 (74.5 examples/sec; 0.054 sec/batch; 87h:45m:32s remains)
INFO - root - 2019-11-04 02:10:29.494829: step 116490, total loss = 0.42, predict loss = 0.11 (64.2 examples/sec; 0.062 sec/batch; 101h:47m:59s remains)
INFO - root - 2019-11-04 02:10:30.155775: step 116500, total loss = 0.29, predict loss = 0.06 (70.5 examples/sec; 0.057 sec/batch; 92h:45m:04s remains)
INFO - root - 2019-11-04 02:10:30.825175: step 116510, total loss = 0.26, predict loss = 0.06 (64.0 examples/sec; 0.062 sec/batch; 102h:04m:50s remains)
INFO - root - 2019-11-04 02:10:31.440083: step 116520, total loss = 0.37, predict loss = 0.09 (73.0 examples/sec; 0.055 sec/batch; 89h:32m:51s remains)
INFO - root - 2019-11-04 02:10:32.071452: step 116530, total loss = 0.33, predict loss = 0.07 (70.5 examples/sec; 0.057 sec/batch; 92h:41m:56s remains)
INFO - root - 2019-11-04 02:10:32.728485: step 116540, total loss = 0.20, predict loss = 0.04 (76.6 examples/sec; 0.052 sec/batch; 85h:18m:30s remains)
INFO - root - 2019-11-04 02:10:33.373276: step 116550, total loss = 0.31, predict loss = 0.07 (72.8 examples/sec; 0.055 sec/batch; 89h:50m:37s remains)
INFO - root - 2019-11-04 02:10:33.984701: step 116560, total loss = 0.49, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 89h:29m:51s remains)
INFO - root - 2019-11-04 02:10:34.570558: step 116570, total loss = 0.69, predict loss = 0.16 (80.9 examples/sec; 0.049 sec/batch; 80h:48m:20s remains)
INFO - root - 2019-11-04 02:10:35.178779: step 116580, total loss = 0.40, predict loss = 0.09 (75.8 examples/sec; 0.053 sec/batch; 86h:17m:49s remains)
INFO - root - 2019-11-04 02:10:35.780673: step 116590, total loss = 0.44, predict loss = 0.10 (69.9 examples/sec; 0.057 sec/batch; 93h:30m:54s remains)
INFO - root - 2019-11-04 02:10:36.392926: step 116600, total loss = 0.47, predict loss = 0.11 (68.2 examples/sec; 0.059 sec/batch; 95h:53m:27s remains)
INFO - root - 2019-11-04 02:10:37.016423: step 116610, total loss = 0.54, predict loss = 0.13 (74.7 examples/sec; 0.054 sec/batch; 87h:34m:04s remains)
INFO - root - 2019-11-04 02:10:37.677902: step 116620, total loss = 0.48, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 93h:35m:23s remains)
INFO - root - 2019-11-04 02:10:38.296035: step 116630, total loss = 0.45, predict loss = 0.11 (68.9 examples/sec; 0.058 sec/batch; 94h:55m:32s remains)
INFO - root - 2019-11-04 02:10:38.926059: step 116640, total loss = 0.46, predict loss = 0.11 (63.3 examples/sec; 0.063 sec/batch; 103h:12m:53s remains)
INFO - root - 2019-11-04 02:10:39.625583: step 116650, total loss = 0.50, predict loss = 0.12 (63.7 examples/sec; 0.063 sec/batch; 102h:41m:51s remains)
INFO - root - 2019-11-04 02:10:40.308417: step 116660, total loss = 0.48, predict loss = 0.11 (73.2 examples/sec; 0.055 sec/batch; 89h:19m:10s remains)
INFO - root - 2019-11-04 02:10:40.953548: step 116670, total loss = 0.60, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 92h:56m:22s remains)
INFO - root - 2019-11-04 02:10:41.556821: step 116680, total loss = 0.36, predict loss = 0.08 (72.1 examples/sec; 0.055 sec/batch; 90h:41m:06s remains)
INFO - root - 2019-11-04 02:10:42.203430: step 116690, total loss = 0.50, predict loss = 0.12 (71.9 examples/sec; 0.056 sec/batch; 90h:54m:41s remains)
INFO - root - 2019-11-04 02:10:42.814827: step 116700, total loss = 0.48, predict loss = 0.12 (77.3 examples/sec; 0.052 sec/batch; 84h:32m:59s remains)
INFO - root - 2019-11-04 02:10:43.446489: step 116710, total loss = 0.59, predict loss = 0.16 (80.2 examples/sec; 0.050 sec/batch; 81h:32m:11s remains)
INFO - root - 2019-11-04 02:10:44.136030: step 116720, total loss = 0.50, predict loss = 0.11 (60.3 examples/sec; 0.066 sec/batch; 108h:29m:37s remains)
INFO - root - 2019-11-04 02:10:44.769879: step 116730, total loss = 0.30, predict loss = 0.07 (83.6 examples/sec; 0.048 sec/batch; 78h:14m:00s remains)
INFO - root - 2019-11-04 02:10:45.402216: step 116740, total loss = 0.36, predict loss = 0.08 (79.6 examples/sec; 0.050 sec/batch; 82h:05m:27s remains)
INFO - root - 2019-11-04 02:10:45.989818: step 116750, total loss = 0.23, predict loss = 0.05 (70.3 examples/sec; 0.057 sec/batch; 93h:02m:41s remains)
INFO - root - 2019-11-04 02:10:46.608433: step 116760, total loss = 0.27, predict loss = 0.06 (66.8 examples/sec; 0.060 sec/batch; 97h:54m:28s remains)
INFO - root - 2019-11-04 02:10:47.231310: step 116770, total loss = 0.23, predict loss = 0.05 (68.1 examples/sec; 0.059 sec/batch; 95h:55m:32s remains)
INFO - root - 2019-11-04 02:10:47.876399: step 116780, total loss = 0.38, predict loss = 0.09 (66.3 examples/sec; 0.060 sec/batch; 98h:35m:22s remains)
INFO - root - 2019-11-04 02:10:48.507771: step 116790, total loss = 0.26, predict loss = 0.06 (73.5 examples/sec; 0.054 sec/batch; 88h:56m:26s remains)
INFO - root - 2019-11-04 02:10:49.125525: step 116800, total loss = 0.22, predict loss = 0.04 (74.2 examples/sec; 0.054 sec/batch; 88h:08m:46s remains)
INFO - root - 2019-11-04 02:10:49.755118: step 116810, total loss = 0.46, predict loss = 0.10 (68.4 examples/sec; 0.058 sec/batch; 95h:32m:29s remains)
INFO - root - 2019-11-04 02:10:50.379941: step 116820, total loss = 0.33, predict loss = 0.07 (84.6 examples/sec; 0.047 sec/batch; 77h:16m:06s remains)
INFO - root - 2019-11-04 02:10:51.051608: step 116830, total loss = 0.29, predict loss = 0.06 (72.9 examples/sec; 0.055 sec/batch; 89h:41m:13s remains)
INFO - root - 2019-11-04 02:10:51.688984: step 116840, total loss = 0.38, predict loss = 0.09 (81.7 examples/sec; 0.049 sec/batch; 80h:00m:19s remains)
INFO - root - 2019-11-04 02:10:52.304976: step 116850, total loss = 0.41, predict loss = 0.09 (70.5 examples/sec; 0.057 sec/batch; 92h:40m:12s remains)
INFO - root - 2019-11-04 02:10:52.909071: step 116860, total loss = 0.33, predict loss = 0.07 (78.6 examples/sec; 0.051 sec/batch; 83h:10m:22s remains)
INFO - root - 2019-11-04 02:10:53.573851: step 116870, total loss = 0.43, predict loss = 0.10 (59.8 examples/sec; 0.067 sec/batch; 109h:17m:42s remains)
INFO - root - 2019-11-04 02:10:54.250834: step 116880, total loss = 0.32, predict loss = 0.07 (61.2 examples/sec; 0.065 sec/batch; 106h:45m:05s remains)
INFO - root - 2019-11-04 02:10:54.900138: step 116890, total loss = 0.38, predict loss = 0.09 (79.5 examples/sec; 0.050 sec/batch; 82h:13m:33s remains)
INFO - root - 2019-11-04 02:10:55.565215: step 116900, total loss = 0.30, predict loss = 0.07 (58.3 examples/sec; 0.069 sec/batch; 112h:10m:29s remains)
INFO - root - 2019-11-04 02:10:56.223305: step 116910, total loss = 0.40, predict loss = 0.08 (61.5 examples/sec; 0.065 sec/batch; 106h:18m:07s remains)
INFO - root - 2019-11-04 02:10:56.846570: step 116920, total loss = 0.41, predict loss = 0.10 (67.8 examples/sec; 0.059 sec/batch; 96h:20m:52s remains)
INFO - root - 2019-11-04 02:10:57.509762: step 116930, total loss = 0.42, predict loss = 0.10 (76.0 examples/sec; 0.053 sec/batch; 85h:59m:26s remains)
INFO - root - 2019-11-04 02:10:58.179856: step 116940, total loss = 0.53, predict loss = 0.13 (66.5 examples/sec; 0.060 sec/batch; 98h:18m:19s remains)
INFO - root - 2019-11-04 02:10:58.797435: step 116950, total loss = 0.33, predict loss = 0.08 (79.4 examples/sec; 0.050 sec/batch; 82h:16m:40s remains)
INFO - root - 2019-11-04 02:10:59.414734: step 116960, total loss = 0.45, predict loss = 0.11 (72.9 examples/sec; 0.055 sec/batch; 89h:43m:29s remains)
INFO - root - 2019-11-04 02:11:00.056849: step 116970, total loss = 0.46, predict loss = 0.11 (65.8 examples/sec; 0.061 sec/batch; 99h:23m:30s remains)
INFO - root - 2019-11-04 02:11:00.743457: step 116980, total loss = 0.27, predict loss = 0.06 (64.0 examples/sec; 0.063 sec/batch; 102h:10m:51s remains)
INFO - root - 2019-11-04 02:11:01.414239: step 116990, total loss = 0.27, predict loss = 0.06 (59.9 examples/sec; 0.067 sec/batch; 109h:10m:47s remains)
INFO - root - 2019-11-04 02:11:02.031223: step 117000, total loss = 0.31, predict loss = 0.07 (70.8 examples/sec; 0.056 sec/batch; 92h:17m:16s remains)
INFO - root - 2019-11-04 02:11:02.677153: step 117010, total loss = 0.26, predict loss = 0.06 (67.2 examples/sec; 0.060 sec/batch; 97h:14m:07s remains)
INFO - root - 2019-11-04 02:11:03.323346: step 117020, total loss = 0.32, predict loss = 0.07 (73.3 examples/sec; 0.055 sec/batch; 89h:12m:43s remains)
INFO - root - 2019-11-04 02:11:03.973046: step 117030, total loss = 0.38, predict loss = 0.08 (66.1 examples/sec; 0.061 sec/batch; 98h:53m:14s remains)
INFO - root - 2019-11-04 02:11:04.593322: step 117040, total loss = 0.44, predict loss = 0.10 (69.3 examples/sec; 0.058 sec/batch; 94h:22m:04s remains)
INFO - root - 2019-11-04 02:11:05.192527: step 117050, total loss = 0.39, predict loss = 0.09 (75.5 examples/sec; 0.053 sec/batch; 86h:32m:19s remains)
INFO - root - 2019-11-04 02:11:05.795764: step 117060, total loss = 0.52, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 91h:38m:33s remains)
INFO - root - 2019-11-04 02:11:06.434161: step 117070, total loss = 0.51, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 97h:03m:38s remains)
INFO - root - 2019-11-04 02:11:07.069974: step 117080, total loss = 0.39, predict loss = 0.08 (74.6 examples/sec; 0.054 sec/batch; 87h:39m:30s remains)
INFO - root - 2019-11-04 02:11:07.722027: step 117090, total loss = 0.63, predict loss = 0.14 (65.3 examples/sec; 0.061 sec/batch; 100h:02m:57s remains)
INFO - root - 2019-11-04 02:11:08.356923: step 117100, total loss = 0.31, predict loss = 0.07 (63.6 examples/sec; 0.063 sec/batch; 102h:47m:58s remains)
INFO - root - 2019-11-04 02:11:09.001355: step 117110, total loss = 0.36, predict loss = 0.08 (67.5 examples/sec; 0.059 sec/batch; 96h:52m:25s remains)
INFO - root - 2019-11-04 02:11:09.654532: step 117120, total loss = 0.36, predict loss = 0.09 (65.0 examples/sec; 0.061 sec/batch; 100h:29m:14s remains)
INFO - root - 2019-11-04 02:11:10.398949: step 117130, total loss = 0.43, predict loss = 0.10 (69.4 examples/sec; 0.058 sec/batch; 94h:09m:24s remains)
INFO - root - 2019-11-04 02:11:11.043080: step 117140, total loss = 0.50, predict loss = 0.12 (80.4 examples/sec; 0.050 sec/batch; 81h:15m:32s remains)
INFO - root - 2019-11-04 02:11:11.666252: step 117150, total loss = 0.25, predict loss = 0.05 (68.3 examples/sec; 0.059 sec/batch; 95h:41m:01s remains)
INFO - root - 2019-11-04 02:11:12.263417: step 117160, total loss = 0.46, predict loss = 0.11 (76.1 examples/sec; 0.053 sec/batch; 85h:52m:29s remains)
INFO - root - 2019-11-04 02:11:12.845965: step 117170, total loss = 0.46, predict loss = 0.10 (78.3 examples/sec; 0.051 sec/batch; 83h:30m:37s remains)
INFO - root - 2019-11-04 02:11:13.449447: step 117180, total loss = 0.52, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 91h:33m:34s remains)
INFO - root - 2019-11-04 02:11:14.076208: step 117190, total loss = 0.45, predict loss = 0.11 (71.7 examples/sec; 0.056 sec/batch; 91h:11m:46s remains)
INFO - root - 2019-11-04 02:11:14.694874: step 117200, total loss = 0.51, predict loss = 0.12 (71.5 examples/sec; 0.056 sec/batch; 91h:27m:09s remains)
INFO - root - 2019-11-04 02:11:15.351939: step 117210, total loss = 0.45, predict loss = 0.09 (67.7 examples/sec; 0.059 sec/batch; 96h:34m:59s remains)
INFO - root - 2019-11-04 02:11:15.981527: step 117220, total loss = 0.52, predict loss = 0.13 (80.5 examples/sec; 0.050 sec/batch; 81h:13m:19s remains)
INFO - root - 2019-11-04 02:11:16.616187: step 117230, total loss = 0.66, predict loss = 0.16 (68.8 examples/sec; 0.058 sec/batch; 95h:03m:18s remains)
INFO - root - 2019-11-04 02:11:17.230398: step 117240, total loss = 0.36, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 83h:28m:09s remains)
INFO - root - 2019-11-04 02:11:17.834247: step 117250, total loss = 0.51, predict loss = 0.11 (81.8 examples/sec; 0.049 sec/batch; 79h:52m:13s remains)
INFO - root - 2019-11-04 02:11:18.435360: step 117260, total loss = 0.52, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 94h:35m:44s remains)
INFO - root - 2019-11-04 02:11:19.077158: step 117270, total loss = 0.55, predict loss = 0.12 (71.1 examples/sec; 0.056 sec/batch; 91h:53m:16s remains)
INFO - root - 2019-11-04 02:11:19.717220: step 117280, total loss = 0.43, predict loss = 0.10 (71.5 examples/sec; 0.056 sec/batch; 91h:26m:49s remains)
INFO - root - 2019-11-04 02:11:20.341182: step 117290, total loss = 0.56, predict loss = 0.13 (73.7 examples/sec; 0.054 sec/batch; 88h:44m:23s remains)
INFO - root - 2019-11-04 02:11:20.990683: step 117300, total loss = 0.49, predict loss = 0.11 (72.9 examples/sec; 0.055 sec/batch; 89h:42m:28s remains)
INFO - root - 2019-11-04 02:11:21.652640: step 117310, total loss = 0.50, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 94h:44m:56s remains)
INFO - root - 2019-11-04 02:11:22.321228: step 117320, total loss = 0.61, predict loss = 0.14 (70.7 examples/sec; 0.057 sec/batch; 92h:23m:23s remains)
INFO - root - 2019-11-04 02:11:22.936123: step 117330, total loss = 0.49, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 89h:31m:08s remains)
INFO - root - 2019-11-04 02:11:23.560661: step 117340, total loss = 0.51, predict loss = 0.11 (65.3 examples/sec; 0.061 sec/batch; 100h:02m:41s remains)
INFO - root - 2019-11-04 02:11:24.184167: step 117350, total loss = 0.71, predict loss = 0.18 (66.6 examples/sec; 0.060 sec/batch; 98h:05m:33s remains)
INFO - root - 2019-11-04 02:11:24.821562: step 117360, total loss = 0.67, predict loss = 0.15 (76.7 examples/sec; 0.052 sec/batch; 85h:11m:38s remains)
INFO - root - 2019-11-04 02:11:25.494430: step 117370, total loss = 0.76, predict loss = 0.18 (64.8 examples/sec; 0.062 sec/batch; 100h:50m:55s remains)
INFO - root - 2019-11-04 02:11:26.184131: step 117380, total loss = 0.50, predict loss = 0.12 (60.8 examples/sec; 0.066 sec/batch; 107h:30m:41s remains)
INFO - root - 2019-11-04 02:11:26.846896: step 117390, total loss = 0.64, predict loss = 0.15 (68.0 examples/sec; 0.059 sec/batch; 96h:04m:57s remains)
INFO - root - 2019-11-04 02:11:27.518808: step 117400, total loss = 0.70, predict loss = 0.17 (64.4 examples/sec; 0.062 sec/batch; 101h:30m:38s remains)
INFO - root - 2019-11-04 02:11:28.131914: step 117410, total loss = 0.56, predict loss = 0.13 (70.8 examples/sec; 0.056 sec/batch; 92h:17m:29s remains)
INFO - root - 2019-11-04 02:11:28.772605: step 117420, total loss = 0.47, predict loss = 0.11 (63.7 examples/sec; 0.063 sec/batch; 102h:40m:37s remains)
INFO - root - 2019-11-04 02:11:29.395247: step 117430, total loss = 0.54, predict loss = 0.11 (76.1 examples/sec; 0.053 sec/batch; 85h:51m:10s remains)
INFO - root - 2019-11-04 02:11:30.024692: step 117440, total loss = 0.58, predict loss = 0.14 (62.1 examples/sec; 0.064 sec/batch; 105h:12m:32s remains)
INFO - root - 2019-11-04 02:11:30.684601: step 117450, total loss = 0.48, predict loss = 0.10 (66.2 examples/sec; 0.060 sec/batch; 98h:44m:54s remains)
INFO - root - 2019-11-04 02:11:31.306084: step 117460, total loss = 0.54, predict loss = 0.12 (77.7 examples/sec; 0.052 sec/batch; 84h:09m:40s remains)
INFO - root - 2019-11-04 02:11:31.923707: step 117470, total loss = 0.61, predict loss = 0.13 (75.0 examples/sec; 0.053 sec/batch; 87h:11m:30s remains)
INFO - root - 2019-11-04 02:11:32.498901: step 117480, total loss = 0.39, predict loss = 0.08 (75.4 examples/sec; 0.053 sec/batch; 86h:41m:48s remains)
INFO - root - 2019-11-04 02:11:33.094480: step 117490, total loss = 0.45, predict loss = 0.10 (77.9 examples/sec; 0.051 sec/batch; 83h:53m:28s remains)
INFO - root - 2019-11-04 02:11:33.692276: step 117500, total loss = 0.59, predict loss = 0.14 (73.8 examples/sec; 0.054 sec/batch; 88h:36m:30s remains)
INFO - root - 2019-11-04 02:11:34.320076: step 117510, total loss = 0.50, predict loss = 0.12 (63.7 examples/sec; 0.063 sec/batch; 102h:34m:50s remains)
INFO - root - 2019-11-04 02:11:34.939289: step 117520, total loss = 0.47, predict loss = 0.11 (69.8 examples/sec; 0.057 sec/batch; 93h:39m:29s remains)
INFO - root - 2019-11-04 02:11:35.551523: step 117530, total loss = 0.55, predict loss = 0.12 (75.1 examples/sec; 0.053 sec/batch; 87h:00m:08s remains)
INFO - root - 2019-11-04 02:11:36.191462: step 117540, total loss = 0.51, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 89h:46m:32s remains)
INFO - root - 2019-11-04 02:11:36.786543: step 117550, total loss = 0.57, predict loss = 0.13 (67.8 examples/sec; 0.059 sec/batch; 96h:28m:00s remains)
INFO - root - 2019-11-04 02:11:37.413615: step 117560, total loss = 0.52, predict loss = 0.12 (65.6 examples/sec; 0.061 sec/batch; 99h:34m:12s remains)
INFO - root - 2019-11-04 02:11:38.079131: step 117570, total loss = 0.43, predict loss = 0.10 (78.2 examples/sec; 0.051 sec/batch; 83h:33m:19s remains)
INFO - root - 2019-11-04 02:11:38.720999: step 117580, total loss = 0.37, predict loss = 0.08 (70.1 examples/sec; 0.057 sec/batch; 93h:16m:11s remains)
INFO - root - 2019-11-04 02:11:39.359781: step 117590, total loss = 0.49, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 95h:57m:49s remains)
INFO - root - 2019-11-04 02:11:39.985226: step 117600, total loss = 0.36, predict loss = 0.08 (73.2 examples/sec; 0.055 sec/batch; 89h:15m:13s remains)
INFO - root - 2019-11-04 02:11:40.605286: step 117610, total loss = 0.38, predict loss = 0.09 (68.4 examples/sec; 0.059 sec/batch; 95h:36m:34s remains)
INFO - root - 2019-11-04 02:11:41.222929: step 117620, total loss = 0.33, predict loss = 0.08 (79.0 examples/sec; 0.051 sec/batch; 82h:46m:23s remains)
INFO - root - 2019-11-04 02:11:41.908300: step 117630, total loss = 0.39, predict loss = 0.09 (56.3 examples/sec; 0.071 sec/batch; 116h:10m:16s remains)
INFO - root - 2019-11-04 02:11:42.555566: step 117640, total loss = 0.53, predict loss = 0.12 (72.8 examples/sec; 0.055 sec/batch; 89h:44m:02s remains)
INFO - root - 2019-11-04 02:11:43.185984: step 117650, total loss = 0.38, predict loss = 0.09 (81.5 examples/sec; 0.049 sec/batch; 80h:13m:38s remains)
INFO - root - 2019-11-04 02:11:43.812720: step 117660, total loss = 0.39, predict loss = 0.09 (73.1 examples/sec; 0.055 sec/batch; 89h:22m:44s remains)
INFO - root - 2019-11-04 02:11:44.425536: step 117670, total loss = 0.41, predict loss = 0.09 (83.2 examples/sec; 0.048 sec/batch; 78h:30m:37s remains)
INFO - root - 2019-11-04 02:11:45.016084: step 117680, total loss = 0.45, predict loss = 0.09 (81.4 examples/sec; 0.049 sec/batch; 80h:16m:28s remains)
INFO - root - 2019-11-04 02:11:45.620017: step 117690, total loss = 0.44, predict loss = 0.10 (79.1 examples/sec; 0.051 sec/batch; 82h:35m:18s remains)
INFO - root - 2019-11-04 02:11:46.218286: step 117700, total loss = 0.43, predict loss = 0.10 (70.7 examples/sec; 0.057 sec/batch; 92h:27m:51s remains)
INFO - root - 2019-11-04 02:11:46.861533: step 117710, total loss = 0.58, predict loss = 0.14 (71.4 examples/sec; 0.056 sec/batch; 91h:31m:46s remains)
INFO - root - 2019-11-04 02:11:47.453209: step 117720, total loss = 0.48, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 89h:43m:21s remains)
INFO - root - 2019-11-04 02:11:48.086349: step 117730, total loss = 0.46, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 97h:36m:45s remains)
INFO - root - 2019-11-04 02:11:48.750828: step 117740, total loss = 0.62, predict loss = 0.14 (67.4 examples/sec; 0.059 sec/batch; 96h:55m:57s remains)
INFO - root - 2019-11-04 02:11:49.361998: step 117750, total loss = 0.62, predict loss = 0.15 (70.6 examples/sec; 0.057 sec/batch; 92h:33m:34s remains)
INFO - root - 2019-11-04 02:11:49.998832: step 117760, total loss = 0.47, predict loss = 0.11 (74.3 examples/sec; 0.054 sec/batch; 87h:56m:22s remains)
INFO - root - 2019-11-04 02:11:50.603611: step 117770, total loss = 0.63, predict loss = 0.15 (78.5 examples/sec; 0.051 sec/batch; 83h:16m:18s remains)
INFO - root - 2019-11-04 02:11:51.258840: step 117780, total loss = 0.71, predict loss = 0.17 (71.1 examples/sec; 0.056 sec/batch; 91h:57m:44s remains)
INFO - root - 2019-11-04 02:11:51.906195: step 117790, total loss = 0.58, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 93h:54m:27s remains)
INFO - root - 2019-11-04 02:11:52.542109: step 117800, total loss = 0.62, predict loss = 0.15 (80.0 examples/sec; 0.050 sec/batch; 81h:41m:38s remains)
INFO - root - 2019-11-04 02:11:53.165673: step 117810, total loss = 0.68, predict loss = 0.17 (74.2 examples/sec; 0.054 sec/batch; 88h:06m:20s remains)
INFO - root - 2019-11-04 02:11:53.805333: step 117820, total loss = 0.72, predict loss = 0.18 (65.8 examples/sec; 0.061 sec/batch; 99h:16m:14s remains)
INFO - root - 2019-11-04 02:11:54.453897: step 117830, total loss = 0.67, predict loss = 0.15 (66.2 examples/sec; 0.060 sec/batch; 98h:40m:39s remains)
INFO - root - 2019-11-04 02:11:55.102706: step 117840, total loss = 0.47, predict loss = 0.10 (66.7 examples/sec; 0.060 sec/batch; 98h:02m:33s remains)
INFO - root - 2019-11-04 02:11:55.740111: step 117850, total loss = 0.55, predict loss = 0.13 (78.6 examples/sec; 0.051 sec/batch; 83h:08m:06s remains)
INFO - root - 2019-11-04 02:11:56.400733: step 117860, total loss = 0.46, predict loss = 0.10 (67.5 examples/sec; 0.059 sec/batch; 96h:47m:30s remains)
INFO - root - 2019-11-04 02:11:57.030921: step 117870, total loss = 0.40, predict loss = 0.09 (71.8 examples/sec; 0.056 sec/batch; 91h:04m:58s remains)
INFO - root - 2019-11-04 02:11:57.651501: step 117880, total loss = 0.40, predict loss = 0.09 (67.1 examples/sec; 0.060 sec/batch; 97h:20m:46s remains)
INFO - root - 2019-11-04 02:11:58.273299: step 117890, total loss = 0.37, predict loss = 0.08 (73.0 examples/sec; 0.055 sec/batch; 89h:32m:15s remains)
INFO - root - 2019-11-04 02:11:58.926869: step 117900, total loss = 0.30, predict loss = 0.06 (67.8 examples/sec; 0.059 sec/batch; 96h:27m:56s remains)
INFO - root - 2019-11-04 02:11:59.548337: step 117910, total loss = 0.38, predict loss = 0.08 (75.2 examples/sec; 0.053 sec/batch; 86h:51m:11s remains)
INFO - root - 2019-11-04 02:12:00.171057: step 117920, total loss = 0.43, predict loss = 0.10 (71.9 examples/sec; 0.056 sec/batch; 90h:57m:06s remains)
INFO - root - 2019-11-04 02:12:00.819189: step 117930, total loss = 0.57, predict loss = 0.14 (74.2 examples/sec; 0.054 sec/batch; 88h:05m:54s remains)
INFO - root - 2019-11-04 02:12:01.488521: step 117940, total loss = 0.55, predict loss = 0.13 (66.5 examples/sec; 0.060 sec/batch; 98h:12m:52s remains)
INFO - root - 2019-11-04 02:12:02.179831: step 117950, total loss = 0.41, predict loss = 0.09 (60.5 examples/sec; 0.066 sec/batch; 108h:04m:43s remains)
INFO - root - 2019-11-04 02:12:02.815139: step 117960, total loss = 0.40, predict loss = 0.09 (70.5 examples/sec; 0.057 sec/batch; 92h:42m:38s remains)
INFO - root - 2019-11-04 02:12:03.486743: step 117970, total loss = 0.61, predict loss = 0.14 (68.2 examples/sec; 0.059 sec/batch; 95h:52m:30s remains)
INFO - root - 2019-11-04 02:12:04.120019: step 117980, total loss = 0.63, predict loss = 0.15 (75.3 examples/sec; 0.053 sec/batch; 86h:49m:03s remains)
INFO - root - 2019-11-04 02:12:04.731403: step 117990, total loss = 0.67, predict loss = 0.16 (78.1 examples/sec; 0.051 sec/batch; 83h:42m:54s remains)
INFO - root - 2019-11-04 02:12:05.314547: step 118000, total loss = 0.52, predict loss = 0.12 (73.0 examples/sec; 0.055 sec/batch; 89h:33m:33s remains)
INFO - root - 2019-11-04 02:12:05.929792: step 118010, total loss = 0.57, predict loss = 0.14 (65.5 examples/sec; 0.061 sec/batch; 99h:47m:30s remains)
INFO - root - 2019-11-04 02:12:06.593622: step 118020, total loss = 0.57, predict loss = 0.14 (64.6 examples/sec; 0.062 sec/batch; 101h:07m:16s remains)
INFO - root - 2019-11-04 02:12:07.206249: step 118030, total loss = 0.64, predict loss = 0.16 (73.9 examples/sec; 0.054 sec/batch; 88h:24m:11s remains)
INFO - root - 2019-11-04 02:12:07.847728: step 118040, total loss = 0.63, predict loss = 0.15 (73.2 examples/sec; 0.055 sec/batch; 89h:16m:46s remains)
INFO - root - 2019-11-04 02:12:08.524382: step 118050, total loss = 0.57, predict loss = 0.14 (62.9 examples/sec; 0.064 sec/batch; 103h:58m:38s remains)
INFO - root - 2019-11-04 02:12:09.161450: step 118060, total loss = 0.56, predict loss = 0.13 (68.3 examples/sec; 0.059 sec/batch; 95h:38m:11s remains)
INFO - root - 2019-11-04 02:12:09.846842: step 118070, total loss = 0.46, predict loss = 0.11 (63.3 examples/sec; 0.063 sec/batch; 103h:15m:33s remains)
INFO - root - 2019-11-04 02:12:10.485125: step 118080, total loss = 0.49, predict loss = 0.12 (76.6 examples/sec; 0.052 sec/batch; 85h:16m:48s remains)
INFO - root - 2019-11-04 02:12:11.141672: step 118090, total loss = 0.46, predict loss = 0.11 (64.0 examples/sec; 0.062 sec/batch; 102h:05m:53s remains)
INFO - root - 2019-11-04 02:12:11.826844: step 118100, total loss = 0.48, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 93h:45m:32s remains)
INFO - root - 2019-11-04 02:12:12.427870: step 118110, total loss = 0.59, predict loss = 0.13 (74.0 examples/sec; 0.054 sec/batch; 88h:21m:11s remains)
INFO - root - 2019-11-04 02:12:13.030339: step 118120, total loss = 0.49, predict loss = 0.12 (74.6 examples/sec; 0.054 sec/batch; 87h:36m:48s remains)
INFO - root - 2019-11-04 02:12:13.651053: step 118130, total loss = 0.38, predict loss = 0.08 (72.5 examples/sec; 0.055 sec/batch; 90h:08m:07s remains)
INFO - root - 2019-11-04 02:12:14.266902: step 118140, total loss = 0.43, predict loss = 0.10 (73.8 examples/sec; 0.054 sec/batch; 88h:31m:31s remains)
INFO - root - 2019-11-04 02:12:14.852234: step 118150, total loss = 0.56, predict loss = 0.14 (75.0 examples/sec; 0.053 sec/batch; 87h:07m:27s remains)
INFO - root - 2019-11-04 02:12:15.469295: step 118160, total loss = 0.47, predict loss = 0.11 (70.7 examples/sec; 0.057 sec/batch; 92h:27m:47s remains)
INFO - root - 2019-11-04 02:12:16.093432: step 118170, total loss = 0.46, predict loss = 0.11 (74.5 examples/sec; 0.054 sec/batch; 87h:45m:41s remains)
INFO - root - 2019-11-04 02:12:16.737014: step 118180, total loss = 0.45, predict loss = 0.10 (62.6 examples/sec; 0.064 sec/batch; 104h:25m:49s remains)
INFO - root - 2019-11-04 02:12:17.331521: step 118190, total loss = 0.35, predict loss = 0.08 (82.0 examples/sec; 0.049 sec/batch; 79h:40m:17s remains)
INFO - root - 2019-11-04 02:12:17.938137: step 118200, total loss = 0.41, predict loss = 0.09 (80.4 examples/sec; 0.050 sec/batch; 81h:18m:56s remains)
INFO - root - 2019-11-04 02:12:18.566509: step 118210, total loss = 0.31, predict loss = 0.06 (76.1 examples/sec; 0.053 sec/batch; 85h:52m:17s remains)
INFO - root - 2019-11-04 02:12:19.176268: step 118220, total loss = 0.45, predict loss = 0.10 (71.8 examples/sec; 0.056 sec/batch; 91h:03m:55s remains)
INFO - root - 2019-11-04 02:12:19.777507: step 118230, total loss = 0.48, predict loss = 0.11 (70.5 examples/sec; 0.057 sec/batch; 92h:39m:30s remains)
INFO - root - 2019-11-04 02:12:20.420505: step 118240, total loss = 0.46, predict loss = 0.10 (61.5 examples/sec; 0.065 sec/batch; 106h:11m:54s remains)
INFO - root - 2019-11-04 02:12:21.013806: step 118250, total loss = 0.35, predict loss = 0.07 (70.4 examples/sec; 0.057 sec/batch; 92h:53m:40s remains)
INFO - root - 2019-11-04 02:12:21.655384: step 118260, total loss = 0.56, predict loss = 0.13 (62.8 examples/sec; 0.064 sec/batch; 104h:07m:19s remains)
INFO - root - 2019-11-04 02:12:22.326792: step 118270, total loss = 0.45, predict loss = 0.10 (62.4 examples/sec; 0.064 sec/batch; 104h:46m:58s remains)
INFO - root - 2019-11-04 02:12:22.987963: step 118280, total loss = 0.46, predict loss = 0.10 (79.0 examples/sec; 0.051 sec/batch; 82h:43m:33s remains)
INFO - root - 2019-11-04 02:12:23.619288: step 118290, total loss = 0.52, predict loss = 0.13 (71.9 examples/sec; 0.056 sec/batch; 90h:54m:46s remains)
INFO - root - 2019-11-04 02:12:24.282465: step 118300, total loss = 0.51, predict loss = 0.12 (61.9 examples/sec; 0.065 sec/batch; 105h:38m:04s remains)
INFO - root - 2019-11-04 02:12:24.918092: step 118310, total loss = 0.52, predict loss = 0.12 (72.0 examples/sec; 0.056 sec/batch; 90h:47m:22s remains)
INFO - root - 2019-11-04 02:12:25.548743: step 118320, total loss = 0.54, predict loss = 0.13 (74.4 examples/sec; 0.054 sec/batch; 87h:48m:50s remains)
INFO - root - 2019-11-04 02:12:26.160812: step 118330, total loss = 0.50, predict loss = 0.12 (66.1 examples/sec; 0.060 sec/batch; 98h:49m:10s remains)
INFO - root - 2019-11-04 02:12:26.784235: step 118340, total loss = 0.49, predict loss = 0.11 (82.7 examples/sec; 0.048 sec/batch; 79h:03m:20s remains)
INFO - root - 2019-11-04 02:12:27.415208: step 118350, total loss = 0.42, predict loss = 0.10 (80.3 examples/sec; 0.050 sec/batch; 81h:25m:07s remains)
INFO - root - 2019-11-04 02:12:28.017487: step 118360, total loss = 0.59, predict loss = 0.15 (73.2 examples/sec; 0.055 sec/batch; 89h:14m:13s remains)
INFO - root - 2019-11-04 02:12:28.621897: step 118370, total loss = 0.62, predict loss = 0.15 (73.5 examples/sec; 0.054 sec/batch; 88h:51m:38s remains)
INFO - root - 2019-11-04 02:12:29.230514: step 118380, total loss = 0.49, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 95h:40m:13s remains)
INFO - root - 2019-11-04 02:12:29.922117: step 118390, total loss = 0.65, predict loss = 0.15 (65.4 examples/sec; 0.061 sec/batch; 99h:56m:25s remains)
INFO - root - 2019-11-04 02:12:30.520494: step 118400, total loss = 0.57, predict loss = 0.13 (79.8 examples/sec; 0.050 sec/batch; 81h:51m:31s remains)
INFO - root - 2019-11-04 02:12:31.128611: step 118410, total loss = 0.65, predict loss = 0.16 (77.8 examples/sec; 0.051 sec/batch; 83h:57m:09s remains)
INFO - root - 2019-11-04 02:12:31.740266: step 118420, total loss = 0.65, predict loss = 0.15 (65.8 examples/sec; 0.061 sec/batch; 99h:23m:25s remains)
INFO - root - 2019-11-04 02:12:32.367782: step 118430, total loss = 0.79, predict loss = 0.19 (63.9 examples/sec; 0.063 sec/batch; 102h:14m:01s remains)
INFO - root - 2019-11-04 02:12:33.045021: step 118440, total loss = 0.65, predict loss = 0.15 (57.6 examples/sec; 0.069 sec/batch; 113h:29m:48s remains)
INFO - root - 2019-11-04 02:12:33.729460: step 118450, total loss = 0.59, predict loss = 0.13 (68.5 examples/sec; 0.058 sec/batch; 95h:23m:22s remains)
INFO - root - 2019-11-04 02:12:34.348194: step 118460, total loss = 0.65, predict loss = 0.14 (66.9 examples/sec; 0.060 sec/batch; 97h:37m:44s remains)
INFO - root - 2019-11-04 02:12:34.980745: step 118470, total loss = 0.44, predict loss = 0.10 (64.5 examples/sec; 0.062 sec/batch; 101h:21m:44s remains)
INFO - root - 2019-11-04 02:12:35.600011: step 118480, total loss = 0.42, predict loss = 0.10 (72.2 examples/sec; 0.055 sec/batch; 90h:32m:43s remains)
INFO - root - 2019-11-04 02:12:36.202579: step 118490, total loss = 0.55, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 91h:08m:19s remains)
INFO - root - 2019-11-04 02:12:36.829227: step 118500, total loss = 0.56, predict loss = 0.13 (74.0 examples/sec; 0.054 sec/batch; 88h:20m:28s remains)
INFO - root - 2019-11-04 02:12:37.459117: step 118510, total loss = 0.41, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 93h:58m:28s remains)
INFO - root - 2019-11-04 02:12:38.122605: step 118520, total loss = 0.54, predict loss = 0.12 (65.2 examples/sec; 0.061 sec/batch; 100h:12m:14s remains)
INFO - root - 2019-11-04 02:12:38.775969: step 118530, total loss = 0.59, predict loss = 0.14 (61.6 examples/sec; 0.065 sec/batch; 106h:02m:26s remains)
INFO - root - 2019-11-04 02:12:39.412067: step 118540, total loss = 0.50, predict loss = 0.11 (77.7 examples/sec; 0.051 sec/batch; 84h:07m:25s remains)
INFO - root - 2019-11-04 02:12:40.059458: step 118550, total loss = 0.45, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 93h:14m:36s remains)
INFO - root - 2019-11-04 02:12:40.702499: step 118560, total loss = 0.48, predict loss = 0.11 (65.8 examples/sec; 0.061 sec/batch; 99h:19m:04s remains)
INFO - root - 2019-11-04 02:12:41.378332: step 118570, total loss = 0.58, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 92h:57m:37s remains)
INFO - root - 2019-11-04 02:12:42.071789: step 118580, total loss = 0.56, predict loss = 0.12 (66.8 examples/sec; 0.060 sec/batch; 97h:50m:35s remains)
INFO - root - 2019-11-04 02:12:42.755435: step 118590, total loss = 0.61, predict loss = 0.14 (68.3 examples/sec; 0.059 sec/batch; 95h:37m:43s remains)
INFO - root - 2019-11-04 02:12:43.422815: step 118600, total loss = 0.42, predict loss = 0.09 (60.3 examples/sec; 0.066 sec/batch; 108h:24m:24s remains)
INFO - root - 2019-11-04 02:12:44.048310: step 118610, total loss = 0.41, predict loss = 0.09 (72.6 examples/sec; 0.055 sec/batch; 90h:03m:17s remains)
INFO - root - 2019-11-04 02:12:44.560810: step 118620, total loss = 0.48, predict loss = 0.11 (94.4 examples/sec; 0.042 sec/batch; 69h:11m:25s remains)
INFO - root - 2019-11-04 02:12:45.010160: step 118630, total loss = 0.49, predict loss = 0.11 (96.0 examples/sec; 0.042 sec/batch; 68h:03m:20s remains)
INFO - root - 2019-11-04 02:12:46.575094: step 118640, total loss = 0.34, predict loss = 0.06 (78.6 examples/sec; 0.051 sec/batch; 83h:08m:18s remains)
INFO - root - 2019-11-04 02:12:47.176645: step 118650, total loss = 0.43, predict loss = 0.09 (73.0 examples/sec; 0.055 sec/batch; 89h:32m:51s remains)
INFO - root - 2019-11-04 02:12:47.847743: step 118660, total loss = 0.49, predict loss = 0.11 (65.0 examples/sec; 0.062 sec/batch; 100h:34m:00s remains)
INFO - root - 2019-11-04 02:12:48.519876: step 118670, total loss = 0.41, predict loss = 0.10 (61.5 examples/sec; 0.065 sec/batch; 106h:11m:11s remains)
INFO - root - 2019-11-04 02:12:49.155150: step 118680, total loss = 0.59, predict loss = 0.14 (66.9 examples/sec; 0.060 sec/batch; 97h:37m:22s remains)
INFO - root - 2019-11-04 02:12:49.790886: step 118690, total loss = 0.53, predict loss = 0.13 (68.4 examples/sec; 0.059 sec/batch; 95h:34m:42s remains)
INFO - root - 2019-11-04 02:12:50.388580: step 118700, total loss = 0.42, predict loss = 0.09 (85.8 examples/sec; 0.047 sec/batch; 76h:08m:52s remains)
INFO - root - 2019-11-04 02:12:50.998670: step 118710, total loss = 0.44, predict loss = 0.10 (80.0 examples/sec; 0.050 sec/batch; 81h:38m:52s remains)
INFO - root - 2019-11-04 02:12:51.608303: step 118720, total loss = 0.62, predict loss = 0.14 (72.6 examples/sec; 0.055 sec/batch; 89h:59m:38s remains)
INFO - root - 2019-11-04 02:12:52.233784: step 118730, total loss = 0.45, predict loss = 0.10 (75.3 examples/sec; 0.053 sec/batch; 86h:48m:01s remains)
INFO - root - 2019-11-04 02:12:52.876593: step 118740, total loss = 0.55, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 95h:39m:53s remains)
INFO - root - 2019-11-04 02:12:53.502683: step 118750, total loss = 0.51, predict loss = 0.11 (70.9 examples/sec; 0.056 sec/batch; 92h:06m:20s remains)
INFO - root - 2019-11-04 02:12:54.152222: step 118760, total loss = 0.55, predict loss = 0.13 (77.4 examples/sec; 0.052 sec/batch; 84h:25m:24s remains)
INFO - root - 2019-11-04 02:12:54.786820: step 118770, total loss = 0.57, predict loss = 0.13 (64.4 examples/sec; 0.062 sec/batch; 101h:27m:36s remains)
INFO - root - 2019-11-04 02:12:55.421849: step 118780, total loss = 0.60, predict loss = 0.14 (66.7 examples/sec; 0.060 sec/batch; 97h:56m:05s remains)
INFO - root - 2019-11-04 02:12:56.043125: step 118790, total loss = 0.40, predict loss = 0.08 (71.0 examples/sec; 0.056 sec/batch; 92h:02m:46s remains)
INFO - root - 2019-11-04 02:12:56.687788: step 118800, total loss = 0.52, predict loss = 0.11 (67.3 examples/sec; 0.059 sec/batch; 97h:03m:25s remains)
INFO - root - 2019-11-04 02:12:57.306033: step 118810, total loss = 0.46, predict loss = 0.10 (75.2 examples/sec; 0.053 sec/batch; 86h:52m:54s remains)
INFO - root - 2019-11-04 02:12:57.936246: step 118820, total loss = 0.44, predict loss = 0.10 (72.8 examples/sec; 0.055 sec/batch; 89h:47m:41s remains)
INFO - root - 2019-11-04 02:12:58.544502: step 118830, total loss = 0.44, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 95h:17m:31s remains)
INFO - root - 2019-11-04 02:12:59.209595: step 118840, total loss = 0.33, predict loss = 0.07 (61.9 examples/sec; 0.065 sec/batch; 105h:36m:32s remains)
INFO - root - 2019-11-04 02:12:59.812452: step 118850, total loss = 0.30, predict loss = 0.06 (72.7 examples/sec; 0.055 sec/batch; 89h:49m:45s remains)
INFO - root - 2019-11-04 02:13:00.405376: step 118860, total loss = 0.59, predict loss = 0.14 (82.6 examples/sec; 0.048 sec/batch; 79h:09m:03s remains)
INFO - root - 2019-11-04 02:13:01.032029: step 118870, total loss = 0.39, predict loss = 0.09 (74.3 examples/sec; 0.054 sec/batch; 87h:54m:56s remains)
INFO - root - 2019-11-04 02:13:01.656071: step 118880, total loss = 0.54, predict loss = 0.13 (78.4 examples/sec; 0.051 sec/batch; 83h:18m:00s remains)
INFO - root - 2019-11-04 02:13:02.255586: step 118890, total loss = 0.59, predict loss = 0.14 (72.9 examples/sec; 0.055 sec/batch; 89h:41m:26s remains)
INFO - root - 2019-11-04 02:13:02.874134: step 118900, total loss = 0.54, predict loss = 0.12 (62.9 examples/sec; 0.064 sec/batch; 103h:49m:41s remains)
INFO - root - 2019-11-04 02:13:03.575656: step 118910, total loss = 0.63, predict loss = 0.15 (59.2 examples/sec; 0.068 sec/batch; 110h:22m:55s remains)
INFO - root - 2019-11-04 02:13:04.192255: step 118920, total loss = 0.46, predict loss = 0.11 (69.5 examples/sec; 0.058 sec/batch; 93h:59m:14s remains)
INFO - root - 2019-11-04 02:13:04.841097: step 118930, total loss = 0.57, predict loss = 0.14 (75.2 examples/sec; 0.053 sec/batch; 86h:50m:51s remains)
INFO - root - 2019-11-04 02:13:05.571817: step 118940, total loss = 0.62, predict loss = 0.15 (58.6 examples/sec; 0.068 sec/batch; 111h:30m:38s remains)
INFO - root - 2019-11-04 02:13:06.236051: step 118950, total loss = 0.60, predict loss = 0.14 (76.8 examples/sec; 0.052 sec/batch; 85h:01m:56s remains)
INFO - root - 2019-11-04 02:13:06.926899: step 118960, total loss = 0.57, predict loss = 0.13 (56.2 examples/sec; 0.071 sec/batch; 116h:18m:03s remains)
INFO - root - 2019-11-04 02:13:07.567717: step 118970, total loss = 0.51, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 89h:48m:10s remains)
INFO - root - 2019-11-04 02:13:08.189800: step 118980, total loss = 0.50, predict loss = 0.12 (67.0 examples/sec; 0.060 sec/batch; 97h:32m:23s remains)
INFO - root - 2019-11-04 02:13:08.812436: step 118990, total loss = 0.58, predict loss = 0.13 (75.7 examples/sec; 0.053 sec/batch; 86h:17m:17s remains)
INFO - root - 2019-11-04 02:13:09.432436: step 119000, total loss = 0.51, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 97h:36m:06s remains)
INFO - root - 2019-11-04 02:13:10.087010: step 119010, total loss = 0.38, predict loss = 0.08 (76.5 examples/sec; 0.052 sec/batch; 85h:27m:24s remains)
INFO - root - 2019-11-04 02:13:10.712355: step 119020, total loss = 0.41, predict loss = 0.09 (76.6 examples/sec; 0.052 sec/batch; 85h:18m:36s remains)
INFO - root - 2019-11-04 02:13:11.341227: step 119030, total loss = 0.54, predict loss = 0.12 (78.1 examples/sec; 0.051 sec/batch; 83h:42m:38s remains)
INFO - root - 2019-11-04 02:13:11.961099: step 119040, total loss = 0.45, predict loss = 0.10 (75.1 examples/sec; 0.053 sec/batch; 86h:59m:51s remains)
INFO - root - 2019-11-04 02:13:12.568267: step 119050, total loss = 0.53, predict loss = 0.13 (71.8 examples/sec; 0.056 sec/batch; 90h:57m:45s remains)
INFO - root - 2019-11-04 02:13:13.167485: step 119060, total loss = 0.61, predict loss = 0.15 (76.6 examples/sec; 0.052 sec/batch; 85h:17m:18s remains)
INFO - root - 2019-11-04 02:13:13.766634: step 119070, total loss = 0.47, predict loss = 0.10 (72.0 examples/sec; 0.056 sec/batch; 90h:48m:20s remains)
INFO - root - 2019-11-04 02:13:14.393861: step 119080, total loss = 0.42, predict loss = 0.09 (61.6 examples/sec; 0.065 sec/batch; 106h:07m:24s remains)
INFO - root - 2019-11-04 02:13:15.001557: step 119090, total loss = 0.45, predict loss = 0.11 (73.6 examples/sec; 0.054 sec/batch; 88h:45m:25s remains)
INFO - root - 2019-11-04 02:13:15.601782: step 119100, total loss = 0.47, predict loss = 0.11 (73.5 examples/sec; 0.054 sec/batch; 88h:53m:12s remains)
INFO - root - 2019-11-04 02:13:16.253129: step 119110, total loss = 0.62, predict loss = 0.14 (72.3 examples/sec; 0.055 sec/batch; 90h:24m:42s remains)
INFO - root - 2019-11-04 02:13:16.899499: step 119120, total loss = 0.32, predict loss = 0.07 (72.3 examples/sec; 0.055 sec/batch; 90h:23m:40s remains)
INFO - root - 2019-11-04 02:13:17.552046: step 119130, total loss = 0.41, predict loss = 0.09 (68.1 examples/sec; 0.059 sec/batch; 95h:53m:52s remains)
INFO - root - 2019-11-04 02:13:18.198296: step 119140, total loss = 0.56, predict loss = 0.14 (75.2 examples/sec; 0.053 sec/batch; 86h:55m:50s remains)
INFO - root - 2019-11-04 02:13:18.844724: step 119150, total loss = 0.59, predict loss = 0.14 (75.8 examples/sec; 0.053 sec/batch; 86h:14m:58s remains)
INFO - root - 2019-11-04 02:13:19.475583: step 119160, total loss = 0.51, predict loss = 0.12 (65.5 examples/sec; 0.061 sec/batch; 99h:44m:32s remains)
INFO - root - 2019-11-04 02:13:20.119205: step 119170, total loss = 0.62, predict loss = 0.15 (69.1 examples/sec; 0.058 sec/batch; 94h:34m:55s remains)
INFO - root - 2019-11-04 02:13:20.725364: step 119180, total loss = 0.50, predict loss = 0.12 (84.1 examples/sec; 0.048 sec/batch; 77h:41m:00s remains)
INFO - root - 2019-11-04 02:13:21.326506: step 119190, total loss = 0.31, predict loss = 0.08 (74.1 examples/sec; 0.054 sec/batch; 88h:11m:03s remains)
INFO - root - 2019-11-04 02:13:21.950762: step 119200, total loss = 0.25, predict loss = 0.06 (69.3 examples/sec; 0.058 sec/batch; 94h:20m:21s remains)
INFO - root - 2019-11-04 02:13:22.599627: step 119210, total loss = 0.42, predict loss = 0.11 (63.5 examples/sec; 0.063 sec/batch; 102h:49m:24s remains)
INFO - root - 2019-11-04 02:13:23.237993: step 119220, total loss = 0.28, predict loss = 0.06 (74.4 examples/sec; 0.054 sec/batch; 87h:52m:09s remains)
INFO - root - 2019-11-04 02:13:23.904667: step 119230, total loss = 0.39, predict loss = 0.09 (65.3 examples/sec; 0.061 sec/batch; 100h:03m:00s remains)
INFO - root - 2019-11-04 02:13:24.557894: step 119240, total loss = 0.36, predict loss = 0.08 (73.1 examples/sec; 0.055 sec/batch; 89h:25m:36s remains)
INFO - root - 2019-11-04 02:13:25.164212: step 119250, total loss = 0.31, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 81h:59m:59s remains)
INFO - root - 2019-11-04 02:13:25.769654: step 119260, total loss = 0.46, predict loss = 0.11 (80.9 examples/sec; 0.049 sec/batch; 80h:48m:06s remains)
INFO - root - 2019-11-04 02:13:26.430986: step 119270, total loss = 0.36, predict loss = 0.08 (64.6 examples/sec; 0.062 sec/batch; 101h:04m:15s remains)
INFO - root - 2019-11-04 02:13:27.129040: step 119280, total loss = 0.34, predict loss = 0.07 (62.9 examples/sec; 0.064 sec/batch; 103h:55m:19s remains)
INFO - root - 2019-11-04 02:13:27.774996: step 119290, total loss = 0.47, predict loss = 0.10 (68.7 examples/sec; 0.058 sec/batch; 95h:05m:02s remains)
INFO - root - 2019-11-04 02:13:28.390130: step 119300, total loss = 0.50, predict loss = 0.12 (74.7 examples/sec; 0.054 sec/batch; 87h:31m:42s remains)
INFO - root - 2019-11-04 02:13:29.022660: step 119310, total loss = 0.41, predict loss = 0.09 (74.1 examples/sec; 0.054 sec/batch; 88h:14m:13s remains)
INFO - root - 2019-11-04 02:13:29.625438: step 119320, total loss = 0.47, predict loss = 0.11 (77.3 examples/sec; 0.052 sec/batch; 84h:33m:16s remains)
INFO - root - 2019-11-04 02:13:30.237928: step 119330, total loss = 0.51, predict loss = 0.12 (73.6 examples/sec; 0.054 sec/batch; 88h:50m:04s remains)
INFO - root - 2019-11-04 02:13:30.874838: step 119340, total loss = 0.64, predict loss = 0.15 (71.1 examples/sec; 0.056 sec/batch; 91h:55m:23s remains)
INFO - root - 2019-11-04 02:13:31.502708: step 119350, total loss = 0.54, predict loss = 0.13 (79.6 examples/sec; 0.050 sec/batch; 82h:04m:44s remains)
INFO - root - 2019-11-04 02:13:32.086760: step 119360, total loss = 0.38, predict loss = 0.09 (79.2 examples/sec; 0.050 sec/batch; 82h:27m:50s remains)
INFO - root - 2019-11-04 02:13:32.665002: step 119370, total loss = 0.48, predict loss = 0.11 (76.5 examples/sec; 0.052 sec/batch; 85h:23m:26s remains)
INFO - root - 2019-11-04 02:13:33.259492: step 119380, total loss = 0.66, predict loss = 0.15 (72.0 examples/sec; 0.056 sec/batch; 90h:44m:58s remains)
INFO - root - 2019-11-04 02:13:33.885241: step 119390, total loss = 0.37, predict loss = 0.08 (76.9 examples/sec; 0.052 sec/batch; 85h:01m:10s remains)
INFO - root - 2019-11-04 02:13:34.495238: step 119400, total loss = 0.42, predict loss = 0.09 (79.8 examples/sec; 0.050 sec/batch; 81h:54m:40s remains)
INFO - root - 2019-11-04 02:13:35.129778: step 119410, total loss = 0.40, predict loss = 0.10 (80.5 examples/sec; 0.050 sec/batch; 81h:07m:21s remains)
INFO - root - 2019-11-04 02:13:35.758413: step 119420, total loss = 0.70, predict loss = 0.18 (69.9 examples/sec; 0.057 sec/batch; 93h:27m:54s remains)
INFO - root - 2019-11-04 02:13:36.382118: step 119430, total loss = 0.62, predict loss = 0.16 (72.8 examples/sec; 0.055 sec/batch; 89h:47m:49s remains)
INFO - root - 2019-11-04 02:13:37.026954: step 119440, total loss = 0.34, predict loss = 0.07 (65.1 examples/sec; 0.061 sec/batch; 100h:18m:04s remains)
INFO - root - 2019-11-04 02:13:37.659374: step 119450, total loss = 0.46, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 92h:56m:01s remains)
INFO - root - 2019-11-04 02:13:38.299051: step 119460, total loss = 0.32, predict loss = 0.08 (69.9 examples/sec; 0.057 sec/batch; 93h:28m:54s remains)
INFO - root - 2019-11-04 02:13:38.952477: step 119470, total loss = 0.36, predict loss = 0.08 (65.0 examples/sec; 0.062 sec/batch; 100h:28m:10s remains)
INFO - root - 2019-11-04 02:13:39.589602: step 119480, total loss = 0.31, predict loss = 0.07 (74.4 examples/sec; 0.054 sec/batch; 87h:50m:47s remains)
INFO - root - 2019-11-04 02:13:40.223517: step 119490, total loss = 0.21, predict loss = 0.04 (72.8 examples/sec; 0.055 sec/batch; 89h:46m:31s remains)
INFO - root - 2019-11-04 02:13:40.886691: step 119500, total loss = 0.34, predict loss = 0.08 (68.4 examples/sec; 0.058 sec/batch; 95h:31m:44s remains)
INFO - root - 2019-11-04 02:13:41.530956: step 119510, total loss = 0.20, predict loss = 0.04 (74.2 examples/sec; 0.054 sec/batch; 88h:03m:48s remains)
INFO - root - 2019-11-04 02:13:42.161423: step 119520, total loss = 0.28, predict loss = 0.06 (66.4 examples/sec; 0.060 sec/batch; 98h:20m:22s remains)
INFO - root - 2019-11-04 02:13:42.828137: step 119530, total loss = 0.28, predict loss = 0.06 (66.9 examples/sec; 0.060 sec/batch; 97h:42m:22s remains)
INFO - root - 2019-11-04 02:13:43.477145: step 119540, total loss = 0.34, predict loss = 0.07 (74.7 examples/sec; 0.054 sec/batch; 87h:31m:30s remains)
INFO - root - 2019-11-04 02:13:44.086522: step 119550, total loss = 0.44, predict loss = 0.10 (75.7 examples/sec; 0.053 sec/batch; 86h:18m:23s remains)
INFO - root - 2019-11-04 02:13:44.679199: step 119560, total loss = 0.29, predict loss = 0.06 (66.0 examples/sec; 0.061 sec/batch; 98h:56m:14s remains)
INFO - root - 2019-11-04 02:13:45.289027: step 119570, total loss = 0.36, predict loss = 0.07 (69.4 examples/sec; 0.058 sec/batch; 94h:07m:31s remains)
INFO - root - 2019-11-04 02:13:45.901596: step 119580, total loss = 0.38, predict loss = 0.09 (69.2 examples/sec; 0.058 sec/batch; 94h:28m:42s remains)
INFO - root - 2019-11-04 02:13:46.534273: step 119590, total loss = 0.29, predict loss = 0.06 (85.4 examples/sec; 0.047 sec/batch; 76h:30m:04s remains)
INFO - root - 2019-11-04 02:13:47.124768: step 119600, total loss = 0.38, predict loss = 0.08 (81.8 examples/sec; 0.049 sec/batch; 79h:53m:07s remains)
INFO - root - 2019-11-04 02:13:47.727321: step 119610, total loss = 0.30, predict loss = 0.07 (83.3 examples/sec; 0.048 sec/batch; 78h:26m:21s remains)
INFO - root - 2019-11-04 02:13:48.333982: step 119620, total loss = 0.44, predict loss = 0.10 (71.2 examples/sec; 0.056 sec/batch; 91h:48m:59s remains)
INFO - root - 2019-11-04 02:13:48.981404: step 119630, total loss = 0.38, predict loss = 0.09 (62.8 examples/sec; 0.064 sec/batch; 104h:01m:42s remains)
INFO - root - 2019-11-04 02:13:49.627044: step 119640, total loss = 0.64, predict loss = 0.16 (78.5 examples/sec; 0.051 sec/batch; 83h:11m:01s remains)
INFO - root - 2019-11-04 02:13:50.241277: step 119650, total loss = 0.67, predict loss = 0.16 (76.0 examples/sec; 0.053 sec/batch; 86h:00m:22s remains)
INFO - root - 2019-11-04 02:13:50.851509: step 119660, total loss = 0.59, predict loss = 0.15 (73.9 examples/sec; 0.054 sec/batch; 88h:22m:10s remains)
INFO - root - 2019-11-04 02:13:51.485473: step 119670, total loss = 0.37, predict loss = 0.09 (70.7 examples/sec; 0.057 sec/batch; 92h:24m:31s remains)
INFO - root - 2019-11-04 02:13:52.095692: step 119680, total loss = 0.29, predict loss = 0.07 (72.9 examples/sec; 0.055 sec/batch; 89h:37m:50s remains)
INFO - root - 2019-11-04 02:13:52.679190: step 119690, total loss = 0.29, predict loss = 0.06 (78.9 examples/sec; 0.051 sec/batch; 82h:49m:26s remains)
INFO - root - 2019-11-04 02:13:53.260229: step 119700, total loss = 0.44, predict loss = 0.11 (80.0 examples/sec; 0.050 sec/batch; 81h:40m:06s remains)
INFO - root - 2019-11-04 02:13:53.867717: step 119710, total loss = 0.46, predict loss = 0.11 (75.6 examples/sec; 0.053 sec/batch; 86h:28m:24s remains)
INFO - root - 2019-11-04 02:13:54.504275: step 119720, total loss = 0.23, predict loss = 0.05 (74.9 examples/sec; 0.053 sec/batch; 87h:13m:48s remains)
INFO - root - 2019-11-04 02:13:55.109707: step 119730, total loss = 0.31, predict loss = 0.07 (71.8 examples/sec; 0.056 sec/batch; 91h:03m:29s remains)
INFO - root - 2019-11-04 02:13:55.836665: step 119740, total loss = 0.27, predict loss = 0.06 (56.8 examples/sec; 0.070 sec/batch; 115h:02m:28s remains)
INFO - root - 2019-11-04 02:13:56.554086: step 119750, total loss = 0.34, predict loss = 0.08 (60.7 examples/sec; 0.066 sec/batch; 107h:38m:06s remains)
INFO - root - 2019-11-04 02:13:57.246097: step 119760, total loss = 0.38, predict loss = 0.08 (57.6 examples/sec; 0.069 sec/batch; 113h:21m:09s remains)
INFO - root - 2019-11-04 02:13:57.907138: step 119770, total loss = 0.35, predict loss = 0.08 (66.8 examples/sec; 0.060 sec/batch; 97h:49m:27s remains)
INFO - root - 2019-11-04 02:13:58.566369: step 119780, total loss = 0.54, predict loss = 0.13 (73.3 examples/sec; 0.055 sec/batch; 89h:05m:22s remains)
INFO - root - 2019-11-04 02:13:59.208957: step 119790, total loss = 0.46, predict loss = 0.10 (72.7 examples/sec; 0.055 sec/batch; 89h:54m:50s remains)
INFO - root - 2019-11-04 02:13:59.825501: step 119800, total loss = 0.51, predict loss = 0.12 (71.9 examples/sec; 0.056 sec/batch; 90h:51m:31s remains)
INFO - root - 2019-11-04 02:14:00.438211: step 119810, total loss = 0.47, predict loss = 0.12 (87.6 examples/sec; 0.046 sec/batch; 74h:35m:48s remains)
INFO - root - 2019-11-04 02:14:01.100407: step 119820, total loss = 0.45, predict loss = 0.11 (74.0 examples/sec; 0.054 sec/batch; 88h:14m:11s remains)
INFO - root - 2019-11-04 02:14:01.761466: step 119830, total loss = 0.36, predict loss = 0.08 (61.9 examples/sec; 0.065 sec/batch; 105h:32m:41s remains)
INFO - root - 2019-11-04 02:14:02.429049: step 119840, total loss = 0.36, predict loss = 0.08 (66.2 examples/sec; 0.060 sec/batch; 98h:37m:49s remains)
INFO - root - 2019-11-04 02:14:03.105681: step 119850, total loss = 0.40, predict loss = 0.09 (62.5 examples/sec; 0.064 sec/batch; 104h:28m:24s remains)
INFO - root - 2019-11-04 02:14:03.746357: step 119860, total loss = 0.36, predict loss = 0.08 (77.6 examples/sec; 0.052 sec/batch; 84h:10m:23s remains)
INFO - root - 2019-11-04 02:14:04.366239: step 119870, total loss = 0.38, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 93h:58m:06s remains)
INFO - root - 2019-11-04 02:14:04.944720: step 119880, total loss = 0.47, predict loss = 0.11 (76.3 examples/sec; 0.052 sec/batch; 85h:38m:22s remains)
INFO - root - 2019-11-04 02:14:05.538779: step 119890, total loss = 0.37, predict loss = 0.08 (81.0 examples/sec; 0.049 sec/batch; 80h:38m:24s remains)
INFO - root - 2019-11-04 02:14:06.159528: step 119900, total loss = 0.47, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 91h:14m:05s remains)
INFO - root - 2019-11-04 02:14:06.818662: step 119910, total loss = 0.36, predict loss = 0.08 (67.3 examples/sec; 0.059 sec/batch; 97h:08m:12s remains)
INFO - root - 2019-11-04 02:14:07.472820: step 119920, total loss = 0.53, predict loss = 0.13 (66.1 examples/sec; 0.061 sec/batch; 98h:52m:40s remains)
INFO - root - 2019-11-04 02:14:08.143083: step 119930, total loss = 0.38, predict loss = 0.09 (60.7 examples/sec; 0.066 sec/batch; 107h:38m:40s remains)
INFO - root - 2019-11-04 02:14:08.769038: step 119940, total loss = 0.52, predict loss = 0.12 (75.7 examples/sec; 0.053 sec/batch; 86h:17m:23s remains)
INFO - root - 2019-11-04 02:14:09.384631: step 119950, total loss = 0.45, predict loss = 0.10 (67.7 examples/sec; 0.059 sec/batch; 96h:30m:23s remains)
INFO - root - 2019-11-04 02:14:10.028899: step 119960, total loss = 0.36, predict loss = 0.08 (64.0 examples/sec; 0.062 sec/batch; 102h:01m:43s remains)
INFO - root - 2019-11-04 02:14:10.717631: step 119970, total loss = 0.26, predict loss = 0.06 (56.3 examples/sec; 0.071 sec/batch; 115h:56m:45s remains)
INFO - root - 2019-11-04 02:14:11.360915: step 119980, total loss = 0.26, predict loss = 0.06 (77.2 examples/sec; 0.052 sec/batch; 84h:37m:47s remains)
INFO - root - 2019-11-04 02:14:12.009593: step 119990, total loss = 0.37, predict loss = 0.08 (72.6 examples/sec; 0.055 sec/batch; 90h:02m:23s remains)
INFO - root - 2019-11-04 02:14:12.629654: step 120000, total loss = 0.39, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 92h:12m:12s remains)
INFO - root - 2019-11-04 02:14:14.369083: step 120010, total loss = 0.36, predict loss = 0.08 (79.5 examples/sec; 0.050 sec/batch; 82h:13m:49s remains)
INFO - root - 2019-11-04 02:14:14.931049: step 120020, total loss = 0.44, predict loss = 0.11 (79.6 examples/sec; 0.050 sec/batch; 82h:02m:42s remains)
INFO - root - 2019-11-04 02:14:15.517929: step 120030, total loss = 0.38, predict loss = 0.08 (70.5 examples/sec; 0.057 sec/batch; 92h:41m:48s remains)
INFO - root - 2019-11-04 02:14:16.124603: step 120040, total loss = 0.60, predict loss = 0.14 (78.4 examples/sec; 0.051 sec/batch; 83h:20m:42s remains)
INFO - root - 2019-11-04 02:14:16.762409: step 120050, total loss = 0.48, predict loss = 0.11 (75.3 examples/sec; 0.053 sec/batch; 86h:48m:16s remains)
INFO - root - 2019-11-04 02:14:17.386176: step 120060, total loss = 0.64, predict loss = 0.15 (71.8 examples/sec; 0.056 sec/batch; 91h:01m:53s remains)
INFO - root - 2019-11-04 02:14:18.041494: step 120070, total loss = 0.87, predict loss = 0.21 (69.1 examples/sec; 0.058 sec/batch; 94h:29m:38s remains)
INFO - root - 2019-11-04 02:14:18.687793: step 120080, total loss = 0.62, predict loss = 0.15 (69.9 examples/sec; 0.057 sec/batch; 93h:24m:16s remains)
INFO - root - 2019-11-04 02:14:19.289600: step 120090, total loss = 0.70, predict loss = 0.17 (70.9 examples/sec; 0.056 sec/batch; 92h:09m:52s remains)
INFO - root - 2019-11-04 02:14:20.000112: step 120100, total loss = 0.65, predict loss = 0.14 (54.0 examples/sec; 0.074 sec/batch; 121h:05m:30s remains)
INFO - root - 2019-11-04 02:14:20.657051: step 120110, total loss = 0.69, predict loss = 0.16 (68.9 examples/sec; 0.058 sec/batch; 94h:45m:25s remains)
INFO - root - 2019-11-04 02:14:21.298832: step 120120, total loss = 0.59, predict loss = 0.13 (66.3 examples/sec; 0.060 sec/batch; 98h:33m:40s remains)
INFO - root - 2019-11-04 02:14:21.927463: step 120130, total loss = 0.61, predict loss = 0.14 (76.3 examples/sec; 0.052 sec/batch; 85h:40m:07s remains)
INFO - root - 2019-11-04 02:14:22.553173: step 120140, total loss = 0.50, predict loss = 0.11 (71.7 examples/sec; 0.056 sec/batch; 91h:06m:09s remains)
INFO - root - 2019-11-04 02:14:23.198543: step 120150, total loss = 0.59, predict loss = 0.14 (80.2 examples/sec; 0.050 sec/batch; 81h:26m:58s remains)
INFO - root - 2019-11-04 02:14:23.829224: step 120160, total loss = 0.49, predict loss = 0.11 (78.9 examples/sec; 0.051 sec/batch; 82h:50m:12s remains)
INFO - root - 2019-11-04 02:14:24.438423: step 120170, total loss = 0.44, predict loss = 0.09 (77.0 examples/sec; 0.052 sec/batch; 84h:49m:17s remains)
INFO - root - 2019-11-04 02:14:25.080025: step 120180, total loss = 0.47, predict loss = 0.11 (71.8 examples/sec; 0.056 sec/batch; 91h:02m:10s remains)
INFO - root - 2019-11-04 02:14:25.715284: step 120190, total loss = 0.46, predict loss = 0.10 (76.8 examples/sec; 0.052 sec/batch; 85h:02m:20s remains)
INFO - root - 2019-11-04 02:14:26.405426: step 120200, total loss = 0.50, predict loss = 0.11 (65.8 examples/sec; 0.061 sec/batch; 99h:16m:46s remains)
INFO - root - 2019-11-04 02:14:27.002095: step 120210, total loss = 0.54, predict loss = 0.12 (80.4 examples/sec; 0.050 sec/batch; 81h:15m:29s remains)
INFO - root - 2019-11-04 02:14:27.602036: step 120220, total loss = 0.48, predict loss = 0.11 (60.0 examples/sec; 0.067 sec/batch; 108h:52m:25s remains)
INFO - root - 2019-11-04 02:14:28.275932: step 120230, total loss = 0.45, predict loss = 0.10 (70.1 examples/sec; 0.057 sec/batch; 93h:12m:16s remains)
INFO - root - 2019-11-04 02:14:28.920572: step 120240, total loss = 0.50, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 89h:58m:52s remains)
INFO - root - 2019-11-04 02:14:29.555197: step 120250, total loss = 0.40, predict loss = 0.09 (64.8 examples/sec; 0.062 sec/batch; 100h:46m:35s remains)
INFO - root - 2019-11-04 02:14:30.170189: step 120260, total loss = 0.55, predict loss = 0.13 (71.1 examples/sec; 0.056 sec/batch; 91h:51m:14s remains)
INFO - root - 2019-11-04 02:14:30.777255: step 120270, total loss = 0.51, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 96h:00m:07s remains)
INFO - root - 2019-11-04 02:14:31.419185: step 120280, total loss = 0.52, predict loss = 0.12 (72.2 examples/sec; 0.055 sec/batch; 90h:31m:56s remains)
INFO - root - 2019-11-04 02:14:32.059051: step 120290, total loss = 0.55, predict loss = 0.12 (71.1 examples/sec; 0.056 sec/batch; 91h:55m:55s remains)
INFO - root - 2019-11-04 02:14:32.696300: step 120300, total loss = 0.57, predict loss = 0.13 (68.4 examples/sec; 0.058 sec/batch; 95h:32m:30s remains)
INFO - root - 2019-11-04 02:14:33.367274: step 120310, total loss = 0.36, predict loss = 0.08 (65.4 examples/sec; 0.061 sec/batch; 99h:49m:15s remains)
INFO - root - 2019-11-04 02:14:34.030957: step 120320, total loss = 0.39, predict loss = 0.09 (63.8 examples/sec; 0.063 sec/batch; 102h:28m:15s remains)
INFO - root - 2019-11-04 02:14:34.649088: step 120330, total loss = 0.36, predict loss = 0.08 (74.3 examples/sec; 0.054 sec/batch; 87h:58m:49s remains)
INFO - root - 2019-11-04 02:14:35.288782: step 120340, total loss = 0.28, predict loss = 0.06 (71.6 examples/sec; 0.056 sec/batch; 91h:16m:00s remains)
INFO - root - 2019-11-04 02:14:35.915285: step 120350, total loss = 0.38, predict loss = 0.09 (76.0 examples/sec; 0.053 sec/batch; 86h:00m:55s remains)
INFO - root - 2019-11-04 02:14:36.524909: step 120360, total loss = 0.46, predict loss = 0.11 (77.5 examples/sec; 0.052 sec/batch; 84h:14m:52s remains)
INFO - root - 2019-11-04 02:14:37.185290: step 120370, total loss = 0.35, predict loss = 0.08 (65.5 examples/sec; 0.061 sec/batch; 99h:47m:24s remains)
INFO - root - 2019-11-04 02:14:37.784498: step 120380, total loss = 0.54, predict loss = 0.12 (75.3 examples/sec; 0.053 sec/batch; 86h:48m:22s remains)
INFO - root - 2019-11-04 02:14:38.358156: step 120390, total loss = 0.42, predict loss = 0.09 (86.9 examples/sec; 0.046 sec/batch; 75h:09m:39s remains)
INFO - root - 2019-11-04 02:14:38.984545: step 120400, total loss = 0.40, predict loss = 0.09 (64.3 examples/sec; 0.062 sec/batch; 101h:33m:19s remains)
INFO - root - 2019-11-04 02:14:39.636844: step 120410, total loss = 0.43, predict loss = 0.10 (59.6 examples/sec; 0.067 sec/batch; 109h:38m:48s remains)
INFO - root - 2019-11-04 02:14:40.278636: step 120420, total loss = 0.47, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 98h:58m:50s remains)
INFO - root - 2019-11-04 02:14:40.936813: step 120430, total loss = 0.42, predict loss = 0.10 (62.8 examples/sec; 0.064 sec/batch; 104h:04m:46s remains)
INFO - root - 2019-11-04 02:14:41.580165: step 120440, total loss = 0.51, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 94h:07m:29s remains)
INFO - root - 2019-11-04 02:14:42.224265: step 120450, total loss = 0.49, predict loss = 0.11 (70.2 examples/sec; 0.057 sec/batch; 93h:04m:08s remains)
INFO - root - 2019-11-04 02:14:42.865893: step 120460, total loss = 0.42, predict loss = 0.10 (72.1 examples/sec; 0.055 sec/batch; 90h:35m:07s remains)
INFO - root - 2019-11-04 02:14:43.500292: step 120470, total loss = 0.52, predict loss = 0.12 (73.3 examples/sec; 0.055 sec/batch; 89h:10m:25s remains)
INFO - root - 2019-11-04 02:14:44.132673: step 120480, total loss = 0.57, predict loss = 0.14 (68.1 examples/sec; 0.059 sec/batch; 95h:52m:54s remains)
INFO - root - 2019-11-04 02:14:44.726242: step 120490, total loss = 0.67, predict loss = 0.16 (79.7 examples/sec; 0.050 sec/batch; 81h:58m:37s remains)
INFO - root - 2019-11-04 02:14:45.373065: step 120500, total loss = 0.64, predict loss = 0.15 (71.4 examples/sec; 0.056 sec/batch; 91h:28m:54s remains)
INFO - root - 2019-11-04 02:14:46.003220: step 120510, total loss = 0.63, predict loss = 0.15 (75.6 examples/sec; 0.053 sec/batch; 86h:25m:15s remains)
INFO - root - 2019-11-04 02:14:46.649658: step 120520, total loss = 0.49, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 94h:06m:04s remains)
INFO - root - 2019-11-04 02:14:47.303936: step 120530, total loss = 0.58, predict loss = 0.14 (66.4 examples/sec; 0.060 sec/batch; 98h:22m:24s remains)
INFO - root - 2019-11-04 02:14:47.933251: step 120540, total loss = 0.56, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 89h:19m:49s remains)
INFO - root - 2019-11-04 02:14:48.561694: step 120550, total loss = 0.60, predict loss = 0.14 (69.7 examples/sec; 0.057 sec/batch; 93h:47m:29s remains)
INFO - root - 2019-11-04 02:14:49.160376: step 120560, total loss = 0.53, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 97h:42m:12s remains)
INFO - root - 2019-11-04 02:14:49.781487: step 120570, total loss = 0.61, predict loss = 0.14 (74.7 examples/sec; 0.054 sec/batch; 87h:24m:01s remains)
INFO - root - 2019-11-04 02:14:50.435520: step 120580, total loss = 0.54, predict loss = 0.12 (64.5 examples/sec; 0.062 sec/batch; 101h:19m:19s remains)
INFO - root - 2019-11-04 02:14:51.086437: step 120590, total loss = 0.45, predict loss = 0.11 (61.7 examples/sec; 0.065 sec/batch; 105h:48m:31s remains)
INFO - root - 2019-11-04 02:14:51.692436: step 120600, total loss = 0.36, predict loss = 0.08 (71.7 examples/sec; 0.056 sec/batch; 91h:08m:35s remains)
INFO - root - 2019-11-04 02:14:52.307941: step 120610, total loss = 0.50, predict loss = 0.11 (72.6 examples/sec; 0.055 sec/batch; 89h:59m:50s remains)
INFO - root - 2019-11-04 02:14:52.908575: step 120620, total loss = 0.36, predict loss = 0.08 (65.8 examples/sec; 0.061 sec/batch; 99h:13m:44s remains)
INFO - root - 2019-11-04 02:14:53.531972: step 120630, total loss = 0.34, predict loss = 0.07 (63.5 examples/sec; 0.063 sec/batch; 102h:48m:08s remains)
INFO - root - 2019-11-04 02:14:54.166942: step 120640, total loss = 0.49, predict loss = 0.11 (63.3 examples/sec; 0.063 sec/batch; 103h:15m:40s remains)
INFO - root - 2019-11-04 02:14:54.791204: step 120650, total loss = 0.46, predict loss = 0.10 (71.7 examples/sec; 0.056 sec/batch; 91h:03m:57s remains)
INFO - root - 2019-11-04 02:14:55.421954: step 120660, total loss = 0.42, predict loss = 0.10 (72.1 examples/sec; 0.055 sec/batch; 90h:32m:40s remains)
INFO - root - 2019-11-04 02:14:56.050590: step 120670, total loss = 0.38, predict loss = 0.08 (69.2 examples/sec; 0.058 sec/batch; 94h:23m:04s remains)
INFO - root - 2019-11-04 02:14:56.691709: step 120680, total loss = 0.58, predict loss = 0.13 (60.6 examples/sec; 0.066 sec/batch; 107h:49m:38s remains)
INFO - root - 2019-11-04 02:14:57.303009: step 120690, total loss = 0.44, predict loss = 0.10 (68.0 examples/sec; 0.059 sec/batch; 96h:01m:41s remains)
INFO - root - 2019-11-04 02:14:57.909998: step 120700, total loss = 0.52, predict loss = 0.13 (71.9 examples/sec; 0.056 sec/batch; 90h:48m:56s remains)
INFO - root - 2019-11-04 02:14:58.516938: step 120710, total loss = 0.65, predict loss = 0.15 (73.8 examples/sec; 0.054 sec/batch; 88h:34m:07s remains)
INFO - root - 2019-11-04 02:14:59.139397: step 120720, total loss = 0.53, predict loss = 0.13 (77.3 examples/sec; 0.052 sec/batch; 84h:30m:11s remains)
INFO - root - 2019-11-04 02:14:59.809181: step 120730, total loss = 0.57, predict loss = 0.14 (72.1 examples/sec; 0.055 sec/batch; 90h:36m:13s remains)
INFO - root - 2019-11-04 02:15:00.452789: step 120740, total loss = 0.55, predict loss = 0.13 (77.9 examples/sec; 0.051 sec/batch; 83h:52m:10s remains)
INFO - root - 2019-11-04 02:15:01.148574: step 120750, total loss = 0.50, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 97h:02m:31s remains)
INFO - root - 2019-11-04 02:15:01.795775: step 120760, total loss = 0.52, predict loss = 0.13 (67.7 examples/sec; 0.059 sec/batch; 96h:26m:41s remains)
INFO - root - 2019-11-04 02:15:02.444185: step 120770, total loss = 0.64, predict loss = 0.16 (68.8 examples/sec; 0.058 sec/batch; 95h:00m:48s remains)
INFO - root - 2019-11-04 02:15:03.087977: step 120780, total loss = 0.52, predict loss = 0.12 (75.6 examples/sec; 0.053 sec/batch; 86h:23m:24s remains)
INFO - root - 2019-11-04 02:15:03.752373: step 120790, total loss = 0.45, predict loss = 0.10 (66.7 examples/sec; 0.060 sec/batch; 97h:59m:51s remains)
INFO - root - 2019-11-04 02:15:04.385497: step 120800, total loss = 0.64, predict loss = 0.15 (62.0 examples/sec; 0.065 sec/batch; 105h:25m:11s remains)
INFO - root - 2019-11-04 02:15:04.993695: step 120810, total loss = 0.49, predict loss = 0.11 (66.7 examples/sec; 0.060 sec/batch; 97h:56m:25s remains)
INFO - root - 2019-11-04 02:15:05.597321: step 120820, total loss = 0.44, predict loss = 0.10 (79.5 examples/sec; 0.050 sec/batch; 82h:10m:52s remains)
INFO - root - 2019-11-04 02:15:06.229041: step 120830, total loss = 0.56, predict loss = 0.12 (78.3 examples/sec; 0.051 sec/batch; 83h:23m:38s remains)
INFO - root - 2019-11-04 02:15:06.859996: step 120840, total loss = 0.57, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 95h:56m:31s remains)
INFO - root - 2019-11-04 02:15:07.480370: step 120850, total loss = 0.51, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 90h:57m:40s remains)
INFO - root - 2019-11-04 02:15:08.113039: step 120860, total loss = 0.48, predict loss = 0.11 (75.8 examples/sec; 0.053 sec/batch; 86h:10m:14s remains)
INFO - root - 2019-11-04 02:15:08.780694: step 120870, total loss = 0.47, predict loss = 0.11 (68.6 examples/sec; 0.058 sec/batch; 95h:11m:19s remains)
INFO - root - 2019-11-04 02:15:09.448021: step 120880, total loss = 0.35, predict loss = 0.08 (68.5 examples/sec; 0.058 sec/batch; 95h:19m:01s remains)
INFO - root - 2019-11-04 02:15:10.129227: step 120890, total loss = 0.42, predict loss = 0.10 (69.1 examples/sec; 0.058 sec/batch; 94h:30m:34s remains)
INFO - root - 2019-11-04 02:15:10.761257: step 120900, total loss = 0.47, predict loss = 0.11 (79.2 examples/sec; 0.051 sec/batch; 82h:30m:23s remains)
INFO - root - 2019-11-04 02:15:11.379728: step 120910, total loss = 0.50, predict loss = 0.12 (76.0 examples/sec; 0.053 sec/batch; 85h:54m:47s remains)
INFO - root - 2019-11-04 02:15:11.994462: step 120920, total loss = 0.42, predict loss = 0.10 (85.5 examples/sec; 0.047 sec/batch; 76h:25m:52s remains)
INFO - root - 2019-11-04 02:15:12.620203: step 120930, total loss = 0.45, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:22m:36s remains)
INFO - root - 2019-11-04 02:15:13.310899: step 120940, total loss = 0.53, predict loss = 0.12 (59.8 examples/sec; 0.067 sec/batch; 109h:18m:14s remains)
INFO - root - 2019-11-04 02:15:13.949281: step 120950, total loss = 0.40, predict loss = 0.09 (79.5 examples/sec; 0.050 sec/batch; 82h:11m:08s remains)
INFO - root - 2019-11-04 02:15:14.598360: step 120960, total loss = 0.41, predict loss = 0.09 (64.5 examples/sec; 0.062 sec/batch; 101h:17m:51s remains)
INFO - root - 2019-11-04 02:15:15.324074: step 120970, total loss = 0.45, predict loss = 0.10 (63.0 examples/sec; 0.063 sec/batch; 103h:37m:03s remains)
INFO - root - 2019-11-04 02:15:15.984310: step 120980, total loss = 0.48, predict loss = 0.11 (61.8 examples/sec; 0.065 sec/batch; 105h:44m:08s remains)
INFO - root - 2019-11-04 02:15:16.616786: step 120990, total loss = 0.52, predict loss = 0.11 (74.1 examples/sec; 0.054 sec/batch; 88h:07m:21s remains)
INFO - root - 2019-11-04 02:15:17.232816: step 121000, total loss = 0.53, predict loss = 0.12 (75.1 examples/sec; 0.053 sec/batch; 86h:59m:34s remains)
INFO - root - 2019-11-04 02:15:17.858688: step 121010, total loss = 0.50, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 97h:23m:15s remains)
INFO - root - 2019-11-04 02:15:18.498728: step 121020, total loss = 0.55, predict loss = 0.13 (62.3 examples/sec; 0.064 sec/batch; 104h:51m:12s remains)
INFO - root - 2019-11-04 02:15:19.152725: step 121030, total loss = 0.52, predict loss = 0.12 (75.1 examples/sec; 0.053 sec/batch; 86h:56m:04s remains)
INFO - root - 2019-11-04 02:15:19.763166: step 121040, total loss = 0.57, predict loss = 0.15 (75.3 examples/sec; 0.053 sec/batch; 86h:46m:12s remains)
INFO - root - 2019-11-04 02:15:20.379997: step 121050, total loss = 0.52, predict loss = 0.12 (69.2 examples/sec; 0.058 sec/batch; 94h:23m:40s remains)
INFO - root - 2019-11-04 02:15:20.957081: step 121060, total loss = 0.53, predict loss = 0.12 (82.3 examples/sec; 0.049 sec/batch; 79h:20m:05s remains)
INFO - root - 2019-11-04 02:15:21.608771: step 121070, total loss = 0.52, predict loss = 0.13 (58.4 examples/sec; 0.068 sec/batch; 111h:46m:02s remains)
INFO - root - 2019-11-04 02:15:22.231114: step 121080, total loss = 0.55, predict loss = 0.13 (79.7 examples/sec; 0.050 sec/batch; 82h:00m:01s remains)
INFO - root - 2019-11-04 02:15:22.840551: step 121090, total loss = 0.59, predict loss = 0.14 (75.8 examples/sec; 0.053 sec/batch; 86h:13m:02s remains)
INFO - root - 2019-11-04 02:15:23.433129: step 121100, total loss = 0.61, predict loss = 0.15 (77.7 examples/sec; 0.051 sec/batch; 84h:01m:45s remains)
INFO - root - 2019-11-04 02:15:24.039413: step 121110, total loss = 0.59, predict loss = 0.14 (64.9 examples/sec; 0.062 sec/batch; 100h:42m:53s remains)
INFO - root - 2019-11-04 02:15:24.725940: step 121120, total loss = 0.67, predict loss = 0.16 (59.2 examples/sec; 0.068 sec/batch; 110h:17m:55s remains)
INFO - root - 2019-11-04 02:15:25.421901: step 121130, total loss = 0.55, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 92h:55m:55s remains)
INFO - root - 2019-11-04 02:15:26.078895: step 121140, total loss = 0.52, predict loss = 0.13 (67.1 examples/sec; 0.060 sec/batch; 97h:17m:57s remains)
INFO - root - 2019-11-04 02:15:26.711869: step 121150, total loss = 0.57, predict loss = 0.14 (68.8 examples/sec; 0.058 sec/batch; 94h:57m:18s remains)
INFO - root - 2019-11-04 02:15:27.324540: step 121160, total loss = 0.63, predict loss = 0.14 (77.3 examples/sec; 0.052 sec/batch; 84h:33m:20s remains)
INFO - root - 2019-11-04 02:15:27.922425: step 121170, total loss = 0.71, predict loss = 0.17 (78.6 examples/sec; 0.051 sec/batch; 83h:08m:25s remains)
INFO - root - 2019-11-04 02:15:28.543133: step 121180, total loss = 0.52, predict loss = 0.12 (78.2 examples/sec; 0.051 sec/batch; 83h:32m:15s remains)
INFO - root - 2019-11-04 02:15:29.183094: step 121190, total loss = 0.77, predict loss = 0.19 (68.7 examples/sec; 0.058 sec/batch; 95h:07m:49s remains)
INFO - root - 2019-11-04 02:15:29.828507: step 121200, total loss = 0.47, predict loss = 0.10 (71.3 examples/sec; 0.056 sec/batch; 91h:38m:30s remains)
INFO - root - 2019-11-04 02:15:30.455026: step 121210, total loss = 0.55, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 100h:23m:56s remains)
INFO - root - 2019-11-04 02:15:31.056979: step 121220, total loss = 0.72, predict loss = 0.18 (68.6 examples/sec; 0.058 sec/batch; 95h:12m:42s remains)
INFO - root - 2019-11-04 02:15:31.681809: step 121230, total loss = 0.50, predict loss = 0.12 (75.0 examples/sec; 0.053 sec/batch; 87h:05m:59s remains)
INFO - root - 2019-11-04 02:15:32.298062: step 121240, total loss = 0.38, predict loss = 0.08 (79.0 examples/sec; 0.051 sec/batch; 82h:39m:01s remains)
INFO - root - 2019-11-04 02:15:32.916113: step 121250, total loss = 0.42, predict loss = 0.10 (75.2 examples/sec; 0.053 sec/batch; 86h:48m:14s remains)
INFO - root - 2019-11-04 02:15:33.552537: step 121260, total loss = 0.45, predict loss = 0.10 (63.8 examples/sec; 0.063 sec/batch; 102h:25m:14s remains)
INFO - root - 2019-11-04 02:15:34.182866: step 121270, total loss = 0.48, predict loss = 0.11 (71.5 examples/sec; 0.056 sec/batch; 91h:18m:18s remains)
INFO - root - 2019-11-04 02:15:34.828028: step 121280, total loss = 0.56, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 89h:55m:26s remains)
INFO - root - 2019-11-04 02:15:35.472627: step 121290, total loss = 0.50, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 85h:09m:40s remains)
INFO - root - 2019-11-04 02:15:36.146626: step 121300, total loss = 0.52, predict loss = 0.12 (68.0 examples/sec; 0.059 sec/batch; 96h:05m:37s remains)
INFO - root - 2019-11-04 02:15:36.771229: step 121310, total loss = 0.47, predict loss = 0.11 (76.2 examples/sec; 0.052 sec/batch; 85h:42m:14s remains)
INFO - root - 2019-11-04 02:15:37.442717: step 121320, total loss = 0.36, predict loss = 0.07 (65.1 examples/sec; 0.061 sec/batch; 100h:16m:15s remains)
INFO - root - 2019-11-04 02:15:38.093555: step 121330, total loss = 0.51, predict loss = 0.11 (74.0 examples/sec; 0.054 sec/batch; 88h:14m:54s remains)
INFO - root - 2019-11-04 02:15:38.750470: step 121340, total loss = 0.49, predict loss = 0.11 (71.1 examples/sec; 0.056 sec/batch; 91h:50m:16s remains)
INFO - root - 2019-11-04 02:15:39.259572: step 121350, total loss = 0.54, predict loss = 0.12 (84.5 examples/sec; 0.047 sec/batch; 77h:18m:10s remains)
INFO - root - 2019-11-04 02:15:39.711720: step 121360, total loss = 0.37, predict loss = 0.08 (96.3 examples/sec; 0.042 sec/batch; 67h:50m:27s remains)
INFO - root - 2019-11-04 02:15:40.769138: step 121370, total loss = 0.36, predict loss = 0.07 (72.1 examples/sec; 0.056 sec/batch; 90h:39m:11s remains)
INFO - root - 2019-11-04 02:15:41.341520: step 121380, total loss = 0.26, predict loss = 0.06 (72.9 examples/sec; 0.055 sec/batch; 89h:37m:51s remains)
INFO - root - 2019-11-04 02:15:41.938477: step 121390, total loss = 0.45, predict loss = 0.11 (71.4 examples/sec; 0.056 sec/batch; 91h:29m:40s remains)
INFO - root - 2019-11-04 02:15:42.531127: step 121400, total loss = 0.44, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 96h:26m:54s remains)
INFO - root - 2019-11-04 02:15:43.208412: step 121410, total loss = 0.48, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 98h:26m:14s remains)
INFO - root - 2019-11-04 02:15:43.923483: step 121420, total loss = 0.46, predict loss = 0.10 (75.8 examples/sec; 0.053 sec/batch; 86h:10m:03s remains)
INFO - root - 2019-11-04 02:15:44.536587: step 121430, total loss = 0.48, predict loss = 0.11 (71.0 examples/sec; 0.056 sec/batch; 91h:59m:16s remains)
INFO - root - 2019-11-04 02:15:45.129077: step 121440, total loss = 0.49, predict loss = 0.12 (78.7 examples/sec; 0.051 sec/batch; 82h:58m:15s remains)
INFO - root - 2019-11-04 02:15:45.783456: step 121450, total loss = 0.41, predict loss = 0.09 (66.2 examples/sec; 0.060 sec/batch; 98h:42m:47s remains)
INFO - root - 2019-11-04 02:15:46.431195: step 121460, total loss = 0.49, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 86h:32m:04s remains)
INFO - root - 2019-11-04 02:15:47.040001: step 121470, total loss = 0.52, predict loss = 0.12 (71.1 examples/sec; 0.056 sec/batch; 91h:48m:11s remains)
INFO - root - 2019-11-04 02:15:47.662021: step 121480, total loss = 0.58, predict loss = 0.13 (77.0 examples/sec; 0.052 sec/batch; 84h:47m:05s remains)
INFO - root - 2019-11-04 02:15:48.286162: step 121490, total loss = 0.44, predict loss = 0.10 (80.8 examples/sec; 0.050 sec/batch; 80h:51m:25s remains)
INFO - root - 2019-11-04 02:15:48.883685: step 121500, total loss = 0.46, predict loss = 0.10 (76.0 examples/sec; 0.053 sec/batch; 85h:57m:57s remains)
INFO - root - 2019-11-04 02:15:49.529555: step 121510, total loss = 0.45, predict loss = 0.11 (65.3 examples/sec; 0.061 sec/batch; 100h:03m:39s remains)
INFO - root - 2019-11-04 02:15:50.172700: step 121520, total loss = 0.46, predict loss = 0.10 (74.8 examples/sec; 0.053 sec/batch; 87h:17m:28s remains)
INFO - root - 2019-11-04 02:15:50.814995: step 121530, total loss = 0.47, predict loss = 0.11 (73.7 examples/sec; 0.054 sec/batch; 88h:39m:06s remains)
INFO - root - 2019-11-04 02:15:51.457066: step 121540, total loss = 0.43, predict loss = 0.09 (57.4 examples/sec; 0.070 sec/batch; 113h:49m:54s remains)
INFO - root - 2019-11-04 02:15:52.121886: step 121550, total loss = 0.41, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 93h:55m:32s remains)
INFO - root - 2019-11-04 02:15:52.791978: step 121560, total loss = 0.47, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:20m:57s remains)
INFO - root - 2019-11-04 02:15:53.447974: step 121570, total loss = 0.44, predict loss = 0.10 (68.2 examples/sec; 0.059 sec/batch; 95h:42m:58s remains)
INFO - root - 2019-11-04 02:15:54.055578: step 121580, total loss = 0.36, predict loss = 0.08 (68.6 examples/sec; 0.058 sec/batch; 95h:09m:15s remains)
INFO - root - 2019-11-04 02:15:54.691309: step 121590, total loss = 0.29, predict loss = 0.07 (72.9 examples/sec; 0.055 sec/batch; 89h:38m:05s remains)
INFO - root - 2019-11-04 02:15:55.300273: step 121600, total loss = 0.40, predict loss = 0.09 (72.4 examples/sec; 0.055 sec/batch; 90h:09m:36s remains)
INFO - root - 2019-11-04 02:15:55.949770: step 121610, total loss = 0.41, predict loss = 0.09 (70.8 examples/sec; 0.056 sec/batch; 92h:12m:37s remains)
INFO - root - 2019-11-04 02:15:56.560387: step 121620, total loss = 0.47, predict loss = 0.10 (72.6 examples/sec; 0.055 sec/batch; 90h:01m:19s remains)
INFO - root - 2019-11-04 02:15:57.184819: step 121630, total loss = 0.47, predict loss = 0.10 (66.3 examples/sec; 0.060 sec/batch; 98h:28m:57s remains)
INFO - root - 2019-11-04 02:15:57.802007: step 121640, total loss = 0.44, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 93h:44m:45s remains)
INFO - root - 2019-11-04 02:15:58.398690: step 121650, total loss = 0.49, predict loss = 0.10 (86.0 examples/sec; 0.046 sec/batch; 75h:54m:20s remains)
INFO - root - 2019-11-04 02:15:59.013627: step 121660, total loss = 0.57, predict loss = 0.13 (71.2 examples/sec; 0.056 sec/batch; 91h:46m:32s remains)
INFO - root - 2019-11-04 02:15:59.646530: step 121670, total loss = 0.54, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 91h:36m:06s remains)
INFO - root - 2019-11-04 02:16:00.290648: step 121680, total loss = 0.67, predict loss = 0.15 (66.4 examples/sec; 0.060 sec/batch; 98h:18m:22s remains)
INFO - root - 2019-11-04 02:16:00.903750: step 121690, total loss = 0.52, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 91h:36m:30s remains)
INFO - root - 2019-11-04 02:16:01.519712: step 121700, total loss = 0.53, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 91h:26m:40s remains)
INFO - root - 2019-11-04 02:16:02.187675: step 121710, total loss = 0.56, predict loss = 0.12 (64.8 examples/sec; 0.062 sec/batch; 100h:49m:59s remains)
INFO - root - 2019-11-04 02:16:02.827478: step 121720, total loss = 0.45, predict loss = 0.10 (67.2 examples/sec; 0.059 sec/batch; 97h:08m:11s remains)
INFO - root - 2019-11-04 02:16:03.433564: step 121730, total loss = 0.51, predict loss = 0.12 (77.4 examples/sec; 0.052 sec/batch; 84h:22m:44s remains)
INFO - root - 2019-11-04 02:16:04.076907: step 121740, total loss = 0.53, predict loss = 0.12 (69.8 examples/sec; 0.057 sec/batch; 93h:31m:48s remains)
INFO - root - 2019-11-04 02:16:04.682956: step 121750, total loss = 0.35, predict loss = 0.08 (74.7 examples/sec; 0.054 sec/batch; 87h:23m:32s remains)
INFO - root - 2019-11-04 02:16:05.283950: step 121760, total loss = 0.47, predict loss = 0.11 (71.5 examples/sec; 0.056 sec/batch; 91h:17m:16s remains)
INFO - root - 2019-11-04 02:16:05.911755: step 121770, total loss = 0.54, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 92h:35m:19s remains)
INFO - root - 2019-11-04 02:16:06.571691: step 121780, total loss = 0.52, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 91h:34m:56s remains)
INFO - root - 2019-11-04 02:16:07.161954: step 121790, total loss = 0.64, predict loss = 0.16 (80.1 examples/sec; 0.050 sec/batch; 81h:33m:59s remains)
INFO - root - 2019-11-04 02:16:07.800002: step 121800, total loss = 0.53, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 97h:06m:20s remains)
INFO - root - 2019-11-04 02:16:08.460490: step 121810, total loss = 0.45, predict loss = 0.11 (64.4 examples/sec; 0.062 sec/batch; 101h:26m:51s remains)
INFO - root - 2019-11-04 02:16:09.103304: step 121820, total loss = 0.37, predict loss = 0.09 (67.5 examples/sec; 0.059 sec/batch; 96h:45m:27s remains)
INFO - root - 2019-11-04 02:16:09.737846: step 121830, total loss = 0.44, predict loss = 0.11 (67.8 examples/sec; 0.059 sec/batch; 96h:23m:32s remains)
INFO - root - 2019-11-04 02:16:10.332975: step 121840, total loss = 0.58, predict loss = 0.14 (71.2 examples/sec; 0.056 sec/batch; 91h:46m:17s remains)
INFO - root - 2019-11-04 02:16:10.952983: step 121850, total loss = 0.39, predict loss = 0.09 (78.3 examples/sec; 0.051 sec/batch; 83h:24m:50s remains)
INFO - root - 2019-11-04 02:16:11.565389: step 121860, total loss = 0.53, predict loss = 0.13 (80.5 examples/sec; 0.050 sec/batch; 81h:07m:43s remains)
INFO - root - 2019-11-04 02:16:12.206798: step 121870, total loss = 0.44, predict loss = 0.10 (75.5 examples/sec; 0.053 sec/batch; 86h:32m:41s remains)
INFO - root - 2019-11-04 02:16:12.877668: step 121880, total loss = 0.62, predict loss = 0.16 (70.9 examples/sec; 0.056 sec/batch; 92h:08m:13s remains)
INFO - root - 2019-11-04 02:16:13.485912: step 121890, total loss = 0.41, predict loss = 0.10 (69.1 examples/sec; 0.058 sec/batch; 94h:31m:10s remains)
INFO - root - 2019-11-04 02:16:14.182013: step 121900, total loss = 0.50, predict loss = 0.12 (62.5 examples/sec; 0.064 sec/batch; 104h:26m:48s remains)
INFO - root - 2019-11-04 02:16:14.816590: step 121910, total loss = 0.47, predict loss = 0.11 (80.0 examples/sec; 0.050 sec/batch; 81h:40m:28s remains)
INFO - root - 2019-11-04 02:16:15.412570: step 121920, total loss = 0.49, predict loss = 0.12 (82.0 examples/sec; 0.049 sec/batch; 79h:41m:29s remains)
INFO - root - 2019-11-04 02:16:16.043564: step 121930, total loss = 0.45, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 89h:26m:22s remains)
INFO - root - 2019-11-04 02:16:16.694764: step 121940, total loss = 0.37, predict loss = 0.09 (64.5 examples/sec; 0.062 sec/batch; 101h:14m:33s remains)
INFO - root - 2019-11-04 02:16:17.349975: step 121950, total loss = 0.27, predict loss = 0.06 (69.2 examples/sec; 0.058 sec/batch; 94h:20m:54s remains)
INFO - root - 2019-11-04 02:16:17.996592: step 121960, total loss = 0.28, predict loss = 0.06 (71.0 examples/sec; 0.056 sec/batch; 92h:00m:05s remains)
INFO - root - 2019-11-04 02:16:18.680558: step 121970, total loss = 0.43, predict loss = 0.10 (60.4 examples/sec; 0.066 sec/batch; 108h:02m:40s remains)
INFO - root - 2019-11-04 02:16:19.349209: step 121980, total loss = 0.30, predict loss = 0.06 (60.2 examples/sec; 0.066 sec/batch; 108h:27m:34s remains)
INFO - root - 2019-11-04 02:16:20.027793: step 121990, total loss = 0.33, predict loss = 0.07 (62.3 examples/sec; 0.064 sec/batch; 104h:45m:30s remains)
INFO - root - 2019-11-04 02:16:20.669646: step 122000, total loss = 0.21, predict loss = 0.03 (72.1 examples/sec; 0.055 sec/batch; 90h:33m:51s remains)
INFO - root - 2019-11-04 02:16:21.305601: step 122010, total loss = 0.38, predict loss = 0.08 (65.6 examples/sec; 0.061 sec/batch; 99h:31m:20s remains)
INFO - root - 2019-11-04 02:16:21.948556: step 122020, total loss = 0.37, predict loss = 0.08 (62.0 examples/sec; 0.064 sec/batch; 105h:15m:52s remains)
INFO - root - 2019-11-04 02:16:22.629026: step 122030, total loss = 0.40, predict loss = 0.09 (68.4 examples/sec; 0.058 sec/batch; 95h:28m:45s remains)
INFO - root - 2019-11-04 02:16:23.239390: step 122040, total loss = 0.46, predict loss = 0.11 (68.7 examples/sec; 0.058 sec/batch; 95h:04m:27s remains)
INFO - root - 2019-11-04 02:16:23.829508: step 122050, total loss = 0.48, predict loss = 0.11 (66.3 examples/sec; 0.060 sec/batch; 98h:26m:30s remains)
INFO - root - 2019-11-04 02:16:24.430177: step 122060, total loss = 0.59, predict loss = 0.14 (75.1 examples/sec; 0.053 sec/batch; 86h:57m:45s remains)
INFO - root - 2019-11-04 02:16:25.057155: step 122070, total loss = 0.48, predict loss = 0.11 (83.0 examples/sec; 0.048 sec/batch; 78h:38m:25s remains)
INFO - root - 2019-11-04 02:16:25.699054: step 122080, total loss = 0.45, predict loss = 0.11 (73.5 examples/sec; 0.054 sec/batch; 88h:47m:53s remains)
INFO - root - 2019-11-04 02:16:26.329831: step 122090, total loss = 0.45, predict loss = 0.11 (72.2 examples/sec; 0.055 sec/batch; 90h:27m:47s remains)
INFO - root - 2019-11-04 02:16:26.981112: step 122100, total loss = 0.45, predict loss = 0.10 (66.6 examples/sec; 0.060 sec/batch; 98h:06m:09s remains)
INFO - root - 2019-11-04 02:16:27.649742: step 122110, total loss = 0.52, predict loss = 0.12 (68.2 examples/sec; 0.059 sec/batch; 95h:46m:54s remains)
INFO - root - 2019-11-04 02:16:28.300513: step 122120, total loss = 0.45, predict loss = 0.10 (66.9 examples/sec; 0.060 sec/batch; 97h:34m:52s remains)
INFO - root - 2019-11-04 02:16:28.920468: step 122130, total loss = 0.43, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 95h:09m:36s remains)
INFO - root - 2019-11-04 02:16:29.557602: step 122140, total loss = 0.41, predict loss = 0.10 (69.6 examples/sec; 0.057 sec/batch; 93h:49m:28s remains)
INFO - root - 2019-11-04 02:16:30.182944: step 122150, total loss = 0.62, predict loss = 0.15 (75.6 examples/sec; 0.053 sec/batch; 86h:24m:59s remains)
INFO - root - 2019-11-04 02:16:30.852312: step 122160, total loss = 0.51, predict loss = 0.12 (75.4 examples/sec; 0.053 sec/batch; 86h:37m:01s remains)
INFO - root - 2019-11-04 02:16:31.494384: step 122170, total loss = 0.63, predict loss = 0.16 (65.0 examples/sec; 0.062 sec/batch; 100h:31m:27s remains)
INFO - root - 2019-11-04 02:16:32.106366: step 122180, total loss = 0.28, predict loss = 0.06 (71.9 examples/sec; 0.056 sec/batch; 90h:47m:01s remains)
INFO - root - 2019-11-04 02:16:32.730306: step 122190, total loss = 0.51, predict loss = 0.12 (69.1 examples/sec; 0.058 sec/batch; 94h:29m:12s remains)
INFO - root - 2019-11-04 02:16:33.387825: step 122200, total loss = 0.34, predict loss = 0.08 (67.1 examples/sec; 0.060 sec/batch; 97h:21m:28s remains)
INFO - root - 2019-11-04 02:16:34.018822: step 122210, total loss = 0.27, predict loss = 0.06 (67.7 examples/sec; 0.059 sec/batch; 96h:30m:56s remains)
INFO - root - 2019-11-04 02:16:34.671853: step 122220, total loss = 0.32, predict loss = 0.07 (62.9 examples/sec; 0.064 sec/batch; 103h:49m:42s remains)
INFO - root - 2019-11-04 02:16:35.302821: step 122230, total loss = 0.30, predict loss = 0.07 (64.0 examples/sec; 0.063 sec/batch; 102h:06m:44s remains)
INFO - root - 2019-11-04 02:16:35.957574: step 122240, total loss = 0.27, predict loss = 0.06 (61.9 examples/sec; 0.065 sec/batch; 105h:33m:10s remains)
INFO - root - 2019-11-04 02:16:36.599433: step 122250, total loss = 0.40, predict loss = 0.09 (78.9 examples/sec; 0.051 sec/batch; 82h:48m:57s remains)
INFO - root - 2019-11-04 02:16:37.274932: step 122260, total loss = 0.34, predict loss = 0.07 (71.1 examples/sec; 0.056 sec/batch; 91h:49m:24s remains)
INFO - root - 2019-11-04 02:16:37.896918: step 122270, total loss = 0.25, predict loss = 0.05 (79.5 examples/sec; 0.050 sec/batch; 82h:10m:55s remains)
INFO - root - 2019-11-04 02:16:38.499882: step 122280, total loss = 0.48, predict loss = 0.10 (68.8 examples/sec; 0.058 sec/batch; 94h:58m:35s remains)
INFO - root - 2019-11-04 02:16:39.136088: step 122290, total loss = 0.44, predict loss = 0.10 (80.6 examples/sec; 0.050 sec/batch; 81h:02m:25s remains)
INFO - root - 2019-11-04 02:16:39.829228: step 122300, total loss = 0.45, predict loss = 0.11 (65.4 examples/sec; 0.061 sec/batch; 99h:47m:01s remains)
INFO - root - 2019-11-04 02:16:40.495111: step 122310, total loss = 0.64, predict loss = 0.16 (68.5 examples/sec; 0.058 sec/batch; 95h:16m:37s remains)
INFO - root - 2019-11-04 02:16:41.151708: step 122320, total loss = 0.39, predict loss = 0.09 (75.0 examples/sec; 0.053 sec/batch; 87h:05m:37s remains)
INFO - root - 2019-11-04 02:16:41.762717: step 122330, total loss = 0.48, predict loss = 0.12 (79.6 examples/sec; 0.050 sec/batch; 82h:05m:08s remains)
INFO - root - 2019-11-04 02:16:42.412779: step 122340, total loss = 0.35, predict loss = 0.08 (75.4 examples/sec; 0.053 sec/batch; 86h:37m:27s remains)
INFO - root - 2019-11-04 02:16:43.019336: step 122350, total loss = 0.38, predict loss = 0.08 (69.9 examples/sec; 0.057 sec/batch; 93h:25m:52s remains)
INFO - root - 2019-11-04 02:16:43.631847: step 122360, total loss = 0.65, predict loss = 0.15 (74.5 examples/sec; 0.054 sec/batch; 87h:36m:35s remains)
INFO - root - 2019-11-04 02:16:44.240883: step 122370, total loss = 0.53, predict loss = 0.13 (85.9 examples/sec; 0.047 sec/batch; 76h:03m:32s remains)
INFO - root - 2019-11-04 02:16:44.879102: step 122380, total loss = 0.63, predict loss = 0.15 (68.6 examples/sec; 0.058 sec/batch; 95h:13m:21s remains)
INFO - root - 2019-11-04 02:16:45.565044: step 122390, total loss = 0.62, predict loss = 0.16 (60.7 examples/sec; 0.066 sec/batch; 107h:39m:13s remains)
INFO - root - 2019-11-04 02:16:46.203202: step 122400, total loss = 0.33, predict loss = 0.08 (65.3 examples/sec; 0.061 sec/batch; 99h:58m:31s remains)
INFO - root - 2019-11-04 02:16:46.839903: step 122410, total loss = 0.47, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 98h:58m:05s remains)
INFO - root - 2019-11-04 02:16:47.505359: step 122420, total loss = 0.31, predict loss = 0.07 (69.4 examples/sec; 0.058 sec/batch; 94h:05m:40s remains)
INFO - root - 2019-11-04 02:16:48.117338: step 122430, total loss = 0.26, predict loss = 0.05 (76.7 examples/sec; 0.052 sec/batch; 85h:09m:07s remains)
INFO - root - 2019-11-04 02:16:48.741321: step 122440, total loss = 0.25, predict loss = 0.05 (76.9 examples/sec; 0.052 sec/batch; 84h:55m:34s remains)
INFO - root - 2019-11-04 02:16:49.365381: step 122450, total loss = 0.29, predict loss = 0.06 (71.2 examples/sec; 0.056 sec/batch; 91h:43m:05s remains)
INFO - root - 2019-11-04 02:16:49.982511: step 122460, total loss = 0.18, predict loss = 0.04 (79.6 examples/sec; 0.050 sec/batch; 82h:00m:24s remains)
INFO - root - 2019-11-04 02:16:50.622304: step 122470, total loss = 0.25, predict loss = 0.06 (70.8 examples/sec; 0.057 sec/batch; 92h:17m:47s remains)
INFO - root - 2019-11-04 02:16:51.257113: step 122480, total loss = 0.47, predict loss = 0.11 (67.5 examples/sec; 0.059 sec/batch; 96h:43m:13s remains)
INFO - root - 2019-11-04 02:16:51.865039: step 122490, total loss = 0.44, predict loss = 0.11 (75.6 examples/sec; 0.053 sec/batch; 86h:23m:14s remains)
INFO - root - 2019-11-04 02:16:52.510363: step 122500, total loss = 0.48, predict loss = 0.10 (69.5 examples/sec; 0.058 sec/batch; 93h:55m:15s remains)
INFO - root - 2019-11-04 02:16:53.128053: step 122510, total loss = 0.50, predict loss = 0.11 (75.9 examples/sec; 0.053 sec/batch; 86h:05m:06s remains)
INFO - root - 2019-11-04 02:16:53.781306: step 122520, total loss = 0.48, predict loss = 0.11 (74.2 examples/sec; 0.054 sec/batch; 87h:59m:22s remains)
INFO - root - 2019-11-04 02:16:54.415185: step 122530, total loss = 0.40, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 92h:10m:23s remains)
INFO - root - 2019-11-04 02:16:55.045915: step 122540, total loss = 0.54, predict loss = 0.12 (74.6 examples/sec; 0.054 sec/batch; 87h:31m:43s remains)
INFO - root - 2019-11-04 02:16:55.646911: step 122550, total loss = 0.39, predict loss = 0.09 (69.3 examples/sec; 0.058 sec/batch; 94h:11m:06s remains)
INFO - root - 2019-11-04 02:16:56.251635: step 122560, total loss = 0.43, predict loss = 0.10 (61.7 examples/sec; 0.065 sec/batch; 105h:46m:02s remains)
INFO - root - 2019-11-04 02:16:56.859807: step 122570, total loss = 0.29, predict loss = 0.06 (72.8 examples/sec; 0.055 sec/batch; 89h:40m:31s remains)
INFO - root - 2019-11-04 02:16:57.512801: step 122580, total loss = 0.37, predict loss = 0.09 (77.9 examples/sec; 0.051 sec/batch; 83h:51m:26s remains)
INFO - root - 2019-11-04 02:16:58.159737: step 122590, total loss = 0.23, predict loss = 0.05 (72.1 examples/sec; 0.055 sec/batch; 90h:34m:58s remains)
INFO - root - 2019-11-04 02:16:58.782799: step 122600, total loss = 0.32, predict loss = 0.07 (69.6 examples/sec; 0.057 sec/batch; 93h:50m:18s remains)
INFO - root - 2019-11-04 02:16:59.412759: step 122610, total loss = 0.46, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 91h:15m:43s remains)
INFO - root - 2019-11-04 02:17:00.076408: step 122620, total loss = 0.49, predict loss = 0.11 (79.8 examples/sec; 0.050 sec/batch; 81h:48m:45s remains)
INFO - root - 2019-11-04 02:17:00.691844: step 122630, total loss = 0.48, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 95h:29m:03s remains)
INFO - root - 2019-11-04 02:17:01.302982: step 122640, total loss = 0.46, predict loss = 0.10 (66.8 examples/sec; 0.060 sec/batch; 97h:43m:34s remains)
INFO - root - 2019-11-04 02:17:01.969056: step 122650, total loss = 0.39, predict loss = 0.09 (66.8 examples/sec; 0.060 sec/batch; 97h:41m:26s remains)
INFO - root - 2019-11-04 02:17:02.629141: step 122660, total loss = 0.48, predict loss = 0.11 (63.9 examples/sec; 0.063 sec/batch; 102h:13m:45s remains)
INFO - root - 2019-11-04 02:17:03.275201: step 122670, total loss = 0.64, predict loss = 0.15 (84.1 examples/sec; 0.048 sec/batch; 77h:37m:10s remains)
INFO - root - 2019-11-04 02:17:03.896890: step 122680, total loss = 0.48, predict loss = 0.11 (82.8 examples/sec; 0.048 sec/batch; 78h:53m:32s remains)
INFO - root - 2019-11-04 02:17:04.494687: step 122690, total loss = 0.62, predict loss = 0.14 (68.2 examples/sec; 0.059 sec/batch; 95h:44m:32s remains)
INFO - root - 2019-11-04 02:17:05.089142: step 122700, total loss = 0.36, predict loss = 0.08 (72.7 examples/sec; 0.055 sec/batch; 89h:50m:13s remains)
INFO - root - 2019-11-04 02:17:05.688820: step 122710, total loss = 0.46, predict loss = 0.10 (64.3 examples/sec; 0.062 sec/batch; 101h:34m:59s remains)
INFO - root - 2019-11-04 02:17:06.291293: step 122720, total loss = 0.33, predict loss = 0.07 (75.5 examples/sec; 0.053 sec/batch; 86h:29m:37s remains)
INFO - root - 2019-11-04 02:17:06.883445: step 122730, total loss = 0.39, predict loss = 0.09 (68.2 examples/sec; 0.059 sec/batch; 95h:43m:57s remains)
INFO - root - 2019-11-04 02:17:07.514875: step 122740, total loss = 0.38, predict loss = 0.09 (80.6 examples/sec; 0.050 sec/batch; 80h:58m:39s remains)
INFO - root - 2019-11-04 02:17:08.094366: step 122750, total loss = 0.39, predict loss = 0.08 (69.7 examples/sec; 0.057 sec/batch; 93h:44m:45s remains)
INFO - root - 2019-11-04 02:17:08.668126: step 122760, total loss = 0.47, predict loss = 0.11 (79.6 examples/sec; 0.050 sec/batch; 82h:02m:27s remains)
INFO - root - 2019-11-04 02:17:09.259766: step 122770, total loss = 0.52, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 94h:43m:04s remains)
INFO - root - 2019-11-04 02:17:09.864597: step 122780, total loss = 0.53, predict loss = 0.13 (70.2 examples/sec; 0.057 sec/batch; 92h:58m:24s remains)
INFO - root - 2019-11-04 02:17:10.454337: step 122790, total loss = 0.55, predict loss = 0.12 (72.0 examples/sec; 0.056 sec/batch; 90h:44m:13s remains)
INFO - root - 2019-11-04 02:17:11.075198: step 122800, total loss = 0.67, predict loss = 0.16 (74.9 examples/sec; 0.053 sec/batch; 87h:13m:28s remains)
INFO - root - 2019-11-04 02:17:11.637222: step 122810, total loss = 0.61, predict loss = 0.15 (86.6 examples/sec; 0.046 sec/batch; 75h:24m:42s remains)
INFO - root - 2019-11-04 02:17:12.312388: step 122820, total loss = 0.75, predict loss = 0.17 (60.2 examples/sec; 0.066 sec/batch; 108h:32m:08s remains)
INFO - root - 2019-11-04 02:17:12.977506: step 122830, total loss = 0.77, predict loss = 0.18 (79.7 examples/sec; 0.050 sec/batch; 81h:58m:42s remains)
INFO - root - 2019-11-04 02:17:13.624676: step 122840, total loss = 0.64, predict loss = 0.15 (66.1 examples/sec; 0.060 sec/batch; 98h:44m:56s remains)
INFO - root - 2019-11-04 02:17:14.284223: step 122850, total loss = 0.69, predict loss = 0.15 (68.0 examples/sec; 0.059 sec/batch; 96h:04m:16s remains)
INFO - root - 2019-11-04 02:17:14.953932: step 122860, total loss = 0.47, predict loss = 0.11 (62.2 examples/sec; 0.064 sec/batch; 105h:01m:05s remains)
INFO - root - 2019-11-04 02:17:15.646519: step 122870, total loss = 0.51, predict loss = 0.12 (61.5 examples/sec; 0.065 sec/batch; 106h:12m:14s remains)
INFO - root - 2019-11-04 02:17:16.318670: step 122880, total loss = 0.65, predict loss = 0.15 (64.2 examples/sec; 0.062 sec/batch; 101h:38m:31s remains)
INFO - root - 2019-11-04 02:17:16.961865: step 122890, total loss = 0.46, predict loss = 0.10 (73.5 examples/sec; 0.054 sec/batch; 88h:48m:30s remains)
INFO - root - 2019-11-04 02:17:17.595690: step 122900, total loss = 0.48, predict loss = 0.11 (65.1 examples/sec; 0.061 sec/batch; 100h:20m:01s remains)
INFO - root - 2019-11-04 02:17:18.224156: step 122910, total loss = 0.57, predict loss = 0.13 (71.1 examples/sec; 0.056 sec/batch; 91h:48m:07s remains)
INFO - root - 2019-11-04 02:17:18.868127: step 122920, total loss = 0.49, predict loss = 0.11 (71.3 examples/sec; 0.056 sec/batch; 91h:32m:33s remains)
INFO - root - 2019-11-04 02:17:19.528730: step 122930, total loss = 0.59, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 97h:03m:19s remains)
INFO - root - 2019-11-04 02:17:20.203040: step 122940, total loss = 0.46, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 95h:10m:36s remains)
INFO - root - 2019-11-04 02:17:20.862115: step 122950, total loss = 0.38, predict loss = 0.09 (64.3 examples/sec; 0.062 sec/batch; 101h:32m:23s remains)
INFO - root - 2019-11-04 02:17:21.481018: step 122960, total loss = 0.56, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 89h:18m:25s remains)
INFO - root - 2019-11-04 02:17:22.057483: step 122970, total loss = 0.52, predict loss = 0.12 (77.1 examples/sec; 0.052 sec/batch; 84h:39m:47s remains)
INFO - root - 2019-11-04 02:17:22.698050: step 122980, total loss = 0.44, predict loss = 0.10 (65.8 examples/sec; 0.061 sec/batch; 99h:17m:18s remains)
INFO - root - 2019-11-04 02:17:23.359007: step 122990, total loss = 0.51, predict loss = 0.12 (61.9 examples/sec; 0.065 sec/batch; 105h:29m:38s remains)
INFO - root - 2019-11-04 02:17:24.048268: step 123000, total loss = 0.51, predict loss = 0.11 (62.3 examples/sec; 0.064 sec/batch; 104h:50m:36s remains)
INFO - root - 2019-11-04 02:17:24.745388: step 123010, total loss = 0.49, predict loss = 0.11 (74.9 examples/sec; 0.053 sec/batch; 87h:09m:10s remains)
INFO - root - 2019-11-04 02:17:25.379971: step 123020, total loss = 0.48, predict loss = 0.12 (64.4 examples/sec; 0.062 sec/batch; 101h:23m:33s remains)
INFO - root - 2019-11-04 02:17:26.030468: step 123030, total loss = 0.52, predict loss = 0.12 (70.7 examples/sec; 0.057 sec/batch; 92h:25m:12s remains)
INFO - root - 2019-11-04 02:17:26.656255: step 123040, total loss = 0.41, predict loss = 0.09 (65.9 examples/sec; 0.061 sec/batch; 99h:05m:28s remains)
INFO - root - 2019-11-04 02:17:27.364843: step 123050, total loss = 0.35, predict loss = 0.08 (72.8 examples/sec; 0.055 sec/batch; 89h:38m:35s remains)
INFO - root - 2019-11-04 02:17:27.974439: step 123060, total loss = 0.36, predict loss = 0.08 (71.3 examples/sec; 0.056 sec/batch; 91h:35m:39s remains)
INFO - root - 2019-11-04 02:17:28.598132: step 123070, total loss = 0.43, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 97h:03m:45s remains)
INFO - root - 2019-11-04 02:17:29.231459: step 123080, total loss = 0.35, predict loss = 0.08 (68.9 examples/sec; 0.058 sec/batch; 94h:47m:55s remains)
INFO - root - 2019-11-04 02:17:29.866734: step 123090, total loss = 0.44, predict loss = 0.10 (61.7 examples/sec; 0.065 sec/batch; 105h:47m:45s remains)
INFO - root - 2019-11-04 02:17:30.502215: step 123100, total loss = 0.38, predict loss = 0.08 (81.6 examples/sec; 0.049 sec/batch; 79h:59m:19s remains)
INFO - root - 2019-11-04 02:17:31.107426: step 123110, total loss = 0.38, predict loss = 0.09 (76.7 examples/sec; 0.052 sec/batch; 85h:05m:35s remains)
INFO - root - 2019-11-04 02:17:31.728392: step 123120, total loss = 0.33, predict loss = 0.08 (71.4 examples/sec; 0.056 sec/batch; 91h:30m:10s remains)
INFO - root - 2019-11-04 02:17:32.345639: step 123130, total loss = 0.45, predict loss = 0.10 (69.0 examples/sec; 0.058 sec/batch; 94h:38m:59s remains)
INFO - root - 2019-11-04 02:17:33.024084: step 123140, total loss = 0.42, predict loss = 0.10 (68.9 examples/sec; 0.058 sec/batch; 94h:48m:18s remains)
INFO - root - 2019-11-04 02:17:33.687233: step 123150, total loss = 0.44, predict loss = 0.10 (75.1 examples/sec; 0.053 sec/batch; 86h:58m:35s remains)
INFO - root - 2019-11-04 02:17:34.305552: step 123160, total loss = 0.44, predict loss = 0.10 (71.1 examples/sec; 0.056 sec/batch; 91h:46m:37s remains)
INFO - root - 2019-11-04 02:17:34.945042: step 123170, total loss = 0.39, predict loss = 0.08 (63.4 examples/sec; 0.063 sec/batch; 102h:55m:17s remains)
INFO - root - 2019-11-04 02:17:36.056357: step 123180, total loss = 0.49, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 93h:11m:00s remains)
INFO - root - 2019-11-04 02:17:36.662264: step 123190, total loss = 0.54, predict loss = 0.12 (72.3 examples/sec; 0.055 sec/batch; 90h:22m:31s remains)
INFO - root - 2019-11-04 02:17:37.284702: step 123200, total loss = 0.58, predict loss = 0.14 (75.7 examples/sec; 0.053 sec/batch; 86h:14m:35s remains)
INFO - root - 2019-11-04 02:17:37.900212: step 123210, total loss = 0.56, predict loss = 0.14 (77.9 examples/sec; 0.051 sec/batch; 83h:48m:58s remains)
INFO - root - 2019-11-04 02:17:38.488399: step 123220, total loss = 0.55, predict loss = 0.13 (81.8 examples/sec; 0.049 sec/batch; 79h:47m:33s remains)
INFO - root - 2019-11-04 02:17:39.087933: step 123230, total loss = 0.60, predict loss = 0.14 (69.5 examples/sec; 0.058 sec/batch; 93h:56m:32s remains)
INFO - root - 2019-11-04 02:17:39.752436: step 123240, total loss = 0.66, predict loss = 0.17 (66.8 examples/sec; 0.060 sec/batch; 97h:43m:36s remains)
INFO - root - 2019-11-04 02:17:40.372123: step 123250, total loss = 0.59, predict loss = 0.14 (69.4 examples/sec; 0.058 sec/batch; 94h:05m:57s remains)
INFO - root - 2019-11-04 02:17:41.004579: step 123260, total loss = 0.51, predict loss = 0.11 (75.2 examples/sec; 0.053 sec/batch; 86h:53m:04s remains)
INFO - root - 2019-11-04 02:17:41.643428: step 123270, total loss = 0.59, predict loss = 0.14 (74.6 examples/sec; 0.054 sec/batch; 87h:33m:02s remains)
INFO - root - 2019-11-04 02:17:42.285565: step 123280, total loss = 0.51, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 94h:36m:52s remains)
INFO - root - 2019-11-04 02:17:42.911949: step 123290, total loss = 0.58, predict loss = 0.14 (71.9 examples/sec; 0.056 sec/batch; 90h:49m:02s remains)
INFO - root - 2019-11-04 02:17:43.561441: step 123300, total loss = 0.47, predict loss = 0.10 (70.4 examples/sec; 0.057 sec/batch; 92h:42m:46s remains)
INFO - root - 2019-11-04 02:17:44.224207: step 123310, total loss = 0.45, predict loss = 0.11 (62.2 examples/sec; 0.064 sec/batch; 104h:54m:28s remains)
INFO - root - 2019-11-04 02:17:44.849152: step 123320, total loss = 0.45, predict loss = 0.10 (74.5 examples/sec; 0.054 sec/batch; 87h:41m:56s remains)
INFO - root - 2019-11-04 02:17:45.494550: step 123330, total loss = 0.50, predict loss = 0.12 (66.3 examples/sec; 0.060 sec/batch; 98h:26m:13s remains)
INFO - root - 2019-11-04 02:17:46.128752: step 123340, total loss = 0.45, predict loss = 0.10 (67.2 examples/sec; 0.060 sec/batch; 97h:08m:49s remains)
INFO - root - 2019-11-04 02:17:46.793248: step 123350, total loss = 0.37, predict loss = 0.07 (66.3 examples/sec; 0.060 sec/batch; 98h:25m:02s remains)
INFO - root - 2019-11-04 02:17:47.486371: step 123360, total loss = 0.43, predict loss = 0.10 (63.5 examples/sec; 0.063 sec/batch; 102h:50m:14s remains)
INFO - root - 2019-11-04 02:17:48.094756: step 123370, total loss = 0.31, predict loss = 0.07 (73.2 examples/sec; 0.055 sec/batch; 89h:11m:22s remains)
INFO - root - 2019-11-04 02:17:48.717500: step 123380, total loss = 0.47, predict loss = 0.11 (68.7 examples/sec; 0.058 sec/batch; 95h:05m:12s remains)
INFO - root - 2019-11-04 02:17:49.366535: step 123390, total loss = 0.58, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 93h:31m:39s remains)
INFO - root - 2019-11-04 02:17:50.030021: step 123400, total loss = 0.54, predict loss = 0.13 (67.5 examples/sec; 0.059 sec/batch; 96h:41m:24s remains)
INFO - root - 2019-11-04 02:17:50.644233: step 123410, total loss = 0.45, predict loss = 0.10 (72.5 examples/sec; 0.055 sec/batch; 90h:03m:54s remains)
INFO - root - 2019-11-04 02:17:51.263447: step 123420, total loss = 0.42, predict loss = 0.09 (85.6 examples/sec; 0.047 sec/batch; 76h:15m:27s remains)
INFO - root - 2019-11-04 02:17:51.891056: step 123430, total loss = 0.64, predict loss = 0.15 (68.5 examples/sec; 0.058 sec/batch; 95h:16m:47s remains)
INFO - root - 2019-11-04 02:17:52.572069: step 123440, total loss = 0.50, predict loss = 0.12 (71.2 examples/sec; 0.056 sec/batch; 91h:43m:42s remains)
INFO - root - 2019-11-04 02:17:53.189749: step 123450, total loss = 0.61, predict loss = 0.14 (69.4 examples/sec; 0.058 sec/batch; 94h:01m:30s remains)
INFO - root - 2019-11-04 02:17:53.828186: step 123460, total loss = 0.61, predict loss = 0.14 (66.4 examples/sec; 0.060 sec/batch; 98h:23m:44s remains)
INFO - root - 2019-11-04 02:17:54.455954: step 123470, total loss = 0.52, predict loss = 0.12 (78.0 examples/sec; 0.051 sec/batch; 83h:41m:56s remains)
INFO - root - 2019-11-04 02:17:55.062416: step 123480, total loss = 0.49, predict loss = 0.12 (73.9 examples/sec; 0.054 sec/batch; 88h:18m:52s remains)
INFO - root - 2019-11-04 02:17:55.666960: step 123490, total loss = 0.52, predict loss = 0.13 (73.5 examples/sec; 0.054 sec/batch; 88h:51m:15s remains)
INFO - root - 2019-11-04 02:17:56.311188: step 123500, total loss = 0.46, predict loss = 0.10 (61.5 examples/sec; 0.065 sec/batch; 106h:10m:46s remains)
INFO - root - 2019-11-04 02:17:56.958304: step 123510, total loss = 0.54, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 85h:59m:32s remains)
INFO - root - 2019-11-04 02:17:57.596274: step 123520, total loss = 0.58, predict loss = 0.13 (72.3 examples/sec; 0.055 sec/batch; 90h:19m:08s remains)
INFO - root - 2019-11-04 02:17:58.189273: step 123530, total loss = 0.52, predict loss = 0.12 (87.9 examples/sec; 0.046 sec/batch; 74h:18m:25s remains)
INFO - root - 2019-11-04 02:17:58.822903: step 123540, total loss = 0.51, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 92h:40m:50s remains)
INFO - root - 2019-11-04 02:17:59.432641: step 123550, total loss = 0.54, predict loss = 0.12 (81.2 examples/sec; 0.049 sec/batch; 80h:23m:17s remains)
INFO - root - 2019-11-04 02:18:00.091853: step 123560, total loss = 0.56, predict loss = 0.13 (76.0 examples/sec; 0.053 sec/batch; 85h:53m:29s remains)
INFO - root - 2019-11-04 02:18:00.722265: step 123570, total loss = 0.41, predict loss = 0.09 (63.2 examples/sec; 0.063 sec/batch; 103h:18m:38s remains)
INFO - root - 2019-11-04 02:18:01.298853: step 123580, total loss = 0.51, predict loss = 0.11 (74.6 examples/sec; 0.054 sec/batch; 87h:33m:50s remains)
INFO - root - 2019-11-04 02:18:01.918505: step 123590, total loss = 0.40, predict loss = 0.09 (69.4 examples/sec; 0.058 sec/batch; 94h:05m:19s remains)
INFO - root - 2019-11-04 02:18:02.619094: step 123600, total loss = 0.55, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 100h:20m:26s remains)
INFO - root - 2019-11-04 02:18:03.267851: step 123610, total loss = 0.35, predict loss = 0.08 (75.3 examples/sec; 0.053 sec/batch; 86h:44m:46s remains)
INFO - root - 2019-11-04 02:18:03.868133: step 123620, total loss = 0.54, predict loss = 0.12 (63.9 examples/sec; 0.063 sec/batch; 102h:09m:39s remains)
INFO - root - 2019-11-04 02:18:04.484015: step 123630, total loss = 0.51, predict loss = 0.12 (66.3 examples/sec; 0.060 sec/batch; 98h:31m:48s remains)
INFO - root - 2019-11-04 02:18:05.120493: step 123640, total loss = 0.39, predict loss = 0.09 (65.2 examples/sec; 0.061 sec/batch; 100h:08m:39s remains)
INFO - root - 2019-11-04 02:18:05.723210: step 123650, total loss = 0.45, predict loss = 0.11 (78.0 examples/sec; 0.051 sec/batch; 83h:40m:59s remains)
INFO - root - 2019-11-04 02:18:06.342800: step 123660, total loss = 0.53, predict loss = 0.12 (62.2 examples/sec; 0.064 sec/batch; 105h:02m:30s remains)
INFO - root - 2019-11-04 02:18:06.981442: step 123670, total loss = 0.42, predict loss = 0.10 (72.8 examples/sec; 0.055 sec/batch; 89h:40m:00s remains)
INFO - root - 2019-11-04 02:18:07.605031: step 123680, total loss = 0.41, predict loss = 0.09 (74.6 examples/sec; 0.054 sec/batch; 87h:33m:26s remains)
INFO - root - 2019-11-04 02:18:08.188813: step 123690, total loss = 0.42, predict loss = 0.09 (72.4 examples/sec; 0.055 sec/batch; 90h:11m:19s remains)
INFO - root - 2019-11-04 02:18:08.824975: step 123700, total loss = 0.49, predict loss = 0.11 (60.3 examples/sec; 0.066 sec/batch; 108h:17m:16s remains)
INFO - root - 2019-11-04 02:18:09.486659: step 123710, total loss = 0.49, predict loss = 0.11 (68.8 examples/sec; 0.058 sec/batch; 94h:53m:06s remains)
INFO - root - 2019-11-04 02:18:10.130059: step 123720, total loss = 0.45, predict loss = 0.10 (61.4 examples/sec; 0.065 sec/batch; 106h:24m:49s remains)
INFO - root - 2019-11-04 02:18:10.811331: step 123730, total loss = 0.53, predict loss = 0.12 (72.7 examples/sec; 0.055 sec/batch; 89h:48m:06s remains)
INFO - root - 2019-11-04 02:18:11.453698: step 123740, total loss = 0.51, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 90h:57m:00s remains)
INFO - root - 2019-11-04 02:18:12.106940: step 123750, total loss = 0.55, predict loss = 0.14 (74.9 examples/sec; 0.053 sec/batch; 87h:09m:36s remains)
INFO - root - 2019-11-04 02:18:12.721882: step 123760, total loss = 0.44, predict loss = 0.10 (86.6 examples/sec; 0.046 sec/batch; 75h:25m:21s remains)
INFO - root - 2019-11-04 02:18:13.352139: step 123770, total loss = 0.51, predict loss = 0.13 (76.2 examples/sec; 0.052 sec/batch; 85h:38m:00s remains)
INFO - root - 2019-11-04 02:18:13.948194: step 123780, total loss = 0.55, predict loss = 0.13 (83.9 examples/sec; 0.048 sec/batch; 77h:47m:21s remains)
INFO - root - 2019-11-04 02:18:14.547517: step 123790, total loss = 0.55, predict loss = 0.13 (67.0 examples/sec; 0.060 sec/batch; 97h:28m:20s remains)
INFO - root - 2019-11-04 02:18:15.185597: step 123800, total loss = 0.74, predict loss = 0.18 (68.8 examples/sec; 0.058 sec/batch; 94h:56m:49s remains)
INFO - root - 2019-11-04 02:18:15.802394: step 123810, total loss = 0.54, predict loss = 0.13 (72.9 examples/sec; 0.055 sec/batch; 89h:35m:36s remains)
INFO - root - 2019-11-04 02:18:16.417085: step 123820, total loss = 0.73, predict loss = 0.18 (78.0 examples/sec; 0.051 sec/batch; 83h:40m:33s remains)
INFO - root - 2019-11-04 02:18:17.003014: step 123830, total loss = 0.47, predict loss = 0.11 (83.0 examples/sec; 0.048 sec/batch; 78h:41m:36s remains)
INFO - root - 2019-11-04 02:18:17.604402: step 123840, total loss = 0.69, predict loss = 0.16 (80.4 examples/sec; 0.050 sec/batch; 81h:13m:04s remains)
INFO - root - 2019-11-04 02:18:18.205903: step 123850, total loss = 0.56, predict loss = 0.13 (71.4 examples/sec; 0.056 sec/batch; 91h:22m:54s remains)
INFO - root - 2019-11-04 02:18:18.826190: step 123860, total loss = 0.51, predict loss = 0.12 (75.4 examples/sec; 0.053 sec/batch; 86h:36m:35s remains)
INFO - root - 2019-11-04 02:18:19.463023: step 123870, total loss = 0.38, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 93h:58m:14s remains)
INFO - root - 2019-11-04 02:18:20.120178: step 123880, total loss = 0.62, predict loss = 0.15 (63.0 examples/sec; 0.063 sec/batch; 103h:37m:27s remains)
INFO - root - 2019-11-04 02:18:20.850600: step 123890, total loss = 0.61, predict loss = 0.14 (63.9 examples/sec; 0.063 sec/batch; 102h:09m:26s remains)
INFO - root - 2019-11-04 02:18:21.491602: step 123900, total loss = 0.53, predict loss = 0.12 (72.2 examples/sec; 0.055 sec/batch; 90h:26m:24s remains)
INFO - root - 2019-11-04 02:18:22.100868: step 123910, total loss = 0.72, predict loss = 0.17 (74.1 examples/sec; 0.054 sec/batch; 88h:04m:28s remains)
INFO - root - 2019-11-04 02:18:22.765082: step 123920, total loss = 0.68, predict loss = 0.16 (67.2 examples/sec; 0.060 sec/batch; 97h:07m:27s remains)
INFO - root - 2019-11-04 02:18:23.427699: step 123930, total loss = 0.52, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 94h:53m:07s remains)
INFO - root - 2019-11-04 02:18:24.080269: step 123940, total loss = 0.45, predict loss = 0.10 (72.2 examples/sec; 0.055 sec/batch; 90h:26m:13s remains)
INFO - root - 2019-11-04 02:18:24.752280: step 123950, total loss = 0.60, predict loss = 0.15 (70.7 examples/sec; 0.057 sec/batch; 92h:20m:37s remains)
INFO - root - 2019-11-04 02:18:25.401320: step 123960, total loss = 0.50, predict loss = 0.12 (69.8 examples/sec; 0.057 sec/batch; 93h:30m:58s remains)
INFO - root - 2019-11-04 02:18:26.033890: step 123970, total loss = 0.52, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 92h:50m:43s remains)
INFO - root - 2019-11-04 02:18:26.650543: step 123980, total loss = 0.40, predict loss = 0.09 (77.8 examples/sec; 0.051 sec/batch; 83h:52m:37s remains)
INFO - root - 2019-11-04 02:18:27.249238: step 123990, total loss = 0.41, predict loss = 0.09 (82.1 examples/sec; 0.049 sec/batch; 79h:32m:04s remains)
INFO - root - 2019-11-04 02:18:27.884121: step 124000, total loss = 0.45, predict loss = 0.11 (68.3 examples/sec; 0.059 sec/batch; 95h:34m:04s remains)
INFO - root - 2019-11-04 02:18:28.524613: step 124010, total loss = 0.65, predict loss = 0.16 (71.2 examples/sec; 0.056 sec/batch; 91h:43m:08s remains)
INFO - root - 2019-11-04 02:18:29.192696: step 124020, total loss = 0.49, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 98h:21m:40s remains)
INFO - root - 2019-11-04 02:18:29.837193: step 124030, total loss = 0.51, predict loss = 0.12 (67.5 examples/sec; 0.059 sec/batch; 96h:44m:52s remains)
INFO - root - 2019-11-04 02:18:30.478351: step 124040, total loss = 0.51, predict loss = 0.11 (64.7 examples/sec; 0.062 sec/batch; 100h:52m:18s remains)
INFO - root - 2019-11-04 02:18:31.172058: step 124050, total loss = 0.41, predict loss = 0.10 (64.2 examples/sec; 0.062 sec/batch; 101h:44m:47s remains)
INFO - root - 2019-11-04 02:18:31.776632: step 124060, total loss = 0.53, predict loss = 0.12 (72.9 examples/sec; 0.055 sec/batch; 89h:30m:44s remains)
INFO - root - 2019-11-04 02:18:32.335286: step 124070, total loss = 0.48, predict loss = 0.11 (61.4 examples/sec; 0.065 sec/batch; 106h:24m:02s remains)
INFO - root - 2019-11-04 02:18:32.843407: step 124080, total loss = 0.35, predict loss = 0.07 (95.2 examples/sec; 0.042 sec/batch; 68h:33m:22s remains)
INFO - root - 2019-11-04 02:18:33.296228: step 124090, total loss = 0.44, predict loss = 0.10 (95.5 examples/sec; 0.042 sec/batch; 68h:21m:56s remains)
INFO - root - 2019-11-04 02:18:34.381369: step 124100, total loss = 0.31, predict loss = 0.07 (70.7 examples/sec; 0.057 sec/batch; 92h:18m:16s remains)
INFO - root - 2019-11-04 02:18:34.971394: step 124110, total loss = 0.38, predict loss = 0.08 (74.6 examples/sec; 0.054 sec/batch; 87h:30m:46s remains)
INFO - root - 2019-11-04 02:18:35.562038: step 124120, total loss = 0.29, predict loss = 0.06 (70.5 examples/sec; 0.057 sec/batch; 92h:34m:14s remains)
INFO - root - 2019-11-04 02:18:36.173319: step 124130, total loss = 0.49, predict loss = 0.12 (74.5 examples/sec; 0.054 sec/batch; 87h:38m:58s remains)
INFO - root - 2019-11-04 02:18:36.770042: step 124140, total loss = 0.46, predict loss = 0.10 (74.2 examples/sec; 0.054 sec/batch; 88h:01m:11s remains)
INFO - root - 2019-11-04 02:18:37.384110: step 124150, total loss = 0.39, predict loss = 0.09 (68.5 examples/sec; 0.058 sec/batch; 95h:20m:34s remains)
INFO - root - 2019-11-04 02:18:38.009138: step 124160, total loss = 0.34, predict loss = 0.08 (69.4 examples/sec; 0.058 sec/batch; 94h:01m:41s remains)
INFO - root - 2019-11-04 02:18:38.669864: step 124170, total loss = 0.50, predict loss = 0.11 (67.9 examples/sec; 0.059 sec/batch; 96h:12m:06s remains)
INFO - root - 2019-11-04 02:18:39.359093: step 124180, total loss = 0.69, predict loss = 0.16 (76.9 examples/sec; 0.052 sec/batch; 84h:56m:07s remains)
INFO - root - 2019-11-04 02:18:40.040915: step 124190, total loss = 0.57, predict loss = 0.13 (61.3 examples/sec; 0.065 sec/batch; 106h:30m:53s remains)
INFO - root - 2019-11-04 02:18:40.712027: step 124200, total loss = 0.62, predict loss = 0.14 (65.6 examples/sec; 0.061 sec/batch; 99h:28m:19s remains)
INFO - root - 2019-11-04 02:18:41.313145: step 124210, total loss = 0.50, predict loss = 0.11 (76.4 examples/sec; 0.052 sec/batch; 85h:27m:25s remains)
INFO - root - 2019-11-04 02:18:41.919811: step 124220, total loss = 0.50, predict loss = 0.12 (70.4 examples/sec; 0.057 sec/batch; 92h:42m:09s remains)
INFO - root - 2019-11-04 02:18:42.543703: step 124230, total loss = 0.46, predict loss = 0.11 (62.8 examples/sec; 0.064 sec/batch; 103h:55m:19s remains)
INFO - root - 2019-11-04 02:18:43.192520: step 124240, total loss = 0.44, predict loss = 0.09 (63.9 examples/sec; 0.063 sec/batch; 102h:08m:49s remains)
INFO - root - 2019-11-04 02:18:43.831417: step 124250, total loss = 0.39, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 83h:27m:11s remains)
INFO - root - 2019-11-04 02:18:44.470259: step 124260, total loss = 0.45, predict loss = 0.10 (61.2 examples/sec; 0.065 sec/batch; 106h:38m:06s remains)
INFO - root - 2019-11-04 02:18:45.070885: step 124270, total loss = 0.40, predict loss = 0.09 (89.4 examples/sec; 0.045 sec/batch; 73h:03m:56s remains)
INFO - root - 2019-11-04 02:18:45.669849: step 124280, total loss = 0.45, predict loss = 0.10 (76.9 examples/sec; 0.052 sec/batch; 84h:55m:40s remains)
INFO - root - 2019-11-04 02:18:46.259322: step 124290, total loss = 0.51, predict loss = 0.12 (70.1 examples/sec; 0.057 sec/batch; 93h:07m:45s remains)
INFO - root - 2019-11-04 02:18:46.942843: step 124300, total loss = 0.44, predict loss = 0.10 (64.6 examples/sec; 0.062 sec/batch; 101h:03m:05s remains)
INFO - root - 2019-11-04 02:18:47.591319: step 124310, total loss = 0.42, predict loss = 0.10 (71.6 examples/sec; 0.056 sec/batch; 91h:10m:45s remains)
INFO - root - 2019-11-04 02:18:48.253390: step 124320, total loss = 0.34, predict loss = 0.07 (64.1 examples/sec; 0.062 sec/batch; 101h:49m:47s remains)
INFO - root - 2019-11-04 02:18:48.876594: step 124330, total loss = 0.37, predict loss = 0.08 (75.7 examples/sec; 0.053 sec/batch; 86h:13m:07s remains)
INFO - root - 2019-11-04 02:18:49.510816: step 124340, total loss = 0.36, predict loss = 0.08 (74.8 examples/sec; 0.053 sec/batch; 87h:14m:06s remains)
INFO - root - 2019-11-04 02:18:50.144062: step 124350, total loss = 0.44, predict loss = 0.11 (61.0 examples/sec; 0.066 sec/batch; 107h:06m:26s remains)
INFO - root - 2019-11-04 02:18:50.812212: step 124360, total loss = 0.40, predict loss = 0.09 (65.0 examples/sec; 0.062 sec/batch; 100h:24m:32s remains)
INFO - root - 2019-11-04 02:18:51.468426: step 124370, total loss = 0.46, predict loss = 0.10 (72.5 examples/sec; 0.055 sec/batch; 90h:04m:22s remains)
INFO - root - 2019-11-04 02:18:52.091770: step 124380, total loss = 0.57, predict loss = 0.14 (68.9 examples/sec; 0.058 sec/batch; 94h:46m:49s remains)
INFO - root - 2019-11-04 02:18:52.743921: step 124390, total loss = 0.60, predict loss = 0.13 (71.4 examples/sec; 0.056 sec/batch; 91h:24m:55s remains)
INFO - root - 2019-11-04 02:18:53.366153: step 124400, total loss = 0.46, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 96h:53m:25s remains)
INFO - root - 2019-11-04 02:18:53.950897: step 124410, total loss = 0.60, predict loss = 0.14 (86.5 examples/sec; 0.046 sec/batch; 75h:30m:51s remains)
INFO - root - 2019-11-04 02:18:54.551816: step 124420, total loss = 0.55, predict loss = 0.13 (73.6 examples/sec; 0.054 sec/batch; 88h:42m:00s remains)
INFO - root - 2019-11-04 02:18:55.192818: step 124430, total loss = 0.65, predict loss = 0.16 (72.0 examples/sec; 0.056 sec/batch; 90h:43m:34s remains)
INFO - root - 2019-11-04 02:18:55.836255: step 124440, total loss = 0.51, predict loss = 0.12 (60.2 examples/sec; 0.066 sec/batch; 108h:23m:42s remains)
INFO - root - 2019-11-04 02:18:56.447758: step 124450, total loss = 0.48, predict loss = 0.11 (71.0 examples/sec; 0.056 sec/batch; 91h:59m:29s remains)
INFO - root - 2019-11-04 02:18:57.117454: step 124460, total loss = 0.46, predict loss = 0.10 (62.7 examples/sec; 0.064 sec/batch; 104h:09m:06s remains)
INFO - root - 2019-11-04 02:18:57.728403: step 124470, total loss = 0.35, predict loss = 0.07 (75.0 examples/sec; 0.053 sec/batch; 87h:03m:24s remains)
INFO - root - 2019-11-04 02:18:58.340646: step 124480, total loss = 0.53, predict loss = 0.12 (66.1 examples/sec; 0.060 sec/batch; 98h:41m:51s remains)
INFO - root - 2019-11-04 02:18:58.943845: step 124490, total loss = 0.45, predict loss = 0.10 (77.2 examples/sec; 0.052 sec/batch; 84h:31m:39s remains)
INFO - root - 2019-11-04 02:18:59.565596: step 124500, total loss = 0.46, predict loss = 0.10 (73.1 examples/sec; 0.055 sec/batch; 89h:16m:01s remains)
INFO - root - 2019-11-04 02:19:00.222415: step 124510, total loss = 0.48, predict loss = 0.10 (67.9 examples/sec; 0.059 sec/batch; 96h:10m:56s remains)
INFO - root - 2019-11-04 02:19:00.848721: step 124520, total loss = 0.43, predict loss = 0.10 (74.3 examples/sec; 0.054 sec/batch; 87h:49m:38s remains)
INFO - root - 2019-11-04 02:19:01.462989: step 124530, total loss = 0.48, predict loss = 0.10 (71.7 examples/sec; 0.056 sec/batch; 91h:06m:46s remains)
INFO - root - 2019-11-04 02:19:02.091208: step 124540, total loss = 0.55, predict loss = 0.13 (82.1 examples/sec; 0.049 sec/batch; 79h:30m:33s remains)
INFO - root - 2019-11-04 02:19:02.723984: step 124550, total loss = 0.50, predict loss = 0.12 (64.1 examples/sec; 0.062 sec/batch; 101h:54m:24s remains)
INFO - root - 2019-11-04 02:19:03.333851: step 124560, total loss = 0.50, predict loss = 0.12 (75.2 examples/sec; 0.053 sec/batch; 86h:45m:49s remains)
INFO - root - 2019-11-04 02:19:03.966629: step 124570, total loss = 0.35, predict loss = 0.09 (69.1 examples/sec; 0.058 sec/batch; 94h:30m:50s remains)
INFO - root - 2019-11-04 02:19:04.629994: step 124580, total loss = 0.44, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 91h:12m:00s remains)
INFO - root - 2019-11-04 02:19:05.235648: step 124590, total loss = 0.59, predict loss = 0.14 (74.3 examples/sec; 0.054 sec/batch; 87h:54m:27s remains)
INFO - root - 2019-11-04 02:19:05.849159: step 124600, total loss = 0.51, predict loss = 0.13 (66.8 examples/sec; 0.060 sec/batch; 97h:41m:25s remains)
INFO - root - 2019-11-04 02:19:06.473435: step 124610, total loss = 0.51, predict loss = 0.12 (67.7 examples/sec; 0.059 sec/batch; 96h:27m:13s remains)
INFO - root - 2019-11-04 02:19:07.120051: step 124620, total loss = 0.50, predict loss = 0.12 (61.5 examples/sec; 0.065 sec/batch; 106h:06m:14s remains)
INFO - root - 2019-11-04 02:19:07.766250: step 124630, total loss = 0.38, predict loss = 0.09 (69.1 examples/sec; 0.058 sec/batch; 94h:25m:41s remains)
INFO - root - 2019-11-04 02:19:08.441001: step 124640, total loss = 0.43, predict loss = 0.11 (69.4 examples/sec; 0.058 sec/batch; 94h:00m:57s remains)
INFO - root - 2019-11-04 02:19:09.102771: step 124650, total loss = 0.45, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 98h:16m:35s remains)
INFO - root - 2019-11-04 02:19:09.715411: step 124660, total loss = 0.41, predict loss = 0.10 (74.2 examples/sec; 0.054 sec/batch; 88h:01m:26s remains)
INFO - root - 2019-11-04 02:19:10.366603: step 124670, total loss = 0.39, predict loss = 0.09 (67.8 examples/sec; 0.059 sec/batch; 96h:16m:29s remains)
INFO - root - 2019-11-04 02:19:11.041112: step 124680, total loss = 0.42, predict loss = 0.10 (68.9 examples/sec; 0.058 sec/batch; 94h:43m:35s remains)
INFO - root - 2019-11-04 02:19:11.684090: step 124690, total loss = 0.39, predict loss = 0.10 (73.7 examples/sec; 0.054 sec/batch; 88h:35m:59s remains)
INFO - root - 2019-11-04 02:19:12.374596: step 124700, total loss = 0.39, predict loss = 0.09 (59.3 examples/sec; 0.067 sec/batch; 110h:09m:37s remains)
INFO - root - 2019-11-04 02:19:13.026868: step 124710, total loss = 0.34, predict loss = 0.07 (74.3 examples/sec; 0.054 sec/batch; 87h:53m:09s remains)
INFO - root - 2019-11-04 02:19:13.696203: step 124720, total loss = 0.41, predict loss = 0.10 (63.8 examples/sec; 0.063 sec/batch; 102h:18m:48s remains)
INFO - root - 2019-11-04 02:19:14.310128: step 124730, total loss = 0.36, predict loss = 0.08 (79.0 examples/sec; 0.051 sec/batch; 82h:36m:24s remains)
INFO - root - 2019-11-04 02:19:14.960168: step 124740, total loss = 0.32, predict loss = 0.07 (76.6 examples/sec; 0.052 sec/batch; 85h:12m:37s remains)
INFO - root - 2019-11-04 02:19:15.623360: step 124750, total loss = 0.44, predict loss = 0.10 (68.1 examples/sec; 0.059 sec/batch; 95h:50m:13s remains)
INFO - root - 2019-11-04 02:19:16.283605: step 124760, total loss = 0.50, predict loss = 0.12 (60.5 examples/sec; 0.066 sec/batch; 107h:59m:21s remains)
INFO - root - 2019-11-04 02:19:16.916277: step 124770, total loss = 0.40, predict loss = 0.10 (72.6 examples/sec; 0.055 sec/batch; 89h:51m:24s remains)
INFO - root - 2019-11-04 02:19:17.572089: step 124780, total loss = 0.43, predict loss = 0.10 (62.4 examples/sec; 0.064 sec/batch; 104h:32m:00s remains)
INFO - root - 2019-11-04 02:19:18.186110: step 124790, total loss = 0.50, predict loss = 0.12 (82.0 examples/sec; 0.049 sec/batch; 79h:36m:32s remains)
INFO - root - 2019-11-04 02:19:18.811117: step 124800, total loss = 0.54, predict loss = 0.12 (63.3 examples/sec; 0.063 sec/batch; 103h:06m:55s remains)
INFO - root - 2019-11-04 02:19:19.453429: step 124810, total loss = 0.60, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 92h:27m:44s remains)
INFO - root - 2019-11-04 02:19:20.112630: step 124820, total loss = 0.39, predict loss = 0.09 (62.5 examples/sec; 0.064 sec/batch; 104h:26m:13s remains)
INFO - root - 2019-11-04 02:19:20.730386: step 124830, total loss = 0.40, predict loss = 0.09 (75.1 examples/sec; 0.053 sec/batch; 86h:55m:37s remains)
INFO - root - 2019-11-04 02:19:21.328435: step 124840, total loss = 0.55, predict loss = 0.13 (67.4 examples/sec; 0.059 sec/batch; 96h:49m:54s remains)
INFO - root - 2019-11-04 02:19:21.927318: step 124850, total loss = 0.35, predict loss = 0.07 (69.7 examples/sec; 0.057 sec/batch; 93h:42m:29s remains)
INFO - root - 2019-11-04 02:19:22.572224: step 124860, total loss = 0.48, predict loss = 0.11 (71.7 examples/sec; 0.056 sec/batch; 91h:01m:39s remains)
INFO - root - 2019-11-04 02:19:23.228801: step 124870, total loss = 0.55, predict loss = 0.14 (71.7 examples/sec; 0.056 sec/batch; 91h:04m:56s remains)
INFO - root - 2019-11-04 02:19:23.888191: step 124880, total loss = 0.53, predict loss = 0.13 (68.4 examples/sec; 0.058 sec/batch; 95h:25m:14s remains)
INFO - root - 2019-11-04 02:19:24.573665: step 124890, total loss = 0.59, predict loss = 0.15 (71.7 examples/sec; 0.056 sec/batch; 91h:03m:42s remains)
INFO - root - 2019-11-04 02:19:25.194496: step 124900, total loss = 0.40, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 93h:52m:30s remains)
INFO - root - 2019-11-04 02:19:25.799151: step 124910, total loss = 0.26, predict loss = 0.05 (74.5 examples/sec; 0.054 sec/batch; 87h:34m:36s remains)
INFO - root - 2019-11-04 02:19:26.448559: step 124920, total loss = 0.32, predict loss = 0.06 (68.2 examples/sec; 0.059 sec/batch; 95h:41m:23s remains)
INFO - root - 2019-11-04 02:19:27.085804: step 124930, total loss = 0.46, predict loss = 0.11 (73.4 examples/sec; 0.055 sec/batch; 88h:56m:31s remains)
INFO - root - 2019-11-04 02:19:27.713235: step 124940, total loss = 0.34, predict loss = 0.08 (73.3 examples/sec; 0.055 sec/batch; 89h:05m:18s remains)
INFO - root - 2019-11-04 02:19:28.308681: step 124950, total loss = 0.26, predict loss = 0.05 (78.4 examples/sec; 0.051 sec/batch; 83h:14m:55s remains)
INFO - root - 2019-11-04 02:19:28.914905: step 124960, total loss = 0.22, predict loss = 0.05 (72.2 examples/sec; 0.055 sec/batch; 90h:27m:21s remains)
INFO - root - 2019-11-04 02:19:29.518798: step 124970, total loss = 0.40, predict loss = 0.09 (72.2 examples/sec; 0.055 sec/batch; 90h:28m:08s remains)
INFO - root - 2019-11-04 02:19:30.122135: step 124980, total loss = 0.37, predict loss = 0.08 (71.6 examples/sec; 0.056 sec/batch; 91h:11m:28s remains)
INFO - root - 2019-11-04 02:19:30.738268: step 124990, total loss = 0.47, predict loss = 0.10 (71.2 examples/sec; 0.056 sec/batch; 91h:39m:46s remains)
INFO - root - 2019-11-04 02:19:31.362378: step 125000, total loss = 0.45, predict loss = 0.10 (69.1 examples/sec; 0.058 sec/batch; 94h:27m:59s remains)
INFO - root - 2019-11-04 02:19:31.974920: step 125010, total loss = 0.37, predict loss = 0.08 (75.7 examples/sec; 0.053 sec/batch; 86h:13m:34s remains)
INFO - root - 2019-11-04 02:19:32.574181: step 125020, total loss = 0.36, predict loss = 0.08 (71.9 examples/sec; 0.056 sec/batch; 90h:46m:48s remains)
INFO - root - 2019-11-04 02:19:33.196057: step 125030, total loss = 0.35, predict loss = 0.08 (68.7 examples/sec; 0.058 sec/batch; 94h:59m:51s remains)
INFO - root - 2019-11-04 02:19:33.813902: step 125040, total loss = 0.52, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 94h:25m:56s remains)
INFO - root - 2019-11-04 02:19:34.420799: step 125050, total loss = 0.46, predict loss = 0.11 (64.8 examples/sec; 0.062 sec/batch; 100h:44m:43s remains)
INFO - root - 2019-11-04 02:19:35.049292: step 125060, total loss = 0.44, predict loss = 0.10 (64.3 examples/sec; 0.062 sec/batch; 101h:26m:38s remains)
INFO - root - 2019-11-04 02:19:35.696942: step 125070, total loss = 0.40, predict loss = 0.09 (66.6 examples/sec; 0.060 sec/batch; 97h:59m:38s remains)
INFO - root - 2019-11-04 02:19:36.393457: step 125080, total loss = 0.37, predict loss = 0.09 (71.0 examples/sec; 0.056 sec/batch; 91h:55m:25s remains)
INFO - root - 2019-11-04 02:19:37.026410: step 125090, total loss = 0.49, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 87h:01m:55s remains)
INFO - root - 2019-11-04 02:19:37.640329: step 125100, total loss = 0.54, predict loss = 0.12 (74.2 examples/sec; 0.054 sec/batch; 87h:56m:29s remains)
INFO - root - 2019-11-04 02:19:38.286834: step 125110, total loss = 0.74, predict loss = 0.19 (67.6 examples/sec; 0.059 sec/batch; 96h:33m:38s remains)
INFO - root - 2019-11-04 02:19:38.944497: step 125120, total loss = 0.60, predict loss = 0.16 (67.5 examples/sec; 0.059 sec/batch; 96h:40m:11s remains)
INFO - root - 2019-11-04 02:19:39.568347: step 125130, total loss = 0.48, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 93h:02m:43s remains)
INFO - root - 2019-11-04 02:19:40.207624: step 125140, total loss = 0.27, predict loss = 0.06 (69.8 examples/sec; 0.057 sec/batch; 93h:27m:29s remains)
INFO - root - 2019-11-04 02:19:40.863748: step 125150, total loss = 0.45, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 97h:35m:39s remains)
INFO - root - 2019-11-04 02:19:41.530827: step 125160, total loss = 0.45, predict loss = 0.11 (67.1 examples/sec; 0.060 sec/batch; 97h:18m:08s remains)
INFO - root - 2019-11-04 02:19:42.204448: step 125170, total loss = 0.34, predict loss = 0.08 (65.3 examples/sec; 0.061 sec/batch; 99h:54m:25s remains)
INFO - root - 2019-11-04 02:19:42.809501: step 125180, total loss = 0.18, predict loss = 0.03 (69.6 examples/sec; 0.057 sec/batch; 93h:49m:13s remains)
INFO - root - 2019-11-04 02:19:43.426818: step 125190, total loss = 0.49, predict loss = 0.12 (78.3 examples/sec; 0.051 sec/batch; 83h:21m:44s remains)
INFO - root - 2019-11-04 02:19:44.055255: step 125200, total loss = 0.37, predict loss = 0.08 (72.1 examples/sec; 0.056 sec/batch; 90h:35m:32s remains)
INFO - root - 2019-11-04 02:19:44.695202: step 125210, total loss = 0.28, predict loss = 0.06 (65.6 examples/sec; 0.061 sec/batch; 99h:28m:04s remains)
INFO - root - 2019-11-04 02:19:45.379114: step 125220, total loss = 0.36, predict loss = 0.09 (62.3 examples/sec; 0.064 sec/batch; 104h:43m:31s remains)
INFO - root - 2019-11-04 02:19:45.980866: step 125230, total loss = 0.50, predict loss = 0.12 (69.8 examples/sec; 0.057 sec/batch; 93h:28m:53s remains)
INFO - root - 2019-11-04 02:19:46.578720: step 125240, total loss = 0.40, predict loss = 0.09 (83.8 examples/sec; 0.048 sec/batch; 77h:54m:55s remains)
INFO - root - 2019-11-04 02:19:47.212667: step 125250, total loss = 0.51, predict loss = 0.11 (64.7 examples/sec; 0.062 sec/batch; 100h:52m:29s remains)
INFO - root - 2019-11-04 02:19:47.834716: step 125260, total loss = 0.32, predict loss = 0.07 (73.3 examples/sec; 0.055 sec/batch; 89h:06m:20s remains)
INFO - root - 2019-11-04 02:19:48.465078: step 125270, total loss = 0.40, predict loss = 0.10 (81.4 examples/sec; 0.049 sec/batch; 80h:10m:00s remains)
INFO - root - 2019-11-04 02:19:49.084756: step 125280, total loss = 0.33, predict loss = 0.08 (71.4 examples/sec; 0.056 sec/batch; 91h:23m:25s remains)
INFO - root - 2019-11-04 02:19:49.696272: step 125290, total loss = 0.44, predict loss = 0.10 (65.8 examples/sec; 0.061 sec/batch; 99h:13m:12s remains)
INFO - root - 2019-11-04 02:19:50.311615: step 125300, total loss = 0.49, predict loss = 0.12 (75.3 examples/sec; 0.053 sec/batch; 86h:39m:54s remains)
INFO - root - 2019-11-04 02:19:50.952122: step 125310, total loss = 0.42, predict loss = 0.10 (71.1 examples/sec; 0.056 sec/batch; 91h:45m:18s remains)
INFO - root - 2019-11-04 02:19:51.588781: step 125320, total loss = 0.36, predict loss = 0.08 (82.2 examples/sec; 0.049 sec/batch; 79h:26m:08s remains)
INFO - root - 2019-11-04 02:19:52.213140: step 125330, total loss = 0.30, predict loss = 0.07 (77.7 examples/sec; 0.052 sec/batch; 84h:03m:02s remains)
INFO - root - 2019-11-04 02:19:52.838348: step 125340, total loss = 0.51, predict loss = 0.12 (73.7 examples/sec; 0.054 sec/batch; 88h:32m:55s remains)
INFO - root - 2019-11-04 02:19:53.417403: step 125350, total loss = 0.52, predict loss = 0.12 (79.1 examples/sec; 0.051 sec/batch; 82h:31m:25s remains)
INFO - root - 2019-11-04 02:19:54.054497: step 125360, total loss = 0.46, predict loss = 0.10 (68.4 examples/sec; 0.058 sec/batch; 95h:21m:39s remains)
INFO - root - 2019-11-04 02:19:54.698058: step 125370, total loss = 0.40, predict loss = 0.09 (62.8 examples/sec; 0.064 sec/batch; 103h:57m:32s remains)
INFO - root - 2019-11-04 02:19:55.335054: step 125380, total loss = 0.52, predict loss = 0.12 (72.9 examples/sec; 0.055 sec/batch; 89h:30m:17s remains)
INFO - root - 2019-11-04 02:19:55.946642: step 125390, total loss = 0.41, predict loss = 0.09 (74.2 examples/sec; 0.054 sec/batch; 87h:58m:38s remains)
INFO - root - 2019-11-04 02:19:56.584889: step 125400, total loss = 0.47, predict loss = 0.10 (68.7 examples/sec; 0.058 sec/batch; 94h:59m:01s remains)
INFO - root - 2019-11-04 02:19:57.199190: step 125410, total loss = 0.40, predict loss = 0.09 (68.2 examples/sec; 0.059 sec/batch; 95h:40m:17s remains)
INFO - root - 2019-11-04 02:19:57.822934: step 125420, total loss = 0.42, predict loss = 0.10 (70.4 examples/sec; 0.057 sec/batch; 92h:45m:00s remains)
INFO - root - 2019-11-04 02:19:58.467273: step 125430, total loss = 0.27, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 83h:19m:16s remains)
INFO - root - 2019-11-04 02:19:59.072374: step 125440, total loss = 0.24, predict loss = 0.05 (82.8 examples/sec; 0.048 sec/batch; 78h:50m:29s remains)
INFO - root - 2019-11-04 02:19:59.688183: step 125450, total loss = 0.35, predict loss = 0.08 (72.6 examples/sec; 0.055 sec/batch; 89h:53m:06s remains)
INFO - root - 2019-11-04 02:20:00.316574: step 125460, total loss = 0.51, predict loss = 0.12 (77.5 examples/sec; 0.052 sec/batch; 84h:15m:57s remains)
INFO - root - 2019-11-04 02:20:00.931728: step 125470, total loss = 0.42, predict loss = 0.10 (72.7 examples/sec; 0.055 sec/batch; 89h:46m:03s remains)
INFO - root - 2019-11-04 02:20:01.587307: step 125480, total loss = 0.53, predict loss = 0.13 (60.4 examples/sec; 0.066 sec/batch; 108h:01m:20s remains)
INFO - root - 2019-11-04 02:20:02.255721: step 125490, total loss = 0.50, predict loss = 0.12 (67.0 examples/sec; 0.060 sec/batch; 97h:25m:47s remains)
INFO - root - 2019-11-04 02:20:02.924984: step 125500, total loss = 0.47, predict loss = 0.10 (77.6 examples/sec; 0.052 sec/batch; 84h:04m:20s remains)
INFO - root - 2019-11-04 02:20:03.539504: step 125510, total loss = 0.62, predict loss = 0.15 (69.9 examples/sec; 0.057 sec/batch; 93h:20m:45s remains)
INFO - root - 2019-11-04 02:20:04.170953: step 125520, total loss = 0.66, predict loss = 0.16 (74.1 examples/sec; 0.054 sec/batch; 88h:06m:08s remains)
INFO - root - 2019-11-04 02:20:04.831311: step 125530, total loss = 0.65, predict loss = 0.15 (63.4 examples/sec; 0.063 sec/batch; 102h:55m:29s remains)
INFO - root - 2019-11-04 02:20:05.462907: step 125540, total loss = 0.68, predict loss = 0.16 (76.2 examples/sec; 0.053 sec/batch; 85h:40m:42s remains)
INFO - root - 2019-11-04 02:20:06.102566: step 125550, total loss = 0.61, predict loss = 0.14 (63.3 examples/sec; 0.063 sec/batch; 103h:10m:43s remains)
INFO - root - 2019-11-04 02:20:06.764538: step 125560, total loss = 0.52, predict loss = 0.12 (60.9 examples/sec; 0.066 sec/batch; 107h:08m:31s remains)
INFO - root - 2019-11-04 02:20:07.342946: step 125570, total loss = 0.96, predict loss = 0.23 (84.2 examples/sec; 0.048 sec/batch; 77h:30m:47s remains)
INFO - root - 2019-11-04 02:20:07.984561: step 125580, total loss = 0.72, predict loss = 0.17 (71.4 examples/sec; 0.056 sec/batch; 91h:24m:04s remains)
INFO - root - 2019-11-04 02:20:08.656684: step 125590, total loss = 0.69, predict loss = 0.16 (68.0 examples/sec; 0.059 sec/batch; 96h:00m:33s remains)
INFO - root - 2019-11-04 02:20:09.290075: step 125600, total loss = 0.64, predict loss = 0.16 (82.1 examples/sec; 0.049 sec/batch; 79h:30m:04s remains)
INFO - root - 2019-11-04 02:20:09.930454: step 125610, total loss = 0.54, predict loss = 0.13 (75.2 examples/sec; 0.053 sec/batch; 86h:46m:32s remains)
INFO - root - 2019-11-04 02:20:10.562207: step 125620, total loss = 0.40, predict loss = 0.09 (72.9 examples/sec; 0.055 sec/batch; 89h:33m:25s remains)
INFO - root - 2019-11-04 02:20:11.183671: step 125630, total loss = 0.47, predict loss = 0.11 (67.1 examples/sec; 0.060 sec/batch; 97h:16m:06s remains)
INFO - root - 2019-11-04 02:20:11.813177: step 125640, total loss = 0.43, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 98h:08m:30s remains)
INFO - root - 2019-11-04 02:20:12.457958: step 125650, total loss = 0.49, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 94h:14m:34s remains)
INFO - root - 2019-11-04 02:20:13.108261: step 125660, total loss = 0.53, predict loss = 0.12 (79.0 examples/sec; 0.051 sec/batch; 82h:36m:42s remains)
INFO - root - 2019-11-04 02:20:13.741648: step 125670, total loss = 0.53, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 97h:30m:19s remains)
INFO - root - 2019-11-04 02:20:14.402610: step 125680, total loss = 0.52, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 96h:06m:14s remains)
INFO - root - 2019-11-04 02:20:15.029526: step 125690, total loss = 0.54, predict loss = 0.13 (76.7 examples/sec; 0.052 sec/batch; 85h:07m:59s remains)
INFO - root - 2019-11-04 02:20:15.651949: step 125700, total loss = 0.52, predict loss = 0.12 (78.3 examples/sec; 0.051 sec/batch; 83h:22m:20s remains)
INFO - root - 2019-11-04 02:20:16.292104: step 125710, total loss = 0.54, predict loss = 0.13 (62.7 examples/sec; 0.064 sec/batch; 104h:02m:45s remains)
INFO - root - 2019-11-04 02:20:16.928785: step 125720, total loss = 0.58, predict loss = 0.13 (63.9 examples/sec; 0.063 sec/batch; 102h:12m:32s remains)
INFO - root - 2019-11-04 02:20:17.569674: step 125730, total loss = 0.43, predict loss = 0.09 (76.0 examples/sec; 0.053 sec/batch; 85h:53m:20s remains)
INFO - root - 2019-11-04 02:20:18.237039: step 125740, total loss = 0.39, predict loss = 0.08 (71.0 examples/sec; 0.056 sec/batch; 91h:56m:37s remains)
INFO - root - 2019-11-04 02:20:18.841407: step 125750, total loss = 0.59, predict loss = 0.13 (77.1 examples/sec; 0.052 sec/batch; 84h:38m:52s remains)
INFO - root - 2019-11-04 02:20:19.446292: step 125760, total loss = 0.35, predict loss = 0.07 (64.0 examples/sec; 0.062 sec/batch; 101h:57m:34s remains)
INFO - root - 2019-11-04 02:20:20.046898: step 125770, total loss = 0.52, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 91h:55m:13s remains)
INFO - root - 2019-11-04 02:20:20.654627: step 125780, total loss = 0.28, predict loss = 0.06 (80.0 examples/sec; 0.050 sec/batch; 81h:33m:42s remains)
INFO - root - 2019-11-04 02:20:21.243582: step 125790, total loss = 0.27, predict loss = 0.06 (79.4 examples/sec; 0.050 sec/batch; 82h:12m:38s remains)
INFO - root - 2019-11-04 02:20:21.853626: step 125800, total loss = 0.29, predict loss = 0.06 (76.5 examples/sec; 0.052 sec/batch; 85h:16m:58s remains)
INFO - root - 2019-11-04 02:20:22.481388: step 125810, total loss = 0.49, predict loss = 0.12 (67.0 examples/sec; 0.060 sec/batch; 97h:25m:20s remains)
INFO - root - 2019-11-04 02:20:23.081285: step 125820, total loss = 0.37, predict loss = 0.08 (78.4 examples/sec; 0.051 sec/batch; 83h:14m:34s remains)
INFO - root - 2019-11-04 02:20:23.674403: step 125830, total loss = 0.40, predict loss = 0.09 (65.1 examples/sec; 0.061 sec/batch; 100h:17m:34s remains)
INFO - root - 2019-11-04 02:20:24.288412: step 125840, total loss = 0.37, predict loss = 0.09 (67.1 examples/sec; 0.060 sec/batch; 97h:19m:30s remains)
INFO - root - 2019-11-04 02:20:24.905279: step 125850, total loss = 0.43, predict loss = 0.10 (64.1 examples/sec; 0.062 sec/batch; 101h:47m:11s remains)
INFO - root - 2019-11-04 02:20:25.532840: step 125860, total loss = 0.39, predict loss = 0.09 (72.9 examples/sec; 0.055 sec/batch; 89h:28m:12s remains)
INFO - root - 2019-11-04 02:20:26.187826: step 125870, total loss = 0.39, predict loss = 0.09 (66.6 examples/sec; 0.060 sec/batch; 97h:58m:44s remains)
INFO - root - 2019-11-04 02:20:26.856805: step 125880, total loss = 0.44, predict loss = 0.10 (65.2 examples/sec; 0.061 sec/batch; 100h:08m:46s remains)
INFO - root - 2019-11-04 02:20:27.446456: step 125890, total loss = 0.43, predict loss = 0.10 (82.4 examples/sec; 0.049 sec/batch; 79h:14m:28s remains)
INFO - root - 2019-11-04 02:20:28.061415: step 125900, total loss = 0.49, predict loss = 0.12 (65.4 examples/sec; 0.061 sec/batch; 99h:50m:15s remains)
INFO - root - 2019-11-04 02:20:28.708135: step 125910, total loss = 0.47, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 98h:11m:22s remains)
INFO - root - 2019-11-04 02:20:29.343361: step 125920, total loss = 0.54, predict loss = 0.12 (75.5 examples/sec; 0.053 sec/batch; 86h:27m:47s remains)
INFO - root - 2019-11-04 02:20:29.970550: step 125930, total loss = 0.68, predict loss = 0.16 (73.0 examples/sec; 0.055 sec/batch; 89h:22m:09s remains)
INFO - root - 2019-11-04 02:20:30.569301: step 125940, total loss = 0.44, predict loss = 0.10 (86.8 examples/sec; 0.046 sec/batch; 75h:11m:03s remains)
INFO - root - 2019-11-04 02:20:31.143007: step 125950, total loss = 0.56, predict loss = 0.13 (84.1 examples/sec; 0.048 sec/batch; 77h:34m:21s remains)
INFO - root - 2019-11-04 02:20:31.738500: step 125960, total loss = 0.51, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 94h:49m:52s remains)
INFO - root - 2019-11-04 02:20:32.361245: step 125970, total loss = 0.61, predict loss = 0.15 (62.7 examples/sec; 0.064 sec/batch; 104h:03m:07s remains)
INFO - root - 2019-11-04 02:20:33.004073: step 125980, total loss = 0.49, predict loss = 0.11 (73.6 examples/sec; 0.054 sec/batch; 88h:40m:28s remains)
INFO - root - 2019-11-04 02:20:33.625979: step 125990, total loss = 0.64, predict loss = 0.16 (77.0 examples/sec; 0.052 sec/batch; 84h:44m:32s remains)
INFO - root - 2019-11-04 02:20:34.252685: step 126000, total loss = 0.65, predict loss = 0.16 (66.3 examples/sec; 0.060 sec/batch; 98h:26m:51s remains)
INFO - root - 2019-11-04 02:20:34.936326: step 126010, total loss = 0.73, predict loss = 0.17 (60.6 examples/sec; 0.066 sec/batch; 107h:46m:00s remains)
INFO - root - 2019-11-04 02:20:35.575097: step 126020, total loss = 0.54, predict loss = 0.12 (77.1 examples/sec; 0.052 sec/batch; 84h:36m:58s remains)
INFO - root - 2019-11-04 02:20:36.198761: step 126030, total loss = 0.50, predict loss = 0.12 (72.1 examples/sec; 0.055 sec/batch; 90h:27m:59s remains)
INFO - root - 2019-11-04 02:20:36.794521: step 126040, total loss = 0.42, predict loss = 0.10 (80.8 examples/sec; 0.050 sec/batch; 80h:47m:59s remains)
INFO - root - 2019-11-04 02:20:37.420232: step 126050, total loss = 0.52, predict loss = 0.12 (77.3 examples/sec; 0.052 sec/batch; 84h:25m:34s remains)
INFO - root - 2019-11-04 02:20:38.056010: step 126060, total loss = 0.47, predict loss = 0.10 (73.6 examples/sec; 0.054 sec/batch; 88h:37m:59s remains)
INFO - root - 2019-11-04 02:20:38.710081: step 126070, total loss = 0.52, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 94h:54m:31s remains)
INFO - root - 2019-11-04 02:20:39.346973: step 126080, total loss = 0.44, predict loss = 0.10 (63.3 examples/sec; 0.063 sec/batch; 103h:07m:38s remains)
INFO - root - 2019-11-04 02:20:39.939438: step 126090, total loss = 0.43, predict loss = 0.09 (75.2 examples/sec; 0.053 sec/batch; 86h:48m:45s remains)
INFO - root - 2019-11-04 02:20:40.548633: step 126100, total loss = 0.45, predict loss = 0.10 (75.7 examples/sec; 0.053 sec/batch; 86h:10m:00s remains)
INFO - root - 2019-11-04 02:20:41.169159: step 126110, total loss = 0.37, predict loss = 0.09 (68.9 examples/sec; 0.058 sec/batch; 94h:39m:42s remains)
INFO - root - 2019-11-04 02:20:41.816742: step 126120, total loss = 0.52, predict loss = 0.12 (61.8 examples/sec; 0.065 sec/batch; 105h:32m:43s remains)
INFO - root - 2019-11-04 02:20:42.504935: step 126130, total loss = 0.32, predict loss = 0.07 (63.8 examples/sec; 0.063 sec/batch; 102h:15m:30s remains)
INFO - root - 2019-11-04 02:20:43.121674: step 126140, total loss = 0.48, predict loss = 0.12 (80.9 examples/sec; 0.049 sec/batch; 80h:40m:50s remains)
INFO - root - 2019-11-04 02:20:43.775584: step 126150, total loss = 0.50, predict loss = 0.12 (73.8 examples/sec; 0.054 sec/batch; 88h:24m:22s remains)
INFO - root - 2019-11-04 02:20:44.414493: step 126160, total loss = 0.61, predict loss = 0.15 (70.5 examples/sec; 0.057 sec/batch; 92h:30m:53s remains)
INFO - root - 2019-11-04 02:20:45.062569: step 126170, total loss = 0.55, predict loss = 0.13 (67.9 examples/sec; 0.059 sec/batch; 96h:03m:38s remains)
INFO - root - 2019-11-04 02:20:45.692691: step 126180, total loss = 0.59, predict loss = 0.14 (72.8 examples/sec; 0.055 sec/batch; 89h:37m:34s remains)
INFO - root - 2019-11-04 02:20:46.338765: step 126190, total loss = 0.57, predict loss = 0.13 (66.9 examples/sec; 0.060 sec/batch; 97h:34m:20s remains)
INFO - root - 2019-11-04 02:20:46.952095: step 126200, total loss = 0.58, predict loss = 0.14 (71.9 examples/sec; 0.056 sec/batch; 90h:48m:02s remains)
INFO - root - 2019-11-04 02:20:47.550869: step 126210, total loss = 0.66, predict loss = 0.15 (71.3 examples/sec; 0.056 sec/batch; 91h:28m:37s remains)
INFO - root - 2019-11-04 02:20:48.152996: step 126220, total loss = 0.49, predict loss = 0.12 (74.5 examples/sec; 0.054 sec/batch; 87h:36m:54s remains)
INFO - root - 2019-11-04 02:20:48.750411: step 126230, total loss = 0.45, predict loss = 0.10 (78.0 examples/sec; 0.051 sec/batch; 83h:39m:20s remains)
INFO - root - 2019-11-04 02:20:49.354748: step 126240, total loss = 0.55, predict loss = 0.13 (65.0 examples/sec; 0.062 sec/batch; 100h:23m:54s remains)
INFO - root - 2019-11-04 02:20:49.969815: step 126250, total loss = 0.53, predict loss = 0.12 (81.9 examples/sec; 0.049 sec/batch; 79h:41m:45s remains)
INFO - root - 2019-11-04 02:20:50.584012: step 126260, total loss = 0.54, predict loss = 0.13 (79.2 examples/sec; 0.050 sec/batch; 82h:23m:34s remains)
INFO - root - 2019-11-04 02:20:51.184913: step 126270, total loss = 0.55, predict loss = 0.13 (77.0 examples/sec; 0.052 sec/batch; 84h:48m:19s remains)
INFO - root - 2019-11-04 02:20:51.775970: step 126280, total loss = 0.56, predict loss = 0.13 (79.3 examples/sec; 0.050 sec/batch; 82h:19m:38s remains)
INFO - root - 2019-11-04 02:20:52.384239: step 126290, total loss = 0.53, predict loss = 0.12 (74.2 examples/sec; 0.054 sec/batch; 88h:00m:31s remains)
INFO - root - 2019-11-04 02:20:52.984420: step 126300, total loss = 0.38, predict loss = 0.09 (68.5 examples/sec; 0.058 sec/batch; 95h:12m:40s remains)
INFO - root - 2019-11-04 02:20:53.608671: step 126310, total loss = 0.55, predict loss = 0.13 (67.9 examples/sec; 0.059 sec/batch; 96h:11m:13s remains)
INFO - root - 2019-11-04 02:20:54.297101: step 126320, total loss = 0.61, predict loss = 0.15 (57.5 examples/sec; 0.070 sec/batch; 113h:34m:47s remains)
INFO - root - 2019-11-04 02:20:54.913197: step 126330, total loss = 0.53, predict loss = 0.12 (79.5 examples/sec; 0.050 sec/batch; 82h:05m:16s remains)
INFO - root - 2019-11-04 02:20:55.540407: step 126340, total loss = 0.47, predict loss = 0.11 (63.8 examples/sec; 0.063 sec/batch; 102h:20m:00s remains)
INFO - root - 2019-11-04 02:20:56.162183: step 126350, total loss = 0.33, predict loss = 0.07 (82.1 examples/sec; 0.049 sec/batch; 79h:31m:26s remains)
INFO - root - 2019-11-04 02:20:56.805307: step 126360, total loss = 0.38, predict loss = 0.08 (63.0 examples/sec; 0.063 sec/batch; 103h:35m:46s remains)
INFO - root - 2019-11-04 02:20:57.392085: step 126370, total loss = 0.32, predict loss = 0.07 (82.5 examples/sec; 0.048 sec/batch; 79h:04m:24s remains)
INFO - root - 2019-11-04 02:20:58.033895: step 126380, total loss = 0.39, predict loss = 0.09 (74.4 examples/sec; 0.054 sec/batch; 87h:43m:29s remains)
INFO - root - 2019-11-04 02:20:58.648375: step 126390, total loss = 0.45, predict loss = 0.10 (75.8 examples/sec; 0.053 sec/batch; 86h:08m:28s remains)
INFO - root - 2019-11-04 02:20:59.263816: step 126400, total loss = 0.44, predict loss = 0.10 (75.3 examples/sec; 0.053 sec/batch; 86h:38m:10s remains)
INFO - root - 2019-11-04 02:20:59.903886: step 126410, total loss = 0.42, predict loss = 0.10 (66.9 examples/sec; 0.060 sec/batch; 97h:32m:37s remains)
INFO - root - 2019-11-04 02:21:00.529056: step 126420, total loss = 0.50, predict loss = 0.11 (76.0 examples/sec; 0.053 sec/batch; 85h:50m:18s remains)
INFO - root - 2019-11-04 02:21:01.155160: step 126430, total loss = 0.46, predict loss = 0.11 (66.9 examples/sec; 0.060 sec/batch; 97h:31m:08s remains)
INFO - root - 2019-11-04 02:21:01.872194: step 126440, total loss = 0.58, predict loss = 0.14 (62.9 examples/sec; 0.064 sec/batch; 103h:48m:34s remains)
INFO - root - 2019-11-04 02:21:02.451626: step 126450, total loss = 0.48, predict loss = 0.11 (87.2 examples/sec; 0.046 sec/batch; 74h:53m:01s remains)
INFO - root - 2019-11-04 02:21:03.093569: step 126460, total loss = 0.50, predict loss = 0.12 (74.0 examples/sec; 0.054 sec/batch; 88h:13m:31s remains)
INFO - root - 2019-11-04 02:21:03.719193: step 126470, total loss = 0.55, predict loss = 0.12 (76.1 examples/sec; 0.053 sec/batch; 85h:42m:12s remains)
INFO - root - 2019-11-04 02:21:04.309773: step 126480, total loss = 0.46, predict loss = 0.10 (77.0 examples/sec; 0.052 sec/batch; 84h:46m:49s remains)
INFO - root - 2019-11-04 02:21:04.886765: step 126490, total loss = 0.44, predict loss = 0.11 (82.7 examples/sec; 0.048 sec/batch; 78h:56m:27s remains)
INFO - root - 2019-11-04 02:21:05.511075: step 126500, total loss = 0.58, predict loss = 0.15 (62.5 examples/sec; 0.064 sec/batch; 104h:25m:11s remains)
INFO - root - 2019-11-04 02:21:06.188776: step 126510, total loss = 0.51, predict loss = 0.13 (70.2 examples/sec; 0.057 sec/batch; 92h:54m:20s remains)
INFO - root - 2019-11-04 02:21:06.810035: step 126520, total loss = 0.52, predict loss = 0.12 (75.2 examples/sec; 0.053 sec/batch; 86h:46m:44s remains)
INFO - root - 2019-11-04 02:21:07.470376: step 126530, total loss = 0.55, predict loss = 0.12 (64.6 examples/sec; 0.062 sec/batch; 100h:58m:10s remains)
INFO - root - 2019-11-04 02:21:08.096237: step 126540, total loss = 0.59, predict loss = 0.14 (74.1 examples/sec; 0.054 sec/batch; 88h:05m:41s remains)
INFO - root - 2019-11-04 02:21:08.741057: step 126550, total loss = 0.44, predict loss = 0.10 (73.8 examples/sec; 0.054 sec/batch; 88h:25m:16s remains)
INFO - root - 2019-11-04 02:21:09.366653: step 126560, total loss = 0.71, predict loss = 0.17 (68.0 examples/sec; 0.059 sec/batch; 95h:54m:21s remains)
INFO - root - 2019-11-04 02:21:10.005930: step 126570, total loss = 0.62, predict loss = 0.15 (82.3 examples/sec; 0.049 sec/batch; 79h:18m:17s remains)
INFO - root - 2019-11-04 02:21:10.597961: step 126580, total loss = 0.64, predict loss = 0.15 (83.3 examples/sec; 0.048 sec/batch; 78h:20m:59s remains)
INFO - root - 2019-11-04 02:21:11.211336: step 126590, total loss = 0.61, predict loss = 0.15 (71.4 examples/sec; 0.056 sec/batch; 91h:23m:55s remains)
INFO - root - 2019-11-04 02:21:11.850413: step 126600, total loss = 0.55, predict loss = 0.13 (63.4 examples/sec; 0.063 sec/batch; 102h:58m:55s remains)
INFO - root - 2019-11-04 02:21:12.505903: step 126610, total loss = 0.56, predict loss = 0.13 (79.6 examples/sec; 0.050 sec/batch; 81h:59m:23s remains)
INFO - root - 2019-11-04 02:21:13.171160: step 126620, total loss = 0.64, predict loss = 0.15 (63.9 examples/sec; 0.063 sec/batch; 102h:11m:09s remains)
INFO - root - 2019-11-04 02:21:13.808183: step 126630, total loss = 0.70, predict loss = 0.17 (70.1 examples/sec; 0.057 sec/batch; 93h:05m:31s remains)
INFO - root - 2019-11-04 02:21:14.430640: step 126640, total loss = 0.72, predict loss = 0.18 (83.6 examples/sec; 0.048 sec/batch; 78h:05m:02s remains)
INFO - root - 2019-11-04 02:21:15.027996: step 126650, total loss = 0.65, predict loss = 0.16 (80.1 examples/sec; 0.050 sec/batch; 81h:28m:37s remains)
INFO - root - 2019-11-04 02:21:15.641069: step 126660, total loss = 0.41, predict loss = 0.09 (73.0 examples/sec; 0.055 sec/batch; 89h:26m:32s remains)
INFO - root - 2019-11-04 02:21:16.229208: step 126670, total loss = 0.66, predict loss = 0.16 (84.7 examples/sec; 0.047 sec/batch; 77h:03m:07s remains)
INFO - root - 2019-11-04 02:21:16.830184: step 126680, total loss = 0.51, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 92h:27m:19s remains)
INFO - root - 2019-11-04 02:21:17.442747: step 126690, total loss = 0.63, predict loss = 0.15 (75.6 examples/sec; 0.053 sec/batch; 86h:18m:26s remains)
INFO - root - 2019-11-04 02:21:18.090668: step 126700, total loss = 0.47, predict loss = 0.11 (67.6 examples/sec; 0.059 sec/batch; 96h:28m:52s remains)
INFO - root - 2019-11-04 02:21:18.729560: step 126710, total loss = 0.54, predict loss = 0.12 (71.9 examples/sec; 0.056 sec/batch; 90h:47m:00s remains)
INFO - root - 2019-11-04 02:21:19.347939: step 126720, total loss = 0.61, predict loss = 0.14 (71.8 examples/sec; 0.056 sec/batch; 90h:49m:55s remains)
INFO - root - 2019-11-04 02:21:19.943011: step 126730, total loss = 0.44, predict loss = 0.10 (72.6 examples/sec; 0.055 sec/batch; 89h:53m:17s remains)
INFO - root - 2019-11-04 02:21:20.591492: step 126740, total loss = 0.53, predict loss = 0.13 (64.5 examples/sec; 0.062 sec/batch; 101h:10m:30s remains)
INFO - root - 2019-11-04 02:21:21.230262: step 126750, total loss = 0.67, predict loss = 0.15 (76.4 examples/sec; 0.052 sec/batch; 85h:25m:44s remains)
INFO - root - 2019-11-04 02:21:21.841640: step 126760, total loss = 0.58, predict loss = 0.13 (78.9 examples/sec; 0.051 sec/batch; 82h:40m:53s remains)
INFO - root - 2019-11-04 02:21:22.439639: step 126770, total loss = 0.58, predict loss = 0.13 (68.6 examples/sec; 0.058 sec/batch; 95h:09m:48s remains)
INFO - root - 2019-11-04 02:21:23.072305: step 126780, total loss = 0.40, predict loss = 0.09 (77.5 examples/sec; 0.052 sec/batch; 84h:09m:56s remains)
INFO - root - 2019-11-04 02:21:23.732431: step 126790, total loss = 0.38, predict loss = 0.08 (64.9 examples/sec; 0.062 sec/batch; 100h:30m:58s remains)
INFO - root - 2019-11-04 02:21:24.323617: step 126800, total loss = 0.50, predict loss = 0.11 (85.2 examples/sec; 0.047 sec/batch; 76h:34m:13s remains)
INFO - root - 2019-11-04 02:21:24.794099: step 126810, total loss = 0.45, predict loss = 0.10 (86.4 examples/sec; 0.046 sec/batch; 75h:29m:16s remains)
INFO - root - 2019-11-04 02:21:25.780483: step 126820, total loss = 0.27, predict loss = 0.06 (7.0 examples/sec; 0.575 sec/batch; 938h:40m:43s remains)
INFO - root - 2019-11-04 02:21:26.356521: step 126830, total loss = 0.56, predict loss = 0.13 (84.9 examples/sec; 0.047 sec/batch; 76h:54m:14s remains)
INFO - root - 2019-11-04 02:21:26.974472: step 126840, total loss = 0.51, predict loss = 0.13 (70.9 examples/sec; 0.056 sec/batch; 92h:01m:53s remains)
INFO - root - 2019-11-04 02:21:27.588757: step 126850, total loss = 0.53, predict loss = 0.13 (76.9 examples/sec; 0.052 sec/batch; 84h:50m:13s remains)
INFO - root - 2019-11-04 02:21:28.215555: step 126860, total loss = 0.66, predict loss = 0.16 (74.7 examples/sec; 0.054 sec/batch; 87h:24m:41s remains)
INFO - root - 2019-11-04 02:21:28.870877: step 126870, total loss = 0.47, predict loss = 0.11 (73.2 examples/sec; 0.055 sec/batch; 89h:09m:43s remains)
INFO - root - 2019-11-04 02:21:29.498752: step 126880, total loss = 0.60, predict loss = 0.14 (66.1 examples/sec; 0.061 sec/batch; 98h:43m:38s remains)
INFO - root - 2019-11-04 02:21:30.108149: step 126890, total loss = 0.70, predict loss = 0.16 (75.4 examples/sec; 0.053 sec/batch; 86h:31m:45s remains)
INFO - root - 2019-11-04 02:21:30.737034: step 126900, total loss = 0.65, predict loss = 0.15 (77.5 examples/sec; 0.052 sec/batch; 84h:12m:42s remains)
INFO - root - 2019-11-04 02:21:31.373146: step 126910, total loss = 0.53, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 98h:30m:14s remains)
INFO - root - 2019-11-04 02:21:32.017104: step 126920, total loss = 0.55, predict loss = 0.12 (73.2 examples/sec; 0.055 sec/batch; 89h:08m:46s remains)
INFO - root - 2019-11-04 02:21:32.606180: step 126930, total loss = 0.45, predict loss = 0.10 (74.6 examples/sec; 0.054 sec/batch; 87h:28m:09s remains)
INFO - root - 2019-11-04 02:21:33.270075: step 126940, total loss = 0.57, predict loss = 0.14 (58.1 examples/sec; 0.069 sec/batch; 112h:23m:52s remains)
INFO - root - 2019-11-04 02:21:33.896622: step 126950, total loss = 0.43, predict loss = 0.09 (67.2 examples/sec; 0.059 sec/batch; 97h:02m:14s remains)
INFO - root - 2019-11-04 02:21:34.517522: step 126960, total loss = 0.54, predict loss = 0.13 (68.7 examples/sec; 0.058 sec/batch; 95h:01m:58s remains)
INFO - root - 2019-11-04 02:21:35.111145: step 126970, total loss = 0.46, predict loss = 0.10 (71.0 examples/sec; 0.056 sec/batch; 91h:51m:23s remains)
INFO - root - 2019-11-04 02:21:35.709945: step 126980, total loss = 0.41, predict loss = 0.09 (74.6 examples/sec; 0.054 sec/batch; 87h:26m:13s remains)
INFO - root - 2019-11-04 02:21:36.311783: step 126990, total loss = 0.49, predict loss = 0.11 (74.2 examples/sec; 0.054 sec/batch; 87h:59m:40s remains)
INFO - root - 2019-11-04 02:21:36.934076: step 127000, total loss = 0.45, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 96h:27m:05s remains)
INFO - root - 2019-11-04 02:21:37.532381: step 127010, total loss = 0.45, predict loss = 0.10 (77.4 examples/sec; 0.052 sec/batch; 84h:15m:50s remains)
INFO - root - 2019-11-04 02:21:38.128328: step 127020, total loss = 0.48, predict loss = 0.11 (72.7 examples/sec; 0.055 sec/batch; 89h:47m:58s remains)
INFO - root - 2019-11-04 02:21:38.734661: step 127030, total loss = 0.33, predict loss = 0.07 (73.2 examples/sec; 0.055 sec/batch; 89h:08m:36s remains)
INFO - root - 2019-11-04 02:21:39.349252: step 127040, total loss = 0.43, predict loss = 0.10 (71.3 examples/sec; 0.056 sec/batch; 91h:31m:47s remains)
INFO - root - 2019-11-04 02:21:39.988527: step 127050, total loss = 0.41, predict loss = 0.09 (65.4 examples/sec; 0.061 sec/batch; 99h:45m:18s remains)
INFO - root - 2019-11-04 02:21:40.677214: step 127060, total loss = 0.48, predict loss = 0.10 (62.8 examples/sec; 0.064 sec/batch; 103h:49m:53s remains)
INFO - root - 2019-11-04 02:21:41.350597: step 127070, total loss = 0.48, predict loss = 0.11 (66.8 examples/sec; 0.060 sec/batch; 97h:38m:27s remains)
INFO - root - 2019-11-04 02:21:41.982709: step 127080, total loss = 0.65, predict loss = 0.16 (66.9 examples/sec; 0.060 sec/batch; 97h:30m:08s remains)
INFO - root - 2019-11-04 02:21:42.608860: step 127090, total loss = 0.49, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 88h:16m:15s remains)
INFO - root - 2019-11-04 02:21:43.219307: step 127100, total loss = 0.58, predict loss = 0.13 (81.3 examples/sec; 0.049 sec/batch; 80h:13m:21s remains)
INFO - root - 2019-11-04 02:21:43.872593: step 127110, total loss = 0.67, predict loss = 0.16 (69.4 examples/sec; 0.058 sec/batch; 93h:58m:18s remains)
INFO - root - 2019-11-04 02:21:44.545111: step 127120, total loss = 0.72, predict loss = 0.17 (68.2 examples/sec; 0.059 sec/batch; 95h:39m:12s remains)
INFO - root - 2019-11-04 02:21:45.180752: step 127130, total loss = 0.64, predict loss = 0.16 (71.0 examples/sec; 0.056 sec/batch; 91h:52m:55s remains)
INFO - root - 2019-11-04 02:21:45.820428: step 127140, total loss = 0.63, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 90h:20m:18s remains)
INFO - root - 2019-11-04 02:21:46.460092: step 127150, total loss = 0.62, predict loss = 0.14 (61.1 examples/sec; 0.065 sec/batch; 106h:49m:30s remains)
INFO - root - 2019-11-04 02:21:47.176328: step 127160, total loss = 0.35, predict loss = 0.08 (68.9 examples/sec; 0.058 sec/batch; 94h:45m:41s remains)
INFO - root - 2019-11-04 02:21:47.843539: step 127170, total loss = 0.55, predict loss = 0.13 (62.6 examples/sec; 0.064 sec/batch; 104h:18m:30s remains)
INFO - root - 2019-11-04 02:21:48.530859: step 127180, total loss = 0.43, predict loss = 0.09 (62.0 examples/sec; 0.064 sec/batch; 105h:10m:25s remains)
INFO - root - 2019-11-04 02:21:49.176191: step 127190, total loss = 0.48, predict loss = 0.11 (69.0 examples/sec; 0.058 sec/batch; 94h:30m:17s remains)
INFO - root - 2019-11-04 02:21:49.768895: step 127200, total loss = 0.50, predict loss = 0.12 (63.8 examples/sec; 0.063 sec/batch; 102h:19m:20s remains)
INFO - root - 2019-11-04 02:21:50.393078: step 127210, total loss = 0.38, predict loss = 0.08 (72.2 examples/sec; 0.055 sec/batch; 90h:21m:27s remains)
INFO - root - 2019-11-04 02:21:51.044286: step 127220, total loss = 0.57, predict loss = 0.13 (71.6 examples/sec; 0.056 sec/batch; 91h:06m:53s remains)
INFO - root - 2019-11-04 02:21:51.705624: step 127230, total loss = 0.56, predict loss = 0.14 (65.4 examples/sec; 0.061 sec/batch; 99h:49m:44s remains)
INFO - root - 2019-11-04 02:21:52.375863: step 127240, total loss = 0.43, predict loss = 0.09 (73.8 examples/sec; 0.054 sec/batch; 88h:26m:48s remains)
INFO - root - 2019-11-04 02:21:53.020967: step 127250, total loss = 0.46, predict loss = 0.11 (71.5 examples/sec; 0.056 sec/batch; 91h:19m:26s remains)
INFO - root - 2019-11-04 02:21:53.672036: step 127260, total loss = 0.49, predict loss = 0.12 (65.8 examples/sec; 0.061 sec/batch; 99h:07m:05s remains)
INFO - root - 2019-11-04 02:21:54.365118: step 127270, total loss = 0.78, predict loss = 0.20 (61.0 examples/sec; 0.066 sec/batch; 106h:58m:41s remains)
INFO - root - 2019-11-04 02:21:55.020030: step 127280, total loss = 0.44, predict loss = 0.11 (71.4 examples/sec; 0.056 sec/batch; 91h:20m:20s remains)
INFO - root - 2019-11-04 02:21:55.645966: step 127290, total loss = 0.38, predict loss = 0.09 (65.8 examples/sec; 0.061 sec/batch; 99h:11m:54s remains)
INFO - root - 2019-11-04 02:21:56.246617: step 127300, total loss = 0.42, predict loss = 0.10 (67.4 examples/sec; 0.059 sec/batch; 96h:51m:20s remains)
INFO - root - 2019-11-04 02:21:56.836350: step 127310, total loss = 0.50, predict loss = 0.13 (69.6 examples/sec; 0.057 sec/batch; 93h:44m:07s remains)
INFO - root - 2019-11-04 02:21:57.504016: step 127320, total loss = 0.52, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 98h:35m:57s remains)
INFO - root - 2019-11-04 02:21:58.166786: step 127330, total loss = 0.33, predict loss = 0.08 (73.1 examples/sec; 0.055 sec/batch; 89h:14m:22s remains)
INFO - root - 2019-11-04 02:21:58.814954: step 127340, total loss = 0.46, predict loss = 0.10 (76.2 examples/sec; 0.053 sec/batch; 85h:40m:28s remains)
INFO - root - 2019-11-04 02:21:59.450719: step 127350, total loss = 0.57, predict loss = 0.14 (69.4 examples/sec; 0.058 sec/batch; 93h:59m:22s remains)
INFO - root - 2019-11-04 02:22:00.094846: step 127360, total loss = 0.49, predict loss = 0.12 (75.4 examples/sec; 0.053 sec/batch; 86h:35m:21s remains)
INFO - root - 2019-11-04 02:22:00.715935: step 127370, total loss = 0.47, predict loss = 0.11 (62.8 examples/sec; 0.064 sec/batch; 103h:55m:25s remains)
INFO - root - 2019-11-04 02:22:01.339545: step 127380, total loss = 0.36, predict loss = 0.08 (65.6 examples/sec; 0.061 sec/batch; 99h:24m:47s remains)
INFO - root - 2019-11-04 02:22:02.006624: step 127390, total loss = 0.43, predict loss = 0.11 (62.8 examples/sec; 0.064 sec/batch; 103h:51m:47s remains)
INFO - root - 2019-11-04 02:22:02.664458: step 127400, total loss = 0.33, predict loss = 0.07 (63.2 examples/sec; 0.063 sec/batch; 103h:17m:32s remains)
INFO - root - 2019-11-04 02:22:03.308389: step 127410, total loss = 0.48, predict loss = 0.12 (65.1 examples/sec; 0.061 sec/batch; 100h:15m:57s remains)
INFO - root - 2019-11-04 02:22:03.964117: step 127420, total loss = 0.38, predict loss = 0.09 (66.6 examples/sec; 0.060 sec/batch; 98h:00m:50s remains)
INFO - root - 2019-11-04 02:22:04.595966: step 127430, total loss = 0.31, predict loss = 0.07 (69.4 examples/sec; 0.058 sec/batch; 94h:04m:14s remains)
INFO - root - 2019-11-04 02:22:05.233840: step 127440, total loss = 0.30, predict loss = 0.06 (66.4 examples/sec; 0.060 sec/batch; 98h:12m:45s remains)
INFO - root - 2019-11-04 02:22:05.834540: step 127450, total loss = 0.26, predict loss = 0.05 (66.3 examples/sec; 0.060 sec/batch; 98h:27m:07s remains)
INFO - root - 2019-11-04 02:22:06.476279: step 127460, total loss = 0.26, predict loss = 0.06 (64.1 examples/sec; 0.062 sec/batch; 101h:44m:08s remains)
INFO - root - 2019-11-04 02:22:07.117238: step 127470, total loss = 0.36, predict loss = 0.08 (75.4 examples/sec; 0.053 sec/batch; 86h:32m:05s remains)
INFO - root - 2019-11-04 02:22:07.769108: step 127480, total loss = 0.41, predict loss = 0.09 (70.7 examples/sec; 0.057 sec/batch; 92h:13m:35s remains)
INFO - root - 2019-11-04 02:22:08.400272: step 127490, total loss = 0.47, predict loss = 0.11 (63.3 examples/sec; 0.063 sec/batch; 103h:09m:09s remains)
INFO - root - 2019-11-04 02:22:09.098478: step 127500, total loss = 0.46, predict loss = 0.11 (60.7 examples/sec; 0.066 sec/batch; 107h:25m:21s remains)
INFO - root - 2019-11-04 02:22:09.730752: step 127510, total loss = 0.39, predict loss = 0.09 (72.7 examples/sec; 0.055 sec/batch; 89h:46m:27s remains)
INFO - root - 2019-11-04 02:22:10.387082: step 127520, total loss = 0.54, predict loss = 0.13 (69.3 examples/sec; 0.058 sec/batch; 94h:10m:26s remains)
INFO - root - 2019-11-04 02:22:11.082078: step 127530, total loss = 0.47, predict loss = 0.10 (63.4 examples/sec; 0.063 sec/batch; 102h:52m:12s remains)
INFO - root - 2019-11-04 02:22:11.724011: step 127540, total loss = 0.51, predict loss = 0.12 (68.6 examples/sec; 0.058 sec/batch; 95h:10m:59s remains)
INFO - root - 2019-11-04 02:22:12.389011: step 127550, total loss = 0.49, predict loss = 0.11 (65.5 examples/sec; 0.061 sec/batch; 99h:38m:46s remains)
INFO - root - 2019-11-04 02:22:13.017884: step 127560, total loss = 0.48, predict loss = 0.11 (78.6 examples/sec; 0.051 sec/batch; 82h:59m:04s remains)
INFO - root - 2019-11-04 02:22:13.604714: step 127570, total loss = 0.68, predict loss = 0.16 (75.1 examples/sec; 0.053 sec/batch; 86h:54m:32s remains)
INFO - root - 2019-11-04 02:22:14.221898: step 127580, total loss = 0.48, predict loss = 0.11 (71.3 examples/sec; 0.056 sec/batch; 91h:28m:52s remains)
INFO - root - 2019-11-04 02:22:14.887206: step 127590, total loss = 0.55, predict loss = 0.14 (56.3 examples/sec; 0.071 sec/batch; 115h:51m:38s remains)
INFO - root - 2019-11-04 02:22:15.558919: step 127600, total loss = 0.53, predict loss = 0.13 (64.4 examples/sec; 0.062 sec/batch; 101h:18m:27s remains)
INFO - root - 2019-11-04 02:22:16.201721: step 127610, total loss = 0.56, predict loss = 0.14 (70.6 examples/sec; 0.057 sec/batch; 92h:27m:29s remains)
INFO - root - 2019-11-04 02:22:16.814016: step 127620, total loss = 0.53, predict loss = 0.13 (73.3 examples/sec; 0.055 sec/batch; 89h:00m:42s remains)
INFO - root - 2019-11-04 02:22:17.419663: step 127630, total loss = 0.34, predict loss = 0.08 (82.6 examples/sec; 0.048 sec/batch; 78h:58m:09s remains)
INFO - root - 2019-11-04 02:22:18.011168: step 127640, total loss = 0.41, predict loss = 0.10 (77.6 examples/sec; 0.052 sec/batch; 84h:04m:06s remains)
INFO - root - 2019-11-04 02:22:18.611732: step 127650, total loss = 0.35, predict loss = 0.08 (74.8 examples/sec; 0.054 sec/batch; 87h:16m:53s remains)
INFO - root - 2019-11-04 02:22:19.240592: step 127660, total loss = 0.32, predict loss = 0.08 (69.3 examples/sec; 0.058 sec/batch; 94h:09m:47s remains)
INFO - root - 2019-11-04 02:22:19.836037: step 127670, total loss = 0.25, predict loss = 0.05 (81.0 examples/sec; 0.049 sec/batch; 80h:34m:57s remains)
INFO - root - 2019-11-04 02:22:20.469410: step 127680, total loss = 0.22, predict loss = 0.04 (65.0 examples/sec; 0.062 sec/batch; 100h:20m:48s remains)
INFO - root - 2019-11-04 02:22:21.075751: step 127690, total loss = 0.20, predict loss = 0.04 (74.5 examples/sec; 0.054 sec/batch; 87h:34m:17s remains)
INFO - root - 2019-11-04 02:22:21.680575: step 127700, total loss = 0.20, predict loss = 0.04 (75.8 examples/sec; 0.053 sec/batch; 86h:01m:21s remains)
INFO - root - 2019-11-04 02:22:22.310369: step 127710, total loss = 0.21, predict loss = 0.04 (70.8 examples/sec; 0.056 sec/batch; 92h:07m:35s remains)
INFO - root - 2019-11-04 02:22:23.464060: step 127720, total loss = 0.38, predict loss = 0.08 (69.7 examples/sec; 0.057 sec/batch; 93h:39m:38s remains)
INFO - root - 2019-11-04 02:22:24.084047: step 127730, total loss = 0.36, predict loss = 0.08 (70.3 examples/sec; 0.057 sec/batch; 92h:48m:29s remains)
INFO - root - 2019-11-04 02:22:24.729862: step 127740, total loss = 0.40, predict loss = 0.08 (61.8 examples/sec; 0.065 sec/batch; 105h:34m:02s remains)
INFO - root - 2019-11-04 02:22:25.339059: step 127750, total loss = 0.32, predict loss = 0.07 (68.5 examples/sec; 0.058 sec/batch; 95h:17m:58s remains)
INFO - root - 2019-11-04 02:22:25.949126: step 127760, total loss = 0.25, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 83h:29m:56s remains)
INFO - root - 2019-11-04 02:22:26.576293: step 127770, total loss = 0.31, predict loss = 0.07 (65.4 examples/sec; 0.061 sec/batch; 99h:43m:33s remains)
INFO - root - 2019-11-04 02:22:27.283163: step 127780, total loss = 0.48, predict loss = 0.11 (56.6 examples/sec; 0.071 sec/batch; 115h:11m:02s remains)
INFO - root - 2019-11-04 02:22:27.967430: step 127790, total loss = 0.27, predict loss = 0.06 (75.3 examples/sec; 0.053 sec/batch; 86h:37m:48s remains)
INFO - root - 2019-11-04 02:22:28.568436: step 127800, total loss = 0.34, predict loss = 0.07 (77.1 examples/sec; 0.052 sec/batch; 84h:37m:19s remains)
INFO - root - 2019-11-04 02:22:29.178076: step 127810, total loss = 0.54, predict loss = 0.13 (73.3 examples/sec; 0.055 sec/batch; 88h:58m:32s remains)
INFO - root - 2019-11-04 02:22:29.766967: step 127820, total loss = 0.44, predict loss = 0.11 (75.1 examples/sec; 0.053 sec/batch; 86h:51m:32s remains)
INFO - root - 2019-11-04 02:22:30.425170: step 127830, total loss = 0.55, predict loss = 0.13 (62.7 examples/sec; 0.064 sec/batch; 104h:01m:40s remains)
INFO - root - 2019-11-04 02:22:31.053153: step 127840, total loss = 0.40, predict loss = 0.09 (68.7 examples/sec; 0.058 sec/batch; 95h:02m:18s remains)
INFO - root - 2019-11-04 02:22:31.727040: step 127850, total loss = 0.36, predict loss = 0.08 (70.9 examples/sec; 0.056 sec/batch; 92h:05m:00s remains)
INFO - root - 2019-11-04 02:22:32.345136: step 127860, total loss = 0.42, predict loss = 0.09 (68.6 examples/sec; 0.058 sec/batch; 95h:03m:25s remains)
INFO - root - 2019-11-04 02:22:32.965820: step 127870, total loss = 0.29, predict loss = 0.07 (76.4 examples/sec; 0.052 sec/batch; 85h:25m:23s remains)
INFO - root - 2019-11-04 02:22:33.644264: step 127880, total loss = 0.27, predict loss = 0.06 (63.3 examples/sec; 0.063 sec/batch; 103h:02m:11s remains)
INFO - root - 2019-11-04 02:22:34.284714: step 127890, total loss = 0.31, predict loss = 0.07 (78.6 examples/sec; 0.051 sec/batch; 83h:01m:31s remains)
INFO - root - 2019-11-04 02:22:34.951045: step 127900, total loss = 0.29, predict loss = 0.06 (59.8 examples/sec; 0.067 sec/batch; 109h:08m:50s remains)
INFO - root - 2019-11-04 02:22:35.579891: step 127910, total loss = 0.30, predict loss = 0.07 (77.4 examples/sec; 0.052 sec/batch; 84h:16m:06s remains)
INFO - root - 2019-11-04 02:22:36.233937: step 127920, total loss = 0.24, predict loss = 0.05 (72.6 examples/sec; 0.055 sec/batch; 89h:50m:25s remains)
INFO - root - 2019-11-04 02:22:36.888923: step 127930, total loss = 0.40, predict loss = 0.09 (63.9 examples/sec; 0.063 sec/batch; 102h:07m:49s remains)
INFO - root - 2019-11-04 02:22:37.515196: step 127940, total loss = 0.45, predict loss = 0.11 (80.6 examples/sec; 0.050 sec/batch; 80h:58m:53s remains)
INFO - root - 2019-11-04 02:22:38.172483: step 127950, total loss = 0.27, predict loss = 0.06 (67.6 examples/sec; 0.059 sec/batch; 96h:28m:40s remains)
INFO - root - 2019-11-04 02:22:38.840617: step 127960, total loss = 0.40, predict loss = 0.10 (76.5 examples/sec; 0.052 sec/batch; 85h:20m:22s remains)
INFO - root - 2019-11-04 02:22:39.509752: step 127970, total loss = 0.29, predict loss = 0.06 (78.3 examples/sec; 0.051 sec/batch; 83h:22m:15s remains)
INFO - root - 2019-11-04 02:22:40.180024: step 127980, total loss = 0.52, predict loss = 0.11 (61.6 examples/sec; 0.065 sec/batch; 105h:58m:56s remains)
INFO - root - 2019-11-04 02:22:40.884085: step 127990, total loss = 0.41, predict loss = 0.09 (64.6 examples/sec; 0.062 sec/batch; 101h:00m:39s remains)
INFO - root - 2019-11-04 02:22:41.508557: step 128000, total loss = 0.43, predict loss = 0.10 (72.2 examples/sec; 0.055 sec/batch; 90h:23m:55s remains)
INFO - root - 2019-11-04 02:22:42.146112: step 128010, total loss = 0.38, predict loss = 0.09 (60.4 examples/sec; 0.066 sec/batch; 108h:06m:06s remains)
INFO - root - 2019-11-04 02:22:42.837630: step 128020, total loss = 0.40, predict loss = 0.10 (62.7 examples/sec; 0.064 sec/batch; 104h:05m:42s remains)
INFO - root - 2019-11-04 02:22:43.502319: step 128030, total loss = 0.38, predict loss = 0.09 (71.0 examples/sec; 0.056 sec/batch; 91h:52m:03s remains)
INFO - root - 2019-11-04 02:22:44.195483: step 128040, total loss = 0.41, predict loss = 0.09 (67.6 examples/sec; 0.059 sec/batch; 96h:28m:11s remains)
INFO - root - 2019-11-04 02:22:44.809019: step 128050, total loss = 0.47, predict loss = 0.11 (67.9 examples/sec; 0.059 sec/batch; 96h:05m:43s remains)
INFO - root - 2019-11-04 02:22:45.488148: step 128060, total loss = 0.47, predict loss = 0.11 (67.4 examples/sec; 0.059 sec/batch; 96h:48m:18s remains)
INFO - root - 2019-11-04 02:22:46.080412: step 128070, total loss = 0.43, predict loss = 0.10 (78.1 examples/sec; 0.051 sec/batch; 83h:30m:07s remains)
INFO - root - 2019-11-04 02:22:46.659001: step 128080, total loss = 0.43, predict loss = 0.10 (80.6 examples/sec; 0.050 sec/batch; 80h:57m:15s remains)
INFO - root - 2019-11-04 02:22:47.291428: step 128090, total loss = 0.56, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 90h:24m:07s remains)
INFO - root - 2019-11-04 02:22:47.935793: step 128100, total loss = 0.50, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 92h:23m:37s remains)
INFO - root - 2019-11-04 02:22:48.576765: step 128110, total loss = 0.39, predict loss = 0.09 (75.2 examples/sec; 0.053 sec/batch; 86h:44m:22s remains)
INFO - root - 2019-11-04 02:22:49.192899: step 128120, total loss = 0.51, predict loss = 0.11 (76.6 examples/sec; 0.052 sec/batch; 85h:09m:31s remains)
INFO - root - 2019-11-04 02:22:49.825904: step 128130, total loss = 0.26, predict loss = 0.05 (66.8 examples/sec; 0.060 sec/batch; 97h:36m:06s remains)
INFO - root - 2019-11-04 02:22:50.427133: step 128140, total loss = 0.39, predict loss = 0.09 (81.6 examples/sec; 0.049 sec/batch; 79h:59m:31s remains)
INFO - root - 2019-11-04 02:22:51.035926: step 128150, total loss = 0.43, predict loss = 0.10 (62.7 examples/sec; 0.064 sec/batch; 103h:59m:06s remains)
INFO - root - 2019-11-04 02:22:51.728795: step 128160, total loss = 0.33, predict loss = 0.07 (65.0 examples/sec; 0.062 sec/batch; 100h:20m:38s remains)
INFO - root - 2019-11-04 02:22:52.346735: step 128170, total loss = 0.32, predict loss = 0.07 (66.9 examples/sec; 0.060 sec/batch; 97h:34m:26s remains)
INFO - root - 2019-11-04 02:22:52.959761: step 128180, total loss = 0.31, predict loss = 0.07 (68.3 examples/sec; 0.059 sec/batch; 95h:32m:46s remains)
INFO - root - 2019-11-04 02:22:53.588575: step 128190, total loss = 0.57, predict loss = 0.15 (71.1 examples/sec; 0.056 sec/batch; 91h:45m:17s remains)
INFO - root - 2019-11-04 02:22:54.206917: step 128200, total loss = 0.52, predict loss = 0.12 (82.9 examples/sec; 0.048 sec/batch; 78h:39m:27s remains)
INFO - root - 2019-11-04 02:22:54.819910: step 128210, total loss = 0.23, predict loss = 0.05 (71.5 examples/sec; 0.056 sec/batch; 91h:15m:02s remains)
INFO - root - 2019-11-04 02:22:55.436145: step 128220, total loss = 0.54, predict loss = 0.12 (82.2 examples/sec; 0.049 sec/batch; 79h:23m:09s remains)
INFO - root - 2019-11-04 02:22:56.036149: step 128230, total loss = 0.57, predict loss = 0.13 (78.7 examples/sec; 0.051 sec/batch; 82h:52m:58s remains)
INFO - root - 2019-11-04 02:22:56.643942: step 128240, total loss = 0.50, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 94h:57m:38s remains)
INFO - root - 2019-11-04 02:22:57.267702: step 128250, total loss = 0.59, predict loss = 0.14 (71.0 examples/sec; 0.056 sec/batch; 91h:50m:25s remains)
INFO - root - 2019-11-04 02:22:57.912353: step 128260, total loss = 0.58, predict loss = 0.13 (71.6 examples/sec; 0.056 sec/batch; 91h:10m:35s remains)
INFO - root - 2019-11-04 02:22:58.552019: step 128270, total loss = 0.72, predict loss = 0.17 (73.0 examples/sec; 0.055 sec/batch; 89h:20m:54s remains)
INFO - root - 2019-11-04 02:22:59.175144: step 128280, total loss = 0.59, predict loss = 0.13 (66.9 examples/sec; 0.060 sec/batch; 97h:35m:10s remains)
INFO - root - 2019-11-04 02:22:59.812270: step 128290, total loss = 0.74, predict loss = 0.18 (76.3 examples/sec; 0.052 sec/batch; 85h:31m:28s remains)
INFO - root - 2019-11-04 02:23:00.429146: step 128300, total loss = 0.63, predict loss = 0.15 (86.0 examples/sec; 0.047 sec/batch; 75h:52m:18s remains)
INFO - root - 2019-11-04 02:23:01.086824: step 128310, total loss = 0.59, predict loss = 0.14 (68.8 examples/sec; 0.058 sec/batch; 94h:51m:33s remains)
INFO - root - 2019-11-04 02:23:01.754694: step 128320, total loss = 0.59, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 92h:50m:45s remains)
INFO - root - 2019-11-04 02:23:02.391354: step 128330, total loss = 0.66, predict loss = 0.16 (83.5 examples/sec; 0.048 sec/batch; 78h:05m:15s remains)
INFO - root - 2019-11-04 02:23:03.038583: step 128340, total loss = 0.48, predict loss = 0.11 (70.4 examples/sec; 0.057 sec/batch; 92h:36m:57s remains)
INFO - root - 2019-11-04 02:23:03.687399: step 128350, total loss = 0.57, predict loss = 0.14 (72.1 examples/sec; 0.055 sec/batch; 90h:27m:28s remains)
INFO - root - 2019-11-04 02:23:04.343705: step 128360, total loss = 0.47, predict loss = 0.11 (63.7 examples/sec; 0.063 sec/batch; 102h:28m:27s remains)
INFO - root - 2019-11-04 02:23:04.996985: step 128370, total loss = 0.56, predict loss = 0.13 (67.6 examples/sec; 0.059 sec/batch; 96h:26m:34s remains)
INFO - root - 2019-11-04 02:23:05.660981: step 128380, total loss = 0.51, predict loss = 0.11 (70.8 examples/sec; 0.057 sec/batch; 92h:11m:40s remains)
INFO - root - 2019-11-04 02:23:06.261567: step 128390, total loss = 0.49, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 86h:56m:43s remains)
INFO - root - 2019-11-04 02:23:06.837238: step 128400, total loss = 0.43, predict loss = 0.10 (75.0 examples/sec; 0.053 sec/batch; 86h:57m:26s remains)
INFO - root - 2019-11-04 02:23:07.453787: step 128410, total loss = 0.53, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 90h:59m:48s remains)
INFO - root - 2019-11-04 02:23:08.075147: step 128420, total loss = 0.48, predict loss = 0.11 (67.2 examples/sec; 0.059 sec/batch; 97h:01m:05s remains)
INFO - root - 2019-11-04 02:23:08.667337: step 128430, total loss = 0.45, predict loss = 0.10 (74.2 examples/sec; 0.054 sec/batch; 87h:52m:12s remains)
INFO - root - 2019-11-04 02:23:09.277730: step 128440, total loss = 0.47, predict loss = 0.11 (77.2 examples/sec; 0.052 sec/batch; 84h:27m:41s remains)
INFO - root - 2019-11-04 02:23:09.903284: step 128450, total loss = 0.39, predict loss = 0.08 (80.8 examples/sec; 0.049 sec/batch; 80h:43m:53s remains)
INFO - root - 2019-11-04 02:23:10.514435: step 128460, total loss = 0.53, predict loss = 0.12 (69.9 examples/sec; 0.057 sec/batch; 93h:17m:15s remains)
INFO - root - 2019-11-04 02:23:11.148615: step 128470, total loss = 0.45, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 84h:07m:03s remains)
INFO - root - 2019-11-04 02:23:11.776207: step 128480, total loss = 0.46, predict loss = 0.10 (70.0 examples/sec; 0.057 sec/batch; 93h:09m:15s remains)
INFO - root - 2019-11-04 02:23:12.391667: step 128490, total loss = 0.32, predict loss = 0.07 (67.4 examples/sec; 0.059 sec/batch; 96h:44m:42s remains)
INFO - root - 2019-11-04 02:23:13.005898: step 128500, total loss = 0.48, predict loss = 0.11 (78.3 examples/sec; 0.051 sec/batch; 83h:18m:02s remains)
INFO - root - 2019-11-04 02:23:13.654244: step 128510, total loss = 0.47, predict loss = 0.11 (65.8 examples/sec; 0.061 sec/batch; 99h:09m:44s remains)
INFO - root - 2019-11-04 02:23:14.299006: step 128520, total loss = 0.29, predict loss = 0.07 (69.3 examples/sec; 0.058 sec/batch; 94h:09m:51s remains)
INFO - root - 2019-11-04 02:23:14.933652: step 128530, total loss = 0.44, predict loss = 0.10 (75.0 examples/sec; 0.053 sec/batch; 86h:57m:42s remains)
INFO - root - 2019-11-04 02:23:15.554351: step 128540, total loss = 0.35, predict loss = 0.07 (77.1 examples/sec; 0.052 sec/batch; 84h:37m:34s remains)
INFO - root - 2019-11-04 02:23:16.191126: step 128550, total loss = 0.37, predict loss = 0.08 (69.8 examples/sec; 0.057 sec/batch; 93h:28m:58s remains)
INFO - root - 2019-11-04 02:23:16.813866: step 128560, total loss = 0.41, predict loss = 0.09 (74.2 examples/sec; 0.054 sec/batch; 87h:57m:22s remains)
INFO - root - 2019-11-04 02:23:17.450431: step 128570, total loss = 0.38, predict loss = 0.09 (66.3 examples/sec; 0.060 sec/batch; 98h:20m:56s remains)
INFO - root - 2019-11-04 02:23:18.066535: step 128580, total loss = 0.42, predict loss = 0.09 (76.5 examples/sec; 0.052 sec/batch; 85h:14m:11s remains)
INFO - root - 2019-11-04 02:23:18.698957: step 128590, total loss = 0.47, predict loss = 0.11 (70.0 examples/sec; 0.057 sec/batch; 93h:13m:19s remains)
INFO - root - 2019-11-04 02:23:19.342354: step 128600, total loss = 0.35, predict loss = 0.08 (72.3 examples/sec; 0.055 sec/batch; 90h:12m:32s remains)
INFO - root - 2019-11-04 02:23:19.978425: step 128610, total loss = 0.41, predict loss = 0.09 (73.7 examples/sec; 0.054 sec/batch; 88h:32m:08s remains)
INFO - root - 2019-11-04 02:23:20.654398: step 128620, total loss = 0.50, predict loss = 0.12 (70.9 examples/sec; 0.056 sec/batch; 92h:00m:45s remains)
INFO - root - 2019-11-04 02:23:21.265918: step 128630, total loss = 0.50, predict loss = 0.11 (83.7 examples/sec; 0.048 sec/batch; 77h:54m:03s remains)
INFO - root - 2019-11-04 02:23:21.903241: step 128640, total loss = 0.46, predict loss = 0.10 (72.0 examples/sec; 0.056 sec/batch; 90h:36m:10s remains)
INFO - root - 2019-11-04 02:23:22.504783: step 128650, total loss = 0.55, predict loss = 0.13 (74.7 examples/sec; 0.054 sec/batch; 87h:17m:07s remains)
INFO - root - 2019-11-04 02:23:23.123355: step 128660, total loss = 0.53, predict loss = 0.12 (64.2 examples/sec; 0.062 sec/batch; 101h:39m:29s remains)
INFO - root - 2019-11-04 02:23:23.771057: step 128670, total loss = 0.49, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 98h:02m:53s remains)
INFO - root - 2019-11-04 02:23:24.475350: step 128680, total loss = 0.68, predict loss = 0.16 (60.9 examples/sec; 0.066 sec/batch; 107h:03m:52s remains)
INFO - root - 2019-11-04 02:23:25.116429: step 128690, total loss = 0.50, predict loss = 0.12 (66.3 examples/sec; 0.060 sec/batch; 98h:26m:55s remains)
INFO - root - 2019-11-04 02:23:25.768022: step 128700, total loss = 0.61, predict loss = 0.15 (71.0 examples/sec; 0.056 sec/batch; 91h:55m:13s remains)
INFO - root - 2019-11-04 02:23:26.394785: step 128710, total loss = 0.63, predict loss = 0.15 (71.4 examples/sec; 0.056 sec/batch; 91h:22m:07s remains)
INFO - root - 2019-11-04 02:23:27.043544: step 128720, total loss = 0.66, predict loss = 0.16 (63.2 examples/sec; 0.063 sec/batch; 103h:15m:18s remains)
INFO - root - 2019-11-04 02:23:27.701824: step 128730, total loss = 0.59, predict loss = 0.14 (66.8 examples/sec; 0.060 sec/batch; 97h:35m:43s remains)
INFO - root - 2019-11-04 02:23:28.320187: step 128740, total loss = 0.46, predict loss = 0.11 (70.5 examples/sec; 0.057 sec/batch; 92h:33m:04s remains)
INFO - root - 2019-11-04 02:23:28.955148: step 128750, total loss = 0.50, predict loss = 0.12 (79.6 examples/sec; 0.050 sec/batch; 81h:57m:35s remains)
INFO - root - 2019-11-04 02:23:29.594183: step 128760, total loss = 0.43, predict loss = 0.10 (72.3 examples/sec; 0.055 sec/batch; 90h:14m:28s remains)
INFO - root - 2019-11-04 02:23:30.212320: step 128770, total loss = 0.52, predict loss = 0.13 (78.5 examples/sec; 0.051 sec/batch; 83h:06m:10s remains)
INFO - root - 2019-11-04 02:23:30.829635: step 128780, total loss = 0.36, predict loss = 0.09 (75.5 examples/sec; 0.053 sec/batch; 86h:22m:48s remains)
INFO - root - 2019-11-04 02:23:31.446645: step 128790, total loss = 0.37, predict loss = 0.08 (78.4 examples/sec; 0.051 sec/batch; 83h:14m:14s remains)
INFO - root - 2019-11-04 02:23:32.057946: step 128800, total loss = 0.34, predict loss = 0.08 (69.6 examples/sec; 0.057 sec/batch; 93h:44m:37s remains)
INFO - root - 2019-11-04 02:23:32.668439: step 128810, total loss = 0.38, predict loss = 0.08 (74.8 examples/sec; 0.053 sec/batch; 87h:14m:06s remains)
INFO - root - 2019-11-04 02:23:33.284472: step 128820, total loss = 0.40, predict loss = 0.09 (73.2 examples/sec; 0.055 sec/batch; 89h:10m:45s remains)
INFO - root - 2019-11-04 02:23:33.918650: step 128830, total loss = 0.36, predict loss = 0.08 (66.3 examples/sec; 0.060 sec/batch; 98h:27m:12s remains)
INFO - root - 2019-11-04 02:23:34.571441: step 128840, total loss = 0.45, predict loss = 0.10 (66.1 examples/sec; 0.060 sec/batch; 98h:39m:53s remains)
INFO - root - 2019-11-04 02:23:35.177065: step 128850, total loss = 0.33, predict loss = 0.07 (76.8 examples/sec; 0.052 sec/batch; 84h:54m:45s remains)
INFO - root - 2019-11-04 02:23:35.768886: step 128860, total loss = 0.64, predict loss = 0.15 (74.8 examples/sec; 0.053 sec/batch; 87h:10m:07s remains)
INFO - root - 2019-11-04 02:23:36.377034: step 128870, total loss = 0.50, predict loss = 0.12 (74.1 examples/sec; 0.054 sec/batch; 88h:00m:37s remains)
INFO - root - 2019-11-04 02:23:36.986655: step 128880, total loss = 0.57, predict loss = 0.14 (70.8 examples/sec; 0.057 sec/batch; 92h:09m:17s remains)
INFO - root - 2019-11-04 02:23:37.635308: step 128890, total loss = 0.59, predict loss = 0.14 (62.1 examples/sec; 0.064 sec/batch; 105h:03m:07s remains)
INFO - root - 2019-11-04 02:23:38.296988: step 128900, total loss = 0.45, predict loss = 0.10 (77.5 examples/sec; 0.052 sec/batch; 84h:07m:26s remains)
INFO - root - 2019-11-04 02:23:38.912897: step 128910, total loss = 0.57, predict loss = 0.14 (70.7 examples/sec; 0.057 sec/batch; 92h:16m:23s remains)
INFO - root - 2019-11-04 02:23:39.541514: step 128920, total loss = 0.52, predict loss = 0.13 (72.1 examples/sec; 0.055 sec/batch; 90h:25m:04s remains)
INFO - root - 2019-11-04 02:23:40.167246: step 128930, total loss = 0.58, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 94h:11m:29s remains)
INFO - root - 2019-11-04 02:23:40.771503: step 128940, total loss = 0.44, predict loss = 0.11 (83.1 examples/sec; 0.048 sec/batch; 78h:29m:14s remains)
INFO - root - 2019-11-04 02:23:41.398102: step 128950, total loss = 0.63, predict loss = 0.16 (72.1 examples/sec; 0.055 sec/batch; 90h:26m:14s remains)
INFO - root - 2019-11-04 02:23:42.041904: step 128960, total loss = 0.54, predict loss = 0.13 (73.2 examples/sec; 0.055 sec/batch; 89h:06m:29s remains)
INFO - root - 2019-11-04 02:23:42.675338: step 128970, total loss = 0.50, predict loss = 0.11 (70.9 examples/sec; 0.056 sec/batch; 92h:02m:55s remains)
INFO - root - 2019-11-04 02:23:43.304952: step 128980, total loss = 0.61, predict loss = 0.15 (62.5 examples/sec; 0.064 sec/batch; 104h:25m:25s remains)
INFO - root - 2019-11-04 02:23:43.937628: step 128990, total loss = 0.54, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 94h:50m:49s remains)
INFO - root - 2019-11-04 02:23:44.566890: step 129000, total loss = 0.48, predict loss = 0.11 (72.9 examples/sec; 0.055 sec/batch; 89h:26m:48s remains)
INFO - root - 2019-11-04 02:23:45.199568: step 129010, total loss = 0.48, predict loss = 0.11 (68.5 examples/sec; 0.058 sec/batch; 95h:11m:41s remains)
INFO - root - 2019-11-04 02:23:45.817345: step 129020, total loss = 0.48, predict loss = 0.11 (64.2 examples/sec; 0.062 sec/batch; 101h:34m:07s remains)
INFO - root - 2019-11-04 02:23:46.412247: step 129030, total loss = 0.43, predict loss = 0.10 (74.0 examples/sec; 0.054 sec/batch; 88h:11m:29s remains)
INFO - root - 2019-11-04 02:23:47.017168: step 129040, total loss = 0.41, predict loss = 0.09 (75.1 examples/sec; 0.053 sec/batch; 86h:51m:12s remains)
INFO - root - 2019-11-04 02:23:47.652195: step 129050, total loss = 0.50, predict loss = 0.12 (80.2 examples/sec; 0.050 sec/batch; 81h:21m:08s remains)
INFO - root - 2019-11-04 02:23:48.296499: step 129060, total loss = 0.52, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 94h:33m:02s remains)
INFO - root - 2019-11-04 02:23:48.932337: step 129070, total loss = 0.42, predict loss = 0.10 (73.0 examples/sec; 0.055 sec/batch; 89h:19m:52s remains)
INFO - root - 2019-11-04 02:23:49.551300: step 129080, total loss = 0.49, predict loss = 0.11 (77.3 examples/sec; 0.052 sec/batch; 84h:24m:28s remains)
INFO - root - 2019-11-04 02:23:50.167430: step 129090, total loss = 0.50, predict loss = 0.12 (77.2 examples/sec; 0.052 sec/batch; 84h:29m:19s remains)
INFO - root - 2019-11-04 02:23:50.800425: step 129100, total loss = 0.46, predict loss = 0.10 (72.7 examples/sec; 0.055 sec/batch; 89h:45m:33s remains)
INFO - root - 2019-11-04 02:23:51.453615: step 129110, total loss = 0.45, predict loss = 0.10 (69.2 examples/sec; 0.058 sec/batch; 94h:13m:07s remains)
INFO - root - 2019-11-04 02:23:52.037982: step 129120, total loss = 0.47, predict loss = 0.11 (73.9 examples/sec; 0.054 sec/batch; 88h:14m:46s remains)
INFO - root - 2019-11-04 02:23:52.647375: step 129130, total loss = 0.46, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 91h:09m:43s remains)
INFO - root - 2019-11-04 02:23:53.270853: step 129140, total loss = 0.46, predict loss = 0.11 (72.9 examples/sec; 0.055 sec/batch; 89h:29m:55s remains)
INFO - root - 2019-11-04 02:23:53.888008: step 129150, total loss = 0.42, predict loss = 0.10 (71.6 examples/sec; 0.056 sec/batch; 91h:05m:51s remains)
INFO - root - 2019-11-04 02:23:54.536992: step 129160, total loss = 0.43, predict loss = 0.10 (67.6 examples/sec; 0.059 sec/batch; 96h:26m:51s remains)
INFO - root - 2019-11-04 02:23:55.236657: step 129170, total loss = 0.43, predict loss = 0.09 (58.4 examples/sec; 0.069 sec/batch; 111h:46m:45s remains)
INFO - root - 2019-11-04 02:23:55.892518: step 129180, total loss = 0.54, predict loss = 0.13 (67.9 examples/sec; 0.059 sec/batch; 96h:02m:03s remains)
INFO - root - 2019-11-04 02:23:56.513916: step 129190, total loss = 0.50, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 98h:58m:14s remains)
INFO - root - 2019-11-04 02:23:57.167517: step 129200, total loss = 0.44, predict loss = 0.10 (77.0 examples/sec; 0.052 sec/batch; 84h:42m:38s remains)
INFO - root - 2019-11-04 02:23:57.814762: step 129210, total loss = 0.43, predict loss = 0.10 (69.9 examples/sec; 0.057 sec/batch; 93h:19m:50s remains)
INFO - root - 2019-11-04 02:23:58.455641: step 129220, total loss = 0.48, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 91h:25m:37s remains)
INFO - root - 2019-11-04 02:23:59.076049: step 129230, total loss = 0.54, predict loss = 0.13 (72.5 examples/sec; 0.055 sec/batch; 90h:00m:48s remains)
INFO - root - 2019-11-04 02:23:59.734747: step 129240, total loss = 0.45, predict loss = 0.11 (63.9 examples/sec; 0.063 sec/batch; 102h:01m:33s remains)
INFO - root - 2019-11-04 02:24:00.360308: step 129250, total loss = 0.58, predict loss = 0.14 (71.1 examples/sec; 0.056 sec/batch; 91h:41m:48s remains)
INFO - root - 2019-11-04 02:24:01.019659: step 129260, total loss = 0.46, predict loss = 0.11 (68.4 examples/sec; 0.059 sec/batch; 95h:25m:49s remains)
INFO - root - 2019-11-04 02:24:01.653749: step 129270, total loss = 0.60, predict loss = 0.14 (73.6 examples/sec; 0.054 sec/batch; 88h:34m:11s remains)
INFO - root - 2019-11-04 02:24:02.330900: step 129280, total loss = 0.61, predict loss = 0.14 (66.1 examples/sec; 0.061 sec/batch; 98h:41m:25s remains)
INFO - root - 2019-11-04 02:24:02.968152: step 129290, total loss = 0.53, predict loss = 0.13 (68.6 examples/sec; 0.058 sec/batch; 95h:02m:54s remains)
INFO - root - 2019-11-04 02:24:03.633514: step 129300, total loss = 0.64, predict loss = 0.15 (62.8 examples/sec; 0.064 sec/batch; 103h:52m:37s remains)
INFO - root - 2019-11-04 02:24:04.289449: step 129310, total loss = 0.57, predict loss = 0.14 (70.8 examples/sec; 0.056 sec/batch; 92h:04m:59s remains)
INFO - root - 2019-11-04 02:24:04.979213: step 129320, total loss = 0.62, predict loss = 0.15 (60.7 examples/sec; 0.066 sec/batch; 107h:23m:28s remains)
INFO - root - 2019-11-04 02:24:05.592490: step 129330, total loss = 0.56, predict loss = 0.13 (61.7 examples/sec; 0.065 sec/batch; 105h:47m:51s remains)
INFO - root - 2019-11-04 02:24:06.182250: step 129340, total loss = 0.60, predict loss = 0.14 (76.5 examples/sec; 0.052 sec/batch; 85h:19m:03s remains)
INFO - root - 2019-11-04 02:24:06.786695: step 129350, total loss = 0.60, predict loss = 0.15 (81.3 examples/sec; 0.049 sec/batch; 80h:12m:01s remains)
INFO - root - 2019-11-04 02:24:07.419114: step 129360, total loss = 0.78, predict loss = 0.18 (73.2 examples/sec; 0.055 sec/batch; 89h:04m:42s remains)
INFO - root - 2019-11-04 02:24:08.047615: step 129370, total loss = 0.56, predict loss = 0.13 (68.6 examples/sec; 0.058 sec/batch; 95h:01m:01s remains)
INFO - root - 2019-11-04 02:24:08.714675: step 129380, total loss = 0.51, predict loss = 0.12 (63.1 examples/sec; 0.063 sec/batch; 103h:25m:10s remains)
INFO - root - 2019-11-04 02:24:09.372458: step 129390, total loss = 0.40, predict loss = 0.09 (68.3 examples/sec; 0.059 sec/batch; 95h:29m:45s remains)
INFO - root - 2019-11-04 02:24:10.065178: step 129400, total loss = 0.67, predict loss = 0.16 (70.8 examples/sec; 0.056 sec/batch; 92h:04m:33s remains)
INFO - root - 2019-11-04 02:24:10.681880: step 129410, total loss = 0.55, predict loss = 0.13 (74.3 examples/sec; 0.054 sec/batch; 87h:49m:17s remains)
INFO - root - 2019-11-04 02:24:11.267140: step 129420, total loss = 0.43, predict loss = 0.10 (78.4 examples/sec; 0.051 sec/batch; 83h:14m:20s remains)
INFO - root - 2019-11-04 02:24:11.908047: step 129430, total loss = 0.38, predict loss = 0.09 (65.3 examples/sec; 0.061 sec/batch; 99h:50m:39s remains)
INFO - root - 2019-11-04 02:24:12.546664: step 129440, total loss = 0.38, predict loss = 0.08 (68.4 examples/sec; 0.058 sec/batch; 95h:19m:10s remains)
INFO - root - 2019-11-04 02:24:13.173239: step 129450, total loss = 0.41, predict loss = 0.09 (68.1 examples/sec; 0.059 sec/batch; 95h:43m:35s remains)
INFO - root - 2019-11-04 02:24:13.749272: step 129460, total loss = 0.43, predict loss = 0.10 (72.2 examples/sec; 0.055 sec/batch; 90h:24m:15s remains)
INFO - root - 2019-11-04 02:24:14.370480: step 129470, total loss = 0.50, predict loss = 0.12 (68.4 examples/sec; 0.058 sec/batch; 95h:22m:51s remains)
INFO - root - 2019-11-04 02:24:14.978094: step 129480, total loss = 0.62, predict loss = 0.14 (85.0 examples/sec; 0.047 sec/batch; 76h:43m:06s remains)
INFO - root - 2019-11-04 02:24:15.601566: step 129490, total loss = 0.49, predict loss = 0.11 (70.1 examples/sec; 0.057 sec/batch; 93h:04m:34s remains)
INFO - root - 2019-11-04 02:24:16.225657: step 129500, total loss = 0.40, predict loss = 0.08 (73.0 examples/sec; 0.055 sec/batch; 89h:21m:18s remains)
INFO - root - 2019-11-04 02:24:16.880591: step 129510, total loss = 0.43, predict loss = 0.10 (72.8 examples/sec; 0.055 sec/batch; 89h:39m:09s remains)
INFO - root - 2019-11-04 02:24:17.478594: step 129520, total loss = 0.45, predict loss = 0.10 (77.5 examples/sec; 0.052 sec/batch; 84h:09m:10s remains)
INFO - root - 2019-11-04 02:24:18.032580: step 129530, total loss = 0.38, predict loss = 0.08 (91.7 examples/sec; 0.044 sec/batch; 71h:09m:32s remains)
INFO - root - 2019-11-04 02:24:18.491900: step 129540, total loss = 0.56, predict loss = 0.13 (97.9 examples/sec; 0.041 sec/batch; 66h:37m:18s remains)
INFO - root - 2019-11-04 02:24:19.518832: step 129550, total loss = 0.20, predict loss = 0.04 (79.3 examples/sec; 0.050 sec/batch; 82h:13m:41s remains)
INFO - root - 2019-11-04 02:24:20.153926: step 129560, total loss = 0.39, predict loss = 0.09 (71.7 examples/sec; 0.056 sec/batch; 90h:55m:42s remains)
INFO - root - 2019-11-04 02:24:20.838620: step 129570, total loss = 0.39, predict loss = 0.08 (67.4 examples/sec; 0.059 sec/batch; 96h:48m:37s remains)
INFO - root - 2019-11-04 02:24:21.495198: step 129580, total loss = 0.52, predict loss = 0.12 (69.7 examples/sec; 0.057 sec/batch; 93h:31m:37s remains)
INFO - root - 2019-11-04 02:24:22.174158: step 129590, total loss = 0.42, predict loss = 0.10 (65.7 examples/sec; 0.061 sec/batch; 99h:20m:36s remains)
INFO - root - 2019-11-04 02:24:22.794858: step 129600, total loss = 0.53, predict loss = 0.13 (71.5 examples/sec; 0.056 sec/batch; 91h:13m:11s remains)
INFO - root - 2019-11-04 02:24:23.420494: step 129610, total loss = 0.39, predict loss = 0.09 (82.0 examples/sec; 0.049 sec/batch; 79h:30m:25s remains)
INFO - root - 2019-11-04 02:24:24.064265: step 129620, total loss = 0.60, predict loss = 0.14 (79.5 examples/sec; 0.050 sec/batch; 82h:04m:59s remains)
INFO - root - 2019-11-04 02:24:24.710762: step 129630, total loss = 0.49, predict loss = 0.10 (67.2 examples/sec; 0.060 sec/batch; 97h:03m:39s remains)
INFO - root - 2019-11-04 02:24:25.321352: step 129640, total loss = 0.60, predict loss = 0.14 (75.0 examples/sec; 0.053 sec/batch; 86h:55m:27s remains)
INFO - root - 2019-11-04 02:24:25.941213: step 129650, total loss = 0.50, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 93h:36m:52s remains)
INFO - root - 2019-11-04 02:24:26.561010: step 129660, total loss = 0.67, predict loss = 0.16 (72.3 examples/sec; 0.055 sec/batch; 90h:13m:40s remains)
INFO - root - 2019-11-04 02:24:27.209261: step 129670, total loss = 0.37, predict loss = 0.08 (67.8 examples/sec; 0.059 sec/batch; 96h:15m:00s remains)
INFO - root - 2019-11-04 02:24:27.864271: step 129680, total loss = 0.46, predict loss = 0.10 (66.6 examples/sec; 0.060 sec/batch; 97h:53m:32s remains)
INFO - root - 2019-11-04 02:24:28.501598: step 129690, total loss = 0.54, predict loss = 0.13 (77.8 examples/sec; 0.051 sec/batch; 83h:49m:14s remains)
INFO - root - 2019-11-04 02:24:29.156373: step 129700, total loss = 0.43, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 93h:38m:37s remains)
INFO - root - 2019-11-04 02:24:29.810013: step 129710, total loss = 0.52, predict loss = 0.13 (70.2 examples/sec; 0.057 sec/batch; 92h:51m:40s remains)
INFO - root - 2019-11-04 02:24:30.439671: step 129720, total loss = 0.54, predict loss = 0.13 (65.8 examples/sec; 0.061 sec/batch; 99h:07m:24s remains)
INFO - root - 2019-11-04 02:24:31.035571: step 129730, total loss = 0.37, predict loss = 0.08 (69.5 examples/sec; 0.058 sec/batch; 93h:53m:32s remains)
INFO - root - 2019-11-04 02:24:31.655501: step 129740, total loss = 0.39, predict loss = 0.08 (75.5 examples/sec; 0.053 sec/batch; 86h:24m:30s remains)
INFO - root - 2019-11-04 02:24:32.265095: step 129750, total loss = 0.43, predict loss = 0.10 (72.1 examples/sec; 0.055 sec/batch; 90h:25m:49s remains)
INFO - root - 2019-11-04 02:24:32.882229: step 129760, total loss = 0.44, predict loss = 0.10 (67.4 examples/sec; 0.059 sec/batch; 96h:47m:16s remains)
INFO - root - 2019-11-04 02:24:33.534821: step 129770, total loss = 0.34, predict loss = 0.08 (67.0 examples/sec; 0.060 sec/batch; 97h:20m:24s remains)
INFO - root - 2019-11-04 02:24:34.143244: step 129780, total loss = 0.41, predict loss = 0.09 (77.3 examples/sec; 0.052 sec/batch; 84h:25m:23s remains)
INFO - root - 2019-11-04 02:24:34.782074: step 129790, total loss = 0.42, predict loss = 0.09 (68.8 examples/sec; 0.058 sec/batch; 94h:44m:56s remains)
INFO - root - 2019-11-04 02:24:35.424726: step 129800, total loss = 0.44, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 92h:22m:57s remains)
INFO - root - 2019-11-04 02:24:36.060421: step 129810, total loss = 0.55, predict loss = 0.13 (71.7 examples/sec; 0.056 sec/batch; 90h:57m:38s remains)
INFO - root - 2019-11-04 02:24:36.724543: step 129820, total loss = 0.49, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 89h:34m:29s remains)
INFO - root - 2019-11-04 02:24:37.340179: step 129830, total loss = 0.53, predict loss = 0.12 (72.8 examples/sec; 0.055 sec/batch; 89h:36m:22s remains)
INFO - root - 2019-11-04 02:24:37.991065: step 129840, total loss = 0.49, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 98h:50m:42s remains)
INFO - root - 2019-11-04 02:24:38.660801: step 129850, total loss = 0.51, predict loss = 0.12 (63.6 examples/sec; 0.063 sec/batch; 102h:30m:41s remains)
INFO - root - 2019-11-04 02:24:39.318984: step 129860, total loss = 0.43, predict loss = 0.09 (67.5 examples/sec; 0.059 sec/batch; 96h:35m:26s remains)
INFO - root - 2019-11-04 02:24:39.937891: step 129870, total loss = 0.50, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 89h:18m:50s remains)
INFO - root - 2019-11-04 02:24:40.611114: step 129880, total loss = 0.52, predict loss = 0.12 (66.3 examples/sec; 0.060 sec/batch; 98h:25m:39s remains)
INFO - root - 2019-11-04 02:24:41.253874: step 129890, total loss = 0.57, predict loss = 0.13 (77.1 examples/sec; 0.052 sec/batch; 84h:38m:22s remains)
INFO - root - 2019-11-04 02:24:41.879690: step 129900, total loss = 0.46, predict loss = 0.10 (67.1 examples/sec; 0.060 sec/batch; 97h:09m:43s remains)
INFO - root - 2019-11-04 02:24:42.522363: step 129910, total loss = 0.55, predict loss = 0.13 (70.6 examples/sec; 0.057 sec/batch; 92h:26m:58s remains)
INFO - root - 2019-11-04 02:24:43.139652: step 129920, total loss = 0.55, predict loss = 0.13 (72.7 examples/sec; 0.055 sec/batch; 89h:41m:15s remains)
INFO - root - 2019-11-04 02:24:43.736718: step 129930, total loss = 0.51, predict loss = 0.11 (76.5 examples/sec; 0.052 sec/batch; 85h:15m:21s remains)
INFO - root - 2019-11-04 02:24:44.369701: step 129940, total loss = 0.43, predict loss = 0.09 (78.5 examples/sec; 0.051 sec/batch; 83h:05m:14s remains)
INFO - root - 2019-11-04 02:24:44.972490: step 129950, total loss = 0.44, predict loss = 0.10 (73.4 examples/sec; 0.054 sec/batch; 88h:48m:01s remains)
INFO - root - 2019-11-04 02:24:45.578193: step 129960, total loss = 0.53, predict loss = 0.12 (77.8 examples/sec; 0.051 sec/batch; 83h:48m:31s remains)
INFO - root - 2019-11-04 02:24:46.194801: step 129970, total loss = 0.54, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 92h:32m:52s remains)
INFO - root - 2019-11-04 02:24:46.841252: step 129980, total loss = 0.41, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 96h:52m:47s remains)
INFO - root - 2019-11-04 02:24:47.507829: step 129990, total loss = 0.40, predict loss = 0.09 (75.3 examples/sec; 0.053 sec/batch; 86h:34m:35s remains)
INFO - root - 2019-11-04 02:24:48.122205: step 130000, total loss = 0.51, predict loss = 0.12 (74.8 examples/sec; 0.054 sec/batch; 87h:14m:57s remains)
INFO - root - 2019-11-04 02:24:48.760837: step 130010, total loss = 0.54, predict loss = 0.14 (62.9 examples/sec; 0.064 sec/batch; 103h:37m:43s remains)
INFO - root - 2019-11-04 02:24:49.445948: step 130020, total loss = 0.37, predict loss = 0.09 (64.4 examples/sec; 0.062 sec/batch; 101h:16m:14s remains)
INFO - root - 2019-11-04 02:24:50.099136: step 130030, total loss = 0.24, predict loss = 0.04 (73.2 examples/sec; 0.055 sec/batch; 89h:04m:45s remains)
INFO - root - 2019-11-04 02:24:50.729569: step 130040, total loss = 0.66, predict loss = 0.17 (71.0 examples/sec; 0.056 sec/batch; 91h:48m:12s remains)
INFO - root - 2019-11-04 02:24:51.360393: step 130050, total loss = 0.48, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 92h:19m:03s remains)
INFO - root - 2019-11-04 02:24:52.015320: step 130060, total loss = 0.52, predict loss = 0.13 (68.3 examples/sec; 0.059 sec/batch; 95h:33m:35s remains)
INFO - root - 2019-11-04 02:24:52.646365: step 130070, total loss = 0.56, predict loss = 0.14 (61.2 examples/sec; 0.065 sec/batch; 106h:37m:51s remains)
INFO - root - 2019-11-04 02:24:53.295738: step 130080, total loss = 0.51, predict loss = 0.13 (70.2 examples/sec; 0.057 sec/batch; 92h:54m:54s remains)
INFO - root - 2019-11-04 02:24:53.944734: step 130090, total loss = 0.36, predict loss = 0.09 (65.8 examples/sec; 0.061 sec/batch; 99h:10m:28s remains)
INFO - root - 2019-11-04 02:24:54.575412: step 130100, total loss = 0.50, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 91h:21m:36s remains)
INFO - root - 2019-11-04 02:24:55.216085: step 130110, total loss = 0.34, predict loss = 0.08 (85.8 examples/sec; 0.047 sec/batch; 76h:00m:38s remains)
INFO - root - 2019-11-04 02:24:55.880430: step 130120, total loss = 0.40, predict loss = 0.10 (72.6 examples/sec; 0.055 sec/batch; 89h:53m:11s remains)
INFO - root - 2019-11-04 02:24:56.538402: step 130130, total loss = 0.34, predict loss = 0.08 (72.9 examples/sec; 0.055 sec/batch; 89h:24m:52s remains)
INFO - root - 2019-11-04 02:24:57.218925: step 130140, total loss = 0.31, predict loss = 0.07 (63.4 examples/sec; 0.063 sec/batch; 102h:54m:36s remains)
INFO - root - 2019-11-04 02:24:57.861720: step 130150, total loss = 0.41, predict loss = 0.09 (78.0 examples/sec; 0.051 sec/batch; 83h:38m:37s remains)
INFO - root - 2019-11-04 02:24:58.496463: step 130160, total loss = 0.38, predict loss = 0.08 (77.6 examples/sec; 0.052 sec/batch; 84h:00m:06s remains)
INFO - root - 2019-11-04 02:24:59.109556: step 130170, total loss = 0.31, predict loss = 0.07 (71.3 examples/sec; 0.056 sec/batch; 91h:24m:39s remains)
INFO - root - 2019-11-04 02:24:59.707415: step 130180, total loss = 0.38, predict loss = 0.09 (77.0 examples/sec; 0.052 sec/batch; 84h:43m:19s remains)
INFO - root - 2019-11-04 02:25:00.315669: step 130190, total loss = 0.23, predict loss = 0.05 (71.9 examples/sec; 0.056 sec/batch; 90h:40m:05s remains)
INFO - root - 2019-11-04 02:25:00.919226: step 130200, total loss = 0.41, predict loss = 0.10 (71.0 examples/sec; 0.056 sec/batch; 91h:49m:49s remains)
INFO - root - 2019-11-04 02:25:01.568506: step 130210, total loss = 0.39, predict loss = 0.09 (66.2 examples/sec; 0.060 sec/batch; 98h:33m:25s remains)
INFO - root - 2019-11-04 02:25:02.159912: step 130220, total loss = 0.42, predict loss = 0.10 (73.2 examples/sec; 0.055 sec/batch; 89h:07m:53s remains)
INFO - root - 2019-11-04 02:25:02.771514: step 130230, total loss = 0.46, predict loss = 0.10 (72.6 examples/sec; 0.055 sec/batch; 89h:53m:17s remains)
INFO - root - 2019-11-04 02:25:03.405368: step 130240, total loss = 0.36, predict loss = 0.09 (65.9 examples/sec; 0.061 sec/batch; 98h:55m:42s remains)
INFO - root - 2019-11-04 02:25:04.036489: step 130250, total loss = 0.46, predict loss = 0.11 (74.3 examples/sec; 0.054 sec/batch; 87h:44m:16s remains)
INFO - root - 2019-11-04 02:25:04.653852: step 130260, total loss = 0.52, predict loss = 0.12 (77.3 examples/sec; 0.052 sec/batch; 84h:23m:10s remains)
INFO - root - 2019-11-04 02:25:05.308339: step 130270, total loss = 0.46, predict loss = 0.11 (68.7 examples/sec; 0.058 sec/batch; 94h:52m:24s remains)
INFO - root - 2019-11-04 02:25:05.914951: step 130280, total loss = 0.44, predict loss = 0.10 (65.8 examples/sec; 0.061 sec/batch; 99h:04m:01s remains)
INFO - root - 2019-11-04 02:25:06.530123: step 130290, total loss = 0.53, predict loss = 0.12 (72.2 examples/sec; 0.055 sec/batch; 90h:21m:47s remains)
INFO - root - 2019-11-04 02:25:07.153905: step 130300, total loss = 0.38, predict loss = 0.08 (71.0 examples/sec; 0.056 sec/batch; 91h:48m:36s remains)
INFO - root - 2019-11-04 02:25:07.807483: step 130310, total loss = 0.51, predict loss = 0.13 (71.7 examples/sec; 0.056 sec/batch; 90h:57m:37s remains)
INFO - root - 2019-11-04 02:25:08.468228: step 130320, total loss = 0.36, predict loss = 0.08 (68.6 examples/sec; 0.058 sec/batch; 95h:04m:42s remains)
INFO - root - 2019-11-04 02:25:09.108988: step 130330, total loss = 0.55, predict loss = 0.13 (67.4 examples/sec; 0.059 sec/batch; 96h:46m:28s remains)
INFO - root - 2019-11-04 02:25:09.746017: step 130340, total loss = 0.56, predict loss = 0.14 (78.0 examples/sec; 0.051 sec/batch; 83h:39m:11s remains)
INFO - root - 2019-11-04 02:25:10.392666: step 130350, total loss = 0.62, predict loss = 0.15 (75.8 examples/sec; 0.053 sec/batch; 86h:02m:16s remains)
INFO - root - 2019-11-04 02:25:11.042189: step 130360, total loss = 0.50, predict loss = 0.12 (69.9 examples/sec; 0.057 sec/batch; 93h:21m:17s remains)
INFO - root - 2019-11-04 02:25:11.693934: step 130370, total loss = 0.50, predict loss = 0.11 (67.8 examples/sec; 0.059 sec/batch; 96h:11m:53s remains)
INFO - root - 2019-11-04 02:25:12.348816: step 130380, total loss = 0.25, predict loss = 0.05 (58.0 examples/sec; 0.069 sec/batch; 112h:28m:55s remains)
INFO - root - 2019-11-04 02:25:13.003243: step 130390, total loss = 0.25, predict loss = 0.05 (64.3 examples/sec; 0.062 sec/batch; 101h:22m:19s remains)
INFO - root - 2019-11-04 02:25:13.633920: step 130400, total loss = 0.29, predict loss = 0.07 (75.9 examples/sec; 0.053 sec/batch; 85h:54m:18s remains)
INFO - root - 2019-11-04 02:25:14.260826: step 130410, total loss = 0.33, predict loss = 0.08 (71.9 examples/sec; 0.056 sec/batch; 90h:43m:09s remains)
INFO - root - 2019-11-04 02:25:14.933146: step 130420, total loss = 0.27, predict loss = 0.06 (70.8 examples/sec; 0.056 sec/batch; 92h:03m:34s remains)
INFO - root - 2019-11-04 02:25:15.553155: step 130430, total loss = 0.19, predict loss = 0.03 (83.0 examples/sec; 0.048 sec/batch; 78h:36m:31s remains)
INFO - root - 2019-11-04 02:25:16.177948: step 130440, total loss = 0.41, predict loss = 0.09 (62.4 examples/sec; 0.064 sec/batch; 104h:34m:18s remains)
INFO - root - 2019-11-04 02:25:16.801541: step 130450, total loss = 0.33, predict loss = 0.07 (75.3 examples/sec; 0.053 sec/batch; 86h:38m:56s remains)
INFO - root - 2019-11-04 02:25:17.422793: step 130460, total loss = 0.57, predict loss = 0.13 (67.6 examples/sec; 0.059 sec/batch; 96h:30m:57s remains)
INFO - root - 2019-11-04 02:25:18.051570: step 130470, total loss = 0.33, predict loss = 0.07 (70.6 examples/sec; 0.057 sec/batch; 92h:25m:50s remains)
INFO - root - 2019-11-04 02:25:18.695685: step 130480, total loss = 0.40, predict loss = 0.09 (62.6 examples/sec; 0.064 sec/batch; 104h:15m:22s remains)
INFO - root - 2019-11-04 02:25:19.344064: step 130490, total loss = 0.42, predict loss = 0.10 (66.4 examples/sec; 0.060 sec/batch; 98h:14m:22s remains)
INFO - root - 2019-11-04 02:25:19.957969: step 130500, total loss = 0.34, predict loss = 0.07 (71.6 examples/sec; 0.056 sec/batch; 91h:06m:53s remains)
INFO - root - 2019-11-04 02:25:20.578647: step 130510, total loss = 0.38, predict loss = 0.09 (80.8 examples/sec; 0.050 sec/batch; 80h:42m:43s remains)
INFO - root - 2019-11-04 02:25:21.213309: step 130520, total loss = 0.51, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 98h:57m:26s remains)
INFO - root - 2019-11-04 02:25:21.820007: step 130530, total loss = 0.47, predict loss = 0.11 (70.7 examples/sec; 0.057 sec/batch; 92h:13m:32s remains)
INFO - root - 2019-11-04 02:25:22.482293: step 130540, total loss = 0.35, predict loss = 0.08 (63.4 examples/sec; 0.063 sec/batch; 102h:51m:36s remains)
INFO - root - 2019-11-04 02:25:23.135084: step 130550, total loss = 0.49, predict loss = 0.12 (64.9 examples/sec; 0.062 sec/batch; 100h:32m:49s remains)
INFO - root - 2019-11-04 02:25:23.782359: step 130560, total loss = 0.40, predict loss = 0.09 (66.2 examples/sec; 0.060 sec/batch; 98h:33m:28s remains)
INFO - root - 2019-11-04 02:25:24.395314: step 130570, total loss = 0.53, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 85h:57m:53s remains)
INFO - root - 2019-11-04 02:25:25.050770: step 130580, total loss = 0.42, predict loss = 0.10 (67.4 examples/sec; 0.059 sec/batch; 96h:46m:29s remains)
INFO - root - 2019-11-04 02:25:25.726193: step 130590, total loss = 0.47, predict loss = 0.11 (69.9 examples/sec; 0.057 sec/batch; 93h:14m:04s remains)
INFO - root - 2019-11-04 02:25:26.341622: step 130600, total loss = 0.58, predict loss = 0.15 (65.7 examples/sec; 0.061 sec/batch; 99h:13m:11s remains)
INFO - root - 2019-11-04 02:25:26.948196: step 130610, total loss = 0.43, predict loss = 0.10 (75.7 examples/sec; 0.053 sec/batch; 86h:07m:34s remains)
INFO - root - 2019-11-04 02:25:27.563281: step 130620, total loss = 0.35, predict loss = 0.08 (67.7 examples/sec; 0.059 sec/batch; 96h:18m:22s remains)
INFO - root - 2019-11-04 02:25:28.199561: step 130630, total loss = 0.30, predict loss = 0.07 (64.3 examples/sec; 0.062 sec/batch; 101h:21m:08s remains)
INFO - root - 2019-11-04 02:25:28.847485: step 130640, total loss = 0.24, predict loss = 0.05 (65.4 examples/sec; 0.061 sec/batch; 99h:42m:22s remains)
INFO - root - 2019-11-04 02:25:29.508479: step 130650, total loss = 0.27, predict loss = 0.06 (69.1 examples/sec; 0.058 sec/batch; 94h:23m:21s remains)
INFO - root - 2019-11-04 02:25:30.116614: step 130660, total loss = 0.47, predict loss = 0.11 (72.2 examples/sec; 0.055 sec/batch; 90h:21m:46s remains)
INFO - root - 2019-11-04 02:25:30.733367: step 130670, total loss = 0.42, predict loss = 0.10 (68.3 examples/sec; 0.059 sec/batch; 95h:32m:15s remains)
INFO - root - 2019-11-04 02:25:31.361358: step 130680, total loss = 0.41, predict loss = 0.10 (70.1 examples/sec; 0.057 sec/batch; 93h:04m:03s remains)
INFO - root - 2019-11-04 02:25:31.974113: step 130690, total loss = 0.45, predict loss = 0.10 (68.3 examples/sec; 0.059 sec/batch; 95h:27m:12s remains)
INFO - root - 2019-11-04 02:25:32.587780: step 130700, total loss = 0.47, predict loss = 0.11 (67.1 examples/sec; 0.060 sec/batch; 97h:14m:29s remains)
INFO - root - 2019-11-04 02:25:33.231755: step 130710, total loss = 0.50, predict loss = 0.11 (76.3 examples/sec; 0.052 sec/batch; 85h:29m:08s remains)
INFO - root - 2019-11-04 02:25:33.866084: step 130720, total loss = 0.45, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 94h:59m:43s remains)
INFO - root - 2019-11-04 02:25:34.540323: step 130730, total loss = 0.39, predict loss = 0.09 (66.0 examples/sec; 0.061 sec/batch; 98h:46m:17s remains)
INFO - root - 2019-11-04 02:25:35.180727: step 130740, total loss = 0.39, predict loss = 0.09 (78.9 examples/sec; 0.051 sec/batch; 82h:42m:21s remains)
INFO - root - 2019-11-04 02:25:35.811426: step 130750, total loss = 0.32, predict loss = 0.08 (76.0 examples/sec; 0.053 sec/batch; 85h:47m:33s remains)
INFO - root - 2019-11-04 02:25:36.414529: step 130760, total loss = 0.45, predict loss = 0.10 (75.4 examples/sec; 0.053 sec/batch; 86h:30m:31s remains)
INFO - root - 2019-11-04 02:25:37.023673: step 130770, total loss = 0.42, predict loss = 0.10 (77.4 examples/sec; 0.052 sec/batch; 84h:16m:26s remains)
INFO - root - 2019-11-04 02:25:37.613059: step 130780, total loss = 0.31, predict loss = 0.06 (76.2 examples/sec; 0.053 sec/batch; 85h:37m:12s remains)
INFO - root - 2019-11-04 02:25:38.201292: step 130790, total loss = 0.53, predict loss = 0.13 (69.0 examples/sec; 0.058 sec/batch; 94h:28m:38s remains)
INFO - root - 2019-11-04 02:25:38.823267: step 130800, total loss = 0.55, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 89h:12m:49s remains)
INFO - root - 2019-11-04 02:25:39.459614: step 130810, total loss = 0.49, predict loss = 0.11 (68.5 examples/sec; 0.058 sec/batch; 95h:15m:56s remains)
INFO - root - 2019-11-04 02:25:40.126739: step 130820, total loss = 0.36, predict loss = 0.08 (66.7 examples/sec; 0.060 sec/batch; 97h:48m:32s remains)
INFO - root - 2019-11-04 02:25:40.773098: step 130830, total loss = 0.54, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 94h:47m:03s remains)
INFO - root - 2019-11-04 02:25:41.405668: step 130840, total loss = 0.40, predict loss = 0.09 (78.4 examples/sec; 0.051 sec/batch; 83h:09m:52s remains)
INFO - root - 2019-11-04 02:25:42.038453: step 130850, total loss = 0.46, predict loss = 0.10 (72.6 examples/sec; 0.055 sec/batch; 89h:49m:06s remains)
INFO - root - 2019-11-04 02:25:42.634259: step 130860, total loss = 0.42, predict loss = 0.09 (80.5 examples/sec; 0.050 sec/batch; 81h:03m:29s remains)
INFO - root - 2019-11-04 02:25:43.276866: step 130870, total loss = 0.29, predict loss = 0.06 (61.2 examples/sec; 0.065 sec/batch; 106h:31m:18s remains)
INFO - root - 2019-11-04 02:25:43.908252: step 130880, total loss = 0.32, predict loss = 0.07 (71.3 examples/sec; 0.056 sec/batch; 91h:26m:24s remains)
INFO - root - 2019-11-04 02:25:44.547588: step 130890, total loss = 0.34, predict loss = 0.07 (72.6 examples/sec; 0.055 sec/batch; 89h:46m:41s remains)
INFO - root - 2019-11-04 02:25:45.254465: step 130900, total loss = 0.28, predict loss = 0.06 (73.9 examples/sec; 0.054 sec/batch; 88h:18m:05s remains)
INFO - root - 2019-11-04 02:25:45.946103: step 130910, total loss = 0.29, predict loss = 0.06 (66.5 examples/sec; 0.060 sec/batch; 98h:05m:06s remains)
INFO - root - 2019-11-04 02:25:46.597592: step 130920, total loss = 0.62, predict loss = 0.16 (63.3 examples/sec; 0.063 sec/batch; 103h:05m:52s remains)
INFO - root - 2019-11-04 02:25:47.227923: step 130930, total loss = 0.48, predict loss = 0.12 (72.2 examples/sec; 0.055 sec/batch; 90h:15m:31s remains)
INFO - root - 2019-11-04 02:25:47.872087: step 130940, total loss = 0.51, predict loss = 0.12 (64.9 examples/sec; 0.062 sec/batch; 100h:30m:03s remains)
INFO - root - 2019-11-04 02:25:48.509078: step 130950, total loss = 0.39, predict loss = 0.09 (74.9 examples/sec; 0.053 sec/batch; 87h:05m:39s remains)
INFO - root - 2019-11-04 02:25:49.152480: step 130960, total loss = 0.56, predict loss = 0.13 (71.6 examples/sec; 0.056 sec/batch; 91h:04m:09s remains)
INFO - root - 2019-11-04 02:25:49.740077: step 130970, total loss = 0.61, predict loss = 0.14 (78.3 examples/sec; 0.051 sec/batch; 83h:14m:08s remains)
INFO - root - 2019-11-04 02:25:50.373787: step 130980, total loss = 0.69, predict loss = 0.17 (69.6 examples/sec; 0.058 sec/batch; 93h:44m:58s remains)
INFO - root - 2019-11-04 02:25:50.978455: step 130990, total loss = 0.67, predict loss = 0.15 (67.7 examples/sec; 0.059 sec/batch; 96h:16m:50s remains)
INFO - root - 2019-11-04 02:25:51.587798: step 131000, total loss = 0.69, predict loss = 0.16 (70.3 examples/sec; 0.057 sec/batch; 92h:45m:29s remains)
INFO - root - 2019-11-04 02:25:52.176956: step 131010, total loss = 0.72, predict loss = 0.17 (87.4 examples/sec; 0.046 sec/batch; 74h:36m:35s remains)
INFO - root - 2019-11-04 02:25:52.773180: step 131020, total loss = 0.65, predict loss = 0.16 (80.6 examples/sec; 0.050 sec/batch; 80h:52m:04s remains)
INFO - root - 2019-11-04 02:25:53.365484: step 131030, total loss = 0.74, predict loss = 0.18 (77.8 examples/sec; 0.051 sec/batch; 83h:49m:31s remains)
INFO - root - 2019-11-04 02:25:54.004829: step 131040, total loss = 0.70, predict loss = 0.16 (64.3 examples/sec; 0.062 sec/batch; 101h:26m:31s remains)
INFO - root - 2019-11-04 02:25:54.619687: step 131050, total loss = 0.65, predict loss = 0.15 (71.2 examples/sec; 0.056 sec/batch; 91h:34m:38s remains)
INFO - root - 2019-11-04 02:25:55.229317: step 131060, total loss = 0.51, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 97h:29m:01s remains)
INFO - root - 2019-11-04 02:25:55.827192: step 131070, total loss = 0.49, predict loss = 0.11 (77.9 examples/sec; 0.051 sec/batch; 83h:41m:03s remains)
INFO - root - 2019-11-04 02:25:56.492970: step 131080, total loss = 0.45, predict loss = 0.10 (67.4 examples/sec; 0.059 sec/batch; 96h:43m:03s remains)
INFO - root - 2019-11-04 02:25:57.148755: step 131090, total loss = 0.44, predict loss = 0.10 (72.1 examples/sec; 0.055 sec/batch; 90h:26m:12s remains)
INFO - root - 2019-11-04 02:25:57.816286: step 131100, total loss = 0.44, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 96h:57m:31s remains)
INFO - root - 2019-11-04 02:25:58.436554: step 131110, total loss = 0.52, predict loss = 0.12 (75.5 examples/sec; 0.053 sec/batch; 86h:19m:36s remains)
INFO - root - 2019-11-04 02:25:59.084735: step 131120, total loss = 0.47, predict loss = 0.11 (65.9 examples/sec; 0.061 sec/batch; 98h:55m:17s remains)
INFO - root - 2019-11-04 02:25:59.753772: step 131130, total loss = 0.41, predict loss = 0.09 (72.3 examples/sec; 0.055 sec/batch; 90h:13m:28s remains)
INFO - root - 2019-11-04 02:26:00.424220: step 131140, total loss = 0.55, predict loss = 0.13 (71.7 examples/sec; 0.056 sec/batch; 90h:57m:31s remains)
INFO - root - 2019-11-04 02:26:01.069552: step 131150, total loss = 0.39, predict loss = 0.09 (64.8 examples/sec; 0.062 sec/batch; 100h:35m:07s remains)
INFO - root - 2019-11-04 02:26:01.710295: step 131160, total loss = 0.31, predict loss = 0.07 (72.9 examples/sec; 0.055 sec/batch; 89h:25m:06s remains)
INFO - root - 2019-11-04 02:26:02.353567: step 131170, total loss = 0.50, predict loss = 0.11 (71.2 examples/sec; 0.056 sec/batch; 91h:38m:05s remains)
INFO - root - 2019-11-04 02:26:02.962608: step 131180, total loss = 0.47, predict loss = 0.10 (84.8 examples/sec; 0.047 sec/batch; 76h:53m:18s remains)
INFO - root - 2019-11-04 02:26:03.590649: step 131190, total loss = 0.39, predict loss = 0.09 (77.4 examples/sec; 0.052 sec/batch; 84h:16m:10s remains)
INFO - root - 2019-11-04 02:26:04.237753: step 131200, total loss = 0.49, predict loss = 0.11 (68.9 examples/sec; 0.058 sec/batch; 94h:38m:49s remains)
INFO - root - 2019-11-04 02:26:04.900952: step 131210, total loss = 0.46, predict loss = 0.10 (65.0 examples/sec; 0.062 sec/batch; 100h:15m:51s remains)
INFO - root - 2019-11-04 02:26:05.585736: step 131220, total loss = 0.36, predict loss = 0.08 (57.1 examples/sec; 0.070 sec/batch; 114h:11m:42s remains)
INFO - root - 2019-11-04 02:26:06.249230: step 131230, total loss = 0.40, predict loss = 0.09 (66.0 examples/sec; 0.061 sec/batch; 98h:51m:32s remains)
INFO - root - 2019-11-04 02:26:06.921098: step 131240, total loss = 0.42, predict loss = 0.10 (60.8 examples/sec; 0.066 sec/batch; 107h:09m:58s remains)
INFO - root - 2019-11-04 02:26:07.568611: step 131250, total loss = 0.59, predict loss = 0.14 (70.1 examples/sec; 0.057 sec/batch; 92h:57m:57s remains)
INFO - root - 2019-11-04 02:26:08.207184: step 131260, total loss = 0.32, predict loss = 0.07 (71.5 examples/sec; 0.056 sec/batch; 91h:13m:03s remains)
INFO - root - 2019-11-04 02:26:08.835397: step 131270, total loss = 0.37, predict loss = 0.08 (77.2 examples/sec; 0.052 sec/batch; 84h:25m:17s remains)
INFO - root - 2019-11-04 02:26:09.461054: step 131280, total loss = 0.37, predict loss = 0.08 (67.4 examples/sec; 0.059 sec/batch; 96h:40m:37s remains)
INFO - root - 2019-11-04 02:26:10.112122: step 131290, total loss = 0.36, predict loss = 0.08 (67.3 examples/sec; 0.059 sec/batch; 96h:49m:32s remains)
INFO - root - 2019-11-04 02:26:10.770333: step 131300, total loss = 0.37, predict loss = 0.08 (69.6 examples/sec; 0.057 sec/batch; 93h:37m:53s remains)
INFO - root - 2019-11-04 02:26:11.393312: step 131310, total loss = 0.37, predict loss = 0.08 (66.3 examples/sec; 0.060 sec/batch; 98h:19m:33s remains)
INFO - root - 2019-11-04 02:26:12.043995: step 131320, total loss = 0.41, predict loss = 0.09 (65.1 examples/sec; 0.061 sec/batch; 100h:07m:03s remains)
INFO - root - 2019-11-04 02:26:12.654239: step 131330, total loss = 0.52, predict loss = 0.13 (82.0 examples/sec; 0.049 sec/batch; 79h:29m:24s remains)
INFO - root - 2019-11-04 02:26:13.266614: step 131340, total loss = 0.48, predict loss = 0.12 (75.9 examples/sec; 0.053 sec/batch; 85h:55m:49s remains)
INFO - root - 2019-11-04 02:26:13.900292: step 131350, total loss = 0.26, predict loss = 0.05 (69.4 examples/sec; 0.058 sec/batch; 94h:00m:10s remains)
INFO - root - 2019-11-04 02:26:14.541385: step 131360, total loss = 0.49, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 86h:19m:02s remains)
INFO - root - 2019-11-04 02:26:15.169110: step 131370, total loss = 0.32, predict loss = 0.07 (76.6 examples/sec; 0.052 sec/batch; 85h:05m:40s remains)
INFO - root - 2019-11-04 02:26:15.790857: step 131380, total loss = 0.64, predict loss = 0.15 (76.5 examples/sec; 0.052 sec/batch; 85h:15m:21s remains)
INFO - root - 2019-11-04 02:26:16.426721: step 131390, total loss = 0.63, predict loss = 0.15 (62.7 examples/sec; 0.064 sec/batch; 103h:55m:15s remains)
INFO - root - 2019-11-04 02:26:17.085999: step 131400, total loss = 0.63, predict loss = 0.15 (66.5 examples/sec; 0.060 sec/batch; 98h:03m:01s remains)
INFO - root - 2019-11-04 02:26:17.717973: step 131410, total loss = 0.56, predict loss = 0.14 (70.0 examples/sec; 0.057 sec/batch; 93h:06m:43s remains)
INFO - root - 2019-11-04 02:26:18.302160: step 131420, total loss = 0.58, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 94h:09m:13s remains)
INFO - root - 2019-11-04 02:26:18.920717: step 131430, total loss = 0.55, predict loss = 0.14 (63.3 examples/sec; 0.063 sec/batch; 102h:59m:08s remains)
INFO - root - 2019-11-04 02:26:19.556949: step 131440, total loss = 0.57, predict loss = 0.14 (67.7 examples/sec; 0.059 sec/batch; 96h:19m:53s remains)
INFO - root - 2019-11-04 02:26:20.176711: step 131450, total loss = 0.68, predict loss = 0.16 (76.5 examples/sec; 0.052 sec/batch; 85h:12m:16s remains)
INFO - root - 2019-11-04 02:26:20.784790: step 131460, total loss = 0.63, predict loss = 0.15 (78.0 examples/sec; 0.051 sec/batch; 83h:36m:37s remains)
INFO - root - 2019-11-04 02:26:21.405607: step 131470, total loss = 0.60, predict loss = 0.14 (76.8 examples/sec; 0.052 sec/batch; 84h:57m:04s remains)
INFO - root - 2019-11-04 02:26:22.052630: step 131480, total loss = 0.44, predict loss = 0.09 (62.0 examples/sec; 0.065 sec/batch; 105h:08m:58s remains)
INFO - root - 2019-11-04 02:26:22.676454: step 131490, total loss = 0.60, predict loss = 0.14 (70.5 examples/sec; 0.057 sec/batch; 92h:29m:00s remains)
INFO - root - 2019-11-04 02:26:23.279642: step 131500, total loss = 0.45, predict loss = 0.10 (76.1 examples/sec; 0.053 sec/batch; 85h:41m:04s remains)
INFO - root - 2019-11-04 02:26:23.888997: step 131510, total loss = 0.20, predict loss = 0.04 (74.2 examples/sec; 0.054 sec/batch; 87h:55m:55s remains)
INFO - root - 2019-11-04 02:26:24.503738: step 131520, total loss = 0.44, predict loss = 0.10 (67.8 examples/sec; 0.059 sec/batch; 96h:06m:35s remains)
INFO - root - 2019-11-04 02:26:25.142954: step 131530, total loss = 0.41, predict loss = 0.09 (79.6 examples/sec; 0.050 sec/batch; 81h:52m:22s remains)
INFO - root - 2019-11-04 02:26:25.752966: step 131540, total loss = 0.42, predict loss = 0.10 (71.9 examples/sec; 0.056 sec/batch; 90h:42m:01s remains)
INFO - root - 2019-11-04 02:26:26.357401: step 131550, total loss = 0.47, predict loss = 0.10 (81.0 examples/sec; 0.049 sec/batch; 80h:28m:48s remains)
INFO - root - 2019-11-04 02:26:27.040636: step 131560, total loss = 0.40, predict loss = 0.09 (57.5 examples/sec; 0.070 sec/batch; 113h:20m:13s remains)
INFO - root - 2019-11-04 02:26:27.713796: step 131570, total loss = 0.36, predict loss = 0.08 (63.1 examples/sec; 0.063 sec/batch; 103h:15m:18s remains)
INFO - root - 2019-11-04 02:26:28.321658: step 131580, total loss = 0.42, predict loss = 0.10 (75.2 examples/sec; 0.053 sec/batch; 86h:44m:15s remains)
INFO - root - 2019-11-04 02:26:28.995335: step 131590, total loss = 0.46, predict loss = 0.11 (61.7 examples/sec; 0.065 sec/batch; 105h:39m:01s remains)
INFO - root - 2019-11-04 02:26:29.681086: step 131600, total loss = 0.37, predict loss = 0.08 (64.3 examples/sec; 0.062 sec/batch; 101h:21m:30s remains)
INFO - root - 2019-11-04 02:26:30.348553: step 131610, total loss = 0.59, predict loss = 0.14 (73.8 examples/sec; 0.054 sec/batch; 88h:17m:42s remains)
INFO - root - 2019-11-04 02:26:30.947396: step 131620, total loss = 0.58, predict loss = 0.14 (81.3 examples/sec; 0.049 sec/batch; 80h:13m:35s remains)
INFO - root - 2019-11-04 02:26:31.528388: step 131630, total loss = 0.58, predict loss = 0.14 (75.7 examples/sec; 0.053 sec/batch; 86h:07m:47s remains)
INFO - root - 2019-11-04 02:26:32.148022: step 131640, total loss = 0.49, predict loss = 0.12 (66.3 examples/sec; 0.060 sec/batch; 98h:18m:45s remains)
INFO - root - 2019-11-04 02:26:32.806110: step 131650, total loss = 0.63, predict loss = 0.15 (72.6 examples/sec; 0.055 sec/batch; 89h:45m:33s remains)
INFO - root - 2019-11-04 02:26:33.447914: step 131660, total loss = 0.64, predict loss = 0.15 (68.5 examples/sec; 0.058 sec/batch; 95h:07m:13s remains)
INFO - root - 2019-11-04 02:26:34.058955: step 131670, total loss = 0.54, predict loss = 0.13 (70.9 examples/sec; 0.056 sec/batch; 91h:55m:02s remains)
INFO - root - 2019-11-04 02:26:34.720166: step 131680, total loss = 0.58, predict loss = 0.14 (64.9 examples/sec; 0.062 sec/batch; 100h:27m:08s remains)
INFO - root - 2019-11-04 02:26:35.337639: step 131690, total loss = 0.53, predict loss = 0.12 (74.3 examples/sec; 0.054 sec/batch; 87h:46m:59s remains)
INFO - root - 2019-11-04 02:26:35.955016: step 131700, total loss = 0.58, predict loss = 0.14 (70.4 examples/sec; 0.057 sec/batch; 92h:39m:27s remains)
INFO - root - 2019-11-04 02:26:36.552070: step 131710, total loss = 0.65, predict loss = 0.16 (72.9 examples/sec; 0.055 sec/batch; 89h:25m:37s remains)
INFO - root - 2019-11-04 02:26:37.182226: step 131720, total loss = 0.53, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 101h:24m:33s remains)
INFO - root - 2019-11-04 02:26:37.809928: step 131730, total loss = 0.58, predict loss = 0.13 (68.7 examples/sec; 0.058 sec/batch; 94h:51m:25s remains)
INFO - root - 2019-11-04 02:26:38.444227: step 131740, total loss = 0.50, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 91h:05m:48s remains)
INFO - root - 2019-11-04 02:26:39.077925: step 131750, total loss = 0.55, predict loss = 0.13 (65.8 examples/sec; 0.061 sec/batch; 99h:04m:56s remains)
INFO - root - 2019-11-04 02:26:39.740448: step 131760, total loss = 0.48, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 97h:59m:42s remains)
INFO - root - 2019-11-04 02:26:40.391425: step 131770, total loss = 0.42, predict loss = 0.10 (64.1 examples/sec; 0.062 sec/batch; 101h:39m:07s remains)
INFO - root - 2019-11-04 02:26:41.043519: step 131780, total loss = 0.52, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 96h:55m:58s remains)
INFO - root - 2019-11-04 02:26:41.723112: step 131790, total loss = 0.54, predict loss = 0.13 (63.0 examples/sec; 0.063 sec/batch; 103h:29m:16s remains)
INFO - root - 2019-11-04 02:26:42.361655: step 131800, total loss = 0.38, predict loss = 0.09 (63.5 examples/sec; 0.063 sec/batch; 102h:40m:58s remains)
INFO - root - 2019-11-04 02:26:43.042463: step 131810, total loss = 0.37, predict loss = 0.08 (67.2 examples/sec; 0.060 sec/batch; 97h:03m:16s remains)
INFO - root - 2019-11-04 02:26:43.675584: step 131820, total loss = 0.36, predict loss = 0.08 (69.1 examples/sec; 0.058 sec/batch; 94h:21m:02s remains)
INFO - root - 2019-11-04 02:26:44.340310: step 131830, total loss = 0.34, predict loss = 0.07 (69.5 examples/sec; 0.058 sec/batch; 93h:50m:10s remains)
INFO - root - 2019-11-04 02:26:44.971024: step 131840, total loss = 0.41, predict loss = 0.10 (73.2 examples/sec; 0.055 sec/batch; 89h:07m:18s remains)
INFO - root - 2019-11-04 02:26:45.583037: step 131850, total loss = 0.37, predict loss = 0.08 (68.9 examples/sec; 0.058 sec/batch; 94h:41m:08s remains)
INFO - root - 2019-11-04 02:26:46.218171: step 131860, total loss = 0.49, predict loss = 0.11 (76.5 examples/sec; 0.052 sec/batch; 85h:10m:58s remains)
INFO - root - 2019-11-04 02:26:46.833047: step 131870, total loss = 0.33, predict loss = 0.07 (73.1 examples/sec; 0.055 sec/batch; 89h:13m:34s remains)
INFO - root - 2019-11-04 02:26:47.429148: step 131880, total loss = 0.47, predict loss = 0.11 (76.5 examples/sec; 0.052 sec/batch; 85h:11m:08s remains)
INFO - root - 2019-11-04 02:26:47.992545: step 131890, total loss = 0.52, predict loss = 0.12 (72.2 examples/sec; 0.055 sec/batch; 90h:19m:42s remains)
INFO - root - 2019-11-04 02:26:48.590110: step 131900, total loss = 0.47, predict loss = 0.11 (78.1 examples/sec; 0.051 sec/batch; 83h:25m:56s remains)
INFO - root - 2019-11-04 02:26:49.188422: step 131910, total loss = 0.52, predict loss = 0.12 (77.0 examples/sec; 0.052 sec/batch; 84h:43m:00s remains)
INFO - root - 2019-11-04 02:26:49.806387: step 131920, total loss = 0.50, predict loss = 0.11 (65.9 examples/sec; 0.061 sec/batch; 98h:58m:35s remains)
INFO - root - 2019-11-04 02:26:50.426398: step 131930, total loss = 0.38, predict loss = 0.08 (68.8 examples/sec; 0.058 sec/batch; 94h:42m:21s remains)
INFO - root - 2019-11-04 02:26:51.047072: step 131940, total loss = 0.38, predict loss = 0.08 (68.2 examples/sec; 0.059 sec/batch; 95h:39m:00s remains)
INFO - root - 2019-11-04 02:26:51.658594: step 131950, total loss = 0.52, predict loss = 0.13 (73.7 examples/sec; 0.054 sec/batch; 88h:27m:47s remains)
INFO - root - 2019-11-04 02:26:52.303453: step 131960, total loss = 0.57, predict loss = 0.14 (58.6 examples/sec; 0.068 sec/batch; 111h:18m:44s remains)
INFO - root - 2019-11-04 02:26:52.923120: step 131970, total loss = 0.37, predict loss = 0.09 (72.8 examples/sec; 0.055 sec/batch; 89h:36m:37s remains)
INFO - root - 2019-11-04 02:26:53.564298: step 131980, total loss = 0.41, predict loss = 0.10 (68.9 examples/sec; 0.058 sec/batch; 94h:39m:14s remains)
INFO - root - 2019-11-04 02:26:54.164802: step 131990, total loss = 0.54, predict loss = 0.13 (75.4 examples/sec; 0.053 sec/batch; 86h:26m:20s remains)
INFO - root - 2019-11-04 02:26:54.797794: step 132000, total loss = 0.44, predict loss = 0.10 (68.2 examples/sec; 0.059 sec/batch; 95h:33m:41s remains)
INFO - root - 2019-11-04 02:26:55.419648: step 132010, total loss = 0.69, predict loss = 0.16 (74.5 examples/sec; 0.054 sec/batch; 87h:34m:16s remains)
INFO - root - 2019-11-04 02:26:56.036717: step 132020, total loss = 0.56, predict loss = 0.14 (78.3 examples/sec; 0.051 sec/batch; 83h:13m:37s remains)
INFO - root - 2019-11-04 02:26:56.647540: step 132030, total loss = 0.57, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 94h:16m:28s remains)
INFO - root - 2019-11-04 02:26:57.278626: step 132040, total loss = 0.53, predict loss = 0.12 (70.9 examples/sec; 0.056 sec/batch; 91h:54m:01s remains)
INFO - root - 2019-11-04 02:26:57.879156: step 132050, total loss = 0.64, predict loss = 0.15 (70.6 examples/sec; 0.057 sec/batch; 92h:20m:45s remains)
INFO - root - 2019-11-04 02:26:58.512192: step 132060, total loss = 0.62, predict loss = 0.15 (77.9 examples/sec; 0.051 sec/batch; 83h:41m:17s remains)
INFO - root - 2019-11-04 02:26:59.109311: step 132070, total loss = 0.65, predict loss = 0.16 (77.8 examples/sec; 0.051 sec/batch; 83h:45m:35s remains)
INFO - root - 2019-11-04 02:26:59.741914: step 132080, total loss = 0.56, predict loss = 0.12 (70.0 examples/sec; 0.057 sec/batch; 93h:05m:21s remains)
INFO - root - 2019-11-04 02:27:00.360952: step 132090, total loss = 0.52, predict loss = 0.12 (68.5 examples/sec; 0.058 sec/batch; 95h:09m:58s remains)
INFO - root - 2019-11-04 02:27:00.972527: step 132100, total loss = 0.27, predict loss = 0.06 (79.4 examples/sec; 0.050 sec/batch; 82h:04m:46s remains)
INFO - root - 2019-11-04 02:27:01.603941: step 132110, total loss = 0.65, predict loss = 0.16 (64.9 examples/sec; 0.062 sec/batch; 100h:32m:02s remains)
INFO - root - 2019-11-04 02:27:02.222551: step 132120, total loss = 0.39, predict loss = 0.08 (74.9 examples/sec; 0.053 sec/batch; 87h:06m:11s remains)
INFO - root - 2019-11-04 02:27:02.847145: step 132130, total loss = 0.48, predict loss = 0.11 (63.4 examples/sec; 0.063 sec/batch; 102h:50m:35s remains)
INFO - root - 2019-11-04 02:27:03.485462: step 132140, total loss = 0.60, predict loss = 0.14 (68.0 examples/sec; 0.059 sec/batch; 95h:50m:51s remains)
INFO - root - 2019-11-04 02:27:04.095086: step 132150, total loss = 0.35, predict loss = 0.08 (73.5 examples/sec; 0.054 sec/batch; 88h:44m:00s remains)
INFO - root - 2019-11-04 02:27:04.719246: step 132160, total loss = 0.36, predict loss = 0.08 (63.7 examples/sec; 0.063 sec/batch; 102h:18m:32s remains)
INFO - root - 2019-11-04 02:27:05.344198: step 132170, total loss = 0.48, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 89h:21m:46s remains)
INFO - root - 2019-11-04 02:27:05.948570: step 132180, total loss = 0.42, predict loss = 0.10 (68.9 examples/sec; 0.058 sec/batch; 94h:37m:00s remains)
INFO - root - 2019-11-04 02:27:06.584832: step 132190, total loss = 0.45, predict loss = 0.10 (68.9 examples/sec; 0.058 sec/batch; 94h:36m:37s remains)
INFO - root - 2019-11-04 02:27:07.218662: step 132200, total loss = 0.52, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 94h:54m:33s remains)
INFO - root - 2019-11-04 02:27:07.861528: step 132210, total loss = 0.51, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 101h:19m:42s remains)
INFO - root - 2019-11-04 02:27:08.482005: step 132220, total loss = 0.46, predict loss = 0.11 (75.4 examples/sec; 0.053 sec/batch; 86h:27m:42s remains)
INFO - root - 2019-11-04 02:27:09.127435: step 132230, total loss = 0.50, predict loss = 0.11 (68.5 examples/sec; 0.058 sec/batch; 95h:12m:41s remains)
INFO - root - 2019-11-04 02:27:09.742739: step 132240, total loss = 0.22, predict loss = 0.05 (78.1 examples/sec; 0.051 sec/batch; 83h:30m:26s remains)
INFO - root - 2019-11-04 02:27:10.361869: step 132250, total loss = 0.34, predict loss = 0.07 (85.6 examples/sec; 0.047 sec/batch; 76h:11m:21s remains)
INFO - root - 2019-11-04 02:27:11.413916: step 132260, total loss = 0.59, predict loss = 0.13 (87.3 examples/sec; 0.046 sec/batch; 74h:40m:17s remains)
INFO - root - 2019-11-04 02:27:11.863804: step 132270, total loss = 0.46, predict loss = 0.10 (96.9 examples/sec; 0.041 sec/batch; 67h:18m:21s remains)
INFO - root - 2019-11-04 02:27:12.930884: step 132280, total loss = 0.43, predict loss = 0.09 (64.1 examples/sec; 0.062 sec/batch; 101h:41m:05s remains)
INFO - root - 2019-11-04 02:27:13.587270: step 132290, total loss = 0.22, predict loss = 0.04 (70.3 examples/sec; 0.057 sec/batch; 92h:44m:03s remains)
INFO - root - 2019-11-04 02:27:14.205044: step 132300, total loss = 0.55, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 92h:19m:04s remains)
INFO - root - 2019-11-04 02:27:14.830572: step 132310, total loss = 0.44, predict loss = 0.10 (69.2 examples/sec; 0.058 sec/batch; 94h:16m:57s remains)
INFO - root - 2019-11-04 02:27:15.446409: step 132320, total loss = 0.56, predict loss = 0.13 (82.3 examples/sec; 0.049 sec/batch; 79h:12m:46s remains)
INFO - root - 2019-11-04 02:27:16.079399: step 132330, total loss = 0.53, predict loss = 0.13 (74.3 examples/sec; 0.054 sec/batch; 87h:46m:55s remains)
INFO - root - 2019-11-04 02:27:16.713206: step 132340, total loss = 0.74, predict loss = 0.18 (65.7 examples/sec; 0.061 sec/batch; 99h:12m:20s remains)
INFO - root - 2019-11-04 02:27:17.392540: step 132350, total loss = 0.63, predict loss = 0.15 (60.2 examples/sec; 0.066 sec/batch; 108h:22m:00s remains)
INFO - root - 2019-11-04 02:27:17.969309: step 132360, total loss = 0.58, predict loss = 0.13 (83.8 examples/sec; 0.048 sec/batch; 77h:49m:24s remains)
INFO - root - 2019-11-04 02:27:18.563934: step 132370, total loss = 0.57, predict loss = 0.13 (72.5 examples/sec; 0.055 sec/batch; 89h:52m:22s remains)
INFO - root - 2019-11-04 02:27:19.173839: step 132380, total loss = 0.48, predict loss = 0.12 (64.9 examples/sec; 0.062 sec/batch; 100h:28m:21s remains)
INFO - root - 2019-11-04 02:27:19.803568: step 132390, total loss = 0.48, predict loss = 0.11 (74.3 examples/sec; 0.054 sec/batch; 87h:43m:43s remains)
INFO - root - 2019-11-04 02:27:20.439268: step 132400, total loss = 0.58, predict loss = 0.14 (77.8 examples/sec; 0.051 sec/batch; 83h:49m:51s remains)
INFO - root - 2019-11-04 02:27:21.069416: step 132410, total loss = 0.47, predict loss = 0.11 (72.6 examples/sec; 0.055 sec/batch; 89h:44m:42s remains)
INFO - root - 2019-11-04 02:27:21.678629: step 132420, total loss = 0.29, predict loss = 0.06 (71.4 examples/sec; 0.056 sec/batch; 91h:18m:06s remains)
INFO - root - 2019-11-04 02:27:22.342491: step 132430, total loss = 0.54, predict loss = 0.13 (78.4 examples/sec; 0.051 sec/batch; 83h:06m:30s remains)
INFO - root - 2019-11-04 02:27:22.993645: step 132440, total loss = 0.46, predict loss = 0.10 (75.9 examples/sec; 0.053 sec/batch; 85h:53m:54s remains)
INFO - root - 2019-11-04 02:27:23.648636: step 132450, total loss = 0.47, predict loss = 0.10 (63.7 examples/sec; 0.063 sec/batch; 102h:23m:13s remains)
INFO - root - 2019-11-04 02:27:24.323114: step 132460, total loss = 0.40, predict loss = 0.09 (64.2 examples/sec; 0.062 sec/batch; 101h:33m:05s remains)
INFO - root - 2019-11-04 02:27:24.991928: step 132470, total loss = 0.40, predict loss = 0.09 (72.5 examples/sec; 0.055 sec/batch; 89h:52m:21s remains)
INFO - root - 2019-11-04 02:27:25.636599: step 132480, total loss = 0.35, predict loss = 0.08 (66.9 examples/sec; 0.060 sec/batch; 97h:26m:33s remains)
INFO - root - 2019-11-04 02:27:26.273439: step 132490, total loss = 0.31, predict loss = 0.07 (67.2 examples/sec; 0.060 sec/batch; 97h:04m:22s remains)
INFO - root - 2019-11-04 02:27:26.916299: step 132500, total loss = 0.49, predict loss = 0.12 (74.3 examples/sec; 0.054 sec/batch; 87h:43m:57s remains)
INFO - root - 2019-11-04 02:27:27.529207: step 132510, total loss = 0.47, predict loss = 0.11 (76.0 examples/sec; 0.053 sec/batch; 85h:47m:23s remains)
INFO - root - 2019-11-04 02:27:28.159083: step 132520, total loss = 0.41, predict loss = 0.10 (77.1 examples/sec; 0.052 sec/batch; 84h:32m:11s remains)
INFO - root - 2019-11-04 02:27:28.819740: step 132530, total loss = 0.45, predict loss = 0.10 (66.1 examples/sec; 0.060 sec/batch; 98h:36m:21s remains)
INFO - root - 2019-11-04 02:27:29.438023: step 132540, total loss = 0.55, predict loss = 0.13 (76.3 examples/sec; 0.052 sec/batch; 85h:23m:43s remains)
INFO - root - 2019-11-04 02:27:30.090195: step 132550, total loss = 0.42, predict loss = 0.09 (64.8 examples/sec; 0.062 sec/batch; 100h:38m:16s remains)
INFO - root - 2019-11-04 02:27:30.728476: step 132560, total loss = 0.46, predict loss = 0.10 (72.6 examples/sec; 0.055 sec/batch; 89h:49m:39s remains)
INFO - root - 2019-11-04 02:27:31.351393: step 132570, total loss = 0.62, predict loss = 0.16 (73.2 examples/sec; 0.055 sec/batch; 89h:07m:15s remains)
INFO - root - 2019-11-04 02:27:32.009004: step 132580, total loss = 0.56, predict loss = 0.12 (74.0 examples/sec; 0.054 sec/batch; 88h:08m:18s remains)
INFO - root - 2019-11-04 02:27:32.641199: step 132590, total loss = 0.53, predict loss = 0.12 (75.0 examples/sec; 0.053 sec/batch; 86h:54m:36s remains)
INFO - root - 2019-11-04 02:27:33.277241: step 132600, total loss = 0.46, predict loss = 0.11 (72.5 examples/sec; 0.055 sec/batch; 89h:56m:06s remains)
INFO - root - 2019-11-04 02:27:33.901802: step 132610, total loss = 0.48, predict loss = 0.11 (67.6 examples/sec; 0.059 sec/batch; 96h:25m:49s remains)
INFO - root - 2019-11-04 02:27:34.477513: step 132620, total loss = 0.53, predict loss = 0.12 (82.6 examples/sec; 0.048 sec/batch; 78h:58m:22s remains)
INFO - root - 2019-11-04 02:27:35.075125: step 132630, total loss = 0.46, predict loss = 0.11 (65.2 examples/sec; 0.061 sec/batch; 99h:56m:12s remains)
INFO - root - 2019-11-04 02:27:35.705548: step 132640, total loss = 0.44, predict loss = 0.10 (70.7 examples/sec; 0.057 sec/batch; 92h:14m:34s remains)
INFO - root - 2019-11-04 02:27:36.323972: step 132650, total loss = 0.47, predict loss = 0.11 (78.4 examples/sec; 0.051 sec/batch; 83h:12m:10s remains)
INFO - root - 2019-11-04 02:27:36.917785: step 132660, total loss = 0.40, predict loss = 0.09 (88.3 examples/sec; 0.045 sec/batch; 73h:49m:50s remains)
INFO - root - 2019-11-04 02:27:37.535820: step 132670, total loss = 0.45, predict loss = 0.10 (69.0 examples/sec; 0.058 sec/batch; 94h:30m:02s remains)
INFO - root - 2019-11-04 02:27:38.166653: step 132680, total loss = 0.51, predict loss = 0.11 (69.1 examples/sec; 0.058 sec/batch; 94h:21m:25s remains)
INFO - root - 2019-11-04 02:27:38.822305: step 132690, total loss = 0.52, predict loss = 0.12 (64.8 examples/sec; 0.062 sec/batch; 100h:40m:45s remains)
INFO - root - 2019-11-04 02:27:39.416513: step 132700, total loss = 0.49, predict loss = 0.11 (83.6 examples/sec; 0.048 sec/batch; 78h:00m:30s remains)
INFO - root - 2019-11-04 02:27:40.019291: step 132710, total loss = 0.43, predict loss = 0.10 (82.6 examples/sec; 0.048 sec/batch; 78h:57m:02s remains)
INFO - root - 2019-11-04 02:27:40.650946: step 132720, total loss = 0.55, predict loss = 0.13 (73.5 examples/sec; 0.054 sec/batch; 88h:41m:09s remains)
INFO - root - 2019-11-04 02:27:41.241168: step 132730, total loss = 0.34, predict loss = 0.08 (82.6 examples/sec; 0.048 sec/batch; 78h:55m:04s remains)
INFO - root - 2019-11-04 02:27:41.862750: step 132740, total loss = 0.31, predict loss = 0.07 (74.4 examples/sec; 0.054 sec/batch; 87h:38m:18s remains)
INFO - root - 2019-11-04 02:27:42.542936: step 132750, total loss = 0.41, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 98h:59m:46s remains)
INFO - root - 2019-11-04 02:27:43.174046: step 132760, total loss = 0.78, predict loss = 0.20 (75.4 examples/sec; 0.053 sec/batch; 86h:27m:20s remains)
INFO - root - 2019-11-04 02:27:43.774262: step 132770, total loss = 0.42, predict loss = 0.10 (72.8 examples/sec; 0.055 sec/batch; 89h:32m:21s remains)
INFO - root - 2019-11-04 02:27:44.407430: step 132780, total loss = 0.44, predict loss = 0.10 (71.5 examples/sec; 0.056 sec/batch; 91h:07m:05s remains)
INFO - root - 2019-11-04 02:27:45.039313: step 132790, total loss = 0.53, predict loss = 0.13 (64.0 examples/sec; 0.063 sec/batch; 101h:53m:47s remains)
INFO - root - 2019-11-04 02:27:45.651828: step 132800, total loss = 0.57, predict loss = 0.13 (72.3 examples/sec; 0.055 sec/batch; 90h:13m:06s remains)
INFO - root - 2019-11-04 02:27:46.238477: step 132810, total loss = 0.38, predict loss = 0.08 (76.8 examples/sec; 0.052 sec/batch; 84h:54m:36s remains)
INFO - root - 2019-11-04 02:27:46.828916: step 132820, total loss = 0.46, predict loss = 0.11 (75.0 examples/sec; 0.053 sec/batch; 86h:54m:08s remains)
INFO - root - 2019-11-04 02:27:47.429212: step 132830, total loss = 0.46, predict loss = 0.12 (78.2 examples/sec; 0.051 sec/batch; 83h:22m:53s remains)
INFO - root - 2019-11-04 02:27:48.086245: step 132840, total loss = 0.35, predict loss = 0.08 (78.3 examples/sec; 0.051 sec/batch; 83h:17m:00s remains)
INFO - root - 2019-11-04 02:27:48.750750: step 132850, total loss = 0.34, predict loss = 0.09 (82.9 examples/sec; 0.048 sec/batch; 78h:37m:36s remains)
INFO - root - 2019-11-04 02:27:49.383617: step 132860, total loss = 0.36, predict loss = 0.09 (67.0 examples/sec; 0.060 sec/batch; 97h:15m:45s remains)
INFO - root - 2019-11-04 02:27:49.991501: step 132870, total loss = 0.43, predict loss = 0.10 (79.5 examples/sec; 0.050 sec/batch; 82h:03m:01s remains)
INFO - root - 2019-11-04 02:27:50.611965: step 132880, total loss = 0.36, predict loss = 0.08 (70.4 examples/sec; 0.057 sec/batch; 92h:34m:45s remains)
INFO - root - 2019-11-04 02:27:51.224877: step 132890, total loss = 0.43, predict loss = 0.10 (72.2 examples/sec; 0.055 sec/batch; 90h:16m:19s remains)
INFO - root - 2019-11-04 02:27:51.812278: step 132900, total loss = 0.31, predict loss = 0.07 (73.7 examples/sec; 0.054 sec/batch; 88h:24m:26s remains)
INFO - root - 2019-11-04 02:27:52.408422: step 132910, total loss = 0.37, predict loss = 0.08 (69.5 examples/sec; 0.058 sec/batch; 93h:49m:19s remains)
INFO - root - 2019-11-04 02:27:53.036906: step 132920, total loss = 0.31, predict loss = 0.07 (66.3 examples/sec; 0.060 sec/batch; 98h:22m:37s remains)
INFO - root - 2019-11-04 02:27:53.678323: step 132930, total loss = 0.39, predict loss = 0.09 (71.0 examples/sec; 0.056 sec/batch; 91h:51m:12s remains)
INFO - root - 2019-11-04 02:27:54.328707: step 132940, total loss = 0.40, predict loss = 0.09 (67.8 examples/sec; 0.059 sec/batch; 96h:12m:03s remains)
INFO - root - 2019-11-04 02:27:54.953088: step 132950, total loss = 0.41, predict loss = 0.09 (75.4 examples/sec; 0.053 sec/batch; 86h:27m:00s remains)
INFO - root - 2019-11-04 02:27:55.571608: step 132960, total loss = 0.64, predict loss = 0.15 (75.3 examples/sec; 0.053 sec/batch; 86h:33m:08s remains)
INFO - root - 2019-11-04 02:27:56.166684: step 132970, total loss = 0.62, predict loss = 0.14 (68.2 examples/sec; 0.059 sec/batch; 95h:31m:18s remains)
INFO - root - 2019-11-04 02:27:56.749814: step 132980, total loss = 0.59, predict loss = 0.14 (76.3 examples/sec; 0.052 sec/batch; 85h:27m:55s remains)
INFO - root - 2019-11-04 02:27:57.346225: step 132990, total loss = 0.66, predict loss = 0.16 (69.4 examples/sec; 0.058 sec/batch; 93h:59m:16s remains)
INFO - root - 2019-11-04 02:27:57.970196: step 133000, total loss = 0.41, predict loss = 0.10 (68.9 examples/sec; 0.058 sec/batch; 94h:34m:32s remains)
INFO - root - 2019-11-04 02:27:58.620136: step 133010, total loss = 0.57, predict loss = 0.13 (62.0 examples/sec; 0.065 sec/batch; 105h:12m:02s remains)
INFO - root - 2019-11-04 02:27:59.251900: step 133020, total loss = 0.46, predict loss = 0.11 (70.9 examples/sec; 0.056 sec/batch; 91h:53m:13s remains)
INFO - root - 2019-11-04 02:27:59.911949: step 133030, total loss = 0.43, predict loss = 0.10 (62.5 examples/sec; 0.064 sec/batch; 104h:16m:03s remains)
INFO - root - 2019-11-04 02:28:00.579686: step 133040, total loss = 0.55, predict loss = 0.13 (66.6 examples/sec; 0.060 sec/batch; 97h:51m:52s remains)
INFO - root - 2019-11-04 02:28:01.211254: step 133050, total loss = 0.40, predict loss = 0.09 (74.3 examples/sec; 0.054 sec/batch; 87h:47m:32s remains)
INFO - root - 2019-11-04 02:28:01.812293: step 133060, total loss = 0.54, predict loss = 0.12 (80.2 examples/sec; 0.050 sec/batch; 81h:14m:33s remains)
INFO - root - 2019-11-04 02:28:02.454773: step 133070, total loss = 0.35, predict loss = 0.08 (72.5 examples/sec; 0.055 sec/batch; 89h:56m:27s remains)
INFO - root - 2019-11-04 02:28:03.059974: step 133080, total loss = 0.47, predict loss = 0.11 (81.8 examples/sec; 0.049 sec/batch; 79h:40m:46s remains)
INFO - root - 2019-11-04 02:28:03.699482: step 133090, total loss = 0.53, predict loss = 0.14 (75.0 examples/sec; 0.053 sec/batch; 86h:52m:37s remains)
INFO - root - 2019-11-04 02:28:04.357373: step 133100, total loss = 0.36, predict loss = 0.08 (64.7 examples/sec; 0.062 sec/batch; 100h:41m:59s remains)
INFO - root - 2019-11-04 02:28:04.982996: step 133110, total loss = 0.23, predict loss = 0.05 (70.5 examples/sec; 0.057 sec/batch; 92h:31m:33s remains)
INFO - root - 2019-11-04 02:28:05.632754: step 133120, total loss = 0.22, predict loss = 0.04 (64.8 examples/sec; 0.062 sec/batch; 100h:32m:51s remains)
INFO - root - 2019-11-04 02:28:06.282025: step 133130, total loss = 0.32, predict loss = 0.07 (66.6 examples/sec; 0.060 sec/batch; 97h:53m:55s remains)
INFO - root - 2019-11-04 02:28:06.892157: step 133140, total loss = 0.20, predict loss = 0.04 (68.0 examples/sec; 0.059 sec/batch; 95h:50m:57s remains)
INFO - root - 2019-11-04 02:28:07.519191: step 133150, total loss = 0.31, predict loss = 0.07 (75.6 examples/sec; 0.053 sec/batch; 86h:10m:25s remains)
INFO - root - 2019-11-04 02:28:08.133787: step 133160, total loss = 0.34, predict loss = 0.07 (74.9 examples/sec; 0.053 sec/batch; 87h:01m:57s remains)
INFO - root - 2019-11-04 02:28:08.754051: step 133170, total loss = 0.40, predict loss = 0.09 (69.8 examples/sec; 0.057 sec/batch; 93h:20m:20s remains)
INFO - root - 2019-11-04 02:28:09.346439: step 133180, total loss = 0.41, predict loss = 0.09 (74.2 examples/sec; 0.054 sec/batch; 87h:52m:56s remains)
INFO - root - 2019-11-04 02:28:09.963840: step 133190, total loss = 0.38, predict loss = 0.08 (70.3 examples/sec; 0.057 sec/batch; 92h:40m:03s remains)
INFO - root - 2019-11-04 02:28:10.579970: step 133200, total loss = 0.40, predict loss = 0.09 (64.7 examples/sec; 0.062 sec/batch; 100h:42m:09s remains)
INFO - root - 2019-11-04 02:28:11.217981: step 133210, total loss = 0.50, predict loss = 0.11 (64.9 examples/sec; 0.062 sec/batch; 100h:22m:26s remains)
INFO - root - 2019-11-04 02:28:11.890489: step 133220, total loss = 0.31, predict loss = 0.07 (57.3 examples/sec; 0.070 sec/batch; 113h:43m:31s remains)
INFO - root - 2019-11-04 02:28:12.512322: step 133230, total loss = 0.48, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 98h:13m:26s remains)
INFO - root - 2019-11-04 02:28:13.128576: step 133240, total loss = 0.36, predict loss = 0.08 (65.7 examples/sec; 0.061 sec/batch; 99h:11m:46s remains)
INFO - root - 2019-11-04 02:28:13.827035: step 133250, total loss = 0.34, predict loss = 0.07 (65.8 examples/sec; 0.061 sec/batch; 98h:59m:53s remains)
INFO - root - 2019-11-04 02:28:14.424819: step 133260, total loss = 0.56, predict loss = 0.14 (75.0 examples/sec; 0.053 sec/batch; 86h:52m:00s remains)
INFO - root - 2019-11-04 02:28:15.053011: step 133270, total loss = 0.39, predict loss = 0.09 (62.7 examples/sec; 0.064 sec/batch; 103h:56m:54s remains)
INFO - root - 2019-11-04 02:28:15.679157: step 133280, total loss = 0.45, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 96h:55m:03s remains)
INFO - root - 2019-11-04 02:28:16.326424: step 133290, total loss = 0.44, predict loss = 0.11 (73.7 examples/sec; 0.054 sec/batch; 88h:23m:44s remains)
INFO - root - 2019-11-04 02:28:16.972823: step 133300, total loss = 0.46, predict loss = 0.11 (66.1 examples/sec; 0.061 sec/batch; 98h:38m:56s remains)
INFO - root - 2019-11-04 02:28:17.614671: step 133310, total loss = 0.39, predict loss = 0.09 (75.1 examples/sec; 0.053 sec/batch; 86h:48m:57s remains)
INFO - root - 2019-11-04 02:28:18.235472: step 133320, total loss = 0.38, predict loss = 0.10 (76.0 examples/sec; 0.053 sec/batch; 85h:45m:00s remains)
INFO - root - 2019-11-04 02:28:18.849445: step 133330, total loss = 0.40, predict loss = 0.10 (64.3 examples/sec; 0.062 sec/batch; 101h:22m:20s remains)
INFO - root - 2019-11-04 02:28:19.515096: step 133340, total loss = 0.52, predict loss = 0.13 (68.0 examples/sec; 0.059 sec/batch; 95h:48m:09s remains)
INFO - root - 2019-11-04 02:28:20.139755: step 133350, total loss = 0.41, predict loss = 0.10 (75.5 examples/sec; 0.053 sec/batch; 86h:19m:13s remains)
INFO - root - 2019-11-04 02:28:20.756735: step 133360, total loss = 0.26, predict loss = 0.06 (69.3 examples/sec; 0.058 sec/batch; 94h:02m:11s remains)
INFO - root - 2019-11-04 02:28:21.408741: step 133370, total loss = 0.26, predict loss = 0.05 (71.2 examples/sec; 0.056 sec/batch; 91h:35m:08s remains)
INFO - root - 2019-11-04 02:28:22.051662: step 133380, total loss = 0.37, predict loss = 0.09 (68.3 examples/sec; 0.059 sec/batch; 95h:22m:20s remains)
INFO - root - 2019-11-04 02:28:22.673395: step 133390, total loss = 0.31, predict loss = 0.07 (79.6 examples/sec; 0.050 sec/batch; 81h:54m:20s remains)
INFO - root - 2019-11-04 02:28:23.301673: step 133400, total loss = 0.41, predict loss = 0.10 (66.2 examples/sec; 0.060 sec/batch; 98h:28m:47s remains)
INFO - root - 2019-11-04 02:28:23.912255: step 133410, total loss = 0.37, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 93h:49m:55s remains)
INFO - root - 2019-11-04 02:28:24.521565: step 133420, total loss = 0.37, predict loss = 0.08 (74.2 examples/sec; 0.054 sec/batch; 87h:47m:58s remains)
INFO - root - 2019-11-04 02:28:25.157078: step 133430, total loss = 0.54, predict loss = 0.13 (64.1 examples/sec; 0.062 sec/batch; 101h:42m:04s remains)
INFO - root - 2019-11-04 02:28:25.747114: step 133440, total loss = 0.53, predict loss = 0.12 (76.5 examples/sec; 0.052 sec/batch; 85h:15m:37s remains)
INFO - root - 2019-11-04 02:28:26.366166: step 133450, total loss = 0.50, predict loss = 0.12 (72.9 examples/sec; 0.055 sec/batch; 89h:22m:25s remains)
INFO - root - 2019-11-04 02:28:27.008180: step 133460, total loss = 0.45, predict loss = 0.10 (70.4 examples/sec; 0.057 sec/batch; 92h:33m:34s remains)
INFO - root - 2019-11-04 02:28:27.665909: step 133470, total loss = 0.49, predict loss = 0.12 (74.8 examples/sec; 0.053 sec/batch; 87h:08m:02s remains)
INFO - root - 2019-11-04 02:28:28.276913: step 133480, total loss = 0.28, predict loss = 0.07 (75.9 examples/sec; 0.053 sec/batch; 85h:53m:09s remains)
INFO - root - 2019-11-04 02:28:28.891632: step 133490, total loss = 0.34, predict loss = 0.08 (77.5 examples/sec; 0.052 sec/batch; 84h:05m:00s remains)
INFO - root - 2019-11-04 02:28:29.524382: step 133500, total loss = 0.19, predict loss = 0.04 (68.8 examples/sec; 0.058 sec/batch; 94h:42m:41s remains)
INFO - root - 2019-11-04 02:28:30.138164: step 133510, total loss = 0.39, predict loss = 0.09 (70.3 examples/sec; 0.057 sec/batch; 92h:39m:49s remains)
INFO - root - 2019-11-04 02:28:30.770506: step 133520, total loss = 0.45, predict loss = 0.10 (71.7 examples/sec; 0.056 sec/batch; 90h:58m:09s remains)
INFO - root - 2019-11-04 02:28:31.416831: step 133530, total loss = 0.49, predict loss = 0.11 (69.9 examples/sec; 0.057 sec/batch; 93h:17m:43s remains)
INFO - root - 2019-11-04 02:28:32.090736: step 133540, total loss = 0.54, predict loss = 0.13 (62.3 examples/sec; 0.064 sec/batch; 104h:39m:35s remains)
INFO - root - 2019-11-04 02:28:32.782120: step 133550, total loss = 0.46, predict loss = 0.10 (61.9 examples/sec; 0.065 sec/batch; 105h:21m:56s remains)
INFO - root - 2019-11-04 02:28:33.398237: step 133560, total loss = 0.52, predict loss = 0.13 (64.9 examples/sec; 0.062 sec/batch; 100h:26m:02s remains)
INFO - root - 2019-11-04 02:28:34.022701: step 133570, total loss = 0.57, predict loss = 0.13 (66.7 examples/sec; 0.060 sec/batch; 97h:42m:45s remains)
INFO - root - 2019-11-04 02:28:34.664735: step 133580, total loss = 0.48, predict loss = 0.11 (65.9 examples/sec; 0.061 sec/batch; 98h:56m:20s remains)
INFO - root - 2019-11-04 02:28:35.269080: step 133590, total loss = 0.25, predict loss = 0.05 (64.8 examples/sec; 0.062 sec/batch; 100h:37m:24s remains)
INFO - root - 2019-11-04 02:28:35.892336: step 133600, total loss = 0.44, predict loss = 0.10 (86.2 examples/sec; 0.046 sec/batch; 75h:39m:19s remains)
INFO - root - 2019-11-04 02:28:36.491181: step 133610, total loss = 0.49, predict loss = 0.12 (72.3 examples/sec; 0.055 sec/batch; 90h:06m:00s remains)
INFO - root - 2019-11-04 02:28:37.071126: step 133620, total loss = 0.31, predict loss = 0.07 (77.5 examples/sec; 0.052 sec/batch; 84h:03m:14s remains)
INFO - root - 2019-11-04 02:28:37.662050: step 133630, total loss = 0.38, predict loss = 0.09 (76.3 examples/sec; 0.052 sec/batch; 85h:27m:20s remains)
INFO - root - 2019-11-04 02:28:38.253824: step 133640, total loss = 0.47, predict loss = 0.10 (64.3 examples/sec; 0.062 sec/batch; 101h:21m:00s remains)
INFO - root - 2019-11-04 02:28:38.887714: step 133650, total loss = 0.41, predict loss = 0.09 (69.8 examples/sec; 0.057 sec/batch; 93h:21m:53s remains)
INFO - root - 2019-11-04 02:28:39.521624: step 133660, total loss = 0.41, predict loss = 0.09 (69.0 examples/sec; 0.058 sec/batch; 94h:28m:33s remains)
INFO - root - 2019-11-04 02:28:40.184600: step 133670, total loss = 0.54, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 96h:51m:42s remains)
INFO - root - 2019-11-04 02:28:40.846612: step 133680, total loss = 0.43, predict loss = 0.10 (66.0 examples/sec; 0.061 sec/batch; 98h:46m:20s remains)
INFO - root - 2019-11-04 02:28:41.505209: step 133690, total loss = 0.53, predict loss = 0.13 (68.0 examples/sec; 0.059 sec/batch; 95h:49m:53s remains)
INFO - root - 2019-11-04 02:28:42.177800: step 133700, total loss = 0.81, predict loss = 0.19 (75.1 examples/sec; 0.053 sec/batch; 86h:49m:18s remains)
INFO - root - 2019-11-04 02:28:42.842427: step 133710, total loss = 0.80, predict loss = 0.19 (64.2 examples/sec; 0.062 sec/batch; 101h:29m:31s remains)
INFO - root - 2019-11-04 02:28:43.540372: step 133720, total loss = 0.57, predict loss = 0.13 (60.0 examples/sec; 0.067 sec/batch; 108h:43m:23s remains)
INFO - root - 2019-11-04 02:28:44.158456: step 133730, total loss = 0.71, predict loss = 0.16 (73.5 examples/sec; 0.054 sec/batch; 88h:38m:16s remains)
INFO - root - 2019-11-04 02:28:44.772873: step 133740, total loss = 0.61, predict loss = 0.14 (72.5 examples/sec; 0.055 sec/batch; 89h:57m:40s remains)
INFO - root - 2019-11-04 02:28:45.402558: step 133750, total loss = 0.54, predict loss = 0.12 (65.8 examples/sec; 0.061 sec/batch; 99h:06m:20s remains)
INFO - root - 2019-11-04 02:28:46.103606: step 133760, total loss = 0.73, predict loss = 0.18 (62.6 examples/sec; 0.064 sec/batch; 104h:03m:06s remains)
INFO - root - 2019-11-04 02:28:46.741492: step 133770, total loss = 0.61, predict loss = 0.14 (64.4 examples/sec; 0.062 sec/batch; 101h:08m:09s remains)
INFO - root - 2019-11-04 02:28:47.350323: step 133780, total loss = 0.49, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 97h:15m:04s remains)
INFO - root - 2019-11-04 02:28:47.977329: step 133790, total loss = 0.68, predict loss = 0.16 (71.5 examples/sec; 0.056 sec/batch; 91h:07m:16s remains)
INFO - root - 2019-11-04 02:28:48.626847: step 133800, total loss = 0.53, predict loss = 0.12 (64.8 examples/sec; 0.062 sec/batch; 100h:36m:31s remains)
INFO - root - 2019-11-04 02:28:49.254233: step 133810, total loss = 0.45, predict loss = 0.10 (60.4 examples/sec; 0.066 sec/batch; 107h:50m:18s remains)
INFO - root - 2019-11-04 02:28:49.924719: step 133820, total loss = 0.57, predict loss = 0.13 (64.9 examples/sec; 0.062 sec/batch; 100h:26m:51s remains)
INFO - root - 2019-11-04 02:28:50.563196: step 133830, total loss = 0.51, predict loss = 0.11 (83.7 examples/sec; 0.048 sec/batch; 77h:54m:03s remains)
INFO - root - 2019-11-04 02:28:51.173100: step 133840, total loss = 0.49, predict loss = 0.11 (81.2 examples/sec; 0.049 sec/batch; 80h:18m:20s remains)
INFO - root - 2019-11-04 02:28:51.790603: step 133850, total loss = 0.46, predict loss = 0.11 (73.1 examples/sec; 0.055 sec/batch; 89h:11m:13s remains)
INFO - root - 2019-11-04 02:28:52.373650: step 133860, total loss = 0.48, predict loss = 0.11 (75.2 examples/sec; 0.053 sec/batch; 86h:38m:28s remains)
INFO - root - 2019-11-04 02:28:52.999032: step 133870, total loss = 0.49, predict loss = 0.11 (75.8 examples/sec; 0.053 sec/batch; 86h:00m:36s remains)
INFO - root - 2019-11-04 02:28:53.633473: step 133880, total loss = 0.43, predict loss = 0.10 (68.8 examples/sec; 0.058 sec/batch; 94h:44m:48s remains)
INFO - root - 2019-11-04 02:28:54.223669: step 133890, total loss = 0.39, predict loss = 0.09 (71.8 examples/sec; 0.056 sec/batch; 90h:50m:02s remains)
INFO - root - 2019-11-04 02:28:54.798471: step 133900, total loss = 0.50, predict loss = 0.11 (78.3 examples/sec; 0.051 sec/batch; 83h:13m:25s remains)
INFO - root - 2019-11-04 02:28:55.396510: step 133910, total loss = 0.56, predict loss = 0.13 (70.8 examples/sec; 0.057 sec/batch; 92h:05m:09s remains)
INFO - root - 2019-11-04 02:28:56.049705: step 133920, total loss = 0.51, predict loss = 0.11 (69.9 examples/sec; 0.057 sec/batch; 93h:11m:26s remains)
INFO - root - 2019-11-04 02:28:56.671403: step 133930, total loss = 0.46, predict loss = 0.11 (73.4 examples/sec; 0.054 sec/batch; 88h:47m:32s remains)
INFO - root - 2019-11-04 02:28:57.272802: step 133940, total loss = 0.39, predict loss = 0.08 (71.1 examples/sec; 0.056 sec/batch; 91h:43m:35s remains)
INFO - root - 2019-11-04 02:28:57.896329: step 133950, total loss = 0.34, predict loss = 0.07 (73.8 examples/sec; 0.054 sec/batch; 88h:17m:20s remains)
INFO - root - 2019-11-04 02:28:58.544495: step 133960, total loss = 0.43, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 92h:22m:46s remains)
INFO - root - 2019-11-04 02:28:59.170214: step 133970, total loss = 0.35, predict loss = 0.08 (62.2 examples/sec; 0.064 sec/batch; 104h:42m:14s remains)
INFO - root - 2019-11-04 02:28:59.807868: step 133980, total loss = 0.45, predict loss = 0.10 (66.1 examples/sec; 0.061 sec/batch; 98h:36m:38s remains)
INFO - root - 2019-11-04 02:29:00.401050: step 133990, total loss = 0.41, predict loss = 0.09 (79.6 examples/sec; 0.050 sec/batch; 81h:52m:54s remains)
INFO - root - 2019-11-04 02:29:01.071356: step 134000, total loss = 0.32, predict loss = 0.07 (68.2 examples/sec; 0.059 sec/batch; 95h:35m:02s remains)
INFO - root - 2019-11-04 02:29:01.686970: step 134010, total loss = 0.39, predict loss = 0.09 (69.3 examples/sec; 0.058 sec/batch; 94h:04m:04s remains)
INFO - root - 2019-11-04 02:29:02.347396: step 134020, total loss = 0.49, predict loss = 0.11 (60.1 examples/sec; 0.067 sec/batch; 108h:25m:55s remains)
INFO - root - 2019-11-04 02:29:02.969211: step 134030, total loss = 0.40, predict loss = 0.09 (74.0 examples/sec; 0.054 sec/batch; 88h:07m:45s remains)
INFO - root - 2019-11-04 02:29:03.585918: step 134040, total loss = 0.42, predict loss = 0.10 (73.9 examples/sec; 0.054 sec/batch; 88h:12m:00s remains)
INFO - root - 2019-11-04 02:29:04.215375: step 134050, total loss = 0.42, predict loss = 0.10 (75.4 examples/sec; 0.053 sec/batch; 86h:27m:55s remains)
INFO - root - 2019-11-04 02:29:04.844800: step 134060, total loss = 0.45, predict loss = 0.10 (68.1 examples/sec; 0.059 sec/batch; 95h:42m:47s remains)
INFO - root - 2019-11-04 02:29:05.464985: step 134070, total loss = 0.43, predict loss = 0.10 (76.6 examples/sec; 0.052 sec/batch; 85h:03m:45s remains)
INFO - root - 2019-11-04 02:29:06.122101: step 134080, total loss = 0.49, predict loss = 0.11 (68.7 examples/sec; 0.058 sec/batch; 94h:55m:10s remains)
INFO - root - 2019-11-04 02:29:06.806492: step 134090, total loss = 0.49, predict loss = 0.10 (65.2 examples/sec; 0.061 sec/batch; 99h:54m:41s remains)
INFO - root - 2019-11-04 02:29:07.436702: step 134100, total loss = 0.47, predict loss = 0.11 (79.7 examples/sec; 0.050 sec/batch; 81h:44m:32s remains)
INFO - root - 2019-11-04 02:29:08.050269: step 134110, total loss = 0.61, predict loss = 0.15 (77.3 examples/sec; 0.052 sec/batch; 84h:17m:37s remains)
INFO - root - 2019-11-04 02:29:08.714516: step 134120, total loss = 0.50, predict loss = 0.12 (65.2 examples/sec; 0.061 sec/batch; 99h:56m:27s remains)
INFO - root - 2019-11-04 02:29:09.367220: step 134130, total loss = 0.53, predict loss = 0.13 (64.0 examples/sec; 0.062 sec/batch; 101h:46m:34s remains)
INFO - root - 2019-11-04 02:29:09.975927: step 134140, total loss = 0.59, predict loss = 0.14 (75.6 examples/sec; 0.053 sec/batch; 86h:12m:27s remains)
INFO - root - 2019-11-04 02:29:10.632826: step 134150, total loss = 0.67, predict loss = 0.16 (72.7 examples/sec; 0.055 sec/batch; 89h:39m:09s remains)
INFO - root - 2019-11-04 02:29:11.292569: step 134160, total loss = 0.60, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 100h:02m:49s remains)
INFO - root - 2019-11-04 02:29:11.934530: step 134170, total loss = 0.62, predict loss = 0.15 (69.6 examples/sec; 0.057 sec/batch; 93h:37m:05s remains)
INFO - root - 2019-11-04 02:29:12.593829: step 134180, total loss = 0.55, predict loss = 0.13 (69.5 examples/sec; 0.058 sec/batch; 93h:49m:19s remains)
INFO - root - 2019-11-04 02:29:13.192004: step 134190, total loss = 0.70, predict loss = 0.16 (69.2 examples/sec; 0.058 sec/batch; 94h:13m:19s remains)
INFO - root - 2019-11-04 02:29:13.826218: step 134200, total loss = 0.61, predict loss = 0.13 (71.8 examples/sec; 0.056 sec/batch; 90h:44m:19s remains)
INFO - root - 2019-11-04 02:29:14.444744: step 134210, total loss = 0.45, predict loss = 0.10 (68.6 examples/sec; 0.058 sec/batch; 95h:00m:38s remains)
INFO - root - 2019-11-04 02:29:15.069675: step 134220, total loss = 0.47, predict loss = 0.11 (72.4 examples/sec; 0.055 sec/batch; 90h:03m:18s remains)
INFO - root - 2019-11-04 02:29:15.697498: step 134230, total loss = 0.57, predict loss = 0.14 (75.8 examples/sec; 0.053 sec/batch; 85h:56m:22s remains)
INFO - root - 2019-11-04 02:29:16.340516: step 134240, total loss = 0.33, predict loss = 0.07 (63.8 examples/sec; 0.063 sec/batch; 102h:10m:32s remains)
INFO - root - 2019-11-04 02:29:16.959889: step 134250, total loss = 0.33, predict loss = 0.07 (78.1 examples/sec; 0.051 sec/batch; 83h:25m:00s remains)
INFO - root - 2019-11-04 02:29:17.599438: step 134260, total loss = 0.35, predict loss = 0.08 (72.8 examples/sec; 0.055 sec/batch; 89h:27m:53s remains)
INFO - root - 2019-11-04 02:29:18.304348: step 134270, total loss = 0.31, predict loss = 0.07 (67.8 examples/sec; 0.059 sec/batch; 96h:05m:15s remains)
INFO - root - 2019-11-04 02:29:18.922909: step 134280, total loss = 0.43, predict loss = 0.10 (73.0 examples/sec; 0.055 sec/batch; 89h:18m:42s remains)
INFO - root - 2019-11-04 02:29:19.507522: step 134290, total loss = 0.43, predict loss = 0.10 (74.6 examples/sec; 0.054 sec/batch; 87h:25m:19s remains)
INFO - root - 2019-11-04 02:29:20.144841: step 134300, total loss = 0.43, predict loss = 0.09 (73.4 examples/sec; 0.054 sec/batch; 88h:44m:30s remains)
INFO - root - 2019-11-04 02:29:20.755746: step 134310, total loss = 0.50, predict loss = 0.12 (64.8 examples/sec; 0.062 sec/batch; 100h:35m:26s remains)
INFO - root - 2019-11-04 02:29:21.408174: step 134320, total loss = 0.44, predict loss = 0.11 (75.1 examples/sec; 0.053 sec/batch; 86h:47m:34s remains)
INFO - root - 2019-11-04 02:29:22.029919: step 134330, total loss = 0.51, predict loss = 0.11 (79.5 examples/sec; 0.050 sec/batch; 81h:56m:51s remains)
INFO - root - 2019-11-04 02:29:22.664667: step 134340, total loss = 0.39, predict loss = 0.09 (61.7 examples/sec; 0.065 sec/batch; 105h:40m:48s remains)
INFO - root - 2019-11-04 02:29:23.301584: step 134350, total loss = 0.55, predict loss = 0.13 (65.5 examples/sec; 0.061 sec/batch; 99h:28m:38s remains)
INFO - root - 2019-11-04 02:29:23.996024: step 134360, total loss = 0.58, predict loss = 0.14 (65.5 examples/sec; 0.061 sec/batch; 99h:31m:57s remains)
INFO - root - 2019-11-04 02:29:24.605509: step 134370, total loss = 0.49, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 91h:16m:13s remains)
INFO - root - 2019-11-04 02:29:25.229931: step 134380, total loss = 0.42, predict loss = 0.10 (67.9 examples/sec; 0.059 sec/batch; 96h:00m:19s remains)
INFO - root - 2019-11-04 02:29:25.877340: step 134390, total loss = 0.56, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 90h:17m:26s remains)
INFO - root - 2019-11-04 02:29:26.508138: step 134400, total loss = 0.57, predict loss = 0.13 (64.8 examples/sec; 0.062 sec/batch; 100h:37m:39s remains)
INFO - root - 2019-11-04 02:29:27.141078: step 134410, total loss = 0.46, predict loss = 0.11 (67.5 examples/sec; 0.059 sec/batch; 96h:32m:04s remains)
INFO - root - 2019-11-04 02:29:27.751979: step 134420, total loss = 0.52, predict loss = 0.12 (76.9 examples/sec; 0.052 sec/batch; 84h:48m:17s remains)
INFO - root - 2019-11-04 02:29:28.388315: step 134430, total loss = 0.51, predict loss = 0.13 (62.7 examples/sec; 0.064 sec/batch; 103h:55m:52s remains)
INFO - root - 2019-11-04 02:29:29.002077: step 134440, total loss = 0.52, predict loss = 0.12 (81.1 examples/sec; 0.049 sec/batch; 80h:21m:53s remains)
INFO - root - 2019-11-04 02:29:29.656143: step 134450, total loss = 0.47, predict loss = 0.11 (61.6 examples/sec; 0.065 sec/batch; 105h:44m:33s remains)
INFO - root - 2019-11-04 02:29:30.307880: step 134460, total loss = 0.54, predict loss = 0.12 (58.8 examples/sec; 0.068 sec/batch; 110h:52m:25s remains)
INFO - root - 2019-11-04 02:29:31.005670: step 134470, total loss = 0.58, predict loss = 0.13 (68.4 examples/sec; 0.059 sec/batch; 95h:19m:11s remains)
INFO - root - 2019-11-04 02:29:31.682854: step 134480, total loss = 0.46, predict loss = 0.11 (74.4 examples/sec; 0.054 sec/batch; 87h:38m:24s remains)
INFO - root - 2019-11-04 02:29:32.290285: step 134490, total loss = 0.44, predict loss = 0.10 (75.2 examples/sec; 0.053 sec/batch; 86h:37m:54s remains)
INFO - root - 2019-11-04 02:29:32.887192: step 134500, total loss = 0.44, predict loss = 0.10 (67.7 examples/sec; 0.059 sec/batch; 96h:18m:33s remains)
INFO - root - 2019-11-04 02:29:33.492034: step 134510, total loss = 0.39, predict loss = 0.09 (67.6 examples/sec; 0.059 sec/batch; 96h:20m:21s remains)
INFO - root - 2019-11-04 02:29:34.098876: step 134520, total loss = 0.46, predict loss = 0.11 (64.8 examples/sec; 0.062 sec/batch; 100h:37m:00s remains)
INFO - root - 2019-11-04 02:29:34.714195: step 134530, total loss = 0.39, predict loss = 0.09 (68.6 examples/sec; 0.058 sec/batch; 94h:58m:47s remains)
INFO - root - 2019-11-04 02:29:35.393478: step 134540, total loss = 0.44, predict loss = 0.10 (56.0 examples/sec; 0.071 sec/batch; 116h:20m:12s remains)
INFO - root - 2019-11-04 02:29:36.074714: step 134550, total loss = 0.39, predict loss = 0.09 (68.6 examples/sec; 0.058 sec/batch; 95h:03m:03s remains)
INFO - root - 2019-11-04 02:29:36.758502: step 134560, total loss = 0.30, predict loss = 0.06 (71.8 examples/sec; 0.056 sec/batch; 90h:49m:15s remains)
INFO - root - 2019-11-04 02:29:37.450052: step 134570, total loss = 0.43, predict loss = 0.10 (63.1 examples/sec; 0.063 sec/batch; 103h:20m:48s remains)
INFO - root - 2019-11-04 02:29:38.095529: step 134580, total loss = 0.35, predict loss = 0.08 (62.9 examples/sec; 0.064 sec/batch; 103h:33m:09s remains)
INFO - root - 2019-11-04 02:29:38.710360: step 134590, total loss = 0.47, predict loss = 0.11 (77.0 examples/sec; 0.052 sec/batch; 84h:35m:26s remains)
INFO - root - 2019-11-04 02:29:39.342843: step 134600, total loss = 0.40, predict loss = 0.08 (73.3 examples/sec; 0.055 sec/batch; 88h:54m:24s remains)
INFO - root - 2019-11-04 02:29:39.968368: step 134610, total loss = 0.51, predict loss = 0.12 (71.6 examples/sec; 0.056 sec/batch; 91h:03m:20s remains)
INFO - root - 2019-11-04 02:29:40.609704: step 134620, total loss = 0.44, predict loss = 0.10 (71.0 examples/sec; 0.056 sec/batch; 91h:49m:51s remains)
INFO - root - 2019-11-04 02:29:41.243429: step 134630, total loss = 0.43, predict loss = 0.10 (75.8 examples/sec; 0.053 sec/batch; 85h:55m:51s remains)
INFO - root - 2019-11-04 02:29:41.888339: step 134640, total loss = 0.46, predict loss = 0.11 (73.4 examples/sec; 0.055 sec/batch; 88h:48m:09s remains)
INFO - root - 2019-11-04 02:29:42.509355: step 134650, total loss = 0.62, predict loss = 0.14 (71.5 examples/sec; 0.056 sec/batch; 91h:05m:33s remains)
INFO - root - 2019-11-04 02:29:43.125195: step 134660, total loss = 0.45, predict loss = 0.10 (77.8 examples/sec; 0.051 sec/batch; 83h:44m:50s remains)
INFO - root - 2019-11-04 02:29:43.726067: step 134670, total loss = 0.49, predict loss = 0.11 (80.6 examples/sec; 0.050 sec/batch; 80h:49m:31s remains)
INFO - root - 2019-11-04 02:29:44.330826: step 134680, total loss = 0.42, predict loss = 0.09 (60.8 examples/sec; 0.066 sec/batch; 107h:09m:25s remains)
INFO - root - 2019-11-04 02:29:44.969634: step 134690, total loss = 0.39, predict loss = 0.09 (70.9 examples/sec; 0.056 sec/batch; 91h:56m:07s remains)
INFO - root - 2019-11-04 02:29:45.610587: step 134700, total loss = 0.36, predict loss = 0.08 (75.1 examples/sec; 0.053 sec/batch; 86h:45m:33s remains)
INFO - root - 2019-11-04 02:29:46.209989: step 134710, total loss = 0.63, predict loss = 0.15 (73.2 examples/sec; 0.055 sec/batch; 89h:04m:24s remains)
INFO - root - 2019-11-04 02:29:46.819223: step 134720, total loss = 0.43, predict loss = 0.10 (66.8 examples/sec; 0.060 sec/batch; 97h:33m:38s remains)
INFO - root - 2019-11-04 02:29:47.430333: step 134730, total loss = 0.57, predict loss = 0.14 (78.1 examples/sec; 0.051 sec/batch; 83h:26m:15s remains)
INFO - root - 2019-11-04 02:29:48.051155: step 134740, total loss = 0.60, predict loss = 0.15 (74.2 examples/sec; 0.054 sec/batch; 87h:49m:53s remains)
INFO - root - 2019-11-04 02:29:48.658206: step 134750, total loss = 0.54, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 91h:43m:27s remains)
INFO - root - 2019-11-04 02:29:49.320894: step 134760, total loss = 0.67, predict loss = 0.15 (60.8 examples/sec; 0.066 sec/batch; 107h:14m:14s remains)
INFO - root - 2019-11-04 02:29:50.018463: step 134770, total loss = 0.62, predict loss = 0.15 (68.2 examples/sec; 0.059 sec/batch; 95h:31m:40s remains)
INFO - root - 2019-11-04 02:29:50.680312: step 134780, total loss = 0.54, predict loss = 0.13 (62.9 examples/sec; 0.064 sec/batch; 103h:37m:29s remains)
INFO - root - 2019-11-04 02:29:51.347433: step 134790, total loss = 0.66, predict loss = 0.16 (66.6 examples/sec; 0.060 sec/batch; 97h:50m:07s remains)
INFO - root - 2019-11-04 02:29:51.968729: step 134800, total loss = 0.55, predict loss = 0.13 (79.5 examples/sec; 0.050 sec/batch; 81h:57m:45s remains)
INFO - root - 2019-11-04 02:29:52.577598: step 134810, total loss = 0.52, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 94h:44m:08s remains)
INFO - root - 2019-11-04 02:29:53.184813: step 134820, total loss = 0.56, predict loss = 0.13 (74.3 examples/sec; 0.054 sec/batch; 87h:45m:17s remains)
INFO - root - 2019-11-04 02:29:53.789900: step 134830, total loss = 0.55, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 90h:15m:07s remains)
INFO - root - 2019-11-04 02:29:54.366044: step 134840, total loss = 0.40, predict loss = 0.09 (78.4 examples/sec; 0.051 sec/batch; 83h:05m:12s remains)
INFO - root - 2019-11-04 02:29:54.979308: step 134850, total loss = 0.61, predict loss = 0.15 (79.0 examples/sec; 0.051 sec/batch; 82h:26m:47s remains)
INFO - root - 2019-11-04 02:29:55.603480: step 134860, total loss = 0.45, predict loss = 0.10 (55.1 examples/sec; 0.073 sec/batch; 118h:17m:37s remains)
INFO - root - 2019-11-04 02:29:56.302631: step 134870, total loss = 0.36, predict loss = 0.09 (63.0 examples/sec; 0.064 sec/batch; 103h:29m:37s remains)
INFO - root - 2019-11-04 02:29:56.980381: step 134880, total loss = 0.43, predict loss = 0.10 (77.5 examples/sec; 0.052 sec/batch; 84h:08m:07s remains)
INFO - root - 2019-11-04 02:29:57.638350: step 134890, total loss = 0.33, predict loss = 0.07 (73.5 examples/sec; 0.054 sec/batch; 88h:41m:03s remains)
INFO - root - 2019-11-04 02:29:58.312961: step 134900, total loss = 0.51, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 95h:25m:33s remains)
INFO - root - 2019-11-04 02:29:59.014681: step 134910, total loss = 0.44, predict loss = 0.10 (74.0 examples/sec; 0.054 sec/batch; 88h:03m:48s remains)
INFO - root - 2019-11-04 02:29:59.677992: step 134920, total loss = 0.48, predict loss = 0.11 (70.9 examples/sec; 0.056 sec/batch; 91h:51m:14s remains)
INFO - root - 2019-11-04 02:30:00.316335: step 134930, total loss = 0.51, predict loss = 0.12 (76.9 examples/sec; 0.052 sec/batch; 84h:45m:16s remains)
INFO - root - 2019-11-04 02:30:00.946268: step 134940, total loss = 0.65, predict loss = 0.15 (66.2 examples/sec; 0.060 sec/batch; 98h:26m:33s remains)
INFO - root - 2019-11-04 02:30:01.554431: step 134950, total loss = 0.52, predict loss = 0.11 (78.6 examples/sec; 0.051 sec/batch; 82h:53m:58s remains)
INFO - root - 2019-11-04 02:30:02.182315: step 134960, total loss = 0.40, predict loss = 0.09 (72.4 examples/sec; 0.055 sec/batch; 89h:57m:23s remains)
INFO - root - 2019-11-04 02:30:02.802126: step 134970, total loss = 0.37, predict loss = 0.09 (79.6 examples/sec; 0.050 sec/batch; 81h:52m:55s remains)
INFO - root - 2019-11-04 02:30:03.401942: step 134980, total loss = 0.40, predict loss = 0.08 (88.9 examples/sec; 0.045 sec/batch; 73h:16m:11s remains)
INFO - root - 2019-11-04 02:30:03.919924: step 134990, total loss = 0.57, predict loss = 0.14 (92.6 examples/sec; 0.043 sec/batch; 70h:21m:30s remains)
INFO - root - 2019-11-04 02:30:04.375676: step 135000, total loss = 0.42, predict loss = 0.10 (100.5 examples/sec; 0.040 sec/batch; 64h:49m:13s remains)
INFO - root - 2019-11-04 02:30:06.141745: step 135010, total loss = 0.44, predict loss = 0.11 (68.1 examples/sec; 0.059 sec/batch; 95h:38m:44s remains)
INFO - root - 2019-11-04 02:30:06.791911: step 135020, total loss = 0.26, predict loss = 0.05 (66.3 examples/sec; 0.060 sec/batch; 98h:13m:00s remains)
INFO - root - 2019-11-04 02:30:07.420624: step 135030, total loss = 0.53, predict loss = 0.12 (68.2 examples/sec; 0.059 sec/batch; 95h:29m:08s remains)
INFO - root - 2019-11-04 02:30:08.067980: step 135040, total loss = 0.51, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 99h:51m:04s remains)
INFO - root - 2019-11-04 02:30:08.714590: step 135050, total loss = 0.22, predict loss = 0.04 (69.0 examples/sec; 0.058 sec/batch; 94h:25m:29s remains)
INFO - root - 2019-11-04 02:30:09.365146: step 135060, total loss = 0.52, predict loss = 0.13 (70.9 examples/sec; 0.056 sec/batch; 91h:58m:24s remains)
INFO - root - 2019-11-04 02:30:10.003904: step 135070, total loss = 0.51, predict loss = 0.12 (64.9 examples/sec; 0.062 sec/batch; 100h:25m:49s remains)
INFO - root - 2019-11-04 02:30:10.630397: step 135080, total loss = 0.52, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 95h:58m:24s remains)
INFO - root - 2019-11-04 02:30:11.249312: step 135090, total loss = 0.57, predict loss = 0.13 (72.4 examples/sec; 0.055 sec/batch; 89h:58m:56s remains)
INFO - root - 2019-11-04 02:30:11.870361: step 135100, total loss = 0.69, predict loss = 0.17 (77.9 examples/sec; 0.051 sec/batch; 83h:38m:02s remains)
INFO - root - 2019-11-04 02:30:12.490452: step 135110, total loss = 0.44, predict loss = 0.10 (73.0 examples/sec; 0.055 sec/batch; 89h:13m:23s remains)
INFO - root - 2019-11-04 02:30:13.102083: step 135120, total loss = 0.48, predict loss = 0.11 (75.3 examples/sec; 0.053 sec/batch; 86h:31m:56s remains)
INFO - root - 2019-11-04 02:30:13.738351: step 135130, total loss = 0.51, predict loss = 0.11 (72.3 examples/sec; 0.055 sec/batch; 90h:05m:55s remains)
INFO - root - 2019-11-04 02:30:14.328109: step 135140, total loss = 0.43, predict loss = 0.09 (76.3 examples/sec; 0.052 sec/batch; 85h:24m:35s remains)
INFO - root - 2019-11-04 02:30:14.932162: step 135150, total loss = 0.41, predict loss = 0.09 (69.1 examples/sec; 0.058 sec/batch; 94h:20m:54s remains)
INFO - root - 2019-11-04 02:30:15.541214: step 135160, total loss = 0.54, predict loss = 0.13 (79.6 examples/sec; 0.050 sec/batch; 81h:50m:03s remains)
INFO - root - 2019-11-04 02:30:16.144609: step 135170, total loss = 0.50, predict loss = 0.12 (77.9 examples/sec; 0.051 sec/batch; 83h:40m:07s remains)
INFO - root - 2019-11-04 02:30:16.783101: step 135180, total loss = 0.44, predict loss = 0.10 (76.1 examples/sec; 0.053 sec/batch; 85h:39m:53s remains)
INFO - root - 2019-11-04 02:30:17.412365: step 135190, total loss = 0.35, predict loss = 0.07 (64.1 examples/sec; 0.062 sec/batch; 101h:41m:43s remains)
INFO - root - 2019-11-04 02:30:18.029222: step 135200, total loss = 0.43, predict loss = 0.10 (72.7 examples/sec; 0.055 sec/batch; 89h:35m:54s remains)
INFO - root - 2019-11-04 02:30:18.668723: step 135210, total loss = 0.49, predict loss = 0.11 (72.7 examples/sec; 0.055 sec/batch; 89h:38m:37s remains)
INFO - root - 2019-11-04 02:30:19.320992: step 135220, total loss = 0.43, predict loss = 0.10 (71.1 examples/sec; 0.056 sec/batch; 91h:40m:04s remains)
INFO - root - 2019-11-04 02:30:19.955533: step 135230, total loss = 0.32, predict loss = 0.06 (66.8 examples/sec; 0.060 sec/batch; 97h:29m:21s remains)
INFO - root - 2019-11-04 02:30:20.594192: step 135240, total loss = 0.44, predict loss = 0.10 (62.9 examples/sec; 0.064 sec/batch; 103h:36m:50s remains)
INFO - root - 2019-11-04 02:30:21.230290: step 135250, total loss = 0.61, predict loss = 0.14 (78.4 examples/sec; 0.051 sec/batch; 83h:08m:00s remains)
INFO - root - 2019-11-04 02:30:21.866043: step 135260, total loss = 0.62, predict loss = 0.15 (69.1 examples/sec; 0.058 sec/batch; 94h:14m:59s remains)
INFO - root - 2019-11-04 02:30:22.503069: step 135270, total loss = 0.64, predict loss = 0.15 (71.9 examples/sec; 0.056 sec/batch; 90h:41m:36s remains)
INFO - root - 2019-11-04 02:30:23.141805: step 135280, total loss = 0.43, predict loss = 0.09 (64.4 examples/sec; 0.062 sec/batch; 101h:06m:31s remains)
INFO - root - 2019-11-04 02:30:23.777540: step 135290, total loss = 0.57, predict loss = 0.13 (67.8 examples/sec; 0.059 sec/batch; 96h:05m:28s remains)
INFO - root - 2019-11-04 02:30:24.428385: step 135300, total loss = 0.65, predict loss = 0.16 (71.0 examples/sec; 0.056 sec/batch; 91h:46m:27s remains)
INFO - root - 2019-11-04 02:30:25.064918: step 135310, total loss = 0.73, predict loss = 0.17 (74.5 examples/sec; 0.054 sec/batch; 87h:25m:53s remains)
INFO - root - 2019-11-04 02:30:25.722432: step 135320, total loss = 0.57, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 94h:39m:55s remains)
INFO - root - 2019-11-04 02:30:26.354517: step 135330, total loss = 0.57, predict loss = 0.13 (85.2 examples/sec; 0.047 sec/batch; 76h:26m:26s remains)
INFO - root - 2019-11-04 02:30:26.956529: step 135340, total loss = 0.48, predict loss = 0.12 (78.8 examples/sec; 0.051 sec/batch; 82h:41m:13s remains)
INFO - root - 2019-11-04 02:30:27.570335: step 135350, total loss = 0.48, predict loss = 0.11 (86.4 examples/sec; 0.046 sec/batch; 75h:26m:33s remains)
INFO - root - 2019-11-04 02:30:28.194173: step 135360, total loss = 0.44, predict loss = 0.10 (73.9 examples/sec; 0.054 sec/batch; 88h:10m:46s remains)
INFO - root - 2019-11-04 02:30:28.800897: step 135370, total loss = 0.54, predict loss = 0.13 (66.7 examples/sec; 0.060 sec/batch; 97h:43m:10s remains)
INFO - root - 2019-11-04 02:30:29.385474: step 135380, total loss = 0.46, predict loss = 0.10 (86.1 examples/sec; 0.046 sec/batch; 75h:43m:18s remains)
INFO - root - 2019-11-04 02:30:29.988670: step 135390, total loss = 0.49, predict loss = 0.11 (75.2 examples/sec; 0.053 sec/batch; 86h:40m:14s remains)
INFO - root - 2019-11-04 02:30:30.616950: step 135400, total loss = 0.43, predict loss = 0.10 (72.6 examples/sec; 0.055 sec/batch; 89h:42m:41s remains)
INFO - root - 2019-11-04 02:30:31.247429: step 135410, total loss = 0.47, predict loss = 0.11 (73.5 examples/sec; 0.054 sec/batch; 88h:40m:22s remains)
INFO - root - 2019-11-04 02:30:31.863259: step 135420, total loss = 0.51, predict loss = 0.12 (70.1 examples/sec; 0.057 sec/batch; 93h:00m:38s remains)
INFO - root - 2019-11-04 02:30:32.481993: step 135430, total loss = 0.46, predict loss = 0.10 (73.9 examples/sec; 0.054 sec/batch; 88h:07m:59s remains)
INFO - root - 2019-11-04 02:30:33.087218: step 135440, total loss = 0.44, predict loss = 0.10 (74.0 examples/sec; 0.054 sec/batch; 88h:03m:28s remains)
INFO - root - 2019-11-04 02:30:33.701548: step 135450, total loss = 0.42, predict loss = 0.10 (73.3 examples/sec; 0.055 sec/batch; 88h:52m:11s remains)
INFO - root - 2019-11-04 02:30:34.325997: step 135460, total loss = 0.41, predict loss = 0.10 (79.1 examples/sec; 0.051 sec/batch; 82h:19m:45s remains)
INFO - root - 2019-11-04 02:30:34.973490: step 135470, total loss = 0.56, predict loss = 0.14 (66.0 examples/sec; 0.061 sec/batch; 98h:41m:41s remains)
INFO - root - 2019-11-04 02:30:35.587191: step 135480, total loss = 0.46, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 97h:17m:56s remains)
INFO - root - 2019-11-04 02:30:36.210167: step 135490, total loss = 0.32, predict loss = 0.07 (70.0 examples/sec; 0.057 sec/batch; 93h:07m:08s remains)
INFO - root - 2019-11-04 02:30:36.829773: step 135500, total loss = 0.47, predict loss = 0.11 (65.4 examples/sec; 0.061 sec/batch; 99h:41m:55s remains)
INFO - root - 2019-11-04 02:30:37.497851: step 135510, total loss = 0.55, predict loss = 0.14 (68.1 examples/sec; 0.059 sec/batch; 95h:42m:22s remains)
INFO - root - 2019-11-04 02:30:38.186547: step 135520, total loss = 0.47, predict loss = 0.11 (68.7 examples/sec; 0.058 sec/batch; 94h:52m:15s remains)
INFO - root - 2019-11-04 02:30:38.814159: step 135530, total loss = 0.49, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 92h:45m:23s remains)
INFO - root - 2019-11-04 02:30:39.452372: step 135540, total loss = 0.36, predict loss = 0.08 (72.4 examples/sec; 0.055 sec/batch; 90h:02m:36s remains)
INFO - root - 2019-11-04 02:30:40.083475: step 135550, total loss = 0.34, predict loss = 0.08 (77.1 examples/sec; 0.052 sec/batch; 84h:32m:36s remains)
INFO - root - 2019-11-04 02:30:40.729486: step 135560, total loss = 0.31, predict loss = 0.07 (65.9 examples/sec; 0.061 sec/batch; 98h:53m:58s remains)
INFO - root - 2019-11-04 02:30:41.399111: step 135570, total loss = 0.33, predict loss = 0.08 (62.7 examples/sec; 0.064 sec/batch; 103h:53m:22s remains)
INFO - root - 2019-11-04 02:30:42.027523: step 135580, total loss = 0.36, predict loss = 0.09 (79.4 examples/sec; 0.050 sec/batch; 82h:03m:42s remains)
INFO - root - 2019-11-04 02:30:42.646173: step 135590, total loss = 0.49, predict loss = 0.12 (74.8 examples/sec; 0.053 sec/batch; 87h:06m:13s remains)
INFO - root - 2019-11-04 02:30:43.259909: step 135600, total loss = 0.40, predict loss = 0.09 (68.3 examples/sec; 0.059 sec/batch; 95h:21m:17s remains)
INFO - root - 2019-11-04 02:30:43.886134: step 135610, total loss = 0.30, predict loss = 0.06 (76.2 examples/sec; 0.052 sec/batch; 85h:29m:23s remains)
INFO - root - 2019-11-04 02:30:44.532669: step 135620, total loss = 0.34, predict loss = 0.07 (73.3 examples/sec; 0.055 sec/batch; 88h:56m:08s remains)
INFO - root - 2019-11-04 02:30:45.195056: step 135630, total loss = 0.28, predict loss = 0.06 (71.3 examples/sec; 0.056 sec/batch; 91h:20m:18s remains)
INFO - root - 2019-11-04 02:30:45.831992: step 135640, total loss = 0.43, predict loss = 0.11 (74.2 examples/sec; 0.054 sec/batch; 87h:46m:35s remains)
INFO - root - 2019-11-04 02:30:46.479645: step 135650, total loss = 0.45, predict loss = 0.10 (65.3 examples/sec; 0.061 sec/batch; 99h:49m:10s remains)
INFO - root - 2019-11-04 02:30:47.125898: step 135660, total loss = 0.57, predict loss = 0.13 (74.1 examples/sec; 0.054 sec/batch; 87h:52m:39s remains)
INFO - root - 2019-11-04 02:30:47.764313: step 135670, total loss = 0.50, predict loss = 0.12 (69.7 examples/sec; 0.057 sec/batch; 93h:25m:58s remains)
INFO - root - 2019-11-04 02:30:48.389542: step 135680, total loss = 0.55, predict loss = 0.13 (74.5 examples/sec; 0.054 sec/batch; 87h:27m:56s remains)
INFO - root - 2019-11-04 02:30:48.998530: step 135690, total loss = 0.36, predict loss = 0.08 (74.3 examples/sec; 0.054 sec/batch; 87h:42m:12s remains)
INFO - root - 2019-11-04 02:30:49.665090: step 135700, total loss = 0.56, predict loss = 0.14 (71.7 examples/sec; 0.056 sec/batch; 90h:55m:52s remains)
INFO - root - 2019-11-04 02:30:50.317823: step 135710, total loss = 0.43, predict loss = 0.10 (61.2 examples/sec; 0.065 sec/batch; 106h:31m:35s remains)
INFO - root - 2019-11-04 02:30:50.933804: step 135720, total loss = 0.37, predict loss = 0.08 (81.9 examples/sec; 0.049 sec/batch; 79h:32m:12s remains)
INFO - root - 2019-11-04 02:30:51.602177: step 135730, total loss = 0.39, predict loss = 0.09 (64.6 examples/sec; 0.062 sec/batch; 100h:52m:18s remains)
INFO - root - 2019-11-04 02:30:52.221126: step 135740, total loss = 0.36, predict loss = 0.08 (78.7 examples/sec; 0.051 sec/batch; 82h:49m:38s remains)
INFO - root - 2019-11-04 02:30:52.824256: step 135750, total loss = 0.58, predict loss = 0.14 (72.3 examples/sec; 0.055 sec/batch; 90h:06m:32s remains)
INFO - root - 2019-11-04 02:30:53.461468: step 135760, total loss = 0.44, predict loss = 0.10 (76.4 examples/sec; 0.052 sec/batch; 85h:15m:34s remains)
INFO - root - 2019-11-04 02:30:54.108773: step 135770, total loss = 0.54, predict loss = 0.13 (63.5 examples/sec; 0.063 sec/batch; 102h:38m:11s remains)
INFO - root - 2019-11-04 02:30:54.768429: step 135780, total loss = 0.49, predict loss = 0.11 (62.7 examples/sec; 0.064 sec/batch; 103h:51m:24s remains)
INFO - root - 2019-11-04 02:30:55.385324: step 135790, total loss = 0.61, predict loss = 0.15 (71.3 examples/sec; 0.056 sec/batch; 91h:22m:55s remains)
INFO - root - 2019-11-04 02:30:56.037353: step 135800, total loss = 0.53, predict loss = 0.12 (67.5 examples/sec; 0.059 sec/batch; 96h:35m:59s remains)
INFO - root - 2019-11-04 02:30:56.707847: step 135810, total loss = 0.21, predict loss = 0.04 (67.2 examples/sec; 0.059 sec/batch; 96h:54m:03s remains)
INFO - root - 2019-11-04 02:30:57.337439: step 135820, total loss = 0.24, predict loss = 0.05 (72.8 examples/sec; 0.055 sec/batch; 89h:28m:30s remains)
INFO - root - 2019-11-04 02:30:57.959159: step 135830, total loss = 0.27, predict loss = 0.06 (68.4 examples/sec; 0.058 sec/batch; 95h:13m:01s remains)
INFO - root - 2019-11-04 02:30:58.574329: step 135840, total loss = 0.24, predict loss = 0.05 (73.9 examples/sec; 0.054 sec/batch; 88h:13m:34s remains)
INFO - root - 2019-11-04 02:30:59.214114: step 135850, total loss = 0.19, predict loss = 0.04 (81.5 examples/sec; 0.049 sec/batch; 79h:55m:34s remains)
INFO - root - 2019-11-04 02:30:59.840931: step 135860, total loss = 0.24, predict loss = 0.05 (70.2 examples/sec; 0.057 sec/batch; 92h:50m:37s remains)
INFO - root - 2019-11-04 02:31:00.440916: step 135870, total loss = 0.36, predict loss = 0.09 (65.3 examples/sec; 0.061 sec/batch; 99h:44m:31s remains)
INFO - root - 2019-11-04 02:31:01.048213: step 135880, total loss = 0.26, predict loss = 0.06 (78.0 examples/sec; 0.051 sec/batch; 83h:30m:45s remains)
INFO - root - 2019-11-04 02:31:01.692359: step 135890, total loss = 0.48, predict loss = 0.11 (62.9 examples/sec; 0.064 sec/batch; 103h:32m:58s remains)
INFO - root - 2019-11-04 02:31:02.332788: step 135900, total loss = 0.32, predict loss = 0.07 (65.7 examples/sec; 0.061 sec/batch; 99h:08m:50s remains)
INFO - root - 2019-11-04 02:31:02.962461: step 135910, total loss = 0.34, predict loss = 0.07 (74.2 examples/sec; 0.054 sec/batch; 87h:47m:46s remains)
INFO - root - 2019-11-04 02:31:03.602484: step 135920, total loss = 0.33, predict loss = 0.07 (64.1 examples/sec; 0.062 sec/batch; 101h:41m:24s remains)
INFO - root - 2019-11-04 02:31:04.213931: step 135930, total loss = 0.35, predict loss = 0.07 (68.1 examples/sec; 0.059 sec/batch; 95h:44m:37s remains)
INFO - root - 2019-11-04 02:31:04.890309: step 135940, total loss = 0.37, predict loss = 0.08 (46.2 examples/sec; 0.087 sec/batch; 141h:09m:26s remains)
INFO - root - 2019-11-04 02:31:05.543588: step 135950, total loss = 0.45, predict loss = 0.11 (74.3 examples/sec; 0.054 sec/batch; 87h:39m:23s remains)
INFO - root - 2019-11-04 02:31:06.142033: step 135960, total loss = 0.36, predict loss = 0.08 (75.0 examples/sec; 0.053 sec/batch; 86h:51m:29s remains)
INFO - root - 2019-11-04 02:31:06.748626: step 135970, total loss = 0.47, predict loss = 0.11 (72.4 examples/sec; 0.055 sec/batch; 89h:58m:09s remains)
INFO - root - 2019-11-04 02:31:07.380738: step 135980, total loss = 0.37, predict loss = 0.08 (67.7 examples/sec; 0.059 sec/batch; 96h:16m:00s remains)
INFO - root - 2019-11-04 02:31:07.980044: step 135990, total loss = 0.45, predict loss = 0.11 (77.1 examples/sec; 0.052 sec/batch; 84h:32m:45s remains)
INFO - root - 2019-11-04 02:31:08.644205: step 136000, total loss = 0.61, predict loss = 0.14 (63.4 examples/sec; 0.063 sec/batch; 102h:47m:50s remains)
INFO - root - 2019-11-04 02:31:09.383240: step 136010, total loss = 0.55, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 92h:38m:20s remains)
INFO - root - 2019-11-04 02:31:10.048269: step 136020, total loss = 0.45, predict loss = 0.11 (61.1 examples/sec; 0.065 sec/batch; 106h:39m:23s remains)
INFO - root - 2019-11-04 02:31:10.689549: step 136030, total loss = 0.37, predict loss = 0.08 (84.4 examples/sec; 0.047 sec/batch; 77h:11m:08s remains)
INFO - root - 2019-11-04 02:31:11.327069: step 136040, total loss = 0.47, predict loss = 0.11 (70.7 examples/sec; 0.057 sec/batch; 92h:11m:26s remains)
INFO - root - 2019-11-04 02:31:11.978509: step 136050, total loss = 0.53, predict loss = 0.12 (74.8 examples/sec; 0.054 sec/batch; 87h:09m:04s remains)
INFO - root - 2019-11-04 02:31:12.631199: step 136060, total loss = 0.55, predict loss = 0.14 (65.3 examples/sec; 0.061 sec/batch; 99h:47m:48s remains)
INFO - root - 2019-11-04 02:31:13.260247: step 136070, total loss = 0.34, predict loss = 0.08 (80.3 examples/sec; 0.050 sec/batch; 81h:10m:44s remains)
INFO - root - 2019-11-04 02:31:13.926052: step 136080, total loss = 0.42, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 93h:48m:17s remains)
INFO - root - 2019-11-04 02:31:14.514592: step 136090, total loss = 0.24, predict loss = 0.05 (71.2 examples/sec; 0.056 sec/batch; 91h:33m:39s remains)
INFO - root - 2019-11-04 02:31:15.222349: step 136100, total loss = 0.32, predict loss = 0.06 (52.3 examples/sec; 0.077 sec/batch; 124h:40m:31s remains)
INFO - root - 2019-11-04 02:31:15.841151: step 136110, total loss = 0.38, predict loss = 0.09 (79.2 examples/sec; 0.051 sec/batch; 82h:16m:42s remains)
INFO - root - 2019-11-04 02:31:16.464806: step 136120, total loss = 0.31, predict loss = 0.07 (75.3 examples/sec; 0.053 sec/batch; 86h:31m:06s remains)
