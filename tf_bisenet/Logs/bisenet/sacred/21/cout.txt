INFO - bisenet-v2 - Running command 'main'
INFO - bisenet-v2 - Started run with ID "21"
INFO - root - nvidia-ml-py is not installed, automatically select gpu is disabled!
WARNING:tensorflow:From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - tensorflow - From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:88: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:88: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:100: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:100: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
INFO - root - preproces -- augment
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:108: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:108: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:218: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map__image_mirroring, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(_image_mirroring, num_parallel_calls=threads)
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:119: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:119: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:122: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:122: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:123: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:123: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:220: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map__image_scaling, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(_image_scaling, num_parallel_calls=threads)
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:148: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:148: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.

/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:223: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map_<lambda>, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  num_parallel_calls=threads)
/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:224: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map_<lambda>, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(lambda image, label: _apply_with_random_selector(image, lambda x, ordering: _distort_color
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:235: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:235: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554f0389b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554f0389b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554f0389b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554f0389b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554efd9198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554efd9198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554efd9198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554efd9198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554efd9390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554efd9390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554efd9390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554efd9390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554efd9a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554efd9a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554efd9a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554efd9a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554efd9390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554efd9390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554efd9390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554efd9390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554f038940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554f038940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554f038940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554f038940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554f038c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554f038c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554f038c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554f038c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554eee06a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554eee06a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554eee06a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554eee06a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554eee0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554eee0e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554eee0e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554eee0e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554efd9c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554efd9c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554efd9c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554efd9c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f554ef67fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f554ef67fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f554ef67fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f554ef67fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554efd9ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554efd9ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554efd9ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554efd9ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee5c780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee5c780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee5c780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee5c780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554f042630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554f042630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554f042630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554f042630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee0f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee0f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee0f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee0f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554eeb0dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554eeb0dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554eeb0dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554eeb0dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee5c780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee5c780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee5c780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee5c780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ed51400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ed51400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ed51400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ed51400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee5cbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee5cbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee5cbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ee5cbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ee0f7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ee0f7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ee0f7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ee0f7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ecf96d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ecf96d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ecf96d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ecf96d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ee5c908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ee5c908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ee5c908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ee5c908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ec724a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ec724a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ec724a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ec724a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ed77da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ed77da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ed77da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ed77da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ebb0f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ebb0f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ebb0f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ebb0f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ed77da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ed77da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ed77da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ed77da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ec444e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ec444e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ec444e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ec444e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ec72320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ec72320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ec72320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ec72320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554eaed438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554eaed438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554eaed438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554eaed438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ec44630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ec44630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ec44630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ec44630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ed9c978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ed9c978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ed9c978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ed9c978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ec44b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ec44b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ec44b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ec44b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ea3eac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ea3eac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ea3eac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ea3eac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e9ef5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e9ef5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e9ef5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e9ef5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554eaed240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554eaed240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554eaed240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554eaed240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ea20f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ea20f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ea20f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ea20f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e9bc6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e9bc6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e9bc6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e9bc6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e8d85c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e8d85c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e8d85c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e8d85c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ef67d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ef67d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ef67d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ef67d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ea70b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ea70b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ea70b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ea70b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e7e4160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e7e4160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e7e4160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e7e4160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e9c4240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e9c4240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e9c4240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e9c4240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ef67ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ef67ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ef67ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ef67ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e92d4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e92d4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e92d4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e92d4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e6f1358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e6f1358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e6f1358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e6f1358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e92db70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e92db70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e92db70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e92db70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ef674a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ef674a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ef674a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ef674a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e685e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e685e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e685e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e685e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e5d8a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e5d8a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e5d8a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e5d8a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ea68198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ea68198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ea68198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ea68198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e6555c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e6555c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e6555c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e6555c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e92d5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e92d5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e92d5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e92d5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e655898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e655898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e655898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e655898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e5d8748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e5d8748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e5d8748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e5d8748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e53eb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e53eb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e53eb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e53eb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e5d8a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e5d8a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e5d8a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e5d8a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e4312b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e4312b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e4312b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e4312b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e4bb940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e4bb940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e4bb940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e4bb940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e39cef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e39cef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e39cef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e39cef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e53e208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e53e208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e53e208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e53e208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e2f05c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e2f05c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e2f05c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e2f05c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e32c710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e32c710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e32c710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e32c710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e2c03c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e2c03c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e2c03c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e2c03c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e39cef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e39cef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e39cef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e39cef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e30a6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e30a6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e30a6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e30a6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e3afb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e3afb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e3afb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e3afb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e21f860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e21f860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e21f860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e21f860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e39cef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e39cef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e39cef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e39cef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e19c6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e19c6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e19c6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e19c6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e28eef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e28eef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e28eef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e28eef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e103e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e103e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e103e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e103e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e28eb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e28eb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e28eb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e28eb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e3280f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e3280f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e3280f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e3280f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e103e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e103e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e103e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e103e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e116128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e116128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e116128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e116128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e3280f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e3280f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e3280f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e3280f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e82fda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e82fda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e82fda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e82fda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dea5828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dea5828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dea5828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dea5828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e7f04a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e7f04a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e7f04a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e7f04a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e82f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e82f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e82f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e82f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e075fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e075fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e075fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e075fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554df0c1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554df0c1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554df0c1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554df0c1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554de6eac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554de6eac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554de6eac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554de6eac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e19ce48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e19ce48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e19ce48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e19ce48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dd5b048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dd5b048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dd5b048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dd5b048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554de6eac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554de6eac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554de6eac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554de6eac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e00ee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e00ee48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e00ee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554e00ee48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e00ee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e00ee48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e00ee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e00ee48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dcc8358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dcc8358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dcc8358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dcc8358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dd5b898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dd5b898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dd5b898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dd5b898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dcf9908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dcf9908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dcf9908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dcf9908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dcb1e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dcb1e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dcb1e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dcb1e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dc755f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dc755f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dc755f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dc755f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e7f0128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e7f0128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e7f0128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554e7f0128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dc00e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dc00e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dc00e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554dc00e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dbd1e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dbd1e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dbd1e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dbd1e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554da61c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554da61c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554da61c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554da61c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dc5fcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dc5fcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dc5fcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dc5fcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554daf9748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554daf9748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554daf9748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554daf9748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dc5fb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dc5fb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dc5fb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554dc5fb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554da61eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554da61eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554da61eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554da61eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9b26d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9b26d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9b26d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9b26d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d961a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d961a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d961a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d961a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9b2470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9b2470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9b2470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9b2470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d9e7438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d9e7438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d9e7438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d9e7438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9e1e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9e1e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9e1e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9e1e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d83cd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d83cd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d83cd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d83cd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9e1390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9e1390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9e1390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d9e1390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554da61358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554da61358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554da61358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554da61358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d86cba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d86cba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d86cba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554d86cba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d712978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d712978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d712978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d712978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554daf9f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554daf9f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554daf9f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554daf9f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d7ab048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d7ab048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d7ab048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d7ab048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d7ab668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d7ab668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d7ab668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d7ab668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d5bcb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d5bcb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d5bcb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d5bcb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:179: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:179: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554df91a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554df91a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554df91a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554df91a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d5bc2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d5bc2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d5bc2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d5bc2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d5a6dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d5a6dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d5a6dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d5a6dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d609a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d609a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d609a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d609a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d5a6c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d5a6c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d5a6c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d5a6c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554df050f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554df050f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554df050f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554df050f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d5a6c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d5a6c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d5a6c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d5a6c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d7abf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d7abf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d7abf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d7abf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d38cb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d38cb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d38cb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d38cb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d38c668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d38c668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d38c668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d38c668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d34a978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d34a978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d34a978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d34a978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d311400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d311400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d311400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d311400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d311b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d311b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d311b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d311b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d3113c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d3113c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d3113c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d3113c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d311588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d311588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d311588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d311588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554deefe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554deefe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554deefe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554deefe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d2d4fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d2d4fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d2d4fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d2d4fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d24d320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d24d320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d24d320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d24d320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d63b6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d63b6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d63b6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d63b6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d24d4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d24d4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d24d4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d24d4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d228d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d228d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d228d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d228d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d447c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d447c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d447c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d447c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d1122e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d1122e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d1122e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d1122e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d162748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d162748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d162748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d162748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d1127b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d1127b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d1127b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d1127b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d0d1128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d0d1128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d0d1128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d0d1128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d036400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d036400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d036400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d036400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:217: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:217: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:221: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:221: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:224: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:224: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:229: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:229: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:240: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:240: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING - root - random_scale is not explicitly specified, using default value: False
WARNING - root - random_mirror is not explicitly specified, using default value: True
INFO - root - preproces -- None
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd5de48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd5de48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd5de48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd5de48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd27358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd27358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd27358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd27358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd27cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd27cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd27cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd27cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd27668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd27668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd27668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd27668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd277b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd277b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd277b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd277b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd27978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd27978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd27978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd27978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd27668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd27668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd27668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cd27668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd276d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd276d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd276d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd276d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d04e128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d04e128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d04e128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554d04e128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccd0dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccd0dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccd0dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccd0dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f554d04e128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f554d04e128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f554d04e128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f554d04e128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccd0860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccd0860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccd0860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccd0860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0da20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0da20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0da20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0da20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0d5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0d5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0d5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0d5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cceab70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cceab70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cceab70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cceab70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd27eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd27eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd27eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd27eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cce22b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cce22b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cce22b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cce22b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cce2eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cce2eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cce2eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cce2eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccecb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccecb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccecb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccecb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cce25c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cce25c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cce25c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cce25c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd380f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd380f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd380f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd380f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cce22e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cce22e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cce22e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cce22e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd38978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd38978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd38978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd38978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd38208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd38208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd38208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd38208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0d668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0d128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0d128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0d128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0d128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccea6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccea6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccea6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccea6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0d2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0d2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0d2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0d2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0a470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0a470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0a470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd0a470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cceae48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cceae48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cceae48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cceae48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccea780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccea780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccea780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccffa58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccffa58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccffa58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccffa58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd08048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd08048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd08048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd08048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccffa58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccffa58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccffa58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccffa58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd6fe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd6fe48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd6fe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cd6fe48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccfa0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccfa0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccfa0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccfa0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd6f400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd6f400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd6f400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd6f400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc8d860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc51240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc51240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc51240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc51240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc8d860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc8d860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc8d860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc8d860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cced5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cced5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cced5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cced5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0bcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0bcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0bcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0bcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccc6748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccc6438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccc6438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccc6438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccc6438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc0bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc0bd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc0bd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc0bd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccc6358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccc6358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccc6358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccc6358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc245f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc245f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc245f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc245f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc612b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc612b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc612b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc612b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc24d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc24d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc24d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc24d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0a550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0a550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0a550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cd0a550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccaeeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccaeeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccaeeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ccaeeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc51278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc51278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc51278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc51278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc37438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc37438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc37438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cc37438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccae710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccae710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccae710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ccae710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb97390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb97390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb97390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb97390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb971d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb971d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb971d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb971d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb97630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb97630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb97630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb97630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc269b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc269b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc269b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc269b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb972e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb972e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb972e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb972e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc265c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc265c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc265c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc265c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc269b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc269b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc269b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cc269b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb46d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb46d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb46d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb46d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbebda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbebda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbebda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbebda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cba6240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cba6240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cba6240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cba6240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbea390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbea390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbea390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbea390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb97da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb97da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb97da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb97da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbaae48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbaae48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbaae48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbaae48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cba6cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cba6cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cba6cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cba6cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbaa588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbaa588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbaa588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbaa588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cba60f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cba60f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cba60f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cba60f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbc8550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbc8550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbc8550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbc8550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbc8550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbc8550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbc8550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbc8550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb7c208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb7c208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb7c208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb7c208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbc86a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbc86a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbc86a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cbc86a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb7ca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb7ca90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb7ca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb7ca90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb7ca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb7ca90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb7ca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb7ca90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb80e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb80e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb80e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb80e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb7c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb7c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb7c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb7c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb7c208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb7c208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb7c208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb7c208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cbb4828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb802b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb802b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb802b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb802b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb2be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb2be10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb2be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cb2be10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb80710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb80710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb80710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb80710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caeec18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caeec18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caeec18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caeec18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb2bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb2bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb2bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb2bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb3b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb3b588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb3b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb3b588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb3b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb3b1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb3b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb3b1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb3b358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb3b358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb3b358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb3b358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caee208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb545c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb545c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb545c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554cb545c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caf07f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caf07f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caf07f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554caf07f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554caeea20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554caeea20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554caeea20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554caeea20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca9ae80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca9ae80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca9ae80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca9ae80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ca8f588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ca8f588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ca8f588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f554ca8f588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca655c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca655c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca655c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca655c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca9a160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca9a160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca9a160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca9a160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cab8978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cab8978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cab8978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cab8978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca9ae80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca9ae80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca9ae80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca9ae80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d5412e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d5412e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d5412e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554d5412e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cab8978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cab8978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cab8978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554cab8978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cac76d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cac76d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cac76d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cac76d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca65978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca65978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca65978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca65978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cac7978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cac7978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cac7978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554cac7978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca8f4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca8f4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca8f4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca8f4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c9cd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c9cd048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c9cd048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c9cd048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c9cea20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c9cea20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c9cea20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c9cea20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c9cef28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c9cef28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c9cef28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c9cef28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca03e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca03e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca03e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca03e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca03c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca03c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca03c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554ca03c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca099b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca099b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca099b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca099b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca03828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c99a3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c99a3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c99a3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c99a3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca84fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca84fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca84fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca84fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c99a128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c99a128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c99a128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c99a128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c99a080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c99a080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c99a080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c99a080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c99a860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c99a860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c99a860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c99a860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c9c83c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c9c83c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c9c83c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c9c83c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c9c81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c9c81d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c9c81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f554c9c81d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca095f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca095f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca095f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554ca095f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c968550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c968550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c968550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c968550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c968080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c968080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c968080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f554c968080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING - tensorflow - From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING:tensorflow:From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING - tensorflow - From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING - tensorflow - From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING - tensorflow - From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

2019-11-06 18:01:03.449384: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-06 18:01:03.454187: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-11-06 18:01:03.570961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 18:01:03.571428: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5559a3c2bf50 executing computations on platform CUDA. Devices:
2019-11-06 18:01:03.571443: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-11-06 18:01:03.597048: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz
2019-11-06 18:01:03.598303: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5559a3c73300 executing computations on platform Host. Devices:
2019-11-06 18:01:03.598370: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-11-06 18:01:03.598780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 18:01:03.600218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-11-06 18:01:03.600781: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-06 18:01:03.603798: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-11-06 18:01:03.604457: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-11-06 18:01:03.604616: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-11-06 18:01:03.605405: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-11-06 18:01:03.605982: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-11-06 18:01:03.607892: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-11-06 18:01:03.607969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 18:01:03.608387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 18:01:03.608806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-11-06 18:01:03.608835: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-06 18:01:03.609448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-06 18:01:03.609461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-11-06 18:01:03.609467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-11-06 18:01:03.609598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 18:01:03.609983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 18:01:03.610353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6830 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-11-06 18:01:04.938282: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
INFO - root - Train for 150000 steps
2019-11-06 18:01:10.244744: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
INFO - root - 2019-11-06 18:01:11.737520: step 0, total loss = 4.57, predict loss = 1.38 (0.6 examples/sec; 6.157 sec/batch; 256h:31m:23s remains)
2019-11-06 18:01:13.185756: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
INFO - root - 2019-11-06 18:01:15.284521: step 10, total loss = 6.78, predict loss = 1.93 (63.3 examples/sec; 0.063 sec/batch; 2h:37m:51s remains)
INFO - root - 2019-11-06 18:01:15.954843: step 20, total loss = 7.15, predict loss = 2.03 (59.1 examples/sec; 0.068 sec/batch; 2h:49m:04s remains)
INFO - root - 2019-11-06 18:01:16.645457: step 30, total loss = 6.63, predict loss = 1.81 (67.1 examples/sec; 0.060 sec/batch; 2h:29m:02s remains)
INFO - root - 2019-11-06 18:01:17.367730: step 40, total loss = 3.65, predict loss = 1.07 (60.9 examples/sec; 0.066 sec/batch; 2h:44m:13s remains)
INFO - root - 2019-11-06 18:01:18.021382: step 50, total loss = 4.33, predict loss = 1.23 (93.7 examples/sec; 0.043 sec/batch; 1h:46m:38s remains)
INFO - root - 2019-11-06 18:01:18.472869: step 60, total loss = 5.07, predict loss = 1.38 (96.4 examples/sec; 0.041 sec/batch; 1h:43m:41s remains)
INFO - root - 2019-11-06 18:01:18.917114: step 70, total loss = 6.03, predict loss = 1.78 (94.2 examples/sec; 0.042 sec/batch; 1h:46m:06s remains)
INFO - root - 2019-11-06 18:01:20.391303: step 80, total loss = 3.22, predict loss = 0.93 (65.9 examples/sec; 0.061 sec/batch; 2h:31m:39s remains)
INFO - root - 2019-11-06 18:01:21.204688: step 90, total loss = 6.07, predict loss = 1.72 (52.8 examples/sec; 0.076 sec/batch; 3h:09m:24s remains)
INFO - root - 2019-11-06 18:01:21.999657: step 100, total loss = 5.03, predict loss = 1.49 (51.9 examples/sec; 0.077 sec/batch; 3h:12m:43s remains)
INFO - root - 2019-11-06 18:01:22.745041: step 110, total loss = 3.72, predict loss = 1.06 (60.3 examples/sec; 0.066 sec/batch; 2h:45m:44s remains)
INFO - root - 2019-11-06 18:01:23.517489: step 120, total loss = 4.88, predict loss = 1.38 (63.4 examples/sec; 0.063 sec/batch; 2h:37m:29s remains)
INFO - root - 2019-11-06 18:01:24.103812: step 130, total loss = 3.68, predict loss = 1.10 (82.0 examples/sec; 0.049 sec/batch; 2h:01m:53s remains)
INFO - root - 2019-11-06 18:01:24.556363: step 140, total loss = 6.31, predict loss = 1.80 (95.9 examples/sec; 0.042 sec/batch; 1h:44m:09s remains)
INFO - root - 2019-11-06 18:01:25.709295: step 150, total loss = 6.93, predict loss = 1.97 (73.8 examples/sec; 0.054 sec/batch; 2h:15m:16s remains)
INFO - root - 2019-11-06 18:01:26.445835: step 160, total loss = 5.46, predict loss = 1.61 (70.1 examples/sec; 0.057 sec/batch; 2h:22m:27s remains)
INFO - root - 2019-11-06 18:01:27.187319: step 170, total loss = 3.87, predict loss = 1.10 (63.8 examples/sec; 0.063 sec/batch; 2h:36m:26s remains)
INFO - root - 2019-11-06 18:01:27.924009: step 180, total loss = 3.91, predict loss = 1.21 (56.9 examples/sec; 0.070 sec/batch; 2h:55m:40s remains)
INFO - root - 2019-11-06 18:01:28.650501: step 190, total loss = 4.76, predict loss = 1.39 (54.4 examples/sec; 0.073 sec/batch; 3h:03m:29s remains)
INFO - root - 2019-11-06 18:01:29.281840: step 200, total loss = 6.57, predict loss = 1.83 (94.0 examples/sec; 0.043 sec/batch; 1h:46m:11s remains)
INFO - root - 2019-11-06 18:01:29.762832: step 210, total loss = 6.57, predict loss = 1.79 (91.9 examples/sec; 0.044 sec/batch; 1h:48m:37s remains)
INFO - root - 2019-11-06 18:01:30.225757: step 220, total loss = 3.41, predict loss = 0.92 (73.8 examples/sec; 0.054 sec/batch; 2h:15m:16s remains)
INFO - root - 2019-11-06 18:01:31.506989: step 230, total loss = 5.21, predict loss = 1.58 (55.0 examples/sec; 0.073 sec/batch; 3h:01m:40s remains)
INFO - root - 2019-11-06 18:01:32.229117: step 240, total loss = 6.04, predict loss = 1.74 (58.9 examples/sec; 0.068 sec/batch; 2h:49m:32s remains)
INFO - root - 2019-11-06 18:01:32.993058: step 250, total loss = 5.26, predict loss = 1.44 (63.0 examples/sec; 0.063 sec/batch; 2h:38m:27s remains)
INFO - root - 2019-11-06 18:01:33.709906: step 260, total loss = 5.67, predict loss = 1.65 (55.6 examples/sec; 0.072 sec/batch; 2h:59m:35s remains)
INFO - root - 2019-11-06 18:01:34.440016: step 270, total loss = 4.12, predict loss = 1.20 (68.8 examples/sec; 0.058 sec/batch; 2h:24m:59s remains)
INFO - root - 2019-11-06 18:01:34.970233: step 280, total loss = 6.44, predict loss = 1.90 (95.9 examples/sec; 0.042 sec/batch; 1h:44m:06s remains)
INFO - root - 2019-11-06 18:01:35.449330: step 290, total loss = 6.13, predict loss = 1.78 (88.2 examples/sec; 0.045 sec/batch; 1h:53m:09s remains)
INFO - root - 2019-11-06 18:01:36.593592: step 300, total loss = 5.97, predict loss = 1.84 (70.0 examples/sec; 0.057 sec/batch; 2h:22m:30s remains)
INFO - root - 2019-11-06 18:01:37.309982: step 310, total loss = 5.25, predict loss = 1.55 (56.8 examples/sec; 0.070 sec/batch; 2h:55m:44s remains)
INFO - root - 2019-11-06 18:01:38.019316: step 320, total loss = 4.35, predict loss = 1.28 (69.4 examples/sec; 0.058 sec/batch; 2h:23m:41s remains)
INFO - root - 2019-11-06 18:01:38.765345: step 330, total loss = 4.97, predict loss = 1.46 (63.3 examples/sec; 0.063 sec/batch; 2h:37m:32s remains)
INFO - root - 2019-11-06 18:01:39.504561: step 340, total loss = 6.67, predict loss = 1.98 (56.6 examples/sec; 0.071 sec/batch; 2h:56m:21s remains)
INFO - root - 2019-11-06 18:01:40.113368: step 350, total loss = 3.80, predict loss = 1.04 (101.0 examples/sec; 0.040 sec/batch; 1h:38m:48s remains)
INFO - root - 2019-11-06 18:01:40.562025: step 360, total loss = 4.06, predict loss = 1.16 (100.5 examples/sec; 0.040 sec/batch; 1h:39m:17s remains)
INFO - root - 2019-11-06 18:01:41.047180: step 370, total loss = 5.10, predict loss = 1.49 (93.0 examples/sec; 0.043 sec/batch; 1h:47m:13s remains)
INFO - root - 2019-11-06 18:01:42.329435: step 380, total loss = 5.41, predict loss = 1.65 (63.9 examples/sec; 0.063 sec/batch; 2h:36m:01s remains)
INFO - root - 2019-11-06 18:01:43.080462: step 390, total loss = 5.03, predict loss = 1.50 (60.8 examples/sec; 0.066 sec/batch; 2h:43m:57s remains)
INFO - root - 2019-11-06 18:01:43.842936: step 400, total loss = 4.93, predict loss = 1.42 (57.9 examples/sec; 0.069 sec/batch; 2h:52m:15s remains)
INFO - root - 2019-11-06 18:01:44.590973: step 410, total loss = 4.01, predict loss = 1.20 (58.5 examples/sec; 0.068 sec/batch; 2h:50m:22s remains)
INFO - root - 2019-11-06 18:01:45.319366: step 420, total loss = 6.19, predict loss = 1.70 (71.5 examples/sec; 0.056 sec/batch; 2h:19m:25s remains)
INFO - root - 2019-11-06 18:01:45.841530: step 430, total loss = 6.58, predict loss = 1.82 (96.5 examples/sec; 0.041 sec/batch; 1h:43m:19s remains)
INFO - root - 2019-11-06 18:01:46.285490: step 440, total loss = 4.83, predict loss = 1.34 (97.7 examples/sec; 0.041 sec/batch; 1h:42m:05s remains)
INFO - root - 2019-11-06 18:01:47.503037: step 450, total loss = 2.90, predict loss = 0.86 (72.6 examples/sec; 0.055 sec/batch; 2h:17m:17s remains)
INFO - root - 2019-11-06 18:01:48.302283: step 460, total loss = 4.30, predict loss = 1.20 (48.8 examples/sec; 0.082 sec/batch; 3h:24m:11s remains)
INFO - root - 2019-11-06 18:01:49.011473: step 470, total loss = 3.91, predict loss = 1.14 (64.3 examples/sec; 0.062 sec/batch; 2h:35m:08s remains)
INFO - root - 2019-11-06 18:01:49.765043: step 480, total loss = 5.21, predict loss = 1.40 (52.1 examples/sec; 0.077 sec/batch; 3h:11m:18s remains)
INFO - root - 2019-11-06 18:01:50.479377: step 490, total loss = 5.26, predict loss = 1.45 (62.0 examples/sec; 0.064 sec/batch; 2h:40m:38s remains)
INFO - root - 2019-11-06 18:01:51.026315: step 500, total loss = 4.51, predict loss = 1.34 (104.8 examples/sec; 0.038 sec/batch; 1h:35m:08s remains)
INFO - root - 2019-11-06 18:01:51.472620: step 510, total loss = 6.24, predict loss = 1.75 (96.5 examples/sec; 0.041 sec/batch; 1h:43m:14s remains)
INFO - root - 2019-11-06 18:01:51.970586: step 520, total loss = 4.49, predict loss = 1.31 (81.6 examples/sec; 0.049 sec/batch; 2h:02m:10s remains)
INFO - root - 2019-11-06 18:01:53.381223: step 530, total loss = 3.21, predict loss = 0.87 (49.2 examples/sec; 0.081 sec/batch; 3h:22m:33s remains)
INFO - root - 2019-11-06 18:01:54.165555: step 540, total loss = 5.82, predict loss = 1.67 (55.6 examples/sec; 0.072 sec/batch; 2h:59m:12s remains)
INFO - root - 2019-11-06 18:01:54.896448: step 550, total loss = 6.39, predict loss = 1.81 (63.8 examples/sec; 0.063 sec/batch; 2h:36m:12s remains)
INFO - root - 2019-11-06 18:01:55.652314: step 560, total loss = 5.45, predict loss = 1.59 (54.4 examples/sec; 0.074 sec/batch; 3h:03m:08s remains)
INFO - root - 2019-11-06 18:01:56.366235: step 570, total loss = 5.99, predict loss = 1.66 (69.2 examples/sec; 0.058 sec/batch; 2h:23m:56s remains)
INFO - root - 2019-11-06 18:01:56.858446: step 580, total loss = 5.35, predict loss = 1.53 (90.9 examples/sec; 0.044 sec/batch; 1h:49m:35s remains)
INFO - root - 2019-11-06 18:01:57.308245: step 590, total loss = 4.73, predict loss = 1.41 (91.5 examples/sec; 0.044 sec/batch; 1h:48m:53s remains)
INFO - root - 2019-11-06 18:01:58.507479: step 600, total loss = 4.37, predict loss = 1.25 (62.7 examples/sec; 0.064 sec/batch; 2h:38m:45s remains)
INFO - root - 2019-11-06 18:01:59.239233: step 610, total loss = 4.38, predict loss = 1.29 (60.3 examples/sec; 0.066 sec/batch; 2h:45m:15s remains)
INFO - root - 2019-11-06 18:01:59.977121: step 620, total loss = 3.74, predict loss = 1.12 (57.4 examples/sec; 0.070 sec/batch; 2h:53m:30s remains)
INFO - root - 2019-11-06 18:02:00.751471: step 630, total loss = 2.41, predict loss = 0.69 (65.3 examples/sec; 0.061 sec/batch; 2h:32m:35s remains)
INFO - root - 2019-11-06 18:02:01.485133: step 640, total loss = 5.02, predict loss = 1.49 (60.2 examples/sec; 0.066 sec/batch; 2h:45m:29s remains)
INFO - root - 2019-11-06 18:02:02.079269: step 650, total loss = 5.01, predict loss = 1.49 (104.4 examples/sec; 0.038 sec/batch; 1h:35m:23s remains)
INFO - root - 2019-11-06 18:02:02.519188: step 660, total loss = 4.40, predict loss = 1.27 (100.9 examples/sec; 0.040 sec/batch; 1h:38m:41s remains)
INFO - root - 2019-11-06 18:02:02.950528: step 670, total loss = 5.99, predict loss = 1.74 (127.3 examples/sec; 0.031 sec/batch; 1h:18m:12s remains)
INFO - root - 2019-11-06 18:02:04.361241: step 680, total loss = 5.58, predict loss = 1.61 (57.1 examples/sec; 0.070 sec/batch; 2h:54m:24s remains)
INFO - root - 2019-11-06 18:02:05.151807: step 690, total loss = 4.14, predict loss = 1.10 (48.1 examples/sec; 0.083 sec/batch; 3h:26m:44s remains)
INFO - root - 2019-11-06 18:02:05.921021: step 700, total loss = 5.15, predict loss = 1.49 (61.4 examples/sec; 0.065 sec/batch; 2h:42m:03s remains)
INFO - root - 2019-11-06 18:02:06.644556: step 710, total loss = 5.54, predict loss = 1.52 (53.1 examples/sec; 0.075 sec/batch; 3h:07m:33s remains)
INFO - root - 2019-11-06 18:02:07.345302: step 720, total loss = 5.39, predict loss = 1.52 (73.7 examples/sec; 0.054 sec/batch; 2h:15m:06s remains)
INFO - root - 2019-11-06 18:02:07.829248: step 730, total loss = 4.09, predict loss = 1.24 (94.0 examples/sec; 0.043 sec/batch; 1h:45m:51s remains)
INFO - root - 2019-11-06 18:02:08.274509: step 740, total loss = 4.47, predict loss = 1.30 (93.0 examples/sec; 0.043 sec/batch; 1h:47m:02s remains)
INFO - root - 2019-11-06 18:02:09.482457: step 750, total loss = 3.99, predict loss = 1.22 (63.7 examples/sec; 0.063 sec/batch; 2h:36m:06s remains)
INFO - root - 2019-11-06 18:02:10.234340: step 760, total loss = 4.65, predict loss = 1.35 (55.0 examples/sec; 0.073 sec/batch; 3h:00m:54s remains)
INFO - root - 2019-11-06 18:02:10.971137: step 770, total loss = 5.77, predict loss = 1.68 (63.3 examples/sec; 0.063 sec/batch; 2h:37m:07s remains)
INFO - root - 2019-11-06 18:02:11.668324: step 780, total loss = 5.71, predict loss = 1.61 (63.8 examples/sec; 0.063 sec/batch; 2h:35m:55s remains)
INFO - root - 2019-11-06 18:02:12.410680: step 790, total loss = 4.60, predict loss = 1.38 (69.2 examples/sec; 0.058 sec/batch; 2h:23m:42s remains)
INFO - root - 2019-11-06 18:02:12.978393: step 800, total loss = 6.57, predict loss = 1.84 (99.6 examples/sec; 0.040 sec/batch; 1h:39m:50s remains)
INFO - root - 2019-11-06 18:02:13.465939: step 810, total loss = 5.03, predict loss = 1.36 (96.8 examples/sec; 0.041 sec/batch; 1h:42m:45s remains)
INFO - root - 2019-11-06 18:02:14.609425: step 820, total loss = 5.72, predict loss = 1.60 (5.4 examples/sec; 0.743 sec/batch; 30h:46m:51s remains)
INFO - root - 2019-11-06 18:02:15.330540: step 830, total loss = 5.57, predict loss = 1.63 (54.5 examples/sec; 0.073 sec/batch; 3h:02m:35s remains)
INFO - root - 2019-11-06 18:02:16.094432: step 840, total loss = 5.33, predict loss = 1.56 (59.6 examples/sec; 0.067 sec/batch; 2h:46m:51s remains)
INFO - root - 2019-11-06 18:02:16.922755: step 850, total loss = 5.53, predict loss = 1.59 (49.4 examples/sec; 0.081 sec/batch; 3h:21m:24s remains)
INFO - root - 2019-11-06 18:02:17.622710: step 860, total loss = 5.77, predict loss = 1.59 (68.0 examples/sec; 0.059 sec/batch; 2h:26m:09s remains)
INFO - root - 2019-11-06 18:02:18.243279: step 870, total loss = 4.93, predict loss = 1.45 (93.9 examples/sec; 0.043 sec/batch; 1h:45m:51s remains)
INFO - root - 2019-11-06 18:02:18.711893: step 880, total loss = 5.11, predict loss = 1.51 (89.8 examples/sec; 0.045 sec/batch; 1h:50m:43s remains)
INFO - root - 2019-11-06 18:02:19.203199: step 890, total loss = 4.83, predict loss = 1.40 (107.6 examples/sec; 0.037 sec/batch; 1h:32m:23s remains)
INFO - root - 2019-11-06 18:02:20.414319: step 900, total loss = 5.34, predict loss = 1.53 (68.2 examples/sec; 0.059 sec/batch; 2h:25m:50s remains)
INFO - root - 2019-11-06 18:02:21.130600: step 910, total loss = 5.35, predict loss = 1.44 (59.2 examples/sec; 0.068 sec/batch; 2h:47m:47s remains)
INFO - root - 2019-11-06 18:02:21.986488: step 920, total loss = 5.07, predict loss = 1.48 (41.4 examples/sec; 0.097 sec/batch; 4h:00m:00s remains)
INFO - root - 2019-11-06 18:02:22.700231: step 930, total loss = 5.00, predict loss = 1.44 (63.5 examples/sec; 0.063 sec/batch; 2h:36m:32s remains)
INFO - root - 2019-11-06 18:02:23.408303: step 940, total loss = 3.90, predict loss = 1.14 (63.7 examples/sec; 0.063 sec/batch; 2h:35m:53s remains)
INFO - root - 2019-11-06 18:02:23.934800: step 950, total loss = 5.19, predict loss = 1.51 (103.0 examples/sec; 0.039 sec/batch; 1h:36m:29s remains)
INFO - root - 2019-11-06 18:02:24.401871: step 960, total loss = 5.54, predict loss = 1.58 (92.8 examples/sec; 0.043 sec/batch; 1h:47m:05s remains)
INFO - root - 2019-11-06 18:02:25.557596: step 970, total loss = 5.09, predict loss = 1.43 (66.8 examples/sec; 0.060 sec/batch; 2h:28m:43s remains)
INFO - root - 2019-11-06 18:02:26.204595: step 980, total loss = 4.60, predict loss = 1.37 (65.8 examples/sec; 0.061 sec/batch; 2h:30m:54s remains)
INFO - root - 2019-11-06 18:02:26.908099: step 990, total loss = 5.97, predict loss = 1.76 (59.3 examples/sec; 0.067 sec/batch; 2h:47m:29s remains)
INFO - root - 2019-11-06 18:02:27.644798: step 1000, total loss = 5.40, predict loss = 1.59 (60.9 examples/sec; 0.066 sec/batch; 2h:43m:11s remains)
INFO - root - 2019-11-06 18:02:28.398594: step 1010, total loss = 4.04, predict loss = 1.09 (54.7 examples/sec; 0.073 sec/batch; 3h:01m:38s remains)
INFO - root - 2019-11-06 18:02:29.048615: step 1020, total loss = 4.05, predict loss = 1.23 (97.4 examples/sec; 0.041 sec/batch; 1h:41m:59s remains)
INFO - root - 2019-11-06 18:02:29.495251: step 1030, total loss = 3.75, predict loss = 1.09 (95.8 examples/sec; 0.042 sec/batch; 1h:43m:40s remains)
INFO - root - 2019-11-06 18:02:29.943999: step 1040, total loss = 5.03, predict loss = 1.39 (99.3 examples/sec; 0.040 sec/batch; 1h:40m:00s remains)
INFO - root - 2019-11-06 18:02:31.251267: step 1050, total loss = 6.29, predict loss = 1.74 (63.9 examples/sec; 0.063 sec/batch; 2h:35m:17s remains)
INFO - root - 2019-11-06 18:02:31.984206: step 1060, total loss = 5.97, predict loss = 1.81 (55.4 examples/sec; 0.072 sec/batch; 2h:59m:07s remains)
INFO - root - 2019-11-06 18:02:32.711261: step 1070, total loss = 5.54, predict loss = 1.62 (59.0 examples/sec; 0.068 sec/batch; 2h:48m:15s remains)
INFO - root - 2019-11-06 18:02:33.465100: step 1080, total loss = 6.28, predict loss = 1.80 (61.7 examples/sec; 0.065 sec/batch; 2h:40m:52s remains)
INFO - root - 2019-11-06 18:02:34.223862: step 1090, total loss = 3.81, predict loss = 1.08 (64.2 examples/sec; 0.062 sec/batch; 2h:34m:41s remains)
INFO - root - 2019-11-06 18:02:34.756193: step 1100, total loss = 6.43, predict loss = 1.87 (91.3 examples/sec; 0.044 sec/batch; 1h:48m:45s remains)
INFO - root - 2019-11-06 18:02:35.205775: step 1110, total loss = 4.96, predict loss = 1.39 (97.1 examples/sec; 0.041 sec/batch; 1h:42m:14s remains)
INFO - root - 2019-11-06 18:02:36.355789: step 1120, total loss = 4.95, predict loss = 1.57 (66.3 examples/sec; 0.060 sec/batch; 2h:29m:39s remains)
INFO - root - 2019-11-06 18:02:37.073236: step 1130, total loss = 4.78, predict loss = 1.34 (56.5 examples/sec; 0.071 sec/batch; 2h:55m:32s remains)
INFO - root - 2019-11-06 18:02:37.823649: step 1140, total loss = 4.52, predict loss = 1.38 (56.0 examples/sec; 0.071 sec/batch; 2h:57m:14s remains)
INFO - root - 2019-11-06 18:02:38.589552: step 1150, total loss = 5.61, predict loss = 1.58 (58.2 examples/sec; 0.069 sec/batch; 2h:50m:31s remains)
INFO - root - 2019-11-06 18:02:39.322412: step 1160, total loss = 5.62, predict loss = 1.62 (56.7 examples/sec; 0.070 sec/batch; 2h:54m:52s remains)
INFO - root - 2019-11-06 18:02:39.976995: step 1170, total loss = 5.48, predict loss = 1.56 (91.5 examples/sec; 0.044 sec/batch; 1h:48m:28s remains)
INFO - root - 2019-11-06 18:02:40.430195: step 1180, total loss = 3.62, predict loss = 1.03 (95.3 examples/sec; 0.042 sec/batch; 1h:44m:09s remains)
INFO - root - 2019-11-06 18:02:40.888421: step 1190, total loss = 4.67, predict loss = 1.32 (105.6 examples/sec; 0.038 sec/batch; 1h:33m:56s remains)
INFO - root - 2019-11-06 18:02:42.196139: step 1200, total loss = 4.35, predict loss = 1.25 (61.3 examples/sec; 0.065 sec/batch; 2h:41m:42s remains)
INFO - root - 2019-11-06 18:02:42.937472: step 1210, total loss = 4.70, predict loss = 1.35 (64.3 examples/sec; 0.062 sec/batch; 2h:34m:15s remains)
INFO - root - 2019-11-06 18:02:43.667999: step 1220, total loss = 4.42, predict loss = 1.28 (61.9 examples/sec; 0.065 sec/batch; 2h:40m:21s remains)
INFO - root - 2019-11-06 18:02:44.445499: step 1230, total loss = 6.88, predict loss = 1.98 (59.9 examples/sec; 0.067 sec/batch; 2h:45m:37s remains)
INFO - root - 2019-11-06 18:02:45.217160: step 1240, total loss = 4.54, predict loss = 1.29 (56.6 examples/sec; 0.071 sec/batch; 2h:55m:04s remains)
INFO - root - 2019-11-06 18:02:45.763557: step 1250, total loss = 5.46, predict loss = 1.56 (96.0 examples/sec; 0.042 sec/batch; 1h:43m:17s remains)
INFO - root - 2019-11-06 18:02:46.221064: step 1260, total loss = 5.93, predict loss = 1.60 (95.1 examples/sec; 0.042 sec/batch; 1h:44m:15s remains)
INFO - root - 2019-11-06 18:02:47.372724: step 1270, total loss = 6.16, predict loss = 1.81 (78.6 examples/sec; 0.051 sec/batch; 2h:06m:13s remains)
INFO - root - 2019-11-06 18:02:48.122276: step 1280, total loss = 4.37, predict loss = 1.26 (58.9 examples/sec; 0.068 sec/batch; 2h:48m:14s remains)
INFO - root - 2019-11-06 18:02:48.879699: step 1290, total loss = 3.42, predict loss = 0.95 (58.4 examples/sec; 0.068 sec/batch; 2h:49m:40s remains)
INFO - root - 2019-11-06 18:02:49.606668: step 1300, total loss = 5.14, predict loss = 1.48 (59.6 examples/sec; 0.067 sec/batch; 2h:46m:21s remains)
INFO - root - 2019-11-06 18:02:50.324509: step 1310, total loss = 3.72, predict loss = 1.17 (61.9 examples/sec; 0.065 sec/batch; 2h:40m:12s remains)
INFO - root - 2019-11-06 18:02:50.930476: step 1320, total loss = 5.63, predict loss = 1.64 (96.0 examples/sec; 0.042 sec/batch; 1h:43m:17s remains)
INFO - root - 2019-11-06 18:02:51.404506: step 1330, total loss = 5.16, predict loss = 1.50 (95.1 examples/sec; 0.042 sec/batch; 1h:44m:12s remains)
INFO - root - 2019-11-06 18:02:51.894272: step 1340, total loss = 5.98, predict loss = 1.77 (73.1 examples/sec; 0.055 sec/batch; 2h:15m:37s remains)
INFO - root - 2019-11-06 18:02:53.244705: step 1350, total loss = 3.70, predict loss = 1.04 (58.2 examples/sec; 0.069 sec/batch; 2h:50m:21s remains)
INFO - root - 2019-11-06 18:02:54.016377: step 1360, total loss = 5.66, predict loss = 1.71 (56.0 examples/sec; 0.071 sec/batch; 2h:56m:54s remains)
INFO - root - 2019-11-06 18:02:54.773309: step 1370, total loss = 5.67, predict loss = 1.58 (63.5 examples/sec; 0.063 sec/batch; 2h:36m:00s remains)
INFO - root - 2019-11-06 18:02:55.521958: step 1380, total loss = 6.89, predict loss = 1.97 (56.9 examples/sec; 0.070 sec/batch; 2h:54m:16s remains)
INFO - root - 2019-11-06 18:02:56.219537: step 1390, total loss = 6.26, predict loss = 1.77 (68.1 examples/sec; 0.059 sec/batch; 2h:25m:26s remains)
INFO - root - 2019-11-06 18:02:56.672137: step 1400, total loss = 6.37, predict loss = 1.83 (106.8 examples/sec; 0.037 sec/batch; 1h:32m:48s remains)
INFO - root - 2019-11-06 18:02:57.158611: step 1410, total loss = 4.41, predict loss = 1.31 (93.2 examples/sec; 0.043 sec/batch; 1h:46m:14s remains)
INFO - root - 2019-11-06 18:02:58.343210: step 1420, total loss = 4.92, predict loss = 1.42 (65.2 examples/sec; 0.061 sec/batch; 2h:31m:58s remains)
INFO - root - 2019-11-06 18:02:59.106722: step 1430, total loss = 3.90, predict loss = 1.13 (63.4 examples/sec; 0.063 sec/batch; 2h:36m:13s remains)
INFO - root - 2019-11-06 18:02:59.862349: step 1440, total loss = 5.45, predict loss = 1.62 (57.6 examples/sec; 0.069 sec/batch; 2h:51m:55s remains)
INFO - root - 2019-11-06 18:03:00.621171: step 1450, total loss = 5.53, predict loss = 1.59 (61.7 examples/sec; 0.065 sec/batch; 2h:40m:31s remains)
INFO - root - 2019-11-06 18:03:01.304786: step 1460, total loss = 4.51, predict loss = 1.29 (73.6 examples/sec; 0.054 sec/batch; 2h:14m:36s remains)
INFO - root - 2019-11-06 18:03:01.815844: step 1470, total loss = 3.49, predict loss = 1.03 (100.4 examples/sec; 0.040 sec/batch; 1h:38m:34s remains)
INFO - root - 2019-11-06 18:03:02.265754: step 1480, total loss = 5.13, predict loss = 1.50 (104.8 examples/sec; 0.038 sec/batch; 1h:34m:27s remains)
INFO - root - 2019-11-06 18:03:02.735598: step 1490, total loss = 6.23, predict loss = 1.90 (124.8 examples/sec; 0.032 sec/batch; 1h:19m:18s remains)
INFO - root - 2019-11-06 18:03:04.057280: step 1500, total loss = 3.84, predict loss = 1.10 (59.6 examples/sec; 0.067 sec/batch; 2h:46m:13s remains)
INFO - root - 2019-11-06 18:03:04.783793: step 1510, total loss = 5.30, predict loss = 1.48 (63.8 examples/sec; 0.063 sec/batch; 2h:35m:12s remains)
INFO - root - 2019-11-06 18:03:05.556488: step 1520, total loss = 5.81, predict loss = 1.64 (54.7 examples/sec; 0.073 sec/batch; 3h:00m:51s remains)
INFO - root - 2019-11-06 18:03:06.302205: step 1530, total loss = 5.53, predict loss = 1.64 (60.5 examples/sec; 0.066 sec/batch; 2h:43m:37s remains)
INFO - root - 2019-11-06 18:03:06.948540: step 1540, total loss = 2.91, predict loss = 0.83 (74.8 examples/sec; 0.053 sec/batch; 2h:12m:21s remains)
INFO - root - 2019-11-06 18:03:07.413996: step 1550, total loss = 4.66, predict loss = 1.37 (97.1 examples/sec; 0.041 sec/batch; 1h:41m:58s remains)
INFO - root - 2019-11-06 18:03:07.875620: step 1560, total loss = 3.72, predict loss = 1.08 (92.5 examples/sec; 0.043 sec/batch; 1h:46m:56s remains)
INFO - root - 2019-11-06 18:03:09.099133: step 1570, total loss = 5.84, predict loss = 1.79 (64.9 examples/sec; 0.062 sec/batch; 2h:32m:27s remains)
INFO - root - 2019-11-06 18:03:09.820903: step 1580, total loss = 5.97, predict loss = 1.65 (60.5 examples/sec; 0.066 sec/batch; 2h:43m:25s remains)
INFO - root - 2019-11-06 18:03:10.594944: step 1590, total loss = 5.83, predict loss = 1.68 (51.8 examples/sec; 0.077 sec/batch; 3h:11m:06s remains)
INFO - root - 2019-11-06 18:03:11.362763: step 1600, total loss = 6.00, predict loss = 1.68 (57.2 examples/sec; 0.070 sec/batch; 2h:52m:56s remains)
INFO - root - 2019-11-06 18:03:12.157626: step 1610, total loss = 3.23, predict loss = 0.96 (63.5 examples/sec; 0.063 sec/batch; 2h:35m:45s remains)
INFO - root - 2019-11-06 18:03:12.715263: step 1620, total loss = 4.75, predict loss = 1.32 (93.6 examples/sec; 0.043 sec/batch; 1h:45m:42s remains)
INFO - root - 2019-11-06 18:03:13.166515: step 1630, total loss = 4.76, predict loss = 1.37 (100.0 examples/sec; 0.040 sec/batch; 1h:38m:54s remains)
INFO - root - 2019-11-06 18:03:14.255252: step 1640, total loss = 6.71, predict loss = 1.87 (5.8 examples/sec; 0.691 sec/batch; 28h:28m:29s remains)
INFO - root - 2019-11-06 18:03:14.942588: step 1650, total loss = 5.27, predict loss = 1.54 (67.9 examples/sec; 0.059 sec/batch; 2h:25m:36s remains)
INFO - root - 2019-11-06 18:03:15.697989: step 1660, total loss = 6.78, predict loss = 1.98 (57.6 examples/sec; 0.069 sec/batch; 2h:51m:35s remains)
INFO - root - 2019-11-06 18:03:16.492020: step 1670, total loss = 4.43, predict loss = 1.31 (57.5 examples/sec; 0.070 sec/batch; 2h:51m:54s remains)
INFO - root - 2019-11-06 18:03:17.290327: step 1680, total loss = 5.37, predict loss = 1.50 (56.8 examples/sec; 0.070 sec/batch; 2h:54m:05s remains)
INFO - root - 2019-11-06 18:03:17.954859: step 1690, total loss = 4.97, predict loss = 1.49 (90.8 examples/sec; 0.044 sec/batch; 1h:48m:55s remains)
INFO - root - 2019-11-06 18:03:18.408571: step 1700, total loss = 5.10, predict loss = 1.44 (84.7 examples/sec; 0.047 sec/batch; 1h:56m:42s remains)
INFO - root - 2019-11-06 18:03:18.864446: step 1710, total loss = 2.80, predict loss = 0.83 (94.6 examples/sec; 0.042 sec/batch; 1h:44m:33s remains)
INFO - root - 2019-11-06 18:03:20.082282: step 1720, total loss = 6.16, predict loss = 1.71 (61.9 examples/sec; 0.065 sec/batch; 2h:39m:38s remains)
INFO - root - 2019-11-06 18:03:20.835508: step 1730, total loss = 4.35, predict loss = 1.30 (60.6 examples/sec; 0.066 sec/batch; 2h:43m:08s remains)
INFO - root - 2019-11-06 18:03:21.588853: step 1740, total loss = 4.87, predict loss = 1.38 (56.8 examples/sec; 0.070 sec/batch; 2h:53m:59s remains)
INFO - root - 2019-11-06 18:03:22.350856: step 1750, total loss = 3.79, predict loss = 1.09 (63.1 examples/sec; 0.063 sec/batch; 2h:36m:41s remains)
INFO - root - 2019-11-06 18:03:23.055907: step 1760, total loss = 5.22, predict loss = 1.49 (62.4 examples/sec; 0.064 sec/batch; 2h:38m:20s remains)
INFO - root - 2019-11-06 18:03:23.612786: step 1770, total loss = 4.12, predict loss = 1.21 (103.0 examples/sec; 0.039 sec/batch; 1h:35m:56s remains)
INFO - root - 2019-11-06 18:03:24.067182: step 1780, total loss = 5.41, predict loss = 1.55 (88.0 examples/sec; 0.045 sec/batch; 1h:52m:18s remains)
INFO - root - 2019-11-06 18:03:25.219818: step 1790, total loss = 5.79, predict loss = 1.71 (71.0 examples/sec; 0.056 sec/batch; 2h:19m:14s remains)
INFO - root - 2019-11-06 18:03:25.906494: step 1800, total loss = 5.56, predict loss = 1.70 (64.8 examples/sec; 0.062 sec/batch; 2h:32m:23s remains)
INFO - root - 2019-11-06 18:03:26.656290: step 1810, total loss = 6.28, predict loss = 1.81 (58.0 examples/sec; 0.069 sec/batch; 2h:50m:11s remains)
INFO - root - 2019-11-06 18:03:27.398038: step 1820, total loss = 5.76, predict loss = 1.70 (55.5 examples/sec; 0.072 sec/batch; 2h:58m:05s remains)
INFO - root - 2019-11-06 18:03:28.099696: step 1830, total loss = 4.64, predict loss = 1.28 (67.6 examples/sec; 0.059 sec/batch; 2h:26m:12s remains)
INFO - root - 2019-11-06 18:03:28.802070: step 1840, total loss = 5.14, predict loss = 1.54 (96.1 examples/sec; 0.042 sec/batch; 1h:42m:45s remains)
INFO - root - 2019-11-06 18:03:29.272584: step 1850, total loss = 4.44, predict loss = 1.32 (97.2 examples/sec; 0.041 sec/batch; 1h:41m:37s remains)
INFO - root - 2019-11-06 18:03:29.723486: step 1860, total loss = 5.17, predict loss = 1.40 (94.2 examples/sec; 0.042 sec/batch; 1h:44m:50s remains)
INFO - root - 2019-11-06 18:03:30.937523: step 1870, total loss = 5.50, predict loss = 1.56 (55.2 examples/sec; 0.072 sec/batch; 2h:58m:56s remains)
INFO - root - 2019-11-06 18:03:31.712770: step 1880, total loss = 4.83, predict loss = 1.40 (62.0 examples/sec; 0.065 sec/batch; 2h:39m:19s remains)
INFO - root - 2019-11-06 18:03:32.472118: step 1890, total loss = 2.83, predict loss = 0.86 (55.3 examples/sec; 0.072 sec/batch; 2h:58m:30s remains)
INFO - root - 2019-11-06 18:03:33.264910: step 1900, total loss = 6.47, predict loss = 1.84 (60.7 examples/sec; 0.066 sec/batch; 2h:42m:45s remains)
INFO - root - 2019-11-06 18:03:33.956798: step 1910, total loss = 5.02, predict loss = 1.50 (68.1 examples/sec; 0.059 sec/batch; 2h:24m:55s remains)
INFO - root - 2019-11-06 18:03:34.465641: step 1920, total loss = 5.96, predict loss = 1.71 (89.7 examples/sec; 0.045 sec/batch; 1h:50m:05s remains)
INFO - root - 2019-11-06 18:03:34.940340: step 1930, total loss = 5.00, predict loss = 1.40 (96.5 examples/sec; 0.041 sec/batch; 1h:42m:14s remains)
INFO - root - 2019-11-06 18:03:36.109244: step 1940, total loss = 4.88, predict loss = 1.43 (69.8 examples/sec; 0.057 sec/batch; 2h:21m:30s remains)
INFO - root - 2019-11-06 18:03:36.744496: step 1950, total loss = 5.70, predict loss = 1.67 (68.1 examples/sec; 0.059 sec/batch; 2h:24m:58s remains)
INFO - root - 2019-11-06 18:03:37.429524: step 1960, total loss = 5.21, predict loss = 1.57 (59.6 examples/sec; 0.067 sec/batch; 2h:45m:43s remains)
INFO - root - 2019-11-06 18:03:38.146253: step 1970, total loss = 5.94, predict loss = 1.67 (56.4 examples/sec; 0.071 sec/batch; 2h:54m:55s remains)
INFO - root - 2019-11-06 18:03:38.994511: step 1980, total loss = 6.06, predict loss = 1.74 (55.3 examples/sec; 0.072 sec/batch; 2h:58m:18s remains)
INFO - root - 2019-11-06 18:03:39.584873: step 1990, total loss = 5.36, predict loss = 1.50 (102.4 examples/sec; 0.039 sec/batch; 1h:36m:22s remains)
INFO - root - 2019-11-06 18:03:40.030758: step 2000, total loss = 4.77, predict loss = 1.35 (96.4 examples/sec; 0.041 sec/batch; 1h:42m:19s remains)
INFO - root - 2019-11-06 18:03:40.517050: step 2010, total loss = 5.03, predict loss = 1.43 (88.5 examples/sec; 0.045 sec/batch; 1h:51m:28s remains)
INFO - root - 2019-11-06 18:03:41.761444: step 2020, total loss = 4.56, predict loss = 1.25 (68.6 examples/sec; 0.058 sec/batch; 2h:23m:53s remains)
INFO - root - 2019-11-06 18:03:42.512382: step 2030, total loss = 4.07, predict loss = 1.19 (61.6 examples/sec; 0.065 sec/batch; 2h:40m:08s remains)
INFO - root - 2019-11-06 18:03:43.280618: step 2040, total loss = 5.60, predict loss = 1.62 (56.1 examples/sec; 0.071 sec/batch; 2h:55m:52s remains)
INFO - root - 2019-11-06 18:03:44.013703: step 2050, total loss = 5.54, predict loss = 1.50 (55.3 examples/sec; 0.072 sec/batch; 2h:58m:14s remains)
INFO - root - 2019-11-06 18:03:44.677462: step 2060, total loss = 4.24, predict loss = 1.26 (80.7 examples/sec; 0.050 sec/batch; 2h:02m:15s remains)
INFO - root - 2019-11-06 18:03:45.158869: step 2070, total loss = 4.13, predict loss = 1.20 (95.4 examples/sec; 0.042 sec/batch; 1h:43m:20s remains)
INFO - root - 2019-11-06 18:03:45.613930: step 2080, total loss = 5.06, predict loss = 1.44 (92.9 examples/sec; 0.043 sec/batch; 1h:46m:10s remains)
INFO - root - 2019-11-06 18:03:46.844830: step 2090, total loss = 5.50, predict loss = 1.60 (67.4 examples/sec; 0.059 sec/batch; 2h:26m:12s remains)
INFO - root - 2019-11-06 18:03:47.528534: step 2100, total loss = 4.68, predict loss = 1.36 (62.4 examples/sec; 0.064 sec/batch; 2h:38m:07s remains)
INFO - root - 2019-11-06 18:03:48.251246: step 2110, total loss = 5.26, predict loss = 1.55 (56.1 examples/sec; 0.071 sec/batch; 2h:55m:36s remains)
INFO - root - 2019-11-06 18:03:49.046813: step 2120, total loss = 5.00, predict loss = 1.46 (58.9 examples/sec; 0.068 sec/batch; 2h:47m:29s remains)
INFO - root - 2019-11-06 18:03:49.818053: step 2130, total loss = 5.30, predict loss = 1.49 (50.5 examples/sec; 0.079 sec/batch; 3h:15m:07s remains)
INFO - root - 2019-11-06 18:03:50.424499: step 2140, total loss = 5.26, predict loss = 1.47 (99.3 examples/sec; 0.040 sec/batch; 1h:39m:17s remains)
INFO - root - 2019-11-06 18:03:50.872127: step 2150, total loss = 5.69, predict loss = 1.61 (97.4 examples/sec; 0.041 sec/batch; 1h:41m:10s remains)
INFO - root - 2019-11-06 18:03:51.313374: step 2160, total loss = 4.37, predict loss = 1.36 (96.7 examples/sec; 0.041 sec/batch; 1h:41m:52s remains)
INFO - root - 2019-11-06 18:03:52.760287: step 2170, total loss = 6.07, predict loss = 1.82 (68.7 examples/sec; 0.058 sec/batch; 2h:23m:22s remains)
INFO - root - 2019-11-06 18:03:53.488918: step 2180, total loss = 4.38, predict loss = 1.34 (57.8 examples/sec; 0.069 sec/batch; 2h:50m:33s remains)
INFO - root - 2019-11-06 18:03:54.281293: step 2190, total loss = 6.37, predict loss = 1.84 (60.9 examples/sec; 0.066 sec/batch; 2h:41m:47s remains)
INFO - root - 2019-11-06 18:03:55.039492: step 2200, total loss = 5.40, predict loss = 1.65 (57.6 examples/sec; 0.069 sec/batch; 2h:51m:10s remains)
INFO - root - 2019-11-06 18:03:55.772636: step 2210, total loss = 4.56, predict loss = 1.19 (60.4 examples/sec; 0.066 sec/batch; 2h:43m:01s remains)
INFO - root - 2019-11-06 18:03:56.279438: step 2220, total loss = 6.26, predict loss = 1.69 (96.3 examples/sec; 0.042 sec/batch; 1h:42m:16s remains)
INFO - root - 2019-11-06 18:03:56.738858: step 2230, total loss = 5.99, predict loss = 1.71 (96.9 examples/sec; 0.041 sec/batch; 1h:41m:41s remains)
INFO - root - 2019-11-06 18:03:57.932606: step 2240, total loss = 6.34, predict loss = 1.85 (63.6 examples/sec; 0.063 sec/batch; 2h:34m:57s remains)
INFO - root - 2019-11-06 18:03:58.651421: step 2250, total loss = 5.30, predict loss = 1.49 (62.8 examples/sec; 0.064 sec/batch; 2h:36m:43s remains)
INFO - root - 2019-11-06 18:03:59.406392: step 2260, total loss = 4.64, predict loss = 1.37 (54.3 examples/sec; 0.074 sec/batch; 3h:01m:15s remains)
INFO - root - 2019-11-06 18:04:00.166907: step 2270, total loss = 5.49, predict loss = 1.57 (49.9 examples/sec; 0.080 sec/batch; 3h:17m:32s remains)
INFO - root - 2019-11-06 18:04:00.979397: step 2280, total loss = 5.27, predict loss = 1.54 (58.0 examples/sec; 0.069 sec/batch; 2h:49m:55s remains)
INFO - root - 2019-11-06 18:04:01.550121: step 2290, total loss = 4.25, predict loss = 1.26 (97.8 examples/sec; 0.041 sec/batch; 1h:40m:38s remains)
INFO - root - 2019-11-06 18:04:01.991923: step 2300, total loss = 5.23, predict loss = 1.51 (94.7 examples/sec; 0.042 sec/batch; 1h:43m:58s remains)
INFO - root - 2019-11-06 18:04:02.434564: step 2310, total loss = 4.04, predict loss = 1.14 (130.4 examples/sec; 0.031 sec/batch; 1h:15m:29s remains)
INFO - root - 2019-11-06 18:04:03.813548: step 2320, total loss = 3.96, predict loss = 1.23 (56.2 examples/sec; 0.071 sec/batch; 2h:55m:05s remains)
INFO - root - 2019-11-06 18:04:04.533877: step 2330, total loss = 4.72, predict loss = 1.47 (63.5 examples/sec; 0.063 sec/batch; 2h:35m:01s remains)
INFO - root - 2019-11-06 18:04:05.242689: step 2340, total loss = 4.91, predict loss = 1.36 (60.0 examples/sec; 0.067 sec/batch; 2h:43m:57s remains)
INFO - root - 2019-11-06 18:04:05.992107: step 2350, total loss = 5.09, predict loss = 1.40 (58.9 examples/sec; 0.068 sec/batch; 2h:47m:00s remains)
INFO - root - 2019-11-06 18:04:06.635997: step 2360, total loss = 5.75, predict loss = 1.64 (76.3 examples/sec; 0.052 sec/batch; 2h:09m:02s remains)
INFO - root - 2019-11-06 18:04:07.129142: step 2370, total loss = 5.95, predict loss = 1.71 (86.6 examples/sec; 0.046 sec/batch; 1h:53m:41s remains)
INFO - root - 2019-11-06 18:04:07.586478: step 2380, total loss = 5.39, predict loss = 1.54 (94.2 examples/sec; 0.042 sec/batch; 1h:44m:31s remains)
INFO - root - 2019-11-06 18:04:08.818870: step 2390, total loss = 5.48, predict loss = 1.60 (68.6 examples/sec; 0.058 sec/batch; 2h:23m:29s remains)
INFO - root - 2019-11-06 18:04:09.544364: step 2400, total loss = 5.02, predict loss = 1.43 (56.6 examples/sec; 0.071 sec/batch; 2h:53m:43s remains)
INFO - root - 2019-11-06 18:04:10.350332: step 2410, total loss = 4.54, predict loss = 1.34 (59.4 examples/sec; 0.067 sec/batch; 2h:45m:46s remains)
INFO - root - 2019-11-06 18:04:11.119300: step 2420, total loss = 6.40, predict loss = 1.83 (58.9 examples/sec; 0.068 sec/batch; 2h:47m:08s remains)
INFO - root - 2019-11-06 18:04:11.854253: step 2430, total loss = 5.46, predict loss = 1.55 (73.2 examples/sec; 0.055 sec/batch; 2h:14m:27s remains)
INFO - root - 2019-11-06 18:04:12.375542: step 2440, total loss = 4.95, predict loss = 1.44 (101.9 examples/sec; 0.039 sec/batch; 1h:36m:33s remains)
INFO - root - 2019-11-06 18:04:12.878304: step 2450, total loss = 4.45, predict loss = 1.24 (97.8 examples/sec; 0.041 sec/batch; 1h:40m:33s remains)
INFO - root - 2019-11-06 18:04:14.003079: step 2460, total loss = 4.51, predict loss = 1.30 (5.6 examples/sec; 0.716 sec/batch; 29h:21m:36s remains)
INFO - root - 2019-11-06 18:04:14.701682: step 2470, total loss = 4.29, predict loss = 1.23 (58.7 examples/sec; 0.068 sec/batch; 2h:47m:29s remains)
INFO - root - 2019-11-06 18:04:15.529263: step 2480, total loss = 3.64, predict loss = 1.00 (57.4 examples/sec; 0.070 sec/batch; 2h:51m:15s remains)
INFO - root - 2019-11-06 18:04:16.287679: step 2490, total loss = 4.71, predict loss = 1.35 (56.3 examples/sec; 0.071 sec/batch; 2h:54m:35s remains)
INFO - root - 2019-11-06 18:04:17.046206: step 2500, total loss = 5.18, predict loss = 1.56 (54.7 examples/sec; 0.073 sec/batch; 2h:59m:48s remains)
INFO - root - 2019-11-06 18:04:17.699700: step 2510, total loss = 3.83, predict loss = 1.11 (91.8 examples/sec; 0.044 sec/batch; 1h:47m:05s remains)
INFO - root - 2019-11-06 18:04:18.155704: step 2520, total loss = 5.66, predict loss = 1.61 (96.1 examples/sec; 0.042 sec/batch; 1h:42m:21s remains)
INFO - root - 2019-11-06 18:04:18.631236: step 2530, total loss = 4.59, predict loss = 1.34 (97.1 examples/sec; 0.041 sec/batch; 1h:41m:12s remains)
INFO - root - 2019-11-06 18:04:19.893664: step 2540, total loss = 5.70, predict loss = 1.68 (53.2 examples/sec; 0.075 sec/batch; 3h:04m:54s remains)
INFO - root - 2019-11-06 18:04:20.597598: step 2550, total loss = 5.43, predict loss = 1.56 (61.3 examples/sec; 0.065 sec/batch; 2h:40m:22s remains)
INFO - root - 2019-11-06 18:04:21.390734: step 2560, total loss = 4.03, predict loss = 1.17 (61.9 examples/sec; 0.065 sec/batch; 2h:38m:40s remains)
INFO - root - 2019-11-06 18:04:22.190266: step 2570, total loss = 6.26, predict loss = 1.85 (67.7 examples/sec; 0.059 sec/batch; 2h:25m:07s remains)
INFO - root - 2019-11-06 18:04:22.866976: step 2580, total loss = 5.90, predict loss = 1.74 (68.4 examples/sec; 0.058 sec/batch; 2h:23m:36s remains)
INFO - root - 2019-11-06 18:04:23.383665: step 2590, total loss = 5.12, predict loss = 1.46 (98.8 examples/sec; 0.040 sec/batch; 1h:39m:25s remains)
INFO - root - 2019-11-06 18:04:23.835147: step 2600, total loss = 4.35, predict loss = 1.23 (96.2 examples/sec; 0.042 sec/batch; 1h:42m:07s remains)
INFO - root - 2019-11-06 18:04:24.987538: step 2610, total loss = 5.82, predict loss = 1.67 (70.7 examples/sec; 0.057 sec/batch; 2h:18m:56s remains)
INFO - root - 2019-11-06 18:04:25.689640: step 2620, total loss = 4.81, predict loss = 1.42 (57.8 examples/sec; 0.069 sec/batch; 2h:49m:59s remains)
INFO - root - 2019-11-06 18:04:26.448596: step 2630, total loss = 5.63, predict loss = 1.61 (64.0 examples/sec; 0.063 sec/batch; 2h:33m:36s remains)
INFO - root - 2019-11-06 18:04:27.196573: step 2640, total loss = 3.94, predict loss = 1.18 (62.5 examples/sec; 0.064 sec/batch; 2h:37m:17s remains)
INFO - root - 2019-11-06 18:04:27.939573: step 2650, total loss = 3.85, predict loss = 1.14 (60.5 examples/sec; 0.066 sec/batch; 2h:42m:18s remains)
INFO - root - 2019-11-06 18:04:28.589489: step 2660, total loss = 5.01, predict loss = 1.49 (93.3 examples/sec; 0.043 sec/batch; 1h:45m:20s remains)
INFO - root - 2019-11-06 18:04:29.022050: step 2670, total loss = 4.01, predict loss = 1.19 (100.7 examples/sec; 0.040 sec/batch; 1h:37m:31s remains)
INFO - root - 2019-11-06 18:04:29.474055: step 2680, total loss = 5.54, predict loss = 1.59 (97.3 examples/sec; 0.041 sec/batch; 1h:40m:54s remains)
INFO - root - 2019-11-06 18:04:30.729535: step 2690, total loss = 4.54, predict loss = 1.25 (71.1 examples/sec; 0.056 sec/batch; 2h:18m:09s remains)
INFO - root - 2019-11-06 18:04:31.460324: step 2700, total loss = 4.29, predict loss = 1.28 (57.2 examples/sec; 0.070 sec/batch; 2h:51m:39s remains)
INFO - root - 2019-11-06 18:04:32.172946: step 2710, total loss = 5.79, predict loss = 1.69 (57.9 examples/sec; 0.069 sec/batch; 2h:49m:29s remains)
INFO - root - 2019-11-06 18:04:32.955053: step 2720, total loss = 3.45, predict loss = 1.08 (56.4 examples/sec; 0.071 sec/batch; 2h:54m:05s remains)
INFO - root - 2019-11-06 18:04:33.731602: step 2730, total loss = 4.33, predict loss = 1.28 (64.6 examples/sec; 0.062 sec/batch; 2h:31m:55s remains)
INFO - root - 2019-11-06 18:04:34.300700: step 2740, total loss = 4.84, predict loss = 1.32 (95.2 examples/sec; 0.042 sec/batch; 1h:43m:08s remains)
INFO - root - 2019-11-06 18:04:34.754950: step 2750, total loss = 3.50, predict loss = 1.01 (95.8 examples/sec; 0.042 sec/batch; 1h:42m:28s remains)
INFO - root - 2019-11-06 18:04:35.908830: step 2760, total loss = 2.88, predict loss = 0.88 (67.8 examples/sec; 0.059 sec/batch; 2h:24m:45s remains)
INFO - root - 2019-11-06 18:04:36.641406: step 2770, total loss = 4.29, predict loss = 1.14 (57.0 examples/sec; 0.070 sec/batch; 2h:52m:13s remains)
INFO - root - 2019-11-06 18:04:37.431805: step 2780, total loss = 4.55, predict loss = 1.39 (50.9 examples/sec; 0.079 sec/batch; 3h:12m:39s remains)
INFO - root - 2019-11-06 18:04:38.188775: step 2790, total loss = 4.20, predict loss = 1.25 (57.2 examples/sec; 0.070 sec/batch; 2h:51m:31s remains)
INFO - root - 2019-11-06 18:04:38.952033: step 2800, total loss = 5.33, predict loss = 1.68 (57.1 examples/sec; 0.070 sec/batch; 2h:51m:57s remains)
INFO - root - 2019-11-06 18:04:39.638593: step 2810, total loss = 5.11, predict loss = 1.54 (87.8 examples/sec; 0.046 sec/batch; 1h:51m:46s remains)
INFO - root - 2019-11-06 18:04:40.081828: step 2820, total loss = 6.06, predict loss = 1.72 (94.6 examples/sec; 0.042 sec/batch; 1h:43m:43s remains)
INFO - root - 2019-11-06 18:04:40.534806: step 2830, total loss = 4.08, predict loss = 1.15 (97.5 examples/sec; 0.041 sec/batch; 1h:40m:40s remains)
INFO - root - 2019-11-06 18:04:41.806967: step 2840, total loss = 4.90, predict loss = 1.41 (61.7 examples/sec; 0.065 sec/batch; 2h:39m:00s remains)
INFO - root - 2019-11-06 18:04:42.563494: step 2850, total loss = 5.39, predict loss = 1.50 (55.1 examples/sec; 0.073 sec/batch; 2h:57m:52s remains)
INFO - root - 2019-11-06 18:04:43.319296: step 2860, total loss = 4.64, predict loss = 1.35 (60.1 examples/sec; 0.067 sec/batch; 2h:43m:10s remains)
INFO - root - 2019-11-06 18:04:44.093390: step 2870, total loss = 6.29, predict loss = 1.84 (56.0 examples/sec; 0.071 sec/batch; 2h:55m:05s remains)
INFO - root - 2019-11-06 18:04:44.865656: step 2880, total loss = 5.15, predict loss = 1.51 (69.3 examples/sec; 0.058 sec/batch; 2h:21m:28s remains)
INFO - root - 2019-11-06 18:04:45.407342: step 2890, total loss = 5.22, predict loss = 1.55 (96.2 examples/sec; 0.042 sec/batch; 1h:41m:57s remains)
INFO - root - 2019-11-06 18:04:45.845119: step 2900, total loss = 4.44, predict loss = 1.29 (94.5 examples/sec; 0.042 sec/batch; 1h:43m:45s remains)
INFO - root - 2019-11-06 18:04:47.023595: step 2910, total loss = 5.96, predict loss = 1.70 (70.5 examples/sec; 0.057 sec/batch; 2h:19m:11s remains)
INFO - root - 2019-11-06 18:04:47.722230: step 2920, total loss = 4.24, predict loss = 1.23 (56.1 examples/sec; 0.071 sec/batch; 2h:54m:51s remains)
INFO - root - 2019-11-06 18:04:48.452477: step 2930, total loss = 4.09, predict loss = 1.20 (59.1 examples/sec; 0.068 sec/batch; 2h:45m:53s remains)
INFO - root - 2019-11-06 18:04:49.219229: step 2940, total loss = 4.33, predict loss = 1.25 (55.1 examples/sec; 0.073 sec/batch; 2h:57m:57s remains)
INFO - root - 2019-11-06 18:04:49.981476: step 2950, total loss = 5.25, predict loss = 1.47 (56.8 examples/sec; 0.070 sec/batch; 2h:52m:35s remains)
INFO - root - 2019-11-06 18:04:50.551820: step 2960, total loss = 4.85, predict loss = 1.38 (94.3 examples/sec; 0.042 sec/batch; 1h:43m:57s remains)
INFO - root - 2019-11-06 18:04:51.026522: step 2970, total loss = 4.45, predict loss = 1.28 (90.5 examples/sec; 0.044 sec/batch; 1h:48m:21s remains)
INFO - root - 2019-11-06 18:04:51.489276: step 2980, total loss = 4.05, predict loss = 1.19 (93.1 examples/sec; 0.043 sec/batch; 1h:45m:16s remains)
INFO - root - 2019-11-06 18:04:52.888306: step 2990, total loss = 5.30, predict loss = 1.60 (65.3 examples/sec; 0.061 sec/batch; 2h:30m:01s remains)
INFO - root - 2019-11-06 18:04:53.653812: step 3000, total loss = 4.95, predict loss = 1.33 (54.6 examples/sec; 0.073 sec/batch; 2h:59m:32s remains)
INFO - root - 2019-11-06 18:04:54.388205: step 3010, total loss = 5.37, predict loss = 1.58 (56.7 examples/sec; 0.071 sec/batch; 2h:52m:52s remains)
INFO - root - 2019-11-06 18:04:55.144203: step 3020, total loss = 5.43, predict loss = 1.52 (50.4 examples/sec; 0.079 sec/batch; 3h:14m:27s remains)
INFO - root - 2019-11-06 18:04:55.870990: step 3030, total loss = 5.31, predict loss = 1.50 (77.2 examples/sec; 0.052 sec/batch; 2h:06m:55s remains)
INFO - root - 2019-11-06 18:04:56.372606: step 3040, total loss = 5.66, predict loss = 1.53 (95.1 examples/sec; 0.042 sec/batch; 1h:42m:58s remains)
INFO - root - 2019-11-06 18:04:56.861765: step 3050, total loss = 3.77, predict loss = 1.08 (96.8 examples/sec; 0.041 sec/batch; 1h:41m:12s remains)
INFO - root - 2019-11-06 18:04:58.046890: step 3060, total loss = 6.29, predict loss = 1.78 (70.1 examples/sec; 0.057 sec/batch; 2h:19m:43s remains)
INFO - root - 2019-11-06 18:04:58.785481: step 3070, total loss = 5.50, predict loss = 1.60 (51.5 examples/sec; 0.078 sec/batch; 3h:10m:06s remains)
INFO - root - 2019-11-06 18:04:59.553682: step 3080, total loss = 5.49, predict loss = 1.68 (58.9 examples/sec; 0.068 sec/batch; 2h:46m:22s remains)
INFO - root - 2019-11-06 18:05:00.340357: step 3090, total loss = 5.20, predict loss = 1.50 (58.4 examples/sec; 0.069 sec/batch; 2h:47m:48s remains)
INFO - root - 2019-11-06 18:05:01.108125: step 3100, total loss = 4.56, predict loss = 1.30 (62.7 examples/sec; 0.064 sec/batch; 2h:36m:17s remains)
INFO - root - 2019-11-06 18:05:01.680041: step 3110, total loss = 4.45, predict loss = 1.24 (92.8 examples/sec; 0.043 sec/batch; 1h:45m:28s remains)
INFO - root - 2019-11-06 18:05:02.139793: step 3120, total loss = 4.39, predict loss = 1.19 (90.9 examples/sec; 0.044 sec/batch; 1h:47m:42s remains)
INFO - root - 2019-11-06 18:05:02.588626: step 3130, total loss = 6.43, predict loss = 1.81 (116.5 examples/sec; 0.034 sec/batch; 1h:24m:03s remains)
INFO - root - 2019-11-06 18:05:03.942403: step 3140, total loss = 3.84, predict loss = 1.12 (60.0 examples/sec; 0.067 sec/batch; 2h:43m:18s remains)
INFO - root - 2019-11-06 18:05:04.686640: step 3150, total loss = 5.84, predict loss = 1.70 (57.0 examples/sec; 0.070 sec/batch; 2h:51m:44s remains)
INFO - root - 2019-11-06 18:05:05.470478: step 3160, total loss = 4.74, predict loss = 1.36 (58.1 examples/sec; 0.069 sec/batch; 2h:48m:31s remains)
INFO - root - 2019-11-06 18:05:06.218912: step 3170, total loss = 5.43, predict loss = 1.54 (63.5 examples/sec; 0.063 sec/batch; 2h:34m:02s remains)
INFO - root - 2019-11-06 18:05:06.870916: step 3180, total loss = 4.25, predict loss = 1.17 (78.4 examples/sec; 0.051 sec/batch; 2h:04m:55s remains)
INFO - root - 2019-11-06 18:05:07.325782: step 3190, total loss = 5.71, predict loss = 1.62 (95.3 examples/sec; 0.042 sec/batch; 1h:42m:40s remains)
INFO - root - 2019-11-06 18:05:07.788154: step 3200, total loss = 6.19, predict loss = 1.80 (97.8 examples/sec; 0.041 sec/batch; 1h:40m:04s remains)
INFO - root - 2019-11-06 18:05:09.017320: step 3210, total loss = 5.47, predict loss = 1.61 (66.5 examples/sec; 0.060 sec/batch; 2h:27m:06s remains)
INFO - root - 2019-11-06 18:05:09.770423: step 3220, total loss = 5.73, predict loss = 1.55 (54.8 examples/sec; 0.073 sec/batch; 2h:58m:25s remains)
INFO - root - 2019-11-06 18:05:10.477493: step 3230, total loss = 4.91, predict loss = 1.38 (69.2 examples/sec; 0.058 sec/batch; 2h:21m:21s remains)
INFO - root - 2019-11-06 18:05:11.226860: step 3240, total loss = 5.82, predict loss = 1.59 (55.3 examples/sec; 0.072 sec/batch; 2h:56m:52s remains)
INFO - root - 2019-11-06 18:05:11.962605: step 3250, total loss = 4.00, predict loss = 1.19 (64.1 examples/sec; 0.062 sec/batch; 2h:32m:33s remains)
INFO - root - 2019-11-06 18:05:12.494819: step 3260, total loss = 4.64, predict loss = 1.36 (99.7 examples/sec; 0.040 sec/batch; 1h:38m:05s remains)
INFO - root - 2019-11-06 18:05:12.968072: step 3270, total loss = 4.45, predict loss = 1.32 (91.2 examples/sec; 0.044 sec/batch; 1h:47m:12s remains)
INFO - root - 2019-11-06 18:05:14.111862: step 3280, total loss = 3.84, predict loss = 1.13 (5.4 examples/sec; 0.737 sec/batch; 30h:02m:17s remains)
INFO - root - 2019-11-06 18:05:14.790010: step 3290, total loss = 5.03, predict loss = 1.49 (65.2 examples/sec; 0.061 sec/batch; 2h:30m:00s remains)
INFO - root - 2019-11-06 18:05:15.524406: step 3300, total loss = 5.28, predict loss = 1.55 (55.1 examples/sec; 0.073 sec/batch; 2h:57m:20s remains)
INFO - root - 2019-11-06 18:05:16.230284: step 3310, total loss = 4.72, predict loss = 1.40 (69.2 examples/sec; 0.058 sec/batch; 2h:21m:15s remains)
INFO - root - 2019-11-06 18:05:16.899344: step 3320, total loss = 4.93, predict loss = 1.41 (65.6 examples/sec; 0.061 sec/batch; 2h:28m:59s remains)
INFO - root - 2019-11-06 18:05:17.604971: step 3330, total loss = 4.07, predict loss = 1.13 (87.8 examples/sec; 0.046 sec/batch; 1h:51m:25s remains)
INFO - root - 2019-11-06 18:05:18.061956: step 3340, total loss = 4.42, predict loss = 1.27 (92.4 examples/sec; 0.043 sec/batch; 1h:45m:48s remains)
INFO - root - 2019-11-06 18:05:18.517466: step 3350, total loss = 6.63, predict loss = 1.81 (90.8 examples/sec; 0.044 sec/batch; 1h:47m:38s remains)
INFO - root - 2019-11-06 18:05:19.754581: step 3360, total loss = 6.19, predict loss = 1.73 (67.6 examples/sec; 0.059 sec/batch; 2h:24m:42s remains)
INFO - root - 2019-11-06 18:05:20.503464: step 3370, total loss = 4.26, predict loss = 1.21 (60.7 examples/sec; 0.066 sec/batch; 2h:40m:58s remains)
INFO - root - 2019-11-06 18:05:21.209695: step 3380, total loss = 5.22, predict loss = 1.55 (53.4 examples/sec; 0.075 sec/batch; 3h:03m:01s remains)
INFO - root - 2019-11-06 18:05:21.971427: step 3390, total loss = 3.53, predict loss = 0.99 (43.9 examples/sec; 0.091 sec/batch; 3h:42m:28s remains)
INFO - root - 2019-11-06 18:05:22.672250: step 3400, total loss = 5.92, predict loss = 1.75 (61.9 examples/sec; 0.065 sec/batch; 2h:37m:50s remains)
INFO - root - 2019-11-06 18:05:23.257458: step 3410, total loss = 3.62, predict loss = 1.04 (94.5 examples/sec; 0.042 sec/batch; 1h:43m:26s remains)
INFO - root - 2019-11-06 18:05:23.710786: step 3420, total loss = 6.68, predict loss = 1.98 (96.2 examples/sec; 0.042 sec/batch; 1h:41m:35s remains)
INFO - root - 2019-11-06 18:05:24.862947: step 3430, total loss = 3.72, predict loss = 1.13 (71.0 examples/sec; 0.056 sec/batch; 2h:17m:36s remains)
INFO - root - 2019-11-06 18:05:25.636047: step 3440, total loss = 6.12, predict loss = 1.73 (46.8 examples/sec; 0.085 sec/batch; 3h:28m:37s remains)
INFO - root - 2019-11-06 18:05:26.387398: step 3450, total loss = 3.45, predict loss = 1.02 (60.9 examples/sec; 0.066 sec/batch; 2h:40m:32s remains)
INFO - root - 2019-11-06 18:05:27.092263: step 3460, total loss = 6.85, predict loss = 2.02 (61.2 examples/sec; 0.065 sec/batch; 2h:39m:35s remains)
INFO - root - 2019-11-06 18:05:27.854798: step 3470, total loss = 3.01, predict loss = 0.83 (53.4 examples/sec; 0.075 sec/batch; 3h:03m:04s remains)
INFO - root - 2019-11-06 18:05:28.469325: step 3480, total loss = 5.54, predict loss = 1.54 (98.6 examples/sec; 0.041 sec/batch; 1h:39m:04s remains)
INFO - root - 2019-11-06 18:05:28.939264: step 3490, total loss = 5.61, predict loss = 1.63 (96.1 examples/sec; 0.042 sec/batch; 1h:41m:35s remains)
INFO - root - 2019-11-06 18:05:29.400485: step 3500, total loss = 4.99, predict loss = 1.47 (92.2 examples/sec; 0.043 sec/batch; 1h:45m:54s remains)
INFO - root - 2019-11-06 18:05:30.720688: step 3510, total loss = 4.57, predict loss = 1.31 (59.7 examples/sec; 0.067 sec/batch; 2h:43m:33s remains)
INFO - root - 2019-11-06 18:05:31.423565: step 3520, total loss = 6.52, predict loss = 1.91 (64.9 examples/sec; 0.062 sec/batch; 2h:30m:30s remains)
INFO - root - 2019-11-06 18:05:32.138596: step 3530, total loss = 5.01, predict loss = 1.53 (65.5 examples/sec; 0.061 sec/batch; 2h:29m:11s remains)
INFO - root - 2019-11-06 18:05:32.868912: step 3540, total loss = 5.28, predict loss = 1.56 (59.3 examples/sec; 0.067 sec/batch; 2h:44m:40s remains)
INFO - root - 2019-11-06 18:05:33.558529: step 3550, total loss = 4.41, predict loss = 1.25 (77.4 examples/sec; 0.052 sec/batch; 2h:06m:09s remains)
INFO - root - 2019-11-06 18:05:34.048400: step 3560, total loss = 4.46, predict loss = 1.28 (101.3 examples/sec; 0.040 sec/batch; 1h:36m:24s remains)
INFO - root - 2019-11-06 18:05:34.519760: step 3570, total loss = 5.70, predict loss = 1.69 (97.0 examples/sec; 0.041 sec/batch; 1h:40m:35s remains)
INFO - root - 2019-11-06 18:05:35.702124: step 3580, total loss = 6.75, predict loss = 2.03 (67.0 examples/sec; 0.060 sec/batch; 2h:25m:37s remains)
INFO - root - 2019-11-06 18:05:36.404592: step 3590, total loss = 4.88, predict loss = 1.39 (61.0 examples/sec; 0.066 sec/batch; 2h:40m:04s remains)
INFO - root - 2019-11-06 18:05:37.118992: step 3600, total loss = 5.54, predict loss = 1.58 (69.2 examples/sec; 0.058 sec/batch; 2h:21m:04s remains)
INFO - root - 2019-11-06 18:05:37.886601: step 3610, total loss = 5.20, predict loss = 1.47 (59.8 examples/sec; 0.067 sec/batch; 2h:43m:11s remains)
INFO - root - 2019-11-06 18:05:38.639088: step 3620, total loss = 5.09, predict loss = 1.46 (52.5 examples/sec; 0.076 sec/batch; 3h:05m:52s remains)
INFO - root - 2019-11-06 18:05:39.260491: step 3630, total loss = 6.01, predict loss = 1.69 (95.9 examples/sec; 0.042 sec/batch; 1h:41m:47s remains)
INFO - root - 2019-11-06 18:05:39.721052: step 3640, total loss = 5.66, predict loss = 1.55 (90.9 examples/sec; 0.044 sec/batch; 1h:47m:22s remains)
INFO - root - 2019-11-06 18:05:40.226595: step 3650, total loss = 4.76, predict loss = 1.33 (89.3 examples/sec; 0.045 sec/batch; 1h:49m:15s remains)
INFO - root - 2019-11-06 18:05:41.527271: step 3660, total loss = 6.64, predict loss = 1.84 (60.7 examples/sec; 0.066 sec/batch; 2h:40m:46s remains)
INFO - root - 2019-11-06 18:05:42.355625: step 3670, total loss = 4.35, predict loss = 1.24 (56.5 examples/sec; 0.071 sec/batch; 2h:52m:45s remains)
INFO - root - 2019-11-06 18:05:43.101705: step 3680, total loss = 6.03, predict loss = 1.80 (53.7 examples/sec; 0.074 sec/batch; 3h:01m:32s remains)
INFO - root - 2019-11-06 18:05:43.866735: step 3690, total loss = 4.31, predict loss = 1.20 (57.8 examples/sec; 0.069 sec/batch; 2h:48m:37s remains)
INFO - root - 2019-11-06 18:05:44.616994: step 3700, total loss = 6.30, predict loss = 1.79 (62.0 examples/sec; 0.065 sec/batch; 2h:37m:21s remains)
INFO - root - 2019-11-06 18:05:45.127023: step 3710, total loss = 5.61, predict loss = 1.62 (97.2 examples/sec; 0.041 sec/batch; 1h:40m:22s remains)
INFO - root - 2019-11-06 18:05:45.579308: step 3720, total loss = 5.02, predict loss = 1.44 (87.4 examples/sec; 0.046 sec/batch; 1h:51m:32s remains)
INFO - root - 2019-11-06 18:05:46.795477: step 3730, total loss = 4.09, predict loss = 1.20 (75.1 examples/sec; 0.053 sec/batch; 2h:09m:49s remains)
INFO - root - 2019-11-06 18:05:47.583955: step 3740, total loss = 4.21, predict loss = 1.22 (42.1 examples/sec; 0.095 sec/batch; 3h:51m:42s remains)
INFO - root - 2019-11-06 18:05:48.342035: step 3750, total loss = 5.13, predict loss = 1.49 (59.2 examples/sec; 0.068 sec/batch; 2h:44m:36s remains)
INFO - root - 2019-11-06 18:05:49.151336: step 3760, total loss = 4.95, predict loss = 1.41 (59.3 examples/sec; 0.067 sec/batch; 2h:44m:16s remains)
INFO - root - 2019-11-06 18:05:49.915417: step 3770, total loss = 5.82, predict loss = 1.64 (56.6 examples/sec; 0.071 sec/batch; 2h:52m:05s remains)
INFO - root - 2019-11-06 18:05:50.519080: step 3780, total loss = 4.98, predict loss = 1.40 (98.7 examples/sec; 0.041 sec/batch; 1h:38m:45s remains)
INFO - root - 2019-11-06 18:05:50.978213: step 3790, total loss = 4.90, predict loss = 1.41 (96.8 examples/sec; 0.041 sec/batch; 1h:40m:40s remains)
INFO - root - 2019-11-06 18:05:51.439768: step 3800, total loss = 5.64, predict loss = 1.58 (95.7 examples/sec; 0.042 sec/batch; 1h:41m:47s remains)
INFO - root - 2019-11-06 18:05:52.830579: step 3810, total loss = 4.99, predict loss = 1.47 (56.0 examples/sec; 0.071 sec/batch; 2h:54m:04s remains)
INFO - root - 2019-11-06 18:05:53.554534: step 3820, total loss = 5.57, predict loss = 1.61 (60.0 examples/sec; 0.067 sec/batch; 2h:42m:18s remains)
INFO - root - 2019-11-06 18:05:54.283526: step 3830, total loss = 4.81, predict loss = 1.47 (67.0 examples/sec; 0.060 sec/batch; 2h:25m:29s remains)
INFO - root - 2019-11-06 18:05:55.051739: step 3840, total loss = 5.48, predict loss = 1.60 (56.6 examples/sec; 0.071 sec/batch; 2h:52m:03s remains)
INFO - root - 2019-11-06 18:05:55.762926: step 3850, total loss = 3.87, predict loss = 1.09 (68.0 examples/sec; 0.059 sec/batch; 2h:23m:14s remains)
INFO - root - 2019-11-06 18:05:56.261150: step 3860, total loss = 4.28, predict loss = 1.26 (96.0 examples/sec; 0.042 sec/batch; 1h:41m:29s remains)
INFO - root - 2019-11-06 18:05:56.717930: step 3870, total loss = 4.65, predict loss = 1.36 (94.7 examples/sec; 0.042 sec/batch; 1h:42m:53s remains)
INFO - root - 2019-11-06 18:05:57.918018: step 3880, total loss = 5.72, predict loss = 1.71 (71.4 examples/sec; 0.056 sec/batch; 2h:16m:30s remains)
INFO - root - 2019-11-06 18:05:58.644655: step 3890, total loss = 5.74, predict loss = 1.61 (67.2 examples/sec; 0.060 sec/batch; 2h:25m:03s remains)
INFO - root - 2019-11-06 18:05:59.406991: step 3900, total loss = 5.29, predict loss = 1.49 (54.9 examples/sec; 0.073 sec/batch; 2h:57m:24s remains)
INFO - root - 2019-11-06 18:06:00.190756: step 3910, total loss = 4.93, predict loss = 1.44 (49.1 examples/sec; 0.082 sec/batch; 3h:18m:26s remains)
INFO - root - 2019-11-06 18:06:00.903793: step 3920, total loss = 4.18, predict loss = 1.22 (66.0 examples/sec; 0.061 sec/batch; 2h:27m:34s remains)
INFO - root - 2019-11-06 18:06:01.460897: step 3930, total loss = 5.45, predict loss = 1.56 (99.9 examples/sec; 0.040 sec/batch; 1h:37m:31s remains)
INFO - root - 2019-11-06 18:06:01.914031: step 3940, total loss = 5.57, predict loss = 1.53 (99.2 examples/sec; 0.040 sec/batch; 1h:38m:08s remains)
INFO - root - 2019-11-06 18:06:02.365229: step 3950, total loss = 2.65, predict loss = 0.76 (124.8 examples/sec; 0.032 sec/batch; 1h:18m:02s remains)
INFO - root - 2019-11-06 18:06:03.738820: step 3960, total loss = 5.95, predict loss = 1.69 (57.2 examples/sec; 0.070 sec/batch; 2h:50m:08s remains)
INFO - root - 2019-11-06 18:06:04.487663: step 3970, total loss = 5.30, predict loss = 1.64 (59.7 examples/sec; 0.067 sec/batch; 2h:43m:08s remains)
INFO - root - 2019-11-06 18:06:05.211911: step 3980, total loss = 4.44, predict loss = 1.26 (61.9 examples/sec; 0.065 sec/batch; 2h:37m:22s remains)
INFO - root - 2019-11-06 18:06:06.018411: step 3990, total loss = 5.87, predict loss = 1.67 (56.3 examples/sec; 0.071 sec/batch; 2h:52m:53s remains)
INFO - root - 2019-11-06 18:06:06.728442: step 4000, total loss = 5.10, predict loss = 1.43 (77.3 examples/sec; 0.052 sec/batch; 2h:05m:53s remains)
INFO - root - 2019-11-06 18:06:07.235813: step 4010, total loss = 5.23, predict loss = 1.54 (93.7 examples/sec; 0.043 sec/batch; 1h:43m:51s remains)
INFO - root - 2019-11-06 18:06:07.684561: step 4020, total loss = 4.44, predict loss = 1.35 (91.7 examples/sec; 0.044 sec/batch; 1h:46m:08s remains)
INFO - root - 2019-11-06 18:06:08.895638: step 4030, total loss = 5.11, predict loss = 1.48 (66.2 examples/sec; 0.060 sec/batch; 2h:26m:57s remains)
INFO - root - 2019-11-06 18:06:09.606025: step 4040, total loss = 5.01, predict loss = 1.47 (63.2 examples/sec; 0.063 sec/batch; 2h:33m:58s remains)
INFO - root - 2019-11-06 18:06:10.356002: step 4050, total loss = 6.55, predict loss = 1.91 (56.2 examples/sec; 0.071 sec/batch; 2h:53m:10s remains)
INFO - root - 2019-11-06 18:06:11.081875: step 4060, total loss = 3.24, predict loss = 0.97 (61.8 examples/sec; 0.065 sec/batch; 2h:37m:26s remains)
INFO - root - 2019-11-06 18:06:11.844113: step 4070, total loss = 4.40, predict loss = 1.22 (69.2 examples/sec; 0.058 sec/batch; 2h:20m:40s remains)
INFO - root - 2019-11-06 18:06:12.414904: step 4080, total loss = 6.26, predict loss = 1.78 (100.6 examples/sec; 0.040 sec/batch; 1h:36m:39s remains)
INFO - root - 2019-11-06 18:06:12.900331: step 4090, total loss = 5.07, predict loss = 1.45 (89.3 examples/sec; 0.045 sec/batch; 1h:48m:55s remains)
INFO - root - 2019-11-06 18:06:14.025190: step 4100, total loss = 6.14, predict loss = 1.82 (5.7 examples/sec; 0.696 sec/batch; 28h:12m:27s remains)
INFO - root - 2019-11-06 18:06:14.685142: step 4110, total loss = 5.61, predict loss = 1.51 (59.6 examples/sec; 0.067 sec/batch; 2h:43m:16s remains)
INFO - root - 2019-11-06 18:06:15.469024: step 4120, total loss = 4.93, predict loss = 1.38 (58.2 examples/sec; 0.069 sec/batch; 2h:47m:03s remains)
INFO - root - 2019-11-06 18:06:16.220941: step 4130, total loss = 4.48, predict loss = 1.35 (62.0 examples/sec; 0.064 sec/batch; 2h:36m:43s remains)
INFO - root - 2019-11-06 18:06:16.971101: step 4140, total loss = 3.72, predict loss = 1.15 (55.3 examples/sec; 0.072 sec/batch; 2h:55m:46s remains)
INFO - root - 2019-11-06 18:06:17.694855: step 4150, total loss = 3.83, predict loss = 1.13 (81.3 examples/sec; 0.049 sec/batch; 1h:59m:35s remains)
INFO - root - 2019-11-06 18:06:18.149078: step 4160, total loss = 4.03, predict loss = 1.13 (103.2 examples/sec; 0.039 sec/batch; 1h:34m:13s remains)
INFO - root - 2019-11-06 18:06:18.628239: step 4170, total loss = 5.47, predict loss = 1.65 (98.1 examples/sec; 0.041 sec/batch; 1h:39m:08s remains)
INFO - root - 2019-11-06 18:06:19.877319: step 4180, total loss = 5.26, predict loss = 1.51 (61.6 examples/sec; 0.065 sec/batch; 2h:37m:48s remains)
INFO - root - 2019-11-06 18:06:20.626736: step 4190, total loss = 3.36, predict loss = 1.02 (57.9 examples/sec; 0.069 sec/batch; 2h:47m:56s remains)
INFO - root - 2019-11-06 18:06:21.373117: step 4200, total loss = 6.56, predict loss = 1.83 (59.1 examples/sec; 0.068 sec/batch; 2h:44m:29s remains)
INFO - root - 2019-11-06 18:06:22.189232: step 4210, total loss = 4.53, predict loss = 1.44 (60.7 examples/sec; 0.066 sec/batch; 2h:40m:05s remains)
INFO - root - 2019-11-06 18:06:22.851936: step 4220, total loss = 5.13, predict loss = 1.50 (72.5 examples/sec; 0.055 sec/batch; 2h:14m:07s remains)
INFO - root - 2019-11-06 18:06:23.375839: step 4230, total loss = 4.96, predict loss = 1.42 (85.7 examples/sec; 0.047 sec/batch; 1h:53m:24s remains)
INFO - root - 2019-11-06 18:06:23.850245: step 4240, total loss = 4.58, predict loss = 1.31 (92.1 examples/sec; 0.043 sec/batch; 1h:45m:27s remains)
INFO - root - 2019-11-06 18:06:25.044100: step 4250, total loss = 5.53, predict loss = 1.58 (65.5 examples/sec; 0.061 sec/batch; 2h:28m:18s remains)
INFO - root - 2019-11-06 18:06:25.748690: step 4260, total loss = 4.26, predict loss = 1.20 (57.5 examples/sec; 0.070 sec/batch; 2h:48m:56s remains)
INFO - root - 2019-11-06 18:06:26.509055: step 4270, total loss = 4.59, predict loss = 1.35 (50.6 examples/sec; 0.079 sec/batch; 3h:12m:02s remains)
INFO - root - 2019-11-06 18:06:27.246801: step 4280, total loss = 4.10, predict loss = 1.10 (65.9 examples/sec; 0.061 sec/batch; 2h:27m:19s remains)
INFO - root - 2019-11-06 18:06:27.972158: step 4290, total loss = 4.87, predict loss = 1.40 (73.2 examples/sec; 0.055 sec/batch; 2h:12m:45s remains)
INFO - root - 2019-11-06 18:06:28.591083: step 4300, total loss = 6.60, predict loss = 1.91 (94.5 examples/sec; 0.042 sec/batch; 1h:42m:46s remains)
INFO - root - 2019-11-06 18:06:29.024512: step 4310, total loss = 4.41, predict loss = 1.30 (102.7 examples/sec; 0.039 sec/batch; 1h:34m:33s remains)
INFO - root - 2019-11-06 18:06:29.471606: step 4320, total loss = 6.40, predict loss = 1.80 (97.1 examples/sec; 0.041 sec/batch; 1h:40m:02s remains)
INFO - root - 2019-11-06 18:06:30.761153: step 4330, total loss = 4.76, predict loss = 1.40 (60.7 examples/sec; 0.066 sec/batch; 2h:40m:01s remains)
INFO - root - 2019-11-06 18:06:31.512602: step 4340, total loss = 3.99, predict loss = 1.16 (57.2 examples/sec; 0.070 sec/batch; 2h:49m:44s remains)
INFO - root - 2019-11-06 18:06:32.314332: step 4350, total loss = 5.61, predict loss = 1.62 (53.9 examples/sec; 0.074 sec/batch; 3h:00m:04s remains)
INFO - root - 2019-11-06 18:06:33.078313: step 4360, total loss = 5.54, predict loss = 1.57 (53.0 examples/sec; 0.075 sec/batch; 3h:03m:06s remains)
INFO - root - 2019-11-06 18:06:33.828689: step 4370, total loss = 5.11, predict loss = 1.45 (68.2 examples/sec; 0.059 sec/batch; 2h:22m:24s remains)
INFO - root - 2019-11-06 18:06:34.373002: step 4380, total loss = 4.73, predict loss = 1.31 (94.1 examples/sec; 0.043 sec/batch; 1h:43m:09s remains)
INFO - root - 2019-11-06 18:06:34.848748: step 4390, total loss = 6.20, predict loss = 1.66 (88.4 examples/sec; 0.045 sec/batch; 1h:49m:48s remains)
INFO - root - 2019-11-06 18:06:36.043508: step 4400, total loss = 3.55, predict loss = 0.99 (64.2 examples/sec; 0.062 sec/batch; 2h:31m:14s remains)
INFO - root - 2019-11-06 18:06:36.781986: step 4410, total loss = 4.59, predict loss = 1.33 (57.2 examples/sec; 0.070 sec/batch; 2h:49m:33s remains)
INFO - root - 2019-11-06 18:06:37.517251: step 4420, total loss = 5.29, predict loss = 1.48 (64.0 examples/sec; 0.063 sec/batch; 2h:31m:43s remains)
INFO - root - 2019-11-06 18:06:38.273534: step 4430, total loss = 5.34, predict loss = 1.49 (52.2 examples/sec; 0.077 sec/batch; 3h:05m:48s remains)
INFO - root - 2019-11-06 18:06:39.090339: step 4440, total loss = 4.87, predict loss = 1.47 (53.1 examples/sec; 0.075 sec/batch; 3h:02m:52s remains)
INFO - root - 2019-11-06 18:06:39.739487: step 4450, total loss = 6.01, predict loss = 1.78 (95.9 examples/sec; 0.042 sec/batch; 1h:41m:10s remains)
INFO - root - 2019-11-06 18:06:40.203103: step 4460, total loss = 3.02, predict loss = 0.87 (103.3 examples/sec; 0.039 sec/batch; 1h:33m:55s remains)
INFO - root - 2019-11-06 18:06:40.637899: step 4470, total loss = 5.13, predict loss = 1.46 (103.7 examples/sec; 0.039 sec/batch; 1h:33m:30s remains)
INFO - root - 2019-11-06 18:06:41.988544: step 4480, total loss = 4.92, predict loss = 1.45 (65.6 examples/sec; 0.061 sec/batch; 2h:27m:46s remains)
INFO - root - 2019-11-06 18:06:42.717794: step 4490, total loss = 6.42, predict loss = 1.84 (63.3 examples/sec; 0.063 sec/batch; 2h:33m:11s remains)
INFO - root - 2019-11-06 18:06:43.483875: step 4500, total loss = 4.80, predict loss = 1.33 (54.3 examples/sec; 0.074 sec/batch; 2h:58m:36s remains)
INFO - root - 2019-11-06 18:06:44.324485: step 4510, total loss = 5.43, predict loss = 1.51 (58.2 examples/sec; 0.069 sec/batch; 2h:46m:34s remains)
INFO - root - 2019-11-06 18:06:45.037090: step 4520, total loss = 4.28, predict loss = 1.23 (65.6 examples/sec; 0.061 sec/batch; 2h:27m:47s remains)
INFO - root - 2019-11-06 18:06:45.558551: step 4530, total loss = 3.91, predict loss = 1.09 (92.7 examples/sec; 0.043 sec/batch; 1h:44m:34s remains)
INFO - root - 2019-11-06 18:06:46.006419: step 4540, total loss = 2.92, predict loss = 0.80 (96.1 examples/sec; 0.042 sec/batch; 1h:40m:53s remains)
INFO - root - 2019-11-06 18:06:47.196909: step 4550, total loss = 5.82, predict loss = 1.64 (65.7 examples/sec; 0.061 sec/batch; 2h:27m:32s remains)
INFO - root - 2019-11-06 18:06:47.889945: step 4560, total loss = 4.38, predict loss = 1.32 (62.0 examples/sec; 0.065 sec/batch; 2h:36m:21s remains)
INFO - root - 2019-11-06 18:06:48.678725: step 4570, total loss = 2.38, predict loss = 0.69 (61.1 examples/sec; 0.065 sec/batch; 2h:38m:34s remains)
INFO - root - 2019-11-06 18:06:49.420189: step 4580, total loss = 6.49, predict loss = 1.82 (59.5 examples/sec; 0.067 sec/batch; 2h:42m:51s remains)
INFO - root - 2019-11-06 18:06:50.185707: step 4590, total loss = 3.83, predict loss = 1.17 (60.8 examples/sec; 0.066 sec/batch; 2h:39m:22s remains)
INFO - root - 2019-11-06 18:06:50.770327: step 4600, total loss = 4.98, predict loss = 1.41 (94.5 examples/sec; 0.042 sec/batch; 1h:42m:35s remains)
INFO - root - 2019-11-06 18:06:51.256244: step 4610, total loss = 4.76, predict loss = 1.39 (102.4 examples/sec; 0.039 sec/batch; 1h:34m:41s remains)
INFO - root - 2019-11-06 18:06:51.708136: step 4620, total loss = 5.50, predict loss = 1.59 (96.5 examples/sec; 0.041 sec/batch; 1h:40m:26s remains)
INFO - root - 2019-11-06 18:06:53.074408: step 4630, total loss = 5.63, predict loss = 1.58 (62.0 examples/sec; 0.065 sec/batch; 2h:36m:23s remains)
INFO - root - 2019-11-06 18:06:53.763466: step 4640, total loss = 4.66, predict loss = 1.33 (62.1 examples/sec; 0.064 sec/batch; 2h:36m:00s remains)
INFO - root - 2019-11-06 18:06:54.522092: step 4650, total loss = 4.73, predict loss = 1.41 (56.5 examples/sec; 0.071 sec/batch; 2h:51m:21s remains)
INFO - root - 2019-11-06 18:06:55.310374: step 4660, total loss = 4.63, predict loss = 1.36 (52.1 examples/sec; 0.077 sec/batch; 3h:05m:54s remains)
INFO - root - 2019-11-06 18:06:56.042289: step 4670, total loss = 5.47, predict loss = 1.51 (64.5 examples/sec; 0.062 sec/batch; 2h:30m:10s remains)
INFO - root - 2019-11-06 18:06:56.545659: step 4680, total loss = 4.81, predict loss = 1.42 (92.5 examples/sec; 0.043 sec/batch; 1h:44m:42s remains)
INFO - root - 2019-11-06 18:06:57.028409: step 4690, total loss = 5.65, predict loss = 1.72 (97.6 examples/sec; 0.041 sec/batch; 1h:39m:13s remains)
INFO - root - 2019-11-06 18:06:58.211769: step 4700, total loss = 5.45, predict loss = 1.55 (59.2 examples/sec; 0.068 sec/batch; 2h:43m:41s remains)
INFO - root - 2019-11-06 18:06:58.941875: step 4710, total loss = 5.88, predict loss = 1.71 (61.2 examples/sec; 0.065 sec/batch; 2h:38m:21s remains)
INFO - root - 2019-11-06 18:06:59.704359: step 4720, total loss = 4.60, predict loss = 1.21 (57.6 examples/sec; 0.069 sec/batch; 2h:48m:03s remains)
INFO - root - 2019-11-06 18:07:00.453074: step 4730, total loss = 4.86, predict loss = 1.38 (49.8 examples/sec; 0.080 sec/batch; 3h:14m:20s remains)
INFO - root - 2019-11-06 18:07:01.256009: step 4740, total loss = 5.44, predict loss = 1.58 (57.5 examples/sec; 0.070 sec/batch; 2h:48m:27s remains)
INFO - root - 2019-11-06 18:07:01.788663: step 4750, total loss = 4.21, predict loss = 1.18 (103.0 examples/sec; 0.039 sec/batch; 1h:34m:01s remains)
INFO - root - 2019-11-06 18:07:02.223693: step 4760, total loss = 4.82, predict loss = 1.41 (96.7 examples/sec; 0.041 sec/batch; 1h:40m:10s remains)
INFO - root - 2019-11-06 18:07:02.687002: step 4770, total loss = 4.89, predict loss = 1.49 (127.5 examples/sec; 0.031 sec/batch; 1h:15m:55s remains)
INFO - root - 2019-11-06 18:07:04.076605: step 4780, total loss = 3.62, predict loss = 1.09 (61.0 examples/sec; 0.066 sec/batch; 2h:38m:46s remains)
INFO - root - 2019-11-06 18:07:04.812022: step 4790, total loss = 5.03, predict loss = 1.45 (60.2 examples/sec; 0.066 sec/batch; 2h:40m:47s remains)
INFO - root - 2019-11-06 18:07:05.525936: step 4800, total loss = 4.16, predict loss = 1.24 (48.0 examples/sec; 0.083 sec/batch; 3h:21m:29s remains)
INFO - root - 2019-11-06 18:07:06.271832: step 4810, total loss = 5.57, predict loss = 1.52 (68.0 examples/sec; 0.059 sec/batch; 2h:22m:18s remains)
INFO - root - 2019-11-06 18:07:06.923204: step 4820, total loss = 5.34, predict loss = 1.57 (79.6 examples/sec; 0.050 sec/batch; 2h:01m:34s remains)
INFO - root - 2019-11-06 18:07:07.390767: step 4830, total loss = 6.53, predict loss = 1.84 (97.8 examples/sec; 0.041 sec/batch; 1h:38m:55s remains)
INFO - root - 2019-11-06 18:07:07.850671: step 4840, total loss = 4.57, predict loss = 1.32 (92.8 examples/sec; 0.043 sec/batch; 1h:44m:19s remains)
INFO - root - 2019-11-06 18:07:09.077622: step 4850, total loss = 4.78, predict loss = 1.41 (59.3 examples/sec; 0.067 sec/batch; 2h:43m:02s remains)
INFO - root - 2019-11-06 18:07:09.751782: step 4860, total loss = 3.74, predict loss = 1.13 (52.9 examples/sec; 0.076 sec/batch; 3h:03m:00s remains)
INFO - root - 2019-11-06 18:07:10.452102: step 4870, total loss = 5.78, predict loss = 1.64 (66.9 examples/sec; 0.060 sec/batch; 2h:24m:40s remains)
INFO - root - 2019-11-06 18:07:11.211965: step 4880, total loss = 5.62, predict loss = 1.64 (60.5 examples/sec; 0.066 sec/batch; 2h:39m:51s remains)
INFO - root - 2019-11-06 18:07:11.975457: step 4890, total loss = 5.80, predict loss = 1.58 (67.7 examples/sec; 0.059 sec/batch; 2h:22m:48s remains)
INFO - root - 2019-11-06 18:07:12.496796: step 4900, total loss = 4.17, predict loss = 1.20 (99.4 examples/sec; 0.040 sec/batch; 1h:37m:21s remains)
INFO - root - 2019-11-06 18:07:12.952093: step 4910, total loss = 3.54, predict loss = 1.01 (84.6 examples/sec; 0.047 sec/batch; 1h:54m:22s remains)
INFO - root - 2019-11-06 18:07:14.110662: step 4920, total loss = 5.59, predict loss = 1.57 (5.3 examples/sec; 0.758 sec/batch; 30h:33m:38s remains)
INFO - root - 2019-11-06 18:07:14.866072: step 4930, total loss = 5.08, predict loss = 1.44 (55.7 examples/sec; 0.072 sec/batch; 2h:53m:31s remains)
INFO - root - 2019-11-06 18:07:15.567623: step 4940, total loss = 5.07, predict loss = 1.52 (60.4 examples/sec; 0.066 sec/batch; 2h:40m:00s remains)
INFO - root - 2019-11-06 18:07:16.309149: step 4950, total loss = 5.15, predict loss = 1.52 (54.8 examples/sec; 0.073 sec/batch; 2h:56m:19s remains)
INFO - root - 2019-11-06 18:07:17.069442: step 4960, total loss = 4.58, predict loss = 1.39 (58.7 examples/sec; 0.068 sec/batch; 2h:44m:49s remains)
INFO - root - 2019-11-06 18:07:17.788751: step 4970, total loss = 5.10, predict loss = 1.43 (88.2 examples/sec; 0.045 sec/batch; 1h:49m:36s remains)
INFO - root - 2019-11-06 18:07:18.233312: step 4980, total loss = 5.62, predict loss = 1.64 (99.8 examples/sec; 0.040 sec/batch; 1h:36m:51s remains)
INFO - root - 2019-11-06 18:07:18.675299: step 4990, total loss = 4.03, predict loss = 1.16 (95.0 examples/sec; 0.042 sec/batch; 1h:41m:43s remains)
INFO - root - 2019-11-06 18:07:19.908275: step 5000, total loss = 4.79, predict loss = 1.34 (58.7 examples/sec; 0.068 sec/batch; 2h:44m:39s remains)
INFO - root - 2019-11-06 18:07:20.668798: step 5010, total loss = 4.84, predict loss = 1.44 (66.0 examples/sec; 0.061 sec/batch; 2h:26m:28s remains)
INFO - root - 2019-11-06 18:07:21.472912: step 5020, total loss = 5.93, predict loss = 1.71 (52.6 examples/sec; 0.076 sec/batch; 3h:03m:41s remains)
INFO - root - 2019-11-06 18:07:22.294303: step 5030, total loss = 5.06, predict loss = 1.52 (60.3 examples/sec; 0.066 sec/batch; 2h:40m:12s remains)
INFO - root - 2019-11-06 18:07:22.988442: step 5040, total loss = 4.77, predict loss = 1.37 (62.4 examples/sec; 0.064 sec/batch; 2h:34m:45s remains)
INFO - root - 2019-11-06 18:07:23.549407: step 5050, total loss = 4.71, predict loss = 1.33 (93.9 examples/sec; 0.043 sec/batch; 1h:42m:52s remains)
INFO - root - 2019-11-06 18:07:23.996476: step 5060, total loss = 6.73, predict loss = 1.93 (100.2 examples/sec; 0.040 sec/batch; 1h:36m:24s remains)
INFO - root - 2019-11-06 18:07:25.175094: step 5070, total loss = 4.97, predict loss = 1.49 (66.7 examples/sec; 0.060 sec/batch; 2h:24m:47s remains)
INFO - root - 2019-11-06 18:07:25.855182: step 5080, total loss = 3.61, predict loss = 1.06 (62.0 examples/sec; 0.065 sec/batch; 2h:35m:49s remains)
INFO - root - 2019-11-06 18:07:26.601408: step 5090, total loss = 5.16, predict loss = 1.54 (62.0 examples/sec; 0.065 sec/batch; 2h:35m:48s remains)
INFO - root - 2019-11-06 18:07:27.408228: step 5100, total loss = 5.60, predict loss = 1.61 (50.2 examples/sec; 0.080 sec/batch; 3h:12m:34s remains)
INFO - root - 2019-11-06 18:07:28.178502: step 5110, total loss = 5.08, predict loss = 1.49 (50.3 examples/sec; 0.080 sec/batch; 3h:12m:01s remains)
INFO - root - 2019-11-06 18:07:28.827255: step 5120, total loss = 6.04, predict loss = 1.76 (91.2 examples/sec; 0.044 sec/batch; 1h:45m:53s remains)
INFO - root - 2019-11-06 18:07:29.295098: step 5130, total loss = 5.73, predict loss = 1.55 (97.8 examples/sec; 0.041 sec/batch; 1h:38m:43s remains)
INFO - root - 2019-11-06 18:07:29.739358: step 5140, total loss = 5.87, predict loss = 1.68 (102.4 examples/sec; 0.039 sec/batch; 1h:34m:16s remains)
INFO - root - 2019-11-06 18:07:31.013906: step 5150, total loss = 4.98, predict loss = 1.42 (59.8 examples/sec; 0.067 sec/batch; 2h:41m:27s remains)
INFO - root - 2019-11-06 18:07:31.712941: step 5160, total loss = 5.35, predict loss = 1.45 (56.6 examples/sec; 0.071 sec/batch; 2h:50m:31s remains)
INFO - root - 2019-11-06 18:07:32.469582: step 5170, total loss = 6.08, predict loss = 1.67 (61.1 examples/sec; 0.065 sec/batch; 2h:38m:05s remains)
INFO - root - 2019-11-06 18:07:33.246416: step 5180, total loss = 4.30, predict loss = 1.17 (57.6 examples/sec; 0.069 sec/batch; 2h:47m:41s remains)
INFO - root - 2019-11-06 18:07:34.033065: step 5190, total loss = 5.84, predict loss = 1.75 (70.3 examples/sec; 0.057 sec/batch; 2h:17m:23s remains)
INFO - root - 2019-11-06 18:07:34.561748: step 5200, total loss = 4.68, predict loss = 1.35 (97.9 examples/sec; 0.041 sec/batch; 1h:38m:37s remains)
INFO - root - 2019-11-06 18:07:35.037630: step 5210, total loss = 4.63, predict loss = 1.29 (94.6 examples/sec; 0.042 sec/batch; 1h:42m:02s remains)
INFO - root - 2019-11-06 18:07:36.183407: step 5220, total loss = 4.26, predict loss = 1.23 (67.7 examples/sec; 0.059 sec/batch; 2h:22m:30s remains)
INFO - root - 2019-11-06 18:07:36.858349: step 5230, total loss = 3.20, predict loss = 0.92 (63.0 examples/sec; 0.063 sec/batch; 2h:33m:08s remains)
INFO - root - 2019-11-06 18:07:37.582848: step 5240, total loss = 3.77, predict loss = 1.03 (53.0 examples/sec; 0.076 sec/batch; 3h:02m:10s remains)
INFO - root - 2019-11-06 18:07:38.364812: step 5250, total loss = 3.68, predict loss = 1.08 (56.5 examples/sec; 0.071 sec/batch; 2h:50m:54s remains)
INFO - root - 2019-11-06 18:07:39.129243: step 5260, total loss = 5.14, predict loss = 1.50 (61.0 examples/sec; 0.066 sec/batch; 2h:38m:08s remains)
INFO - root - 2019-11-06 18:07:39.735523: step 5270, total loss = 3.36, predict loss = 0.99 (99.4 examples/sec; 0.040 sec/batch; 1h:37m:06s remains)
INFO - root - 2019-11-06 18:07:40.188108: step 5280, total loss = 5.90, predict loss = 1.73 (90.2 examples/sec; 0.044 sec/batch; 1h:46m:59s remains)
INFO - root - 2019-11-06 18:07:40.669025: step 5290, total loss = 5.70, predict loss = 1.59 (86.1 examples/sec; 0.046 sec/batch; 1h:52m:00s remains)
INFO - root - 2019-11-06 18:07:41.952354: step 5300, total loss = 3.71, predict loss = 1.11 (55.2 examples/sec; 0.073 sec/batch; 2h:54m:54s remains)
INFO - root - 2019-11-06 18:07:42.671140: step 5310, total loss = 4.12, predict loss = 1.23 (64.1 examples/sec; 0.062 sec/batch; 2h:30m:35s remains)
INFO - root - 2019-11-06 18:07:43.372117: step 5320, total loss = 3.33, predict loss = 1.01 (50.7 examples/sec; 0.079 sec/batch; 3h:10m:09s remains)
INFO - root - 2019-11-06 18:07:44.132284: step 5330, total loss = 4.97, predict loss = 1.41 (59.7 examples/sec; 0.067 sec/batch; 2h:41m:38s remains)
INFO - root - 2019-11-06 18:07:44.880462: step 5340, total loss = 4.34, predict loss = 1.24 (68.1 examples/sec; 0.059 sec/batch; 2h:21m:36s remains)
INFO - root - 2019-11-06 18:07:45.395574: step 5350, total loss = 5.07, predict loss = 1.46 (87.5 examples/sec; 0.046 sec/batch; 1h:50m:12s remains)
INFO - root - 2019-11-06 18:07:45.853980: step 5360, total loss = 4.43, predict loss = 1.25 (97.1 examples/sec; 0.041 sec/batch; 1h:39m:16s remains)
INFO - root - 2019-11-06 18:07:47.085388: step 5370, total loss = 4.87, predict loss = 1.52 (66.7 examples/sec; 0.060 sec/batch; 2h:24m:32s remains)
INFO - root - 2019-11-06 18:07:47.811636: step 5380, total loss = 4.11, predict loss = 1.17 (60.4 examples/sec; 0.066 sec/batch; 2h:39m:41s remains)
INFO - root - 2019-11-06 18:07:48.590690: step 5390, total loss = 3.69, predict loss = 1.13 (55.4 examples/sec; 0.072 sec/batch; 2h:54m:04s remains)
INFO - root - 2019-11-06 18:07:49.310038: step 5400, total loss = 3.94, predict loss = 1.18 (64.2 examples/sec; 0.062 sec/batch; 2h:30m:02s remains)
INFO - root - 2019-11-06 18:07:50.088290: step 5410, total loss = 3.14, predict loss = 0.94 (61.1 examples/sec; 0.065 sec/batch; 2h:37m:39s remains)
INFO - root - 2019-11-06 18:07:50.700105: step 5420, total loss = 5.56, predict loss = 1.60 (90.0 examples/sec; 0.044 sec/batch; 1h:47m:09s remains)
INFO - root - 2019-11-06 18:07:51.148309: step 5430, total loss = 4.99, predict loss = 1.43 (94.7 examples/sec; 0.042 sec/batch; 1h:41m:46s remains)
INFO - root - 2019-11-06 18:07:51.606203: step 5440, total loss = 5.79, predict loss = 1.55 (94.4 examples/sec; 0.042 sec/batch; 1h:42m:02s remains)
INFO - root - 2019-11-06 18:07:53.075430: step 5450, total loss = 5.24, predict loss = 1.52 (65.2 examples/sec; 0.061 sec/batch; 2h:27m:45s remains)
INFO - root - 2019-11-06 18:07:53.738829: step 5460, total loss = 5.89, predict loss = 1.66 (66.7 examples/sec; 0.060 sec/batch; 2h:24m:33s remains)
INFO - root - 2019-11-06 18:07:54.449777: step 5470, total loss = 6.42, predict loss = 1.87 (62.4 examples/sec; 0.064 sec/batch; 2h:34m:18s remains)
INFO - root - 2019-11-06 18:07:55.188391: step 5480, total loss = 4.10, predict loss = 1.18 (58.3 examples/sec; 0.069 sec/batch; 2h:45m:21s remains)
INFO - root - 2019-11-06 18:07:55.881660: step 5490, total loss = 4.79, predict loss = 1.37 (76.9 examples/sec; 0.052 sec/batch; 2h:05m:20s remains)
INFO - root - 2019-11-06 18:07:56.343181: step 5500, total loss = 4.65, predict loss = 1.31 (97.1 examples/sec; 0.041 sec/batch; 1h:39m:12s remains)
INFO - root - 2019-11-06 18:07:56.793245: step 5510, total loss = 5.66, predict loss = 1.61 (95.3 examples/sec; 0.042 sec/batch; 1h:41m:04s remains)
INFO - root - 2019-11-06 18:07:57.982834: step 5520, total loss = 4.84, predict loss = 1.43 (68.1 examples/sec; 0.059 sec/batch; 2h:21m:23s remains)
INFO - root - 2019-11-06 18:07:58.706240: step 5530, total loss = 4.67, predict loss = 1.39 (53.4 examples/sec; 0.075 sec/batch; 3h:00m:27s remains)
INFO - root - 2019-11-06 18:07:59.459958: step 5540, total loss = 4.76, predict loss = 1.39 (55.3 examples/sec; 0.072 sec/batch; 2h:54m:17s remains)
INFO - root - 2019-11-06 18:08:00.209987: step 5550, total loss = 5.03, predict loss = 1.42 (65.9 examples/sec; 0.061 sec/batch; 2h:26m:13s remains)
INFO - root - 2019-11-06 18:08:01.008973: step 5560, total loss = 4.56, predict loss = 1.26 (62.2 examples/sec; 0.064 sec/batch; 2h:34m:46s remains)
INFO - root - 2019-11-06 18:08:01.555645: step 5570, total loss = 4.44, predict loss = 1.28 (98.4 examples/sec; 0.041 sec/batch; 1h:37m:52s remains)
INFO - root - 2019-11-06 18:08:02.006494: step 5580, total loss = 4.14, predict loss = 1.16 (96.3 examples/sec; 0.042 sec/batch; 1h:39m:56s remains)
INFO - root - 2019-11-06 18:08:02.453692: step 5590, total loss = 6.71, predict loss = 1.93 (131.5 examples/sec; 0.030 sec/batch; 1h:13m:11s remains)
INFO - root - 2019-11-06 18:08:03.782127: step 5600, total loss = 5.85, predict loss = 1.71 (59.6 examples/sec; 0.067 sec/batch; 2h:41m:32s remains)
INFO - root - 2019-11-06 18:08:04.535911: step 5610, total loss = 4.99, predict loss = 1.38 (58.2 examples/sec; 0.069 sec/batch; 2h:45m:16s remains)
INFO - root - 2019-11-06 18:08:05.314103: step 5620, total loss = 4.96, predict loss = 1.35 (55.7 examples/sec; 0.072 sec/batch; 2h:52m:49s remains)
INFO - root - 2019-11-06 18:08:06.136122: step 5630, total loss = 4.09, predict loss = 1.09 (52.5 examples/sec; 0.076 sec/batch; 3h:03m:25s remains)
INFO - root - 2019-11-06 18:08:06.857899: step 5640, total loss = 5.80, predict loss = 1.67 (74.6 examples/sec; 0.054 sec/batch; 2h:09m:05s remains)
INFO - root - 2019-11-06 18:08:07.358149: step 5650, total loss = 4.05, predict loss = 1.13 (97.3 examples/sec; 0.041 sec/batch; 1h:38m:52s remains)
INFO - root - 2019-11-06 18:08:07.820970: step 5660, total loss = 5.31, predict loss = 1.53 (94.4 examples/sec; 0.042 sec/batch; 1h:41m:52s remains)
INFO - root - 2019-11-06 18:08:09.054429: step 5670, total loss = 6.05, predict loss = 1.82 (67.5 examples/sec; 0.059 sec/batch; 2h:22m:33s remains)
INFO - root - 2019-11-06 18:08:09.788157: step 5680, total loss = 4.41, predict loss = 1.33 (61.3 examples/sec; 0.065 sec/batch; 2h:37m:04s remains)
INFO - root - 2019-11-06 18:08:10.596636: step 5690, total loss = 4.57, predict loss = 1.27 (55.1 examples/sec; 0.073 sec/batch; 2h:54m:40s remains)
INFO - root - 2019-11-06 18:08:11.316050: step 5700, total loss = 6.13, predict loss = 1.75 (66.5 examples/sec; 0.060 sec/batch; 2h:24m:42s remains)
INFO - root - 2019-11-06 18:08:12.042407: step 5710, total loss = 4.33, predict loss = 1.34 (64.7 examples/sec; 0.062 sec/batch; 2h:28m:35s remains)
INFO - root - 2019-11-06 18:08:12.647560: step 5720, total loss = 6.76, predict loss = 1.87 (97.8 examples/sec; 0.041 sec/batch; 1h:38m:18s remains)
INFO - root - 2019-11-06 18:08:13.124479: step 5730, total loss = 3.44, predict loss = 1.06 (95.1 examples/sec; 0.042 sec/batch; 1h:41m:09s remains)
INFO - root - 2019-11-06 18:08:14.248641: step 5740, total loss = 4.97, predict loss = 1.41 (5.6 examples/sec; 0.718 sec/batch; 28h:45m:45s remains)
INFO - root - 2019-11-06 18:08:14.947256: step 5750, total loss = 4.92, predict loss = 1.35 (55.7 examples/sec; 0.072 sec/batch; 2h:52m:44s remains)
INFO - root - 2019-11-06 18:08:15.738909: step 5760, total loss = 6.54, predict loss = 1.79 (62.7 examples/sec; 0.064 sec/batch; 2h:33m:25s remains)
INFO - root - 2019-11-06 18:08:16.508930: step 5770, total loss = 5.95, predict loss = 1.71 (56.9 examples/sec; 0.070 sec/batch; 2h:48m:52s remains)
INFO - root - 2019-11-06 18:08:17.268821: step 5780, total loss = 4.73, predict loss = 1.39 (59.4 examples/sec; 0.067 sec/batch; 2h:41m:53s remains)
INFO - root - 2019-11-06 18:08:17.931216: step 5790, total loss = 5.12, predict loss = 1.46 (89.1 examples/sec; 0.045 sec/batch; 1h:47m:57s remains)
INFO - root - 2019-11-06 18:08:18.379494: step 5800, total loss = 4.95, predict loss = 1.46 (95.8 examples/sec; 0.042 sec/batch; 1h:40m:18s remains)
INFO - root - 2019-11-06 18:08:18.866040: step 5810, total loss = 4.67, predict loss = 1.30 (90.0 examples/sec; 0.044 sec/batch; 1h:46m:49s remains)
INFO - root - 2019-11-06 18:08:20.081352: step 5820, total loss = 4.67, predict loss = 1.47 (67.8 examples/sec; 0.059 sec/batch; 2h:21m:41s remains)
INFO - root - 2019-11-06 18:08:20.780600: step 5830, total loss = 4.41, predict loss = 1.31 (60.0 examples/sec; 0.067 sec/batch; 2h:40m:04s remains)
INFO - root - 2019-11-06 18:08:21.568671: step 5840, total loss = 4.29, predict loss = 1.24 (56.0 examples/sec; 0.071 sec/batch; 2h:51m:38s remains)
INFO - root - 2019-11-06 18:08:22.361085: step 5850, total loss = 4.41, predict loss = 1.30 (61.9 examples/sec; 0.065 sec/batch; 2h:35m:21s remains)
INFO - root - 2019-11-06 18:08:23.086163: step 5860, total loss = 4.26, predict loss = 1.23 (68.6 examples/sec; 0.058 sec/batch; 2h:19m:59s remains)
INFO - root - 2019-11-06 18:08:23.619274: step 5870, total loss = 6.10, predict loss = 1.75 (94.4 examples/sec; 0.042 sec/batch; 1h:41m:46s remains)
INFO - root - 2019-11-06 18:08:24.079659: step 5880, total loss = 5.75, predict loss = 1.58 (91.8 examples/sec; 0.044 sec/batch; 1h:44m:37s remains)
INFO - root - 2019-11-06 18:08:25.278555: step 5890, total loss = 4.02, predict loss = 1.17 (69.3 examples/sec; 0.058 sec/batch; 2h:18m:35s remains)
INFO - root - 2019-11-06 18:08:25.966852: step 5900, total loss = 4.70, predict loss = 1.42 (62.3 examples/sec; 0.064 sec/batch; 2h:34m:12s remains)
INFO - root - 2019-11-06 18:08:26.742162: step 5910, total loss = 5.48, predict loss = 1.60 (60.6 examples/sec; 0.066 sec/batch; 2h:38m:23s remains)
INFO - root - 2019-11-06 18:08:27.474272: step 5920, total loss = 5.62, predict loss = 1.60 (60.1 examples/sec; 0.067 sec/batch; 2h:39m:46s remains)
INFO - root - 2019-11-06 18:08:28.206974: step 5930, total loss = 5.13, predict loss = 1.46 (61.6 examples/sec; 0.065 sec/batch; 2h:36m:01s remains)
INFO - root - 2019-11-06 18:08:28.794773: step 5940, total loss = 4.64, predict loss = 1.31 (98.2 examples/sec; 0.041 sec/batch; 1h:37m:47s remains)
INFO - root - 2019-11-06 18:08:29.254538: step 5950, total loss = 5.49, predict loss = 1.56 (96.7 examples/sec; 0.041 sec/batch; 1h:39m:19s remains)
INFO - root - 2019-11-06 18:08:29.710772: step 5960, total loss = 5.83, predict loss = 1.65 (95.8 examples/sec; 0.042 sec/batch; 1h:40m:14s remains)
INFO - root - 2019-11-06 18:08:31.023848: step 5970, total loss = 6.05, predict loss = 1.70 (57.6 examples/sec; 0.069 sec/batch; 2h:46m:35s remains)
INFO - root - 2019-11-06 18:08:31.834606: step 5980, total loss = 5.21, predict loss = 1.52 (52.1 examples/sec; 0.077 sec/batch; 3h:04m:16s remains)
INFO - root - 2019-11-06 18:08:32.612928: step 5990, total loss = 5.32, predict loss = 1.52 (59.4 examples/sec; 0.067 sec/batch; 2h:41m:43s remains)
INFO - root - 2019-11-06 18:08:33.359156: step 6000, total loss = 2.31, predict loss = 0.74 (60.6 examples/sec; 0.066 sec/batch; 2h:38m:30s remains)
INFO - root - 2019-11-06 18:08:34.083704: step 6010, total loss = 4.57, predict loss = 1.35 (63.0 examples/sec; 0.063 sec/batch; 2h:32m:17s remains)
INFO - root - 2019-11-06 18:08:34.606157: step 6020, total loss = 5.02, predict loss = 1.51 (95.8 examples/sec; 0.042 sec/batch; 1h:40m:14s remains)
INFO - root - 2019-11-06 18:08:35.065518: step 6030, total loss = 6.16, predict loss = 1.75 (89.6 examples/sec; 0.045 sec/batch; 1h:47m:05s remains)
INFO - root - 2019-11-06 18:08:36.267530: step 6040, total loss = 4.47, predict loss = 1.29 (69.3 examples/sec; 0.058 sec/batch; 2h:18m:30s remains)
INFO - root - 2019-11-06 18:08:37.028568: step 6050, total loss = 3.85, predict loss = 1.11 (58.0 examples/sec; 0.069 sec/batch; 2h:45m:31s remains)
INFO - root - 2019-11-06 18:08:37.749797: step 6060, total loss = 3.91, predict loss = 1.14 (59.4 examples/sec; 0.067 sec/batch; 2h:41m:40s remains)
INFO - root - 2019-11-06 18:08:38.517224: step 6070, total loss = 6.62, predict loss = 1.96 (53.5 examples/sec; 0.075 sec/batch; 2h:59m:17s remains)
INFO - root - 2019-11-06 18:08:39.267934: step 6080, total loss = 5.14, predict loss = 1.50 (56.0 examples/sec; 0.071 sec/batch; 2h:51m:16s remains)
INFO - root - 2019-11-06 18:08:39.859366: step 6090, total loss = 3.82, predict loss = 1.12 (99.1 examples/sec; 0.040 sec/batch; 1h:36m:47s remains)
INFO - root - 2019-11-06 18:08:40.316060: step 6100, total loss = 5.27, predict loss = 1.51 (93.3 examples/sec; 0.043 sec/batch; 1h:42m:49s remains)
INFO - root - 2019-11-06 18:08:40.785778: step 6110, total loss = 6.31, predict loss = 1.80 (95.4 examples/sec; 0.042 sec/batch; 1h:40m:35s remains)
INFO - root - 2019-11-06 18:08:42.091078: step 6120, total loss = 6.65, predict loss = 1.84 (65.9 examples/sec; 0.061 sec/batch; 2h:25m:35s remains)
INFO - root - 2019-11-06 18:08:42.873708: step 6130, total loss = 5.28, predict loss = 1.53 (51.2 examples/sec; 0.078 sec/batch; 3h:07m:25s remains)
INFO - root - 2019-11-06 18:08:43.582301: step 6140, total loss = 5.66, predict loss = 1.64 (65.6 examples/sec; 0.061 sec/batch; 2h:26m:05s remains)
INFO - root - 2019-11-06 18:08:44.322164: step 6150, total loss = 4.14, predict loss = 1.19 (55.4 examples/sec; 0.072 sec/batch; 2h:52m:59s remains)
INFO - root - 2019-11-06 18:08:45.085067: step 6160, total loss = 6.09, predict loss = 1.82 (54.9 examples/sec; 0.073 sec/batch; 2h:54m:39s remains)
INFO - root - 2019-11-06 18:08:45.624865: step 6170, total loss = 5.76, predict loss = 1.72 (95.9 examples/sec; 0.042 sec/batch; 1h:39m:59s remains)
INFO - root - 2019-11-06 18:08:46.079677: step 6180, total loss = 6.08, predict loss = 1.73 (91.0 examples/sec; 0.044 sec/batch; 1h:45m:20s remains)
INFO - root - 2019-11-06 18:08:47.274996: step 6190, total loss = 5.86, predict loss = 1.74 (65.3 examples/sec; 0.061 sec/batch; 2h:26m:43s remains)
INFO - root - 2019-11-06 18:08:48.002943: step 6200, total loss = 3.30, predict loss = 0.96 (53.0 examples/sec; 0.075 sec/batch; 3h:00m:48s remains)
INFO - root - 2019-11-06 18:08:48.768679: step 6210, total loss = 4.42, predict loss = 1.28 (54.0 examples/sec; 0.074 sec/batch; 2h:57m:21s remains)
INFO - root - 2019-11-06 18:08:49.525366: step 6220, total loss = 5.52, predict loss = 1.54 (60.4 examples/sec; 0.066 sec/batch; 2h:38m:44s remains)
INFO - root - 2019-11-06 18:08:50.234726: step 6230, total loss = 5.94, predict loss = 1.68 (62.9 examples/sec; 0.064 sec/batch; 2h:32m:29s remains)
INFO - root - 2019-11-06 18:08:50.766625: step 6240, total loss = 3.60, predict loss = 1.11 (99.4 examples/sec; 0.040 sec/batch; 1h:36m:24s remains)
INFO - root - 2019-11-06 18:08:51.250128: step 6250, total loss = 5.55, predict loss = 1.53 (97.2 examples/sec; 0.041 sec/batch; 1h:38m:37s remains)
INFO - root - 2019-11-06 18:08:51.692018: step 6260, total loss = 5.28, predict loss = 1.54 (104.1 examples/sec; 0.038 sec/batch; 1h:32m:05s remains)
INFO - root - 2019-11-06 18:08:53.088514: step 6270, total loss = 6.03, predict loss = 1.81 (63.4 examples/sec; 0.063 sec/batch; 2h:31m:06s remains)
INFO - root - 2019-11-06 18:08:53.814865: step 6280, total loss = 5.28, predict loss = 1.56 (56.8 examples/sec; 0.070 sec/batch; 2h:48m:47s remains)
INFO - root - 2019-11-06 18:08:54.545708: step 6290, total loss = 4.29, predict loss = 1.21 (60.9 examples/sec; 0.066 sec/batch; 2h:37m:20s remains)
INFO - root - 2019-11-06 18:08:55.328183: step 6300, total loss = 4.48, predict loss = 1.40 (56.4 examples/sec; 0.071 sec/batch; 2h:49m:44s remains)
INFO - root - 2019-11-06 18:08:56.038386: step 6310, total loss = 4.97, predict loss = 1.35 (58.0 examples/sec; 0.069 sec/batch; 2h:45m:15s remains)
INFO - root - 2019-11-06 18:08:56.575447: step 6320, total loss = 5.34, predict loss = 1.54 (84.2 examples/sec; 0.048 sec/batch; 1h:53m:46s remains)
INFO - root - 2019-11-06 18:08:57.055508: step 6330, total loss = 4.93, predict loss = 1.45 (97.2 examples/sec; 0.041 sec/batch; 1h:38m:31s remains)
INFO - root - 2019-11-06 18:08:58.227078: step 6340, total loss = 4.45, predict loss = 1.38 (64.3 examples/sec; 0.062 sec/batch; 2h:28m:53s remains)
INFO - root - 2019-11-06 18:08:58.926121: step 6350, total loss = 5.02, predict loss = 1.48 (64.2 examples/sec; 0.062 sec/batch; 2h:29m:16s remains)
INFO - root - 2019-11-06 18:08:59.722696: step 6360, total loss = 3.06, predict loss = 0.94 (55.5 examples/sec; 0.072 sec/batch; 2h:52m:24s remains)
INFO - root - 2019-11-06 18:09:00.440578: step 6370, total loss = 6.16, predict loss = 1.76 (68.3 examples/sec; 0.059 sec/batch; 2h:20m:14s remains)
INFO - root - 2019-11-06 18:09:01.156797: step 6380, total loss = 6.16, predict loss = 1.79 (62.6 examples/sec; 0.064 sec/batch; 2h:32m:58s remains)
INFO - root - 2019-11-06 18:09:01.739904: step 6390, total loss = 6.15, predict loss = 1.66 (96.6 examples/sec; 0.041 sec/batch; 1h:39m:04s remains)
INFO - root - 2019-11-06 18:09:02.198469: step 6400, total loss = 5.68, predict loss = 1.66 (94.4 examples/sec; 0.042 sec/batch; 1h:41m:24s remains)
INFO - root - 2019-11-06 18:09:02.659486: step 6410, total loss = 4.82, predict loss = 1.38 (144.7 examples/sec; 0.028 sec/batch; 1h:06m:09s remains)
INFO - root - 2019-11-06 18:09:04.046713: step 6420, total loss = 4.96, predict loss = 1.45 (53.2 examples/sec; 0.075 sec/batch; 2h:59m:49s remains)
INFO - root - 2019-11-06 18:09:04.770954: step 6430, total loss = 4.62, predict loss = 1.34 (56.8 examples/sec; 0.070 sec/batch; 2h:48m:33s remains)
INFO - root - 2019-11-06 18:09:05.536412: step 6440, total loss = 4.78, predict loss = 1.33 (55.9 examples/sec; 0.072 sec/batch; 2h:51m:12s remains)
INFO - root - 2019-11-06 18:09:06.290723: step 6450, total loss = 5.25, predict loss = 1.51 (59.3 examples/sec; 0.067 sec/batch; 2h:41m:29s remains)
INFO - root - 2019-11-06 18:09:07.006556: step 6460, total loss = 5.85, predict loss = 1.75 (69.2 examples/sec; 0.058 sec/batch; 2h:18m:13s remains)
INFO - root - 2019-11-06 18:09:07.461252: step 6470, total loss = 4.12, predict loss = 1.20 (98.9 examples/sec; 0.040 sec/batch; 1h:36m:47s remains)
INFO - root - 2019-11-06 18:09:07.910257: step 6480, total loss = 4.84, predict loss = 1.43 (89.3 examples/sec; 0.045 sec/batch; 1h:47m:07s remains)
INFO - root - 2019-11-06 18:09:09.134220: step 6490, total loss = 5.48, predict loss = 1.64 (62.5 examples/sec; 0.064 sec/batch; 2h:32m:58s remains)
INFO - root - 2019-11-06 18:09:09.845266: step 6500, total loss = 3.95, predict loss = 1.16 (59.7 examples/sec; 0.067 sec/batch; 2h:40m:13s remains)
INFO - root - 2019-11-06 18:09:10.611133: step 6510, total loss = 5.93, predict loss = 1.70 (62.8 examples/sec; 0.064 sec/batch; 2h:32m:21s remains)
INFO - root - 2019-11-06 18:09:11.346906: step 6520, total loss = 5.83, predict loss = 1.72 (58.0 examples/sec; 0.069 sec/batch; 2h:44m:54s remains)
INFO - root - 2019-11-06 18:09:12.080083: step 6530, total loss = 4.53, predict loss = 1.33 (69.3 examples/sec; 0.058 sec/batch; 2h:17m:59s remains)
INFO - root - 2019-11-06 18:09:12.657351: step 6540, total loss = 5.22, predict loss = 1.48 (94.2 examples/sec; 0.042 sec/batch; 1h:41m:31s remains)
INFO - root - 2019-11-06 18:09:13.109811: step 6550, total loss = 6.20, predict loss = 1.75 (95.1 examples/sec; 0.042 sec/batch; 1h:40m:35s remains)
INFO - root - 2019-11-06 18:09:14.214752: step 6560, total loss = 3.99, predict loss = 1.24 (5.7 examples/sec; 0.706 sec/batch; 28h:08m:55s remains)
INFO - root - 2019-11-06 18:09:14.907175: step 6570, total loss = 5.80, predict loss = 1.69 (64.1 examples/sec; 0.062 sec/batch; 2h:29m:15s remains)
INFO - root - 2019-11-06 18:09:15.650424: step 6580, total loss = 4.50, predict loss = 1.24 (58.9 examples/sec; 0.068 sec/batch; 2h:42m:13s remains)
INFO - root - 2019-11-06 18:09:16.412830: step 6590, total loss = 6.24, predict loss = 1.73 (62.9 examples/sec; 0.064 sec/batch; 2h:32m:06s remains)
INFO - root - 2019-11-06 18:09:17.152734: step 6600, total loss = 4.44, predict loss = 1.31 (60.7 examples/sec; 0.066 sec/batch; 2h:37m:34s remains)
INFO - root - 2019-11-06 18:09:17.852803: step 6610, total loss = 5.14, predict loss = 1.49 (88.1 examples/sec; 0.045 sec/batch; 1h:48m:27s remains)
INFO - root - 2019-11-06 18:09:18.309411: step 6620, total loss = 5.68, predict loss = 1.54 (90.9 examples/sec; 0.044 sec/batch; 1h:45m:07s remains)
INFO - root - 2019-11-06 18:09:18.756137: step 6630, total loss = 5.95, predict loss = 1.71 (97.5 examples/sec; 0.041 sec/batch; 1h:38m:02s remains)
INFO - root - 2019-11-06 18:09:19.991622: step 6640, total loss = 6.54, predict loss = 1.91 (57.1 examples/sec; 0.070 sec/batch; 2h:47m:16s remains)
INFO - root - 2019-11-06 18:09:20.787412: step 6650, total loss = 5.24, predict loss = 1.56 (59.0 examples/sec; 0.068 sec/batch; 2h:41m:59s remains)
INFO - root - 2019-11-06 18:09:21.507183: step 6660, total loss = 5.94, predict loss = 1.69 (63.5 examples/sec; 0.063 sec/batch; 2h:30m:27s remains)
INFO - root - 2019-11-06 18:09:22.267394: step 6670, total loss = 4.13, predict loss = 1.25 (70.0 examples/sec; 0.057 sec/batch; 2h:16m:34s remains)
INFO - root - 2019-11-06 18:09:22.971261: step 6680, total loss = 4.48, predict loss = 1.29 (61.6 examples/sec; 0.065 sec/batch; 2h:35m:05s remains)
INFO - root - 2019-11-06 18:09:23.527911: step 6690, total loss = 5.47, predict loss = 1.62 (101.1 examples/sec; 0.040 sec/batch; 1h:34m:31s remains)
INFO - root - 2019-11-06 18:09:23.981633: step 6700, total loss = 4.27, predict loss = 1.24 (92.1 examples/sec; 0.043 sec/batch; 1h:43m:42s remains)
INFO - root - 2019-11-06 18:09:25.177006: step 6710, total loss = 5.79, predict loss = 1.70 (58.1 examples/sec; 0.069 sec/batch; 2h:44m:22s remains)
INFO - root - 2019-11-06 18:09:25.856039: step 6720, total loss = 6.09, predict loss = 1.83 (63.6 examples/sec; 0.063 sec/batch; 2h:30m:14s remains)
INFO - root - 2019-11-06 18:09:26.631173: step 6730, total loss = 5.94, predict loss = 1.68 (56.9 examples/sec; 0.070 sec/batch; 2h:47m:51s remains)
INFO - root - 2019-11-06 18:09:27.352379: step 6740, total loss = 5.00, predict loss = 1.39 (61.5 examples/sec; 0.065 sec/batch; 2h:35m:21s remains)
INFO - root - 2019-11-06 18:09:28.113928: step 6750, total loss = 6.57, predict loss = 1.96 (52.9 examples/sec; 0.076 sec/batch; 3h:00m:34s remains)
INFO - root - 2019-11-06 18:09:28.811825: step 6760, total loss = 5.68, predict loss = 1.65 (87.8 examples/sec; 0.046 sec/batch; 1h:48m:44s remains)
INFO - root - 2019-11-06 18:09:29.294184: step 6770, total loss = 5.34, predict loss = 1.57 (97.1 examples/sec; 0.041 sec/batch; 1h:38m:17s remains)
INFO - root - 2019-11-06 18:09:29.766873: step 6780, total loss = 6.06, predict loss = 1.77 (79.8 examples/sec; 0.050 sec/batch; 1h:59m:41s remains)
INFO - root - 2019-11-06 18:09:31.032551: step 6790, total loss = 5.17, predict loss = 1.47 (56.9 examples/sec; 0.070 sec/batch; 2h:47m:54s remains)
INFO - root - 2019-11-06 18:09:31.805926: step 6800, total loss = 4.16, predict loss = 1.21 (55.4 examples/sec; 0.072 sec/batch; 2h:52m:10s remains)
INFO - root - 2019-11-06 18:09:32.570869: step 6810, total loss = 2.75, predict loss = 0.77 (58.9 examples/sec; 0.068 sec/batch; 2h:41m:59s remains)
INFO - root - 2019-11-06 18:09:33.341426: step 6820, total loss = 4.81, predict loss = 1.45 (56.4 examples/sec; 0.071 sec/batch; 2h:49m:21s remains)
INFO - root - 2019-11-06 18:09:34.085453: step 6830, total loss = 5.23, predict loss = 1.46 (72.0 examples/sec; 0.056 sec/batch; 2h:12m:37s remains)
INFO - root - 2019-11-06 18:09:34.602011: step 6840, total loss = 6.08, predict loss = 1.68 (99.1 examples/sec; 0.040 sec/batch; 1h:36m:19s remains)
INFO - root - 2019-11-06 18:09:35.073788: step 6850, total loss = 5.11, predict loss = 1.53 (101.6 examples/sec; 0.039 sec/batch; 1h:33m:57s remains)
INFO - root - 2019-11-06 18:09:36.238077: step 6860, total loss = 5.65, predict loss = 1.63 (68.2 examples/sec; 0.059 sec/batch; 2h:19m:53s remains)
INFO - root - 2019-11-06 18:09:36.946583: step 6870, total loss = 5.46, predict loss = 1.53 (62.6 examples/sec; 0.064 sec/batch; 2h:32m:20s remains)
INFO - root - 2019-11-06 18:09:37.709291: step 6880, total loss = 4.86, predict loss = 1.50 (57.2 examples/sec; 0.070 sec/batch; 2h:46m:40s remains)
INFO - root - 2019-11-06 18:09:38.523040: step 6890, total loss = 5.82, predict loss = 1.61 (60.2 examples/sec; 0.066 sec/batch; 2h:38m:24s remains)
INFO - root - 2019-11-06 18:09:39.312329: step 6900, total loss = 4.74, predict loss = 1.36 (59.0 examples/sec; 0.068 sec/batch; 2h:41m:47s remains)
INFO - root - 2019-11-06 18:09:39.888603: step 6910, total loss = 5.92, predict loss = 1.67 (102.4 examples/sec; 0.039 sec/batch; 1h:33m:09s remains)
INFO - root - 2019-11-06 18:09:40.326700: step 6920, total loss = 4.37, predict loss = 1.28 (100.4 examples/sec; 0.040 sec/batch; 1h:34m:57s remains)
INFO - root - 2019-11-06 18:09:40.806456: step 6930, total loss = 2.97, predict loss = 0.82 (93.7 examples/sec; 0.043 sec/batch; 1h:41m:46s remains)
INFO - root - 2019-11-06 18:09:42.144665: step 6940, total loss = 2.62, predict loss = 0.83 (59.4 examples/sec; 0.067 sec/batch; 2h:40m:35s remains)
INFO - root - 2019-11-06 18:09:42.902413: step 6950, total loss = 6.27, predict loss = 1.69 (59.5 examples/sec; 0.067 sec/batch; 2h:40m:15s remains)
INFO - root - 2019-11-06 18:09:43.689426: step 6960, total loss = 5.46, predict loss = 1.59 (57.7 examples/sec; 0.069 sec/batch; 2h:45m:22s remains)
INFO - root - 2019-11-06 18:09:44.479836: step 6970, total loss = 3.50, predict loss = 1.10 (60.5 examples/sec; 0.066 sec/batch; 2h:37m:43s remains)
INFO - root - 2019-11-06 18:09:45.209091: step 6980, total loss = 3.96, predict loss = 1.22 (65.0 examples/sec; 0.062 sec/batch; 2h:26m:41s remains)
INFO - root - 2019-11-06 18:09:45.726128: step 6990, total loss = 5.27, predict loss = 1.47 (96.6 examples/sec; 0.041 sec/batch; 1h:38m:39s remains)
INFO - root - 2019-11-06 18:09:46.198386: step 7000, total loss = 3.83, predict loss = 1.06 (98.6 examples/sec; 0.041 sec/batch; 1h:36m:40s remains)
INFO - root - 2019-11-06 18:09:47.408444: step 7010, total loss = 5.05, predict loss = 1.48 (71.1 examples/sec; 0.056 sec/batch; 2h:14m:09s remains)
INFO - root - 2019-11-06 18:09:48.131329: step 7020, total loss = 5.27, predict loss = 1.53 (63.9 examples/sec; 0.063 sec/batch; 2h:29m:07s remains)
INFO - root - 2019-11-06 18:09:48.827670: step 7030, total loss = 5.88, predict loss = 1.62 (64.0 examples/sec; 0.062 sec/batch; 2h:28m:50s remains)
INFO - root - 2019-11-06 18:09:49.525726: step 7040, total loss = 6.61, predict loss = 1.93 (62.1 examples/sec; 0.064 sec/batch; 2h:33m:25s remains)
INFO - root - 2019-11-06 18:09:50.327457: step 7050, total loss = 3.97, predict loss = 1.11 (50.9 examples/sec; 0.079 sec/batch; 3h:07m:03s remains)
INFO - root - 2019-11-06 18:09:50.948179: step 7060, total loss = 5.22, predict loss = 1.47 (95.7 examples/sec; 0.042 sec/batch; 1h:39m:35s remains)
INFO - root - 2019-11-06 18:09:51.405980: step 7070, total loss = 5.27, predict loss = 1.53 (90.8 examples/sec; 0.044 sec/batch; 1h:44m:55s remains)
INFO - root - 2019-11-06 18:09:51.878436: step 7080, total loss = 3.96, predict loss = 1.16 (64.5 examples/sec; 0.062 sec/batch; 2h:27m:39s remains)
INFO - root - 2019-11-06 18:09:53.295945: step 7090, total loss = 5.94, predict loss = 1.76 (66.5 examples/sec; 0.060 sec/batch; 2h:23m:18s remains)
INFO - root - 2019-11-06 18:09:54.023947: step 7100, total loss = 4.01, predict loss = 1.19 (57.5 examples/sec; 0.070 sec/batch; 2h:45m:42s remains)
INFO - root - 2019-11-06 18:09:54.717034: step 7110, total loss = 5.63, predict loss = 1.67 (61.4 examples/sec; 0.065 sec/batch; 2h:35m:11s remains)
INFO - root - 2019-11-06 18:09:55.521205: step 7120, total loss = 5.26, predict loss = 1.49 (56.4 examples/sec; 0.071 sec/batch; 2h:48m:48s remains)
INFO - root - 2019-11-06 18:09:56.241998: step 7130, total loss = 3.22, predict loss = 0.91 (69.0 examples/sec; 0.058 sec/batch; 2h:17m:58s remains)
INFO - root - 2019-11-06 18:09:56.697777: step 7140, total loss = 5.15, predict loss = 1.43 (90.6 examples/sec; 0.044 sec/batch; 1h:45m:08s remains)
INFO - root - 2019-11-06 18:09:57.153465: step 7150, total loss = 6.82, predict loss = 1.93 (95.4 examples/sec; 0.042 sec/batch; 1h:39m:48s remains)
INFO - root - 2019-11-06 18:09:58.310292: step 7160, total loss = 3.65, predict loss = 1.03 (64.4 examples/sec; 0.062 sec/batch; 2h:27m:45s remains)
INFO - root - 2019-11-06 18:09:59.051113: step 7170, total loss = 3.80, predict loss = 1.13 (57.6 examples/sec; 0.069 sec/batch; 2h:45m:12s remains)
INFO - root - 2019-11-06 18:09:59.742864: step 7180, total loss = 5.60, predict loss = 1.56 (63.1 examples/sec; 0.063 sec/batch; 2h:30m:49s remains)
INFO - root - 2019-11-06 18:10:00.435932: step 7190, total loss = 3.13, predict loss = 0.87 (63.7 examples/sec; 0.063 sec/batch; 2h:29m:28s remains)
INFO - root - 2019-11-06 18:10:01.219702: step 7200, total loss = 4.65, predict loss = 1.40 (56.6 examples/sec; 0.071 sec/batch; 2h:48m:11s remains)
INFO - root - 2019-11-06 18:10:01.797587: step 7210, total loss = 5.49, predict loss = 1.56 (100.1 examples/sec; 0.040 sec/batch; 1h:35m:05s remains)
INFO - root - 2019-11-06 18:10:02.253437: step 7220, total loss = 4.56, predict loss = 1.29 (91.4 examples/sec; 0.044 sec/batch; 1h:44m:08s remains)
INFO - root - 2019-11-06 18:10:02.691408: step 7230, total loss = 4.77, predict loss = 1.35 (135.9 examples/sec; 0.029 sec/batch; 1h:10m:02s remains)
INFO - root - 2019-11-06 18:10:04.039392: step 7240, total loss = 5.25, predict loss = 1.54 (60.7 examples/sec; 0.066 sec/batch; 2h:36m:48s remains)
INFO - root - 2019-11-06 18:10:04.807901: step 7250, total loss = 3.38, predict loss = 0.98 (61.7 examples/sec; 0.065 sec/batch; 2h:34m:09s remains)
INFO - root - 2019-11-06 18:10:05.608618: step 7260, total loss = 4.70, predict loss = 1.39 (63.4 examples/sec; 0.063 sec/batch; 2h:30m:08s remains)
INFO - root - 2019-11-06 18:10:06.316196: step 7270, total loss = 4.74, predict loss = 1.33 (65.6 examples/sec; 0.061 sec/batch; 2h:25m:01s remains)
INFO - root - 2019-11-06 18:10:07.014137: step 7280, total loss = 6.25, predict loss = 1.87 (74.6 examples/sec; 0.054 sec/batch; 2h:07m:29s remains)
INFO - root - 2019-11-06 18:10:07.513407: step 7290, total loss = 4.40, predict loss = 1.30 (97.3 examples/sec; 0.041 sec/batch; 1h:37m:49s remains)
INFO - root - 2019-11-06 18:10:07.964395: step 7300, total loss = 5.67, predict loss = 1.60 (97.0 examples/sec; 0.041 sec/batch; 1h:38m:06s remains)
INFO - root - 2019-11-06 18:10:09.208346: step 7310, total loss = 5.72, predict loss = 1.65 (66.1 examples/sec; 0.061 sec/batch; 2h:24m:00s remains)
INFO - root - 2019-11-06 18:10:09.953720: step 7320, total loss = 5.19, predict loss = 1.48 (61.0 examples/sec; 0.066 sec/batch; 2h:35m:56s remains)
INFO - root - 2019-11-06 18:10:10.696026: step 7330, total loss = 5.37, predict loss = 1.53 (65.6 examples/sec; 0.061 sec/batch; 2h:24m:55s remains)
INFO - root - 2019-11-06 18:10:11.414236: step 7340, total loss = 6.86, predict loss = 2.03 (59.3 examples/sec; 0.067 sec/batch; 2h:40m:21s remains)
INFO - root - 2019-11-06 18:10:12.163146: step 7350, total loss = 4.58, predict loss = 1.33 (65.3 examples/sec; 0.061 sec/batch; 2h:25m:31s remains)
INFO - root - 2019-11-06 18:10:12.757410: step 7360, total loss = 4.12, predict loss = 1.19 (97.5 examples/sec; 0.041 sec/batch; 1h:37m:30s remains)
INFO - root - 2019-11-06 18:10:13.244519: step 7370, total loss = 4.83, predict loss = 1.27 (92.2 examples/sec; 0.043 sec/batch; 1h:43m:08s remains)
INFO - root - 2019-11-06 18:10:14.373160: step 7380, total loss = 4.28, predict loss = 1.28 (5.6 examples/sec; 0.713 sec/batch; 28h:14m:41s remains)
INFO - root - 2019-11-06 18:10:15.034602: step 7390, total loss = 5.34, predict loss = 1.50 (60.5 examples/sec; 0.066 sec/batch; 2h:37m:12s remains)
INFO - root - 2019-11-06 18:10:15.775861: step 7400, total loss = 5.29, predict loss = 1.54 (55.2 examples/sec; 0.072 sec/batch; 2h:52m:07s remains)
INFO - root - 2019-11-06 18:10:16.530583: step 7410, total loss = 4.98, predict loss = 1.42 (57.2 examples/sec; 0.070 sec/batch; 2h:46m:08s remains)
INFO - root - 2019-11-06 18:10:17.298935: step 7420, total loss = 3.78, predict loss = 1.08 (57.4 examples/sec; 0.070 sec/batch; 2h:45m:38s remains)
INFO - root - 2019-11-06 18:10:17.955040: step 7430, total loss = 6.60, predict loss = 1.86 (90.9 examples/sec; 0.044 sec/batch; 1h:44m:30s remains)
INFO - root - 2019-11-06 18:10:18.421120: step 7440, total loss = 5.47, predict loss = 1.57 (93.0 examples/sec; 0.043 sec/batch; 1h:42m:14s remains)
INFO - root - 2019-11-06 18:10:18.897044: step 7450, total loss = 5.06, predict loss = 1.41 (92.4 examples/sec; 0.043 sec/batch; 1h:42m:49s remains)
INFO - root - 2019-11-06 18:10:20.129335: step 7460, total loss = 4.15, predict loss = 1.21 (55.7 examples/sec; 0.072 sec/batch; 2h:50m:34s remains)
INFO - root - 2019-11-06 18:10:20.905462: step 7470, total loss = 5.77, predict loss = 1.65 (62.4 examples/sec; 0.064 sec/batch; 2h:32m:20s remains)
INFO - root - 2019-11-06 18:10:21.643853: step 7480, total loss = 4.42, predict loss = 1.26 (54.2 examples/sec; 0.074 sec/batch; 2h:55m:10s remains)
INFO - root - 2019-11-06 18:10:22.416383: step 7490, total loss = 5.55, predict loss = 1.63 (68.5 examples/sec; 0.058 sec/batch; 2h:18m:36s remains)
INFO - root - 2019-11-06 18:10:23.097504: step 7500, total loss = 4.80, predict loss = 1.43 (74.6 examples/sec; 0.054 sec/batch; 2h:07m:19s remains)
INFO - root - 2019-11-06 18:10:23.637551: step 7510, total loss = 5.77, predict loss = 1.59 (100.1 examples/sec; 0.040 sec/batch; 1h:34m:56s remains)
INFO - root - 2019-11-06 18:10:24.100313: step 7520, total loss = 4.48, predict loss = 1.27 (93.0 examples/sec; 0.043 sec/batch; 1h:42m:07s remains)
INFO - root - 2019-11-06 18:10:25.278704: step 7530, total loss = 4.68, predict loss = 1.32 (69.0 examples/sec; 0.058 sec/batch; 2h:17m:39s remains)
INFO - root - 2019-11-06 18:10:25.953639: step 7540, total loss = 6.31, predict loss = 1.84 (60.4 examples/sec; 0.066 sec/batch; 2h:37m:16s remains)
INFO - root - 2019-11-06 18:10:26.655951: step 7550, total loss = 3.73, predict loss = 1.12 (61.7 examples/sec; 0.065 sec/batch; 2h:33m:50s remains)
INFO - root - 2019-11-06 18:10:27.428639: step 7560, total loss = 6.23, predict loss = 1.79 (54.2 examples/sec; 0.074 sec/batch; 2h:55m:05s remains)
INFO - root - 2019-11-06 18:10:28.195137: step 7570, total loss = 4.02, predict loss = 1.20 (61.6 examples/sec; 0.065 sec/batch; 2h:34m:03s remains)
INFO - root - 2019-11-06 18:10:28.844195: step 7580, total loss = 4.05, predict loss = 1.18 (94.5 examples/sec; 0.042 sec/batch; 1h:40m:29s remains)
INFO - root - 2019-11-06 18:10:29.290030: step 7590, total loss = 5.16, predict loss = 1.46 (94.5 examples/sec; 0.042 sec/batch; 1h:40m:25s remains)
INFO - root - 2019-11-06 18:10:29.751649: step 7600, total loss = 5.35, predict loss = 1.53 (92.4 examples/sec; 0.043 sec/batch; 1h:42m:44s remains)
INFO - root - 2019-11-06 18:10:31.017072: step 7610, total loss = 4.88, predict loss = 1.44 (63.6 examples/sec; 0.063 sec/batch; 2h:29m:15s remains)
INFO - root - 2019-11-06 18:10:31.732962: step 7620, total loss = 5.45, predict loss = 1.60 (63.3 examples/sec; 0.063 sec/batch; 2h:29m:59s remains)
INFO - root - 2019-11-06 18:10:32.464860: step 7630, total loss = 3.53, predict loss = 1.06 (57.9 examples/sec; 0.069 sec/batch; 2h:43m:58s remains)
INFO - root - 2019-11-06 18:10:33.195070: step 7640, total loss = 3.21, predict loss = 1.00 (61.2 examples/sec; 0.065 sec/batch; 2h:35m:02s remains)
INFO - root - 2019-11-06 18:10:33.917027: step 7650, total loss = 6.24, predict loss = 1.81 (68.2 examples/sec; 0.059 sec/batch; 2h:19m:12s remains)
INFO - root - 2019-11-06 18:10:34.448123: step 7660, total loss = 5.05, predict loss = 1.45 (97.7 examples/sec; 0.041 sec/batch; 1h:37m:08s remains)
INFO - root - 2019-11-06 18:10:34.898463: step 7670, total loss = 3.15, predict loss = 0.89 (91.9 examples/sec; 0.044 sec/batch; 1h:43m:15s remains)
INFO - root - 2019-11-06 18:10:36.081167: step 7680, total loss = 5.68, predict loss = 1.62 (65.2 examples/sec; 0.061 sec/batch; 2h:25m:35s remains)
INFO - root - 2019-11-06 18:10:36.837370: step 7690, total loss = 4.25, predict loss = 1.19 (55.8 examples/sec; 0.072 sec/batch; 2h:50m:04s remains)
INFO - root - 2019-11-06 18:10:37.603263: step 7700, total loss = 5.37, predict loss = 1.53 (54.4 examples/sec; 0.073 sec/batch; 2h:54m:18s remains)
INFO - root - 2019-11-06 18:10:38.392045: step 7710, total loss = 5.52, predict loss = 1.57 (63.7 examples/sec; 0.063 sec/batch; 2h:28m:50s remains)
INFO - root - 2019-11-06 18:10:39.223082: step 7720, total loss = 4.18, predict loss = 1.24 (53.8 examples/sec; 0.074 sec/batch; 2h:56m:10s remains)
INFO - root - 2019-11-06 18:10:39.887001: step 7730, total loss = 2.93, predict loss = 0.84 (98.6 examples/sec; 0.041 sec/batch; 1h:36m:11s remains)
INFO - root - 2019-11-06 18:10:40.347839: step 7740, total loss = 3.66, predict loss = 1.13 (101.4 examples/sec; 0.039 sec/batch; 1h:33m:33s remains)
INFO - root - 2019-11-06 18:10:40.801974: step 7750, total loss = 3.77, predict loss = 1.14 (95.9 examples/sec; 0.042 sec/batch; 1h:38m:56s remains)
INFO - root - 2019-11-06 18:10:42.077884: step 7760, total loss = 5.30, predict loss = 1.49 (67.2 examples/sec; 0.059 sec/batch; 2h:21m:00s remains)
INFO - root - 2019-11-06 18:10:42.905056: step 7770, total loss = 4.78, predict loss = 1.40 (52.8 examples/sec; 0.076 sec/batch; 2h:59m:39s remains)
INFO - root - 2019-11-06 18:10:43.604828: step 7780, total loss = 5.32, predict loss = 1.54 (57.2 examples/sec; 0.070 sec/batch; 2h:45m:52s remains)
INFO - root - 2019-11-06 18:10:44.365757: step 7790, total loss = 5.84, predict loss = 1.61 (59.0 examples/sec; 0.068 sec/batch; 2h:40m:44s remains)
INFO - root - 2019-11-06 18:10:45.070841: step 7800, total loss = 4.58, predict loss = 1.28 (62.1 examples/sec; 0.064 sec/batch; 2h:32m:45s remains)
INFO - root - 2019-11-06 18:10:45.606520: step 7810, total loss = 5.72, predict loss = 1.68 (93.6 examples/sec; 0.043 sec/batch; 1h:41m:15s remains)
INFO - root - 2019-11-06 18:10:46.060097: step 7820, total loss = 5.13, predict loss = 1.49 (98.2 examples/sec; 0.041 sec/batch; 1h:36m:28s remains)
INFO - root - 2019-11-06 18:10:47.226854: step 7830, total loss = 4.00, predict loss = 1.14 (63.5 examples/sec; 0.063 sec/batch; 2h:29m:12s remains)
INFO - root - 2019-11-06 18:10:47.971603: step 7840, total loss = 6.41, predict loss = 1.83 (60.1 examples/sec; 0.067 sec/batch; 2h:37m:42s remains)
INFO - root - 2019-11-06 18:10:48.734729: step 7850, total loss = 3.99, predict loss = 1.14 (50.6 examples/sec; 0.079 sec/batch; 3h:07m:12s remains)
INFO - root - 2019-11-06 18:10:49.519146: step 7860, total loss = 5.70, predict loss = 1.64 (56.8 examples/sec; 0.070 sec/batch; 2h:46m:56s remains)
INFO - root - 2019-11-06 18:10:50.275045: step 7870, total loss = 4.88, predict loss = 1.42 (63.3 examples/sec; 0.063 sec/batch; 2h:29m:46s remains)
INFO - root - 2019-11-06 18:10:50.839553: step 7880, total loss = 6.01, predict loss = 1.69 (98.2 examples/sec; 0.041 sec/batch; 1h:36m:30s remains)
INFO - root - 2019-11-06 18:10:51.324353: step 7890, total loss = 5.95, predict loss = 1.67 (98.0 examples/sec; 0.041 sec/batch; 1h:36m:39s remains)
INFO - root - 2019-11-06 18:10:51.780962: step 7900, total loss = 2.98, predict loss = 0.91 (89.3 examples/sec; 0.045 sec/batch; 1h:46m:05s remains)
INFO - root - 2019-11-06 18:10:53.232979: step 7910, total loss = 5.91, predict loss = 1.78 (67.7 examples/sec; 0.059 sec/batch; 2h:19m:49s remains)
INFO - root - 2019-11-06 18:10:53.998080: step 7920, total loss = 4.08, predict loss = 1.22 (54.5 examples/sec; 0.073 sec/batch; 2h:53m:39s remains)
INFO - root - 2019-11-06 18:10:54.733036: step 7930, total loss = 4.88, predict loss = 1.39 (68.5 examples/sec; 0.058 sec/batch; 2h:18m:15s remains)
INFO - root - 2019-11-06 18:10:55.457928: step 7940, total loss = 5.12, predict loss = 1.52 (55.2 examples/sec; 0.072 sec/batch; 2h:51m:36s remains)
INFO - root - 2019-11-06 18:10:56.162697: step 7950, total loss = 5.11, predict loss = 1.49 (64.3 examples/sec; 0.062 sec/batch; 2h:27m:09s remains)
INFO - root - 2019-11-06 18:10:56.647259: step 7960, total loss = 5.96, predict loss = 1.71 (96.6 examples/sec; 0.041 sec/batch; 1h:37m:58s remains)
INFO - root - 2019-11-06 18:10:57.119329: step 7970, total loss = 6.32, predict loss = 1.73 (91.1 examples/sec; 0.044 sec/batch; 1h:43m:57s remains)
INFO - root - 2019-11-06 18:10:58.335997: step 7980, total loss = 5.29, predict loss = 1.55 (63.6 examples/sec; 0.063 sec/batch; 2h:28m:59s remains)
INFO - root - 2019-11-06 18:10:59.081247: step 7990, total loss = 3.79, predict loss = 1.13 (57.9 examples/sec; 0.069 sec/batch; 2h:43m:34s remains)
INFO - root - 2019-11-06 18:10:59.823003: step 8000, total loss = 5.02, predict loss = 1.33 (63.0 examples/sec; 0.063 sec/batch; 2h:30m:12s remains)
INFO - root - 2019-11-06 18:11:00.569491: step 8010, total loss = 5.89, predict loss = 1.60 (66.2 examples/sec; 0.060 sec/batch; 2h:23m:00s remains)
INFO - root - 2019-11-06 18:11:01.348643: step 8020, total loss = 3.79, predict loss = 1.07 (53.3 examples/sec; 0.075 sec/batch; 2h:57m:28s remains)
INFO - root - 2019-11-06 18:11:01.941791: step 8030, total loss = 3.77, predict loss = 1.06 (94.6 examples/sec; 0.042 sec/batch; 1h:40m:04s remains)
INFO - root - 2019-11-06 18:11:02.393289: step 8040, total loss = 5.46, predict loss = 1.55 (89.9 examples/sec; 0.044 sec/batch; 1h:45m:14s remains)
INFO - root - 2019-11-06 18:11:02.875849: step 8050, total loss = 5.16, predict loss = 1.43 (109.4 examples/sec; 0.037 sec/batch; 1h:26m:32s remains)
INFO - root - 2019-11-06 18:11:04.206271: step 8060, total loss = 5.08, predict loss = 1.36 (55.2 examples/sec; 0.072 sec/batch; 2h:51m:23s remains)
INFO - root - 2019-11-06 18:11:05.003817: step 8070, total loss = 4.56, predict loss = 1.37 (47.5 examples/sec; 0.084 sec/batch; 3h:19m:22s remains)
INFO - root - 2019-11-06 18:11:05.795723: step 8080, total loss = 5.30, predict loss = 1.48 (54.3 examples/sec; 0.074 sec/batch; 2h:54m:19s remains)
INFO - root - 2019-11-06 18:11:06.542044: step 8090, total loss = 4.98, predict loss = 1.42 (58.5 examples/sec; 0.068 sec/batch; 2h:41m:46s remains)
INFO - root - 2019-11-06 18:11:07.233560: step 8100, total loss = 4.89, predict loss = 1.44 (79.6 examples/sec; 0.050 sec/batch; 1h:58m:51s remains)
INFO - root - 2019-11-06 18:11:07.704292: step 8110, total loss = 2.67, predict loss = 0.79 (98.7 examples/sec; 0.041 sec/batch; 1h:35m:48s remains)
INFO - root - 2019-11-06 18:11:08.165935: step 8120, total loss = 4.50, predict loss = 1.29 (92.2 examples/sec; 0.043 sec/batch; 1h:42m:38s remains)
INFO - root - 2019-11-06 18:11:09.401466: step 8130, total loss = 5.48, predict loss = 1.59 (68.7 examples/sec; 0.058 sec/batch; 2h:17m:43s remains)
INFO - root - 2019-11-06 18:11:10.087478: step 8140, total loss = 4.94, predict loss = 1.36 (58.4 examples/sec; 0.068 sec/batch; 2h:41m:52s remains)
INFO - root - 2019-11-06 18:11:10.826229: step 8150, total loss = 5.44, predict loss = 1.53 (56.3 examples/sec; 0.071 sec/batch; 2h:48m:03s remains)
INFO - root - 2019-11-06 18:11:11.560486: step 8160, total loss = 4.37, predict loss = 1.24 (54.4 examples/sec; 0.074 sec/batch; 2h:53m:50s remains)
INFO - root - 2019-11-06 18:11:12.313487: step 8170, total loss = 5.45, predict loss = 1.58 (71.8 examples/sec; 0.056 sec/batch; 2h:11m:38s remains)
INFO - root - 2019-11-06 18:11:12.881947: step 8180, total loss = 6.77, predict loss = 1.86 (107.5 examples/sec; 0.037 sec/batch; 1h:27m:55s remains)
INFO - root - 2019-11-06 18:11:13.328313: step 8190, total loss = 5.37, predict loss = 1.58 (100.4 examples/sec; 0.040 sec/batch; 1h:34m:07s remains)
INFO - root - 2019-11-06 18:11:14.453949: step 8200, total loss = 5.46, predict loss = 1.61 (5.5 examples/sec; 0.724 sec/batch; 28h:30m:31s remains)
INFO - root - 2019-11-06 18:11:15.166177: step 8210, total loss = 6.28, predict loss = 1.75 (60.1 examples/sec; 0.067 sec/batch; 2h:37m:20s remains)
INFO - root - 2019-11-06 18:11:15.954427: step 8220, total loss = 4.71, predict loss = 1.37 (50.7 examples/sec; 0.079 sec/batch; 3h:06m:18s remains)
INFO - root - 2019-11-06 18:11:16.706350: step 8230, total loss = 3.97, predict loss = 1.17 (63.4 examples/sec; 0.063 sec/batch; 2h:29m:11s remains)
INFO - root - 2019-11-06 18:11:17.390132: step 8240, total loss = 1.64, predict loss = 0.51 (66.0 examples/sec; 0.061 sec/batch; 2h:23m:08s remains)
INFO - root - 2019-11-06 18:11:18.035020: step 8250, total loss = 4.96, predict loss = 1.47 (98.5 examples/sec; 0.041 sec/batch; 1h:35m:54s remains)
INFO - root - 2019-11-06 18:11:18.482742: step 8260, total loss = 5.26, predict loss = 1.40 (96.2 examples/sec; 0.042 sec/batch; 1h:38m:10s remains)
INFO - root - 2019-11-06 18:11:18.938853: step 8270, total loss = 4.43, predict loss = 1.26 (90.4 examples/sec; 0.044 sec/batch; 1h:44m:29s remains)
INFO - root - 2019-11-06 18:11:20.193746: step 8280, total loss = 4.18, predict loss = 1.22 (64.2 examples/sec; 0.062 sec/batch; 2h:27m:05s remains)
INFO - root - 2019-11-06 18:11:20.930886: step 8290, total loss = 5.54, predict loss = 1.63 (62.5 examples/sec; 0.064 sec/batch; 2h:31m:07s remains)
INFO - root - 2019-11-06 18:11:21.650134: step 8300, total loss = 6.25, predict loss = 1.79 (60.3 examples/sec; 0.066 sec/batch; 2h:36m:41s remains)
INFO - root - 2019-11-06 18:11:22.416501: step 8310, total loss = 5.06, predict loss = 1.47 (62.8 examples/sec; 0.064 sec/batch; 2h:30m:20s remains)
INFO - root - 2019-11-06 18:11:23.089840: step 8320, total loss = 4.07, predict loss = 1.13 (75.9 examples/sec; 0.053 sec/batch; 2h:04m:24s remains)
INFO - root - 2019-11-06 18:11:23.673307: step 8330, total loss = 4.85, predict loss = 1.39 (97.8 examples/sec; 0.041 sec/batch; 1h:36m:34s remains)
INFO - root - 2019-11-06 18:11:24.115038: step 8340, total loss = 4.66, predict loss = 1.31 (89.5 examples/sec; 0.045 sec/batch; 1h:45m:30s remains)
INFO - root - 2019-11-06 18:11:25.250321: step 8350, total loss = 3.58, predict loss = 1.06 (64.7 examples/sec; 0.062 sec/batch; 2h:25m:58s remains)
INFO - root - 2019-11-06 18:11:25.956094: step 8360, total loss = 4.97, predict loss = 1.49 (53.2 examples/sec; 0.075 sec/batch; 2h:57m:27s remains)
INFO - root - 2019-11-06 18:11:26.705875: step 8370, total loss = 3.51, predict loss = 1.01 (55.7 examples/sec; 0.072 sec/batch; 2h:49m:30s remains)
INFO - root - 2019-11-06 18:11:27.495904: step 8380, total loss = 6.44, predict loss = 1.91 (54.3 examples/sec; 0.074 sec/batch; 2h:53m:46s remains)
INFO - root - 2019-11-06 18:11:28.185657: step 8390, total loss = 4.95, predict loss = 1.44 (72.9 examples/sec; 0.055 sec/batch; 2h:09m:24s remains)
INFO - root - 2019-11-06 18:11:28.764411: step 8400, total loss = 5.94, predict loss = 1.68 (94.1 examples/sec; 0.042 sec/batch; 1h:40m:17s remains)
INFO - root - 2019-11-06 18:11:29.234367: step 8410, total loss = 3.82, predict loss = 1.15 (100.8 examples/sec; 0.040 sec/batch; 1h:33m:36s remains)
INFO - root - 2019-11-06 18:11:29.674313: step 8420, total loss = 3.38, predict loss = 0.98 (100.7 examples/sec; 0.040 sec/batch; 1h:33m:44s remains)
INFO - root - 2019-11-06 18:11:30.892994: step 8430, total loss = 5.02, predict loss = 1.43 (67.8 examples/sec; 0.059 sec/batch; 2h:19m:16s remains)
INFO - root - 2019-11-06 18:11:31.612867: step 8440, total loss = 5.49, predict loss = 1.55 (57.2 examples/sec; 0.070 sec/batch; 2h:44m:59s remains)
INFO - root - 2019-11-06 18:11:32.422257: step 8450, total loss = 3.46, predict loss = 0.98 (57.9 examples/sec; 0.069 sec/batch; 2h:43m:02s remains)
INFO - root - 2019-11-06 18:11:33.171142: step 8460, total loss = 4.49, predict loss = 1.32 (55.7 examples/sec; 0.072 sec/batch; 2h:49m:19s remains)
INFO - root - 2019-11-06 18:11:33.893230: step 8470, total loss = 4.60, predict loss = 1.31 (63.8 examples/sec; 0.063 sec/batch; 2h:27m:56s remains)
INFO - root - 2019-11-06 18:11:34.415327: step 8480, total loss = 5.93, predict loss = 1.75 (100.6 examples/sec; 0.040 sec/batch; 1h:33m:48s remains)
INFO - root - 2019-11-06 18:11:34.895892: step 8490, total loss = 4.75, predict loss = 1.41 (88.5 examples/sec; 0.045 sec/batch; 1h:46m:35s remains)
INFO - root - 2019-11-06 18:11:36.082903: step 8500, total loss = 5.78, predict loss = 1.67 (70.4 examples/sec; 0.057 sec/batch; 2h:13m:57s remains)
INFO - root - 2019-11-06 18:11:36.793174: step 8510, total loss = 5.10, predict loss = 1.46 (61.6 examples/sec; 0.065 sec/batch; 2h:33m:06s remains)
INFO - root - 2019-11-06 18:11:37.540919: step 8520, total loss = 5.40, predict loss = 1.64 (54.9 examples/sec; 0.073 sec/batch; 2h:51m:49s remains)
INFO - root - 2019-11-06 18:11:38.284986: step 8530, total loss = 5.48, predict loss = 1.60 (58.2 examples/sec; 0.069 sec/batch; 2h:41m:55s remains)
INFO - root - 2019-11-06 18:11:39.037656: step 8540, total loss = 4.20, predict loss = 1.15 (62.3 examples/sec; 0.064 sec/batch; 2h:31m:23s remains)
INFO - root - 2019-11-06 18:11:39.642154: step 8550, total loss = 5.45, predict loss = 1.56 (100.5 examples/sec; 0.040 sec/batch; 1h:33m:51s remains)
INFO - root - 2019-11-06 18:11:40.104229: step 8560, total loss = 4.11, predict loss = 1.17 (89.1 examples/sec; 0.045 sec/batch; 1h:45m:52s remains)
INFO - root - 2019-11-06 18:11:40.593282: step 8570, total loss = 5.46, predict loss = 1.63 (92.9 examples/sec; 0.043 sec/batch; 1h:41m:31s remains)
INFO - root - 2019-11-06 18:11:41.874890: step 8580, total loss = 6.09, predict loss = 1.70 (61.8 examples/sec; 0.065 sec/batch; 2h:32m:32s remains)
INFO - root - 2019-11-06 18:11:42.633493: step 8590, total loss = 5.30, predict loss = 1.51 (62.2 examples/sec; 0.064 sec/batch; 2h:31m:33s remains)
INFO - root - 2019-11-06 18:11:43.378710: step 8600, total loss = 3.89, predict loss = 1.17 (59.7 examples/sec; 0.067 sec/batch; 2h:37m:50s remains)
INFO - root - 2019-11-06 18:11:44.113172: step 8610, total loss = 4.82, predict loss = 1.37 (65.1 examples/sec; 0.061 sec/batch; 2h:24m:47s remains)
INFO - root - 2019-11-06 18:11:44.787756: step 8620, total loss = 6.00, predict loss = 1.80 (72.1 examples/sec; 0.055 sec/batch; 2h:10m:42s remains)
INFO - root - 2019-11-06 18:11:45.283043: step 8630, total loss = 3.16, predict loss = 0.93 (99.1 examples/sec; 0.040 sec/batch; 1h:35m:07s remains)
INFO - root - 2019-11-06 18:11:45.742473: step 8640, total loss = 5.26, predict loss = 1.50 (98.0 examples/sec; 0.041 sec/batch; 1h:36m:09s remains)
INFO - root - 2019-11-06 18:11:46.943867: step 8650, total loss = 5.25, predict loss = 1.56 (72.2 examples/sec; 0.055 sec/batch; 2h:10m:27s remains)
INFO - root - 2019-11-06 18:11:47.658652: step 8660, total loss = 5.58, predict loss = 1.51 (59.3 examples/sec; 0.067 sec/batch; 2h:38m:49s remains)
INFO - root - 2019-11-06 18:11:48.381233: step 8670, total loss = 3.76, predict loss = 1.11 (56.5 examples/sec; 0.071 sec/batch; 2h:46m:50s remains)
INFO - root - 2019-11-06 18:11:49.140291: step 8680, total loss = 5.96, predict loss = 1.66 (53.8 examples/sec; 0.074 sec/batch; 2h:55m:16s remains)
INFO - root - 2019-11-06 18:11:49.939405: step 8690, total loss = 4.56, predict loss = 1.34 (52.4 examples/sec; 0.076 sec/batch; 2h:59m:38s remains)
INFO - root - 2019-11-06 18:11:50.529112: step 8700, total loss = 4.00, predict loss = 1.10 (99.5 examples/sec; 0.040 sec/batch; 1h:34m:42s remains)
INFO - root - 2019-11-06 18:11:50.986821: step 8710, total loss = 5.13, predict loss = 1.55 (95.7 examples/sec; 0.042 sec/batch; 1h:38m:23s remains)
INFO - root - 2019-11-06 18:11:51.438673: step 8720, total loss = 6.39, predict loss = 1.73 (104.7 examples/sec; 0.038 sec/batch; 1h:29m:59s remains)
INFO - root - 2019-11-06 18:11:52.893782: step 8730, total loss = 5.16, predict loss = 1.56 (51.9 examples/sec; 0.077 sec/batch; 3h:01m:33s remains)
INFO - root - 2019-11-06 18:11:53.647133: step 8740, total loss = 4.53, predict loss = 1.33 (58.2 examples/sec; 0.069 sec/batch; 2h:41m:51s remains)
INFO - root - 2019-11-06 18:11:54.439614: step 8750, total loss = 5.53, predict loss = 1.63 (50.2 examples/sec; 0.080 sec/batch; 3h:07m:37s remains)
INFO - root - 2019-11-06 18:11:55.219363: step 8760, total loss = 5.18, predict loss = 1.47 (53.5 examples/sec; 0.075 sec/batch; 2h:56m:08s remains)
INFO - root - 2019-11-06 18:11:55.971471: step 8770, total loss = 5.78, predict loss = 1.62 (59.9 examples/sec; 0.067 sec/batch; 2h:37m:12s remains)
INFO - root - 2019-11-06 18:11:56.477620: step 8780, total loss = 3.89, predict loss = 1.05 (97.5 examples/sec; 0.041 sec/batch; 1h:36m:35s remains)
INFO - root - 2019-11-06 18:11:56.922182: step 8790, total loss = 3.90, predict loss = 1.18 (103.5 examples/sec; 0.039 sec/batch; 1h:30m:56s remains)
INFO - root - 2019-11-06 18:11:58.112312: step 8800, total loss = 5.61, predict loss = 1.56 (61.3 examples/sec; 0.065 sec/batch; 2h:33m:32s remains)
INFO - root - 2019-11-06 18:11:58.837334: step 8810, total loss = 3.97, predict loss = 1.14 (60.3 examples/sec; 0.066 sec/batch; 2h:36m:12s remains)
INFO - root - 2019-11-06 18:11:59.565121: step 8820, total loss = 5.57, predict loss = 1.57 (58.8 examples/sec; 0.068 sec/batch; 2h:40m:06s remains)
INFO - root - 2019-11-06 18:12:00.338471: step 8830, total loss = 6.09, predict loss = 1.68 (60.3 examples/sec; 0.066 sec/batch; 2h:36m:01s remains)
INFO - root - 2019-11-06 18:12:01.074675: step 8840, total loss = 6.33, predict loss = 1.90 (70.8 examples/sec; 0.057 sec/batch; 2h:12m:55s remains)
INFO - root - 2019-11-06 18:12:01.647643: step 8850, total loss = 5.17, predict loss = 1.52 (99.8 examples/sec; 0.040 sec/batch; 1h:34m:15s remains)
INFO - root - 2019-11-06 18:12:02.094436: step 8860, total loss = 6.18, predict loss = 1.70 (102.3 examples/sec; 0.039 sec/batch; 1h:31m:59s remains)
INFO - root - 2019-11-06 18:12:02.535980: step 8870, total loss = 3.58, predict loss = 1.10 (125.0 examples/sec; 0.032 sec/batch; 1h:15m:17s remains)
INFO - root - 2019-11-06 18:12:03.841648: step 8880, total loss = 6.80, predict loss = 1.92 (61.6 examples/sec; 0.065 sec/batch; 2h:32m:48s remains)
INFO - root - 2019-11-06 18:12:04.594026: step 8890, total loss = 6.49, predict loss = 1.86 (60.6 examples/sec; 0.066 sec/batch; 2h:35m:06s remains)
INFO - root - 2019-11-06 18:12:05.310384: step 8900, total loss = 4.87, predict loss = 1.44 (66.9 examples/sec; 0.060 sec/batch; 2h:20m:39s remains)
INFO - root - 2019-11-06 18:12:06.070430: step 8910, total loss = 3.53, predict loss = 1.02 (51.7 examples/sec; 0.077 sec/batch; 3h:01m:46s remains)
INFO - root - 2019-11-06 18:12:06.737089: step 8920, total loss = 5.59, predict loss = 1.58 (74.3 examples/sec; 0.054 sec/batch; 2h:06m:33s remains)
INFO - root - 2019-11-06 18:12:07.234756: step 8930, total loss = 4.10, predict loss = 1.19 (97.0 examples/sec; 0.041 sec/batch; 1h:36m:54s remains)
INFO - root - 2019-11-06 18:12:07.695211: step 8940, total loss = 4.79, predict loss = 1.36 (90.0 examples/sec; 0.044 sec/batch; 1h:44m:26s remains)
INFO - root - 2019-11-06 18:12:08.901105: step 8950, total loss = 3.84, predict loss = 1.17 (64.6 examples/sec; 0.062 sec/batch; 2h:25m:39s remains)
INFO - root - 2019-11-06 18:12:09.623424: step 8960, total loss = 6.11, predict loss = 1.77 (62.1 examples/sec; 0.064 sec/batch; 2h:31m:23s remains)
INFO - root - 2019-11-06 18:12:10.379399: step 8970, total loss = 3.51, predict loss = 1.00 (55.5 examples/sec; 0.072 sec/batch; 2h:49m:21s remains)
INFO - root - 2019-11-06 18:12:11.157219: step 8980, total loss = 5.15, predict loss = 1.49 (55.5 examples/sec; 0.072 sec/batch; 2h:49m:19s remains)
INFO - root - 2019-11-06 18:12:11.943748: step 8990, total loss = 4.96, predict loss = 1.43 (59.8 examples/sec; 0.067 sec/batch; 2h:37m:11s remains)
INFO - root - 2019-11-06 18:12:12.487663: step 9000, total loss = 5.15, predict loss = 1.42 (96.7 examples/sec; 0.041 sec/batch; 1h:37m:10s remains)
INFO - root - 2019-11-06 18:12:12.957751: step 9010, total loss = 4.41, predict loss = 1.23 (95.9 examples/sec; 0.042 sec/batch; 1h:37m:59s remains)
INFO - root - 2019-11-06 18:12:14.073870: step 9020, total loss = 3.30, predict loss = 0.99 (5.6 examples/sec; 0.712 sec/batch; 27h:52m:10s remains)
INFO - root - 2019-11-06 18:12:14.773518: step 9030, total loss = 6.45, predict loss = 1.87 (56.3 examples/sec; 0.071 sec/batch; 2h:46m:53s remains)
INFO - root - 2019-11-06 18:12:15.567248: step 9040, total loss = 5.21, predict loss = 1.56 (55.6 examples/sec; 0.072 sec/batch; 2h:48m:55s remains)
INFO - root - 2019-11-06 18:12:16.459182: step 9050, total loss = 5.66, predict loss = 1.70 (49.9 examples/sec; 0.080 sec/batch; 3h:08m:24s remains)
INFO - root - 2019-11-06 18:12:17.222587: step 9060, total loss = 3.21, predict loss = 0.95 (60.1 examples/sec; 0.067 sec/batch; 2h:36m:20s remains)
INFO - root - 2019-11-06 18:12:17.923981: step 9070, total loss = 6.12, predict loss = 1.76 (86.8 examples/sec; 0.046 sec/batch; 1h:48m:17s remains)
INFO - root - 2019-11-06 18:12:18.390627: step 9080, total loss = 5.78, predict loss = 1.63 (100.1 examples/sec; 0.040 sec/batch; 1h:33m:49s remains)
INFO - root - 2019-11-06 18:12:18.860936: step 9090, total loss = 4.24, predict loss = 1.22 (96.9 examples/sec; 0.041 sec/batch; 1h:36m:57s remains)
INFO - root - 2019-11-06 18:12:20.077537: step 9100, total loss = 4.63, predict loss = 1.40 (68.0 examples/sec; 0.059 sec/batch; 2h:18m:03s remains)
INFO - root - 2019-11-06 18:12:20.825754: step 9110, total loss = 5.30, predict loss = 1.43 (52.7 examples/sec; 0.076 sec/batch; 2h:58m:20s remains)
INFO - root - 2019-11-06 18:12:21.610976: step 9120, total loss = 4.46, predict loss = 1.33 (63.3 examples/sec; 0.063 sec/batch; 2h:28m:21s remains)
INFO - root - 2019-11-06 18:12:22.378321: step 9130, total loss = 5.99, predict loss = 1.75 (66.2 examples/sec; 0.060 sec/batch; 2h:21m:53s remains)
INFO - root - 2019-11-06 18:12:23.071819: step 9140, total loss = 6.09, predict loss = 1.76 (67.7 examples/sec; 0.059 sec/batch; 2h:18m:38s remains)
INFO - root - 2019-11-06 18:12:23.599601: step 9150, total loss = 4.42, predict loss = 1.25 (96.5 examples/sec; 0.041 sec/batch; 1h:37m:16s remains)
INFO - root - 2019-11-06 18:12:24.058266: step 9160, total loss = 3.89, predict loss = 1.13 (95.5 examples/sec; 0.042 sec/batch; 1h:38m:19s remains)
INFO - root - 2019-11-06 18:12:25.201456: step 9170, total loss = 5.93, predict loss = 1.67 (71.3 examples/sec; 0.056 sec/batch; 2h:11m:44s remains)
INFO - root - 2019-11-06 18:12:25.929091: step 9180, total loss = 5.84, predict loss = 1.71 (59.1 examples/sec; 0.068 sec/batch; 2h:38m:48s remains)
INFO - root - 2019-11-06 18:12:26.645100: step 9190, total loss = 5.38, predict loss = 1.49 (59.2 examples/sec; 0.068 sec/batch; 2h:38m:33s remains)
INFO - root - 2019-11-06 18:12:27.377814: step 9200, total loss = 4.55, predict loss = 1.32 (56.5 examples/sec; 0.071 sec/batch; 2h:46m:15s remains)
INFO - root - 2019-11-06 18:12:28.129321: step 9210, total loss = 5.32, predict loss = 1.50 (59.0 examples/sec; 0.068 sec/batch; 2h:39m:09s remains)
INFO - root - 2019-11-06 18:12:28.784389: step 9220, total loss = 5.71, predict loss = 1.64 (90.7 examples/sec; 0.044 sec/batch; 1h:43m:27s remains)
INFO - root - 2019-11-06 18:12:29.248352: step 9230, total loss = 4.40, predict loss = 1.34 (95.4 examples/sec; 0.042 sec/batch; 1h:38m:21s remains)
INFO - root - 2019-11-06 18:12:29.734731: step 9240, total loss = 3.66, predict loss = 1.05 (93.2 examples/sec; 0.043 sec/batch; 1h:40m:39s remains)
INFO - root - 2019-11-06 18:12:31.034086: step 9250, total loss = 6.28, predict loss = 1.87 (49.0 examples/sec; 0.082 sec/batch; 3h:11m:19s remains)
INFO - root - 2019-11-06 18:12:31.879072: step 9260, total loss = 5.66, predict loss = 1.62 (46.7 examples/sec; 0.086 sec/batch; 3h:20m:51s remains)
INFO - root - 2019-11-06 18:12:32.704340: step 9270, total loss = 5.38, predict loss = 1.51 (54.3 examples/sec; 0.074 sec/batch; 2h:52m:54s remains)
INFO - root - 2019-11-06 18:12:33.440221: step 9280, total loss = 5.66, predict loss = 1.66 (64.3 examples/sec; 0.062 sec/batch; 2h:25m:58s remains)
INFO - root - 2019-11-06 18:12:34.148905: step 9290, total loss = 5.21, predict loss = 1.53 (67.1 examples/sec; 0.060 sec/batch; 2h:19m:54s remains)
INFO - root - 2019-11-06 18:12:34.654432: step 9300, total loss = 4.89, predict loss = 1.43 (96.6 examples/sec; 0.041 sec/batch; 1h:37m:03s remains)
INFO - root - 2019-11-06 18:12:35.122747: step 9310, total loss = 4.00, predict loss = 1.15 (95.0 examples/sec; 0.042 sec/batch; 1h:38m:46s remains)
INFO - root - 2019-11-06 18:12:36.276550: step 9320, total loss = 3.76, predict loss = 1.12 (66.2 examples/sec; 0.060 sec/batch; 2h:21m:37s remains)
INFO - root - 2019-11-06 18:12:36.982725: step 9330, total loss = 4.14, predict loss = 1.19 (55.3 examples/sec; 0.072 sec/batch; 2h:49m:29s remains)
INFO - root - 2019-11-06 18:12:37.710318: step 9340, total loss = 6.76, predict loss = 1.95 (61.0 examples/sec; 0.066 sec/batch; 2h:33m:40s remains)
INFO - root - 2019-11-06 18:12:38.465398: step 9350, total loss = 6.77, predict loss = 2.00 (53.4 examples/sec; 0.075 sec/batch; 2h:55m:30s remains)
INFO - root - 2019-11-06 18:12:39.196163: step 9360, total loss = 3.44, predict loss = 0.96 (62.0 examples/sec; 0.064 sec/batch; 2h:31m:08s remains)
INFO - root - 2019-11-06 18:12:39.817638: step 9370, total loss = 6.36, predict loss = 1.89 (97.4 examples/sec; 0.041 sec/batch; 1h:36m:17s remains)
INFO - root - 2019-11-06 18:12:40.269850: step 9380, total loss = 6.40, predict loss = 1.80 (93.6 examples/sec; 0.043 sec/batch; 1h:40m:12s remains)
INFO - root - 2019-11-06 18:12:40.731046: step 9390, total loss = 3.84, predict loss = 1.13 (94.3 examples/sec; 0.042 sec/batch; 1h:39m:24s remains)
INFO - root - 2019-11-06 18:12:42.012690: step 9400, total loss = 6.38, predict loss = 1.81 (65.1 examples/sec; 0.061 sec/batch; 2h:24m:03s remains)
INFO - root - 2019-11-06 18:12:42.762566: step 9410, total loss = 5.57, predict loss = 1.68 (50.4 examples/sec; 0.079 sec/batch; 3h:06m:01s remains)
INFO - root - 2019-11-06 18:12:43.519788: step 9420, total loss = 5.16, predict loss = 1.56 (52.0 examples/sec; 0.077 sec/batch; 3h:00m:19s remains)
INFO - root - 2019-11-06 18:12:44.359359: step 9430, total loss = 3.36, predict loss = 1.01 (52.4 examples/sec; 0.076 sec/batch; 2h:58m:44s remains)
INFO - root - 2019-11-06 18:12:45.089574: step 9440, total loss = 3.38, predict loss = 0.99 (73.1 examples/sec; 0.055 sec/batch; 2h:08m:08s remains)
INFO - root - 2019-11-06 18:12:45.620606: step 9450, total loss = 6.78, predict loss = 1.94 (96.1 examples/sec; 0.042 sec/batch; 1h:37m:29s remains)
INFO - root - 2019-11-06 18:12:46.081219: step 9460, total loss = 4.97, predict loss = 1.44 (93.0 examples/sec; 0.043 sec/batch; 1h:40m:44s remains)
INFO - root - 2019-11-06 18:12:47.241307: step 9470, total loss = 4.37, predict loss = 1.29 (69.1 examples/sec; 0.058 sec/batch; 2h:15m:35s remains)
INFO - root - 2019-11-06 18:12:47.988909: step 9480, total loss = 4.40, predict loss = 1.25 (48.6 examples/sec; 0.082 sec/batch; 3h:12m:48s remains)
INFO - root - 2019-11-06 18:12:48.731648: step 9490, total loss = 4.38, predict loss = 1.35 (65.3 examples/sec; 0.061 sec/batch; 2h:23m:26s remains)
INFO - root - 2019-11-06 18:12:49.451494: step 9500, total loss = 6.41, predict loss = 1.75 (67.0 examples/sec; 0.060 sec/batch; 2h:19m:47s remains)
INFO - root - 2019-11-06 18:12:50.242981: step 9510, total loss = 5.71, predict loss = 1.57 (63.7 examples/sec; 0.063 sec/batch; 2h:26m:58s remains)
INFO - root - 2019-11-06 18:12:50.843684: step 9520, total loss = 5.72, predict loss = 1.55 (100.2 examples/sec; 0.040 sec/batch; 1h:33m:30s remains)
INFO - root - 2019-11-06 18:12:51.319180: step 9530, total loss = 4.25, predict loss = 1.11 (95.3 examples/sec; 0.042 sec/batch; 1h:38m:15s remains)
INFO - root - 2019-11-06 18:12:51.777875: step 9540, total loss = 3.87, predict loss = 1.12 (98.0 examples/sec; 0.041 sec/batch; 1h:35m:31s remains)
INFO - root - 2019-11-06 18:12:53.169708: step 9550, total loss = 5.68, predict loss = 1.64 (64.2 examples/sec; 0.062 sec/batch; 2h:25m:46s remains)
INFO - root - 2019-11-06 18:12:53.894071: step 9560, total loss = 4.56, predict loss = 1.30 (57.5 examples/sec; 0.070 sec/batch; 2h:42m:49s remains)
INFO - root - 2019-11-06 18:12:54.661049: step 9570, total loss = 6.14, predict loss = 1.70 (58.9 examples/sec; 0.068 sec/batch; 2h:38m:50s remains)
INFO - root - 2019-11-06 18:12:55.438925: step 9580, total loss = 4.09, predict loss = 1.19 (53.3 examples/sec; 0.075 sec/batch; 2h:55m:41s remains)
INFO - root - 2019-11-06 18:12:56.142057: step 9590, total loss = 4.39, predict loss = 1.27 (70.5 examples/sec; 0.057 sec/batch; 2h:12m:48s remains)
INFO - root - 2019-11-06 18:12:56.636167: step 9600, total loss = 4.98, predict loss = 1.43 (99.1 examples/sec; 0.040 sec/batch; 1h:34m:29s remains)
INFO - root - 2019-11-06 18:12:57.100211: step 9610, total loss = 5.78, predict loss = 1.62 (96.8 examples/sec; 0.041 sec/batch; 1h:36m:40s remains)
INFO - root - 2019-11-06 18:12:58.343858: step 9620, total loss = 4.45, predict loss = 1.30 (72.5 examples/sec; 0.055 sec/batch; 2h:09m:08s remains)
INFO - root - 2019-11-06 18:12:59.043751: step 9630, total loss = 4.23, predict loss = 1.19 (58.1 examples/sec; 0.069 sec/batch; 2h:41m:09s remains)
INFO - root - 2019-11-06 18:12:59.806026: step 9640, total loss = 5.96, predict loss = 1.69 (63.5 examples/sec; 0.063 sec/batch; 2h:27m:26s remains)
INFO - root - 2019-11-06 18:13:00.574472: step 9650, total loss = 5.91, predict loss = 1.71 (60.3 examples/sec; 0.066 sec/batch; 2h:35m:10s remains)
INFO - root - 2019-11-06 18:13:01.303769: step 9660, total loss = 5.74, predict loss = 1.64 (59.6 examples/sec; 0.067 sec/batch; 2h:37m:01s remains)
INFO - root - 2019-11-06 18:13:01.879315: step 9670, total loss = 4.97, predict loss = 1.44 (100.8 examples/sec; 0.040 sec/batch; 1h:32m:46s remains)
INFO - root - 2019-11-06 18:13:02.322907: step 9680, total loss = 5.75, predict loss = 1.65 (86.4 examples/sec; 0.046 sec/batch; 1h:48m:19s remains)
INFO - root - 2019-11-06 18:13:02.804040: step 9690, total loss = 2.43, predict loss = 0.67 (130.2 examples/sec; 0.031 sec/batch; 1h:11m:50s remains)
INFO - root - 2019-11-06 18:13:04.111473: step 9700, total loss = 5.25, predict loss = 1.54 (57.8 examples/sec; 0.069 sec/batch; 2h:41m:52s remains)
INFO - root - 2019-11-06 18:13:04.883722: step 9710, total loss = 5.87, predict loss = 1.69 (58.6 examples/sec; 0.068 sec/batch; 2h:39m:28s remains)
INFO - root - 2019-11-06 18:13:05.674273: step 9720, total loss = 3.96, predict loss = 1.25 (54.0 examples/sec; 0.074 sec/batch; 2h:53m:18s remains)
INFO - root - 2019-11-06 18:13:06.421872: step 9730, total loss = 4.37, predict loss = 1.32 (59.3 examples/sec; 0.067 sec/batch; 2h:37m:46s remains)
INFO - root - 2019-11-06 18:13:07.135448: step 9740, total loss = 4.48, predict loss = 1.25 (71.7 examples/sec; 0.056 sec/batch; 2h:10m:21s remains)
INFO - root - 2019-11-06 18:13:07.612948: step 9750, total loss = 5.67, predict loss = 1.53 (92.8 examples/sec; 0.043 sec/batch; 1h:40m:45s remains)
INFO - root - 2019-11-06 18:13:08.064846: step 9760, total loss = 4.10, predict loss = 1.15 (97.8 examples/sec; 0.041 sec/batch; 1h:35m:33s remains)
INFO - root - 2019-11-06 18:13:09.307546: step 9770, total loss = 5.12, predict loss = 1.49 (57.9 examples/sec; 0.069 sec/batch; 2h:41m:34s remains)
INFO - root - 2019-11-06 18:13:10.071569: step 9780, total loss = 5.45, predict loss = 1.50 (53.4 examples/sec; 0.075 sec/batch; 2h:55m:05s remains)
INFO - root - 2019-11-06 18:13:10.818578: step 9790, total loss = 5.91, predict loss = 1.68 (57.6 examples/sec; 0.069 sec/batch; 2h:42m:14s remains)
INFO - root - 2019-11-06 18:13:11.563190: step 9800, total loss = 4.85, predict loss = 1.51 (51.2 examples/sec; 0.078 sec/batch; 3h:02m:25s remains)
INFO - root - 2019-11-06 18:13:12.333082: step 9810, total loss = 5.65, predict loss = 1.62 (68.3 examples/sec; 0.059 sec/batch; 2h:16m:48s remains)
INFO - root - 2019-11-06 18:13:12.886383: step 9820, total loss = 5.18, predict loss = 1.51 (99.0 examples/sec; 0.040 sec/batch; 1h:34m:23s remains)
INFO - root - 2019-11-06 18:13:13.354361: step 9830, total loss = 3.87, predict loss = 1.10 (92.8 examples/sec; 0.043 sec/batch; 1h:40m:43s remains)
INFO - root - 2019-11-06 18:13:14.502187: step 9840, total loss = 3.19, predict loss = 0.91 (5.4 examples/sec; 0.736 sec/batch; 28h:38m:19s remains)
INFO - root - 2019-11-06 18:13:15.223231: step 9850, total loss = 3.78, predict loss = 1.17 (55.9 examples/sec; 0.072 sec/batch; 2h:47m:01s remains)
INFO - root - 2019-11-06 18:13:15.957026: step 9860, total loss = 6.18, predict loss = 1.77 (58.7 examples/sec; 0.068 sec/batch; 2h:39m:04s remains)
INFO - root - 2019-11-06 18:13:16.694487: step 9870, total loss = 3.44, predict loss = 0.99 (59.1 examples/sec; 0.068 sec/batch; 2h:38m:04s remains)
INFO - root - 2019-11-06 18:13:17.459328: step 9880, total loss = 5.51, predict loss = 1.57 (53.0 examples/sec; 0.076 sec/batch; 2h:56m:20s remains)
INFO - root - 2019-11-06 18:13:18.138020: step 9890, total loss = 6.31, predict loss = 1.72 (84.3 examples/sec; 0.047 sec/batch; 1h:50m:49s remains)
INFO - root - 2019-11-06 18:13:18.621426: step 9900, total loss = 5.31, predict loss = 1.53 (90.7 examples/sec; 0.044 sec/batch; 1h:43m:01s remains)
INFO - root - 2019-11-06 18:13:19.071887: step 9910, total loss = 3.95, predict loss = 1.11 (100.7 examples/sec; 0.040 sec/batch; 1h:32m:44s remains)
INFO - root - 2019-11-06 18:13:20.303754: step 9920, total loss = 4.44, predict loss = 1.30 (67.8 examples/sec; 0.059 sec/batch; 2h:17m:47s remains)
INFO - root - 2019-11-06 18:13:21.060092: step 9930, total loss = 5.15, predict loss = 1.48 (56.7 examples/sec; 0.071 sec/batch; 2h:44m:47s remains)
INFO - root - 2019-11-06 18:13:21.819267: step 9940, total loss = 4.88, predict loss = 1.40 (61.3 examples/sec; 0.065 sec/batch; 2h:32m:17s remains)
INFO - root - 2019-11-06 18:13:22.573592: step 9950, total loss = 4.03, predict loss = 1.21 (55.5 examples/sec; 0.072 sec/batch; 2h:48m:05s remains)
INFO - root - 2019-11-06 18:13:23.301561: step 9960, total loss = 5.72, predict loss = 1.66 (63.3 examples/sec; 0.063 sec/batch; 2h:27m:26s remains)
INFO - root - 2019-11-06 18:13:23.862684: step 9970, total loss = 3.92, predict loss = 1.13 (98.6 examples/sec; 0.041 sec/batch; 1h:34m:42s remains)
INFO - root - 2019-11-06 18:13:24.324265: step 9980, total loss = 4.83, predict loss = 1.35 (96.3 examples/sec; 0.042 sec/batch; 1h:36m:57s remains)
INFO - root - 2019-11-06 18:13:25.463165: step 9990, total loss = 6.66, predict loss = 1.84 (68.5 examples/sec; 0.058 sec/batch; 2h:16m:13s remains)
INFO - root - 2019-11-06 18:13:26.151628: step 10000, total loss = 4.06, predict loss = 1.18 (59.3 examples/sec; 0.067 sec/batch; 2h:37m:29s remains)
INFO - root - 2019-11-06 18:13:26.882470: step 10010, total loss = 5.39, predict loss = 1.53 (63.0 examples/sec; 0.064 sec/batch; 2h:28m:14s remains)
INFO - root - 2019-11-06 18:13:27.650277: step 10020, total loss = 5.58, predict loss = 1.61 (65.5 examples/sec; 0.061 sec/batch; 2h:22m:23s remains)
INFO - root - 2019-11-06 18:13:28.383722: step 10030, total loss = 4.38, predict loss = 1.26 (60.4 examples/sec; 0.066 sec/batch; 2h:34m:26s remains)
INFO - root - 2019-11-06 18:13:29.003182: step 10040, total loss = 4.77, predict loss = 1.36 (95.1 examples/sec; 0.042 sec/batch; 1h:38m:06s remains)
INFO - root - 2019-11-06 18:13:29.468337: step 10050, total loss = 5.14, predict loss = 1.50 (100.8 examples/sec; 0.040 sec/batch; 1h:32m:36s remains)
INFO - root - 2019-11-06 18:13:29.919572: step 10060, total loss = 5.68, predict loss = 1.69 (99.8 examples/sec; 0.040 sec/batch; 1h:33m:30s remains)
INFO - root - 2019-11-06 18:13:31.187190: step 10070, total loss = 4.14, predict loss = 1.19 (56.6 examples/sec; 0.071 sec/batch; 2h:44m:50s remains)
INFO - root - 2019-11-06 18:13:31.995927: step 10080, total loss = 4.14, predict loss = 1.23 (50.7 examples/sec; 0.079 sec/batch; 3h:03m:50s remains)
INFO - root - 2019-11-06 18:13:32.736176: step 10090, total loss = 4.46, predict loss = 1.24 (64.1 examples/sec; 0.062 sec/batch; 2h:25m:32s remains)
INFO - root - 2019-11-06 18:13:33.510685: step 10100, total loss = 4.77, predict loss = 1.36 (63.5 examples/sec; 0.063 sec/batch; 2h:26m:58s remains)
INFO - root - 2019-11-06 18:13:34.190930: step 10110, total loss = 5.88, predict loss = 1.76 (72.2 examples/sec; 0.055 sec/batch; 2h:09m:10s remains)
INFO - root - 2019-11-06 18:13:34.684395: step 10120, total loss = 3.86, predict loss = 1.15 (102.5 examples/sec; 0.039 sec/batch; 1h:31m:00s remains)
INFO - root - 2019-11-06 18:13:35.142456: step 10130, total loss = 5.87, predict loss = 1.53 (101.9 examples/sec; 0.039 sec/batch; 1h:31m:29s remains)
INFO - root - 2019-11-06 18:13:36.297158: step 10140, total loss = 5.92, predict loss = 1.75 (70.0 examples/sec; 0.057 sec/batch; 2h:13m:07s remains)
INFO - root - 2019-11-06 18:13:37.003952: step 10150, total loss = 5.08, predict loss = 1.46 (54.5 examples/sec; 0.073 sec/batch; 2h:50m:55s remains)
INFO - root - 2019-11-06 18:13:37.736076: step 10160, total loss = 4.48, predict loss = 1.32 (66.2 examples/sec; 0.060 sec/batch; 2h:20m:43s remains)
INFO - root - 2019-11-06 18:13:38.464573: step 10170, total loss = 3.80, predict loss = 1.11 (56.6 examples/sec; 0.071 sec/batch; 2h:44m:37s remains)
INFO - root - 2019-11-06 18:13:39.280715: step 10180, total loss = 2.90, predict loss = 0.87 (58.4 examples/sec; 0.068 sec/batch; 2h:39m:35s remains)
INFO - root - 2019-11-06 18:13:39.860834: step 10190, total loss = 6.15, predict loss = 1.77 (105.3 examples/sec; 0.038 sec/batch; 1h:28m:32s remains)
INFO - root - 2019-11-06 18:13:40.297380: step 10200, total loss = 4.38, predict loss = 1.27 (105.6 examples/sec; 0.038 sec/batch; 1h:28m:15s remains)
INFO - root - 2019-11-06 18:13:40.760646: step 10210, total loss = 5.68, predict loss = 1.64 (100.3 examples/sec; 0.040 sec/batch; 1h:32m:56s remains)
INFO - root - 2019-11-06 18:13:42.083387: step 10220, total loss = 6.43, predict loss = 1.86 (55.5 examples/sec; 0.072 sec/batch; 2h:47m:58s remains)
INFO - root - 2019-11-06 18:13:42.821991: step 10230, total loss = 5.11, predict loss = 1.37 (59.7 examples/sec; 0.067 sec/batch; 2h:36m:03s remains)
INFO - root - 2019-11-06 18:13:43.560296: step 10240, total loss = 5.33, predict loss = 1.55 (59.9 examples/sec; 0.067 sec/batch; 2h:35m:31s remains)
INFO - root - 2019-11-06 18:13:44.341734: step 10250, total loss = 5.43, predict loss = 1.54 (52.9 examples/sec; 0.076 sec/batch; 2h:56m:10s remains)
INFO - root - 2019-11-06 18:13:45.074310: step 10260, total loss = 5.32, predict loss = 1.54 (73.3 examples/sec; 0.055 sec/batch; 2h:07m:02s remains)
INFO - root - 2019-11-06 18:13:45.562674: step 10270, total loss = 5.61, predict loss = 1.58 (97.8 examples/sec; 0.041 sec/batch; 1h:35m:15s remains)
INFO - root - 2019-11-06 18:13:46.026397: step 10280, total loss = 5.08, predict loss = 1.44 (88.8 examples/sec; 0.045 sec/batch; 1h:44m:55s remains)
INFO - root - 2019-11-06 18:13:47.231494: step 10290, total loss = 4.46, predict loss = 1.35 (69.0 examples/sec; 0.058 sec/batch; 2h:14m:59s remains)
INFO - root - 2019-11-06 18:13:47.956447: step 10300, total loss = 5.63, predict loss = 1.55 (63.3 examples/sec; 0.063 sec/batch; 2h:27m:08s remains)
INFO - root - 2019-11-06 18:13:48.695779: step 10310, total loss = 4.20, predict loss = 1.22 (61.4 examples/sec; 0.065 sec/batch; 2h:31m:35s remains)
INFO - root - 2019-11-06 18:13:49.471629: step 10320, total loss = 6.02, predict loss = 1.77 (55.4 examples/sec; 0.072 sec/batch; 2h:48m:08s remains)
INFO - root - 2019-11-06 18:13:50.216264: step 10330, total loss = 5.04, predict loss = 1.40 (64.6 examples/sec; 0.062 sec/batch; 2h:24m:05s remains)
INFO - root - 2019-11-06 18:13:50.814743: step 10340, total loss = 5.90, predict loss = 1.61 (104.2 examples/sec; 0.038 sec/batch; 1h:29m:23s remains)
INFO - root - 2019-11-06 18:13:51.257886: step 10350, total loss = 5.78, predict loss = 1.63 (99.3 examples/sec; 0.040 sec/batch; 1h:33m:44s remains)
INFO - root - 2019-11-06 18:13:51.711856: step 10360, total loss = 4.11, predict loss = 1.18 (100.2 examples/sec; 0.040 sec/batch; 1h:32m:57s remains)
INFO - root - 2019-11-06 18:13:53.122077: step 10370, total loss = 5.21, predict loss = 1.51 (66.8 examples/sec; 0.060 sec/batch; 2h:19m:20s remains)
INFO - root - 2019-11-06 18:13:53.826510: step 10380, total loss = 4.11, predict loss = 1.21 (55.2 examples/sec; 0.073 sec/batch; 2h:48m:44s remains)
INFO - root - 2019-11-06 18:13:54.550057: step 10390, total loss = 5.08, predict loss = 1.53 (61.4 examples/sec; 0.065 sec/batch; 2h:31m:30s remains)
INFO - root - 2019-11-06 18:13:55.297421: step 10400, total loss = 6.50, predict loss = 1.80 (62.7 examples/sec; 0.064 sec/batch; 2h:28m:31s remains)
INFO - root - 2019-11-06 18:13:55.985162: step 10410, total loss = 5.18, predict loss = 1.43 (70.1 examples/sec; 0.057 sec/batch; 2h:12m:42s remains)
INFO - root - 2019-11-06 18:13:56.490745: step 10420, total loss = 4.90, predict loss = 1.45 (98.4 examples/sec; 0.041 sec/batch; 1h:34m:32s remains)
INFO - root - 2019-11-06 18:13:56.941457: step 10430, total loss = 4.36, predict loss = 1.31 (98.0 examples/sec; 0.041 sec/batch; 1h:34m:57s remains)
INFO - root - 2019-11-06 18:13:58.130929: step 10440, total loss = 4.58, predict loss = 1.36 (68.8 examples/sec; 0.058 sec/batch; 2h:15m:08s remains)
INFO - root - 2019-11-06 18:13:58.883213: step 10450, total loss = 5.66, predict loss = 1.67 (60.0 examples/sec; 0.067 sec/batch; 2h:34m:58s remains)
INFO - root - 2019-11-06 18:13:59.628500: step 10460, total loss = 6.14, predict loss = 1.86 (63.2 examples/sec; 0.063 sec/batch; 2h:27m:15s remains)
INFO - root - 2019-11-06 18:14:00.361664: step 10470, total loss = 5.76, predict loss = 1.66 (60.5 examples/sec; 0.066 sec/batch; 2h:33m:42s remains)
INFO - root - 2019-11-06 18:14:01.115001: step 10480, total loss = 3.66, predict loss = 1.04 (57.2 examples/sec; 0.070 sec/batch; 2h:42m:38s remains)
INFO - root - 2019-11-06 18:14:01.681050: step 10490, total loss = 5.28, predict loss = 1.56 (101.9 examples/sec; 0.039 sec/batch; 1h:31m:14s remains)
INFO - root - 2019-11-06 18:14:02.115399: step 10500, total loss = 4.03, predict loss = 1.16 (97.2 examples/sec; 0.041 sec/batch; 1h:35m:41s remains)
INFO - root - 2019-11-06 18:14:02.557577: step 10510, total loss = 6.10, predict loss = 1.83 (133.5 examples/sec; 0.030 sec/batch; 1h:09m:39s remains)
INFO - root - 2019-11-06 18:14:03.939668: step 10520, total loss = 5.95, predict loss = 1.67 (62.2 examples/sec; 0.064 sec/batch; 2h:29m:33s remains)
INFO - root - 2019-11-06 18:14:04.645274: step 10530, total loss = 4.39, predict loss = 1.25 (62.9 examples/sec; 0.064 sec/batch; 2h:27m:49s remains)
INFO - root - 2019-11-06 18:14:05.403378: step 10540, total loss = 3.49, predict loss = 1.00 (49.8 examples/sec; 0.080 sec/batch; 3h:06m:51s remains)
INFO - root - 2019-11-06 18:14:06.164917: step 10550, total loss = 4.36, predict loss = 1.20 (54.0 examples/sec; 0.074 sec/batch; 2h:52m:09s remains)
INFO - root - 2019-11-06 18:14:06.853853: step 10560, total loss = 5.19, predict loss = 1.48 (67.0 examples/sec; 0.060 sec/batch; 2h:18m:50s remains)
INFO - root - 2019-11-06 18:14:07.364383: step 10570, total loss = 6.06, predict loss = 1.69 (97.6 examples/sec; 0.041 sec/batch; 1h:35m:16s remains)
INFO - root - 2019-11-06 18:14:07.830760: step 10580, total loss = 4.64, predict loss = 1.32 (99.4 examples/sec; 0.040 sec/batch; 1h:33m:31s remains)
INFO - root - 2019-11-06 18:14:09.027799: step 10590, total loss = 3.34, predict loss = 0.96 (64.9 examples/sec; 0.062 sec/batch; 2h:23m:07s remains)
INFO - root - 2019-11-06 18:14:09.773013: step 10600, total loss = 5.63, predict loss = 1.57 (56.2 examples/sec; 0.071 sec/batch; 2h:45m:30s remains)
INFO - root - 2019-11-06 18:14:10.547155: step 10610, total loss = 4.80, predict loss = 1.43 (52.2 examples/sec; 0.077 sec/batch; 2h:57m:57s remains)
INFO - root - 2019-11-06 18:14:11.340952: step 10620, total loss = 5.94, predict loss = 1.71 (58.2 examples/sec; 0.069 sec/batch; 2h:39m:42s remains)
INFO - root - 2019-11-06 18:14:12.060652: step 10630, total loss = 5.02, predict loss = 1.46 (72.5 examples/sec; 0.055 sec/batch; 2h:08m:11s remains)
INFO - root - 2019-11-06 18:14:12.645473: step 10640, total loss = 4.72, predict loss = 1.35 (93.8 examples/sec; 0.043 sec/batch; 1h:39m:04s remains)
INFO - root - 2019-11-06 18:14:13.134480: step 10650, total loss = 3.03, predict loss = 0.90 (88.7 examples/sec; 0.045 sec/batch; 1h:44m:46s remains)
INFO - root - 2019-11-06 18:14:14.263669: step 10660, total loss = 4.65, predict loss = 1.27 (5.6 examples/sec; 0.717 sec/batch; 27h:45m:21s remains)
INFO - root - 2019-11-06 18:14:14.978866: step 10670, total loss = 3.93, predict loss = 1.17 (57.7 examples/sec; 0.069 sec/batch; 2h:40m:55s remains)
INFO - root - 2019-11-06 18:14:15.737529: step 10680, total loss = 5.51, predict loss = 1.50 (61.3 examples/sec; 0.065 sec/batch; 2h:31m:35s remains)
INFO - root - 2019-11-06 18:14:16.488280: step 10690, total loss = 4.78, predict loss = 1.38 (59.6 examples/sec; 0.067 sec/batch; 2h:35m:57s remains)
INFO - root - 2019-11-06 18:14:17.240260: step 10700, total loss = 5.28, predict loss = 1.53 (63.3 examples/sec; 0.063 sec/batch; 2h:26m:46s remains)
INFO - root - 2019-11-06 18:14:17.915256: step 10710, total loss = 5.37, predict loss = 1.56 (89.1 examples/sec; 0.045 sec/batch; 1h:44m:15s remains)
INFO - root - 2019-11-06 18:14:18.347696: step 10720, total loss = 4.28, predict loss = 1.22 (95.9 examples/sec; 0.042 sec/batch; 1h:36m:50s remains)
INFO - root - 2019-11-06 18:14:18.828511: step 10730, total loss = 4.34, predict loss = 1.31 (97.5 examples/sec; 0.041 sec/batch; 1h:35m:12s remains)
INFO - root - 2019-11-06 18:14:20.023628: step 10740, total loss = 4.67, predict loss = 1.42 (63.6 examples/sec; 0.063 sec/batch; 2h:25m:56s remains)
INFO - root - 2019-11-06 18:14:20.745389: step 10750, total loss = 5.75, predict loss = 1.67 (57.5 examples/sec; 0.070 sec/batch; 2h:41m:28s remains)
INFO - root - 2019-11-06 18:14:21.484646: step 10760, total loss = 5.20, predict loss = 1.52 (56.7 examples/sec; 0.070 sec/batch; 2h:43m:34s remains)
INFO - root - 2019-11-06 18:14:22.271123: step 10770, total loss = 4.65, predict loss = 1.31 (68.6 examples/sec; 0.058 sec/batch; 2h:15m:16s remains)
INFO - root - 2019-11-06 18:14:22.960982: step 10780, total loss = 5.64, predict loss = 1.58 (76.1 examples/sec; 0.053 sec/batch; 2h:01m:57s remains)
INFO - root - 2019-11-06 18:14:23.475397: step 10790, total loss = 4.76, predict loss = 1.35 (98.3 examples/sec; 0.041 sec/batch; 1h:34m:25s remains)
INFO - root - 2019-11-06 18:14:23.930488: step 10800, total loss = 6.07, predict loss = 1.72 (98.6 examples/sec; 0.041 sec/batch; 1h:34m:09s remains)
INFO - root - 2019-11-06 18:14:25.060251: step 10810, total loss = 5.43, predict loss = 1.57 (68.3 examples/sec; 0.059 sec/batch; 2h:15m:57s remains)
INFO - root - 2019-11-06 18:14:25.738802: step 10820, total loss = 5.29, predict loss = 1.47 (57.6 examples/sec; 0.069 sec/batch; 2h:40m:59s remains)
INFO - root - 2019-11-06 18:14:26.450507: step 10830, total loss = 5.87, predict loss = 1.73 (63.0 examples/sec; 0.064 sec/batch; 2h:27m:17s remains)
INFO - root - 2019-11-06 18:14:27.201144: step 10840, total loss = 4.47, predict loss = 1.33 (62.6 examples/sec; 0.064 sec/batch; 2h:28m:08s remains)
INFO - root - 2019-11-06 18:14:27.956378: step 10850, total loss = 6.12, predict loss = 1.82 (53.6 examples/sec; 0.075 sec/batch; 2h:52m:56s remains)
INFO - root - 2019-11-06 18:14:28.596057: step 10860, total loss = 6.45, predict loss = 1.85 (86.5 examples/sec; 0.046 sec/batch; 1h:47m:15s remains)
INFO - root - 2019-11-06 18:14:29.036142: step 10870, total loss = 4.70, predict loss = 1.41 (94.3 examples/sec; 0.042 sec/batch; 1h:38m:22s remains)
INFO - root - 2019-11-06 18:14:29.508181: step 10880, total loss = 4.49, predict loss = 1.36 (89.1 examples/sec; 0.045 sec/batch; 1h:44m:04s remains)
INFO - root - 2019-11-06 18:14:30.843203: step 10890, total loss = 5.71, predict loss = 1.64 (60.6 examples/sec; 0.066 sec/batch; 2h:32m:58s remains)
INFO - root - 2019-11-06 18:14:31.579605: step 10900, total loss = 5.17, predict loss = 1.53 (56.6 examples/sec; 0.071 sec/batch; 2h:43m:56s remains)
INFO - root - 2019-11-06 18:14:32.373386: step 10910, total loss = 5.30, predict loss = 1.57 (43.7 examples/sec; 0.092 sec/batch; 3h:32m:10s remains)
INFO - root - 2019-11-06 18:14:33.142332: step 10920, total loss = 5.44, predict loss = 1.56 (55.7 examples/sec; 0.072 sec/batch; 2h:46m:36s remains)
INFO - root - 2019-11-06 18:14:33.843238: step 10930, total loss = 6.88, predict loss = 1.95 (78.1 examples/sec; 0.051 sec/batch; 1h:58m:46s remains)
INFO - root - 2019-11-06 18:14:34.352144: step 10940, total loss = 4.82, predict loss = 1.44 (96.1 examples/sec; 0.042 sec/batch; 1h:36m:26s remains)
INFO - root - 2019-11-06 18:14:34.807004: step 10950, total loss = 4.69, predict loss = 1.36 (95.1 examples/sec; 0.042 sec/batch; 1h:37m:30s remains)
INFO - root - 2019-11-06 18:14:35.977059: step 10960, total loss = 5.84, predict loss = 1.66 (67.9 examples/sec; 0.059 sec/batch; 2h:16m:28s remains)
INFO - root - 2019-11-06 18:14:36.688816: step 10970, total loss = 5.07, predict loss = 1.48 (57.5 examples/sec; 0.070 sec/batch; 2h:41m:11s remains)
INFO - root - 2019-11-06 18:14:37.412198: step 10980, total loss = 5.12, predict loss = 1.48 (67.8 examples/sec; 0.059 sec/batch; 2h:16m:46s remains)
INFO - root - 2019-11-06 18:14:38.120571: step 10990, total loss = 4.50, predict loss = 1.26 (56.3 examples/sec; 0.071 sec/batch; 2h:44m:35s remains)
INFO - root - 2019-11-06 18:14:38.876248: step 11000, total loss = 5.76, predict loss = 1.64 (53.6 examples/sec; 0.075 sec/batch; 2h:52m:50s remains)
INFO - root - 2019-11-06 18:14:39.480483: step 11010, total loss = 3.95, predict loss = 1.14 (99.4 examples/sec; 0.040 sec/batch; 1h:33m:15s remains)
INFO - root - 2019-11-06 18:14:39.950225: step 11020, total loss = 5.74, predict loss = 1.63 (102.9 examples/sec; 0.039 sec/batch; 1h:30m:01s remains)
INFO - root - 2019-11-06 18:14:40.403779: step 11030, total loss = 4.51, predict loss = 1.30 (99.8 examples/sec; 0.040 sec/batch; 1h:32m:51s remains)
INFO - root - 2019-11-06 18:14:41.714325: step 11040, total loss = 3.99, predict loss = 1.12 (60.8 examples/sec; 0.066 sec/batch; 2h:32m:28s remains)
INFO - root - 2019-11-06 18:14:42.473648: step 11050, total loss = 5.20, predict loss = 1.55 (61.8 examples/sec; 0.065 sec/batch; 2h:29m:47s remains)
INFO - root - 2019-11-06 18:14:43.234104: step 11060, total loss = 5.28, predict loss = 1.55 (52.8 examples/sec; 0.076 sec/batch; 2h:55m:22s remains)
INFO - root - 2019-11-06 18:14:43.978442: step 11070, total loss = 5.43, predict loss = 1.57 (55.9 examples/sec; 0.072 sec/batch; 2h:45m:34s remains)
INFO - root - 2019-11-06 18:14:44.672058: step 11080, total loss = 4.06, predict loss = 1.17 (71.1 examples/sec; 0.056 sec/batch; 2h:10m:17s remains)
INFO - root - 2019-11-06 18:14:45.195145: step 11090, total loss = 5.77, predict loss = 1.61 (97.7 examples/sec; 0.041 sec/batch; 1h:34m:44s remains)
INFO - root - 2019-11-06 18:14:45.665809: step 11100, total loss = 4.54, predict loss = 1.31 (89.7 examples/sec; 0.045 sec/batch; 1h:43m:14s remains)
INFO - root - 2019-11-06 18:14:46.826954: step 11110, total loss = 3.55, predict loss = 1.00 (70.7 examples/sec; 0.057 sec/batch; 2h:10m:56s remains)
INFO - root - 2019-11-06 18:14:47.529886: step 11120, total loss = 5.09, predict loss = 1.51 (61.9 examples/sec; 0.065 sec/batch; 2h:29m:39s remains)
INFO - root - 2019-11-06 18:14:48.313054: step 11130, total loss = 5.58, predict loss = 1.60 (59.2 examples/sec; 0.068 sec/batch; 2h:36m:27s remains)
INFO - root - 2019-11-06 18:14:49.041196: step 11140, total loss = 5.55, predict loss = 1.57 (59.9 examples/sec; 0.067 sec/batch; 2h:34m:39s remains)
INFO - root - 2019-11-06 18:14:49.787180: step 11150, total loss = 5.39, predict loss = 1.56 (55.6 examples/sec; 0.072 sec/batch; 2h:46m:21s remains)
INFO - root - 2019-11-06 18:14:50.400980: step 11160, total loss = 4.40, predict loss = 1.35 (96.5 examples/sec; 0.041 sec/batch; 1h:35m:52s remains)
INFO - root - 2019-11-06 18:14:50.904212: step 11170, total loss = 6.59, predict loss = 1.92 (89.6 examples/sec; 0.045 sec/batch; 1h:43m:20s remains)
INFO - root - 2019-11-06 18:14:51.361474: step 11180, total loss = 5.84, predict loss = 1.72 (95.5 examples/sec; 0.042 sec/batch; 1h:36m:53s remains)
INFO - root - 2019-11-06 18:14:52.822219: step 11190, total loss = 4.07, predict loss = 1.19 (50.7 examples/sec; 0.079 sec/batch; 3h:02m:29s remains)
INFO - root - 2019-11-06 18:14:53.600597: step 11200, total loss = 5.98, predict loss = 1.65 (55.5 examples/sec; 0.072 sec/batch; 2h:46m:50s remains)
INFO - root - 2019-11-06 18:14:54.374871: step 11210, total loss = 5.60, predict loss = 1.59 (56.0 examples/sec; 0.071 sec/batch; 2h:45m:18s remains)
INFO - root - 2019-11-06 18:14:55.149501: step 11220, total loss = 5.49, predict loss = 1.54 (46.0 examples/sec; 0.087 sec/batch; 3h:21m:05s remains)
INFO - root - 2019-11-06 18:14:55.873375: step 11230, total loss = 6.30, predict loss = 1.79 (62.3 examples/sec; 0.064 sec/batch; 2h:28m:33s remains)
INFO - root - 2019-11-06 18:14:56.387650: step 11240, total loss = 3.76, predict loss = 1.16 (94.0 examples/sec; 0.043 sec/batch; 1h:38m:26s remains)
INFO - root - 2019-11-06 18:14:56.861912: step 11250, total loss = 4.88, predict loss = 1.35 (96.4 examples/sec; 0.042 sec/batch; 1h:35m:59s remains)
INFO - root - 2019-11-06 18:14:58.040148: step 11260, total loss = 4.21, predict loss = 1.24 (66.7 examples/sec; 0.060 sec/batch; 2h:18m:43s remains)
INFO - root - 2019-11-06 18:14:58.750206: step 11270, total loss = 5.57, predict loss = 1.50 (64.5 examples/sec; 0.062 sec/batch; 2h:23m:27s remains)
INFO - root - 2019-11-06 18:14:59.462448: step 11280, total loss = 4.49, predict loss = 1.33 (61.4 examples/sec; 0.065 sec/batch; 2h:30m:31s remains)
INFO - root - 2019-11-06 18:15:00.231770: step 11290, total loss = 3.72, predict loss = 1.10 (60.0 examples/sec; 0.067 sec/batch; 2h:34m:05s remains)
INFO - root - 2019-11-06 18:15:00.946459: step 11300, total loss = 4.62, predict loss = 1.37 (56.5 examples/sec; 0.071 sec/batch; 2h:43m:38s remains)
INFO - root - 2019-11-06 18:15:01.548884: step 11310, total loss = 4.89, predict loss = 1.43 (98.8 examples/sec; 0.041 sec/batch; 1h:33m:37s remains)
INFO - root - 2019-11-06 18:15:02.027309: step 11320, total loss = 5.44, predict loss = 1.64 (93.5 examples/sec; 0.043 sec/batch; 1h:38m:54s remains)
INFO - root - 2019-11-06 18:15:02.486038: step 11330, total loss = 4.51, predict loss = 1.27 (125.5 examples/sec; 0.032 sec/batch; 1h:13m:39s remains)
INFO - root - 2019-11-06 18:15:03.864095: step 11340, total loss = 3.58, predict loss = 1.01 (54.9 examples/sec; 0.073 sec/batch; 2h:48m:20s remains)
INFO - root - 2019-11-06 18:15:04.606620: step 11350, total loss = 2.89, predict loss = 0.78 (58.7 examples/sec; 0.068 sec/batch; 2h:37m:34s remains)
INFO - root - 2019-11-06 18:15:05.367910: step 11360, total loss = 6.45, predict loss = 1.98 (57.2 examples/sec; 0.070 sec/batch; 2h:41m:33s remains)
INFO - root - 2019-11-06 18:15:06.122547: step 11370, total loss = 5.50, predict loss = 1.51 (63.0 examples/sec; 0.064 sec/batch; 2h:26m:46s remains)
INFO - root - 2019-11-06 18:15:06.866094: step 11380, total loss = 4.79, predict loss = 1.38 (66.7 examples/sec; 0.060 sec/batch; 2h:18m:38s remains)
INFO - root - 2019-11-06 18:15:07.353409: step 11390, total loss = 4.90, predict loss = 1.35 (94.2 examples/sec; 0.042 sec/batch; 1h:38m:06s remains)
INFO - root - 2019-11-06 18:15:07.821209: step 11400, total loss = 5.09, predict loss = 1.43 (85.3 examples/sec; 0.047 sec/batch; 1h:48m:16s remains)
INFO - root - 2019-11-06 18:15:09.075835: step 11410, total loss = 5.29, predict loss = 1.48 (62.5 examples/sec; 0.064 sec/batch; 2h:27m:51s remains)
INFO - root - 2019-11-06 18:15:09.835785: step 11420, total loss = 5.37, predict loss = 1.55 (58.1 examples/sec; 0.069 sec/batch; 2h:38m:56s remains)
INFO - root - 2019-11-06 18:15:10.554383: step 11430, total loss = 3.87, predict loss = 1.14 (59.6 examples/sec; 0.067 sec/batch; 2h:35m:04s remains)
INFO - root - 2019-11-06 18:15:11.309981: step 11440, total loss = 5.66, predict loss = 1.59 (54.2 examples/sec; 0.074 sec/batch; 2h:50m:16s remains)
INFO - root - 2019-11-06 18:15:12.072037: step 11450, total loss = 5.37, predict loss = 1.63 (63.0 examples/sec; 0.064 sec/batch; 2h:26m:40s remains)
INFO - root - 2019-11-06 18:15:12.621678: step 11460, total loss = 5.57, predict loss = 1.61 (96.8 examples/sec; 0.041 sec/batch; 1h:35m:22s remains)
INFO - root - 2019-11-06 18:15:13.089959: step 11470, total loss = 4.85, predict loss = 1.45 (90.1 examples/sec; 0.044 sec/batch; 1h:42m:32s remains)
INFO - root - 2019-11-06 18:15:14.253193: step 11480, total loss = 5.85, predict loss = 1.66 (5.3 examples/sec; 0.750 sec/batch; 28h:50m:32s remains)
INFO - root - 2019-11-06 18:15:14.959535: step 11490, total loss = 5.08, predict loss = 1.53 (63.8 examples/sec; 0.063 sec/batch; 2h:24m:41s remains)
INFO - root - 2019-11-06 18:15:15.717619: step 11500, total loss = 5.37, predict loss = 1.47 (58.8 examples/sec; 0.068 sec/batch; 2h:37m:02s remains)
INFO - root - 2019-11-06 18:15:16.525902: step 11510, total loss = 6.14, predict loss = 1.85 (53.5 examples/sec; 0.075 sec/batch; 2h:52m:30s remains)
INFO - root - 2019-11-06 18:15:17.264517: step 11520, total loss = 5.55, predict loss = 1.61 (56.9 examples/sec; 0.070 sec/batch; 2h:42m:07s remains)
INFO - root - 2019-11-06 18:15:17.932490: step 11530, total loss = 5.43, predict loss = 1.58 (94.7 examples/sec; 0.042 sec/batch; 1h:37m:27s remains)
INFO - root - 2019-11-06 18:15:18.365663: step 11540, total loss = 4.41, predict loss = 1.27 (101.3 examples/sec; 0.039 sec/batch; 1h:31m:08s remains)
INFO - root - 2019-11-06 18:15:18.810845: step 11550, total loss = 5.34, predict loss = 1.50 (95.4 examples/sec; 0.042 sec/batch; 1h:36m:46s remains)
INFO - root - 2019-11-06 18:15:20.093463: step 11560, total loss = 5.17, predict loss = 1.51 (69.8 examples/sec; 0.057 sec/batch; 2h:12m:08s remains)
INFO - root - 2019-11-06 18:15:20.884431: step 11570, total loss = 4.93, predict loss = 1.39 (57.2 examples/sec; 0.070 sec/batch; 2h:41m:22s remains)
INFO - root - 2019-11-06 18:15:21.632998: step 11580, total loss = 4.52, predict loss = 1.34 (64.1 examples/sec; 0.062 sec/batch; 2h:24m:03s remains)
INFO - root - 2019-11-06 18:15:22.414830: step 11590, total loss = 5.42, predict loss = 1.57 (66.8 examples/sec; 0.060 sec/batch; 2h:18m:07s remains)
INFO - root - 2019-11-06 18:15:23.128144: step 11600, total loss = 4.85, predict loss = 1.42 (62.3 examples/sec; 0.064 sec/batch; 2h:28m:07s remains)
INFO - root - 2019-11-06 18:15:23.685729: step 11610, total loss = 5.59, predict loss = 1.60 (99.9 examples/sec; 0.040 sec/batch; 1h:32m:19s remains)
INFO - root - 2019-11-06 18:15:24.134266: step 11620, total loss = 3.88, predict loss = 1.11 (96.5 examples/sec; 0.041 sec/batch; 1h:35m:35s remains)
INFO - root - 2019-11-06 18:15:25.288950: step 11630, total loss = 4.96, predict loss = 1.44 (66.6 examples/sec; 0.060 sec/batch; 2h:18m:25s remains)
INFO - root - 2019-11-06 18:15:26.010025: step 11640, total loss = 5.41, predict loss = 1.54 (61.2 examples/sec; 0.065 sec/batch; 2h:30m:38s remains)
INFO - root - 2019-11-06 18:15:26.763821: step 11650, total loss = 5.40, predict loss = 1.57 (54.8 examples/sec; 0.073 sec/batch; 2h:48m:15s remains)
INFO - root - 2019-11-06 18:15:27.523068: step 11660, total loss = 4.63, predict loss = 1.36 (65.5 examples/sec; 0.061 sec/batch; 2h:20m:46s remains)
INFO - root - 2019-11-06 18:15:28.285134: step 11670, total loss = 3.52, predict loss = 0.97 (58.9 examples/sec; 0.068 sec/batch; 2h:36m:36s remains)
INFO - root - 2019-11-06 18:15:28.949014: step 11680, total loss = 3.68, predict loss = 1.12 (92.2 examples/sec; 0.043 sec/batch; 1h:40m:01s remains)
INFO - root - 2019-11-06 18:15:29.414167: step 11690, total loss = 5.67, predict loss = 1.59 (99.8 examples/sec; 0.040 sec/batch; 1h:32m:25s remains)
INFO - root - 2019-11-06 18:15:29.863155: step 11700, total loss = 5.65, predict loss = 1.60 (96.8 examples/sec; 0.041 sec/batch; 1h:35m:12s remains)
INFO - root - 2019-11-06 18:15:31.116875: step 11710, total loss = 2.76, predict loss = 0.87 (70.4 examples/sec; 0.057 sec/batch; 2h:10m:59s remains)
INFO - root - 2019-11-06 18:15:31.827658: step 11720, total loss = 5.00, predict loss = 1.38 (62.6 examples/sec; 0.064 sec/batch; 2h:27m:16s remains)
INFO - root - 2019-11-06 18:15:32.617091: step 11730, total loss = 3.92, predict loss = 1.23 (63.8 examples/sec; 0.063 sec/batch; 2h:24m:33s remains)
INFO - root - 2019-11-06 18:15:33.343382: step 11740, total loss = 5.08, predict loss = 1.51 (56.1 examples/sec; 0.071 sec/batch; 2h:44m:09s remains)
INFO - root - 2019-11-06 18:15:34.006327: step 11750, total loss = 5.26, predict loss = 1.49 (73.3 examples/sec; 0.055 sec/batch; 2h:05m:47s remains)
INFO - root - 2019-11-06 18:15:34.506101: step 11760, total loss = 4.93, predict loss = 1.44 (97.1 examples/sec; 0.041 sec/batch; 1h:34m:54s remains)
INFO - root - 2019-11-06 18:15:34.983300: step 11770, total loss = 3.99, predict loss = 1.11 (95.6 examples/sec; 0.042 sec/batch; 1h:36m:24s remains)
INFO - root - 2019-11-06 18:15:36.098646: step 11780, total loss = 4.27, predict loss = 1.36 (72.2 examples/sec; 0.055 sec/batch; 2h:07m:38s remains)
INFO - root - 2019-11-06 18:15:36.844486: step 11790, total loss = 5.32, predict loss = 1.51 (53.7 examples/sec; 0.074 sec/batch; 2h:51m:33s remains)
INFO - root - 2019-11-06 18:15:37.594093: step 11800, total loss = 4.52, predict loss = 1.32 (63.2 examples/sec; 0.063 sec/batch; 2h:25m:53s remains)
INFO - root - 2019-11-06 18:15:38.404721: step 11810, total loss = 4.47, predict loss = 1.31 (53.1 examples/sec; 0.075 sec/batch; 2h:53m:37s remains)
INFO - root - 2019-11-06 18:15:39.186761: step 11820, total loss = 4.99, predict loss = 1.45 (58.0 examples/sec; 0.069 sec/batch; 2h:38m:47s remains)
INFO - root - 2019-11-06 18:15:39.804729: step 11830, total loss = 5.29, predict loss = 1.49 (88.6 examples/sec; 0.045 sec/batch; 1h:44m:01s remains)
INFO - root - 2019-11-06 18:15:40.244914: step 11840, total loss = 4.28, predict loss = 1.19 (99.5 examples/sec; 0.040 sec/batch; 1h:32m:34s remains)
INFO - root - 2019-11-06 18:15:40.714869: step 11850, total loss = 5.42, predict loss = 1.51 (94.2 examples/sec; 0.042 sec/batch; 1h:37m:47s remains)
INFO - root - 2019-11-06 18:15:42.030775: step 11860, total loss = 5.05, predict loss = 1.47 (60.4 examples/sec; 0.066 sec/batch; 2h:32m:22s remains)
INFO - root - 2019-11-06 18:15:42.751445: step 11870, total loss = 6.30, predict loss = 1.91 (58.9 examples/sec; 0.068 sec/batch; 2h:36m:27s remains)
INFO - root - 2019-11-06 18:15:43.518651: step 11880, total loss = 3.77, predict loss = 1.11 (59.1 examples/sec; 0.068 sec/batch; 2h:35m:50s remains)
INFO - root - 2019-11-06 18:15:44.261006: step 11890, total loss = 5.44, predict loss = 1.50 (58.9 examples/sec; 0.068 sec/batch; 2h:36m:24s remains)
INFO - root - 2019-11-06 18:15:44.940724: step 11900, total loss = 4.29, predict loss = 1.26 (74.8 examples/sec; 0.053 sec/batch; 2h:03m:02s remains)
INFO - root - 2019-11-06 18:15:45.422788: step 11910, total loss = 5.74, predict loss = 1.73 (98.6 examples/sec; 0.041 sec/batch; 1h:33m:21s remains)
INFO - root - 2019-11-06 18:15:45.870802: step 11920, total loss = 4.63, predict loss = 1.32 (97.7 examples/sec; 0.041 sec/batch; 1h:34m:12s remains)
INFO - root - 2019-11-06 18:15:47.042949: step 11930, total loss = 5.43, predict loss = 1.56 (70.4 examples/sec; 0.057 sec/batch; 2h:10m:40s remains)
INFO - root - 2019-11-06 18:15:47.736095: step 11940, total loss = 4.11, predict loss = 1.22 (64.6 examples/sec; 0.062 sec/batch; 2h:22m:22s remains)
INFO - root - 2019-11-06 18:15:48.470502: step 11950, total loss = 3.11, predict loss = 0.92 (58.4 examples/sec; 0.068 sec/batch; 2h:37m:28s remains)
INFO - root - 2019-11-06 18:15:49.193514: step 11960, total loss = 6.51, predict loss = 1.81 (59.4 examples/sec; 0.067 sec/batch; 2h:34m:55s remains)
INFO - root - 2019-11-06 18:15:49.972527: step 11970, total loss = 3.47, predict loss = 1.05 (65.7 examples/sec; 0.061 sec/batch; 2h:20m:08s remains)
INFO - root - 2019-11-06 18:15:50.568478: step 11980, total loss = 5.26, predict loss = 1.49 (97.3 examples/sec; 0.041 sec/batch; 1h:34m:32s remains)
INFO - root - 2019-11-06 18:15:51.020800: step 11990, total loss = 4.44, predict loss = 1.22 (96.9 examples/sec; 0.041 sec/batch; 1h:34m:55s remains)
INFO - root - 2019-11-06 18:15:51.465038: step 12000, total loss = 5.91, predict loss = 1.80 (96.8 examples/sec; 0.041 sec/batch; 1h:35m:04s remains)
INFO - root - 2019-11-06 18:15:52.891268: step 12010, total loss = 6.04, predict loss = 1.69 (62.0 examples/sec; 0.065 sec/batch; 2h:28m:27s remains)
INFO - root - 2019-11-06 18:15:53.631367: step 12020, total loss = 5.75, predict loss = 1.74 (66.8 examples/sec; 0.060 sec/batch; 2h:17m:41s remains)
INFO - root - 2019-11-06 18:15:54.436282: step 12030, total loss = 6.15, predict loss = 1.76 (54.4 examples/sec; 0.074 sec/batch; 2h:49m:12s remains)
INFO - root - 2019-11-06 18:15:55.202130: step 12040, total loss = 4.17, predict loss = 1.30 (61.2 examples/sec; 0.065 sec/batch; 2h:30m:18s remains)
INFO - root - 2019-11-06 18:15:55.897944: step 12050, total loss = 4.31, predict loss = 1.23 (77.2 examples/sec; 0.052 sec/batch; 1h:59m:06s remains)
INFO - root - 2019-11-06 18:15:56.389242: step 12060, total loss = 4.41, predict loss = 1.27 (93.8 examples/sec; 0.043 sec/batch; 1h:38m:01s remains)
INFO - root - 2019-11-06 18:15:56.848277: step 12070, total loss = 5.66, predict loss = 1.63 (94.3 examples/sec; 0.042 sec/batch; 1h:37m:29s remains)
INFO - root - 2019-11-06 18:15:58.062844: step 12080, total loss = 6.40, predict loss = 1.86 (72.4 examples/sec; 0.055 sec/batch; 2h:07m:01s remains)
INFO - root - 2019-11-06 18:15:58.786997: step 12090, total loss = 6.94, predict loss = 1.96 (65.3 examples/sec; 0.061 sec/batch; 2h:20m:53s remains)
INFO - root - 2019-11-06 18:15:59.495479: step 12100, total loss = 4.40, predict loss = 1.33 (60.1 examples/sec; 0.067 sec/batch; 2h:32m:56s remains)
INFO - root - 2019-11-06 18:16:00.262583: step 12110, total loss = 3.91, predict loss = 1.07 (58.5 examples/sec; 0.068 sec/batch; 2h:37m:08s remains)
INFO - root - 2019-11-06 18:16:01.052502: step 12120, total loss = 5.55, predict loss = 1.58 (57.6 examples/sec; 0.069 sec/batch; 2h:39m:41s remains)
INFO - root - 2019-11-06 18:16:01.667042: step 12130, total loss = 4.36, predict loss = 1.28 (93.3 examples/sec; 0.043 sec/batch; 1h:38m:31s remains)
INFO - root - 2019-11-06 18:16:02.127288: step 12140, total loss = 5.18, predict loss = 1.52 (86.7 examples/sec; 0.046 sec/batch; 1h:46m:02s remains)
INFO - root - 2019-11-06 18:16:02.579413: step 12150, total loss = 5.37, predict loss = 1.56 (124.0 examples/sec; 0.032 sec/batch; 1h:14m:06s remains)
INFO - root - 2019-11-06 18:16:03.903169: step 12160, total loss = 5.56, predict loss = 1.58 (60.2 examples/sec; 0.066 sec/batch; 2h:32m:35s remains)
INFO - root - 2019-11-06 18:16:04.651585: step 12170, total loss = 4.82, predict loss = 1.39 (63.8 examples/sec; 0.063 sec/batch; 2h:23m:56s remains)
INFO - root - 2019-11-06 18:16:05.430697: step 12180, total loss = 6.68, predict loss = 1.95 (57.7 examples/sec; 0.069 sec/batch; 2h:39m:08s remains)
INFO - root - 2019-11-06 18:16:06.240759: step 12190, total loss = 4.94, predict loss = 1.42 (52.4 examples/sec; 0.076 sec/batch; 2h:55m:20s remains)
INFO - root - 2019-11-06 18:16:06.956234: step 12200, total loss = 4.90, predict loss = 1.40 (66.3 examples/sec; 0.060 sec/batch; 2h:18m:29s remains)
INFO - root - 2019-11-06 18:16:07.471207: step 12210, total loss = 6.09, predict loss = 1.81 (91.3 examples/sec; 0.044 sec/batch; 1h:40m:36s remains)
INFO - root - 2019-11-06 18:16:07.936460: step 12220, total loss = 5.84, predict loss = 1.69 (92.4 examples/sec; 0.043 sec/batch; 1h:39m:26s remains)
INFO - root - 2019-11-06 18:16:09.155769: step 12230, total loss = 3.94, predict loss = 1.12 (62.3 examples/sec; 0.064 sec/batch; 2h:27m:21s remains)
INFO - root - 2019-11-06 18:16:09.906581: step 12240, total loss = 5.27, predict loss = 1.45 (58.6 examples/sec; 0.068 sec/batch; 2h:36m:43s remains)
INFO - root - 2019-11-06 18:16:10.667730: step 12250, total loss = 5.06, predict loss = 1.47 (63.9 examples/sec; 0.063 sec/batch; 2h:23m:46s remains)
INFO - root - 2019-11-06 18:16:11.364902: step 12260, total loss = 3.38, predict loss = 0.95 (62.4 examples/sec; 0.064 sec/batch; 2h:27m:10s remains)
INFO - root - 2019-11-06 18:16:12.139180: step 12270, total loss = 6.84, predict loss = 1.92 (63.3 examples/sec; 0.063 sec/batch; 2h:25m:00s remains)
INFO - root - 2019-11-06 18:16:12.724450: step 12280, total loss = 5.23, predict loss = 1.56 (94.9 examples/sec; 0.042 sec/batch; 1h:36m:46s remains)
INFO - root - 2019-11-06 18:16:13.202310: step 12290, total loss = 5.52, predict loss = 1.61 (96.5 examples/sec; 0.041 sec/batch; 1h:35m:08s remains)
INFO - root - 2019-11-06 18:16:14.317767: step 12300, total loss = 4.27, predict loss = 1.22 (5.6 examples/sec; 0.712 sec/batch; 27h:15m:05s remains)
INFO - root - 2019-11-06 18:16:15.025492: step 12310, total loss = 5.59, predict loss = 1.54 (55.0 examples/sec; 0.073 sec/batch; 2h:46m:46s remains)
INFO - root - 2019-11-06 18:16:15.774506: step 12320, total loss = 4.27, predict loss = 1.27 (65.5 examples/sec; 0.061 sec/batch; 2h:20m:05s remains)
INFO - root - 2019-11-06 18:16:16.517986: step 12330, total loss = 2.72, predict loss = 0.82 (58.8 examples/sec; 0.068 sec/batch; 2h:36m:11s remains)
INFO - root - 2019-11-06 18:16:17.278648: step 12340, total loss = 4.88, predict loss = 1.44 (52.7 examples/sec; 0.076 sec/batch; 2h:54m:17s remains)
INFO - root - 2019-11-06 18:16:17.950143: step 12350, total loss = 5.54, predict loss = 1.67 (91.0 examples/sec; 0.044 sec/batch; 1h:40m:52s remains)
INFO - root - 2019-11-06 18:16:18.389981: step 12360, total loss = 3.52, predict loss = 0.98 (98.0 examples/sec; 0.041 sec/batch; 1h:33m:38s remains)
INFO - root - 2019-11-06 18:16:18.851978: step 12370, total loss = 5.10, predict loss = 1.48 (102.1 examples/sec; 0.039 sec/batch; 1h:29m:52s remains)
INFO - root - 2019-11-06 18:16:20.095651: step 12380, total loss = 3.97, predict loss = 1.18 (64.8 examples/sec; 0.062 sec/batch; 2h:21m:33s remains)
INFO - root - 2019-11-06 18:16:20.789434: step 12390, total loss = 4.84, predict loss = 1.42 (61.0 examples/sec; 0.066 sec/batch; 2h:30m:22s remains)
INFO - root - 2019-11-06 18:16:21.552190: step 12400, total loss = 5.34, predict loss = 1.53 (62.4 examples/sec; 0.064 sec/batch; 2h:27m:04s remains)
INFO - root - 2019-11-06 18:16:22.355054: step 12410, total loss = 4.47, predict loss = 1.37 (68.0 examples/sec; 0.059 sec/batch; 2h:14m:47s remains)
INFO - root - 2019-11-06 18:16:23.006573: step 12420, total loss = 4.01, predict loss = 1.19 (74.6 examples/sec; 0.054 sec/batch; 2h:02m:53s remains)
INFO - root - 2019-11-06 18:16:23.509262: step 12430, total loss = 3.73, predict loss = 1.07 (95.1 examples/sec; 0.042 sec/batch; 1h:36m:27s remains)
INFO - root - 2019-11-06 18:16:23.964620: step 12440, total loss = 4.44, predict loss = 1.31 (93.7 examples/sec; 0.043 sec/batch; 1h:37m:54s remains)
INFO - root - 2019-11-06 18:16:25.154731: step 12450, total loss = 4.78, predict loss = 1.43 (72.4 examples/sec; 0.055 sec/batch; 2h:06m:37s remains)
INFO - root - 2019-11-06 18:16:25.885292: step 12460, total loss = 4.59, predict loss = 1.37 (59.6 examples/sec; 0.067 sec/batch; 2h:33m:45s remains)
INFO - root - 2019-11-06 18:16:26.679371: step 12470, total loss = 4.92, predict loss = 1.38 (53.5 examples/sec; 0.075 sec/batch; 2h:51m:16s remains)
INFO - root - 2019-11-06 18:16:27.476585: step 12480, total loss = 4.91, predict loss = 1.46 (51.7 examples/sec; 0.077 sec/batch; 2h:57m:24s remains)
INFO - root - 2019-11-06 18:16:28.209018: step 12490, total loss = 5.97, predict loss = 1.67 (60.1 examples/sec; 0.067 sec/batch; 2h:32m:24s remains)
INFO - root - 2019-11-06 18:16:28.842681: step 12500, total loss = 4.72, predict loss = 1.39 (93.1 examples/sec; 0.043 sec/batch; 1h:38m:25s remains)
INFO - root - 2019-11-06 18:16:29.292680: step 12510, total loss = 5.50, predict loss = 1.53 (100.9 examples/sec; 0.040 sec/batch; 1h:30m:49s remains)
INFO - root - 2019-11-06 18:16:29.743753: step 12520, total loss = 5.63, predict loss = 1.60 (94.8 examples/sec; 0.042 sec/batch; 1h:36m:38s remains)
INFO - root - 2019-11-06 18:16:31.034353: step 12530, total loss = 4.23, predict loss = 1.32 (62.0 examples/sec; 0.065 sec/batch; 2h:27m:51s remains)
INFO - root - 2019-11-06 18:16:31.799269: step 12540, total loss = 4.60, predict loss = 1.39 (54.7 examples/sec; 0.073 sec/batch; 2h:47m:30s remains)
INFO - root - 2019-11-06 18:16:32.588889: step 12550, total loss = 5.01, predict loss = 1.48 (55.9 examples/sec; 0.072 sec/batch; 2h:43m:48s remains)
INFO - root - 2019-11-06 18:16:33.401333: step 12560, total loss = 5.74, predict loss = 1.61 (52.6 examples/sec; 0.076 sec/batch; 2h:54m:05s remains)
INFO - root - 2019-11-06 18:16:34.109610: step 12570, total loss = 6.00, predict loss = 1.79 (73.6 examples/sec; 0.054 sec/batch; 2h:04m:34s remains)
INFO - root - 2019-11-06 18:16:34.611447: step 12580, total loss = 4.34, predict loss = 1.25 (98.4 examples/sec; 0.041 sec/batch; 1h:33m:04s remains)
INFO - root - 2019-11-06 18:16:35.069437: step 12590, total loss = 4.99, predict loss = 1.48 (87.0 examples/sec; 0.046 sec/batch; 1h:45m:18s remains)
INFO - root - 2019-11-06 18:16:36.238486: step 12600, total loss = 4.86, predict loss = 1.44 (68.9 examples/sec; 0.058 sec/batch; 2h:13m:00s remains)
INFO - root - 2019-11-06 18:16:36.960778: step 12610, total loss = 4.34, predict loss = 1.23 (62.9 examples/sec; 0.064 sec/batch; 2h:25m:30s remains)
INFO - root - 2019-11-06 18:16:37.680417: step 12620, total loss = 3.42, predict loss = 0.99 (61.4 examples/sec; 0.065 sec/batch; 2h:29m:08s remains)
INFO - root - 2019-11-06 18:16:38.383936: step 12630, total loss = 3.46, predict loss = 1.04 (55.8 examples/sec; 0.072 sec/batch; 2h:44m:07s remains)
INFO - root - 2019-11-06 18:16:39.134755: step 12640, total loss = 4.95, predict loss = 1.48 (57.5 examples/sec; 0.070 sec/batch; 2h:39m:14s remains)
INFO - root - 2019-11-06 18:16:39.794047: step 12650, total loss = 4.33, predict loss = 1.33 (96.8 examples/sec; 0.041 sec/batch; 1h:34m:37s remains)
INFO - root - 2019-11-06 18:16:40.237564: step 12660, total loss = 4.45, predict loss = 1.33 (85.3 examples/sec; 0.047 sec/batch; 1h:47m:24s remains)
INFO - root - 2019-11-06 18:16:40.687845: step 12670, total loss = 4.06, predict loss = 1.12 (102.5 examples/sec; 0.039 sec/batch; 1h:29m:20s remains)
INFO - root - 2019-11-06 18:16:42.006433: step 12680, total loss = 6.03, predict loss = 1.66 (53.5 examples/sec; 0.075 sec/batch; 2h:51m:01s remains)
INFO - root - 2019-11-06 18:16:42.733906: step 12690, total loss = 4.11, predict loss = 1.21 (57.7 examples/sec; 0.069 sec/batch; 2h:38m:39s remains)
INFO - root - 2019-11-06 18:16:43.418362: step 12700, total loss = 6.09, predict loss = 1.72 (71.0 examples/sec; 0.056 sec/batch; 2h:08m:50s remains)
INFO - root - 2019-11-06 18:16:44.078521: step 12710, total loss = 4.39, predict loss = 1.34 (69.6 examples/sec; 0.057 sec/batch; 2h:11m:31s remains)
INFO - root - 2019-11-06 18:16:44.730966: step 12720, total loss = 3.55, predict loss = 1.00 (71.4 examples/sec; 0.056 sec/batch; 2h:08m:13s remains)
INFO - root - 2019-11-06 18:16:45.271905: step 12730, total loss = 5.16, predict loss = 1.49 (91.3 examples/sec; 0.044 sec/batch; 1h:40m:12s remains)
INFO - root - 2019-11-06 18:16:45.717552: step 12740, total loss = 4.43, predict loss = 1.27 (101.4 examples/sec; 0.039 sec/batch; 1h:30m:13s remains)
INFO - root - 2019-11-06 18:16:46.844194: step 12750, total loss = 3.30, predict loss = 0.98 (63.5 examples/sec; 0.063 sec/batch; 2h:24m:00s remains)
INFO - root - 2019-11-06 18:16:47.562656: step 12760, total loss = 5.18, predict loss = 1.46 (52.1 examples/sec; 0.077 sec/batch; 2h:55m:27s remains)
INFO - root - 2019-11-06 18:16:48.309908: step 12770, total loss = 5.50, predict loss = 1.64 (57.8 examples/sec; 0.069 sec/batch; 2h:38m:10s remains)
INFO - root - 2019-11-06 18:16:49.070162: step 12780, total loss = 5.09, predict loss = 1.43 (56.7 examples/sec; 0.071 sec/batch; 2h:41m:23s remains)
INFO - root - 2019-11-06 18:16:49.815958: step 12790, total loss = 5.82, predict loss = 1.67 (57.1 examples/sec; 0.070 sec/batch; 2h:40m:15s remains)
INFO - root - 2019-11-06 18:16:50.420302: step 12800, total loss = 6.37, predict loss = 1.75 (95.0 examples/sec; 0.042 sec/batch; 1h:36m:15s remains)
INFO - root - 2019-11-06 18:16:50.892812: step 12810, total loss = 4.79, predict loss = 1.31 (91.0 examples/sec; 0.044 sec/batch; 1h:40m:27s remains)
INFO - root - 2019-11-06 18:16:51.365036: step 12820, total loss = 5.23, predict loss = 1.51 (90.7 examples/sec; 0.044 sec/batch; 1h:40m:50s remains)
INFO - root - 2019-11-06 18:16:52.781637: step 12830, total loss = 6.72, predict loss = 1.82 (65.0 examples/sec; 0.062 sec/batch; 2h:20m:42s remains)
INFO - root - 2019-11-06 18:16:53.505429: step 12840, total loss = 3.82, predict loss = 1.13 (55.5 examples/sec; 0.072 sec/batch; 2h:44m:52s remains)
INFO - root - 2019-11-06 18:16:54.254575: step 12850, total loss = 5.68, predict loss = 1.59 (59.6 examples/sec; 0.067 sec/batch; 2h:33m:23s remains)
INFO - root - 2019-11-06 18:16:55.002708: step 12860, total loss = 3.59, predict loss = 1.04 (59.8 examples/sec; 0.067 sec/batch; 2h:32m:51s remains)
INFO - root - 2019-11-06 18:16:55.707051: step 12870, total loss = 5.08, predict loss = 1.41 (67.4 examples/sec; 0.059 sec/batch; 2h:15m:36s remains)
INFO - root - 2019-11-06 18:16:56.177038: step 12880, total loss = 4.98, predict loss = 1.47 (98.0 examples/sec; 0.041 sec/batch; 1h:33m:15s remains)
INFO - root - 2019-11-06 18:16:56.656216: step 12890, total loss = 3.64, predict loss = 0.98 (95.9 examples/sec; 0.042 sec/batch; 1h:35m:19s remains)
INFO - root - 2019-11-06 18:16:57.824599: step 12900, total loss = 5.78, predict loss = 1.74 (66.3 examples/sec; 0.060 sec/batch; 2h:17m:46s remains)
INFO - root - 2019-11-06 18:16:58.556500: step 12910, total loss = 6.91, predict loss = 2.01 (55.4 examples/sec; 0.072 sec/batch; 2h:44m:56s remains)
INFO - root - 2019-11-06 18:16:59.325741: step 12920, total loss = 4.89, predict loss = 1.50 (53.4 examples/sec; 0.075 sec/batch; 2h:51m:10s remains)
INFO - root - 2019-11-06 18:17:00.066672: step 12930, total loss = 4.74, predict loss = 1.40 (59.8 examples/sec; 0.067 sec/batch; 2h:32m:42s remains)
INFO - root - 2019-11-06 18:17:00.833245: step 12940, total loss = 5.13, predict loss = 1.51 (58.6 examples/sec; 0.068 sec/batch; 2h:35m:47s remains)
INFO - root - 2019-11-06 18:17:01.449901: step 12950, total loss = 5.85, predict loss = 1.72 (97.5 examples/sec; 0.041 sec/batch; 1h:33m:40s remains)
INFO - root - 2019-11-06 18:17:01.893711: step 12960, total loss = 5.93, predict loss = 1.71 (103.1 examples/sec; 0.039 sec/batch; 1h:28m:34s remains)
INFO - root - 2019-11-06 18:17:02.363819: step 12970, total loss = 6.74, predict loss = 1.90 (113.3 examples/sec; 0.035 sec/batch; 1h:20m:38s remains)
INFO - root - 2019-11-06 18:17:03.777531: step 12980, total loss = 5.77, predict loss = 1.63 (48.7 examples/sec; 0.082 sec/batch; 3h:07m:40s remains)
INFO - root - 2019-11-06 18:17:04.525508: step 12990, total loss = 4.07, predict loss = 1.10 (56.7 examples/sec; 0.071 sec/batch; 2h:41m:02s remains)
INFO - root - 2019-11-06 18:17:05.263436: step 13000, total loss = 4.98, predict loss = 1.40 (62.6 examples/sec; 0.064 sec/batch; 2h:25m:51s remains)
INFO - root - 2019-11-06 18:17:06.038339: step 13010, total loss = 5.09, predict loss = 1.44 (54.1 examples/sec; 0.074 sec/batch; 2h:48m:54s remains)
INFO - root - 2019-11-06 18:17:06.752173: step 13020, total loss = 3.98, predict loss = 1.13 (72.5 examples/sec; 0.055 sec/batch; 2h:06m:00s remains)
INFO - root - 2019-11-06 18:17:07.206799: step 13030, total loss = 5.02, predict loss = 1.44 (95.9 examples/sec; 0.042 sec/batch; 1h:35m:11s remains)
INFO - root - 2019-11-06 18:17:07.664764: step 13040, total loss = 5.14, predict loss = 1.50 (103.6 examples/sec; 0.039 sec/batch; 1h:28m:08s remains)
INFO - root - 2019-11-06 18:17:08.901447: step 13050, total loss = 5.51, predict loss = 1.61 (59.0 examples/sec; 0.068 sec/batch; 2h:34m:38s remains)
INFO - root - 2019-11-06 18:17:09.709220: step 13060, total loss = 6.03, predict loss = 1.72 (52.5 examples/sec; 0.076 sec/batch; 2h:53m:50s remains)
INFO - root - 2019-11-06 18:17:10.426104: step 13070, total loss = 6.08, predict loss = 1.77 (61.3 examples/sec; 0.065 sec/batch; 2h:28m:48s remains)
INFO - root - 2019-11-06 18:17:11.144029: step 13080, total loss = 5.60, predict loss = 1.58 (57.6 examples/sec; 0.069 sec/batch; 2h:38m:30s remains)
INFO - root - 2019-11-06 18:17:11.899833: step 13090, total loss = 4.13, predict loss = 1.17 (72.5 examples/sec; 0.055 sec/batch; 2h:05m:49s remains)
INFO - root - 2019-11-06 18:17:12.478892: step 13100, total loss = 4.45, predict loss = 1.27 (103.9 examples/sec; 0.038 sec/batch; 1h:27m:49s remains)
INFO - root - 2019-11-06 18:17:12.937643: step 13110, total loss = 6.24, predict loss = 1.75 (91.3 examples/sec; 0.044 sec/batch; 1h:39m:57s remains)
INFO - root - 2019-11-06 18:17:14.067007: step 13120, total loss = 4.23, predict loss = 1.20 (5.6 examples/sec; 0.720 sec/batch; 27h:21m:56s remains)
INFO - root - 2019-11-06 18:17:14.783000: step 13130, total loss = 3.19, predict loss = 0.89 (62.6 examples/sec; 0.064 sec/batch; 2h:25m:49s remains)
INFO - root - 2019-11-06 18:17:15.570091: step 13140, total loss = 3.94, predict loss = 1.14 (56.7 examples/sec; 0.071 sec/batch; 2h:40m:57s remains)
INFO - root - 2019-11-06 18:17:16.317575: step 13150, total loss = 6.16, predict loss = 1.77 (60.6 examples/sec; 0.066 sec/batch; 2h:30m:34s remains)
INFO - root - 2019-11-06 18:17:17.112173: step 13160, total loss = 4.90, predict loss = 1.38 (56.5 examples/sec; 0.071 sec/batch; 2h:41m:22s remains)
INFO - root - 2019-11-06 18:17:17.869952: step 13170, total loss = 4.91, predict loss = 1.42 (87.9 examples/sec; 0.046 sec/batch; 1h:43m:46s remains)
INFO - root - 2019-11-06 18:17:18.321076: step 13180, total loss = 5.40, predict loss = 1.55 (90.9 examples/sec; 0.044 sec/batch; 1h:40m:22s remains)
INFO - root - 2019-11-06 18:17:18.786927: step 13190, total loss = 4.91, predict loss = 1.36 (86.2 examples/sec; 0.046 sec/batch; 1h:45m:48s remains)
INFO - root - 2019-11-06 18:17:20.035717: step 13200, total loss = 4.66, predict loss = 1.35 (52.4 examples/sec; 0.076 sec/batch; 2h:53m:53s remains)
INFO - root - 2019-11-06 18:17:20.781823: step 13210, total loss = 4.49, predict loss = 1.31 (65.4 examples/sec; 0.061 sec/batch; 2h:19m:26s remains)
INFO - root - 2019-11-06 18:17:21.507726: step 13220, total loss = 4.79, predict loss = 1.48 (62.2 examples/sec; 0.064 sec/batch; 2h:26m:32s remains)
INFO - root - 2019-11-06 18:17:22.336898: step 13230, total loss = 5.02, predict loss = 1.46 (74.3 examples/sec; 0.054 sec/batch; 2h:02m:46s remains)
INFO - root - 2019-11-06 18:17:23.018784: step 13240, total loss = 5.28, predict loss = 1.52 (80.6 examples/sec; 0.050 sec/batch; 1h:53m:07s remains)
INFO - root - 2019-11-06 18:17:23.598396: step 13250, total loss = 6.67, predict loss = 1.79 (97.4 examples/sec; 0.041 sec/batch; 1h:33m:33s remains)
INFO - root - 2019-11-06 18:17:24.045257: step 13260, total loss = 4.74, predict loss = 1.38 (95.6 examples/sec; 0.042 sec/batch; 1h:35m:20s remains)
INFO - root - 2019-11-06 18:17:25.209248: step 13270, total loss = 4.97, predict loss = 1.51 (69.8 examples/sec; 0.057 sec/batch; 2h:10m:31s remains)
INFO - root - 2019-11-06 18:17:25.891108: step 13280, total loss = 3.69, predict loss = 1.09 (62.2 examples/sec; 0.064 sec/batch; 2h:26m:31s remains)
INFO - root - 2019-11-06 18:17:26.627509: step 13290, total loss = 6.40, predict loss = 1.81 (59.2 examples/sec; 0.068 sec/batch; 2h:33m:59s remains)
INFO - root - 2019-11-06 18:17:27.441303: step 13300, total loss = 4.82, predict loss = 1.40 (52.5 examples/sec; 0.076 sec/batch; 2h:53m:30s remains)
INFO - root - 2019-11-06 18:17:28.177739: step 13310, total loss = 4.85, predict loss = 1.42 (56.9 examples/sec; 0.070 sec/batch; 2h:40m:13s remains)
INFO - root - 2019-11-06 18:17:28.825782: step 13320, total loss = 6.49, predict loss = 1.85 (90.6 examples/sec; 0.044 sec/batch; 1h:40m:34s remains)
INFO - root - 2019-11-06 18:17:29.295853: step 13330, total loss = 5.03, predict loss = 1.47 (97.0 examples/sec; 0.041 sec/batch; 1h:33m:54s remains)
INFO - root - 2019-11-06 18:17:29.757509: step 13340, total loss = 5.79, predict loss = 1.67 (84.9 examples/sec; 0.047 sec/batch; 1h:47m:15s remains)
INFO - root - 2019-11-06 18:17:31.039461: step 13350, total loss = 4.60, predict loss = 1.30 (57.4 examples/sec; 0.070 sec/batch; 2h:38m:50s remains)
INFO - root - 2019-11-06 18:17:31.831266: step 13360, total loss = 5.25, predict loss = 1.50 (55.9 examples/sec; 0.072 sec/batch; 2h:42m:59s remains)
INFO - root - 2019-11-06 18:17:32.568770: step 13370, total loss = 4.84, predict loss = 1.33 (59.4 examples/sec; 0.067 sec/batch; 2h:33m:27s remains)
INFO - root - 2019-11-06 18:17:33.274351: step 13380, total loss = 5.50, predict loss = 1.57 (67.4 examples/sec; 0.059 sec/batch; 2h:15m:05s remains)
INFO - root - 2019-11-06 18:17:34.024952: step 13390, total loss = 4.86, predict loss = 1.35 (68.0 examples/sec; 0.059 sec/batch; 2h:13m:53s remains)
INFO - root - 2019-11-06 18:17:34.545308: step 13400, total loss = 5.23, predict loss = 1.53 (95.4 examples/sec; 0.042 sec/batch; 1h:35m:28s remains)
INFO - root - 2019-11-06 18:17:35.026421: step 13410, total loss = 6.05, predict loss = 1.68 (98.4 examples/sec; 0.041 sec/batch; 1h:32m:32s remains)
INFO - root - 2019-11-06 18:17:36.171116: step 13420, total loss = 3.68, predict loss = 1.05 (71.2 examples/sec; 0.056 sec/batch; 2h:07m:54s remains)
INFO - root - 2019-11-06 18:17:36.900437: step 13430, total loss = 5.30, predict loss = 1.51 (61.8 examples/sec; 0.065 sec/batch; 2h:27m:13s remains)
INFO - root - 2019-11-06 18:17:37.671796: step 13440, total loss = 4.71, predict loss = 1.30 (52.3 examples/sec; 0.076 sec/batch; 2h:54m:06s remains)
INFO - root - 2019-11-06 18:17:38.430012: step 13450, total loss = 4.68, predict loss = 1.35 (58.4 examples/sec; 0.069 sec/batch; 2h:35m:56s remains)
INFO - root - 2019-11-06 18:17:39.166969: step 13460, total loss = 6.34, predict loss = 1.73 (63.8 examples/sec; 0.063 sec/batch; 2h:22m:42s remains)
INFO - root - 2019-11-06 18:17:39.756928: step 13470, total loss = 6.95, predict loss = 2.01 (104.9 examples/sec; 0.038 sec/batch; 1h:26m:44s remains)
INFO - root - 2019-11-06 18:17:40.204134: step 13480, total loss = 6.41, predict loss = 1.89 (100.0 examples/sec; 0.040 sec/batch; 1h:30m:59s remains)
INFO - root - 2019-11-06 18:17:40.697636: step 13490, total loss = 5.69, predict loss = 1.74 (93.7 examples/sec; 0.043 sec/batch; 1h:37m:09s remains)
INFO - root - 2019-11-06 18:17:41.997474: step 13500, total loss = 5.97, predict loss = 1.71 (56.8 examples/sec; 0.070 sec/batch; 2h:40m:09s remains)
INFO - root - 2019-11-06 18:17:42.730390: step 13510, total loss = 4.94, predict loss = 1.53 (62.8 examples/sec; 0.064 sec/batch; 2h:24m:53s remains)
INFO - root - 2019-11-06 18:17:43.441183: step 13520, total loss = 5.75, predict loss = 1.60 (56.6 examples/sec; 0.071 sec/batch; 2h:40m:43s remains)
INFO - root - 2019-11-06 18:17:44.192056: step 13530, total loss = 5.37, predict loss = 1.50 (64.9 examples/sec; 0.062 sec/batch; 2h:20m:05s remains)
INFO - root - 2019-11-06 18:17:44.872022: step 13540, total loss = 4.98, predict loss = 1.31 (57.8 examples/sec; 0.069 sec/batch; 2h:37m:31s remains)
INFO - root - 2019-11-06 18:17:45.414095: step 13550, total loss = 5.37, predict loss = 1.52 (94.7 examples/sec; 0.042 sec/batch; 1h:36m:06s remains)
INFO - root - 2019-11-06 18:17:45.873859: step 13560, total loss = 5.42, predict loss = 1.54 (92.7 examples/sec; 0.043 sec/batch; 1h:38m:06s remains)
INFO - root - 2019-11-06 18:17:47.055916: step 13570, total loss = 4.99, predict loss = 1.48 (70.7 examples/sec; 0.057 sec/batch; 2h:08m:39s remains)
INFO - root - 2019-11-06 18:17:47.743394: step 13580, total loss = 4.88, predict loss = 1.40 (61.6 examples/sec; 0.065 sec/batch; 2h:27m:33s remains)
INFO - root - 2019-11-06 18:17:48.493457: step 13590, total loss = 6.01, predict loss = 1.71 (61.5 examples/sec; 0.065 sec/batch; 2h:27m:55s remains)
INFO - root - 2019-11-06 18:17:49.267322: step 13600, total loss = 3.33, predict loss = 0.95 (54.9 examples/sec; 0.073 sec/batch; 2h:45m:40s remains)
INFO - root - 2019-11-06 18:17:50.012519: step 13610, total loss = 3.70, predict loss = 1.07 (62.1 examples/sec; 0.064 sec/batch; 2h:26m:24s remains)
INFO - root - 2019-11-06 18:17:50.633291: step 13620, total loss = 6.48, predict loss = 1.83 (87.8 examples/sec; 0.046 sec/batch; 1h:43m:36s remains)
INFO - root - 2019-11-06 18:17:51.106856: step 13630, total loss = 4.91, predict loss = 1.42 (97.3 examples/sec; 0.041 sec/batch; 1h:33m:28s remains)
INFO - root - 2019-11-06 18:17:51.575282: step 13640, total loss = 6.10, predict loss = 1.67 (87.9 examples/sec; 0.046 sec/batch; 1h:43m:27s remains)
INFO - root - 2019-11-06 18:17:53.034814: step 13650, total loss = 5.75, predict loss = 1.64 (57.6 examples/sec; 0.069 sec/batch; 2h:37m:45s remains)
INFO - root - 2019-11-06 18:17:53.795953: step 13660, total loss = 4.89, predict loss = 1.47 (51.1 examples/sec; 0.078 sec/batch; 2h:58m:00s remains)
INFO - root - 2019-11-06 18:17:54.531425: step 13670, total loss = 7.18, predict loss = 2.03 (63.9 examples/sec; 0.063 sec/batch; 2h:22m:16s remains)
INFO - root - 2019-11-06 18:17:55.320958: step 13680, total loss = 4.70, predict loss = 1.29 (57.1 examples/sec; 0.070 sec/batch; 2h:39m:01s remains)
INFO - root - 2019-11-06 18:17:56.012242: step 13690, total loss = 4.30, predict loss = 1.31 (75.8 examples/sec; 0.053 sec/batch; 1h:59m:49s remains)
INFO - root - 2019-11-06 18:17:56.472486: step 13700, total loss = 6.37, predict loss = 1.78 (98.5 examples/sec; 0.041 sec/batch; 1h:32m:15s remains)
INFO - root - 2019-11-06 18:17:56.914303: step 13710, total loss = 5.49, predict loss = 1.51 (96.6 examples/sec; 0.041 sec/batch; 1h:34m:04s remains)
INFO - root - 2019-11-06 18:17:58.080008: step 13720, total loss = 5.17, predict loss = 1.53 (65.2 examples/sec; 0.061 sec/batch; 2h:19m:22s remains)
INFO - root - 2019-11-06 18:17:58.788552: step 13730, total loss = 5.61, predict loss = 1.67 (59.0 examples/sec; 0.068 sec/batch; 2h:33m:52s remains)
INFO - root - 2019-11-06 18:17:59.545144: step 13740, total loss = 5.07, predict loss = 1.46 (53.9 examples/sec; 0.074 sec/batch; 2h:48m:40s remains)
INFO - root - 2019-11-06 18:18:00.311276: step 13750, total loss = 5.03, predict loss = 1.49 (54.7 examples/sec; 0.073 sec/batch; 2h:45m:57s remains)
INFO - root - 2019-11-06 18:18:01.049457: step 13760, total loss = 5.67, predict loss = 1.60 (64.2 examples/sec; 0.062 sec/batch; 2h:21m:34s remains)
INFO - root - 2019-11-06 18:18:01.691886: step 13770, total loss = 4.13, predict loss = 1.19 (98.0 examples/sec; 0.041 sec/batch; 1h:32m:38s remains)
INFO - root - 2019-11-06 18:18:02.148199: step 13780, total loss = 3.93, predict loss = 1.15 (85.4 examples/sec; 0.047 sec/batch; 1h:46m:23s remains)
INFO - root - 2019-11-06 18:18:02.594624: step 13790, total loss = 6.65, predict loss = 1.92 (127.4 examples/sec; 0.031 sec/batch; 1h:11m:16s remains)
INFO - root - 2019-11-06 18:18:03.960037: step 13800, total loss = 5.18, predict loss = 1.47 (63.7 examples/sec; 0.063 sec/batch; 2h:22m:37s remains)
INFO - root - 2019-11-06 18:18:04.738013: step 13810, total loss = 4.39, predict loss = 1.26 (55.6 examples/sec; 0.072 sec/batch; 2h:43m:20s remains)
INFO - root - 2019-11-06 18:18:05.470104: step 13820, total loss = 6.10, predict loss = 1.74 (61.2 examples/sec; 0.065 sec/batch; 2h:28m:20s remains)
INFO - root - 2019-11-06 18:18:06.196738: step 13830, total loss = 4.56, predict loss = 1.24 (49.2 examples/sec; 0.081 sec/batch; 3h:04m:25s remains)
INFO - root - 2019-11-06 18:18:06.839181: step 13840, total loss = 6.55, predict loss = 1.88 (82.8 examples/sec; 0.048 sec/batch; 1h:49m:38s remains)
INFO - root - 2019-11-06 18:18:07.315988: step 13850, total loss = 5.65, predict loss = 1.71 (97.5 examples/sec; 0.041 sec/batch; 1h:33m:06s remains)
INFO - root - 2019-11-06 18:18:07.766550: step 13860, total loss = 5.27, predict loss = 1.53 (92.6 examples/sec; 0.043 sec/batch; 1h:37m:58s remains)
INFO - root - 2019-11-06 18:18:08.947353: step 13870, total loss = 4.77, predict loss = 1.36 (64.7 examples/sec; 0.062 sec/batch; 2h:20m:10s remains)
INFO - root - 2019-11-06 18:18:09.699374: step 13880, total loss = 5.16, predict loss = 1.47 (64.6 examples/sec; 0.062 sec/batch; 2h:20m:32s remains)
INFO - root - 2019-11-06 18:18:10.434079: step 13890, total loss = 5.33, predict loss = 1.51 (55.8 examples/sec; 0.072 sec/batch; 2h:42m:36s remains)
INFO - root - 2019-11-06 18:18:11.197149: step 13900, total loss = 4.99, predict loss = 1.43 (53.0 examples/sec; 0.075 sec/batch; 2h:51m:15s remains)
INFO - root - 2019-11-06 18:18:11.947413: step 13910, total loss = 5.46, predict loss = 1.57 (71.1 examples/sec; 0.056 sec/batch; 2h:07m:34s remains)
INFO - root - 2019-11-06 18:18:12.489060: step 13920, total loss = 5.59, predict loss = 1.59 (99.5 examples/sec; 0.040 sec/batch; 1h:31m:13s remains)
INFO - root - 2019-11-06 18:18:12.966152: step 13930, total loss = 4.31, predict loss = 1.24 (96.4 examples/sec; 0.041 sec/batch; 1h:34m:05s remains)
INFO - root - 2019-11-06 18:18:14.082643: step 13940, total loss = 5.92, predict loss = 1.69 (5.6 examples/sec; 0.715 sec/batch; 27h:00m:34s remains)
INFO - root - 2019-11-06 18:18:14.755249: step 13950, total loss = 5.06, predict loss = 1.46 (66.1 examples/sec; 0.061 sec/batch; 2h:17m:14s remains)
INFO - root - 2019-11-06 18:18:15.487066: step 13960, total loss = 2.73, predict loss = 0.80 (52.7 examples/sec; 0.076 sec/batch; 2h:51m:58s remains)
INFO - root - 2019-11-06 18:18:16.246020: step 13970, total loss = 5.95, predict loss = 1.74 (51.4 examples/sec; 0.078 sec/batch; 2h:56m:26s remains)
INFO - root - 2019-11-06 18:18:17.060674: step 13980, total loss = 5.91, predict loss = 1.72 (55.9 examples/sec; 0.072 sec/batch; 2h:42m:11s remains)
INFO - root - 2019-11-06 18:18:17.708878: step 13990, total loss = 5.54, predict loss = 1.62 (93.4 examples/sec; 0.043 sec/batch; 1h:37m:07s remains)
INFO - root - 2019-11-06 18:18:18.160328: step 14000, total loss = 4.37, predict loss = 1.23 (90.0 examples/sec; 0.044 sec/batch; 1h:40m:44s remains)
INFO - root - 2019-11-06 18:18:18.649592: step 14010, total loss = 4.54, predict loss = 1.32 (94.3 examples/sec; 0.042 sec/batch; 1h:36m:09s remains)
INFO - root - 2019-11-06 18:18:19.869405: step 14020, total loss = 5.64, predict loss = 1.64 (62.4 examples/sec; 0.064 sec/batch; 2h:25m:17s remains)
INFO - root - 2019-11-06 18:18:20.650860: step 14030, total loss = 5.98, predict loss = 1.69 (51.8 examples/sec; 0.077 sec/batch; 2h:55m:07s remains)
INFO - root - 2019-11-06 18:18:21.392280: step 14040, total loss = 6.04, predict loss = 1.64 (63.3 examples/sec; 0.063 sec/batch; 2h:23m:08s remains)
INFO - root - 2019-11-06 18:18:22.220172: step 14050, total loss = 5.76, predict loss = 1.69 (60.4 examples/sec; 0.066 sec/batch; 2h:29m:59s remains)
INFO - root - 2019-11-06 18:18:22.870605: step 14060, total loss = 5.40, predict loss = 1.52 (65.6 examples/sec; 0.061 sec/batch; 2h:18m:15s remains)
INFO - root - 2019-11-06 18:18:23.416325: step 14070, total loss = 4.96, predict loss = 1.52 (86.2 examples/sec; 0.046 sec/batch; 1h:45m:05s remains)
INFO - root - 2019-11-06 18:18:23.856820: step 14080, total loss = 4.74, predict loss = 1.39 (94.4 examples/sec; 0.042 sec/batch; 1h:35m:56s remains)
INFO - root - 2019-11-06 18:18:25.025013: step 14090, total loss = 4.76, predict loss = 1.42 (66.2 examples/sec; 0.060 sec/batch; 2h:16m:52s remains)
INFO - root - 2019-11-06 18:18:25.721036: step 14100, total loss = 5.97, predict loss = 1.73 (61.0 examples/sec; 0.066 sec/batch; 2h:28m:27s remains)
INFO - root - 2019-11-06 18:18:26.435864: step 14110, total loss = 3.34, predict loss = 0.99 (62.6 examples/sec; 0.064 sec/batch; 2h:24m:36s remains)
INFO - root - 2019-11-06 18:18:27.199042: step 14120, total loss = 3.49, predict loss = 1.04 (53.3 examples/sec; 0.075 sec/batch; 2h:50m:03s remains)
INFO - root - 2019-11-06 18:18:27.974413: step 14130, total loss = 6.92, predict loss = 2.00 (61.8 examples/sec; 0.065 sec/batch; 2h:26m:38s remains)
INFO - root - 2019-11-06 18:18:28.594937: step 14140, total loss = 5.75, predict loss = 1.55 (94.0 examples/sec; 0.043 sec/batch; 1h:36m:21s remains)
INFO - root - 2019-11-06 18:18:29.041837: step 14150, total loss = 5.20, predict loss = 1.54 (85.9 examples/sec; 0.047 sec/batch; 1h:45m:24s remains)
INFO - root - 2019-11-06 18:18:29.496717: step 14160, total loss = 4.95, predict loss = 1.43 (100.3 examples/sec; 0.040 sec/batch; 1h:30m:17s remains)
INFO - root - 2019-11-06 18:18:30.797445: step 14170, total loss = 6.60, predict loss = 1.99 (59.1 examples/sec; 0.068 sec/batch; 2h:33m:09s remains)
INFO - root - 2019-11-06 18:18:31.611170: step 14180, total loss = 6.18, predict loss = 1.79 (55.9 examples/sec; 0.072 sec/batch; 2h:42m:00s remains)
INFO - root - 2019-11-06 18:18:32.362927: step 14190, total loss = 5.81, predict loss = 1.76 (59.2 examples/sec; 0.068 sec/batch; 2h:32m:55s remains)
INFO - root - 2019-11-06 18:18:33.100158: step 14200, total loss = 4.16, predict loss = 1.30 (62.7 examples/sec; 0.064 sec/batch; 2h:24m:22s remains)
INFO - root - 2019-11-06 18:18:33.836868: step 14210, total loss = 5.69, predict loss = 1.66 (55.7 examples/sec; 0.072 sec/batch; 2h:42m:34s remains)
INFO - root - 2019-11-06 18:18:34.375296: step 14220, total loss = 6.63, predict loss = 1.84 (99.1 examples/sec; 0.040 sec/batch; 1h:31m:18s remains)
INFO - root - 2019-11-06 18:18:34.828657: step 14230, total loss = 4.81, predict loss = 1.35 (95.5 examples/sec; 0.042 sec/batch; 1h:34m:46s remains)
INFO - root - 2019-11-06 18:18:35.978940: step 14240, total loss = 4.14, predict loss = 1.21 (70.5 examples/sec; 0.057 sec/batch; 2h:08m:21s remains)
INFO - root - 2019-11-06 18:18:36.724271: step 14250, total loss = 7.13, predict loss = 2.06 (67.7 examples/sec; 0.059 sec/batch; 2h:13m:35s remains)
INFO - root - 2019-11-06 18:18:37.504749: step 14260, total loss = 6.75, predict loss = 1.96 (61.2 examples/sec; 0.065 sec/batch; 2h:27m:58s remains)
INFO - root - 2019-11-06 18:18:38.317139: step 14270, total loss = 5.85, predict loss = 1.64 (56.2 examples/sec; 0.071 sec/batch; 2h:40m:55s remains)
INFO - root - 2019-11-06 18:18:39.071105: step 14280, total loss = 5.54, predict loss = 1.56 (59.6 examples/sec; 0.067 sec/batch; 2h:31m:48s remains)
INFO - root - 2019-11-06 18:18:39.685860: step 14290, total loss = 3.68, predict loss = 1.07 (102.2 examples/sec; 0.039 sec/batch; 1h:28m:32s remains)
INFO - root - 2019-11-06 18:18:40.136910: step 14300, total loss = 2.71, predict loss = 0.81 (94.9 examples/sec; 0.042 sec/batch; 1h:35m:19s remains)
INFO - root - 2019-11-06 18:18:40.610352: step 14310, total loss = 5.19, predict loss = 1.48 (94.3 examples/sec; 0.042 sec/batch; 1h:35m:52s remains)
INFO - root - 2019-11-06 18:18:41.913125: step 14320, total loss = 3.25, predict loss = 1.02 (59.7 examples/sec; 0.067 sec/batch; 2h:31m:25s remains)
INFO - root - 2019-11-06 18:18:42.697606: step 14330, total loss = 6.31, predict loss = 1.80 (48.6 examples/sec; 0.082 sec/batch; 3h:06m:17s remains)
INFO - root - 2019-11-06 18:18:43.409471: step 14340, total loss = 5.19, predict loss = 1.52 (66.4 examples/sec; 0.060 sec/batch; 2h:16m:09s remains)
INFO - root - 2019-11-06 18:18:44.101536: step 14350, total loss = 4.53, predict loss = 1.30 (58.7 examples/sec; 0.068 sec/batch; 2h:34m:00s remains)
INFO - root - 2019-11-06 18:18:44.786319: step 14360, total loss = 5.55, predict loss = 1.63 (72.0 examples/sec; 0.056 sec/batch; 2h:05m:38s remains)
INFO - root - 2019-11-06 18:18:45.310846: step 14370, total loss = 4.80, predict loss = 1.43 (102.4 examples/sec; 0.039 sec/batch; 1h:28m:18s remains)
INFO - root - 2019-11-06 18:18:45.746539: step 14380, total loss = 4.28, predict loss = 1.22 (97.1 examples/sec; 0.041 sec/batch; 1h:33m:09s remains)
INFO - root - 2019-11-06 18:18:46.897304: step 14390, total loss = 5.75, predict loss = 1.59 (65.2 examples/sec; 0.061 sec/batch; 2h:18m:40s remains)
INFO - root - 2019-11-06 18:18:47.596682: step 14400, total loss = 3.72, predict loss = 1.09 (58.7 examples/sec; 0.068 sec/batch; 2h:34m:01s remains)
INFO - root - 2019-11-06 18:18:48.361186: step 14410, total loss = 4.60, predict loss = 1.26 (62.1 examples/sec; 0.064 sec/batch; 2h:25m:30s remains)
INFO - root - 2019-11-06 18:18:49.115708: step 14420, total loss = 4.73, predict loss = 1.38 (55.8 examples/sec; 0.072 sec/batch; 2h:41m:56s remains)
INFO - root - 2019-11-06 18:18:49.838107: step 14430, total loss = 4.55, predict loss = 1.28 (61.5 examples/sec; 0.065 sec/batch; 2h:27m:02s remains)
INFO - root - 2019-11-06 18:18:50.438975: step 14440, total loss = 4.59, predict loss = 1.35 (96.3 examples/sec; 0.042 sec/batch; 1h:33m:53s remains)
INFO - root - 2019-11-06 18:18:50.902875: step 14450, total loss = 4.14, predict loss = 1.20 (98.6 examples/sec; 0.041 sec/batch; 1h:31m:37s remains)
INFO - root - 2019-11-06 18:18:51.362588: step 14460, total loss = 6.28, predict loss = 1.80 (95.4 examples/sec; 0.042 sec/batch; 1h:34m:45s remains)
INFO - root - 2019-11-06 18:18:52.751639: step 14470, total loss = 4.50, predict loss = 1.39 (64.7 examples/sec; 0.062 sec/batch; 2h:19m:37s remains)
INFO - root - 2019-11-06 18:18:53.480205: step 14480, total loss = 6.39, predict loss = 1.83 (57.8 examples/sec; 0.069 sec/batch; 2h:36m:20s remains)
INFO - root - 2019-11-06 18:18:54.242784: step 14490, total loss = 4.80, predict loss = 1.36 (58.6 examples/sec; 0.068 sec/batch; 2h:34m:06s remains)
INFO - root - 2019-11-06 18:18:55.029384: step 14500, total loss = 6.94, predict loss = 1.97 (57.5 examples/sec; 0.070 sec/batch; 2h:37m:03s remains)
INFO - root - 2019-11-06 18:18:55.733132: step 14510, total loss = 5.09, predict loss = 1.51 (68.1 examples/sec; 0.059 sec/batch; 2h:12m:43s remains)
INFO - root - 2019-11-06 18:18:56.220701: step 14520, total loss = 5.99, predict loss = 1.70 (96.0 examples/sec; 0.042 sec/batch; 1h:34m:05s remains)
INFO - root - 2019-11-06 18:18:56.702721: step 14530, total loss = 5.03, predict loss = 1.48 (99.8 examples/sec; 0.040 sec/batch; 1h:30m:28s remains)
INFO - root - 2019-11-06 18:18:57.893672: step 14540, total loss = 6.18, predict loss = 1.86 (67.1 examples/sec; 0.060 sec/batch; 2h:14m:40s remains)
INFO - root - 2019-11-06 18:18:58.590763: step 14550, total loss = 3.70, predict loss = 1.15 (63.0 examples/sec; 0.063 sec/batch; 2h:23m:19s remains)
INFO - root - 2019-11-06 18:18:59.374813: step 14560, total loss = 4.87, predict loss = 1.39 (59.7 examples/sec; 0.067 sec/batch; 2h:31m:15s remains)
INFO - root - 2019-11-06 18:19:00.132651: step 14570, total loss = 4.26, predict loss = 1.27 (52.8 examples/sec; 0.076 sec/batch; 2h:50m:56s remains)
INFO - root - 2019-11-06 18:19:00.892554: step 14580, total loss = 4.51, predict loss = 1.26 (62.5 examples/sec; 0.064 sec/batch; 2h:24m:23s remains)
INFO - root - 2019-11-06 18:19:01.442502: step 14590, total loss = 5.85, predict loss = 1.72 (93.4 examples/sec; 0.043 sec/batch; 1h:36m:38s remains)
INFO - root - 2019-11-06 18:19:01.901298: step 14600, total loss = 4.66, predict loss = 1.29 (91.5 examples/sec; 0.044 sec/batch; 1h:38m:38s remains)
INFO - root - 2019-11-06 18:19:02.372912: step 14610, total loss = 4.60, predict loss = 1.27 (144.0 examples/sec; 0.028 sec/batch; 1h:02m:40s remains)
INFO - root - 2019-11-06 18:19:03.756944: step 14620, total loss = 4.55, predict loss = 1.28 (58.7 examples/sec; 0.068 sec/batch; 2h:33m:50s remains)
INFO - root - 2019-11-06 18:19:04.491186: step 14630, total loss = 5.02, predict loss = 1.49 (64.2 examples/sec; 0.062 sec/batch; 2h:20m:28s remains)
INFO - root - 2019-11-06 18:19:05.219694: step 14640, total loss = 6.29, predict loss = 1.82 (62.2 examples/sec; 0.064 sec/batch; 2h:25m:00s remains)
INFO - root - 2019-11-06 18:19:05.984517: step 14650, total loss = 4.65, predict loss = 1.32 (53.4 examples/sec; 0.075 sec/batch; 2h:48m:54s remains)
INFO - root - 2019-11-06 18:19:06.680190: step 14660, total loss = 5.89, predict loss = 1.78 (69.6 examples/sec; 0.057 sec/batch; 2h:09m:33s remains)
INFO - root - 2019-11-06 18:19:07.135669: step 14670, total loss = 5.72, predict loss = 1.61 (95.0 examples/sec; 0.042 sec/batch; 1h:35m:00s remains)
INFO - root - 2019-11-06 18:19:07.581809: step 14680, total loss = 5.08, predict loss = 1.48 (97.6 examples/sec; 0.041 sec/batch; 1h:32m:25s remains)
INFO - root - 2019-11-06 18:19:08.832844: step 14690, total loss = 5.75, predict loss = 1.69 (68.3 examples/sec; 0.059 sec/batch; 2h:12m:06s remains)
INFO - root - 2019-11-06 18:19:09.559762: step 14700, total loss = 5.86, predict loss = 1.64 (53.9 examples/sec; 0.074 sec/batch; 2h:47m:15s remains)
INFO - root - 2019-11-06 18:19:10.291121: step 14710, total loss = 4.90, predict loss = 1.45 (59.3 examples/sec; 0.067 sec/batch; 2h:32m:10s remains)
INFO - root - 2019-11-06 18:19:11.052010: step 14720, total loss = 5.63, predict loss = 1.63 (55.3 examples/sec; 0.072 sec/batch; 2h:43m:07s remains)
INFO - root - 2019-11-06 18:19:11.875487: step 14730, total loss = 4.47, predict loss = 1.38 (61.3 examples/sec; 0.065 sec/batch; 2h:27m:09s remains)
INFO - root - 2019-11-06 18:19:12.428473: step 14740, total loss = 4.03, predict loss = 1.21 (88.5 examples/sec; 0.045 sec/batch; 1h:41m:55s remains)
INFO - root - 2019-11-06 18:19:12.889571: step 14750, total loss = 5.28, predict loss = 1.50 (91.1 examples/sec; 0.044 sec/batch; 1h:39m:01s remains)
INFO - root - 2019-11-06 18:19:13.989607: step 14760, total loss = 4.82, predict loss = 1.35 (5.6 examples/sec; 0.720 sec/batch; 27h:02m:15s remains)
INFO - root - 2019-11-06 18:19:14.707145: step 14770, total loss = 3.09, predict loss = 0.99 (59.1 examples/sec; 0.068 sec/batch; 2h:32m:25s remains)
INFO - root - 2019-11-06 18:19:15.440345: step 14780, total loss = 4.51, predict loss = 1.25 (61.9 examples/sec; 0.065 sec/batch; 2h:25m:40s remains)
INFO - root - 2019-11-06 18:19:16.239675: step 14790, total loss = 5.62, predict loss = 1.71 (57.0 examples/sec; 0.070 sec/batch; 2h:38m:12s remains)
INFO - root - 2019-11-06 18:19:17.030001: step 14800, total loss = 4.57, predict loss = 1.37 (60.3 examples/sec; 0.066 sec/batch; 2h:29m:25s remains)
INFO - root - 2019-11-06 18:19:17.687521: step 14810, total loss = 4.96, predict loss = 1.43 (93.7 examples/sec; 0.043 sec/batch; 1h:36m:09s remains)
INFO - root - 2019-11-06 18:19:18.121904: step 14820, total loss = 6.51, predict loss = 1.88 (100.9 examples/sec; 0.040 sec/batch; 1h:29m:17s remains)
INFO - root - 2019-11-06 18:19:18.561619: step 14830, total loss = 3.27, predict loss = 0.86 (102.6 examples/sec; 0.039 sec/batch; 1h:27m:51s remains)
INFO - root - 2019-11-06 18:19:19.782720: step 14840, total loss = 6.13, predict loss = 1.72 (53.4 examples/sec; 0.075 sec/batch; 2h:48m:40s remains)
INFO - root - 2019-11-06 18:19:20.508121: step 14850, total loss = 4.89, predict loss = 1.45 (65.2 examples/sec; 0.061 sec/batch; 2h:18m:12s remains)
INFO - root - 2019-11-06 18:19:21.222536: step 14860, total loss = 4.89, predict loss = 1.49 (63.2 examples/sec; 0.063 sec/batch; 2h:22m:36s remains)
INFO - root - 2019-11-06 18:19:22.072139: step 14870, total loss = 5.64, predict loss = 1.65 (50.4 examples/sec; 0.079 sec/batch; 2h:58m:35s remains)
INFO - root - 2019-11-06 18:19:22.786753: step 14880, total loss = 6.44, predict loss = 1.92 (68.5 examples/sec; 0.058 sec/batch; 2h:11m:26s remains)
INFO - root - 2019-11-06 18:19:23.377908: step 14890, total loss = 3.54, predict loss = 1.04 (97.4 examples/sec; 0.041 sec/batch; 1h:32m:30s remains)
INFO - root - 2019-11-06 18:19:23.837966: step 14900, total loss = 4.41, predict loss = 1.22 (98.8 examples/sec; 0.040 sec/batch; 1h:31m:09s remains)
INFO - root - 2019-11-06 18:19:24.991434: step 14910, total loss = 5.50, predict loss = 1.47 (66.6 examples/sec; 0.060 sec/batch; 2h:15m:07s remains)
INFO - root - 2019-11-06 18:19:25.685198: step 14920, total loss = 4.53, predict loss = 1.38 (53.1 examples/sec; 0.075 sec/batch; 2h:49m:39s remains)
INFO - root - 2019-11-06 18:19:26.440977: step 14930, total loss = 4.70, predict loss = 1.38 (60.1 examples/sec; 0.067 sec/batch; 2h:29m:46s remains)
INFO - root - 2019-11-06 18:19:27.218389: step 14940, total loss = 6.46, predict loss = 1.91 (51.4 examples/sec; 0.078 sec/batch; 2h:55m:18s remains)
INFO - root - 2019-11-06 18:19:27.949728: step 14950, total loss = 5.51, predict loss = 1.62 (59.4 examples/sec; 0.067 sec/batch; 2h:31m:31s remains)
INFO - root - 2019-11-06 18:19:28.639960: step 14960, total loss = 5.36, predict loss = 1.58 (92.1 examples/sec; 0.043 sec/batch; 1h:37m:43s remains)
INFO - root - 2019-11-06 18:19:29.124350: step 14970, total loss = 4.94, predict loss = 1.44 (95.9 examples/sec; 0.042 sec/batch; 1h:33m:50s remains)
INFO - root - 2019-11-06 18:19:29.586747: step 14980, total loss = 3.15, predict loss = 0.86 (85.4 examples/sec; 0.047 sec/batch; 1h:45m:25s remains)
INFO - root - 2019-11-06 18:19:30.820099: step 14990, total loss = 4.84, predict loss = 1.55 (61.2 examples/sec; 0.065 sec/batch; 2h:26m:58s remains)
INFO - root - 2019-11-06 18:19:31.589642: step 15000, total loss = 3.10, predict loss = 0.96 (58.0 examples/sec; 0.069 sec/batch; 2h:35m:14s remains)
INFO - root - 2019-11-06 18:19:32.918567: step 15010, total loss = 5.52, predict loss = 1.52 (66.9 examples/sec; 0.060 sec/batch; 2h:14m:34s remains)
INFO - root - 2019-11-06 18:19:33.588921: step 15020, total loss = 5.24, predict loss = 1.51 (68.7 examples/sec; 0.058 sec/batch; 2h:11m:04s remains)
INFO - root - 2019-11-06 18:19:34.281097: step 15030, total loss = 5.13, predict loss = 1.40 (61.7 examples/sec; 0.065 sec/batch; 2h:25m:53s remains)
INFO - root - 2019-11-06 18:19:34.824042: step 15040, total loss = 5.77, predict loss = 1.61 (94.1 examples/sec; 0.043 sec/batch; 1h:35m:37s remains)
INFO - root - 2019-11-06 18:19:35.311494: step 15050, total loss = 6.99, predict loss = 2.02 (105.0 examples/sec; 0.038 sec/batch; 1h:25m:39s remains)
INFO - root - 2019-11-06 18:19:36.467683: step 15060, total loss = 5.42, predict loss = 1.60 (66.0 examples/sec; 0.061 sec/batch; 2h:16m:18s remains)
INFO - root - 2019-11-06 18:19:37.212792: step 15070, total loss = 4.35, predict loss = 1.23 (55.6 examples/sec; 0.072 sec/batch; 2h:41m:39s remains)
INFO - root - 2019-11-06 18:19:37.949165: step 15080, total loss = 4.82, predict loss = 1.43 (65.6 examples/sec; 0.061 sec/batch; 2h:17m:05s remains)
INFO - root - 2019-11-06 18:19:38.711174: step 15090, total loss = 4.85, predict loss = 1.37 (61.5 examples/sec; 0.065 sec/batch; 2h:26m:19s remains)
INFO - root - 2019-11-06 18:19:39.447320: step 15100, total loss = 5.83, predict loss = 1.67 (56.6 examples/sec; 0.071 sec/batch; 2h:39m:00s remains)
INFO - root - 2019-11-06 18:19:40.051881: step 15110, total loss = 5.09, predict loss = 1.48 (102.4 examples/sec; 0.039 sec/batch; 1h:27m:48s remains)
INFO - root - 2019-11-06 18:19:40.491520: step 15120, total loss = 3.15, predict loss = 0.94 (95.0 examples/sec; 0.042 sec/batch; 1h:34m:41s remains)
INFO - root - 2019-11-06 18:19:40.974776: step 15130, total loss = 4.56, predict loss = 1.24 (100.0 examples/sec; 0.040 sec/batch; 1h:29m:54s remains)
INFO - root - 2019-11-06 18:19:42.266373: step 15140, total loss = 4.07, predict loss = 1.26 (60.7 examples/sec; 0.066 sec/batch; 2h:28m:05s remains)
INFO - root - 2019-11-06 18:19:43.029041: step 15150, total loss = 5.70, predict loss = 1.62 (60.1 examples/sec; 0.067 sec/batch; 2h:29m:39s remains)
INFO - root - 2019-11-06 18:19:43.778522: step 15160, total loss = 6.33, predict loss = 1.86 (52.9 examples/sec; 0.076 sec/batch; 2h:49m:49s remains)
INFO - root - 2019-11-06 18:19:44.539011: step 15170, total loss = 4.30, predict loss = 1.25 (59.5 examples/sec; 0.067 sec/batch; 2h:31m:04s remains)
INFO - root - 2019-11-06 18:19:45.284385: step 15180, total loss = 4.10, predict loss = 1.19 (63.5 examples/sec; 0.063 sec/batch; 2h:21m:31s remains)
INFO - root - 2019-11-06 18:19:45.818014: step 15190, total loss = 6.00, predict loss = 1.75 (97.5 examples/sec; 0.041 sec/batch; 1h:32m:09s remains)
INFO - root - 2019-11-06 18:19:46.267936: step 15200, total loss = 4.69, predict loss = 1.29 (90.8 examples/sec; 0.044 sec/batch; 1h:38m:59s remains)
INFO - root - 2019-11-06 18:19:47.456137: step 15210, total loss = 4.86, predict loss = 1.45 (71.1 examples/sec; 0.056 sec/batch; 2h:06m:18s remains)
INFO - root - 2019-11-06 18:19:48.198789: step 15220, total loss = 4.13, predict loss = 1.24 (58.1 examples/sec; 0.069 sec/batch; 2h:34m:46s remains)
INFO - root - 2019-11-06 18:19:48.968766: step 15230, total loss = 5.46, predict loss = 1.53 (56.9 examples/sec; 0.070 sec/batch; 2h:38m:00s remains)
INFO - root - 2019-11-06 18:19:49.778853: step 15240, total loss = 3.91, predict loss = 1.16 (55.9 examples/sec; 0.072 sec/batch; 2h:40m:38s remains)
INFO - root - 2019-11-06 18:19:50.557699: step 15250, total loss = 1.85, predict loss = 0.56 (51.7 examples/sec; 0.077 sec/batch; 2h:53m:53s remains)
INFO - root - 2019-11-06 18:19:51.142132: step 15260, total loss = 4.76, predict loss = 1.37 (97.4 examples/sec; 0.041 sec/batch; 1h:32m:11s remains)
INFO - root - 2019-11-06 18:19:51.604997: step 15270, total loss = 4.57, predict loss = 1.35 (93.4 examples/sec; 0.043 sec/batch; 1h:36m:08s remains)
INFO - root - 2019-11-06 18:19:52.095135: step 15280, total loss = 3.90, predict loss = 1.12 (86.1 examples/sec; 0.046 sec/batch; 1h:44m:17s remains)
INFO - root - 2019-11-06 18:19:53.445820: step 15290, total loss = 5.48, predict loss = 1.63 (62.9 examples/sec; 0.064 sec/batch; 2h:22m:42s remains)
INFO - root - 2019-11-06 18:19:54.271818: step 15300, total loss = 3.62, predict loss = 1.07 (53.6 examples/sec; 0.075 sec/batch; 2h:47m:34s remains)
INFO - root - 2019-11-06 18:19:55.075396: step 15310, total loss = 4.63, predict loss = 1.36 (54.0 examples/sec; 0.074 sec/batch; 2h:46m:13s remains)
INFO - root - 2019-11-06 18:19:55.799023: step 15320, total loss = 4.35, predict loss = 1.27 (65.7 examples/sec; 0.061 sec/batch; 2h:16m:42s remains)
INFO - root - 2019-11-06 18:19:56.510300: step 15330, total loss = 2.80, predict loss = 0.80 (73.2 examples/sec; 0.055 sec/batch; 2h:02m:35s remains)
INFO - root - 2019-11-06 18:19:56.999585: step 15340, total loss = 4.45, predict loss = 1.21 (92.5 examples/sec; 0.043 sec/batch; 1h:37m:05s remains)
INFO - root - 2019-11-06 18:19:57.446532: step 15350, total loss = 3.45, predict loss = 1.09 (95.4 examples/sec; 0.042 sec/batch; 1h:34m:03s remains)
INFO - root - 2019-11-06 18:19:58.611391: step 15360, total loss = 4.94, predict loss = 1.39 (61.9 examples/sec; 0.065 sec/batch; 2h:25m:02s remains)
INFO - root - 2019-11-06 18:19:59.353662: step 15370, total loss = 2.88, predict loss = 0.82 (67.8 examples/sec; 0.059 sec/batch; 2h:12m:21s remains)
INFO - root - 2019-11-06 18:20:00.107111: step 15380, total loss = 4.61, predict loss = 1.38 (57.4 examples/sec; 0.070 sec/batch; 2h:36m:17s remains)
INFO - root - 2019-11-06 18:20:00.978191: step 15390, total loss = 6.25, predict loss = 1.74 (47.8 examples/sec; 0.084 sec/batch; 3h:07m:53s remains)
INFO - root - 2019-11-06 18:20:01.790860: step 15400, total loss = 5.32, predict loss = 1.63 (52.3 examples/sec; 0.077 sec/batch; 2h:51m:44s remains)
INFO - root - 2019-11-06 18:20:02.399185: step 15410, total loss = 5.11, predict loss = 1.45 (93.4 examples/sec; 0.043 sec/batch; 1h:36m:03s remains)
INFO - root - 2019-11-06 18:20:02.845553: step 15420, total loss = 4.94, predict loss = 1.42 (96.3 examples/sec; 0.042 sec/batch; 1h:33m:11s remains)
INFO - root - 2019-11-06 18:20:03.302762: step 15430, total loss = 4.36, predict loss = 1.20 (127.6 examples/sec; 0.031 sec/batch; 1h:10m:17s remains)
INFO - root - 2019-11-06 18:20:04.663617: step 15440, total loss = 3.87, predict loss = 1.15 (52.7 examples/sec; 0.076 sec/batch; 2h:50m:22s remains)
INFO - root - 2019-11-06 18:20:05.366352: step 15450, total loss = 4.98, predict loss = 1.41 (68.2 examples/sec; 0.059 sec/batch; 2h:11m:30s remains)
INFO - root - 2019-11-06 18:20:06.100403: step 15460, total loss = 6.27, predict loss = 1.84 (63.9 examples/sec; 0.063 sec/batch; 2h:20m:22s remains)
INFO - root - 2019-11-06 18:20:06.904137: step 15470, total loss = 5.49, predict loss = 1.56 (53.8 examples/sec; 0.074 sec/batch; 2h:46m:45s remains)
INFO - root - 2019-11-06 18:20:07.561497: step 15480, total loss = 3.61, predict loss = 0.94 (81.7 examples/sec; 0.049 sec/batch; 1h:49m:49s remains)
INFO - root - 2019-11-06 18:20:08.051012: step 15490, total loss = 6.67, predict loss = 1.89 (89.4 examples/sec; 0.045 sec/batch; 1h:40m:18s remains)
INFO - root - 2019-11-06 18:20:08.507413: step 15500, total loss = 3.43, predict loss = 0.99 (90.7 examples/sec; 0.044 sec/batch; 1h:38m:54s remains)
INFO - root - 2019-11-06 18:20:09.771884: step 15510, total loss = 5.44, predict loss = 1.58 (65.3 examples/sec; 0.061 sec/batch; 2h:17m:23s remains)
INFO - root - 2019-11-06 18:20:10.497412: step 15520, total loss = 3.93, predict loss = 1.13 (58.5 examples/sec; 0.068 sec/batch; 2h:33m:18s remains)
INFO - root - 2019-11-06 18:20:11.236553: step 15530, total loss = 4.60, predict loss = 1.32 (66.8 examples/sec; 0.060 sec/batch; 2h:14m:15s remains)
INFO - root - 2019-11-06 18:20:11.959157: step 15540, total loss = 2.69, predict loss = 0.72 (59.7 examples/sec; 0.067 sec/batch; 2h:30m:07s remains)
INFO - root - 2019-11-06 18:20:12.701423: step 15550, total loss = 6.09, predict loss = 1.78 (63.2 examples/sec; 0.063 sec/batch; 2h:21m:45s remains)
INFO - root - 2019-11-06 18:20:13.237935: step 15560, total loss = 5.32, predict loss = 1.49 (98.9 examples/sec; 0.040 sec/batch; 1h:30m:37s remains)
INFO - root - 2019-11-06 18:20:13.725264: step 15570, total loss = 3.75, predict loss = 1.12 (91.3 examples/sec; 0.044 sec/batch; 1h:38m:10s remains)
INFO - root - 2019-11-06 18:20:14.836815: step 15580, total loss = 5.36, predict loss = 1.58 (5.8 examples/sec; 0.692 sec/batch; 25h:49m:37s remains)
INFO - root - 2019-11-06 18:20:15.509616: step 15590, total loss = 4.30, predict loss = 1.22 (56.0 examples/sec; 0.071 sec/batch; 2h:40m:00s remains)
INFO - root - 2019-11-06 18:20:16.274552: step 15600, total loss = 3.75, predict loss = 1.12 (57.2 examples/sec; 0.070 sec/batch; 2h:36m:35s remains)
INFO - root - 2019-11-06 18:20:17.046833: step 15610, total loss = 4.28, predict loss = 1.24 (55.4 examples/sec; 0.072 sec/batch; 2h:41m:40s remains)
INFO - root - 2019-11-06 18:20:17.785914: step 15620, total loss = 6.30, predict loss = 1.84 (63.5 examples/sec; 0.063 sec/batch; 2h:21m:05s remains)
INFO - root - 2019-11-06 18:20:18.468174: step 15630, total loss = 4.22, predict loss = 1.19 (92.3 examples/sec; 0.043 sec/batch; 1h:37m:04s remains)
INFO - root - 2019-11-06 18:20:18.922504: step 15640, total loss = 5.03, predict loss = 1.42 (95.1 examples/sec; 0.042 sec/batch; 1h:34m:11s remains)
INFO - root - 2019-11-06 18:20:19.394750: step 15650, total loss = 4.07, predict loss = 1.15 (98.6 examples/sec; 0.041 sec/batch; 1h:30m:49s remains)
INFO - root - 2019-11-06 18:20:20.614452: step 15660, total loss = 5.49, predict loss = 1.55 (62.8 examples/sec; 0.064 sec/batch; 2h:22m:34s remains)
INFO - root - 2019-11-06 18:20:21.383147: step 15670, total loss = 4.77, predict loss = 1.38 (51.2 examples/sec; 0.078 sec/batch; 2h:54m:57s remains)
INFO - root - 2019-11-06 18:20:22.224256: step 15680, total loss = 6.80, predict loss = 1.82 (52.0 examples/sec; 0.077 sec/batch; 2h:52m:08s remains)
INFO - root - 2019-11-06 18:20:22.960014: step 15690, total loss = 3.70, predict loss = 1.12 (54.2 examples/sec; 0.074 sec/batch; 2h:45m:18s remains)
INFO - root - 2019-11-06 18:20:23.746853: step 15700, total loss = 5.96, predict loss = 1.79 (57.8 examples/sec; 0.069 sec/batch; 2h:35m:00s remains)
INFO - root - 2019-11-06 18:20:24.276236: step 15710, total loss = 5.87, predict loss = 1.71 (95.9 examples/sec; 0.042 sec/batch; 1h:33m:20s remains)
INFO - root - 2019-11-06 18:20:24.732312: step 15720, total loss = 4.36, predict loss = 1.21 (103.6 examples/sec; 0.039 sec/batch; 1h:26m:24s remains)
INFO - root - 2019-11-06 18:20:25.871611: step 15730, total loss = 4.87, predict loss = 1.44 (72.9 examples/sec; 0.055 sec/batch; 2h:02m:43s remains)
INFO - root - 2019-11-06 18:20:26.578838: step 15740, total loss = 4.14, predict loss = 1.28 (60.3 examples/sec; 0.066 sec/batch; 2h:28m:22s remains)
INFO - root - 2019-11-06 18:20:27.305542: step 15750, total loss = 5.00, predict loss = 1.48 (61.5 examples/sec; 0.065 sec/batch; 2h:25m:27s remains)
INFO - root - 2019-11-06 18:20:28.020835: step 15760, total loss = 3.68, predict loss = 1.06 (57.2 examples/sec; 0.070 sec/batch; 2h:36m:23s remains)
INFO - root - 2019-11-06 18:20:28.754182: step 15770, total loss = 5.85, predict loss = 1.68 (65.1 examples/sec; 0.061 sec/batch; 2h:17m:24s remains)
INFO - root - 2019-11-06 18:20:29.390889: step 15780, total loss = 5.35, predict loss = 1.51 (91.1 examples/sec; 0.044 sec/batch; 1h:38m:10s remains)
INFO - root - 2019-11-06 18:20:29.854504: step 15790, total loss = 4.91, predict loss = 1.34 (91.6 examples/sec; 0.044 sec/batch; 1h:37m:43s remains)
INFO - root - 2019-11-06 18:20:30.308089: step 15800, total loss = 6.14, predict loss = 1.81 (95.0 examples/sec; 0.042 sec/batch; 1h:34m:12s remains)
INFO - root - 2019-11-06 18:20:31.579386: step 15810, total loss = 5.45, predict loss = 1.55 (63.3 examples/sec; 0.063 sec/batch; 2h:21m:19s remains)
INFO - root - 2019-11-06 18:20:32.342525: step 15820, total loss = 4.21, predict loss = 1.25 (55.5 examples/sec; 0.072 sec/batch; 2h:41m:05s remains)
INFO - root - 2019-11-06 18:20:33.124402: step 15830, total loss = 3.75, predict loss = 1.11 (55.4 examples/sec; 0.072 sec/batch; 2h:41m:22s remains)
INFO - root - 2019-11-06 18:20:33.959320: step 15840, total loss = 3.54, predict loss = 1.10 (49.6 examples/sec; 0.081 sec/batch; 3h:00m:16s remains)
INFO - root - 2019-11-06 18:20:34.661369: step 15850, total loss = 5.57, predict loss = 1.65 (64.5 examples/sec; 0.062 sec/batch; 2h:18m:39s remains)
INFO - root - 2019-11-06 18:20:35.172781: step 15860, total loss = 6.44, predict loss = 1.83 (96.0 examples/sec; 0.042 sec/batch; 1h:33m:10s remains)
INFO - root - 2019-11-06 18:20:35.617894: step 15870, total loss = 4.17, predict loss = 1.22 (93.1 examples/sec; 0.043 sec/batch; 1h:36m:01s remains)
INFO - root - 2019-11-06 18:20:36.819883: step 15880, total loss = 6.52, predict loss = 1.93 (67.1 examples/sec; 0.060 sec/batch; 2h:13m:19s remains)
INFO - root - 2019-11-06 18:20:37.536605: step 15890, total loss = 5.70, predict loss = 1.72 (60.2 examples/sec; 0.066 sec/batch; 2h:28m:37s remains)
INFO - root - 2019-11-06 18:20:38.264218: step 15900, total loss = 5.60, predict loss = 1.64 (50.6 examples/sec; 0.079 sec/batch; 2h:56m:43s remains)
INFO - root - 2019-11-06 18:20:39.012325: step 15910, total loss = 5.16, predict loss = 1.54 (58.4 examples/sec; 0.068 sec/batch; 2h:33m:02s remains)
INFO - root - 2019-11-06 18:20:39.755996: step 15920, total loss = 3.15, predict loss = 0.99 (58.3 examples/sec; 0.069 sec/batch; 2h:33m:26s remains)
INFO - root - 2019-11-06 18:20:40.402148: step 15930, total loss = 3.82, predict loss = 1.12 (94.7 examples/sec; 0.042 sec/batch; 1h:34m:20s remains)
INFO - root - 2019-11-06 18:20:40.861466: step 15940, total loss = 6.72, predict loss = 1.90 (94.1 examples/sec; 0.042 sec/batch; 1h:34m:56s remains)
INFO - root - 2019-11-06 18:20:41.320985: step 15950, total loss = 4.81, predict loss = 1.39 (94.8 examples/sec; 0.042 sec/batch; 1h:34m:13s remains)
INFO - root - 2019-11-06 18:20:42.622059: step 15960, total loss = 6.35, predict loss = 1.74 (62.4 examples/sec; 0.064 sec/batch; 2h:23m:09s remains)
INFO - root - 2019-11-06 18:20:43.360194: step 15970, total loss = 5.75, predict loss = 1.61 (57.5 examples/sec; 0.070 sec/batch; 2h:35m:22s remains)
INFO - root - 2019-11-06 18:20:44.044035: step 15980, total loss = 5.03, predict loss = 1.38 (65.9 examples/sec; 0.061 sec/batch; 2h:15m:33s remains)
INFO - root - 2019-11-06 18:20:44.777210: step 15990, total loss = 4.84, predict loss = 1.43 (61.2 examples/sec; 0.065 sec/batch; 2h:25m:58s remains)
INFO - root - 2019-11-06 18:20:45.488423: step 16000, total loss = 6.12, predict loss = 1.76 (63.3 examples/sec; 0.063 sec/batch; 2h:21m:11s remains)
INFO - root - 2019-11-06 18:20:45.995434: step 16010, total loss = 5.74, predict loss = 1.67 (100.2 examples/sec; 0.040 sec/batch; 1h:29m:09s remains)
INFO - root - 2019-11-06 18:20:46.454461: step 16020, total loss = 5.73, predict loss = 1.62 (93.0 examples/sec; 0.043 sec/batch; 1h:36m:01s remains)
INFO - root - 2019-11-06 18:20:47.651216: step 16030, total loss = 3.26, predict loss = 0.96 (66.3 examples/sec; 0.060 sec/batch; 2h:14m:37s remains)
INFO - root - 2019-11-06 18:20:48.402120: step 16040, total loss = 5.17, predict loss = 1.42 (54.7 examples/sec; 0.073 sec/batch; 2h:43m:15s remains)
INFO - root - 2019-11-06 18:20:49.135759: step 16050, total loss = 4.56, predict loss = 1.32 (74.1 examples/sec; 0.054 sec/batch; 2h:00m:30s remains)
INFO - root - 2019-11-06 18:20:49.878987: step 16060, total loss = 4.54, predict loss = 1.28 (57.2 examples/sec; 0.070 sec/batch; 2h:36m:13s remains)
INFO - root - 2019-11-06 18:20:50.663605: step 16070, total loss = 5.94, predict loss = 1.66 (53.1 examples/sec; 0.075 sec/batch; 2h:48m:02s remains)
INFO - root - 2019-11-06 18:20:51.255210: step 16080, total loss = 6.79, predict loss = 1.90 (102.2 examples/sec; 0.039 sec/batch; 1h:27m:20s remains)
INFO - root - 2019-11-06 18:20:51.744026: step 16090, total loss = 6.09, predict loss = 1.70 (85.7 examples/sec; 0.047 sec/batch; 1h:44m:06s remains)
INFO - root - 2019-11-06 18:20:52.225760: step 16100, total loss = 5.11, predict loss = 1.38 (106.4 examples/sec; 0.038 sec/batch; 1h:23m:52s remains)
INFO - root - 2019-11-06 18:20:53.510925: step 16110, total loss = 5.61, predict loss = 1.66 (65.1 examples/sec; 0.061 sec/batch; 2h:17m:06s remains)
INFO - root - 2019-11-06 18:20:54.230578: step 16120, total loss = 5.44, predict loss = 1.48 (57.6 examples/sec; 0.069 sec/batch; 2h:34m:55s remains)
INFO - root - 2019-11-06 18:20:54.989693: step 16130, total loss = 4.79, predict loss = 1.34 (57.4 examples/sec; 0.070 sec/batch; 2h:35m:32s remains)
INFO - root - 2019-11-06 18:20:55.774628: step 16140, total loss = 6.36, predict loss = 1.80 (53.8 examples/sec; 0.074 sec/batch; 2h:45m:50s remains)
INFO - root - 2019-11-06 18:20:56.530729: step 16150, total loss = 5.61, predict loss = 1.55 (61.5 examples/sec; 0.065 sec/batch; 2h:25m:10s remains)
INFO - root - 2019-11-06 18:20:57.030454: step 16160, total loss = 5.05, predict loss = 1.48 (96.1 examples/sec; 0.042 sec/batch; 1h:32m:50s remains)
INFO - root - 2019-11-06 18:20:57.510789: step 16170, total loss = 5.40, predict loss = 1.53 (96.8 examples/sec; 0.041 sec/batch; 1h:32m:12s remains)
INFO - root - 2019-11-06 18:20:58.722487: step 16180, total loss = 6.57, predict loss = 1.94 (67.7 examples/sec; 0.059 sec/batch; 2h:11m:46s remains)
INFO - root - 2019-11-06 18:20:59.450085: step 16190, total loss = 6.48, predict loss = 1.91 (64.8 examples/sec; 0.062 sec/batch; 2h:17m:45s remains)
INFO - root - 2019-11-06 18:21:00.207755: step 16200, total loss = 5.36, predict loss = 1.63 (60.0 examples/sec; 0.067 sec/batch; 2h:28m:42s remains)
INFO - root - 2019-11-06 18:21:00.912544: step 16210, total loss = 5.17, predict loss = 1.48 (61.3 examples/sec; 0.065 sec/batch; 2h:25m:31s remains)
INFO - root - 2019-11-06 18:21:01.682238: step 16220, total loss = 3.84, predict loss = 1.11 (58.1 examples/sec; 0.069 sec/batch; 2h:33m:32s remains)
INFO - root - 2019-11-06 18:21:02.257693: step 16230, total loss = 5.31, predict loss = 1.54 (96.9 examples/sec; 0.041 sec/batch; 1h:31m:59s remains)
INFO - root - 2019-11-06 18:21:02.717893: step 16240, total loss = 3.79, predict loss = 1.07 (96.2 examples/sec; 0.042 sec/batch; 1h:32m:42s remains)
INFO - root - 2019-11-06 18:21:03.194021: step 16250, total loss = 6.42, predict loss = 1.84 (120.4 examples/sec; 0.033 sec/batch; 1h:14m:04s remains)
INFO - root - 2019-11-06 18:21:04.583565: step 16260, total loss = 3.50, predict loss = 1.00 (55.6 examples/sec; 0.072 sec/batch; 2h:40m:16s remains)
INFO - root - 2019-11-06 18:21:05.327263: step 16270, total loss = 6.00, predict loss = 1.75 (59.7 examples/sec; 0.067 sec/batch; 2h:29m:17s remains)
INFO - root - 2019-11-06 18:21:06.102124: step 16280, total loss = 3.69, predict loss = 1.04 (51.9 examples/sec; 0.077 sec/batch; 2h:51m:53s remains)
INFO - root - 2019-11-06 18:21:06.875393: step 16290, total loss = 4.95, predict loss = 1.43 (52.8 examples/sec; 0.076 sec/batch; 2h:48m:43s remains)
INFO - root - 2019-11-06 18:21:07.615696: step 16300, total loss = 3.93, predict loss = 1.16 (69.7 examples/sec; 0.057 sec/batch; 2h:07m:56s remains)
INFO - root - 2019-11-06 18:21:08.075483: step 16310, total loss = 5.37, predict loss = 1.56 (98.1 examples/sec; 0.041 sec/batch; 1h:30m:51s remains)
INFO - root - 2019-11-06 18:21:08.527571: step 16320, total loss = 5.38, predict loss = 1.57 (97.2 examples/sec; 0.041 sec/batch; 1h:31m:41s remains)
INFO - root - 2019-11-06 18:21:09.785356: step 16330, total loss = 5.00, predict loss = 1.35 (66.9 examples/sec; 0.060 sec/batch; 2h:13m:10s remains)
INFO - root - 2019-11-06 18:21:10.486731: step 16340, total loss = 3.47, predict loss = 1.04 (61.1 examples/sec; 0.065 sec/batch; 2h:25m:46s remains)
INFO - root - 2019-11-06 18:21:11.226141: step 16350, total loss = 4.54, predict loss = 1.29 (64.4 examples/sec; 0.062 sec/batch; 2h:18m:24s remains)
INFO - root - 2019-11-06 18:21:11.929589: step 16360, total loss = 5.01, predict loss = 1.50 (57.8 examples/sec; 0.069 sec/batch; 2h:34m:04s remains)
INFO - root - 2019-11-06 18:21:12.652807: step 16370, total loss = 4.73, predict loss = 1.39 (67.2 examples/sec; 0.059 sec/batch; 2h:12m:29s remains)
INFO - root - 2019-11-06 18:21:13.179161: step 16380, total loss = 3.62, predict loss = 1.07 (96.8 examples/sec; 0.041 sec/batch; 1h:32m:03s remains)
INFO - root - 2019-11-06 18:21:13.629519: step 16390, total loss = 3.24, predict loss = 0.90 (92.4 examples/sec; 0.043 sec/batch; 1h:36m:24s remains)
INFO - root - 2019-11-06 18:21:14.763210: step 16400, total loss = 6.63, predict loss = 1.91 (5.5 examples/sec; 0.725 sec/batch; 26h:53m:24s remains)
INFO - root - 2019-11-06 18:21:15.495682: step 16410, total loss = 4.99, predict loss = 1.42 (54.6 examples/sec; 0.073 sec/batch; 2h:43m:05s remains)
INFO - root - 2019-11-06 18:21:16.243042: step 16420, total loss = 6.19, predict loss = 1.77 (54.8 examples/sec; 0.073 sec/batch; 2h:42m:35s remains)
INFO - root - 2019-11-06 18:21:16.994063: step 16430, total loss = 3.40, predict loss = 0.97 (60.8 examples/sec; 0.066 sec/batch; 2h:26m:22s remains)
INFO - root - 2019-11-06 18:21:17.766950: step 16440, total loss = 5.61, predict loss = 1.50 (59.4 examples/sec; 0.067 sec/batch; 2h:30m:00s remains)
INFO - root - 2019-11-06 18:21:18.466866: step 16450, total loss = 3.55, predict loss = 1.03 (87.4 examples/sec; 0.046 sec/batch; 1h:41m:54s remains)
INFO - root - 2019-11-06 18:21:18.911167: step 16460, total loss = 3.39, predict loss = 1.03 (99.0 examples/sec; 0.040 sec/batch; 1h:29m:56s remains)
INFO - root - 2019-11-06 18:21:19.362246: step 16470, total loss = 4.10, predict loss = 1.22 (90.0 examples/sec; 0.044 sec/batch; 1h:38m:53s remains)
INFO - root - 2019-11-06 18:21:20.604717: step 16480, total loss = 5.08, predict loss = 1.45 (60.7 examples/sec; 0.066 sec/batch; 2h:26m:42s remains)
INFO - root - 2019-11-06 18:21:21.326911: step 16490, total loss = 4.86, predict loss = 1.37 (68.3 examples/sec; 0.059 sec/batch; 2h:10m:17s remains)
INFO - root - 2019-11-06 18:21:22.087899: step 16500, total loss = 3.91, predict loss = 1.13 (55.2 examples/sec; 0.073 sec/batch; 2h:41m:19s remains)
INFO - root - 2019-11-06 18:21:22.831369: step 16510, total loss = 5.43, predict loss = 1.56 (52.7 examples/sec; 0.076 sec/batch; 2h:48m:48s remains)
INFO - root - 2019-11-06 18:21:23.629535: step 16520, total loss = 5.89, predict loss = 1.62 (63.4 examples/sec; 0.063 sec/batch; 2h:20m:22s remains)
INFO - root - 2019-11-06 18:21:24.195003: step 16530, total loss = 4.72, predict loss = 1.31 (100.7 examples/sec; 0.040 sec/batch; 1h:28m:20s remains)
INFO - root - 2019-11-06 18:21:24.643016: step 16540, total loss = 5.71, predict loss = 1.56 (96.6 examples/sec; 0.041 sec/batch; 1h:32m:06s remains)
INFO - root - 2019-11-06 18:21:25.748074: step 16550, total loss = 5.37, predict loss = 1.59 (65.3 examples/sec; 0.061 sec/batch; 2h:16m:14s remains)
INFO - root - 2019-11-06 18:21:26.404605: step 16560, total loss = 5.33, predict loss = 1.54 (67.5 examples/sec; 0.059 sec/batch; 2h:11m:44s remains)
INFO - root - 2019-11-06 18:21:27.162322: step 16570, total loss = 5.39, predict loss = 1.53 (66.2 examples/sec; 0.060 sec/batch; 2h:14m:20s remains)
INFO - root - 2019-11-06 18:21:27.946371: step 16580, total loss = 5.78, predict loss = 1.63 (52.9 examples/sec; 0.076 sec/batch; 2h:48m:01s remains)
INFO - root - 2019-11-06 18:21:28.783135: step 16590, total loss = 4.54, predict loss = 1.34 (55.0 examples/sec; 0.073 sec/batch; 2h:41m:45s remains)
INFO - root - 2019-11-06 18:21:29.399863: step 16600, total loss = 4.27, predict loss = 1.24 (100.9 examples/sec; 0.040 sec/batch; 1h:28m:08s remains)
INFO - root - 2019-11-06 18:21:29.858520: step 16610, total loss = 4.75, predict loss = 1.36 (89.4 examples/sec; 0.045 sec/batch; 1h:39m:27s remains)
INFO - root - 2019-11-06 18:21:30.327943: step 16620, total loss = 5.82, predict loss = 1.61 (92.1 examples/sec; 0.043 sec/batch; 1h:36m:30s remains)
INFO - root - 2019-11-06 18:21:31.619203: step 16630, total loss = 5.03, predict loss = 1.50 (48.6 examples/sec; 0.082 sec/batch; 3h:03m:00s remains)
INFO - root - 2019-11-06 18:21:32.363497: step 16640, total loss = 5.33, predict loss = 1.54 (60.5 examples/sec; 0.066 sec/batch; 2h:26m:54s remains)
INFO - root - 2019-11-06 18:21:33.136892: step 16650, total loss = 5.11, predict loss = 1.47 (53.6 examples/sec; 0.075 sec/batch; 2h:45m:54s remains)
INFO - root - 2019-11-06 18:21:33.916272: step 16660, total loss = 5.37, predict loss = 1.60 (46.9 examples/sec; 0.085 sec/batch; 3h:09m:29s remains)
INFO - root - 2019-11-06 18:21:34.655306: step 16670, total loss = 5.72, predict loss = 1.64 (67.7 examples/sec; 0.059 sec/batch; 2h:11m:16s remains)
INFO - root - 2019-11-06 18:21:35.164126: step 16680, total loss = 6.79, predict loss = 1.98 (98.6 examples/sec; 0.041 sec/batch; 1h:30m:07s remains)
INFO - root - 2019-11-06 18:21:35.657109: step 16690, total loss = 5.61, predict loss = 1.63 (91.6 examples/sec; 0.044 sec/batch; 1h:37m:01s remains)
INFO - root - 2019-11-06 18:21:36.816009: step 16700, total loss = 4.92, predict loss = 1.41 (67.3 examples/sec; 0.059 sec/batch; 2h:12m:00s remains)
INFO - root - 2019-11-06 18:21:37.542592: step 16710, total loss = 4.55, predict loss = 1.27 (61.4 examples/sec; 0.065 sec/batch; 2h:24m:49s remains)
INFO - root - 2019-11-06 18:21:38.304555: step 16720, total loss = 6.72, predict loss = 2.02 (59.3 examples/sec; 0.067 sec/batch; 2h:29m:44s remains)
INFO - root - 2019-11-06 18:21:39.048770: step 16730, total loss = 5.86, predict loss = 1.73 (64.0 examples/sec; 0.063 sec/batch; 2h:18m:53s remains)
INFO - root - 2019-11-06 18:21:39.779720: step 16740, total loss = 5.45, predict loss = 1.56 (57.9 examples/sec; 0.069 sec/batch; 2h:33m:20s remains)
INFO - root - 2019-11-06 18:21:40.398865: step 16750, total loss = 5.85, predict loss = 1.65 (96.6 examples/sec; 0.041 sec/batch; 1h:32m:00s remains)
INFO - root - 2019-11-06 18:21:40.862748: step 16760, total loss = 4.93, predict loss = 1.44 (94.6 examples/sec; 0.042 sec/batch; 1h:33m:54s remains)
INFO - root - 2019-11-06 18:21:41.344632: step 16770, total loss = 5.03, predict loss = 1.45 (100.7 examples/sec; 0.040 sec/batch; 1h:28m:14s remains)
INFO - root - 2019-11-06 18:21:42.656403: step 16780, total loss = 6.62, predict loss = 1.94 (60.6 examples/sec; 0.066 sec/batch; 2h:26m:28s remains)
INFO - root - 2019-11-06 18:21:43.442305: step 16790, total loss = 5.17, predict loss = 1.49 (56.4 examples/sec; 0.071 sec/batch; 2h:37m:26s remains)
INFO - root - 2019-11-06 18:21:44.148824: step 16800, total loss = 4.96, predict loss = 1.45 (65.9 examples/sec; 0.061 sec/batch; 2h:14m:47s remains)
INFO - root - 2019-11-06 18:21:44.886171: step 16810, total loss = 3.10, predict loss = 0.90 (57.0 examples/sec; 0.070 sec/batch; 2h:35m:40s remains)
INFO - root - 2019-11-06 18:21:45.587528: step 16820, total loss = 2.79, predict loss = 0.80 (70.3 examples/sec; 0.057 sec/batch; 2h:06m:20s remains)
INFO - root - 2019-11-06 18:21:46.076237: step 16830, total loss = 4.33, predict loss = 1.23 (92.8 examples/sec; 0.043 sec/batch; 1h:35m:39s remains)
INFO - root - 2019-11-06 18:21:46.546070: step 16840, total loss = 5.04, predict loss = 1.44 (90.5 examples/sec; 0.044 sec/batch; 1h:38m:05s remains)
INFO - root - 2019-11-06 18:21:47.755839: step 16850, total loss = 4.90, predict loss = 1.46 (69.9 examples/sec; 0.057 sec/batch; 2h:06m:59s remains)
INFO - root - 2019-11-06 18:21:48.523608: step 16860, total loss = 5.06, predict loss = 1.50 (52.6 examples/sec; 0.076 sec/batch; 2h:48m:44s remains)
INFO - root - 2019-11-06 18:21:49.286106: step 16870, total loss = 3.31, predict loss = 1.00 (55.0 examples/sec; 0.073 sec/batch; 2h:41m:30s remains)
INFO - root - 2019-11-06 18:21:50.048755: step 16880, total loss = 5.69, predict loss = 1.66 (57.8 examples/sec; 0.069 sec/batch; 2h:33m:36s remains)
INFO - root - 2019-11-06 18:21:50.795462: step 16890, total loss = 4.95, predict loss = 1.46 (67.1 examples/sec; 0.060 sec/batch; 2h:12m:11s remains)
INFO - root - 2019-11-06 18:21:51.373084: step 16900, total loss = 5.60, predict loss = 1.56 (99.3 examples/sec; 0.040 sec/batch; 1h:29m:22s remains)
INFO - root - 2019-11-06 18:21:51.820558: step 16910, total loss = 4.48, predict loss = 1.32 (89.8 examples/sec; 0.045 sec/batch; 1h:38m:45s remains)
INFO - root - 2019-11-06 18:21:52.309533: step 16920, total loss = 4.28, predict loss = 1.23 (94.4 examples/sec; 0.042 sec/batch; 1h:34m:00s remains)
INFO - root - 2019-11-06 18:21:53.631752: step 16930, total loss = 4.37, predict loss = 1.26 (51.1 examples/sec; 0.078 sec/batch; 2h:53m:38s remains)
INFO - root - 2019-11-06 18:21:54.336891: step 16940, total loss = 3.39, predict loss = 0.99 (63.2 examples/sec; 0.063 sec/batch; 2h:20m:26s remains)
INFO - root - 2019-11-06 18:21:55.037945: step 16950, total loss = 5.25, predict loss = 1.45 (57.6 examples/sec; 0.069 sec/batch; 2h:34m:02s remains)
INFO - root - 2019-11-06 18:21:55.774911: step 16960, total loss = 6.04, predict loss = 1.68 (56.0 examples/sec; 0.071 sec/batch; 2h:38m:25s remains)
INFO - root - 2019-11-06 18:21:56.447965: step 16970, total loss = 4.56, predict loss = 1.29 (76.6 examples/sec; 0.052 sec/batch; 1h:55m:50s remains)
INFO - root - 2019-11-06 18:21:56.922748: step 16980, total loss = 6.43, predict loss = 1.82 (97.5 examples/sec; 0.041 sec/batch; 1h:30m:59s remains)
INFO - root - 2019-11-06 18:21:57.394311: step 16990, total loss = 4.02, predict loss = 1.15 (89.2 examples/sec; 0.045 sec/batch; 1h:39m:23s remains)
INFO - root - 2019-11-06 18:21:58.571133: step 17000, total loss = 4.77, predict loss = 1.34 (64.0 examples/sec; 0.062 sec/batch; 2h:18m:30s remains)
INFO - root - 2019-11-06 18:21:59.341196: step 17010, total loss = 6.64, predict loss = 1.89 (62.6 examples/sec; 0.064 sec/batch; 2h:21m:34s remains)
INFO - root - 2019-11-06 18:22:00.167420: step 17020, total loss = 5.20, predict loss = 1.51 (48.8 examples/sec; 0.082 sec/batch; 3h:01m:50s remains)
INFO - root - 2019-11-06 18:22:00.917045: step 17030, total loss = 4.70, predict loss = 1.43 (59.0 examples/sec; 0.068 sec/batch; 2h:30m:11s remains)
INFO - root - 2019-11-06 18:22:01.691438: step 17040, total loss = 4.21, predict loss = 1.25 (62.1 examples/sec; 0.064 sec/batch; 2h:22m:48s remains)
INFO - root - 2019-11-06 18:22:02.286339: step 17050, total loss = 4.77, predict loss = 1.32 (96.0 examples/sec; 0.042 sec/batch; 1h:32m:22s remains)
INFO - root - 2019-11-06 18:22:02.749241: step 17060, total loss = 4.33, predict loss = 1.37 (97.0 examples/sec; 0.041 sec/batch; 1h:31m:20s remains)
INFO - root - 2019-11-06 18:22:03.204184: step 17070, total loss = 4.00, predict loss = 1.18 (112.7 examples/sec; 0.035 sec/batch; 1h:18m:37s remains)
INFO - root - 2019-11-06 18:22:04.562451: step 17080, total loss = 4.37, predict loss = 1.25 (57.6 examples/sec; 0.069 sec/batch; 2h:33m:47s remains)
INFO - root - 2019-11-06 18:22:05.327291: step 17090, total loss = 4.72, predict loss = 1.37 (53.9 examples/sec; 0.074 sec/batch; 2h:44m:20s remains)
INFO - root - 2019-11-06 18:22:06.200408: step 17100, total loss = 4.01, predict loss = 1.16 (49.0 examples/sec; 0.082 sec/batch; 3h:00m:48s remains)
INFO - root - 2019-11-06 18:22:06.920155: step 17110, total loss = 5.12, predict loss = 1.40 (69.2 examples/sec; 0.058 sec/batch; 2h:07m:55s remains)
INFO - root - 2019-11-06 18:22:07.658938: step 17120, total loss = 6.19, predict loss = 1.74 (68.4 examples/sec; 0.058 sec/batch; 2h:09m:29s remains)
INFO - root - 2019-11-06 18:22:08.161629: step 17130, total loss = 4.11, predict loss = 1.24 (100.1 examples/sec; 0.040 sec/batch; 1h:28m:29s remains)
INFO - root - 2019-11-06 18:22:08.629244: step 17140, total loss = 5.34, predict loss = 1.51 (94.7 examples/sec; 0.042 sec/batch; 1h:33m:32s remains)
INFO - root - 2019-11-06 18:22:09.818928: step 17150, total loss = 4.95, predict loss = 1.42 (61.8 examples/sec; 0.065 sec/batch; 2h:23m:21s remains)
INFO - root - 2019-11-06 18:22:10.543222: step 17160, total loss = 5.27, predict loss = 1.51 (61.1 examples/sec; 0.065 sec/batch; 2h:25m:00s remains)
INFO - root - 2019-11-06 18:22:11.293627: step 17170, total loss = 6.44, predict loss = 1.88 (59.8 examples/sec; 0.067 sec/batch; 2h:28m:08s remains)
INFO - root - 2019-11-06 18:22:12.080594: step 17180, total loss = 5.71, predict loss = 1.66 (50.6 examples/sec; 0.079 sec/batch; 2h:54m:57s remains)
INFO - root - 2019-11-06 18:22:12.817069: step 17190, total loss = 4.00, predict loss = 1.20 (68.3 examples/sec; 0.059 sec/batch; 2h:09m:41s remains)
INFO - root - 2019-11-06 18:22:13.365385: step 17200, total loss = 5.29, predict loss = 1.51 (87.3 examples/sec; 0.046 sec/batch; 1h:41m:21s remains)
INFO - root - 2019-11-06 18:22:13.840118: step 17210, total loss = 4.36, predict loss = 1.29 (88.8 examples/sec; 0.045 sec/batch; 1h:39m:44s remains)
INFO - root - 2019-11-06 18:22:14.965448: step 17220, total loss = 6.17, predict loss = 1.86 (5.5 examples/sec; 0.721 sec/batch; 26h:36m:30s remains)
INFO - root - 2019-11-06 18:22:15.664771: step 17230, total loss = 4.19, predict loss = 1.18 (65.7 examples/sec; 0.061 sec/batch; 2h:14m:42s remains)
INFO - root - 2019-11-06 18:22:16.421097: step 17240, total loss = 6.42, predict loss = 1.83 (60.3 examples/sec; 0.066 sec/batch; 2h:26m:52s remains)
INFO - root - 2019-11-06 18:22:17.239071: step 17250, total loss = 4.34, predict loss = 1.28 (57.9 examples/sec; 0.069 sec/batch; 2h:32m:50s remains)
INFO - root - 2019-11-06 18:22:18.055570: step 17260, total loss = 5.87, predict loss = 1.67 (53.8 examples/sec; 0.074 sec/batch; 2h:44m:24s remains)
INFO - root - 2019-11-06 18:22:18.758224: step 17270, total loss = 4.88, predict loss = 1.40 (88.0 examples/sec; 0.045 sec/batch; 1h:40m:33s remains)
INFO - root - 2019-11-06 18:22:19.215750: step 17280, total loss = 5.89, predict loss = 1.73 (90.0 examples/sec; 0.044 sec/batch; 1h:38m:18s remains)
INFO - root - 2019-11-06 18:22:19.725773: step 17290, total loss = 5.58, predict loss = 1.54 (96.9 examples/sec; 0.041 sec/batch; 1h:31m:18s remains)
INFO - root - 2019-11-06 18:22:20.965545: step 17300, total loss = 6.00, predict loss = 1.70 (64.3 examples/sec; 0.062 sec/batch; 2h:17m:39s remains)
INFO - root - 2019-11-06 18:22:21.763722: step 17310, total loss = 4.10, predict loss = 1.23 (57.3 examples/sec; 0.070 sec/batch; 2h:34m:17s remains)
INFO - root - 2019-11-06 18:22:22.535168: step 17320, total loss = 4.14, predict loss = 1.22 (61.3 examples/sec; 0.065 sec/batch; 2h:24m:22s remains)
INFO - root - 2019-11-06 18:22:23.339029: step 17330, total loss = 4.91, predict loss = 1.44 (59.4 examples/sec; 0.067 sec/batch; 2h:28m:48s remains)
INFO - root - 2019-11-06 18:22:24.121190: step 17340, total loss = 5.29, predict loss = 1.52 (56.2 examples/sec; 0.071 sec/batch; 2h:37m:23s remains)
INFO - root - 2019-11-06 18:22:24.690552: step 17350, total loss = 5.49, predict loss = 1.57 (95.1 examples/sec; 0.042 sec/batch; 1h:32m:57s remains)
INFO - root - 2019-11-06 18:22:25.149792: step 17360, total loss = 5.15, predict loss = 1.44 (93.9 examples/sec; 0.043 sec/batch; 1h:34m:08s remains)
INFO - root - 2019-11-06 18:22:26.289659: step 17370, total loss = 4.37, predict loss = 1.20 (65.9 examples/sec; 0.061 sec/batch; 2h:14m:11s remains)
INFO - root - 2019-11-06 18:22:26.949843: step 17380, total loss = 6.24, predict loss = 1.76 (63.0 examples/sec; 0.063 sec/batch; 2h:20m:18s remains)
INFO - root - 2019-11-06 18:22:27.727127: step 17390, total loss = 3.90, predict loss = 1.11 (58.6 examples/sec; 0.068 sec/batch; 2h:30m:50s remains)
INFO - root - 2019-11-06 18:22:28.435223: step 17400, total loss = 5.84, predict loss = 1.68 (54.8 examples/sec; 0.073 sec/batch; 2h:41m:12s remains)
INFO - root - 2019-11-06 18:22:29.238551: step 17410, total loss = 4.83, predict loss = 1.39 (55.8 examples/sec; 0.072 sec/batch; 2h:38m:22s remains)
INFO - root - 2019-11-06 18:22:29.856314: step 17420, total loss = 6.31, predict loss = 1.77 (97.1 examples/sec; 0.041 sec/batch; 1h:31m:03s remains)
INFO - root - 2019-11-06 18:22:30.307999: step 17430, total loss = 5.71, predict loss = 1.54 (100.2 examples/sec; 0.040 sec/batch; 1h:28m:13s remains)
INFO - root - 2019-11-06 18:22:30.759166: step 17440, total loss = 4.26, predict loss = 1.25 (90.2 examples/sec; 0.044 sec/batch; 1h:38m:01s remains)
INFO - root - 2019-11-06 18:22:32.075137: step 17450, total loss = 4.80, predict loss = 1.36 (59.3 examples/sec; 0.067 sec/batch; 2h:29m:00s remains)
INFO - root - 2019-11-06 18:22:32.822457: step 17460, total loss = 2.20, predict loss = 0.67 (61.3 examples/sec; 0.065 sec/batch; 2h:24m:04s remains)
INFO - root - 2019-11-06 18:22:33.584450: step 17470, total loss = 5.85, predict loss = 1.61 (54.4 examples/sec; 0.074 sec/batch; 2h:42m:23s remains)
INFO - root - 2019-11-06 18:22:34.385323: step 17480, total loss = 4.40, predict loss = 1.38 (58.2 examples/sec; 0.069 sec/batch; 2h:31m:46s remains)
INFO - root - 2019-11-06 18:22:35.156828: step 17490, total loss = 4.58, predict loss = 1.34 (68.0 examples/sec; 0.059 sec/batch; 2h:09m:58s remains)
INFO - root - 2019-11-06 18:22:35.689502: step 17500, total loss = 3.80, predict loss = 1.03 (92.9 examples/sec; 0.043 sec/batch; 1h:35m:06s remains)
INFO - root - 2019-11-06 18:22:36.118647: step 17510, total loss = 4.18, predict loss = 1.19 (106.2 examples/sec; 0.038 sec/batch; 1h:23m:11s remains)
INFO - root - 2019-11-06 18:22:37.269166: step 17520, total loss = 5.48, predict loss = 1.58 (69.0 examples/sec; 0.058 sec/batch; 2h:07m:55s remains)
INFO - root - 2019-11-06 18:22:38.062503: step 17530, total loss = 4.05, predict loss = 1.19 (47.1 examples/sec; 0.085 sec/batch; 3h:07m:23s remains)
INFO - root - 2019-11-06 18:22:38.819571: step 17540, total loss = 5.02, predict loss = 1.47 (60.1 examples/sec; 0.067 sec/batch; 2h:26m:55s remains)
INFO - root - 2019-11-06 18:22:39.641993: step 17550, total loss = 3.96, predict loss = 1.15 (51.2 examples/sec; 0.078 sec/batch; 2h:52m:35s remains)
INFO - root - 2019-11-06 18:22:40.463114: step 17560, total loss = 4.92, predict loss = 1.41 (61.9 examples/sec; 0.065 sec/batch; 2h:22m:35s remains)
INFO - root - 2019-11-06 18:22:41.053510: step 17570, total loss = 4.27, predict loss = 1.24 (101.9 examples/sec; 0.039 sec/batch; 1h:26m:37s remains)
INFO - root - 2019-11-06 18:22:41.520962: step 17580, total loss = 5.29, predict loss = 1.44 (92.9 examples/sec; 0.043 sec/batch; 1h:35m:03s remains)
INFO - root - 2019-11-06 18:22:41.971372: step 17590, total loss = 3.51, predict loss = 1.11 (97.1 examples/sec; 0.041 sec/batch; 1h:30m:53s remains)
INFO - root - 2019-11-06 18:22:43.274131: step 17600, total loss = 4.56, predict loss = 1.34 (53.0 examples/sec; 0.075 sec/batch; 2h:46m:29s remains)
INFO - root - 2019-11-06 18:22:44.014859: step 17610, total loss = 3.66, predict loss = 0.95 (58.8 examples/sec; 0.068 sec/batch; 2h:30m:09s remains)
INFO - root - 2019-11-06 18:22:44.801586: step 17620, total loss = 3.96, predict loss = 1.15 (64.3 examples/sec; 0.062 sec/batch; 2h:17m:16s remains)
INFO - root - 2019-11-06 18:22:45.580700: step 17630, total loss = 3.90, predict loss = 1.21 (57.6 examples/sec; 0.069 sec/batch; 2h:33m:07s remains)
INFO - root - 2019-11-06 18:22:46.284738: step 17640, total loss = 6.05, predict loss = 1.77 (62.1 examples/sec; 0.064 sec/batch; 2h:22m:09s remains)
INFO - root - 2019-11-06 18:22:46.814999: step 17650, total loss = 4.97, predict loss = 1.44 (94.9 examples/sec; 0.042 sec/batch; 1h:32m:55s remains)
INFO - root - 2019-11-06 18:22:47.275213: step 17660, total loss = 6.08, predict loss = 1.75 (98.6 examples/sec; 0.041 sec/batch; 1h:29m:28s remains)
INFO - root - 2019-11-06 18:22:48.458124: step 17670, total loss = 4.73, predict loss = 1.39 (66.4 examples/sec; 0.060 sec/batch; 2h:12m:57s remains)
INFO - root - 2019-11-06 18:22:49.147647: step 17680, total loss = 5.18, predict loss = 1.44 (64.0 examples/sec; 0.063 sec/batch; 2h:17m:55s remains)
INFO - root - 2019-11-06 18:22:49.849313: step 17690, total loss = 5.13, predict loss = 1.48 (67.1 examples/sec; 0.060 sec/batch; 2h:11m:23s remains)
INFO - root - 2019-11-06 18:22:50.552908: step 17700, total loss = 5.35, predict loss = 1.53 (54.0 examples/sec; 0.074 sec/batch; 2h:43m:26s remains)
INFO - root - 2019-11-06 18:22:51.291142: step 17710, total loss = 5.14, predict loss = 1.45 (58.4 examples/sec; 0.069 sec/batch; 2h:31m:06s remains)
INFO - root - 2019-11-06 18:22:51.847223: step 17720, total loss = 5.64, predict loss = 1.58 (104.7 examples/sec; 0.038 sec/batch; 1h:24m:13s remains)
INFO - root - 2019-11-06 18:22:52.354306: step 17730, total loss = 4.68, predict loss = 1.31 (96.9 examples/sec; 0.041 sec/batch; 1h:31m:00s remains)
INFO - root - 2019-11-06 18:22:52.802196: step 17740, total loss = 4.02, predict loss = 1.16 (105.8 examples/sec; 0.038 sec/batch; 1h:23m:20s remains)
INFO - root - 2019-11-06 18:22:54.105407: step 17750, total loss = 5.22, predict loss = 1.43 (69.0 examples/sec; 0.058 sec/batch; 2h:07m:43s remains)
INFO - root - 2019-11-06 18:22:54.859245: step 17760, total loss = 5.26, predict loss = 1.48 (54.0 examples/sec; 0.074 sec/batch; 2h:43m:20s remains)
INFO - root - 2019-11-06 18:22:55.580862: step 17770, total loss = 5.72, predict loss = 1.61 (64.4 examples/sec; 0.062 sec/batch; 2h:16m:58s remains)
INFO - root - 2019-11-06 18:22:56.356101: step 17780, total loss = 3.79, predict loss = 1.17 (51.7 examples/sec; 0.077 sec/batch; 2h:50m:39s remains)
INFO - root - 2019-11-06 18:22:57.064149: step 17790, total loss = 3.84, predict loss = 1.13 (75.1 examples/sec; 0.053 sec/batch; 1h:57m:19s remains)
INFO - root - 2019-11-06 18:22:57.528885: step 17800, total loss = 6.56, predict loss = 1.83 (87.2 examples/sec; 0.046 sec/batch; 1h:41m:05s remains)
INFO - root - 2019-11-06 18:22:58.012054: step 17810, total loss = 5.93, predict loss = 1.70 (96.6 examples/sec; 0.041 sec/batch; 1h:31m:13s remains)
INFO - root - 2019-11-06 18:22:59.213002: step 17820, total loss = 5.33, predict loss = 1.52 (71.0 examples/sec; 0.056 sec/batch; 2h:04m:08s remains)
INFO - root - 2019-11-06 18:22:59.936629: step 17830, total loss = 5.68, predict loss = 1.54 (60.2 examples/sec; 0.066 sec/batch; 2h:26m:15s remains)
INFO - root - 2019-11-06 18:23:00.687956: step 17840, total loss = 4.33, predict loss = 1.29 (54.9 examples/sec; 0.073 sec/batch; 2h:40m:30s remains)
INFO - root - 2019-11-06 18:23:01.440055: step 17850, total loss = 4.72, predict loss = 1.29 (59.2 examples/sec; 0.068 sec/batch; 2h:28m:53s remains)
INFO - root - 2019-11-06 18:23:02.180550: step 17860, total loss = 3.74, predict loss = 1.04 (59.5 examples/sec; 0.067 sec/batch; 2h:28m:01s remains)
INFO - root - 2019-11-06 18:23:02.754329: step 17870, total loss = 5.64, predict loss = 1.60 (93.2 examples/sec; 0.043 sec/batch; 1h:34m:29s remains)
INFO - root - 2019-11-06 18:23:03.230971: step 17880, total loss = 5.45, predict loss = 1.55 (91.0 examples/sec; 0.044 sec/batch; 1h:36m:44s remains)
INFO - root - 2019-11-06 18:23:03.712279: step 17890, total loss = 3.77, predict loss = 1.20 (120.1 examples/sec; 0.033 sec/batch; 1h:13m:21s remains)
INFO - root - 2019-11-06 18:23:05.059466: step 17900, total loss = 5.45, predict loss = 1.48 (51.8 examples/sec; 0.077 sec/batch; 2h:50m:04s remains)
INFO - root - 2019-11-06 18:23:05.757742: step 17910, total loss = 6.26, predict loss = 1.78 (53.7 examples/sec; 0.074 sec/batch; 2h:43m:59s remains)
INFO - root - 2019-11-06 18:23:06.537619: step 17920, total loss = 5.53, predict loss = 1.60 (57.3 examples/sec; 0.070 sec/batch; 2h:33m:44s remains)
INFO - root - 2019-11-06 18:23:07.281083: step 17930, total loss = 5.68, predict loss = 1.62 (63.0 examples/sec; 0.063 sec/batch; 2h:19m:45s remains)
INFO - root - 2019-11-06 18:23:07.964397: step 17940, total loss = 5.77, predict loss = 1.62 (83.0 examples/sec; 0.048 sec/batch; 1h:46m:04s remains)
INFO - root - 2019-11-06 18:23:08.424424: step 17950, total loss = 5.24, predict loss = 1.50 (94.8 examples/sec; 0.042 sec/batch; 1h:32m:52s remains)
INFO - root - 2019-11-06 18:23:08.881851: step 17960, total loss = 4.04, predict loss = 1.23 (96.0 examples/sec; 0.042 sec/batch; 1h:31m:40s remains)
INFO - root - 2019-11-06 18:23:10.158718: step 17970, total loss = 5.77, predict loss = 1.62 (68.6 examples/sec; 0.058 sec/batch; 2h:08m:21s remains)
INFO - root - 2019-11-06 18:23:10.872441: step 17980, total loss = 3.84, predict loss = 1.06 (59.6 examples/sec; 0.067 sec/batch; 2h:27m:41s remains)
INFO - root - 2019-11-06 18:23:11.637736: step 17990, total loss = 5.08, predict loss = 1.59 (57.5 examples/sec; 0.070 sec/batch; 2h:33m:00s remains)
INFO - root - 2019-11-06 18:23:12.359207: step 18000, total loss = 6.30, predict loss = 1.79 (55.2 examples/sec; 0.073 sec/batch; 2h:39m:31s remains)
INFO - root - 2019-11-06 18:23:13.082475: step 18010, total loss = 5.16, predict loss = 1.46 (63.1 examples/sec; 0.063 sec/batch; 2h:19m:27s remains)
INFO - root - 2019-11-06 18:23:13.655144: step 18020, total loss = 6.21, predict loss = 1.78 (98.0 examples/sec; 0.041 sec/batch; 1h:29m:48s remains)
INFO - root - 2019-11-06 18:23:14.091949: step 18030, total loss = 4.31, predict loss = 1.18 (94.3 examples/sec; 0.042 sec/batch; 1h:33m:18s remains)
INFO - root - 2019-11-06 18:23:15.251305: step 18040, total loss = 4.03, predict loss = 1.22 (5.4 examples/sec; 0.742 sec/batch; 27h:12m:25s remains)
INFO - root - 2019-11-06 18:23:15.970382: step 18050, total loss = 3.58, predict loss = 1.10 (57.2 examples/sec; 0.070 sec/batch; 2h:33m:46s remains)
INFO - root - 2019-11-06 18:23:16.715928: step 18060, total loss = 5.23, predict loss = 1.63 (59.8 examples/sec; 0.067 sec/batch; 2h:27m:01s remains)
INFO - root - 2019-11-06 18:23:17.455600: step 18070, total loss = 6.49, predict loss = 1.89 (60.7 examples/sec; 0.066 sec/batch; 2h:24m:59s remains)
INFO - root - 2019-11-06 18:23:18.280501: step 18080, total loss = 4.86, predict loss = 1.39 (46.8 examples/sec; 0.085 sec/batch; 3h:07m:47s remains)
INFO - root - 2019-11-06 18:23:19.000867: step 18090, total loss = 6.42, predict loss = 1.81 (90.2 examples/sec; 0.044 sec/batch; 1h:37m:27s remains)
INFO - root - 2019-11-06 18:23:19.464776: step 18100, total loss = 3.63, predict loss = 1.06 (90.3 examples/sec; 0.044 sec/batch; 1h:37m:23s remains)
INFO - root - 2019-11-06 18:23:19.910176: step 18110, total loss = 6.33, predict loss = 1.80 (91.5 examples/sec; 0.044 sec/batch; 1h:36m:03s remains)
INFO - root - 2019-11-06 18:23:21.149903: step 18120, total loss = 5.05, predict loss = 1.45 (65.6 examples/sec; 0.061 sec/batch; 2h:13m:57s remains)
INFO - root - 2019-11-06 18:23:21.916161: step 18130, total loss = 5.86, predict loss = 1.67 (41.7 examples/sec; 0.096 sec/batch; 3h:30m:39s remains)
INFO - root - 2019-11-06 18:23:22.720372: step 18140, total loss = 6.26, predict loss = 1.74 (51.3 examples/sec; 0.078 sec/batch; 2h:51m:17s remains)
INFO - root - 2019-11-06 18:23:23.505031: step 18150, total loss = 4.49, predict loss = 1.27 (58.3 examples/sec; 0.069 sec/batch; 2h:30m:38s remains)
INFO - root - 2019-11-06 18:23:24.208079: step 18160, total loss = 4.66, predict loss = 1.29 (63.8 examples/sec; 0.063 sec/batch; 2h:17m:42s remains)
INFO - root - 2019-11-06 18:23:24.767123: step 18170, total loss = 4.30, predict loss = 1.20 (97.7 examples/sec; 0.041 sec/batch; 1h:29m:59s remains)
INFO - root - 2019-11-06 18:23:25.218884: step 18180, total loss = 5.83, predict loss = 1.60 (89.2 examples/sec; 0.045 sec/batch; 1h:38m:28s remains)
INFO - root - 2019-11-06 18:23:26.409212: step 18190, total loss = 6.07, predict loss = 1.82 (60.0 examples/sec; 0.067 sec/batch; 2h:26m:26s remains)
INFO - root - 2019-11-06 18:23:27.114657: step 18200, total loss = 4.74, predict loss = 1.32 (60.5 examples/sec; 0.066 sec/batch; 2h:25m:11s remains)
INFO - root - 2019-11-06 18:23:27.901236: step 18210, total loss = 5.57, predict loss = 1.58 (57.2 examples/sec; 0.070 sec/batch; 2h:33m:36s remains)
INFO - root - 2019-11-06 18:23:28.596101: step 18220, total loss = 4.00, predict loss = 1.14 (63.8 examples/sec; 0.063 sec/batch; 2h:17m:37s remains)
INFO - root - 2019-11-06 18:23:29.359460: step 18230, total loss = 5.88, predict loss = 1.81 (57.4 examples/sec; 0.070 sec/batch; 2h:33m:09s remains)
INFO - root - 2019-11-06 18:23:30.032346: step 18240, total loss = 4.71, predict loss = 1.41 (91.1 examples/sec; 0.044 sec/batch; 1h:36m:25s remains)
INFO - root - 2019-11-06 18:23:30.513135: step 18250, total loss = 3.62, predict loss = 1.09 (107.1 examples/sec; 0.037 sec/batch; 1h:22m:01s remains)
INFO - root - 2019-11-06 18:23:30.962836: step 18260, total loss = 5.17, predict loss = 1.45 (94.3 examples/sec; 0.042 sec/batch; 1h:33m:10s remains)
INFO - root - 2019-11-06 18:23:32.229274: step 18270, total loss = 5.33, predict loss = 1.50 (66.3 examples/sec; 0.060 sec/batch; 2h:12m:32s remains)
INFO - root - 2019-11-06 18:23:32.915879: step 18280, total loss = 5.02, predict loss = 1.36 (68.5 examples/sec; 0.058 sec/batch; 2h:08m:17s remains)
INFO - root - 2019-11-06 18:23:33.607198: step 18290, total loss = 5.56, predict loss = 1.63 (71.2 examples/sec; 0.056 sec/batch; 2h:03m:21s remains)
INFO - root - 2019-11-06 18:23:34.363637: step 18300, total loss = 3.53, predict loss = 1.09 (50.5 examples/sec; 0.079 sec/batch; 2h:53m:59s remains)
INFO - root - 2019-11-06 18:23:35.049020: step 18310, total loss = 3.50, predict loss = 1.04 (72.9 examples/sec; 0.055 sec/batch; 2h:00m:25s remains)
INFO - root - 2019-11-06 18:23:35.566491: step 18320, total loss = 5.81, predict loss = 1.69 (96.7 examples/sec; 0.041 sec/batch; 1h:30m:45s remains)
INFO - root - 2019-11-06 18:23:36.050267: step 18330, total loss = 6.15, predict loss = 1.78 (93.3 examples/sec; 0.043 sec/batch; 1h:34m:04s remains)
INFO - root - 2019-11-06 18:23:37.247908: step 18340, total loss = 3.83, predict loss = 1.13 (64.6 examples/sec; 0.062 sec/batch; 2h:15m:57s remains)
INFO - root - 2019-11-06 18:23:37.993132: step 18350, total loss = 4.88, predict loss = 1.42 (50.6 examples/sec; 0.079 sec/batch; 2h:53m:19s remains)
INFO - root - 2019-11-06 18:23:38.714111: step 18360, total loss = 6.25, predict loss = 1.83 (63.2 examples/sec; 0.063 sec/batch; 2h:18m:47s remains)
INFO - root - 2019-11-06 18:23:39.479985: step 18370, total loss = 5.24, predict loss = 1.47 (56.6 examples/sec; 0.071 sec/batch; 2h:34m:57s remains)
INFO - root - 2019-11-06 18:23:40.256887: step 18380, total loss = 6.69, predict loss = 1.94 (58.3 examples/sec; 0.069 sec/batch; 2h:30m:26s remains)
INFO - root - 2019-11-06 18:23:40.911605: step 18390, total loss = 5.83, predict loss = 1.64 (96.8 examples/sec; 0.041 sec/batch; 1h:30m:36s remains)
INFO - root - 2019-11-06 18:23:41.372030: step 18400, total loss = 6.73, predict loss = 1.97 (89.6 examples/sec; 0.045 sec/batch; 1h:37m:55s remains)
INFO - root - 2019-11-06 18:23:41.851638: step 18410, total loss = 6.10, predict loss = 1.67 (96.7 examples/sec; 0.041 sec/batch; 1h:30m:41s remains)
INFO - root - 2019-11-06 18:23:43.244839: step 18420, total loss = 4.49, predict loss = 1.26 (48.7 examples/sec; 0.082 sec/batch; 3h:00m:02s remains)
INFO - root - 2019-11-06 18:23:43.983341: step 18430, total loss = 6.02, predict loss = 1.74 (65.6 examples/sec; 0.061 sec/batch; 2h:13m:46s remains)
INFO - root - 2019-11-06 18:23:44.714549: step 18440, total loss = 4.55, predict loss = 1.33 (63.4 examples/sec; 0.063 sec/batch; 2h:18m:18s remains)
INFO - root - 2019-11-06 18:23:45.500190: step 18450, total loss = 5.77, predict loss = 1.64 (59.2 examples/sec; 0.068 sec/batch; 2h:28m:11s remains)
INFO - root - 2019-11-06 18:23:46.218294: step 18460, total loss = 3.54, predict loss = 0.98 (63.7 examples/sec; 0.063 sec/batch; 2h:17m:33s remains)
INFO - root - 2019-11-06 18:23:46.710406: step 18470, total loss = 5.39, predict loss = 1.61 (100.6 examples/sec; 0.040 sec/batch; 1h:27m:12s remains)
INFO - root - 2019-11-06 18:23:47.155781: step 18480, total loss = 6.30, predict loss = 1.81 (91.3 examples/sec; 0.044 sec/batch; 1h:36m:01s remains)
INFO - root - 2019-11-06 18:23:48.369982: step 18490, total loss = 4.21, predict loss = 1.23 (70.5 examples/sec; 0.057 sec/batch; 2h:04m:18s remains)
INFO - root - 2019-11-06 18:23:49.115255: step 18500, total loss = 4.41, predict loss = 1.29 (62.7 examples/sec; 0.064 sec/batch; 2h:19m:50s remains)
INFO - root - 2019-11-06 18:23:49.862979: step 18510, total loss = 4.97, predict loss = 1.46 (61.4 examples/sec; 0.065 sec/batch; 2h:22m:49s remains)
INFO - root - 2019-11-06 18:23:50.630583: step 18520, total loss = 5.80, predict loss = 1.65 (58.0 examples/sec; 0.069 sec/batch; 2h:31m:04s remains)
INFO - root - 2019-11-06 18:23:51.379991: step 18530, total loss = 5.26, predict loss = 1.55 (56.6 examples/sec; 0.071 sec/batch; 2h:34m:48s remains)
INFO - root - 2019-11-06 18:23:51.970360: step 18540, total loss = 6.09, predict loss = 1.70 (84.8 examples/sec; 0.047 sec/batch; 1h:43m:18s remains)
INFO - root - 2019-11-06 18:23:52.422410: step 18550, total loss = 6.06, predict loss = 1.64 (102.6 examples/sec; 0.039 sec/batch; 1h:25m:24s remains)
INFO - root - 2019-11-06 18:23:52.860067: step 18560, total loss = 6.09, predict loss = 1.77 (98.8 examples/sec; 0.040 sec/batch; 1h:28m:41s remains)
INFO - root - 2019-11-06 18:23:54.238139: step 18570, total loss = 4.44, predict loss = 1.35 (56.5 examples/sec; 0.071 sec/batch; 2h:35m:11s remains)
INFO - root - 2019-11-06 18:23:54.990042: step 18580, total loss = 4.68, predict loss = 1.37 (59.9 examples/sec; 0.067 sec/batch; 2h:26m:08s remains)
INFO - root - 2019-11-06 18:23:55.789083: step 18590, total loss = 4.50, predict loss = 1.30 (57.2 examples/sec; 0.070 sec/batch; 2h:33m:04s remains)
INFO - root - 2019-11-06 18:23:56.576350: step 18600, total loss = 4.62, predict loss = 1.29 (58.9 examples/sec; 0.068 sec/batch; 2h:28m:42s remains)
INFO - root - 2019-11-06 18:23:57.263513: step 18610, total loss = 5.23, predict loss = 1.55 (74.2 examples/sec; 0.054 sec/batch; 1h:58m:06s remains)
INFO - root - 2019-11-06 18:23:57.725468: step 18620, total loss = 4.44, predict loss = 1.33 (95.2 examples/sec; 0.042 sec/batch; 1h:32m:00s remains)
INFO - root - 2019-11-06 18:23:58.194474: step 18630, total loss = 6.78, predict loss = 1.90 (95.9 examples/sec; 0.042 sec/batch; 1h:31m:17s remains)
INFO - root - 2019-11-06 18:23:59.350779: step 18640, total loss = 5.28, predict loss = 1.50 (68.0 examples/sec; 0.059 sec/batch; 2h:08m:47s remains)
INFO - root - 2019-11-06 18:24:00.107090: step 18650, total loss = 4.29, predict loss = 1.27 (48.7 examples/sec; 0.082 sec/batch; 2h:59m:50s remains)
INFO - root - 2019-11-06 18:24:00.915516: step 18660, total loss = 4.33, predict loss = 1.29 (56.5 examples/sec; 0.071 sec/batch; 2h:34m:52s remains)
INFO - root - 2019-11-06 18:24:01.706259: step 18670, total loss = 4.94, predict loss = 1.49 (55.6 examples/sec; 0.072 sec/batch; 2h:37m:35s remains)
INFO - root - 2019-11-06 18:24:02.469571: step 18680, total loss = 4.80, predict loss = 1.33 (65.6 examples/sec; 0.061 sec/batch; 2h:13m:27s remains)
INFO - root - 2019-11-06 18:24:03.044902: step 18690, total loss = 6.70, predict loss = 1.94 (100.4 examples/sec; 0.040 sec/batch; 1h:27m:11s remains)
INFO - root - 2019-11-06 18:24:03.499008: step 18700, total loss = 5.65, predict loss = 1.63 (91.4 examples/sec; 0.044 sec/batch; 1h:35m:45s remains)
INFO - root - 2019-11-06 18:24:03.940770: step 18710, total loss = 5.20, predict loss = 1.48 (146.4 examples/sec; 0.027 sec/batch; 0h:59m:47s remains)
INFO - root - 2019-11-06 18:24:05.290564: step 18720, total loss = 5.64, predict loss = 1.64 (56.9 examples/sec; 0.070 sec/batch; 2h:33m:41s remains)
INFO - root - 2019-11-06 18:24:06.060015: step 18730, total loss = 5.81, predict loss = 1.62 (56.9 examples/sec; 0.070 sec/batch; 2h:33m:54s remains)
INFO - root - 2019-11-06 18:24:06.835182: step 18740, total loss = 4.39, predict loss = 1.29 (51.0 examples/sec; 0.078 sec/batch; 2h:51m:40s remains)
INFO - root - 2019-11-06 18:24:07.595487: step 18750, total loss = 5.25, predict loss = 1.51 (61.0 examples/sec; 0.066 sec/batch; 2h:23m:21s remains)
INFO - root - 2019-11-06 18:24:08.261692: step 18760, total loss = 3.99, predict loss = 1.13 (78.2 examples/sec; 0.051 sec/batch; 1h:51m:49s remains)
INFO - root - 2019-11-06 18:24:08.764840: step 18770, total loss = 4.54, predict loss = 1.34 (97.0 examples/sec; 0.041 sec/batch; 1h:30m:12s remains)
INFO - root - 2019-11-06 18:24:09.220924: step 18780, total loss = 4.77, predict loss = 1.37 (96.7 examples/sec; 0.041 sec/batch; 1h:30m:26s remains)
INFO - root - 2019-11-06 18:24:10.416708: step 18790, total loss = 5.41, predict loss = 1.56 (57.2 examples/sec; 0.070 sec/batch; 2h:32m:51s remains)
INFO - root - 2019-11-06 18:24:11.113044: step 18800, total loss = 4.56, predict loss = 1.34 (65.4 examples/sec; 0.061 sec/batch; 2h:13m:47s remains)
INFO - root - 2019-11-06 18:24:11.840293: step 18810, total loss = 4.81, predict loss = 1.43 (63.3 examples/sec; 0.063 sec/batch; 2h:18m:16s remains)
INFO - root - 2019-11-06 18:24:12.583384: step 18820, total loss = 5.53, predict loss = 1.58 (59.0 examples/sec; 0.068 sec/batch; 2h:28m:13s remains)
INFO - root - 2019-11-06 18:24:13.327152: step 18830, total loss = 4.91, predict loss = 1.40 (65.9 examples/sec; 0.061 sec/batch; 2h:12m:39s remains)
INFO - root - 2019-11-06 18:24:13.878893: step 18840, total loss = 4.75, predict loss = 1.34 (95.2 examples/sec; 0.042 sec/batch; 1h:31m:52s remains)
INFO - root - 2019-11-06 18:24:14.371180: step 18850, total loss = 5.41, predict loss = 1.56 (91.6 examples/sec; 0.044 sec/batch; 1h:35m:28s remains)
INFO - root - 2019-11-06 18:24:15.478692: step 18860, total loss = 6.12, predict loss = 1.75 (5.7 examples/sec; 0.696 sec/batch; 25h:20m:41s remains)
INFO - root - 2019-11-06 18:24:16.199131: step 18870, total loss = 5.12, predict loss = 1.52 (64.6 examples/sec; 0.062 sec/batch; 2h:15m:18s remains)
INFO - root - 2019-11-06 18:24:16.944088: step 18880, total loss = 6.94, predict loss = 2.03 (56.7 examples/sec; 0.071 sec/batch; 2h:34m:16s remains)
INFO - root - 2019-11-06 18:24:17.711258: step 18890, total loss = 4.29, predict loss = 1.24 (56.3 examples/sec; 0.071 sec/batch; 2h:35m:07s remains)
INFO - root - 2019-11-06 18:24:18.523861: step 18900, total loss = 3.94, predict loss = 1.13 (48.7 examples/sec; 0.082 sec/batch; 2h:59m:25s remains)
INFO - root - 2019-11-06 18:24:19.301921: step 18910, total loss = 6.07, predict loss = 1.79 (71.4 examples/sec; 0.056 sec/batch; 2h:02m:28s remains)
INFO - root - 2019-11-06 18:24:19.761757: step 18920, total loss = 4.32, predict loss = 1.27 (92.9 examples/sec; 0.043 sec/batch; 1h:34m:05s remains)
INFO - root - 2019-11-06 18:24:20.251424: step 18930, total loss = 4.09, predict loss = 1.19 (97.3 examples/sec; 0.041 sec/batch; 1h:29m:46s remains)
INFO - root - 2019-11-06 18:24:21.431387: step 18940, total loss = 4.64, predict loss = 1.37 (74.3 examples/sec; 0.054 sec/batch; 1h:57m:33s remains)
INFO - root - 2019-11-06 18:24:22.284992: step 18950, total loss = 4.88, predict loss = 1.46 (59.5 examples/sec; 0.067 sec/batch; 2h:26m:48s remains)
INFO - root - 2019-11-06 18:24:22.997466: step 18960, total loss = 5.81, predict loss = 1.70 (53.0 examples/sec; 0.075 sec/batch; 2h:44m:44s remains)
INFO - root - 2019-11-06 18:24:23.795058: step 18970, total loss = 3.50, predict loss = 1.08 (52.2 examples/sec; 0.077 sec/batch; 2h:47m:27s remains)
INFO - root - 2019-11-06 18:24:24.537414: step 18980, total loss = 4.21, predict loss = 1.23 (73.9 examples/sec; 0.054 sec/batch; 1h:58m:09s remains)
INFO - root - 2019-11-06 18:24:25.042821: step 18990, total loss = 5.90, predict loss = 1.72 (98.3 examples/sec; 0.041 sec/batch; 1h:28m:49s remains)
INFO - root - 2019-11-06 18:24:25.509137: step 19000, total loss = 4.18, predict loss = 1.13 (99.2 examples/sec; 0.040 sec/batch; 1h:28m:00s remains)
INFO - root - 2019-11-06 18:24:26.692542: step 19010, total loss = 5.75, predict loss = 1.70 (69.8 examples/sec; 0.057 sec/batch; 2h:05m:06s remains)
INFO - root - 2019-11-06 18:24:27.367359: step 19020, total loss = 2.99, predict loss = 0.90 (56.2 examples/sec; 0.071 sec/batch; 2h:35m:25s remains)
INFO - root - 2019-11-06 18:24:28.155811: step 19030, total loss = 4.13, predict loss = 1.23 (56.8 examples/sec; 0.070 sec/batch; 2h:33m:48s remains)
INFO - root - 2019-11-06 18:24:28.864187: step 19040, total loss = 4.66, predict loss = 1.33 (65.5 examples/sec; 0.061 sec/batch; 2h:13m:19s remains)
INFO - root - 2019-11-06 18:24:29.589683: step 19050, total loss = 4.03, predict loss = 1.14 (59.4 examples/sec; 0.067 sec/batch; 2h:26m:51s remains)
INFO - root - 2019-11-06 18:24:30.250475: step 19060, total loss = 4.09, predict loss = 1.17 (92.2 examples/sec; 0.043 sec/batch; 1h:34m:41s remains)
INFO - root - 2019-11-06 18:24:30.700587: step 19070, total loss = 5.82, predict loss = 1.74 (95.6 examples/sec; 0.042 sec/batch; 1h:31m:19s remains)
INFO - root - 2019-11-06 18:24:31.185587: step 19080, total loss = 5.67, predict loss = 1.57 (88.7 examples/sec; 0.045 sec/batch; 1h:38m:22s remains)
INFO - root - 2019-11-06 18:24:32.451110: step 19090, total loss = 4.13, predict loss = 1.20 (66.0 examples/sec; 0.061 sec/batch; 2h:12m:08s remains)
INFO - root - 2019-11-06 18:24:33.165693: step 19100, total loss = 5.46, predict loss = 1.56 (57.3 examples/sec; 0.070 sec/batch; 2h:32m:18s remains)
INFO - root - 2019-11-06 18:24:33.904806: step 19110, total loss = 5.10, predict loss = 1.44 (59.9 examples/sec; 0.067 sec/batch; 2h:25m:44s remains)
INFO - root - 2019-11-06 18:24:34.659506: step 19120, total loss = 4.73, predict loss = 1.38 (55.1 examples/sec; 0.073 sec/batch; 2h:38m:24s remains)
INFO - root - 2019-11-06 18:24:35.359334: step 19130, total loss = 4.91, predict loss = 1.42 (81.6 examples/sec; 0.049 sec/batch; 1h:46m:52s remains)
INFO - root - 2019-11-06 18:24:35.873439: step 19140, total loss = 5.06, predict loss = 1.42 (97.3 examples/sec; 0.041 sec/batch; 1h:29m:39s remains)
INFO - root - 2019-11-06 18:24:36.328243: step 19150, total loss = 5.84, predict loss = 1.61 (94.7 examples/sec; 0.042 sec/batch; 1h:32m:04s remains)
INFO - root - 2019-11-06 18:24:37.494268: step 19160, total loss = 6.02, predict loss = 1.74 (69.3 examples/sec; 0.058 sec/batch; 2h:05m:54s remains)
INFO - root - 2019-11-06 18:24:38.184646: step 19170, total loss = 4.73, predict loss = 1.38 (64.2 examples/sec; 0.062 sec/batch; 2h:15m:45s remains)
INFO - root - 2019-11-06 18:24:38.883813: step 19180, total loss = 6.19, predict loss = 1.81 (62.4 examples/sec; 0.064 sec/batch; 2h:19m:51s remains)
INFO - root - 2019-11-06 18:24:39.628284: step 19190, total loss = 6.63, predict loss = 1.96 (56.7 examples/sec; 0.071 sec/batch; 2h:33m:54s remains)
INFO - root - 2019-11-06 18:24:40.379859: step 19200, total loss = 6.56, predict loss = 1.88 (57.6 examples/sec; 0.069 sec/batch; 2h:31m:21s remains)
INFO - root - 2019-11-06 18:24:41.021347: step 19210, total loss = 5.37, predict loss = 1.55 (90.7 examples/sec; 0.044 sec/batch; 1h:36m:11s remains)
INFO - root - 2019-11-06 18:24:41.475415: step 19220, total loss = 5.33, predict loss = 1.56 (97.9 examples/sec; 0.041 sec/batch; 1h:29m:04s remains)
INFO - root - 2019-11-06 18:24:41.920373: step 19230, total loss = 4.08, predict loss = 1.22 (96.1 examples/sec; 0.042 sec/batch; 1h:30m:41s remains)
INFO - root - 2019-11-06 18:24:43.196410: step 19240, total loss = 5.43, predict loss = 1.58 (62.7 examples/sec; 0.064 sec/batch; 2h:19m:07s remains)
INFO - root - 2019-11-06 18:24:43.918936: step 19250, total loss = 5.32, predict loss = 1.50 (60.6 examples/sec; 0.066 sec/batch; 2h:23m:46s remains)
INFO - root - 2019-11-06 18:24:44.638240: step 19260, total loss = 3.71, predict loss = 1.08 (60.3 examples/sec; 0.066 sec/batch; 2h:24m:33s remains)
INFO - root - 2019-11-06 18:24:45.380706: step 19270, total loss = 4.77, predict loss = 1.33 (57.4 examples/sec; 0.070 sec/batch; 2h:31m:45s remains)
INFO - root - 2019-11-06 18:24:46.067008: step 19280, total loss = 3.15, predict loss = 0.94 (65.2 examples/sec; 0.061 sec/batch; 2h:13m:35s remains)
INFO - root - 2019-11-06 18:24:46.613680: step 19290, total loss = 5.29, predict loss = 1.50 (91.1 examples/sec; 0.044 sec/batch; 1h:35m:38s remains)
INFO - root - 2019-11-06 18:24:47.058969: step 19300, total loss = 4.98, predict loss = 1.43 (98.3 examples/sec; 0.041 sec/batch; 1h:28m:40s remains)
INFO - root - 2019-11-06 18:24:48.236210: step 19310, total loss = 5.98, predict loss = 1.71 (65.9 examples/sec; 0.061 sec/batch; 2h:12m:12s remains)
INFO - root - 2019-11-06 18:24:49.024615: step 19320, total loss = 4.94, predict loss = 1.42 (42.5 examples/sec; 0.094 sec/batch; 3h:24m:58s remains)
INFO - root - 2019-11-06 18:24:49.765070: step 19330, total loss = 5.99, predict loss = 1.74 (60.7 examples/sec; 0.066 sec/batch; 2h:23m:29s remains)
INFO - root - 2019-11-06 18:24:50.523154: step 19340, total loss = 4.10, predict loss = 1.10 (57.5 examples/sec; 0.070 sec/batch; 2h:31m:29s remains)
INFO - root - 2019-11-06 18:24:51.287353: step 19350, total loss = 4.21, predict loss = 1.22 (60.0 examples/sec; 0.067 sec/batch; 2h:25m:12s remains)
INFO - root - 2019-11-06 18:24:51.891256: step 19360, total loss = 5.37, predict loss = 1.54 (69.1 examples/sec; 0.058 sec/batch; 2h:06m:02s remains)
INFO - root - 2019-11-06 18:24:52.387106: step 19370, total loss = 6.26, predict loss = 1.83 (97.3 examples/sec; 0.041 sec/batch; 1h:29m:27s remains)
INFO - root - 2019-11-06 18:24:52.843871: step 19380, total loss = 4.77, predict loss = 1.25 (97.7 examples/sec; 0.041 sec/batch; 1h:29m:06s remains)
INFO - root - 2019-11-06 18:24:54.175775: step 19390, total loss = 6.29, predict loss = 1.76 (62.4 examples/sec; 0.064 sec/batch; 2h:19m:29s remains)
INFO - root - 2019-11-06 18:24:54.908138: step 19400, total loss = 5.19, predict loss = 1.49 (59.4 examples/sec; 0.067 sec/batch; 2h:26m:33s remains)
INFO - root - 2019-11-06 18:24:55.639593: step 19410, total loss = 4.93, predict loss = 1.41 (68.9 examples/sec; 0.058 sec/batch; 2h:06m:19s remains)
INFO - root - 2019-11-06 18:24:56.368149: step 19420, total loss = 6.11, predict loss = 1.83 (64.2 examples/sec; 0.062 sec/batch; 2h:15m:30s remains)
INFO - root - 2019-11-06 18:24:57.062995: step 19430, total loss = 3.73, predict loss = 1.06 (69.8 examples/sec; 0.057 sec/batch; 2h:04m:37s remains)
INFO - root - 2019-11-06 18:24:57.569382: step 19440, total loss = 5.06, predict loss = 1.45 (97.8 examples/sec; 0.041 sec/batch; 1h:29m:01s remains)
INFO - root - 2019-11-06 18:24:58.050872: step 19450, total loss = 4.63, predict loss = 1.37 (92.1 examples/sec; 0.043 sec/batch; 1h:34m:27s remains)
INFO - root - 2019-11-06 18:24:59.224516: step 19460, total loss = 3.55, predict loss = 1.01 (71.9 examples/sec; 0.056 sec/batch; 2h:01m:01s remains)
INFO - root - 2019-11-06 18:25:00.015992: step 19470, total loss = 4.69, predict loss = 1.34 (55.1 examples/sec; 0.073 sec/batch; 2h:37m:48s remains)
INFO - root - 2019-11-06 18:25:00.740029: step 19480, total loss = 4.00, predict loss = 1.23 (61.0 examples/sec; 0.066 sec/batch; 2h:22m:32s remains)
INFO - root - 2019-11-06 18:25:01.489177: step 19490, total loss = 6.02, predict loss = 1.79 (53.3 examples/sec; 0.075 sec/batch; 2h:43m:18s remains)
INFO - root - 2019-11-06 18:25:02.209248: step 19500, total loss = 4.95, predict loss = 1.40 (66.0 examples/sec; 0.061 sec/batch; 2h:11m:53s remains)
INFO - root - 2019-11-06 18:25:02.750654: step 19510, total loss = 4.88, predict loss = 1.39 (96.1 examples/sec; 0.042 sec/batch; 1h:30m:32s remains)
INFO - root - 2019-11-06 18:25:03.206951: step 19520, total loss = 5.24, predict loss = 1.52 (96.1 examples/sec; 0.042 sec/batch; 1h:30m:29s remains)
INFO - root - 2019-11-06 18:25:03.692249: step 19530, total loss = 3.37, predict loss = 0.94 (112.2 examples/sec; 0.036 sec/batch; 1h:17m:29s remains)
INFO - root - 2019-11-06 18:25:04.996777: step 19540, total loss = 3.95, predict loss = 1.14 (57.1 examples/sec; 0.070 sec/batch; 2h:32m:14s remains)
INFO - root - 2019-11-06 18:25:05.734167: step 19550, total loss = 3.76, predict loss = 1.06 (60.0 examples/sec; 0.067 sec/batch; 2h:24m:59s remains)
INFO - root - 2019-11-06 18:25:06.435533: step 19560, total loss = 4.02, predict loss = 1.20 (60.5 examples/sec; 0.066 sec/batch; 2h:23m:39s remains)
INFO - root - 2019-11-06 18:25:07.209090: step 19570, total loss = 4.72, predict loss = 1.32 (55.7 examples/sec; 0.072 sec/batch; 2h:36m:10s remains)
INFO - root - 2019-11-06 18:25:07.957014: step 19580, total loss = 6.48, predict loss = 1.78 (71.2 examples/sec; 0.056 sec/batch; 2h:02m:07s remains)
INFO - root - 2019-11-06 18:25:08.423088: step 19590, total loss = 4.90, predict loss = 1.34 (92.9 examples/sec; 0.043 sec/batch; 1h:33m:32s remains)
INFO - root - 2019-11-06 18:25:08.881427: step 19600, total loss = 4.09, predict loss = 1.17 (96.9 examples/sec; 0.041 sec/batch; 1h:29m:43s remains)
INFO - root - 2019-11-06 18:25:10.134415: step 19610, total loss = 6.23, predict loss = 1.80 (65.1 examples/sec; 0.061 sec/batch; 2h:13m:37s remains)
INFO - root - 2019-11-06 18:25:10.863194: step 19620, total loss = 6.04, predict loss = 1.82 (68.7 examples/sec; 0.058 sec/batch; 2h:06m:31s remains)
INFO - root - 2019-11-06 18:25:11.578409: step 19630, total loss = 6.73, predict loss = 1.91 (62.0 examples/sec; 0.065 sec/batch; 2h:20m:15s remains)
INFO - root - 2019-11-06 18:25:12.361877: step 19640, total loss = 6.13, predict loss = 1.75 (56.1 examples/sec; 0.071 sec/batch; 2h:34m:46s remains)
INFO - root - 2019-11-06 18:25:13.120985: step 19650, total loss = 5.46, predict loss = 1.67 (67.8 examples/sec; 0.059 sec/batch; 2h:08m:09s remains)
INFO - root - 2019-11-06 18:25:13.671648: step 19660, total loss = 5.87, predict loss = 1.69 (99.1 examples/sec; 0.040 sec/batch; 1h:27m:38s remains)
INFO - root - 2019-11-06 18:25:14.112941: step 19670, total loss = 5.33, predict loss = 1.52 (103.5 examples/sec; 0.039 sec/batch; 1h:23m:56s remains)
INFO - root - 2019-11-06 18:25:15.238953: step 19680, total loss = 4.76, predict loss = 1.37 (5.6 examples/sec; 0.719 sec/batch; 26h:01m:51s remains)
INFO - root - 2019-11-06 18:25:15.926834: step 19690, total loss = 3.81, predict loss = 1.20 (63.8 examples/sec; 0.063 sec/batch; 2h:16m:11s remains)
INFO - root - 2019-11-06 18:25:16.681525: step 19700, total loss = 5.82, predict loss = 1.70 (59.7 examples/sec; 0.067 sec/batch; 2h:25m:37s remains)
INFO - root - 2019-11-06 18:25:17.452076: step 19710, total loss = 5.10, predict loss = 1.50 (47.1 examples/sec; 0.085 sec/batch; 3h:04m:31s remains)
INFO - root - 2019-11-06 18:25:18.201227: step 19720, total loss = 2.53, predict loss = 0.72 (59.6 examples/sec; 0.067 sec/batch; 2h:25m:37s remains)
INFO - root - 2019-11-06 18:25:18.912038: step 19730, total loss = 6.03, predict loss = 1.74 (88.4 examples/sec; 0.045 sec/batch; 1h:38m:15s remains)
INFO - root - 2019-11-06 18:25:19.375982: step 19740, total loss = 5.11, predict loss = 1.42 (94.6 examples/sec; 0.042 sec/batch; 1h:31m:47s remains)
INFO - root - 2019-11-06 18:25:19.822519: step 19750, total loss = 4.94, predict loss = 1.45 (90.8 examples/sec; 0.044 sec/batch; 1h:35m:36s remains)
INFO - root - 2019-11-06 18:25:21.071437: step 19760, total loss = 4.44, predict loss = 1.25 (57.1 examples/sec; 0.070 sec/batch; 2h:31m:56s remains)
INFO - root - 2019-11-06 18:25:21.822696: step 19770, total loss = 4.06, predict loss = 1.23 (63.7 examples/sec; 0.063 sec/batch; 2h:16m:12s remains)
INFO - root - 2019-11-06 18:25:22.596798: step 19780, total loss = 5.00, predict loss = 1.43 (65.6 examples/sec; 0.061 sec/batch; 2h:12m:25s remains)
INFO - root - 2019-11-06 18:25:23.340505: step 19790, total loss = 4.51, predict loss = 1.29 (57.2 examples/sec; 0.070 sec/batch; 2h:31m:46s remains)
INFO - root - 2019-11-06 18:25:24.055711: step 19800, total loss = 4.21, predict loss = 1.23 (67.4 examples/sec; 0.059 sec/batch; 2h:08m:50s remains)
INFO - root - 2019-11-06 18:25:24.625226: step 19810, total loss = 6.55, predict loss = 1.88 (91.0 examples/sec; 0.044 sec/batch; 1h:35m:25s remains)
INFO - root - 2019-11-06 18:25:25.072735: step 19820, total loss = 5.46, predict loss = 1.50 (94.2 examples/sec; 0.042 sec/batch; 1h:32m:05s remains)
INFO - root - 2019-11-06 18:25:26.216626: step 19830, total loss = 6.01, predict loss = 1.82 (76.3 examples/sec; 0.052 sec/batch; 1h:53m:47s remains)
INFO - root - 2019-11-06 18:25:26.884433: step 19840, total loss = 5.06, predict loss = 1.48 (56.6 examples/sec; 0.071 sec/batch; 2h:33m:24s remains)
INFO - root - 2019-11-06 18:25:27.642579: step 19850, total loss = 4.32, predict loss = 1.22 (52.0 examples/sec; 0.077 sec/batch; 2h:46m:51s remains)
INFO - root - 2019-11-06 18:25:28.509554: step 19860, total loss = 4.37, predict loss = 1.32 (49.6 examples/sec; 0.081 sec/batch; 2h:54m:45s remains)
INFO - root - 2019-11-06 18:25:29.362667: step 19870, total loss = 5.25, predict loss = 1.56 (49.5 examples/sec; 0.081 sec/batch; 2h:55m:12s remains)
INFO - root - 2019-11-06 18:25:29.975540: step 19880, total loss = 4.00, predict loss = 1.18 (97.8 examples/sec; 0.041 sec/batch; 1h:28m:41s remains)
INFO - root - 2019-11-06 18:25:30.442171: step 19890, total loss = 5.36, predict loss = 1.52 (85.9 examples/sec; 0.047 sec/batch; 1h:40m:58s remains)
INFO - root - 2019-11-06 18:25:30.886047: step 19900, total loss = 5.14, predict loss = 1.50 (95.4 examples/sec; 0.042 sec/batch; 1h:30m:52s remains)
INFO - root - 2019-11-06 18:25:32.174773: step 19910, total loss = 5.52, predict loss = 1.55 (64.5 examples/sec; 0.062 sec/batch; 2h:14m:31s remains)
INFO - root - 2019-11-06 18:25:32.930297: step 19920, total loss = 5.82, predict loss = 1.65 (54.7 examples/sec; 0.073 sec/batch; 2h:38m:24s remains)
INFO - root - 2019-11-06 18:25:33.753182: step 19930, total loss = 3.11, predict loss = 0.89 (54.1 examples/sec; 0.074 sec/batch; 2h:40m:21s remains)
INFO - root - 2019-11-06 18:25:34.483878: step 19940, total loss = 2.69, predict loss = 0.78 (61.1 examples/sec; 0.065 sec/batch; 2h:21m:52s remains)
INFO - root - 2019-11-06 18:25:35.187964: step 19950, total loss = 3.73, predict loss = 1.06 (67.9 examples/sec; 0.059 sec/batch; 2h:07m:39s remains)
INFO - root - 2019-11-06 18:25:35.696784: step 19960, total loss = 4.68, predict loss = 1.34 (102.1 examples/sec; 0.039 sec/batch; 1h:24m:54s remains)
INFO - root - 2019-11-06 18:25:36.190842: step 19970, total loss = 4.36, predict loss = 1.26 (92.7 examples/sec; 0.043 sec/batch; 1h:33m:32s remains)
INFO - root - 2019-11-06 18:25:37.363558: step 19980, total loss = 5.58, predict loss = 1.65 (70.2 examples/sec; 0.057 sec/batch; 2h:03m:32s remains)
INFO - root - 2019-11-06 18:25:38.067187: step 19990, total loss = 5.47, predict loss = 1.66 (59.9 examples/sec; 0.067 sec/batch; 2h:24m:42s remains)
INFO - root - 2019-11-06 18:25:38.791455: step 20000, total loss = 5.09, predict loss = 1.46 (60.3 examples/sec; 0.066 sec/batch; 2h:23m:48s remains)
INFO - root - 2019-11-06 18:25:39.544869: step 20010, total loss = 5.84, predict loss = 1.71 (52.0 examples/sec; 0.077 sec/batch; 2h:46m:40s remains)
INFO - root - 2019-11-06 18:25:40.347660: step 20020, total loss = 5.93, predict loss = 1.86 (55.0 examples/sec; 0.073 sec/batch; 2h:37m:28s remains)
INFO - root - 2019-11-06 18:25:40.966524: step 20030, total loss = 3.83, predict loss = 1.08 (93.6 examples/sec; 0.043 sec/batch; 1h:32m:34s remains)
INFO - root - 2019-11-06 18:25:41.403806: step 20040, total loss = 4.33, predict loss = 1.19 (93.3 examples/sec; 0.043 sec/batch; 1h:32m:51s remains)
INFO - root - 2019-11-06 18:25:41.893436: step 20050, total loss = 5.20, predict loss = 1.51 (97.0 examples/sec; 0.041 sec/batch; 1h:29m:19s remains)
INFO - root - 2019-11-06 18:25:43.178101: step 20060, total loss = 5.41, predict loss = 1.55 (58.0 examples/sec; 0.069 sec/batch; 2h:29m:18s remains)
INFO - root - 2019-11-06 18:25:43.916409: step 20070, total loss = 5.31, predict loss = 1.68 (56.6 examples/sec; 0.071 sec/batch; 2h:32m:55s remains)
INFO - root - 2019-11-06 18:25:44.767010: step 20080, total loss = 4.38, predict loss = 1.33 (47.8 examples/sec; 0.084 sec/batch; 3h:01m:04s remains)
INFO - root - 2019-11-06 18:25:45.508743: step 20090, total loss = 5.89, predict loss = 1.71 (66.9 examples/sec; 0.060 sec/batch; 2h:09m:30s remains)
INFO - root - 2019-11-06 18:25:46.224133: step 20100, total loss = 5.12, predict loss = 1.47 (68.7 examples/sec; 0.058 sec/batch; 2h:06m:08s remains)
INFO - root - 2019-11-06 18:25:46.723088: step 20110, total loss = 4.70, predict loss = 1.22 (99.9 examples/sec; 0.040 sec/batch; 1h:26m:39s remains)
INFO - root - 2019-11-06 18:25:47.181032: step 20120, total loss = 6.01, predict loss = 1.73 (91.0 examples/sec; 0.044 sec/batch; 1h:35m:06s remains)
INFO - root - 2019-11-06 18:25:48.407981: step 20130, total loss = 4.85, predict loss = 1.35 (60.6 examples/sec; 0.066 sec/batch; 2h:22m:51s remains)
INFO - root - 2019-11-06 18:25:49.130267: step 20140, total loss = 4.78, predict loss = 1.41 (62.4 examples/sec; 0.064 sec/batch; 2h:18m:38s remains)
INFO - root - 2019-11-06 18:25:49.889004: step 20150, total loss = 3.94, predict loss = 1.08 (62.2 examples/sec; 0.064 sec/batch; 2h:19m:12s remains)
INFO - root - 2019-11-06 18:25:50.630062: step 20160, total loss = 6.36, predict loss = 1.83 (59.3 examples/sec; 0.067 sec/batch; 2h:25m:59s remains)
INFO - root - 2019-11-06 18:25:51.364679: step 20170, total loss = 4.40, predict loss = 1.27 (57.2 examples/sec; 0.070 sec/batch; 2h:31m:25s remains)
INFO - root - 2019-11-06 18:25:52.000319: step 20180, total loss = 5.57, predict loss = 1.62 (81.7 examples/sec; 0.049 sec/batch; 1h:45m:55s remains)
INFO - root - 2019-11-06 18:25:52.466867: step 20190, total loss = 5.03, predict loss = 1.50 (89.5 examples/sec; 0.045 sec/batch; 1h:36m:43s remains)
INFO - root - 2019-11-06 18:25:52.915292: step 20200, total loss = 4.86, predict loss = 1.35 (98.9 examples/sec; 0.040 sec/batch; 1h:27m:30s remains)
INFO - root - 2019-11-06 18:25:54.276947: step 20210, total loss = 4.62, predict loss = 1.33 (61.1 examples/sec; 0.065 sec/batch; 2h:21m:30s remains)
INFO - root - 2019-11-06 18:25:55.116920: step 20220, total loss = 5.24, predict loss = 1.51 (54.4 examples/sec; 0.073 sec/batch; 2h:38m:54s remains)
INFO - root - 2019-11-06 18:25:55.882016: step 20230, total loss = 4.37, predict loss = 1.29 (56.3 examples/sec; 0.071 sec/batch; 2h:33m:34s remains)
INFO - root - 2019-11-06 18:25:56.670608: step 20240, total loss = 5.05, predict loss = 1.49 (53.6 examples/sec; 0.075 sec/batch; 2h:41m:20s remains)
INFO - root - 2019-11-06 18:25:57.394141: step 20250, total loss = 4.74, predict loss = 1.37 (77.1 examples/sec; 0.052 sec/batch; 1h:52m:07s remains)
INFO - root - 2019-11-06 18:25:57.858207: step 20260, total loss = 4.88, predict loss = 1.41 (94.2 examples/sec; 0.042 sec/batch; 1h:31m:49s remains)
INFO - root - 2019-11-06 18:25:58.328095: step 20270, total loss = 5.98, predict loss = 1.64 (88.3 examples/sec; 0.045 sec/batch; 1h:37m:59s remains)
INFO - root - 2019-11-06 18:25:59.482872: step 20280, total loss = 6.18, predict loss = 1.70 (69.6 examples/sec; 0.057 sec/batch; 2h:04m:18s remains)
INFO - root - 2019-11-06 18:26:00.215994: step 20290, total loss = 4.61, predict loss = 1.40 (60.0 examples/sec; 0.067 sec/batch; 2h:24m:07s remains)
INFO - root - 2019-11-06 18:26:00.971455: step 20300, total loss = 6.61, predict loss = 2.07 (57.3 examples/sec; 0.070 sec/batch; 2h:30m:59s remains)
INFO - root - 2019-11-06 18:26:01.732102: step 20310, total loss = 5.83, predict loss = 1.70 (60.3 examples/sec; 0.066 sec/batch; 2h:23m:18s remains)
INFO - root - 2019-11-06 18:26:02.497519: step 20320, total loss = 5.75, predict loss = 1.65 (54.1 examples/sec; 0.074 sec/batch; 2h:39m:50s remains)
INFO - root - 2019-11-06 18:26:03.095506: step 20330, total loss = 6.49, predict loss = 1.77 (94.7 examples/sec; 0.042 sec/batch; 1h:31m:19s remains)
INFO - root - 2019-11-06 18:26:03.553680: step 20340, total loss = 3.20, predict loss = 0.95 (90.7 examples/sec; 0.044 sec/batch; 1h:35m:20s remains)
INFO - root - 2019-11-06 18:26:04.006882: step 20350, total loss = 6.40, predict loss = 1.87 (127.8 examples/sec; 0.031 sec/batch; 1h:07m:38s remains)
INFO - root - 2019-11-06 18:26:05.358644: step 20360, total loss = 5.58, predict loss = 1.60 (58.4 examples/sec; 0.069 sec/batch; 2h:28m:05s remains)
INFO - root - 2019-11-06 18:26:06.085844: step 20370, total loss = 5.38, predict loss = 1.51 (62.5 examples/sec; 0.064 sec/batch; 2h:18m:10s remains)
INFO - root - 2019-11-06 18:26:06.867124: step 20380, total loss = 2.94, predict loss = 0.87 (55.1 examples/sec; 0.073 sec/batch; 2h:36m:57s remains)
INFO - root - 2019-11-06 18:26:07.715969: step 20390, total loss = 5.62, predict loss = 1.50 (49.1 examples/sec; 0.081 sec/batch; 2h:55m:55s remains)
INFO - root - 2019-11-06 18:26:08.415417: step 20400, total loss = 6.23, predict loss = 1.80 (69.6 examples/sec; 0.057 sec/batch; 2h:04m:06s remains)
INFO - root - 2019-11-06 18:26:08.928775: step 20410, total loss = 6.37, predict loss = 1.82 (92.3 examples/sec; 0.043 sec/batch; 1h:33m:36s remains)
INFO - root - 2019-11-06 18:26:09.365135: step 20420, total loss = 4.64, predict loss = 1.33 (100.4 examples/sec; 0.040 sec/batch; 1h:26m:01s remains)
INFO - root - 2019-11-06 18:26:10.559219: step 20430, total loss = 3.62, predict loss = 1.06 (68.9 examples/sec; 0.058 sec/batch; 2h:05m:19s remains)
INFO - root - 2019-11-06 18:26:11.266636: step 20440, total loss = 6.21, predict loss = 1.70 (57.4 examples/sec; 0.070 sec/batch; 2h:30m:31s remains)
INFO - root - 2019-11-06 18:26:12.005282: step 20450, total loss = 3.74, predict loss = 1.06 (56.0 examples/sec; 0.071 sec/batch; 2h:34m:10s remains)
INFO - root - 2019-11-06 18:26:12.768938: step 20460, total loss = 6.11, predict loss = 1.73 (60.1 examples/sec; 0.067 sec/batch; 2h:23m:43s remains)
INFO - root - 2019-11-06 18:26:13.533960: step 20470, total loss = 3.78, predict loss = 1.18 (69.0 examples/sec; 0.058 sec/batch; 2h:05m:06s remains)
INFO - root - 2019-11-06 18:26:14.095893: step 20480, total loss = 5.69, predict loss = 1.56 (99.5 examples/sec; 0.040 sec/batch; 1h:26m:48s remains)
INFO - root - 2019-11-06 18:26:14.565484: step 20490, total loss = 4.17, predict loss = 1.20 (103.8 examples/sec; 0.039 sec/batch; 1h:23m:12s remains)
INFO - root - 2019-11-06 18:26:15.685899: step 20500, total loss = 4.14, predict loss = 1.20 (5.5 examples/sec; 0.722 sec/batch; 25h:57m:41s remains)
INFO - root - 2019-11-06 18:26:16.354733: step 20510, total loss = 4.78, predict loss = 1.36 (63.2 examples/sec; 0.063 sec/batch; 2h:16m:31s remains)
INFO - root - 2019-11-06 18:26:17.089034: step 20520, total loss = 4.27, predict loss = 1.28 (63.1 examples/sec; 0.063 sec/batch; 2h:16m:49s remains)
INFO - root - 2019-11-06 18:26:17.842267: step 20530, total loss = 4.84, predict loss = 1.40 (56.9 examples/sec; 0.070 sec/batch; 2h:31m:43s remains)
INFO - root - 2019-11-06 18:26:18.607460: step 20540, total loss = 3.59, predict loss = 1.02 (58.6 examples/sec; 0.068 sec/batch; 2h:27m:20s remains)
INFO - root - 2019-11-06 18:26:19.288651: step 20550, total loss = 5.00, predict loss = 1.39 (88.1 examples/sec; 0.045 sec/batch; 1h:37m:59s remains)
INFO - root - 2019-11-06 18:26:19.744788: step 20560, total loss = 6.32, predict loss = 1.71 (96.1 examples/sec; 0.042 sec/batch; 1h:29m:45s remains)
INFO - root - 2019-11-06 18:26:20.238182: step 20570, total loss = 5.83, predict loss = 1.69 (86.2 examples/sec; 0.046 sec/batch; 1h:40m:08s remains)
INFO - root - 2019-11-06 18:26:21.472040: step 20580, total loss = 6.92, predict loss = 1.99 (72.7 examples/sec; 0.055 sec/batch; 1h:58m:39s remains)
INFO - root - 2019-11-06 18:26:22.226110: step 20590, total loss = 6.51, predict loss = 1.85 (63.4 examples/sec; 0.063 sec/batch; 2h:16m:11s remains)
INFO - root - 2019-11-06 18:26:22.964352: step 20600, total loss = 5.98, predict loss = 1.76 (51.9 examples/sec; 0.077 sec/batch; 2h:46m:14s remains)
INFO - root - 2019-11-06 18:26:23.731646: step 20610, total loss = 3.38, predict loss = 0.97 (61.5 examples/sec; 0.065 sec/batch; 2h:20m:13s remains)
INFO - root - 2019-11-06 18:26:24.459088: step 20620, total loss = 5.14, predict loss = 1.47 (67.5 examples/sec; 0.059 sec/batch; 2h:07m:51s remains)
INFO - root - 2019-11-06 18:26:24.983773: step 20630, total loss = 3.48, predict loss = 0.99 (90.4 examples/sec; 0.044 sec/batch; 1h:35m:22s remains)
INFO - root - 2019-11-06 18:26:25.439640: step 20640, total loss = 4.54, predict loss = 1.27 (91.7 examples/sec; 0.044 sec/batch; 1h:34m:01s remains)
INFO - root - 2019-11-06 18:26:26.607102: step 20650, total loss = 5.88, predict loss = 1.69 (75.9 examples/sec; 0.053 sec/batch; 1h:53m:32s remains)
INFO - root - 2019-11-06 18:26:27.292760: step 20660, total loss = 4.24, predict loss = 1.23 (54.7 examples/sec; 0.073 sec/batch; 2h:37m:31s remains)
INFO - root - 2019-11-06 18:26:28.107751: step 20670, total loss = 5.97, predict loss = 1.69 (54.9 examples/sec; 0.073 sec/batch; 2h:37m:08s remains)
INFO - root - 2019-11-06 18:26:28.863654: step 20680, total loss = 5.64, predict loss = 1.62 (57.5 examples/sec; 0.070 sec/batch; 2h:29m:54s remains)
INFO - root - 2019-11-06 18:26:29.619705: step 20690, total loss = 6.13, predict loss = 1.69 (55.0 examples/sec; 0.073 sec/batch; 2h:36m:42s remains)
INFO - root - 2019-11-06 18:26:30.225195: step 20700, total loss = 4.71, predict loss = 1.39 (92.9 examples/sec; 0.043 sec/batch; 1h:32m:49s remains)
INFO - root - 2019-11-06 18:26:30.676891: step 20710, total loss = 4.90, predict loss = 1.44 (93.3 examples/sec; 0.043 sec/batch; 1h:32m:24s remains)
INFO - root - 2019-11-06 18:26:31.138064: step 20720, total loss = 5.76, predict loss = 1.66 (96.7 examples/sec; 0.041 sec/batch; 1h:29m:07s remains)
INFO - root - 2019-11-06 18:26:32.423949: step 20730, total loss = 5.03, predict loss = 1.49 (65.7 examples/sec; 0.061 sec/batch; 2h:11m:16s remains)
INFO - root - 2019-11-06 18:26:33.174310: step 20740, total loss = 5.34, predict loss = 1.58 (55.6 examples/sec; 0.072 sec/batch; 2h:35m:05s remains)
INFO - root - 2019-11-06 18:26:33.904667: step 20750, total loss = 5.74, predict loss = 1.74 (64.2 examples/sec; 0.062 sec/batch; 2h:14m:10s remains)
INFO - root - 2019-11-06 18:26:34.650890: step 20760, total loss = 5.57, predict loss = 1.54 (52.2 examples/sec; 0.077 sec/batch; 2h:44m:58s remains)
INFO - root - 2019-11-06 18:26:35.435038: step 20770, total loss = 4.79, predict loss = 1.34 (76.7 examples/sec; 0.052 sec/batch; 1h:52m:18s remains)
INFO - root - 2019-11-06 18:26:35.947709: step 20780, total loss = 6.40, predict loss = 1.83 (92.9 examples/sec; 0.043 sec/batch; 1h:32m:42s remains)
INFO - root - 2019-11-06 18:26:36.404432: step 20790, total loss = 5.17, predict loss = 1.46 (94.0 examples/sec; 0.043 sec/batch; 1h:31m:39s remains)
INFO - root - 2019-11-06 18:26:37.549987: step 20800, total loss = 4.51, predict loss = 1.33 (68.7 examples/sec; 0.058 sec/batch; 2h:05m:24s remains)
INFO - root - 2019-11-06 18:26:38.272554: step 20810, total loss = 5.97, predict loss = 1.79 (56.0 examples/sec; 0.071 sec/batch; 2h:33m:56s remains)
INFO - root - 2019-11-06 18:26:39.011332: step 20820, total loss = 6.06, predict loss = 1.72 (60.8 examples/sec; 0.066 sec/batch; 2h:21m:39s remains)
INFO - root - 2019-11-06 18:26:39.759166: step 20830, total loss = 5.27, predict loss = 1.48 (58.6 examples/sec; 0.068 sec/batch; 2h:26m:58s remains)
INFO - root - 2019-11-06 18:26:40.464858: step 20840, total loss = 5.06, predict loss = 1.48 (63.2 examples/sec; 0.063 sec/batch; 2h:16m:10s remains)
INFO - root - 2019-11-06 18:26:41.071414: step 20850, total loss = 4.22, predict loss = 1.16 (95.1 examples/sec; 0.042 sec/batch; 1h:30m:30s remains)
INFO - root - 2019-11-06 18:26:41.530807: step 20860, total loss = 5.47, predict loss = 1.56 (97.8 examples/sec; 0.041 sec/batch; 1h:28m:04s remains)
INFO - root - 2019-11-06 18:26:41.969093: step 20870, total loss = 5.16, predict loss = 1.50 (97.8 examples/sec; 0.041 sec/batch; 1h:27m:58s remains)
INFO - root - 2019-11-06 18:26:43.254195: step 20880, total loss = 5.32, predict loss = 1.53 (68.5 examples/sec; 0.058 sec/batch; 2h:05m:43s remains)
INFO - root - 2019-11-06 18:26:44.001094: step 20890, total loss = 4.87, predict loss = 1.42 (54.8 examples/sec; 0.073 sec/batch; 2h:37m:04s remains)
INFO - root - 2019-11-06 18:26:44.736116: step 20900, total loss = 4.95, predict loss = 1.45 (57.8 examples/sec; 0.069 sec/batch; 2h:28m:54s remains)
INFO - root - 2019-11-06 18:26:45.471591: step 20910, total loss = 4.87, predict loss = 1.44 (60.4 examples/sec; 0.066 sec/batch; 2h:22m:27s remains)
INFO - root - 2019-11-06 18:26:46.166128: step 20920, total loss = 5.33, predict loss = 1.59 (77.8 examples/sec; 0.051 sec/batch; 1h:50m:40s remains)
INFO - root - 2019-11-06 18:26:46.661692: step 20930, total loss = 5.10, predict loss = 1.48 (103.6 examples/sec; 0.039 sec/batch; 1h:23m:03s remains)
INFO - root - 2019-11-06 18:26:47.119383: step 20940, total loss = 4.09, predict loss = 1.24 (85.8 examples/sec; 0.047 sec/batch; 1h:40m:17s remains)
INFO - root - 2019-11-06 18:26:48.298782: step 20950, total loss = 5.74, predict loss = 1.65 (67.2 examples/sec; 0.060 sec/batch; 2h:08m:01s remains)
INFO - root - 2019-11-06 18:26:48.997327: step 20960, total loss = 4.18, predict loss = 1.19 (62.8 examples/sec; 0.064 sec/batch; 2h:16m:52s remains)
INFO - root - 2019-11-06 18:26:49.733350: step 20970, total loss = 6.14, predict loss = 1.78 (59.8 examples/sec; 0.067 sec/batch; 2h:23m:54s remains)
INFO - root - 2019-11-06 18:26:50.475224: step 20980, total loss = 4.74, predict loss = 1.47 (64.0 examples/sec; 0.063 sec/batch; 2h:14m:28s remains)
INFO - root - 2019-11-06 18:26:51.260934: step 20990, total loss = 6.34, predict loss = 1.83 (53.0 examples/sec; 0.076 sec/batch; 2h:42m:21s remains)
INFO - root - 2019-11-06 18:26:51.867800: step 21000, total loss = 5.33, predict loss = 1.59 (81.3 examples/sec; 0.049 sec/batch; 1h:45m:48s remains)
INFO - root - 2019-11-06 18:26:52.384571: step 21010, total loss = 4.32, predict loss = 1.27 (98.4 examples/sec; 0.041 sec/batch; 1h:27m:24s remains)
INFO - root - 2019-11-06 18:26:52.853939: step 21020, total loss = 4.55, predict loss = 1.27 (95.2 examples/sec; 0.042 sec/batch; 1h:30m:17s remains)
INFO - root - 2019-11-06 18:26:54.160611: step 21030, total loss = 5.58, predict loss = 1.59 (60.3 examples/sec; 0.066 sec/batch; 2h:22m:36s remains)
INFO - root - 2019-11-06 18:26:54.873768: step 21040, total loss = 4.34, predict loss = 1.26 (60.1 examples/sec; 0.067 sec/batch; 2h:23m:09s remains)
INFO - root - 2019-11-06 18:26:55.611396: step 21050, total loss = 4.79, predict loss = 1.41 (47.1 examples/sec; 0.085 sec/batch; 3h:02m:41s remains)
INFO - root - 2019-11-06 18:26:56.373029: step 21060, total loss = 4.71, predict loss = 1.41 (60.7 examples/sec; 0.066 sec/batch; 2h:21m:31s remains)
INFO - root - 2019-11-06 18:26:57.122856: step 21070, total loss = 4.73, predict loss = 1.44 (66.9 examples/sec; 0.060 sec/batch; 2h:08m:30s remains)
INFO - root - 2019-11-06 18:26:57.627701: step 21080, total loss = 6.46, predict loss = 1.87 (98.9 examples/sec; 0.040 sec/batch; 1h:26m:54s remains)
INFO - root - 2019-11-06 18:26:58.110273: step 21090, total loss = 5.21, predict loss = 1.52 (102.4 examples/sec; 0.039 sec/batch; 1h:23m:53s remains)
INFO - root - 2019-11-06 18:26:59.270574: step 21100, total loss = 4.62, predict loss = 1.41 (66.4 examples/sec; 0.060 sec/batch; 2h:09m:29s remains)
INFO - root - 2019-11-06 18:26:59.988888: step 21110, total loss = 4.39, predict loss = 1.24 (59.6 examples/sec; 0.067 sec/batch; 2h:24m:04s remains)
INFO - root - 2019-11-06 18:27:00.690014: step 21120, total loss = 5.82, predict loss = 1.65 (59.8 examples/sec; 0.067 sec/batch; 2h:23m:37s remains)
INFO - root - 2019-11-06 18:27:01.471412: step 21130, total loss = 6.81, predict loss = 2.02 (58.2 examples/sec; 0.069 sec/batch; 2h:27m:35s remains)
INFO - root - 2019-11-06 18:27:02.245891: step 21140, total loss = 3.28, predict loss = 0.92 (57.5 examples/sec; 0.070 sec/batch; 2h:29m:27s remains)
INFO - root - 2019-11-06 18:27:02.857581: step 21150, total loss = 6.77, predict loss = 1.90 (89.0 examples/sec; 0.045 sec/batch; 1h:36m:33s remains)
INFO - root - 2019-11-06 18:27:03.329680: step 21160, total loss = 5.88, predict loss = 1.78 (98.9 examples/sec; 0.040 sec/batch; 1h:26m:51s remains)
INFO - root - 2019-11-06 18:27:03.810725: step 21170, total loss = 4.63, predict loss = 1.27 (125.0 examples/sec; 0.032 sec/batch; 1h:08m:41s remains)
INFO - root - 2019-11-06 18:27:05.154616: step 21180, total loss = 6.00, predict loss = 1.59 (56.8 examples/sec; 0.070 sec/batch; 2h:31m:05s remains)
INFO - root - 2019-11-06 18:27:05.918136: step 21190, total loss = 2.69, predict loss = 0.80 (59.3 examples/sec; 0.067 sec/batch; 2h:24m:53s remains)
INFO - root - 2019-11-06 18:27:06.692106: step 21200, total loss = 4.05, predict loss = 1.19 (60.1 examples/sec; 0.067 sec/batch; 2h:22m:57s remains)
INFO - root - 2019-11-06 18:27:07.439436: step 21210, total loss = 4.01, predict loss = 1.04 (60.0 examples/sec; 0.067 sec/batch; 2h:23m:06s remains)
INFO - root - 2019-11-06 18:27:08.141613: step 21220, total loss = 3.75, predict loss = 1.15 (75.6 examples/sec; 0.053 sec/batch; 1h:53m:29s remains)
INFO - root - 2019-11-06 18:27:08.621121: step 21230, total loss = 4.23, predict loss = 1.20 (89.4 examples/sec; 0.045 sec/batch; 1h:35m:59s remains)
INFO - root - 2019-11-06 18:27:09.079936: step 21240, total loss = 4.73, predict loss = 1.26 (94.8 examples/sec; 0.042 sec/batch; 1h:30m:33s remains)
INFO - root - 2019-11-06 18:27:10.321181: step 21250, total loss = 6.15, predict loss = 1.74 (62.7 examples/sec; 0.064 sec/batch; 2h:16m:57s remains)
INFO - root - 2019-11-06 18:27:11.074991: step 21260, total loss = 4.76, predict loss = 1.42 (54.8 examples/sec; 0.073 sec/batch; 2h:36m:42s remains)
INFO - root - 2019-11-06 18:27:11.882383: step 21270, total loss = 4.96, predict loss = 1.41 (53.1 examples/sec; 0.075 sec/batch; 2h:41m:43s remains)
INFO - root - 2019-11-06 18:27:12.662029: step 21280, total loss = 3.70, predict loss = 1.12 (55.8 examples/sec; 0.072 sec/batch; 2h:33m:50s remains)
INFO - root - 2019-11-06 18:27:13.410335: step 21290, total loss = 6.27, predict loss = 1.75 (67.9 examples/sec; 0.059 sec/batch; 2h:06m:24s remains)
INFO - root - 2019-11-06 18:27:13.929146: step 21300, total loss = 4.95, predict loss = 1.42 (97.1 examples/sec; 0.041 sec/batch; 1h:28m:20s remains)
INFO - root - 2019-11-06 18:27:14.380139: step 21310, total loss = 4.74, predict loss = 1.32 (94.6 examples/sec; 0.042 sec/batch; 1h:30m:39s remains)
INFO - root - 2019-11-06 18:27:15.458487: step 21320, total loss = 5.03, predict loss = 1.46 (5.8 examples/sec; 0.689 sec/batch; 24h:37m:31s remains)
INFO - root - 2019-11-06 18:27:16.172460: step 21330, total loss = 4.24, predict loss = 1.17 (60.1 examples/sec; 0.067 sec/batch; 2h:22m:43s remains)
INFO - root - 2019-11-06 18:27:16.970717: step 21340, total loss = 5.96, predict loss = 1.70 (53.5 examples/sec; 0.075 sec/batch; 2h:40m:27s remains)
INFO - root - 2019-11-06 18:27:17.745348: step 21350, total loss = 3.62, predict loss = 1.06 (56.9 examples/sec; 0.070 sec/batch; 2h:30m:39s remains)
INFO - root - 2019-11-06 18:27:18.546408: step 21360, total loss = 5.17, predict loss = 1.49 (51.2 examples/sec; 0.078 sec/batch; 2h:47m:23s remains)
INFO - root - 2019-11-06 18:27:19.288383: step 21370, total loss = 3.78, predict loss = 1.14 (77.0 examples/sec; 0.052 sec/batch; 1h:51m:22s remains)
INFO - root - 2019-11-06 18:27:19.735924: step 21380, total loss = 3.68, predict loss = 1.10 (94.5 examples/sec; 0.042 sec/batch; 1h:30m:44s remains)
INFO - root - 2019-11-06 18:27:20.193770: step 21390, total loss = 4.06, predict loss = 1.18 (101.4 examples/sec; 0.039 sec/batch; 1h:24m:35s remains)
INFO - root - 2019-11-06 18:27:21.422069: step 21400, total loss = 6.03, predict loss = 1.76 (59.1 examples/sec; 0.068 sec/batch; 2h:24m:58s remains)
INFO - root - 2019-11-06 18:27:22.238258: step 21410, total loss = 5.96, predict loss = 1.76 (59.9 examples/sec; 0.067 sec/batch; 2h:23m:13s remains)
INFO - root - 2019-11-06 18:27:22.941758: step 21420, total loss = 4.26, predict loss = 1.22 (62.3 examples/sec; 0.064 sec/batch; 2h:17m:31s remains)
INFO - root - 2019-11-06 18:27:23.670538: step 21430, total loss = 5.65, predict loss = 1.67 (66.8 examples/sec; 0.060 sec/batch; 2h:08m:24s remains)
INFO - root - 2019-11-06 18:27:24.400793: step 21440, total loss = 4.21, predict loss = 1.15 (65.7 examples/sec; 0.061 sec/batch; 2h:10m:25s remains)
INFO - root - 2019-11-06 18:27:24.952556: step 21450, total loss = 5.79, predict loss = 1.68 (92.0 examples/sec; 0.043 sec/batch; 1h:33m:09s remains)
INFO - root - 2019-11-06 18:27:25.404000: step 21460, total loss = 4.45, predict loss = 1.24 (95.4 examples/sec; 0.042 sec/batch; 1h:29m:48s remains)
INFO - root - 2019-11-06 18:27:26.544156: step 21470, total loss = 3.64, predict loss = 1.06 (73.4 examples/sec; 0.054 sec/batch; 1h:56m:41s remains)
INFO - root - 2019-11-06 18:27:27.217738: step 21480, total loss = 4.34, predict loss = 1.29 (58.7 examples/sec; 0.068 sec/batch; 2h:25m:53s remains)
INFO - root - 2019-11-06 18:27:28.001064: step 21490, total loss = 2.65, predict loss = 0.80 (55.1 examples/sec; 0.073 sec/batch; 2h:35m:29s remains)
INFO - root - 2019-11-06 18:27:28.800094: step 21500, total loss = 6.14, predict loss = 1.75 (50.9 examples/sec; 0.079 sec/batch; 2h:48m:23s remains)
INFO - root - 2019-11-06 18:27:29.537880: step 21510, total loss = 5.79, predict loss = 1.67 (59.3 examples/sec; 0.068 sec/batch; 2h:24m:33s remains)
INFO - root - 2019-11-06 18:27:30.234636: step 21520, total loss = 6.74, predict loss = 1.86 (85.6 examples/sec; 0.047 sec/batch; 1h:40m:00s remains)
INFO - root - 2019-11-06 18:27:30.716433: step 21530, total loss = 5.21, predict loss = 1.46 (96.4 examples/sec; 0.041 sec/batch; 1h:28m:49s remains)
INFO - root - 2019-11-06 18:27:31.182695: step 21540, total loss = 4.75, predict loss = 1.33 (95.5 examples/sec; 0.042 sec/batch; 1h:29m:38s remains)
INFO - root - 2019-11-06 18:27:32.424025: step 21550, total loss = 4.39, predict loss = 1.25 (71.4 examples/sec; 0.056 sec/batch; 1h:59m:56s remains)
INFO - root - 2019-11-06 18:27:33.136253: step 21560, total loss = 5.56, predict loss = 1.67 (54.9 examples/sec; 0.073 sec/batch; 2h:35m:53s remains)
INFO - root - 2019-11-06 18:27:33.915020: step 21570, total loss = 4.44, predict loss = 1.26 (52.2 examples/sec; 0.077 sec/batch; 2h:44m:05s remains)
INFO - root - 2019-11-06 18:27:34.715291: step 21580, total loss = 5.41, predict loss = 1.55 (52.2 examples/sec; 0.077 sec/batch; 2h:44m:01s remains)
INFO - root - 2019-11-06 18:27:35.441654: step 21590, total loss = 5.16, predict loss = 1.50 (60.2 examples/sec; 0.066 sec/batch; 2h:22m:06s remains)
INFO - root - 2019-11-06 18:27:35.959211: step 21600, total loss = 3.70, predict loss = 1.11 (92.9 examples/sec; 0.043 sec/batch; 1h:32m:10s remains)
INFO - root - 2019-11-06 18:27:36.425478: step 21610, total loss = 3.74, predict loss = 1.11 (96.9 examples/sec; 0.041 sec/batch; 1h:28m:18s remains)
INFO - root - 2019-11-06 18:27:37.532867: step 21620, total loss = 4.51, predict loss = 1.16 (73.9 examples/sec; 0.054 sec/batch; 1h:55m:52s remains)
INFO - root - 2019-11-06 18:27:38.237011: step 21630, total loss = 4.00, predict loss = 1.09 (59.3 examples/sec; 0.067 sec/batch; 2h:24m:19s remains)
INFO - root - 2019-11-06 18:27:39.003637: step 21640, total loss = 5.23, predict loss = 1.43 (61.7 examples/sec; 0.065 sec/batch; 2h:18m:41s remains)
INFO - root - 2019-11-06 18:27:39.775887: step 21650, total loss = 3.44, predict loss = 1.02 (51.6 examples/sec; 0.077 sec/batch; 2h:45m:41s remains)
INFO - root - 2019-11-06 18:27:40.515965: step 21660, total loss = 6.05, predict loss = 1.72 (55.4 examples/sec; 0.072 sec/batch; 2h:34m:29s remains)
INFO - root - 2019-11-06 18:27:41.168440: step 21670, total loss = 4.68, predict loss = 1.46 (96.6 examples/sec; 0.041 sec/batch; 1h:28m:34s remains)
INFO - root - 2019-11-06 18:27:41.618581: step 21680, total loss = 6.00, predict loss = 1.68 (102.1 examples/sec; 0.039 sec/batch; 1h:23m:48s remains)
INFO - root - 2019-11-06 18:27:42.104933: step 21690, total loss = 4.77, predict loss = 1.44 (93.6 examples/sec; 0.043 sec/batch; 1h:31m:23s remains)
INFO - root - 2019-11-06 18:27:43.400811: step 21700, total loss = 5.57, predict loss = 1.58 (62.4 examples/sec; 0.064 sec/batch; 2h:17m:02s remains)
INFO - root - 2019-11-06 18:27:44.129178: step 21710, total loss = 3.10, predict loss = 0.92 (57.0 examples/sec; 0.070 sec/batch; 2h:30m:00s remains)
INFO - root - 2019-11-06 18:27:44.843495: step 21720, total loss = 4.84, predict loss = 1.37 (69.4 examples/sec; 0.058 sec/batch; 2h:03m:11s remains)
INFO - root - 2019-11-06 18:27:45.631727: step 21730, total loss = 4.05, predict loss = 1.17 (55.2 examples/sec; 0.072 sec/batch; 2h:34m:55s remains)
INFO - root - 2019-11-06 18:27:46.315452: step 21740, total loss = 4.43, predict loss = 1.33 (71.9 examples/sec; 0.056 sec/batch; 1h:58m:54s remains)
INFO - root - 2019-11-06 18:27:46.811595: step 21750, total loss = 4.90, predict loss = 1.38 (96.9 examples/sec; 0.041 sec/batch; 1h:28m:14s remains)
INFO - root - 2019-11-06 18:27:47.280661: step 21760, total loss = 4.83, predict loss = 1.31 (91.3 examples/sec; 0.044 sec/batch; 1h:33m:38s remains)
INFO - root - 2019-11-06 18:27:48.531236: step 21770, total loss = 3.88, predict loss = 1.18 (64.8 examples/sec; 0.062 sec/batch; 2h:11m:56s remains)
INFO - root - 2019-11-06 18:27:49.248665: step 21780, total loss = 5.72, predict loss = 1.61 (58.4 examples/sec; 0.068 sec/batch; 2h:26m:22s remains)
INFO - root - 2019-11-06 18:27:49.967742: step 21790, total loss = 4.71, predict loss = 1.34 (62.7 examples/sec; 0.064 sec/batch; 2h:16m:21s remains)
INFO - root - 2019-11-06 18:27:50.710747: step 21800, total loss = 4.88, predict loss = 1.41 (61.6 examples/sec; 0.065 sec/batch; 2h:18m:40s remains)
INFO - root - 2019-11-06 18:27:51.448646: step 21810, total loss = 4.60, predict loss = 1.27 (58.4 examples/sec; 0.069 sec/batch; 2h:26m:25s remains)
INFO - root - 2019-11-06 18:27:52.028642: step 21820, total loss = 4.28, predict loss = 1.28 (77.3 examples/sec; 0.052 sec/batch; 1h:50m:29s remains)
INFO - root - 2019-11-06 18:27:52.487129: step 21830, total loss = 3.66, predict loss = 1.07 (97.6 examples/sec; 0.041 sec/batch; 1h:27m:31s remains)
INFO - root - 2019-11-06 18:27:52.938757: step 21840, total loss = 5.09, predict loss = 1.41 (94.3 examples/sec; 0.042 sec/batch; 1h:30m:34s remains)
INFO - root - 2019-11-06 18:27:54.277972: step 21850, total loss = 3.90, predict loss = 1.17 (62.4 examples/sec; 0.064 sec/batch; 2h:16m:51s remains)
INFO - root - 2019-11-06 18:27:54.981698: step 21860, total loss = 2.55, predict loss = 0.76 (59.0 examples/sec; 0.068 sec/batch; 2h:24m:40s remains)
INFO - root - 2019-11-06 18:27:55.703990: step 21870, total loss = 5.10, predict loss = 1.40 (59.2 examples/sec; 0.068 sec/batch; 2h:24m:13s remains)
INFO - root - 2019-11-06 18:27:56.470418: step 21880, total loss = 5.92, predict loss = 1.73 (59.0 examples/sec; 0.068 sec/batch; 2h:24m:39s remains)
INFO - root - 2019-11-06 18:27:57.150151: step 21890, total loss = 3.73, predict loss = 1.06 (73.9 examples/sec; 0.054 sec/batch; 1h:55m:29s remains)
INFO - root - 2019-11-06 18:27:57.647650: step 21900, total loss = 4.44, predict loss = 1.37 (93.6 examples/sec; 0.043 sec/batch; 1h:31m:11s remains)
INFO - root - 2019-11-06 18:27:58.105790: step 21910, total loss = 5.66, predict loss = 1.71 (98.7 examples/sec; 0.041 sec/batch; 1h:26m:29s remains)
INFO - root - 2019-11-06 18:27:59.325641: step 21920, total loss = 4.05, predict loss = 1.19 (58.5 examples/sec; 0.068 sec/batch; 2h:25m:52s remains)
INFO - root - 2019-11-06 18:28:00.104177: step 21930, total loss = 4.38, predict loss = 1.26 (48.9 examples/sec; 0.082 sec/batch; 2h:54m:29s remains)
INFO - root - 2019-11-06 18:28:00.866076: step 21940, total loss = 4.29, predict loss = 1.33 (56.4 examples/sec; 0.071 sec/batch; 2h:31m:27s remains)
INFO - root - 2019-11-06 18:28:01.636770: step 21950, total loss = 5.78, predict loss = 1.70 (52.4 examples/sec; 0.076 sec/batch; 2h:42m:59s remains)
INFO - root - 2019-11-06 18:28:02.419809: step 21960, total loss = 4.60, predict loss = 1.38 (53.8 examples/sec; 0.074 sec/batch; 2h:38m:43s remains)
INFO - root - 2019-11-06 18:28:03.053318: step 21970, total loss = 4.07, predict loss = 1.17 (95.7 examples/sec; 0.042 sec/batch; 1h:29m:13s remains)
INFO - root - 2019-11-06 18:28:03.505797: step 21980, total loss = 4.50, predict loss = 1.39 (97.0 examples/sec; 0.041 sec/batch; 1h:27m:57s remains)
INFO - root - 2019-11-06 18:28:03.942800: step 21990, total loss = 5.66, predict loss = 1.59 (108.7 examples/sec; 0.037 sec/batch; 1h:18m:31s remains)
INFO - root - 2019-11-06 18:28:05.295142: step 22000, total loss = 5.03, predict loss = 1.45 (65.3 examples/sec; 0.061 sec/batch; 2h:10m:38s remains)
INFO - root - 2019-11-06 18:28:06.007752: step 22010, total loss = 5.72, predict loss = 1.65 (60.3 examples/sec; 0.066 sec/batch; 2h:21m:37s remains)
INFO - root - 2019-11-06 18:28:06.767610: step 22020, total loss = 5.32, predict loss = 1.52 (61.2 examples/sec; 0.065 sec/batch; 2h:19m:20s remains)
INFO - root - 2019-11-06 18:28:07.477699: step 22030, total loss = 5.97, predict loss = 1.68 (58.0 examples/sec; 0.069 sec/batch; 2h:26m:59s remains)
INFO - root - 2019-11-06 18:28:08.152047: step 22040, total loss = 3.33, predict loss = 0.98 (75.2 examples/sec; 0.053 sec/batch; 1h:53m:27s remains)
INFO - root - 2019-11-06 18:28:08.646242: step 22050, total loss = 3.88, predict loss = 1.17 (102.0 examples/sec; 0.039 sec/batch; 1h:23m:38s remains)
INFO - root - 2019-11-06 18:28:09.090314: step 22060, total loss = 5.05, predict loss = 1.49 (96.7 examples/sec; 0.041 sec/batch; 1h:28m:11s remains)
INFO - root - 2019-11-06 18:28:10.312641: step 22070, total loss = 4.90, predict loss = 1.41 (61.2 examples/sec; 0.065 sec/batch; 2h:19m:15s remains)
INFO - root - 2019-11-06 18:28:11.052495: step 22080, total loss = 4.64, predict loss = 1.31 (60.0 examples/sec; 0.067 sec/batch; 2h:22m:01s remains)
INFO - root - 2019-11-06 18:28:11.771678: step 22090, total loss = 4.44, predict loss = 1.32 (68.7 examples/sec; 0.058 sec/batch; 2h:04m:05s remains)
INFO - root - 2019-11-06 18:28:12.466654: step 22100, total loss = 4.76, predict loss = 1.36 (55.7 examples/sec; 0.072 sec/batch; 2h:33m:06s remains)
INFO - root - 2019-11-06 18:28:13.164408: step 22110, total loss = 5.43, predict loss = 1.46 (76.9 examples/sec; 0.052 sec/batch; 1h:50m:50s remains)
INFO - root - 2019-11-06 18:28:13.714585: step 22120, total loss = 4.92, predict loss = 1.43 (100.2 examples/sec; 0.040 sec/batch; 1h:25m:03s remains)
INFO - root - 2019-11-06 18:28:14.200896: step 22130, total loss = 3.42, predict loss = 1.01 (91.6 examples/sec; 0.044 sec/batch; 1h:33m:06s remains)
INFO - root - 2019-11-06 18:28:15.330765: step 22140, total loss = 4.23, predict loss = 1.17 (5.5 examples/sec; 0.729 sec/batch; 25h:54m:21s remains)
INFO - root - 2019-11-06 18:28:16.032194: step 22150, total loss = 4.03, predict loss = 1.18 (56.5 examples/sec; 0.071 sec/batch; 2h:30m:57s remains)
INFO - root - 2019-11-06 18:28:16.816792: step 22160, total loss = 5.56, predict loss = 1.60 (50.6 examples/sec; 0.079 sec/batch; 2h:48m:16s remains)
INFO - root - 2019-11-06 18:28:17.581888: step 22170, total loss = 5.40, predict loss = 1.60 (62.6 examples/sec; 0.064 sec/batch; 2h:16m:05s remains)
INFO - root - 2019-11-06 18:28:18.342831: step 22180, total loss = 5.06, predict loss = 1.50 (57.0 examples/sec; 0.070 sec/batch; 2h:29m:28s remains)
INFO - root - 2019-11-06 18:28:19.000026: step 22190, total loss = 5.69, predict loss = 1.63 (88.2 examples/sec; 0.045 sec/batch; 1h:36m:37s remains)
INFO - root - 2019-11-06 18:28:19.450375: step 22200, total loss = 5.68, predict loss = 1.59 (96.4 examples/sec; 0.041 sec/batch; 1h:28m:23s remains)
INFO - root - 2019-11-06 18:28:19.931794: step 22210, total loss = 6.19, predict loss = 1.76 (96.6 examples/sec; 0.041 sec/batch; 1h:28m:08s remains)
INFO - root - 2019-11-06 18:28:21.127379: step 22220, total loss = 6.05, predict loss = 1.73 (60.8 examples/sec; 0.066 sec/batch; 2h:20m:01s remains)
INFO - root - 2019-11-06 18:28:21.889690: step 22230, total loss = 6.78, predict loss = 1.95 (46.5 examples/sec; 0.086 sec/batch; 3h:03m:20s remains)
INFO - root - 2019-11-06 18:28:22.622558: step 22240, total loss = 3.81, predict loss = 1.10 (58.2 examples/sec; 0.069 sec/batch; 2h:26m:19s remains)
INFO - root - 2019-11-06 18:28:23.403430: step 22250, total loss = 4.31, predict loss = 1.26 (54.8 examples/sec; 0.073 sec/batch; 2h:35m:30s remains)
INFO - root - 2019-11-06 18:28:24.090010: step 22260, total loss = 5.64, predict loss = 1.60 (75.9 examples/sec; 0.053 sec/batch; 1h:52m:11s remains)
INFO - root - 2019-11-06 18:28:24.613394: step 22270, total loss = 6.17, predict loss = 1.79 (79.3 examples/sec; 0.050 sec/batch; 1h:47m:25s remains)
INFO - root - 2019-11-06 18:28:25.079529: step 22280, total loss = 4.32, predict loss = 1.22 (90.4 examples/sec; 0.044 sec/batch; 1h:34m:10s remains)
INFO - root - 2019-11-06 18:28:26.251918: step 22290, total loss = 5.20, predict loss = 1.57 (68.9 examples/sec; 0.058 sec/batch; 2h:03m:30s remains)
INFO - root - 2019-11-06 18:28:26.941986: step 22300, total loss = 4.82, predict loss = 1.33 (53.0 examples/sec; 0.075 sec/batch; 2h:40m:35s remains)
INFO - root - 2019-11-06 18:28:27.687973: step 22310, total loss = 4.90, predict loss = 1.40 (58.6 examples/sec; 0.068 sec/batch; 2h:25m:22s remains)
INFO - root - 2019-11-06 18:28:28.391724: step 22320, total loss = 4.31, predict loss = 1.30 (65.9 examples/sec; 0.061 sec/batch; 2h:09m:09s remains)
INFO - root - 2019-11-06 18:28:29.189480: step 22330, total loss = 4.76, predict loss = 1.49 (52.7 examples/sec; 0.076 sec/batch; 2h:41m:25s remains)
INFO - root - 2019-11-06 18:28:29.862692: step 22340, total loss = 5.94, predict loss = 1.70 (89.8 examples/sec; 0.045 sec/batch; 1h:34m:45s remains)
INFO - root - 2019-11-06 18:28:30.327009: step 22350, total loss = 5.90, predict loss = 1.67 (94.7 examples/sec; 0.042 sec/batch; 1h:29m:50s remains)
INFO - root - 2019-11-06 18:28:30.788155: step 22360, total loss = 5.21, predict loss = 1.47 (94.9 examples/sec; 0.042 sec/batch; 1h:29m:40s remains)
INFO - root - 2019-11-06 18:28:32.070213: step 22370, total loss = 5.59, predict loss = 1.64 (53.6 examples/sec; 0.075 sec/batch; 2h:38m:40s remains)
INFO - root - 2019-11-06 18:28:32.801145: step 22380, total loss = 2.99, predict loss = 0.89 (61.5 examples/sec; 0.065 sec/batch; 2h:18m:21s remains)
INFO - root - 2019-11-06 18:28:33.534736: step 22390, total loss = 6.51, predict loss = 1.84 (46.8 examples/sec; 0.085 sec/batch; 3h:01m:45s remains)
INFO - root - 2019-11-06 18:28:34.326474: step 22400, total loss = 5.46, predict loss = 1.55 (58.0 examples/sec; 0.069 sec/batch; 2h:26m:40s remains)
INFO - root - 2019-11-06 18:28:35.083340: step 22410, total loss = 4.74, predict loss = 1.34 (65.2 examples/sec; 0.061 sec/batch; 2h:10m:23s remains)
INFO - root - 2019-11-06 18:28:35.619894: step 22420, total loss = 4.31, predict loss = 1.22 (96.7 examples/sec; 0.041 sec/batch; 1h:27m:56s remains)
INFO - root - 2019-11-06 18:28:36.055900: step 22430, total loss = 4.37, predict loss = 1.18 (100.5 examples/sec; 0.040 sec/batch; 1h:24m:38s remains)
INFO - root - 2019-11-06 18:28:37.216690: step 22440, total loss = 5.52, predict loss = 1.66 (67.5 examples/sec; 0.059 sec/batch; 2h:06m:02s remains)
INFO - root - 2019-11-06 18:28:37.919228: step 22450, total loss = 6.38, predict loss = 1.83 (60.0 examples/sec; 0.067 sec/batch; 2h:21m:49s remains)
INFO - root - 2019-11-06 18:28:38.699903: step 22460, total loss = 4.30, predict loss = 1.31 (57.5 examples/sec; 0.070 sec/batch; 2h:27m:57s remains)
INFO - root - 2019-11-06 18:28:39.483091: step 22470, total loss = 6.62, predict loss = 1.90 (55.2 examples/sec; 0.072 sec/batch; 2h:33m:55s remains)
INFO - root - 2019-11-06 18:28:40.305752: step 22480, total loss = 5.40, predict loss = 1.61 (55.2 examples/sec; 0.072 sec/batch; 2h:33m:55s remains)
INFO - root - 2019-11-06 18:28:40.970868: step 22490, total loss = 5.02, predict loss = 1.38 (90.1 examples/sec; 0.044 sec/batch; 1h:34m:19s remains)
INFO - root - 2019-11-06 18:28:41.420973: step 22500, total loss = 5.30, predict loss = 1.56 (96.9 examples/sec; 0.041 sec/batch; 1h:27m:42s remains)
INFO - root - 2019-11-06 18:28:41.876763: step 22510, total loss = 4.21, predict loss = 1.29 (100.3 examples/sec; 0.040 sec/batch; 1h:24m:46s remains)
INFO - root - 2019-11-06 18:28:43.158957: step 22520, total loss = 3.76, predict loss = 1.04 (64.7 examples/sec; 0.062 sec/batch; 2h:11m:15s remains)
INFO - root - 2019-11-06 18:28:43.862834: step 22530, total loss = 5.75, predict loss = 1.65 (61.5 examples/sec; 0.065 sec/batch; 2h:18m:08s remains)
INFO - root - 2019-11-06 18:28:44.589457: step 22540, total loss = 4.66, predict loss = 1.39 (67.0 examples/sec; 0.060 sec/batch; 2h:06m:46s remains)
INFO - root - 2019-11-06 18:28:45.323628: step 22550, total loss = 5.65, predict loss = 1.55 (58.3 examples/sec; 0.069 sec/batch; 2h:25m:47s remains)
INFO - root - 2019-11-06 18:28:45.981753: step 22560, total loss = 5.37, predict loss = 1.55 (67.2 examples/sec; 0.060 sec/batch; 2h:06m:23s remains)
INFO - root - 2019-11-06 18:28:46.524162: step 22570, total loss = 5.15, predict loss = 1.50 (85.5 examples/sec; 0.047 sec/batch; 1h:39m:21s remains)
INFO - root - 2019-11-06 18:28:46.994222: step 22580, total loss = 5.72, predict loss = 1.67 (93.5 examples/sec; 0.043 sec/batch; 1h:30m:48s remains)
INFO - root - 2019-11-06 18:28:48.129679: step 22590, total loss = 6.01, predict loss = 1.73 (65.6 examples/sec; 0.061 sec/batch; 2h:09m:26s remains)
INFO - root - 2019-11-06 18:28:48.830994: step 22600, total loss = 4.12, predict loss = 1.27 (56.0 examples/sec; 0.071 sec/batch; 2h:31m:37s remains)
INFO - root - 2019-11-06 18:28:49.649008: step 22610, total loss = 4.54, predict loss = 1.30 (59.8 examples/sec; 0.067 sec/batch; 2h:22m:01s remains)
INFO - root - 2019-11-06 18:28:50.407109: step 22620, total loss = 5.29, predict loss = 1.53 (58.6 examples/sec; 0.068 sec/batch; 2h:24m:48s remains)
INFO - root - 2019-11-06 18:28:51.200031: step 22630, total loss = 4.51, predict loss = 1.25 (56.9 examples/sec; 0.070 sec/batch; 2h:29m:21s remains)
INFO - root - 2019-11-06 18:28:51.838397: step 22640, total loss = 4.33, predict loss = 1.23 (87.7 examples/sec; 0.046 sec/batch; 1h:36m:46s remains)
INFO - root - 2019-11-06 18:28:52.344504: step 22650, total loss = 5.50, predict loss = 1.55 (101.7 examples/sec; 0.039 sec/batch; 1h:23m:31s remains)
INFO - root - 2019-11-06 18:28:52.794918: step 22660, total loss = 3.69, predict loss = 1.11 (95.4 examples/sec; 0.042 sec/batch; 1h:29m:00s remains)
INFO - root - 2019-11-06 18:28:54.130235: step 22670, total loss = 3.50, predict loss = 1.02 (65.8 examples/sec; 0.061 sec/batch; 2h:09m:04s remains)
INFO - root - 2019-11-06 18:28:54.886955: step 22680, total loss = 3.32, predict loss = 1.03 (55.5 examples/sec; 0.072 sec/batch; 2h:32m:57s remains)
INFO - root - 2019-11-06 18:28:55.606595: step 22690, total loss = 5.48, predict loss = 1.70 (70.4 examples/sec; 0.057 sec/batch; 2h:00m:34s remains)
INFO - root - 2019-11-06 18:28:56.323313: step 22700, total loss = 5.64, predict loss = 1.66 (60.6 examples/sec; 0.066 sec/batch; 2h:19m:57s remains)
INFO - root - 2019-11-06 18:28:57.083219: step 22710, total loss = 2.84, predict loss = 0.86 (63.6 examples/sec; 0.063 sec/batch; 2h:13m:29s remains)
INFO - root - 2019-11-06 18:28:57.575783: step 22720, total loss = 5.32, predict loss = 1.48 (104.7 examples/sec; 0.038 sec/batch; 1h:21m:04s remains)
INFO - root - 2019-11-06 18:28:58.059170: step 22730, total loss = 5.36, predict loss = 1.53 (101.0 examples/sec; 0.040 sec/batch; 1h:24m:01s remains)
INFO - root - 2019-11-06 18:28:59.262490: step 22740, total loss = 5.29, predict loss = 1.59 (60.6 examples/sec; 0.066 sec/batch; 2h:19m:55s remains)
INFO - root - 2019-11-06 18:28:59.956088: step 22750, total loss = 5.38, predict loss = 1.59 (64.0 examples/sec; 0.062 sec/batch; 2h:12m:28s remains)
INFO - root - 2019-11-06 18:29:00.662275: step 22760, total loss = 5.87, predict loss = 1.75 (61.9 examples/sec; 0.065 sec/batch; 2h:17m:05s remains)
INFO - root - 2019-11-06 18:29:01.402869: step 22770, total loss = 4.77, predict loss = 1.32 (61.0 examples/sec; 0.066 sec/batch; 2h:18m:58s remains)
INFO - root - 2019-11-06 18:29:02.162708: step 22780, total loss = 5.60, predict loss = 1.60 (57.4 examples/sec; 0.070 sec/batch; 2h:27m:39s remains)
INFO - root - 2019-11-06 18:29:02.745561: step 22790, total loss = 4.66, predict loss = 1.34 (99.9 examples/sec; 0.040 sec/batch; 1h:24m:52s remains)
INFO - root - 2019-11-06 18:29:03.202357: step 22800, total loss = 3.82, predict loss = 1.10 (97.9 examples/sec; 0.041 sec/batch; 1h:26m:35s remains)
INFO - root - 2019-11-06 18:29:03.666405: step 22810, total loss = 4.82, predict loss = 1.35 (137.9 examples/sec; 0.029 sec/batch; 1h:01m:29s remains)
INFO - root - 2019-11-06 18:29:05.057716: step 22820, total loss = 4.29, predict loss = 1.25 (48.7 examples/sec; 0.082 sec/batch; 2h:54m:01s remains)
INFO - root - 2019-11-06 18:29:05.803059: step 22830, total loss = 3.98, predict loss = 1.17 (63.2 examples/sec; 0.063 sec/batch; 2h:14m:05s remains)
INFO - root - 2019-11-06 18:29:06.557766: step 22840, total loss = 3.16, predict loss = 0.96 (53.7 examples/sec; 0.075 sec/batch; 2h:37m:54s remains)
INFO - root - 2019-11-06 18:29:07.331766: step 22850, total loss = 5.96, predict loss = 1.76 (51.6 examples/sec; 0.078 sec/batch; 2h:44m:17s remains)
INFO - root - 2019-11-06 18:29:08.089506: step 22860, total loss = 4.49, predict loss = 1.34 (61.2 examples/sec; 0.065 sec/batch; 2h:18m:29s remains)
INFO - root - 2019-11-06 18:29:08.581556: step 22870, total loss = 3.46, predict loss = 1.07 (86.2 examples/sec; 0.046 sec/batch; 1h:38m:20s remains)
INFO - root - 2019-11-06 18:29:09.026815: step 22880, total loss = 5.06, predict loss = 1.50 (95.8 examples/sec; 0.042 sec/batch; 1h:28m:25s remains)
INFO - root - 2019-11-06 18:29:10.259693: step 22890, total loss = 5.47, predict loss = 1.61 (65.0 examples/sec; 0.062 sec/batch; 2h:10m:18s remains)
INFO - root - 2019-11-06 18:29:11.039509: step 22900, total loss = 5.47, predict loss = 1.54 (55.4 examples/sec; 0.072 sec/batch; 2h:32m:56s remains)
INFO - root - 2019-11-06 18:29:11.819662: step 22910, total loss = 5.94, predict loss = 1.69 (53.8 examples/sec; 0.074 sec/batch; 2h:37m:36s remains)
INFO - root - 2019-11-06 18:29:12.577123: step 22920, total loss = 4.78, predict loss = 1.34 (57.6 examples/sec; 0.069 sec/batch; 2h:27m:01s remains)
INFO - root - 2019-11-06 18:29:13.325706: step 22930, total loss = 6.71, predict loss = 1.94 (63.1 examples/sec; 0.063 sec/batch; 2h:14m:18s remains)
INFO - root - 2019-11-06 18:29:13.907292: step 22940, total loss = 6.81, predict loss = 1.97 (95.0 examples/sec; 0.042 sec/batch; 1h:29m:10s remains)
INFO - root - 2019-11-06 18:29:14.369338: step 22950, total loss = 5.04, predict loss = 1.46 (95.3 examples/sec; 0.042 sec/batch; 1h:28m:54s remains)
INFO - root - 2019-11-06 18:29:15.492159: step 22960, total loss = 3.46, predict loss = 1.02 (5.6 examples/sec; 0.718 sec/batch; 25h:19m:51s remains)
INFO - root - 2019-11-06 18:29:16.184625: step 22970, total loss = 5.39, predict loss = 1.62 (56.9 examples/sec; 0.070 sec/batch; 2h:28m:52s remains)
INFO - root - 2019-11-06 18:29:16.941000: step 22980, total loss = 5.03, predict loss = 1.50 (56.4 examples/sec; 0.071 sec/batch; 2h:30m:11s remains)
INFO - root - 2019-11-06 18:29:17.630848: step 22990, total loss = 4.75, predict loss = 1.40 (71.4 examples/sec; 0.056 sec/batch; 1h:58m:37s remains)
INFO - root - 2019-11-06 18:29:18.343868: step 23000, total loss = 5.93, predict loss = 1.67 (54.8 examples/sec; 0.073 sec/batch; 2h:34m:27s remains)
INFO - root - 2019-11-06 18:29:19.034782: step 23010, total loss = 5.83, predict loss = 1.61 (87.7 examples/sec; 0.046 sec/batch; 1h:36m:29s remains)
INFO - root - 2019-11-06 18:29:19.498904: step 23020, total loss = 4.41, predict loss = 1.28 (96.9 examples/sec; 0.041 sec/batch; 1h:27m:20s remains)
INFO - root - 2019-11-06 18:29:19.960753: step 23030, total loss = 5.64, predict loss = 1.62 (99.6 examples/sec; 0.040 sec/batch; 1h:24m:58s remains)
INFO - root - 2019-11-06 18:29:21.236317: step 23040, total loss = 6.18, predict loss = 1.78 (66.5 examples/sec; 0.060 sec/batch; 2h:07m:20s remains)
INFO - root - 2019-11-06 18:29:22.041917: step 23050, total loss = 4.93, predict loss = 1.45 (56.0 examples/sec; 0.071 sec/batch; 2h:31m:08s remains)
INFO - root - 2019-11-06 18:29:22.794457: step 23060, total loss = 5.45, predict loss = 1.52 (55.4 examples/sec; 0.072 sec/batch; 2h:32m:45s remains)
INFO - root - 2019-11-06 18:29:23.536557: step 23070, total loss = 5.24, predict loss = 1.45 (56.1 examples/sec; 0.071 sec/batch; 2h:30m:50s remains)
INFO - root - 2019-11-06 18:29:24.271484: step 23080, total loss = 5.80, predict loss = 1.69 (55.1 examples/sec; 0.073 sec/batch; 2h:33m:26s remains)
INFO - root - 2019-11-06 18:29:24.851722: step 23090, total loss = 4.69, predict loss = 1.30 (96.8 examples/sec; 0.041 sec/batch; 1h:27m:22s remains)
INFO - root - 2019-11-06 18:29:25.307170: step 23100, total loss = 5.47, predict loss = 1.58 (98.0 examples/sec; 0.041 sec/batch; 1h:26m:17s remains)
INFO - root - 2019-11-06 18:29:26.432570: step 23110, total loss = 3.92, predict loss = 1.13 (70.3 examples/sec; 0.057 sec/batch; 2h:00m:16s remains)
INFO - root - 2019-11-06 18:29:27.138379: step 23120, total loss = 3.72, predict loss = 1.06 (58.1 examples/sec; 0.069 sec/batch; 2h:25m:28s remains)
INFO - root - 2019-11-06 18:29:27.890973: step 23130, total loss = 6.19, predict loss = 1.79 (65.8 examples/sec; 0.061 sec/batch; 2h:08m:28s remains)
INFO - root - 2019-11-06 18:29:28.657868: step 23140, total loss = 5.60, predict loss = 1.50 (54.2 examples/sec; 0.074 sec/batch; 2h:36m:10s remains)
INFO - root - 2019-11-06 18:29:29.406833: step 23150, total loss = 6.49, predict loss = 1.78 (63.1 examples/sec; 0.063 sec/batch; 2h:14m:01s remains)
INFO - root - 2019-11-06 18:29:30.089174: step 23160, total loss = 3.93, predict loss = 1.10 (86.3 examples/sec; 0.046 sec/batch; 1h:37m:56s remains)
INFO - root - 2019-11-06 18:29:30.564275: step 23170, total loss = 6.37, predict loss = 1.84 (99.2 examples/sec; 0.040 sec/batch; 1h:25m:16s remains)
INFO - root - 2019-11-06 18:29:31.027037: step 23180, total loss = 4.89, predict loss = 1.42 (94.0 examples/sec; 0.043 sec/batch; 1h:29m:55s remains)
INFO - root - 2019-11-06 18:29:32.272044: step 23190, total loss = 5.08, predict loss = 1.50 (62.1 examples/sec; 0.064 sec/batch; 2h:16m:03s remains)
INFO - root - 2019-11-06 18:29:33.013384: step 23200, total loss = 4.08, predict loss = 1.19 (60.9 examples/sec; 0.066 sec/batch; 2h:18m:48s remains)
INFO - root - 2019-11-06 18:29:33.811640: step 23210, total loss = 4.67, predict loss = 1.37 (55.9 examples/sec; 0.072 sec/batch; 2h:31m:15s remains)
INFO - root - 2019-11-06 18:29:34.637978: step 23220, total loss = 5.55, predict loss = 1.51 (53.5 examples/sec; 0.075 sec/batch; 2h:38m:00s remains)
INFO - root - 2019-11-06 18:29:35.335955: step 23230, total loss = 5.49, predict loss = 1.57 (68.7 examples/sec; 0.058 sec/batch; 2h:03m:03s remains)
INFO - root - 2019-11-06 18:29:35.824845: step 23240, total loss = 5.07, predict loss = 1.46 (99.9 examples/sec; 0.040 sec/batch; 1h:24m:36s remains)
INFO - root - 2019-11-06 18:29:36.309582: step 23250, total loss = 4.38, predict loss = 1.23 (92.1 examples/sec; 0.043 sec/batch; 1h:31m:42s remains)
INFO - root - 2019-11-06 18:29:37.464703: step 23260, total loss = 4.62, predict loss = 1.45 (66.4 examples/sec; 0.060 sec/batch; 2h:07m:12s remains)
INFO - root - 2019-11-06 18:29:38.247241: step 23270, total loss = 3.92, predict loss = 1.11 (61.1 examples/sec; 0.065 sec/batch; 2h:18m:16s remains)
INFO - root - 2019-11-06 18:29:39.007493: step 23280, total loss = 6.12, predict loss = 1.68 (56.2 examples/sec; 0.071 sec/batch; 2h:30m:23s remains)
INFO - root - 2019-11-06 18:29:39.753052: step 23290, total loss = 5.73, predict loss = 1.71 (56.9 examples/sec; 0.070 sec/batch; 2h:28m:32s remains)
INFO - root - 2019-11-06 18:29:40.473834: step 23300, total loss = 3.50, predict loss = 1.03 (60.5 examples/sec; 0.066 sec/batch; 2h:19m:31s remains)
INFO - root - 2019-11-06 18:29:41.077859: step 23310, total loss = 4.98, predict loss = 1.43 (102.0 examples/sec; 0.039 sec/batch; 1h:22m:46s remains)
INFO - root - 2019-11-06 18:29:41.529883: step 23320, total loss = 3.06, predict loss = 0.81 (104.6 examples/sec; 0.038 sec/batch; 1h:20m:45s remains)
INFO - root - 2019-11-06 18:29:42.010367: step 23330, total loss = 4.42, predict loss = 1.35 (94.1 examples/sec; 0.043 sec/batch; 1h:29m:47s remains)
INFO - root - 2019-11-06 18:29:43.311602: step 23340, total loss = 4.98, predict loss = 1.43 (57.4 examples/sec; 0.070 sec/batch; 2h:26m:59s remains)
INFO - root - 2019-11-06 18:29:44.082832: step 23350, total loss = 4.33, predict loss = 1.23 (55.7 examples/sec; 0.072 sec/batch; 2h:31m:38s remains)
INFO - root - 2019-11-06 18:29:44.843921: step 23360, total loss = 5.90, predict loss = 1.67 (61.2 examples/sec; 0.065 sec/batch; 2h:17m:53s remains)
INFO - root - 2019-11-06 18:29:45.574647: step 23370, total loss = 4.07, predict loss = 1.11 (56.7 examples/sec; 0.071 sec/batch; 2h:28m:53s remains)
INFO - root - 2019-11-06 18:29:46.299232: step 23380, total loss = 4.51, predict loss = 1.24 (64.8 examples/sec; 0.062 sec/batch; 2h:10m:21s remains)
INFO - root - 2019-11-06 18:29:46.817428: step 23390, total loss = 5.12, predict loss = 1.48 (104.6 examples/sec; 0.038 sec/batch; 1h:20m:41s remains)
INFO - root - 2019-11-06 18:29:47.251602: step 23400, total loss = 4.48, predict loss = 1.22 (96.1 examples/sec; 0.042 sec/batch; 1h:27m:47s remains)
INFO - root - 2019-11-06 18:29:48.437189: step 23410, total loss = 3.50, predict loss = 1.00 (70.1 examples/sec; 0.057 sec/batch; 2h:00m:24s remains)
INFO - root - 2019-11-06 18:29:49.213159: step 23420, total loss = 2.98, predict loss = 0.83 (56.3 examples/sec; 0.071 sec/batch; 2h:29m:46s remains)
INFO - root - 2019-11-06 18:29:49.941062: step 23430, total loss = 4.59, predict loss = 1.39 (63.0 examples/sec; 0.063 sec/batch; 2h:13m:56s remains)
INFO - root - 2019-11-06 18:29:50.695297: step 23440, total loss = 5.85, predict loss = 1.70 (60.4 examples/sec; 0.066 sec/batch; 2h:19m:40s remains)
INFO - root - 2019-11-06 18:29:51.462141: step 23450, total loss = 5.25, predict loss = 1.54 (64.8 examples/sec; 0.062 sec/batch; 2h:10m:07s remains)
INFO - root - 2019-11-06 18:29:52.113654: step 23460, total loss = 2.62, predict loss = 0.80 (98.5 examples/sec; 0.041 sec/batch; 1h:25m:40s remains)
INFO - root - 2019-11-06 18:29:52.566892: step 23470, total loss = 3.69, predict loss = 1.07 (97.6 examples/sec; 0.041 sec/batch; 1h:26m:27s remains)
INFO - root - 2019-11-06 18:29:53.032275: step 23480, total loss = 2.86, predict loss = 0.90 (92.9 examples/sec; 0.043 sec/batch; 1h:30m:48s remains)
INFO - root - 2019-11-06 18:29:54.383911: step 23490, total loss = 5.97, predict loss = 1.66 (61.7 examples/sec; 0.065 sec/batch; 2h:16m:39s remains)
INFO - root - 2019-11-06 18:29:55.092390: step 23500, total loss = 5.12, predict loss = 1.50 (59.9 examples/sec; 0.067 sec/batch; 2h:20m:49s remains)
INFO - root - 2019-11-06 18:29:55.804198: step 23510, total loss = 5.34, predict loss = 1.58 (65.7 examples/sec; 0.061 sec/batch; 2h:08m:23s remains)
INFO - root - 2019-11-06 18:29:56.524059: step 23520, total loss = 4.96, predict loss = 1.45 (60.9 examples/sec; 0.066 sec/batch; 2h:18m:27s remains)
INFO - root - 2019-11-06 18:29:57.196796: step 23530, total loss = 4.18, predict loss = 1.17 (78.3 examples/sec; 0.051 sec/batch; 1h:47m:39s remains)
INFO - root - 2019-11-06 18:29:57.632578: step 23540, total loss = 4.73, predict loss = 1.36 (99.2 examples/sec; 0.040 sec/batch; 1h:24m:57s remains)
INFO - root - 2019-11-06 18:29:58.085367: step 23550, total loss = 4.39, predict loss = 1.25 (86.7 examples/sec; 0.046 sec/batch; 1h:37m:13s remains)
INFO - root - 2019-11-06 18:29:59.299708: step 23560, total loss = 4.02, predict loss = 1.22 (71.6 examples/sec; 0.056 sec/batch; 1h:57m:45s remains)
INFO - root - 2019-11-06 18:30:00.022464: step 23570, total loss = 4.76, predict loss = 1.44 (68.9 examples/sec; 0.058 sec/batch; 2h:02m:23s remains)
INFO - root - 2019-11-06 18:30:00.745744: step 23580, total loss = 3.43, predict loss = 1.09 (64.1 examples/sec; 0.062 sec/batch; 2h:11m:29s remains)
INFO - root - 2019-11-06 18:30:01.483119: step 23590, total loss = 4.30, predict loss = 1.28 (60.0 examples/sec; 0.067 sec/batch; 2h:20m:29s remains)
INFO - root - 2019-11-06 18:30:02.201713: step 23600, total loss = 5.38, predict loss = 1.59 (63.6 examples/sec; 0.063 sec/batch; 2h:12m:35s remains)
INFO - root - 2019-11-06 18:30:02.767049: step 23610, total loss = 4.27, predict loss = 1.27 (90.6 examples/sec; 0.044 sec/batch; 1h:32m:59s remains)
INFO - root - 2019-11-06 18:30:03.234799: step 23620, total loss = 4.94, predict loss = 1.38 (92.7 examples/sec; 0.043 sec/batch; 1h:30m:53s remains)
INFO - root - 2019-11-06 18:30:03.684865: step 23630, total loss = 6.46, predict loss = 1.74 (101.6 examples/sec; 0.039 sec/batch; 1h:22m:53s remains)
INFO - root - 2019-11-06 18:30:05.107418: step 23640, total loss = 6.84, predict loss = 2.02 (49.1 examples/sec; 0.082 sec/batch; 2h:51m:40s remains)
INFO - root - 2019-11-06 18:30:05.881890: step 23650, total loss = 4.82, predict loss = 1.41 (60.9 examples/sec; 0.066 sec/batch; 2h:18m:22s remains)
INFO - root - 2019-11-06 18:30:06.700297: step 23660, total loss = 4.42, predict loss = 1.22 (53.3 examples/sec; 0.075 sec/batch; 2h:38m:09s remains)
INFO - root - 2019-11-06 18:30:07.392180: step 23670, total loss = 4.80, predict loss = 1.30 (72.1 examples/sec; 0.055 sec/batch; 1h:56m:46s remains)
INFO - root - 2019-11-06 18:30:07.999782: step 23680, total loss = 4.80, predict loss = 1.43 (77.7 examples/sec; 0.051 sec/batch; 1h:48m:19s remains)
INFO - root - 2019-11-06 18:30:08.480485: step 23690, total loss = 5.58, predict loss = 1.57 (97.1 examples/sec; 0.041 sec/batch; 1h:26m:42s remains)
INFO - root - 2019-11-06 18:30:08.939810: step 23700, total loss = 4.90, predict loss = 1.41 (91.7 examples/sec; 0.044 sec/batch; 1h:31m:48s remains)
INFO - root - 2019-11-06 18:30:10.128751: step 23710, total loss = 5.15, predict loss = 1.44 (60.7 examples/sec; 0.066 sec/batch; 2h:18m:43s remains)
INFO - root - 2019-11-06 18:30:10.896116: step 23720, total loss = 5.30, predict loss = 1.53 (48.3 examples/sec; 0.083 sec/batch; 2h:54m:18s remains)
INFO - root - 2019-11-06 18:30:11.662182: step 23730, total loss = 5.98, predict loss = 1.72 (52.9 examples/sec; 0.076 sec/batch; 2h:39m:01s remains)
INFO - root - 2019-11-06 18:30:12.392674: step 23740, total loss = 4.39, predict loss = 1.26 (59.1 examples/sec; 0.068 sec/batch; 2h:22m:25s remains)
INFO - root - 2019-11-06 18:30:13.151223: step 23750, total loss = 3.93, predict loss = 1.12 (65.1 examples/sec; 0.061 sec/batch; 2h:09m:12s remains)
INFO - root - 2019-11-06 18:30:13.708427: step 23760, total loss = 4.17, predict loss = 1.18 (96.0 examples/sec; 0.042 sec/batch; 1h:27m:41s remains)
INFO - root - 2019-11-06 18:30:14.189843: step 23770, total loss = 4.49, predict loss = 1.33 (99.8 examples/sec; 0.040 sec/batch; 1h:24m:19s remains)
INFO - root - 2019-11-06 18:30:15.292439: step 23780, total loss = 4.42, predict loss = 1.28 (5.7 examples/sec; 0.701 sec/batch; 24h:35m:17s remains)
INFO - root - 2019-11-06 18:30:15.955903: step 23790, total loss = 5.84, predict loss = 1.71 (56.3 examples/sec; 0.071 sec/batch; 2h:29m:25s remains)
INFO - root - 2019-11-06 18:30:16.788238: step 23800, total loss = 4.49, predict loss = 1.27 (55.4 examples/sec; 0.072 sec/batch; 2h:31m:47s remains)
INFO - root - 2019-11-06 18:30:17.538673: step 23810, total loss = 5.40, predict loss = 1.67 (64.0 examples/sec; 0.062 sec/batch; 2h:11m:24s remains)
INFO - root - 2019-11-06 18:30:18.255470: step 23820, total loss = 5.20, predict loss = 1.46 (57.3 examples/sec; 0.070 sec/batch; 2h:26m:50s remains)
INFO - root - 2019-11-06 18:30:18.927508: step 23830, total loss = 5.79, predict loss = 1.64 (85.6 examples/sec; 0.047 sec/batch; 1h:38m:12s remains)
INFO - root - 2019-11-06 18:30:19.406402: step 23840, total loss = 5.51, predict loss = 1.54 (99.6 examples/sec; 0.040 sec/batch; 1h:24m:27s remains)
INFO - root - 2019-11-06 18:30:19.895462: step 23850, total loss = 5.88, predict loss = 1.76 (90.5 examples/sec; 0.044 sec/batch; 1h:32m:52s remains)
INFO - root - 2019-11-06 18:30:21.160687: step 23860, total loss = 5.14, predict loss = 1.49 (62.3 examples/sec; 0.064 sec/batch; 2h:14m:55s remains)
INFO - root - 2019-11-06 18:30:21.971152: step 23870, total loss = 3.56, predict loss = 1.04 (52.3 examples/sec; 0.077 sec/batch; 2h:40m:53s remains)
INFO - root - 2019-11-06 18:30:22.735295: step 23880, total loss = 6.70, predict loss = 1.91 (61.8 examples/sec; 0.065 sec/batch; 2h:15m:59s remains)
INFO - root - 2019-11-06 18:30:23.505568: step 23890, total loss = 5.89, predict loss = 1.70 (61.7 examples/sec; 0.065 sec/batch; 2h:16m:11s remains)
INFO - root - 2019-11-06 18:30:24.268645: step 23900, total loss = 5.31, predict loss = 1.50 (60.9 examples/sec; 0.066 sec/batch; 2h:18m:00s remains)
INFO - root - 2019-11-06 18:30:24.809800: step 23910, total loss = 6.11, predict loss = 1.73 (94.9 examples/sec; 0.042 sec/batch; 1h:28m:36s remains)
INFO - root - 2019-11-06 18:30:25.267096: step 23920, total loss = 4.80, predict loss = 1.49 (91.9 examples/sec; 0.044 sec/batch; 1h:31m:30s remains)
INFO - root - 2019-11-06 18:30:26.425017: step 23930, total loss = 5.65, predict loss = 1.70 (72.5 examples/sec; 0.055 sec/batch; 1h:55m:59s remains)
INFO - root - 2019-11-06 18:30:27.113372: step 23940, total loss = 3.59, predict loss = 1.03 (62.4 examples/sec; 0.064 sec/batch; 2h:14m:43s remains)
INFO - root - 2019-11-06 18:30:27.886528: step 23950, total loss = 4.66, predict loss = 1.38 (56.4 examples/sec; 0.071 sec/batch; 2h:29m:02s remains)
INFO - root - 2019-11-06 18:30:28.606750: step 23960, total loss = 4.44, predict loss = 1.26 (66.5 examples/sec; 0.060 sec/batch; 2h:06m:23s remains)
INFO - root - 2019-11-06 18:30:29.358178: step 23970, total loss = 5.94, predict loss = 1.67 (55.3 examples/sec; 0.072 sec/batch; 2h:31m:55s remains)
INFO - root - 2019-11-06 18:30:29.979959: step 23980, total loss = 4.83, predict loss = 1.36 (95.5 examples/sec; 0.042 sec/batch; 1h:27m:57s remains)
INFO - root - 2019-11-06 18:30:30.416069: step 23990, total loss = 3.65, predict loss = 1.03 (97.2 examples/sec; 0.041 sec/batch; 1h:26m:27s remains)
INFO - root - 2019-11-06 18:30:30.875683: step 24000, total loss = 4.10, predict loss = 1.14 (92.7 examples/sec; 0.043 sec/batch; 1h:30m:37s remains)
INFO - root - 2019-11-06 18:30:32.177345: step 24010, total loss = 5.84, predict loss = 1.74 (59.3 examples/sec; 0.067 sec/batch; 2h:21m:39s remains)
INFO - root - 2019-11-06 18:30:32.890647: step 24020, total loss = 3.38, predict loss = 1.00 (61.6 examples/sec; 0.065 sec/batch; 2h:16m:20s remains)
INFO - root - 2019-11-06 18:30:33.589920: step 24030, total loss = 6.23, predict loss = 1.76 (66.5 examples/sec; 0.060 sec/batch; 2h:06m:19s remains)
INFO - root - 2019-11-06 18:30:34.378687: step 24040, total loss = 4.41, predict loss = 1.30 (56.1 examples/sec; 0.071 sec/batch; 2h:29m:38s remains)
INFO - root - 2019-11-06 18:30:35.062743: step 24050, total loss = 4.93, predict loss = 1.43 (73.2 examples/sec; 0.055 sec/batch; 1h:54m:43s remains)
INFO - root - 2019-11-06 18:30:35.584687: step 24060, total loss = 3.67, predict loss = 1.09 (87.8 examples/sec; 0.046 sec/batch; 1h:35m:40s remains)
INFO - root - 2019-11-06 18:30:36.040775: step 24070, total loss = 4.76, predict loss = 1.35 (101.6 examples/sec; 0.039 sec/batch; 1h:22m:36s remains)
INFO - root - 2019-11-06 18:30:37.231652: step 24080, total loss = 5.44, predict loss = 1.56 (67.8 examples/sec; 0.059 sec/batch; 2h:03m:46s remains)
INFO - root - 2019-11-06 18:30:37.978246: step 24090, total loss = 4.76, predict loss = 1.34 (53.0 examples/sec; 0.075 sec/batch; 2h:38m:25s remains)
INFO - root - 2019-11-06 18:30:38.660979: step 24100, total loss = 4.41, predict loss = 1.35 (64.5 examples/sec; 0.062 sec/batch; 2h:10m:11s remains)
INFO - root - 2019-11-06 18:30:39.364993: step 24110, total loss = 5.43, predict loss = 1.58 (63.3 examples/sec; 0.063 sec/batch; 2h:12m:33s remains)
INFO - root - 2019-11-06 18:30:40.070999: step 24120, total loss = 3.93, predict loss = 1.15 (57.4 examples/sec; 0.070 sec/batch; 2h:26m:12s remains)
INFO - root - 2019-11-06 18:30:40.702733: step 24130, total loss = 4.66, predict loss = 1.32 (93.9 examples/sec; 0.043 sec/batch; 1h:29m:23s remains)
INFO - root - 2019-11-06 18:30:41.153250: step 24140, total loss = 5.24, predict loss = 1.57 (99.0 examples/sec; 0.040 sec/batch; 1h:24m:47s remains)
INFO - root - 2019-11-06 18:30:41.609494: step 24150, total loss = 5.21, predict loss = 1.47 (96.1 examples/sec; 0.042 sec/batch; 1h:27m:20s remains)
INFO - root - 2019-11-06 18:30:42.904897: step 24160, total loss = 4.92, predict loss = 1.50 (60.7 examples/sec; 0.066 sec/batch; 2h:18m:10s remains)
INFO - root - 2019-11-06 18:30:43.632168: step 24170, total loss = 4.13, predict loss = 1.24 (64.4 examples/sec; 0.062 sec/batch; 2h:10m:17s remains)
INFO - root - 2019-11-06 18:30:44.378795: step 24180, total loss = 6.70, predict loss = 1.96 (62.0 examples/sec; 0.064 sec/batch; 2h:15m:14s remains)
INFO - root - 2019-11-06 18:30:45.136395: step 24190, total loss = 5.41, predict loss = 1.53 (53.9 examples/sec; 0.074 sec/batch; 2h:35m:42s remains)
INFO - root - 2019-11-06 18:30:45.896889: step 24200, total loss = 6.27, predict loss = 1.83 (67.2 examples/sec; 0.060 sec/batch; 2h:04m:51s remains)
INFO - root - 2019-11-06 18:30:46.449552: step 24210, total loss = 6.68, predict loss = 1.88 (90.3 examples/sec; 0.044 sec/batch; 1h:32m:50s remains)
INFO - root - 2019-11-06 18:30:46.902453: step 24220, total loss = 4.06, predict loss = 1.22 (93.8 examples/sec; 0.043 sec/batch; 1h:29m:25s remains)
INFO - root - 2019-11-06 18:30:48.079320: step 24230, total loss = 5.50, predict loss = 1.63 (70.4 examples/sec; 0.057 sec/batch; 1h:59m:10s remains)
INFO - root - 2019-11-06 18:30:48.770878: step 24240, total loss = 6.26, predict loss = 1.79 (54.2 examples/sec; 0.074 sec/batch; 2h:34m:43s remains)
INFO - root - 2019-11-06 18:30:49.502928: step 24250, total loss = 7.09, predict loss = 2.01 (68.8 examples/sec; 0.058 sec/batch; 2h:01m:55s remains)
INFO - root - 2019-11-06 18:30:50.207175: step 24260, total loss = 5.43, predict loss = 1.62 (64.3 examples/sec; 0.062 sec/batch; 2h:10m:17s remains)
INFO - root - 2019-11-06 18:30:50.998858: step 24270, total loss = 4.74, predict loss = 1.38 (50.1 examples/sec; 0.080 sec/batch; 2h:47m:16s remains)
INFO - root - 2019-11-06 18:30:51.614392: step 24280, total loss = 5.94, predict loss = 1.62 (96.8 examples/sec; 0.041 sec/batch; 1h:26m:36s remains)
INFO - root - 2019-11-06 18:30:52.137941: step 24290, total loss = 5.46, predict loss = 1.60 (96.9 examples/sec; 0.041 sec/batch; 1h:26m:29s remains)
INFO - root - 2019-11-06 18:30:52.584330: step 24300, total loss = 6.21, predict loss = 1.74 (99.3 examples/sec; 0.040 sec/batch; 1h:24m:21s remains)
INFO - root - 2019-11-06 18:30:53.920305: step 24310, total loss = 4.87, predict loss = 1.41 (59.8 examples/sec; 0.067 sec/batch; 2h:20m:05s remains)
INFO - root - 2019-11-06 18:30:54.637430: step 24320, total loss = 4.50, predict loss = 1.27 (62.5 examples/sec; 0.064 sec/batch; 2h:13m:58s remains)
INFO - root - 2019-11-06 18:30:55.393301: step 24330, total loss = 6.22, predict loss = 1.78 (62.1 examples/sec; 0.064 sec/batch; 2h:14m:54s remains)
INFO - root - 2019-11-06 18:30:56.134757: step 24340, total loss = 3.43, predict loss = 1.07 (61.6 examples/sec; 0.065 sec/batch; 2h:15m:57s remains)
INFO - root - 2019-11-06 18:30:56.814371: step 24350, total loss = 3.25, predict loss = 0.94 (73.9 examples/sec; 0.054 sec/batch; 1h:53m:16s remains)
INFO - root - 2019-11-06 18:30:57.299088: step 24360, total loss = 4.95, predict loss = 1.44 (97.3 examples/sec; 0.041 sec/batch; 1h:26m:03s remains)
INFO - root - 2019-11-06 18:30:57.784672: step 24370, total loss = 4.12, predict loss = 1.14 (97.3 examples/sec; 0.041 sec/batch; 1h:26m:02s remains)
INFO - root - 2019-11-06 18:30:58.990177: step 24380, total loss = 5.29, predict loss = 1.43 (63.4 examples/sec; 0.063 sec/batch; 2h:11m:59s remains)
INFO - root - 2019-11-06 18:30:59.728032: step 24390, total loss = 5.68, predict loss = 1.65 (58.6 examples/sec; 0.068 sec/batch; 2h:22m:55s remains)
INFO - root - 2019-11-06 18:31:00.526418: step 24400, total loss = 4.43, predict loss = 1.33 (55.2 examples/sec; 0.072 sec/batch; 2h:31m:33s remains)
INFO - root - 2019-11-06 18:31:01.326671: step 24410, total loss = 5.99, predict loss = 1.69 (51.2 examples/sec; 0.078 sec/batch; 2h:43m:36s remains)
INFO - root - 2019-11-06 18:31:02.164758: step 24420, total loss = 5.39, predict loss = 1.59 (55.4 examples/sec; 0.072 sec/batch; 2h:31m:09s remains)
INFO - root - 2019-11-06 18:31:02.707174: step 24430, total loss = 5.43, predict loss = 1.54 (103.7 examples/sec; 0.039 sec/batch; 1h:20m:45s remains)
INFO - root - 2019-11-06 18:31:03.151490: step 24440, total loss = 5.86, predict loss = 1.66 (97.3 examples/sec; 0.041 sec/batch; 1h:26m:02s remains)
INFO - root - 2019-11-06 18:31:03.625091: step 24450, total loss = 5.65, predict loss = 1.62 (124.6 examples/sec; 0.032 sec/batch; 1h:07m:10s remains)
INFO - root - 2019-11-06 18:31:04.937850: step 24460, total loss = 3.96, predict loss = 1.12 (55.4 examples/sec; 0.072 sec/batch; 2h:31m:05s remains)
INFO - root - 2019-11-06 18:31:05.766637: step 24470, total loss = 6.24, predict loss = 1.76 (48.8 examples/sec; 0.082 sec/batch; 2h:51m:33s remains)
INFO - root - 2019-11-06 18:31:06.571943: step 24480, total loss = 6.11, predict loss = 1.74 (59.2 examples/sec; 0.068 sec/batch; 2h:21m:21s remains)
INFO - root - 2019-11-06 18:31:07.340266: step 24490, total loss = 5.49, predict loss = 1.54 (59.5 examples/sec; 0.067 sec/batch; 2h:20m:43s remains)
INFO - root - 2019-11-06 18:31:08.016220: step 24500, total loss = 3.74, predict loss = 1.08 (68.1 examples/sec; 0.059 sec/batch; 2h:02m:53s remains)
INFO - root - 2019-11-06 18:31:08.485562: step 24510, total loss = 6.70, predict loss = 1.96 (95.9 examples/sec; 0.042 sec/batch; 1h:27m:14s remains)
INFO - root - 2019-11-06 18:31:08.927551: step 24520, total loss = 4.68, predict loss = 1.40 (92.7 examples/sec; 0.043 sec/batch; 1h:30m:14s remains)
INFO - root - 2019-11-06 18:31:10.183421: step 24530, total loss = 3.34, predict loss = 1.00 (63.3 examples/sec; 0.063 sec/batch; 2h:12m:02s remains)
INFO - root - 2019-11-06 18:31:10.896905: step 24540, total loss = 4.66, predict loss = 1.40 (60.4 examples/sec; 0.066 sec/batch; 2h:18m:35s remains)
INFO - root - 2019-11-06 18:31:11.649069: step 24550, total loss = 4.57, predict loss = 1.36 (54.2 examples/sec; 0.074 sec/batch; 2h:34m:22s remains)
INFO - root - 2019-11-06 18:31:12.399429: step 24560, total loss = 5.05, predict loss = 1.48 (61.3 examples/sec; 0.065 sec/batch; 2h:16m:29s remains)
INFO - root - 2019-11-06 18:31:13.120304: step 24570, total loss = 4.32, predict loss = 1.26 (75.2 examples/sec; 0.053 sec/batch; 1h:51m:11s remains)
INFO - root - 2019-11-06 18:31:13.660578: step 24580, total loss = 5.84, predict loss = 1.58 (90.8 examples/sec; 0.044 sec/batch; 1h:32m:07s remains)
INFO - root - 2019-11-06 18:31:14.112918: step 24590, total loss = 5.25, predict loss = 1.61 (93.4 examples/sec; 0.043 sec/batch; 1h:29m:29s remains)
INFO - root - 2019-11-06 18:31:15.252779: step 24600, total loss = 4.56, predict loss = 1.46 (5.5 examples/sec; 0.731 sec/batch; 25h:28m:03s remains)
INFO - root - 2019-11-06 18:31:15.963469: step 24610, total loss = 6.09, predict loss = 1.75 (62.5 examples/sec; 0.064 sec/batch; 2h:13m:49s remains)
INFO - root - 2019-11-06 18:31:16.716937: step 24620, total loss = 4.37, predict loss = 1.26 (58.7 examples/sec; 0.068 sec/batch; 2h:22m:29s remains)
INFO - root - 2019-11-06 18:31:17.513707: step 24630, total loss = 6.61, predict loss = 1.81 (51.7 examples/sec; 0.077 sec/batch; 2h:41m:40s remains)
INFO - root - 2019-11-06 18:31:18.292369: step 24640, total loss = 4.02, predict loss = 1.20 (58.6 examples/sec; 0.068 sec/batch; 2h:22m:39s remains)
INFO - root - 2019-11-06 18:31:18.977874: step 24650, total loss = 4.97, predict loss = 1.42 (87.2 examples/sec; 0.046 sec/batch; 1h:35m:51s remains)
INFO - root - 2019-11-06 18:31:19.433402: step 24660, total loss = 4.05, predict loss = 1.12 (96.6 examples/sec; 0.041 sec/batch; 1h:26m:29s remains)
INFO - root - 2019-11-06 18:31:19.893057: step 24670, total loss = 5.41, predict loss = 1.55 (99.2 examples/sec; 0.040 sec/batch; 1h:24m:14s remains)
INFO - root - 2019-11-06 18:31:21.169929: step 24680, total loss = 3.88, predict loss = 1.09 (66.8 examples/sec; 0.060 sec/batch; 2h:05m:06s remains)
INFO - root - 2019-11-06 18:31:21.921171: step 24690, total loss = 4.77, predict loss = 1.32 (53.9 examples/sec; 0.074 sec/batch; 2h:34m:54s remains)
INFO - root - 2019-11-06 18:31:22.697226: step 24700, total loss = 5.38, predict loss = 1.62 (55.1 examples/sec; 0.073 sec/batch; 2h:31m:32s remains)
INFO - root - 2019-11-06 18:31:23.454876: step 24710, total loss = 5.60, predict loss = 1.57 (53.1 examples/sec; 0.075 sec/batch; 2h:37m:13s remains)
INFO - root - 2019-11-06 18:31:24.193458: step 24720, total loss = 3.69, predict loss = 1.10 (64.8 examples/sec; 0.062 sec/batch; 2h:08m:50s remains)
INFO - root - 2019-11-06 18:31:24.757268: step 24730, total loss = 4.48, predict loss = 1.32 (97.3 examples/sec; 0.041 sec/batch; 1h:25m:49s remains)
INFO - root - 2019-11-06 18:31:25.214701: step 24740, total loss = 4.51, predict loss = 1.29 (95.3 examples/sec; 0.042 sec/batch; 1h:27m:37s remains)
INFO - root - 2019-11-06 18:31:26.381576: step 24750, total loss = 4.46, predict loss = 1.28 (66.6 examples/sec; 0.060 sec/batch; 2h:05m:20s remains)
INFO - root - 2019-11-06 18:31:27.074780: step 24760, total loss = 3.85, predict loss = 1.05 (58.9 examples/sec; 0.068 sec/batch; 2h:21m:40s remains)
INFO - root - 2019-11-06 18:31:27.867951: step 24770, total loss = 5.10, predict loss = 1.46 (50.5 examples/sec; 0.079 sec/batch; 2h:45m:15s remains)
INFO - root - 2019-11-06 18:31:28.574865: step 24780, total loss = 5.49, predict loss = 1.58 (64.9 examples/sec; 0.062 sec/batch; 2h:08m:43s remains)
INFO - root - 2019-11-06 18:31:29.260524: step 24790, total loss = 5.79, predict loss = 1.66 (63.4 examples/sec; 0.063 sec/batch; 2h:11m:37s remains)
INFO - root - 2019-11-06 18:31:29.896171: step 24800, total loss = 5.08, predict loss = 1.37 (90.7 examples/sec; 0.044 sec/batch; 1h:32m:00s remains)
INFO - root - 2019-11-06 18:31:30.378076: step 24810, total loss = 3.48, predict loss = 1.02 (94.7 examples/sec; 0.042 sec/batch; 1h:28m:06s remains)
INFO - root - 2019-11-06 18:31:30.828624: step 24820, total loss = 4.71, predict loss = 1.33 (96.3 examples/sec; 0.042 sec/batch; 1h:26m:38s remains)
INFO - root - 2019-11-06 18:31:32.075811: step 24830, total loss = 5.05, predict loss = 1.43 (62.0 examples/sec; 0.065 sec/batch; 2h:14m:35s remains)
INFO - root - 2019-11-06 18:31:32.819864: step 24840, total loss = 3.81, predict loss = 1.08 (54.0 examples/sec; 0.074 sec/batch; 2h:34m:31s remains)
INFO - root - 2019-11-06 18:31:33.650904: step 24850, total loss = 4.65, predict loss = 1.37 (57.8 examples/sec; 0.069 sec/batch; 2h:24m:17s remains)
INFO - root - 2019-11-06 18:31:34.416401: step 24860, total loss = 6.00, predict loss = 1.77 (56.5 examples/sec; 0.071 sec/batch; 2h:27m:32s remains)
INFO - root - 2019-11-06 18:31:35.149110: step 24870, total loss = 6.48, predict loss = 1.93 (60.1 examples/sec; 0.067 sec/batch; 2h:18m:47s remains)
INFO - root - 2019-11-06 18:31:35.660328: step 24880, total loss = 6.05, predict loss = 1.69 (90.9 examples/sec; 0.044 sec/batch; 1h:31m:43s remains)
INFO - root - 2019-11-06 18:31:36.146461: step 24890, total loss = 3.48, predict loss = 1.05 (96.6 examples/sec; 0.041 sec/batch; 1h:26m:22s remains)
INFO - root - 2019-11-06 18:31:37.285588: step 24900, total loss = 4.26, predict loss = 1.31 (69.6 examples/sec; 0.057 sec/batch; 1h:59m:46s remains)
INFO - root - 2019-11-06 18:31:37.989186: step 24910, total loss = 5.28, predict loss = 1.61 (59.9 examples/sec; 0.067 sec/batch; 2h:19m:13s remains)
INFO - root - 2019-11-06 18:31:38.724710: step 24920, total loss = 4.73, predict loss = 1.35 (56.6 examples/sec; 0.071 sec/batch; 2h:27m:16s remains)
INFO - root - 2019-11-06 18:31:39.482675: step 24930, total loss = 4.17, predict loss = 1.14 (57.5 examples/sec; 0.070 sec/batch; 2h:24m:59s remains)
INFO - root - 2019-11-06 18:31:40.279876: step 24940, total loss = 4.79, predict loss = 1.43 (57.3 examples/sec; 0.070 sec/batch; 2h:25m:33s remains)
INFO - root - 2019-11-06 18:31:40.915110: step 24950, total loss = 4.07, predict loss = 1.13 (98.8 examples/sec; 0.040 sec/batch; 1h:24m:22s remains)
INFO - root - 2019-11-06 18:31:41.363354: step 24960, total loss = 4.06, predict loss = 1.14 (100.5 examples/sec; 0.040 sec/batch; 1h:22m:58s remains)
INFO - root - 2019-11-06 18:31:41.835030: step 24970, total loss = 4.05, predict loss = 1.13 (94.7 examples/sec; 0.042 sec/batch; 1h:28m:00s remains)
INFO - root - 2019-11-06 18:31:43.154421: step 24980, total loss = 5.80, predict loss = 1.77 (62.6 examples/sec; 0.064 sec/batch; 2h:13m:10s remains)
INFO - root - 2019-11-06 18:31:43.952756: step 24990, total loss = 4.46, predict loss = 1.29 (54.6 examples/sec; 0.073 sec/batch; 2h:32m:36s remains)
INFO - root - 2019-11-06 18:31:44.711740: step 25000, total loss = 5.70, predict loss = 1.66 (54.7 examples/sec; 0.073 sec/batch; 2h:32m:20s remains)
INFO - root - 2019-11-06 18:31:45.451784: step 25010, total loss = 6.02, predict loss = 1.71 (57.0 examples/sec; 0.070 sec/batch; 2h:26m:07s remains)
INFO - root - 2019-11-06 18:31:46.162513: step 25020, total loss = 5.93, predict loss = 1.71 (70.2 examples/sec; 0.057 sec/batch; 1h:58m:37s remains)
INFO - root - 2019-11-06 18:31:46.691358: step 25030, total loss = 5.54, predict loss = 1.65 (97.7 examples/sec; 0.041 sec/batch; 1h:25m:16s remains)
INFO - root - 2019-11-06 18:31:47.157863: step 25040, total loss = 5.31, predict loss = 1.54 (96.1 examples/sec; 0.042 sec/batch; 1h:26m:40s remains)
INFO - root - 2019-11-06 18:31:48.354778: step 25050, total loss = 5.77, predict loss = 1.66 (67.6 examples/sec; 0.059 sec/batch; 2h:03m:17s remains)
INFO - root - 2019-11-06 18:31:49.032643: step 25060, total loss = 5.42, predict loss = 1.59 (60.6 examples/sec; 0.066 sec/batch; 2h:17m:29s remains)
INFO - root - 2019-11-06 18:31:49.764457: step 25070, total loss = 4.98, predict loss = 1.45 (56.9 examples/sec; 0.070 sec/batch; 2h:26m:16s remains)
INFO - root - 2019-11-06 18:31:50.474733: step 25080, total loss = 6.22, predict loss = 1.70 (57.6 examples/sec; 0.069 sec/batch; 2h:24m:40s remains)
INFO - root - 2019-11-06 18:31:51.201494: step 25090, total loss = 6.74, predict loss = 1.98 (59.8 examples/sec; 0.067 sec/batch; 2h:19m:19s remains)
INFO - root - 2019-11-06 18:31:51.778487: step 25100, total loss = 6.02, predict loss = 1.68 (93.6 examples/sec; 0.043 sec/batch; 1h:28m:57s remains)
INFO - root - 2019-11-06 18:31:52.242484: step 25110, total loss = 6.18, predict loss = 1.78 (103.4 examples/sec; 0.039 sec/batch; 1h:20m:32s remains)
INFO - root - 2019-11-06 18:31:52.689061: step 25120, total loss = 5.38, predict loss = 1.54 (93.7 examples/sec; 0.043 sec/batch; 1h:28m:48s remains)
INFO - root - 2019-11-06 18:31:54.026103: step 25130, total loss = 5.05, predict loss = 1.35 (64.2 examples/sec; 0.062 sec/batch; 2h:09m:40s remains)
INFO - root - 2019-11-06 18:31:54.746012: step 25140, total loss = 5.09, predict loss = 1.46 (64.5 examples/sec; 0.062 sec/batch; 2h:09m:01s remains)
INFO - root - 2019-11-06 18:31:55.525307: step 25150, total loss = 5.20, predict loss = 1.58 (58.5 examples/sec; 0.068 sec/batch; 2h:22m:20s remains)
INFO - root - 2019-11-06 18:31:56.265008: step 25160, total loss = 5.58, predict loss = 1.54 (59.5 examples/sec; 0.067 sec/batch; 2h:19m:52s remains)
INFO - root - 2019-11-06 18:31:56.951404: step 25170, total loss = 4.92, predict loss = 1.38 (72.4 examples/sec; 0.055 sec/batch; 1h:54m:55s remains)
INFO - root - 2019-11-06 18:31:57.423155: step 25180, total loss = 4.70, predict loss = 1.35 (94.8 examples/sec; 0.042 sec/batch; 1h:27m:48s remains)
INFO - root - 2019-11-06 18:31:57.873137: step 25190, total loss = 3.04, predict loss = 0.90 (94.8 examples/sec; 0.042 sec/batch; 1h:27m:48s remains)
INFO - root - 2019-11-06 18:31:59.086570: step 25200, total loss = 5.46, predict loss = 1.57 (64.3 examples/sec; 0.062 sec/batch; 2h:09m:23s remains)
INFO - root - 2019-11-06 18:31:59.813522: step 25210, total loss = 3.82, predict loss = 1.09 (67.3 examples/sec; 0.059 sec/batch; 2h:03m:39s remains)
INFO - root - 2019-11-06 18:32:00.514372: step 25220, total loss = 3.63, predict loss = 1.04 (64.3 examples/sec; 0.062 sec/batch; 2h:09m:22s remains)
INFO - root - 2019-11-06 18:32:01.308927: step 25230, total loss = 5.23, predict loss = 1.45 (52.7 examples/sec; 0.076 sec/batch; 2h:37m:43s remains)
INFO - root - 2019-11-06 18:32:02.113335: step 25240, total loss = 5.75, predict loss = 1.68 (57.6 examples/sec; 0.069 sec/batch; 2h:24m:16s remains)
INFO - root - 2019-11-06 18:32:02.693711: step 25250, total loss = 4.42, predict loss = 1.28 (97.4 examples/sec; 0.041 sec/batch; 1h:25m:24s remains)
INFO - root - 2019-11-06 18:32:03.146082: step 25260, total loss = 5.38, predict loss = 1.48 (96.0 examples/sec; 0.042 sec/batch; 1h:26m:37s remains)
INFO - root - 2019-11-06 18:32:03.608408: step 25270, total loss = 5.50, predict loss = 1.50 (108.8 examples/sec; 0.037 sec/batch; 1h:16m:24s remains)
INFO - root - 2019-11-06 18:32:05.039995: step 25280, total loss = 3.56, predict loss = 1.12 (57.4 examples/sec; 0.070 sec/batch; 2h:24m:55s remains)
INFO - root - 2019-11-06 18:32:05.864250: step 25290, total loss = 5.95, predict loss = 1.70 (55.3 examples/sec; 0.072 sec/batch; 2h:30m:28s remains)
INFO - root - 2019-11-06 18:32:06.724737: step 25300, total loss = 5.33, predict loss = 1.46 (49.6 examples/sec; 0.081 sec/batch; 2h:47m:37s remains)
INFO - root - 2019-11-06 18:32:07.488171: step 25310, total loss = 4.82, predict loss = 1.40 (57.8 examples/sec; 0.069 sec/batch; 2h:23m:54s remains)
INFO - root - 2019-11-06 18:32:08.139396: step 25320, total loss = 5.10, predict loss = 1.47 (82.5 examples/sec; 0.048 sec/batch; 1h:40m:44s remains)
INFO - root - 2019-11-06 18:32:08.607828: step 25330, total loss = 3.31, predict loss = 0.92 (96.8 examples/sec; 0.041 sec/batch; 1h:25m:50s remains)
INFO - root - 2019-11-06 18:32:09.068052: step 25340, total loss = 3.94, predict loss = 1.14 (91.3 examples/sec; 0.044 sec/batch; 1h:31m:01s remains)
INFO - root - 2019-11-06 18:32:10.263670: step 25350, total loss = 4.96, predict loss = 1.50 (62.2 examples/sec; 0.064 sec/batch; 2h:13m:34s remains)
INFO - root - 2019-11-06 18:32:11.011470: step 25360, total loss = 4.77, predict loss = 1.34 (61.1 examples/sec; 0.065 sec/batch; 2h:15m:58s remains)
INFO - root - 2019-11-06 18:32:11.734955: step 25370, total loss = 4.97, predict loss = 1.45 (63.8 examples/sec; 0.063 sec/batch; 2h:10m:16s remains)
INFO - root - 2019-11-06 18:32:12.430818: step 25380, total loss = 5.87, predict loss = 1.67 (58.0 examples/sec; 0.069 sec/batch; 2h:23m:09s remains)
INFO - root - 2019-11-06 18:32:13.131254: step 25390, total loss = 3.27, predict loss = 0.94 (75.2 examples/sec; 0.053 sec/batch; 1h:50m:28s remains)
INFO - root - 2019-11-06 18:32:13.664054: step 25400, total loss = 5.65, predict loss = 1.71 (101.3 examples/sec; 0.039 sec/batch; 1h:21m:59s remains)
INFO - root - 2019-11-06 18:32:14.144871: step 25410, total loss = 5.95, predict loss = 1.70 (94.3 examples/sec; 0.042 sec/batch; 1h:28m:03s remains)
INFO - root - 2019-11-06 18:32:15.272754: step 25420, total loss = 4.72, predict loss = 1.33 (5.5 examples/sec; 0.732 sec/batch; 25h:19m:58s remains)
INFO - root - 2019-11-06 18:32:15.949691: step 25430, total loss = 3.70, predict loss = 0.98 (57.2 examples/sec; 0.070 sec/batch; 2h:25m:12s remains)
INFO - root - 2019-11-06 18:32:16.676942: step 25440, total loss = 4.77, predict loss = 1.46 (59.5 examples/sec; 0.067 sec/batch; 2h:19m:30s remains)
INFO - root - 2019-11-06 18:32:17.409071: step 25450, total loss = 4.75, predict loss = 1.36 (58.7 examples/sec; 0.068 sec/batch; 2h:21m:33s remains)
INFO - root - 2019-11-06 18:32:18.175153: step 25460, total loss = 5.39, predict loss = 1.47 (58.3 examples/sec; 0.069 sec/batch; 2h:22m:20s remains)
INFO - root - 2019-11-06 18:32:18.811224: step 25470, total loss = 4.50, predict loss = 1.28 (92.9 examples/sec; 0.043 sec/batch; 1h:29m:20s remains)
INFO - root - 2019-11-06 18:32:19.265790: step 25480, total loss = 5.19, predict loss = 1.45 (95.8 examples/sec; 0.042 sec/batch; 1h:26m:40s remains)
INFO - root - 2019-11-06 18:32:19.757545: step 25490, total loss = 4.59, predict loss = 1.26 (85.3 examples/sec; 0.047 sec/batch; 1h:37m:21s remains)
INFO - root - 2019-11-06 18:32:20.986176: step 25500, total loss = 3.78, predict loss = 1.08 (63.8 examples/sec; 0.063 sec/batch; 2h:10m:01s remains)
INFO - root - 2019-11-06 18:32:21.688163: step 25510, total loss = 2.23, predict loss = 0.63 (52.6 examples/sec; 0.076 sec/batch; 2h:37m:44s remains)
INFO - root - 2019-11-06 18:32:22.516902: step 25520, total loss = 4.46, predict loss = 1.36 (58.9 examples/sec; 0.068 sec/batch; 2h:20m:56s remains)
INFO - root - 2019-11-06 18:32:23.251089: step 25530, total loss = 3.86, predict loss = 1.13 (55.3 examples/sec; 0.072 sec/batch; 2h:29m:57s remains)
INFO - root - 2019-11-06 18:32:23.981209: step 25540, total loss = 3.95, predict loss = 1.08 (69.3 examples/sec; 0.058 sec/batch; 1h:59m:46s remains)
INFO - root - 2019-11-06 18:32:24.505069: step 25550, total loss = 4.60, predict loss = 1.26 (103.0 examples/sec; 0.039 sec/batch; 1h:20m:32s remains)
INFO - root - 2019-11-06 18:32:24.949195: step 25560, total loss = 5.23, predict loss = 1.53 (94.7 examples/sec; 0.042 sec/batch; 1h:27m:38s remains)
INFO - root - 2019-11-06 18:32:26.119644: step 25570, total loss = 3.60, predict loss = 1.05 (69.2 examples/sec; 0.058 sec/batch; 1h:59m:52s remains)
INFO - root - 2019-11-06 18:32:26.812326: step 25580, total loss = 5.19, predict loss = 1.47 (64.9 examples/sec; 0.062 sec/batch; 2h:07m:52s remains)
INFO - root - 2019-11-06 18:32:27.553710: step 25590, total loss = 5.26, predict loss = 1.50 (63.3 examples/sec; 0.063 sec/batch; 2h:11m:05s remains)
INFO - root - 2019-11-06 18:32:28.334758: step 25600, total loss = 4.63, predict loss = 1.35 (54.0 examples/sec; 0.074 sec/batch; 2h:33m:37s remains)
INFO - root - 2019-11-06 18:32:29.116449: step 25610, total loss = 4.22, predict loss = 1.24 (56.3 examples/sec; 0.071 sec/batch; 2h:27m:10s remains)
INFO - root - 2019-11-06 18:32:29.792584: step 25620, total loss = 4.60, predict loss = 1.31 (95.0 examples/sec; 0.042 sec/batch; 1h:27m:17s remains)
INFO - root - 2019-11-06 18:32:30.247705: step 25630, total loss = 4.65, predict loss = 1.38 (90.7 examples/sec; 0.044 sec/batch; 1h:31m:24s remains)
INFO - root - 2019-11-06 18:32:30.699838: step 25640, total loss = 4.56, predict loss = 1.35 (96.6 examples/sec; 0.041 sec/batch; 1h:25m:47s remains)
INFO - root - 2019-11-06 18:32:32.015298: step 25650, total loss = 4.45, predict loss = 1.34 (59.1 examples/sec; 0.068 sec/batch; 2h:20m:23s remains)
INFO - root - 2019-11-06 18:32:32.773778: step 25660, total loss = 5.23, predict loss = 1.54 (55.0 examples/sec; 0.073 sec/batch; 2h:30m:44s remains)
INFO - root - 2019-11-06 18:32:33.491748: step 25670, total loss = 6.80, predict loss = 1.96 (65.6 examples/sec; 0.061 sec/batch; 2h:06m:23s remains)
INFO - root - 2019-11-06 18:32:34.198372: step 25680, total loss = 4.87, predict loss = 1.40 (64.4 examples/sec; 0.062 sec/batch; 2h:08m:47s remains)
INFO - root - 2019-11-06 18:32:34.916959: step 25690, total loss = 4.39, predict loss = 1.29 (82.7 examples/sec; 0.048 sec/batch; 1h:40m:13s remains)
INFO - root - 2019-11-06 18:32:35.387003: step 25700, total loss = 4.82, predict loss = 1.34 (97.1 examples/sec; 0.041 sec/batch; 1h:25m:21s remains)
INFO - root - 2019-11-06 18:32:35.844452: step 25710, total loss = 4.15, predict loss = 1.18 (95.3 examples/sec; 0.042 sec/batch; 1h:26m:57s remains)
INFO - root - 2019-11-06 18:32:37.007656: step 25720, total loss = 5.26, predict loss = 1.47 (68.8 examples/sec; 0.058 sec/batch; 2h:00m:21s remains)
INFO - root - 2019-11-06 18:32:37.695924: step 25730, total loss = 4.89, predict loss = 1.38 (66.5 examples/sec; 0.060 sec/batch; 2h:04m:33s remains)
INFO - root - 2019-11-06 18:32:38.394596: step 25740, total loss = 3.94, predict loss = 1.20 (54.7 examples/sec; 0.073 sec/batch; 2h:31m:34s remains)
INFO - root - 2019-11-06 18:32:39.209778: step 25750, total loss = 4.45, predict loss = 1.28 (56.4 examples/sec; 0.071 sec/batch; 2h:26m:52s remains)
INFO - root - 2019-11-06 18:32:39.957780: step 25760, total loss = 4.05, predict loss = 1.15 (56.5 examples/sec; 0.071 sec/batch; 2h:26m:32s remains)
INFO - root - 2019-11-06 18:32:40.556050: step 25770, total loss = 5.23, predict loss = 1.52 (95.7 examples/sec; 0.042 sec/batch; 1h:26m:31s remains)
INFO - root - 2019-11-06 18:32:41.018399: step 25780, total loss = 5.74, predict loss = 1.58 (91.9 examples/sec; 0.044 sec/batch; 1h:30m:05s remains)
INFO - root - 2019-11-06 18:32:41.468293: step 25790, total loss = 5.87, predict loss = 1.69 (97.3 examples/sec; 0.041 sec/batch; 1h:25m:04s remains)
INFO - root - 2019-11-06 18:32:42.719763: step 25800, total loss = 4.06, predict loss = 1.24 (65.5 examples/sec; 0.061 sec/batch; 2h:06m:29s remains)
INFO - root - 2019-11-06 18:32:43.430737: step 25810, total loss = 3.67, predict loss = 1.01 (64.8 examples/sec; 0.062 sec/batch; 2h:07m:46s remains)
INFO - root - 2019-11-06 18:32:44.190490: step 25820, total loss = 5.09, predict loss = 1.45 (58.2 examples/sec; 0.069 sec/batch; 2h:22m:09s remains)
INFO - root - 2019-11-06 18:32:44.978215: step 25830, total loss = 3.55, predict loss = 0.97 (53.9 examples/sec; 0.074 sec/batch; 2h:33m:27s remains)
INFO - root - 2019-11-06 18:32:45.760410: step 25840, total loss = 5.13, predict loss = 1.50 (55.1 examples/sec; 0.073 sec/batch; 2h:30m:07s remains)
INFO - root - 2019-11-06 18:32:46.291375: step 25850, total loss = 4.81, predict loss = 1.41 (96.6 examples/sec; 0.041 sec/batch; 1h:25m:38s remains)
INFO - root - 2019-11-06 18:32:46.751752: step 25860, total loss = 5.01, predict loss = 1.42 (92.8 examples/sec; 0.043 sec/batch; 1h:29m:12s remains)
INFO - root - 2019-11-06 18:32:47.953981: step 25870, total loss = 3.95, predict loss = 1.17 (62.2 examples/sec; 0.064 sec/batch; 2h:12m:58s remains)
INFO - root - 2019-11-06 18:32:48.666397: step 25880, total loss = 5.30, predict loss = 1.44 (61.1 examples/sec; 0.066 sec/batch; 2h:15m:30s remains)
INFO - root - 2019-11-06 18:32:49.392951: step 25890, total loss = 5.28, predict loss = 1.53 (65.5 examples/sec; 0.061 sec/batch; 2h:06m:15s remains)
INFO - root - 2019-11-06 18:32:50.129431: step 25900, total loss = 5.17, predict loss = 1.54 (63.5 examples/sec; 0.063 sec/batch; 2h:10m:21s remains)
INFO - root - 2019-11-06 18:32:50.913416: step 25910, total loss = 4.54, predict loss = 1.26 (56.8 examples/sec; 0.070 sec/batch; 2h:25m:36s remains)
INFO - root - 2019-11-06 18:32:51.548799: step 25920, total loss = 3.85, predict loss = 1.10 (95.9 examples/sec; 0.042 sec/batch; 1h:26m:14s remains)
INFO - root - 2019-11-06 18:32:52.036616: step 25930, total loss = 5.84, predict loss = 1.61 (93.4 examples/sec; 0.043 sec/batch; 1h:28m:34s remains)
INFO - root - 2019-11-06 18:32:52.497293: step 25940, total loss = 6.11, predict loss = 1.68 (92.8 examples/sec; 0.043 sec/batch; 1h:29m:09s remains)
INFO - root - 2019-11-06 18:32:53.799062: step 25950, total loss = 6.41, predict loss = 1.81 (69.5 examples/sec; 0.058 sec/batch; 1h:59m:03s remains)
INFO - root - 2019-11-06 18:32:54.508675: step 25960, total loss = 5.94, predict loss = 1.67 (62.0 examples/sec; 0.065 sec/batch; 2h:13m:28s remains)
INFO - root - 2019-11-06 18:32:55.264269: step 25970, total loss = 4.85, predict loss = 1.40 (65.0 examples/sec; 0.062 sec/batch; 2h:07m:09s remains)
INFO - root - 2019-11-06 18:32:55.972185: step 25980, total loss = 5.55, predict loss = 1.60 (62.5 examples/sec; 0.064 sec/batch; 2h:12m:14s remains)
INFO - root - 2019-11-06 18:32:56.687363: step 25990, total loss = 5.62, predict loss = 1.57 (73.6 examples/sec; 0.054 sec/batch; 1h:52m:22s remains)
INFO - root - 2019-11-06 18:32:57.165276: step 26000, total loss = 6.30, predict loss = 1.83 (87.9 examples/sec; 0.046 sec/batch; 1h:34m:05s remains)
INFO - root - 2019-11-06 18:32:57.659418: step 26010, total loss = 5.38, predict loss = 1.58 (97.6 examples/sec; 0.041 sec/batch; 1h:24m:41s remains)
INFO - root - 2019-11-06 18:32:58.879174: step 26020, total loss = 6.95, predict loss = 2.00 (61.7 examples/sec; 0.065 sec/batch; 2h:13m:55s remains)
INFO - root - 2019-11-06 18:32:59.589901: step 26030, total loss = 4.61, predict loss = 1.36 (56.8 examples/sec; 0.070 sec/batch; 2h:25m:29s remains)
INFO - root - 2019-11-06 18:33:00.323601: step 26040, total loss = 5.79, predict loss = 1.67 (66.1 examples/sec; 0.061 sec/batch; 2h:05m:05s remains)
INFO - root - 2019-11-06 18:33:01.058532: step 26050, total loss = 6.12, predict loss = 1.72 (55.9 examples/sec; 0.072 sec/batch; 2h:27m:46s remains)
INFO - root - 2019-11-06 18:33:01.794095: step 26060, total loss = 5.11, predict loss = 1.46 (61.5 examples/sec; 0.065 sec/batch; 2h:14m:20s remains)
INFO - root - 2019-11-06 18:33:02.356446: step 26070, total loss = 4.24, predict loss = 1.29 (97.5 examples/sec; 0.041 sec/batch; 1h:24m:43s remains)
INFO - root - 2019-11-06 18:33:02.819933: step 26080, total loss = 4.72, predict loss = 1.30 (87.1 examples/sec; 0.046 sec/batch; 1h:34m:49s remains)
INFO - root - 2019-11-06 18:33:03.284041: step 26090, total loss = 4.30, predict loss = 1.26 (142.8 examples/sec; 0.028 sec/batch; 0h:57m:51s remains)
INFO - root - 2019-11-06 18:33:04.645070: step 26100, total loss = 6.66, predict loss = 1.88 (60.6 examples/sec; 0.066 sec/batch; 2h:16m:17s remains)
INFO - root - 2019-11-06 18:33:05.366621: step 26110, total loss = 6.13, predict loss = 1.71 (62.8 examples/sec; 0.064 sec/batch; 2h:11m:25s remains)
INFO - root - 2019-11-06 18:33:06.196924: step 26120, total loss = 4.14, predict loss = 1.20 (56.1 examples/sec; 0.071 sec/batch; 2h:27m:12s remains)
INFO - root - 2019-11-06 18:33:06.955847: step 26130, total loss = 3.84, predict loss = 1.11 (61.2 examples/sec; 0.065 sec/batch; 2h:14m:52s remains)
INFO - root - 2019-11-06 18:33:07.620785: step 26140, total loss = 6.29, predict loss = 1.82 (74.8 examples/sec; 0.053 sec/batch; 1h:50m:20s remains)
INFO - root - 2019-11-06 18:33:08.067777: step 26150, total loss = 6.07, predict loss = 1.73 (101.7 examples/sec; 0.039 sec/batch; 1h:21m:10s remains)
INFO - root - 2019-11-06 18:33:08.519815: step 26160, total loss = 3.70, predict loss = 1.05 (94.9 examples/sec; 0.042 sec/batch; 1h:26m:58s remains)
INFO - root - 2019-11-06 18:33:09.723501: step 26170, total loss = 5.29, predict loss = 1.52 (67.9 examples/sec; 0.059 sec/batch; 2h:01m:35s remains)
INFO - root - 2019-11-06 18:33:10.462974: step 26180, total loss = 4.98, predict loss = 1.57 (56.2 examples/sec; 0.071 sec/batch; 2h:26m:46s remains)
INFO - root - 2019-11-06 18:33:11.251914: step 26190, total loss = 6.33, predict loss = 1.82 (59.0 examples/sec; 0.068 sec/batch; 2h:19m:58s remains)
INFO - root - 2019-11-06 18:33:12.009851: step 26200, total loss = 5.38, predict loss = 1.62 (58.3 examples/sec; 0.069 sec/batch; 2h:21m:29s remains)
INFO - root - 2019-11-06 18:33:12.741480: step 26210, total loss = 4.62, predict loss = 1.30 (67.5 examples/sec; 0.059 sec/batch; 2h:02m:11s remains)
INFO - root - 2019-11-06 18:33:13.270372: step 26220, total loss = 3.79, predict loss = 1.06 (98.4 examples/sec; 0.041 sec/batch; 1h:23m:50s remains)
INFO - root - 2019-11-06 18:33:13.722527: step 26230, total loss = 3.56, predict loss = 0.94 (97.3 examples/sec; 0.041 sec/batch; 1h:24m:50s remains)
INFO - root - 2019-11-06 18:33:14.835426: step 26240, total loss = 5.15, predict loss = 1.52 (5.6 examples/sec; 0.715 sec/batch; 24h:35m:34s remains)
INFO - root - 2019-11-06 18:33:15.541189: step 26250, total loss = 4.19, predict loss = 1.24 (63.7 examples/sec; 0.063 sec/batch; 2h:09m:25s remains)
INFO - root - 2019-11-06 18:33:16.323405: step 26260, total loss = 4.65, predict loss = 1.38 (58.1 examples/sec; 0.069 sec/batch; 2h:22m:02s remains)
INFO - root - 2019-11-06 18:33:17.111254: step 26270, total loss = 5.73, predict loss = 1.59 (54.2 examples/sec; 0.074 sec/batch; 2h:32m:03s remains)
INFO - root - 2019-11-06 18:33:17.851162: step 26280, total loss = 5.31, predict loss = 1.55 (62.1 examples/sec; 0.064 sec/batch; 2h:12m:45s remains)
INFO - root - 2019-11-06 18:33:18.536164: step 26290, total loss = 4.55, predict loss = 1.24 (87.4 examples/sec; 0.046 sec/batch; 1h:34m:21s remains)
INFO - root - 2019-11-06 18:33:18.987103: step 26300, total loss = 6.15, predict loss = 1.87 (96.7 examples/sec; 0.041 sec/batch; 1h:25m:18s remains)
INFO - root - 2019-11-06 18:33:19.426371: step 26310, total loss = 5.35, predict loss = 1.58 (99.0 examples/sec; 0.040 sec/batch; 1h:23m:18s remains)
INFO - root - 2019-11-06 18:33:20.646676: step 26320, total loss = 6.58, predict loss = 1.84 (70.0 examples/sec; 0.057 sec/batch; 1h:57m:44s remains)
INFO - root - 2019-11-06 18:33:21.389892: step 26330, total loss = 5.93, predict loss = 1.80 (57.0 examples/sec; 0.070 sec/batch; 2h:24m:36s remains)
INFO - root - 2019-11-06 18:33:22.203964: step 26340, total loss = 4.63, predict loss = 1.30 (67.4 examples/sec; 0.059 sec/batch; 2h:02m:20s remains)
INFO - root - 2019-11-06 18:33:22.923510: step 26350, total loss = 4.40, predict loss = 1.31 (56.2 examples/sec; 0.071 sec/batch; 2h:26m:34s remains)
INFO - root - 2019-11-06 18:33:23.620892: step 26360, total loss = 6.31, predict loss = 1.83 (75.8 examples/sec; 0.053 sec/batch; 1h:48m:46s remains)
INFO - root - 2019-11-06 18:33:24.149662: step 26370, total loss = 4.76, predict loss = 1.34 (95.1 examples/sec; 0.042 sec/batch; 1h:26m:42s remains)
INFO - root - 2019-11-06 18:33:24.607590: step 26380, total loss = 5.82, predict loss = 1.67 (91.2 examples/sec; 0.044 sec/batch; 1h:30m:19s remains)
INFO - root - 2019-11-06 18:33:25.772340: step 26390, total loss = 2.89, predict loss = 0.84 (70.6 examples/sec; 0.057 sec/batch; 1h:56m:44s remains)
INFO - root - 2019-11-06 18:33:26.457470: step 26400, total loss = 6.23, predict loss = 1.82 (61.0 examples/sec; 0.066 sec/batch; 2h:15m:03s remains)
INFO - root - 2019-11-06 18:33:27.221607: step 26410, total loss = 6.38, predict loss = 1.82 (52.8 examples/sec; 0.076 sec/batch; 2h:36m:02s remains)
INFO - root - 2019-11-06 18:33:28.016883: step 26420, total loss = 5.44, predict loss = 1.64 (52.8 examples/sec; 0.076 sec/batch; 2h:36m:04s remains)
INFO - root - 2019-11-06 18:33:28.739846: step 26430, total loss = 5.94, predict loss = 1.78 (57.5 examples/sec; 0.070 sec/batch; 2h:23m:13s remains)
INFO - root - 2019-11-06 18:33:29.360654: step 26440, total loss = 4.23, predict loss = 1.16 (99.5 examples/sec; 0.040 sec/batch; 1h:22m:49s remains)
INFO - root - 2019-11-06 18:33:29.820928: step 26450, total loss = 4.62, predict loss = 1.34 (95.3 examples/sec; 0.042 sec/batch; 1h:26m:24s remains)
INFO - root - 2019-11-06 18:33:30.271530: step 26460, total loss = 3.87, predict loss = 1.12 (101.1 examples/sec; 0.040 sec/batch; 1h:21m:26s remains)
INFO - root - 2019-11-06 18:33:31.478034: step 26470, total loss = 5.12, predict loss = 1.53 (60.1 examples/sec; 0.067 sec/batch; 2h:16m:59s remains)
INFO - root - 2019-11-06 18:33:32.215038: step 26480, total loss = 5.52, predict loss = 1.56 (61.9 examples/sec; 0.065 sec/batch; 2h:13m:07s remains)
INFO - root - 2019-11-06 18:33:32.947163: step 26490, total loss = 2.42, predict loss = 0.75 (66.6 examples/sec; 0.060 sec/batch; 2h:03m:41s remains)
INFO - root - 2019-11-06 18:33:33.748776: step 26500, total loss = 3.53, predict loss = 1.12 (49.7 examples/sec; 0.081 sec/batch; 2h:45m:44s remains)
INFO - root - 2019-11-06 18:33:34.443390: step 26510, total loss = 4.14, predict loss = 1.17 (74.5 examples/sec; 0.054 sec/batch; 1h:50m:28s remains)
INFO - root - 2019-11-06 18:33:34.936407: step 26520, total loss = 5.41, predict loss = 1.57 (101.3 examples/sec; 0.040 sec/batch; 1h:21m:17s remains)
INFO - root - 2019-11-06 18:33:35.395919: step 26530, total loss = 5.22, predict loss = 1.46 (93.8 examples/sec; 0.043 sec/batch; 1h:27m:46s remains)
INFO - root - 2019-11-06 18:33:36.564746: step 26540, total loss = 6.16, predict loss = 1.67 (67.3 examples/sec; 0.059 sec/batch; 2h:02m:23s remains)
INFO - root - 2019-11-06 18:33:37.229416: step 26550, total loss = 6.33, predict loss = 1.87 (68.1 examples/sec; 0.059 sec/batch; 2h:00m:49s remains)
INFO - root - 2019-11-06 18:33:37.947636: step 26560, total loss = 6.44, predict loss = 1.91 (59.0 examples/sec; 0.068 sec/batch; 2h:19m:26s remains)
INFO - root - 2019-11-06 18:33:38.703086: step 26570, total loss = 5.77, predict loss = 1.63 (60.9 examples/sec; 0.066 sec/batch; 2h:15m:03s remains)
INFO - root - 2019-11-06 18:33:39.424213: step 26580, total loss = 4.56, predict loss = 1.38 (65.3 examples/sec; 0.061 sec/batch; 2h:05m:54s remains)
INFO - root - 2019-11-06 18:33:40.008829: step 26590, total loss = 4.48, predict loss = 1.32 (98.2 examples/sec; 0.041 sec/batch; 1h:23m:45s remains)
INFO - root - 2019-11-06 18:33:40.459722: step 26600, total loss = 3.52, predict loss = 1.03 (97.7 examples/sec; 0.041 sec/batch; 1h:24m:11s remains)
INFO - root - 2019-11-06 18:33:40.950674: step 26610, total loss = 6.10, predict loss = 1.72 (92.0 examples/sec; 0.043 sec/batch; 1h:29m:27s remains)
INFO - root - 2019-11-06 18:33:42.223459: step 26620, total loss = 5.63, predict loss = 1.69 (66.2 examples/sec; 0.060 sec/batch; 2h:04m:11s remains)
INFO - root - 2019-11-06 18:33:42.980682: step 26630, total loss = 4.91, predict loss = 1.40 (54.9 examples/sec; 0.073 sec/batch; 2h:29m:42s remains)
INFO - root - 2019-11-06 18:33:43.749932: step 26640, total loss = 5.72, predict loss = 1.60 (55.6 examples/sec; 0.072 sec/batch; 2h:28m:01s remains)
INFO - root - 2019-11-06 18:33:44.493336: step 26650, total loss = 6.59, predict loss = 1.89 (58.4 examples/sec; 0.068 sec/batch; 2h:20m:49s remains)
INFO - root - 2019-11-06 18:33:45.226324: step 26660, total loss = 4.70, predict loss = 1.34 (62.0 examples/sec; 0.065 sec/batch; 2h:12m:40s remains)
INFO - root - 2019-11-06 18:33:45.726291: step 26670, total loss = 5.29, predict loss = 1.52 (89.0 examples/sec; 0.045 sec/batch; 1h:32m:23s remains)
INFO - root - 2019-11-06 18:33:46.181444: step 26680, total loss = 4.83, predict loss = 1.39 (101.7 examples/sec; 0.039 sec/batch; 1h:20m:51s remains)
INFO - root - 2019-11-06 18:33:47.383001: step 26690, total loss = 4.65, predict loss = 1.39 (71.8 examples/sec; 0.056 sec/batch; 1h:54m:32s remains)
INFO - root - 2019-11-06 18:33:48.097318: step 26700, total loss = 5.49, predict loss = 1.56 (56.2 examples/sec; 0.071 sec/batch; 2h:26m:09s remains)
INFO - root - 2019-11-06 18:33:48.852805: step 26710, total loss = 4.40, predict loss = 1.28 (57.5 examples/sec; 0.070 sec/batch; 2h:23m:02s remains)
INFO - root - 2019-11-06 18:33:49.593175: step 26720, total loss = 3.67, predict loss = 1.05 (56.6 examples/sec; 0.071 sec/batch; 2h:25m:16s remains)
INFO - root - 2019-11-06 18:33:50.348750: step 26730, total loss = 4.80, predict loss = 1.35 (60.8 examples/sec; 0.066 sec/batch; 2h:15m:05s remains)
INFO - root - 2019-11-06 18:33:50.906124: step 26740, total loss = 5.13, predict loss = 1.58 (98.2 examples/sec; 0.041 sec/batch; 1h:23m:39s remains)
INFO - root - 2019-11-06 18:33:51.366265: step 26750, total loss = 5.37, predict loss = 1.55 (91.9 examples/sec; 0.044 sec/batch; 1h:29m:25s remains)
INFO - root - 2019-11-06 18:33:51.811024: step 26760, total loss = 5.85, predict loss = 1.66 (93.2 examples/sec; 0.043 sec/batch; 1h:28m:06s remains)
INFO - root - 2019-11-06 18:33:53.283330: step 26770, total loss = 4.01, predict loss = 1.18 (59.0 examples/sec; 0.068 sec/batch; 2h:19m:11s remains)
INFO - root - 2019-11-06 18:33:54.031467: step 26780, total loss = 5.26, predict loss = 1.54 (60.0 examples/sec; 0.067 sec/batch; 2h:17m:01s remains)
INFO - root - 2019-11-06 18:33:54.799171: step 26790, total loss = 6.41, predict loss = 1.83 (54.9 examples/sec; 0.073 sec/batch; 2h:29m:37s remains)
INFO - root - 2019-11-06 18:33:55.518224: step 26800, total loss = 4.74, predict loss = 1.43 (68.9 examples/sec; 0.058 sec/batch; 1h:59m:13s remains)
INFO - root - 2019-11-06 18:33:56.185537: step 26810, total loss = 5.47, predict loss = 1.54 (74.3 examples/sec; 0.054 sec/batch; 1h:50m:34s remains)
INFO - root - 2019-11-06 18:33:56.645136: step 26820, total loss = 6.77, predict loss = 1.98 (93.1 examples/sec; 0.043 sec/batch; 1h:28m:10s remains)
INFO - root - 2019-11-06 18:33:57.096351: step 26830, total loss = 5.60, predict loss = 1.53 (95.8 examples/sec; 0.042 sec/batch; 1h:25m:43s remains)
INFO - root - 2019-11-06 18:33:58.309908: step 26840, total loss = 5.89, predict loss = 1.62 (68.5 examples/sec; 0.058 sec/batch; 1h:59m:50s remains)
INFO - root - 2019-11-06 18:33:59.054519: step 26850, total loss = 5.36, predict loss = 1.48 (56.7 examples/sec; 0.071 sec/batch; 2h:24m:46s remains)
INFO - root - 2019-11-06 18:33:59.836283: step 26860, total loss = 4.83, predict loss = 1.46 (57.2 examples/sec; 0.070 sec/batch; 2h:23m:34s remains)
INFO - root - 2019-11-06 18:34:00.598983: step 26870, total loss = 3.71, predict loss = 1.20 (55.3 examples/sec; 0.072 sec/batch; 2h:28m:27s remains)
INFO - root - 2019-11-06 18:34:01.404604: step 26880, total loss = 3.82, predict loss = 1.17 (59.2 examples/sec; 0.068 sec/batch; 2h:18m:40s remains)
INFO - root - 2019-11-06 18:34:01.994765: step 26890, total loss = 3.07, predict loss = 0.88 (100.1 examples/sec; 0.040 sec/batch; 1h:21m:59s remains)
INFO - root - 2019-11-06 18:34:02.451558: step 26900, total loss = 5.71, predict loss = 1.61 (91.9 examples/sec; 0.044 sec/batch; 1h:29m:17s remains)
INFO - root - 2019-11-06 18:34:02.902437: step 26910, total loss = 5.65, predict loss = 1.65 (123.8 examples/sec; 0.032 sec/batch; 1h:06m:16s remains)
INFO - root - 2019-11-06 18:34:04.262456: step 26920, total loss = 5.47, predict loss = 1.54 (52.4 examples/sec; 0.076 sec/batch; 2h:36m:36s remains)
INFO - root - 2019-11-06 18:34:05.010655: step 26930, total loss = 3.79, predict loss = 0.99 (57.0 examples/sec; 0.070 sec/batch; 2h:23m:49s remains)
INFO - root - 2019-11-06 18:34:05.808998: step 26940, total loss = 3.98, predict loss = 1.12 (58.6 examples/sec; 0.068 sec/batch; 2h:20m:05s remains)
INFO - root - 2019-11-06 18:34:06.587585: step 26950, total loss = 5.10, predict loss = 1.46 (58.8 examples/sec; 0.068 sec/batch; 2h:19m:26s remains)
INFO - root - 2019-11-06 18:34:07.317884: step 26960, total loss = 5.25, predict loss = 1.57 (66.9 examples/sec; 0.060 sec/batch; 2h:02m:33s remains)
INFO - root - 2019-11-06 18:34:07.792778: step 26970, total loss = 5.33, predict loss = 1.52 (95.6 examples/sec; 0.042 sec/batch; 1h:25m:50s remains)
INFO - root - 2019-11-06 18:34:08.239222: step 26980, total loss = 4.82, predict loss = 1.38 (95.7 examples/sec; 0.042 sec/batch; 1h:25m:43s remains)
INFO - root - 2019-11-06 18:34:09.451116: step 26990, total loss = 4.60, predict loss = 1.35 (62.2 examples/sec; 0.064 sec/batch; 2h:11m:46s remains)
INFO - root - 2019-11-06 18:34:10.187889: step 27000, total loss = 5.79, predict loss = 1.63 (54.2 examples/sec; 0.074 sec/batch; 2h:31m:17s remains)
INFO - root - 2019-11-06 18:34:10.933094: step 27010, total loss = 3.38, predict loss = 0.98 (58.6 examples/sec; 0.068 sec/batch; 2h:19m:49s remains)
INFO - root - 2019-11-06 18:34:11.634417: step 27020, total loss = 5.94, predict loss = 1.71 (67.6 examples/sec; 0.059 sec/batch; 2h:01m:13s remains)
INFO - root - 2019-11-06 18:34:12.313413: step 27030, total loss = 5.44, predict loss = 1.59 (69.0 examples/sec; 0.058 sec/batch; 1h:58m:47s remains)
INFO - root - 2019-11-06 18:34:12.829434: step 27040, total loss = 5.07, predict loss = 1.51 (100.7 examples/sec; 0.040 sec/batch; 1h:21m:26s remains)
INFO - root - 2019-11-06 18:34:13.319064: step 27050, total loss = 4.81, predict loss = 1.39 (90.8 examples/sec; 0.044 sec/batch; 1h:30m:15s remains)
INFO - root - 2019-11-06 18:34:14.398433: step 27060, total loss = 4.80, predict loss = 1.36 (5.8 examples/sec; 0.686 sec/batch; 23h:25m:38s remains)
INFO - root - 2019-11-06 18:34:15.077383: step 27070, total loss = 4.90, predict loss = 1.40 (62.0 examples/sec; 0.065 sec/batch; 2h:12m:12s remains)
INFO - root - 2019-11-06 18:34:15.821988: step 27080, total loss = 5.51, predict loss = 1.54 (63.3 examples/sec; 0.063 sec/batch; 2h:09m:32s remains)
INFO - root - 2019-11-06 18:34:16.562393: step 27090, total loss = 4.85, predict loss = 1.38 (57.8 examples/sec; 0.069 sec/batch; 2h:21m:41s remains)
INFO - root - 2019-11-06 18:34:17.356219: step 27100, total loss = 5.09, predict loss = 1.51 (52.1 examples/sec; 0.077 sec/batch; 2h:37m:15s remains)
INFO - root - 2019-11-06 18:34:17.991881: step 27110, total loss = 3.90, predict loss = 1.15 (87.9 examples/sec; 0.046 sec/batch; 1h:33m:15s remains)
INFO - root - 2019-11-06 18:34:18.448968: step 27120, total loss = 4.78, predict loss = 1.34 (96.2 examples/sec; 0.042 sec/batch; 1h:25m:11s remains)
INFO - root - 2019-11-06 18:34:18.925003: step 27130, total loss = 2.42, predict loss = 0.70 (96.5 examples/sec; 0.041 sec/batch; 1h:24m:51s remains)
INFO - root - 2019-11-06 18:34:20.127778: step 27140, total loss = 5.78, predict loss = 1.73 (70.6 examples/sec; 0.057 sec/batch; 1h:56m:00s remains)
INFO - root - 2019-11-06 18:34:20.852710: step 27150, total loss = 4.91, predict loss = 1.43 (60.3 examples/sec; 0.066 sec/batch; 2h:15m:45s remains)
INFO - root - 2019-11-06 18:34:21.579158: step 27160, total loss = 6.70, predict loss = 1.87 (61.2 examples/sec; 0.065 sec/batch; 2h:13m:51s remains)
INFO - root - 2019-11-06 18:34:22.438808: step 27170, total loss = 6.28, predict loss = 1.81 (67.5 examples/sec; 0.059 sec/batch; 2h:01m:21s remains)
INFO - root - 2019-11-06 18:34:23.151198: step 27180, total loss = 3.15, predict loss = 0.88 (65.5 examples/sec; 0.061 sec/batch; 2h:04m:57s remains)
INFO - root - 2019-11-06 18:34:23.694932: step 27190, total loss = 3.27, predict loss = 0.97 (99.6 examples/sec; 0.040 sec/batch; 1h:22m:14s remains)
INFO - root - 2019-11-06 18:34:24.143274: step 27200, total loss = 6.01, predict loss = 1.59 (88.5 examples/sec; 0.045 sec/batch; 1h:32m:31s remains)
INFO - root - 2019-11-06 18:34:25.282029: step 27210, total loss = 4.30, predict loss = 1.29 (74.8 examples/sec; 0.053 sec/batch; 1h:49m:24s remains)
INFO - root - 2019-11-06 18:34:26.018102: step 27220, total loss = 3.91, predict loss = 1.10 (50.8 examples/sec; 0.079 sec/batch; 2h:41m:07s remains)
INFO - root - 2019-11-06 18:34:26.801653: step 27230, total loss = 3.69, predict loss = 1.11 (56.8 examples/sec; 0.070 sec/batch; 2h:24m:13s remains)
INFO - root - 2019-11-06 18:34:27.678723: step 27240, total loss = 3.98, predict loss = 1.12 (53.3 examples/sec; 0.075 sec/batch; 2h:33m:39s remains)
INFO - root - 2019-11-06 18:34:28.454846: step 27250, total loss = 4.57, predict loss = 1.38 (63.0 examples/sec; 0.064 sec/batch; 2h:09m:55s remains)
INFO - root - 2019-11-06 18:34:29.098219: step 27260, total loss = 5.34, predict loss = 1.61 (94.4 examples/sec; 0.042 sec/batch; 1h:26m:38s remains)
INFO - root - 2019-11-06 18:34:29.570965: step 27270, total loss = 5.53, predict loss = 1.60 (87.3 examples/sec; 0.046 sec/batch; 1h:33m:44s remains)
INFO - root - 2019-11-06 18:34:30.010943: step 27280, total loss = 4.88, predict loss = 1.46 (101.1 examples/sec; 0.040 sec/batch; 1h:20m:53s remains)
INFO - root - 2019-11-06 18:34:31.264810: step 27290, total loss = 5.58, predict loss = 1.65 (60.9 examples/sec; 0.066 sec/batch; 2h:14m:24s remains)
INFO - root - 2019-11-06 18:34:31.947880: step 27300, total loss = 4.85, predict loss = 1.47 (59.9 examples/sec; 0.067 sec/batch; 2h:16m:37s remains)
INFO - root - 2019-11-06 18:34:32.680956: step 27310, total loss = 5.83, predict loss = 1.76 (64.9 examples/sec; 0.062 sec/batch; 2h:06m:05s remains)
INFO - root - 2019-11-06 18:34:33.423766: step 27320, total loss = 5.58, predict loss = 1.62 (55.0 examples/sec; 0.073 sec/batch; 2h:28m:47s remains)
INFO - root - 2019-11-06 18:34:34.122789: step 27330, total loss = 4.48, predict loss = 1.24 (71.9 examples/sec; 0.056 sec/batch; 1h:53m:44s remains)
INFO - root - 2019-11-06 18:34:34.629992: step 27340, total loss = 3.99, predict loss = 1.07 (92.7 examples/sec; 0.043 sec/batch; 1h:28m:14s remains)
INFO - root - 2019-11-06 18:34:35.082314: step 27350, total loss = 4.40, predict loss = 1.26 (95.7 examples/sec; 0.042 sec/batch; 1h:25m:29s remains)
INFO - root - 2019-11-06 18:34:36.227153: step 27360, total loss = 5.60, predict loss = 1.56 (73.7 examples/sec; 0.054 sec/batch; 1h:50m:53s remains)
INFO - root - 2019-11-06 18:34:36.965024: step 27370, total loss = 4.07, predict loss = 1.24 (53.9 examples/sec; 0.074 sec/batch; 2h:31m:34s remains)
INFO - root - 2019-11-06 18:34:37.730394: step 27380, total loss = 5.72, predict loss = 1.68 (55.1 examples/sec; 0.073 sec/batch; 2h:28m:19s remains)
INFO - root - 2019-11-06 18:34:38.467653: step 27390, total loss = 5.00, predict loss = 1.33 (60.4 examples/sec; 0.066 sec/batch; 2h:15m:25s remains)
INFO - root - 2019-11-06 18:34:39.219110: step 27400, total loss = 5.75, predict loss = 1.67 (54.6 examples/sec; 0.073 sec/batch; 2h:29m:38s remains)
INFO - root - 2019-11-06 18:34:39.869177: step 27410, total loss = 5.09, predict loss = 1.49 (91.1 examples/sec; 0.044 sec/batch; 1h:29m:44s remains)
INFO - root - 2019-11-06 18:34:40.321749: step 27420, total loss = 5.30, predict loss = 1.52 (100.1 examples/sec; 0.040 sec/batch; 1h:21m:36s remains)
INFO - root - 2019-11-06 18:34:40.776823: step 27430, total loss = 3.31, predict loss = 0.96 (89.1 examples/sec; 0.045 sec/batch; 1h:31m:41s remains)
INFO - root - 2019-11-06 18:34:42.060470: step 27440, total loss = 4.06, predict loss = 1.17 (61.1 examples/sec; 0.065 sec/batch; 2h:13m:43s remains)
INFO - root - 2019-11-06 18:34:42.827646: step 27450, total loss = 5.40, predict loss = 1.57 (52.1 examples/sec; 0.077 sec/batch; 2h:36m:50s remains)
INFO - root - 2019-11-06 18:34:43.551989: step 27460, total loss = 3.91, predict loss = 1.27 (60.2 examples/sec; 0.066 sec/batch; 2h:15m:43s remains)
INFO - root - 2019-11-06 18:34:44.277229: step 27470, total loss = 6.39, predict loss = 1.86 (62.4 examples/sec; 0.064 sec/batch; 2h:10m:58s remains)
INFO - root - 2019-11-06 18:34:44.983997: step 27480, total loss = 6.05, predict loss = 1.86 (71.7 examples/sec; 0.056 sec/batch; 1h:53m:58s remains)
INFO - root - 2019-11-06 18:34:45.481730: step 27490, total loss = 4.78, predict loss = 1.49 (98.9 examples/sec; 0.040 sec/batch; 1h:22m:35s remains)
INFO - root - 2019-11-06 18:34:45.925102: step 27500, total loss = 5.96, predict loss = 1.76 (95.5 examples/sec; 0.042 sec/batch; 1h:25m:30s remains)
INFO - root - 2019-11-06 18:34:47.108128: step 27510, total loss = 3.73, predict loss = 1.15 (64.7 examples/sec; 0.062 sec/batch; 2h:06m:12s remains)
INFO - root - 2019-11-06 18:34:47.817308: step 27520, total loss = 3.13, predict loss = 0.95 (61.0 examples/sec; 0.066 sec/batch; 2h:13m:55s remains)
INFO - root - 2019-11-06 18:34:48.536857: step 27530, total loss = 5.90, predict loss = 1.68 (63.6 examples/sec; 0.063 sec/batch; 2h:08m:20s remains)
INFO - root - 2019-11-06 18:34:49.236912: step 27540, total loss = 5.05, predict loss = 1.47 (63.2 examples/sec; 0.063 sec/batch; 2h:09m:08s remains)
INFO - root - 2019-11-06 18:34:49.969948: step 27550, total loss = 3.96, predict loss = 1.14 (61.2 examples/sec; 0.065 sec/batch; 2h:13m:17s remains)
INFO - root - 2019-11-06 18:34:50.558762: step 27560, total loss = 5.55, predict loss = 1.52 (90.4 examples/sec; 0.044 sec/batch; 1h:30m:18s remains)
INFO - root - 2019-11-06 18:34:51.052374: step 27570, total loss = 4.31, predict loss = 1.21 (94.4 examples/sec; 0.042 sec/batch; 1h:26m:26s remains)
INFO - root - 2019-11-06 18:34:51.519538: step 27580, total loss = 4.52, predict loss = 1.28 (90.7 examples/sec; 0.044 sec/batch; 1h:29m:56s remains)
INFO - root - 2019-11-06 18:34:52.929775: step 27590, total loss = 4.41, predict loss = 1.29 (62.0 examples/sec; 0.064 sec/batch; 2h:11m:31s remains)
INFO - root - 2019-11-06 18:34:53.656525: step 27600, total loss = 3.92, predict loss = 1.17 (58.5 examples/sec; 0.068 sec/batch; 2h:19m:23s remains)
INFO - root - 2019-11-06 18:34:54.410453: step 27610, total loss = 3.12, predict loss = 0.88 (67.0 examples/sec; 0.060 sec/batch; 2h:01m:45s remains)
INFO - root - 2019-11-06 18:34:55.191368: step 27620, total loss = 6.08, predict loss = 1.73 (58.3 examples/sec; 0.069 sec/batch; 2h:19m:52s remains)
INFO - root - 2019-11-06 18:34:55.893007: step 27630, total loss = 3.57, predict loss = 1.02 (65.3 examples/sec; 0.061 sec/batch; 2h:04m:51s remains)
INFO - root - 2019-11-06 18:34:56.375249: step 27640, total loss = 5.03, predict loss = 1.42 (89.9 examples/sec; 0.044 sec/batch; 1h:30m:43s remains)
INFO - root - 2019-11-06 18:34:56.845486: step 27650, total loss = 5.80, predict loss = 1.79 (95.7 examples/sec; 0.042 sec/batch; 1h:25m:13s remains)
INFO - root - 2019-11-06 18:34:58.047488: step 27660, total loss = 5.92, predict loss = 1.70 (70.9 examples/sec; 0.056 sec/batch; 1h:55m:01s remains)
INFO - root - 2019-11-06 18:34:58.796022: step 27670, total loss = 6.02, predict loss = 1.74 (59.3 examples/sec; 0.067 sec/batch; 2h:17m:27s remains)
INFO - root - 2019-11-06 18:34:59.529940: step 27680, total loss = 5.72, predict loss = 1.60 (67.1 examples/sec; 0.060 sec/batch; 2h:01m:34s remains)
INFO - root - 2019-11-06 18:35:00.250365: step 27690, total loss = 5.52, predict loss = 1.60 (62.8 examples/sec; 0.064 sec/batch; 2h:09m:45s remains)
INFO - root - 2019-11-06 18:35:00.995774: step 27700, total loss = 5.93, predict loss = 1.74 (55.1 examples/sec; 0.073 sec/batch; 2h:27m:54s remains)
INFO - root - 2019-11-06 18:35:01.561655: step 27710, total loss = 4.70, predict loss = 1.36 (96.4 examples/sec; 0.041 sec/batch; 1h:24m:33s remains)
INFO - root - 2019-11-06 18:35:02.016869: step 27720, total loss = 4.39, predict loss = 1.25 (94.9 examples/sec; 0.042 sec/batch; 1h:25m:54s remains)
INFO - root - 2019-11-06 18:35:02.480726: step 27730, total loss = 6.10, predict loss = 1.75 (137.0 examples/sec; 0.029 sec/batch; 0h:59m:30s remains)
INFO - root - 2019-11-06 18:35:03.855158: step 27740, total loss = 6.10, predict loss = 1.78 (57.8 examples/sec; 0.069 sec/batch; 2h:20m:56s remains)
INFO - root - 2019-11-06 18:35:04.564159: step 27750, total loss = 4.71, predict loss = 1.43 (54.1 examples/sec; 0.074 sec/batch; 2h:30m:45s remains)
INFO - root - 2019-11-06 18:35:05.300198: step 27760, total loss = 4.80, predict loss = 1.38 (63.4 examples/sec; 0.063 sec/batch; 2h:08m:27s remains)
INFO - root - 2019-11-06 18:35:06.092069: step 27770, total loss = 2.61, predict loss = 0.75 (57.9 examples/sec; 0.069 sec/batch; 2h:20m:37s remains)
INFO - root - 2019-11-06 18:35:06.814558: step 27780, total loss = 5.87, predict loss = 1.67 (69.0 examples/sec; 0.058 sec/batch; 1h:58m:03s remains)
INFO - root - 2019-11-06 18:35:07.274694: step 27790, total loss = 5.30, predict loss = 1.39 (97.1 examples/sec; 0.041 sec/batch; 1h:23m:55s remains)
INFO - root - 2019-11-06 18:35:07.741278: step 27800, total loss = 5.07, predict loss = 1.39 (91.2 examples/sec; 0.044 sec/batch; 1h:29m:19s remains)
INFO - root - 2019-11-06 18:35:08.953590: step 27810, total loss = 3.59, predict loss = 0.97 (66.1 examples/sec; 0.061 sec/batch; 2h:03m:18s remains)
INFO - root - 2019-11-06 18:35:09.680986: step 27820, total loss = 2.82, predict loss = 0.76 (59.3 examples/sec; 0.067 sec/batch; 2h:17m:18s remains)
INFO - root - 2019-11-06 18:35:10.370749: step 27830, total loss = 5.45, predict loss = 1.48 (71.4 examples/sec; 0.056 sec/batch; 1h:54m:01s remains)
INFO - root - 2019-11-06 18:35:11.167356: step 27840, total loss = 4.97, predict loss = 1.38 (56.8 examples/sec; 0.070 sec/batch; 2h:23m:28s remains)
INFO - root - 2019-11-06 18:35:11.910105: step 27850, total loss = 4.31, predict loss = 1.27 (62.4 examples/sec; 0.064 sec/batch; 2h:10m:33s remains)
INFO - root - 2019-11-06 18:35:12.445282: step 27860, total loss = 4.31, predict loss = 1.22 (96.9 examples/sec; 0.041 sec/batch; 1h:24m:03s remains)
INFO - root - 2019-11-06 18:35:12.896997: step 27870, total loss = 5.12, predict loss = 1.43 (102.2 examples/sec; 0.039 sec/batch; 1h:19m:39s remains)
INFO - root - 2019-11-06 18:35:14.001091: step 27880, total loss = 6.23, predict loss = 1.80 (5.7 examples/sec; 0.703 sec/batch; 23h:49m:58s remains)
INFO - root - 2019-11-06 18:35:14.717165: step 27890, total loss = 4.68, predict loss = 1.37 (60.9 examples/sec; 0.066 sec/batch; 2h:13m:43s remains)
INFO - root - 2019-11-06 18:35:15.447012: step 27900, total loss = 5.31, predict loss = 1.51 (57.0 examples/sec; 0.070 sec/batch; 2h:22m:45s remains)
INFO - root - 2019-11-06 18:35:16.182956: step 27910, total loss = 3.36, predict loss = 0.97 (57.6 examples/sec; 0.069 sec/batch; 2h:21m:21s remains)
INFO - root - 2019-11-06 18:35:16.939559: step 27920, total loss = 4.82, predict loss = 1.37 (58.0 examples/sec; 0.069 sec/batch; 2h:20m:15s remains)
INFO - root - 2019-11-06 18:35:17.554949: step 27930, total loss = 4.60, predict loss = 1.37 (98.5 examples/sec; 0.041 sec/batch; 1h:22m:36s remains)
INFO - root - 2019-11-06 18:35:17.999803: step 27940, total loss = 4.47, predict loss = 1.29 (101.6 examples/sec; 0.039 sec/batch; 1h:20m:06s remains)
INFO - root - 2019-11-06 18:35:18.472238: step 27950, total loss = 4.22, predict loss = 1.22 (93.4 examples/sec; 0.043 sec/batch; 1h:27m:05s remains)
INFO - root - 2019-11-06 18:35:19.780974: step 27960, total loss = 6.22, predict loss = 1.83 (55.5 examples/sec; 0.072 sec/batch; 2h:26m:37s remains)
INFO - root - 2019-11-06 18:35:20.531647: step 27970, total loss = 5.19, predict loss = 1.53 (56.3 examples/sec; 0.071 sec/batch; 2h:24m:29s remains)
INFO - root - 2019-11-06 18:35:21.354586: step 27980, total loss = 5.19, predict loss = 1.43 (49.8 examples/sec; 0.080 sec/batch; 2h:43m:19s remains)
INFO - root - 2019-11-06 18:35:22.204929: step 27990, total loss = 4.72, predict loss = 1.36 (60.6 examples/sec; 0.066 sec/batch; 2h:14m:14s remains)
INFO - root - 2019-11-06 18:35:22.870542: step 28000, total loss = 5.50, predict loss = 1.62 (67.6 examples/sec; 0.059 sec/batch; 2h:00m:24s remains)
INFO - root - 2019-11-06 18:35:23.399215: step 28010, total loss = 5.93, predict loss = 1.69 (99.9 examples/sec; 0.040 sec/batch; 1h:21m:24s remains)
INFO - root - 2019-11-06 18:35:23.864232: step 28020, total loss = 4.20, predict loss = 1.17 (92.6 examples/sec; 0.043 sec/batch; 1h:27m:50s remains)
INFO - root - 2019-11-06 18:35:25.034721: step 28030, total loss = 4.55, predict loss = 1.34 (69.3 examples/sec; 0.058 sec/batch; 1h:57m:19s remains)
INFO - root - 2019-11-06 18:35:25.726425: step 28040, total loss = 3.25, predict loss = 0.93 (49.7 examples/sec; 0.080 sec/batch; 2h:43m:26s remains)
INFO - root - 2019-11-06 18:35:26.498202: step 28050, total loss = 5.77, predict loss = 1.72 (60.0 examples/sec; 0.067 sec/batch; 2h:15m:29s remains)
INFO - root - 2019-11-06 18:35:27.212784: step 28060, total loss = 4.85, predict loss = 1.37 (59.8 examples/sec; 0.067 sec/batch; 2h:15m:57s remains)
INFO - root - 2019-11-06 18:35:27.970222: step 28070, total loss = 5.47, predict loss = 1.47 (62.1 examples/sec; 0.064 sec/batch; 2h:10m:56s remains)
INFO - root - 2019-11-06 18:35:28.603000: step 28080, total loss = 6.47, predict loss = 1.92 (92.6 examples/sec; 0.043 sec/batch; 1h:27m:45s remains)
INFO - root - 2019-11-06 18:35:29.088646: step 28090, total loss = 6.97, predict loss = 2.04 (103.9 examples/sec; 0.039 sec/batch; 1h:18m:15s remains)
INFO - root - 2019-11-06 18:35:29.534967: step 28100, total loss = 4.06, predict loss = 1.13 (96.7 examples/sec; 0.041 sec/batch; 1h:24m:03s remains)
INFO - root - 2019-11-06 18:35:30.831210: step 28110, total loss = 6.09, predict loss = 1.76 (62.7 examples/sec; 0.064 sec/batch; 2h:09m:34s remains)
INFO - root - 2019-11-06 18:35:31.569507: step 28120, total loss = 5.67, predict loss = 1.55 (56.1 examples/sec; 0.071 sec/batch; 2h:24m:48s remains)
INFO - root - 2019-11-06 18:35:32.303504: step 28130, total loss = 4.48, predict loss = 1.25 (64.7 examples/sec; 0.062 sec/batch; 2h:05m:31s remains)
INFO - root - 2019-11-06 18:35:33.051744: step 28140, total loss = 6.16, predict loss = 1.79 (60.4 examples/sec; 0.066 sec/batch; 2h:14m:31s remains)
INFO - root - 2019-11-06 18:35:33.742913: step 28150, total loss = 2.93, predict loss = 0.89 (79.9 examples/sec; 0.050 sec/batch; 1h:41m:40s remains)
INFO - root - 2019-11-06 18:35:34.236724: step 28160, total loss = 5.07, predict loss = 1.46 (96.8 examples/sec; 0.041 sec/batch; 1h:23m:56s remains)
INFO - root - 2019-11-06 18:35:34.729557: step 28170, total loss = 4.77, predict loss = 1.40 (89.5 examples/sec; 0.045 sec/batch; 1h:30m:45s remains)
INFO - root - 2019-11-06 18:35:35.891277: step 28180, total loss = 6.27, predict loss = 1.73 (67.3 examples/sec; 0.059 sec/batch; 2h:00m:36s remains)
INFO - root - 2019-11-06 18:35:36.612644: step 28190, total loss = 5.77, predict loss = 1.62 (55.5 examples/sec; 0.072 sec/batch; 2h:26m:26s remains)
INFO - root - 2019-11-06 18:35:37.393868: step 28200, total loss = 5.84, predict loss = 1.64 (58.7 examples/sec; 0.068 sec/batch; 2h:18m:20s remains)
INFO - root - 2019-11-06 18:35:38.111579: step 28210, total loss = 5.30, predict loss = 1.50 (61.9 examples/sec; 0.065 sec/batch; 2h:11m:03s remains)
INFO - root - 2019-11-06 18:35:38.803384: step 28220, total loss = 4.36, predict loss = 1.30 (55.4 examples/sec; 0.072 sec/batch; 2h:26m:25s remains)
INFO - root - 2019-11-06 18:35:39.447274: step 28230, total loss = 4.48, predict loss = 1.24 (95.0 examples/sec; 0.042 sec/batch; 1h:25m:29s remains)
INFO - root - 2019-11-06 18:35:39.886176: step 28240, total loss = 3.25, predict loss = 0.92 (97.4 examples/sec; 0.041 sec/batch; 1h:23m:19s remains)
INFO - root - 2019-11-06 18:35:40.365027: step 28250, total loss = 5.86, predict loss = 1.67 (95.5 examples/sec; 0.042 sec/batch; 1h:24m:59s remains)
INFO - root - 2019-11-06 18:35:41.702473: step 28260, total loss = 4.42, predict loss = 1.20 (58.0 examples/sec; 0.069 sec/batch; 2h:19m:52s remains)
INFO - root - 2019-11-06 18:35:42.448165: step 28270, total loss = 5.72, predict loss = 1.72 (55.2 examples/sec; 0.072 sec/batch; 2h:26m:59s remains)
INFO - root - 2019-11-06 18:35:43.182620: step 28280, total loss = 5.56, predict loss = 1.51 (54.0 examples/sec; 0.074 sec/batch; 2h:30m:23s remains)
INFO - root - 2019-11-06 18:35:43.933841: step 28290, total loss = 5.03, predict loss = 1.42 (58.9 examples/sec; 0.068 sec/batch; 2h:17m:48s remains)
INFO - root - 2019-11-06 18:35:44.654143: step 28300, total loss = 5.37, predict loss = 1.61 (62.8 examples/sec; 0.064 sec/batch; 2h:09m:15s remains)
INFO - root - 2019-11-06 18:35:45.169610: step 28310, total loss = 4.21, predict loss = 1.25 (99.8 examples/sec; 0.040 sec/batch; 1h:21m:15s remains)
INFO - root - 2019-11-06 18:35:45.618846: step 28320, total loss = 6.46, predict loss = 1.80 (95.5 examples/sec; 0.042 sec/batch; 1h:24m:55s remains)
INFO - root - 2019-11-06 18:35:46.879707: step 28330, total loss = 6.01, predict loss = 1.65 (63.9 examples/sec; 0.063 sec/batch; 2h:06m:50s remains)
INFO - root - 2019-11-06 18:35:47.550517: step 28340, total loss = 4.80, predict loss = 1.40 (57.4 examples/sec; 0.070 sec/batch; 2h:21m:15s remains)
INFO - root - 2019-11-06 18:35:48.260121: step 28350, total loss = 3.43, predict loss = 0.95 (58.2 examples/sec; 0.069 sec/batch; 2h:19m:25s remains)
INFO - root - 2019-11-06 18:35:48.982429: step 28360, total loss = 4.37, predict loss = 1.25 (62.9 examples/sec; 0.064 sec/batch; 2h:08m:56s remains)
INFO - root - 2019-11-06 18:35:49.764971: step 28370, total loss = 4.77, predict loss = 1.40 (55.3 examples/sec; 0.072 sec/batch; 2h:26m:36s remains)
INFO - root - 2019-11-06 18:35:50.358273: step 28380, total loss = 3.53, predict loss = 1.02 (97.5 examples/sec; 0.041 sec/batch; 1h:23m:11s remains)
INFO - root - 2019-11-06 18:35:50.807421: step 28390, total loss = 5.53, predict loss = 1.60 (97.1 examples/sec; 0.041 sec/batch; 1h:23m:29s remains)
INFO - root - 2019-11-06 18:35:51.248768: step 28400, total loss = 5.00, predict loss = 1.42 (92.8 examples/sec; 0.043 sec/batch; 1h:27m:23s remains)
INFO - root - 2019-11-06 18:35:52.663362: step 28410, total loss = 5.52, predict loss = 1.48 (63.5 examples/sec; 0.063 sec/batch; 2h:07m:43s remains)
INFO - root - 2019-11-06 18:35:53.383694: step 28420, total loss = 3.32, predict loss = 0.97 (62.2 examples/sec; 0.064 sec/batch; 2h:10m:13s remains)
INFO - root - 2019-11-06 18:35:54.105005: step 28430, total loss = 4.84, predict loss = 1.38 (59.4 examples/sec; 0.067 sec/batch; 2h:16m:32s remains)
INFO - root - 2019-11-06 18:35:54.852262: step 28440, total loss = 5.62, predict loss = 1.65 (61.2 examples/sec; 0.065 sec/batch; 2h:12m:28s remains)
INFO - root - 2019-11-06 18:35:55.601746: step 28450, total loss = 4.35, predict loss = 1.27 (69.1 examples/sec; 0.058 sec/batch; 1h:57m:14s remains)
INFO - root - 2019-11-06 18:35:56.093382: step 28460, total loss = 4.42, predict loss = 1.28 (91.0 examples/sec; 0.044 sec/batch; 1h:29m:03s remains)
INFO - root - 2019-11-06 18:35:56.552087: step 28470, total loss = 4.78, predict loss = 1.42 (94.0 examples/sec; 0.043 sec/batch; 1h:26m:14s remains)
INFO - root - 2019-11-06 18:35:57.766226: step 28480, total loss = 4.83, predict loss = 1.34 (69.8 examples/sec; 0.057 sec/batch; 1h:56m:00s remains)
INFO - root - 2019-11-06 18:35:58.504305: step 28490, total loss = 4.29, predict loss = 1.31 (58.8 examples/sec; 0.068 sec/batch; 2h:17m:46s remains)
INFO - root - 2019-11-06 18:35:59.236786: step 28500, total loss = 6.68, predict loss = 1.87 (53.5 examples/sec; 0.075 sec/batch; 2h:31m:29s remains)
INFO - root - 2019-11-06 18:35:59.997086: step 28510, total loss = 4.94, predict loss = 1.38 (55.0 examples/sec; 0.073 sec/batch; 2h:27m:21s remains)
INFO - root - 2019-11-06 18:36:00.715045: step 28520, total loss = 5.12, predict loss = 1.44 (59.8 examples/sec; 0.067 sec/batch; 2h:15m:24s remains)
INFO - root - 2019-11-06 18:36:01.322115: step 28530, total loss = 4.97, predict loss = 1.38 (90.0 examples/sec; 0.044 sec/batch; 1h:29m:56s remains)
INFO - root - 2019-11-06 18:36:01.765991: step 28540, total loss = 4.77, predict loss = 1.34 (97.5 examples/sec; 0.041 sec/batch; 1h:23m:05s remains)
INFO - root - 2019-11-06 18:36:02.210490: step 28550, total loss = 6.49, predict loss = 1.79 (123.5 examples/sec; 0.032 sec/batch; 1h:05m:32s remains)
INFO - root - 2019-11-06 18:36:03.543430: step 28560, total loss = 4.16, predict loss = 1.22 (61.4 examples/sec; 0.065 sec/batch; 2h:11m:48s remains)
INFO - root - 2019-11-06 18:36:04.307124: step 28570, total loss = 4.57, predict loss = 1.39 (64.3 examples/sec; 0.062 sec/batch; 2h:05m:53s remains)
INFO - root - 2019-11-06 18:36:05.082427: step 28580, total loss = 5.50, predict loss = 1.60 (56.7 examples/sec; 0.071 sec/batch; 2h:22m:48s remains)
INFO - root - 2019-11-06 18:36:05.859953: step 28590, total loss = 6.12, predict loss = 1.79 (59.5 examples/sec; 0.067 sec/batch; 2h:15m:59s remains)
INFO - root - 2019-11-06 18:36:06.496328: step 28600, total loss = 5.73, predict loss = 1.67 (86.0 examples/sec; 0.047 sec/batch; 1h:34m:09s remains)
INFO - root - 2019-11-06 18:36:06.971642: step 28610, total loss = 5.49, predict loss = 1.60 (93.6 examples/sec; 0.043 sec/batch; 1h:26m:29s remains)
INFO - root - 2019-11-06 18:36:07.423208: step 28620, total loss = 4.33, predict loss = 1.27 (94.4 examples/sec; 0.042 sec/batch; 1h:25m:43s remains)
INFO - root - 2019-11-06 18:36:08.633540: step 28630, total loss = 5.50, predict loss = 1.52 (66.4 examples/sec; 0.060 sec/batch; 2h:01m:47s remains)
INFO - root - 2019-11-06 18:36:09.402369: step 28640, total loss = 4.69, predict loss = 1.37 (55.1 examples/sec; 0.073 sec/batch; 2h:26m:43s remains)
INFO - root - 2019-11-06 18:36:10.131251: step 28650, total loss = 4.98, predict loss = 1.47 (59.4 examples/sec; 0.067 sec/batch; 2h:16m:16s remains)
INFO - root - 2019-11-06 18:36:10.875635: step 28660, total loss = 4.91, predict loss = 1.41 (57.9 examples/sec; 0.069 sec/batch; 2h:19m:37s remains)
INFO - root - 2019-11-06 18:36:11.717484: step 28670, total loss = 5.61, predict loss = 1.57 (57.7 examples/sec; 0.069 sec/batch; 2h:20m:07s remains)
INFO - root - 2019-11-06 18:36:12.295101: step 28680, total loss = 6.80, predict loss = 1.97 (100.4 examples/sec; 0.040 sec/batch; 1h:20m:31s remains)
INFO - root - 2019-11-06 18:36:12.781417: step 28690, total loss = 5.72, predict loss = 1.57 (95.7 examples/sec; 0.042 sec/batch; 1h:24m:30s remains)
INFO - root - 2019-11-06 18:36:13.920643: step 28700, total loss = 5.88, predict loss = 1.75 (5.4 examples/sec; 0.739 sec/batch; 24h:54m:41s remains)
INFO - root - 2019-11-06 18:36:14.613376: step 28710, total loss = 5.84, predict loss = 1.68 (60.0 examples/sec; 0.067 sec/batch; 2h:14m:52s remains)
INFO - root - 2019-11-06 18:36:15.356008: step 28720, total loss = 3.91, predict loss = 1.23 (56.2 examples/sec; 0.071 sec/batch; 2h:23m:50s remains)
INFO - root - 2019-11-06 18:36:16.109299: step 28730, total loss = 5.77, predict loss = 1.63 (58.4 examples/sec; 0.068 sec/batch; 2h:18m:21s remains)
INFO - root - 2019-11-06 18:36:16.887637: step 28740, total loss = 4.34, predict loss = 1.24 (59.8 examples/sec; 0.067 sec/batch; 2h:15m:13s remains)
INFO - root - 2019-11-06 18:36:17.500035: step 28750, total loss = 5.80, predict loss = 1.68 (98.8 examples/sec; 0.040 sec/batch; 1h:21m:47s remains)
INFO - root - 2019-11-06 18:36:17.963979: step 28760, total loss = 4.56, predict loss = 1.33 (96.5 examples/sec; 0.041 sec/batch; 1h:23m:47s remains)
INFO - root - 2019-11-06 18:36:18.449719: step 28770, total loss = 5.52, predict loss = 1.55 (96.5 examples/sec; 0.041 sec/batch; 1h:23m:44s remains)
INFO - root - 2019-11-06 18:36:19.706459: step 28780, total loss = 4.86, predict loss = 1.49 (64.5 examples/sec; 0.062 sec/batch; 2h:05m:12s remains)
INFO - root - 2019-11-06 18:36:20.417353: step 28790, total loss = 6.18, predict loss = 1.77 (61.3 examples/sec; 0.065 sec/batch; 2h:11m:47s remains)
INFO - root - 2019-11-06 18:36:21.171418: step 28800, total loss = 4.87, predict loss = 1.38 (60.6 examples/sec; 0.066 sec/batch; 2h:13m:22s remains)
INFO - root - 2019-11-06 18:36:21.948666: step 28810, total loss = 3.93, predict loss = 1.12 (47.6 examples/sec; 0.084 sec/batch; 2h:49m:51s remains)
INFO - root - 2019-11-06 18:36:22.655422: step 28820, total loss = 5.43, predict loss = 1.57 (64.6 examples/sec; 0.062 sec/batch; 2h:05m:08s remains)
INFO - root - 2019-11-06 18:36:23.225131: step 28830, total loss = 4.76, predict loss = 1.37 (99.2 examples/sec; 0.040 sec/batch; 1h:21m:24s remains)
INFO - root - 2019-11-06 18:36:23.659566: step 28840, total loss = 2.84, predict loss = 0.79 (99.5 examples/sec; 0.040 sec/batch; 1h:21m:09s remains)
INFO - root - 2019-11-06 18:36:24.794248: step 28850, total loss = 5.40, predict loss = 1.55 (67.7 examples/sec; 0.059 sec/batch; 1h:59m:14s remains)
INFO - root - 2019-11-06 18:36:25.473333: step 28860, total loss = 6.00, predict loss = 1.76 (63.7 examples/sec; 0.063 sec/batch; 2h:06m:50s remains)
INFO - root - 2019-11-06 18:36:26.207472: step 28870, total loss = 4.37, predict loss = 1.32 (63.4 examples/sec; 0.063 sec/batch; 2h:07m:20s remains)
INFO - root - 2019-11-06 18:36:26.965792: step 28880, total loss = 5.00, predict loss = 1.52 (56.2 examples/sec; 0.071 sec/batch; 2h:23m:40s remains)
INFO - root - 2019-11-06 18:36:27.700411: step 28890, total loss = 5.49, predict loss = 1.52 (60.1 examples/sec; 0.067 sec/batch; 2h:14m:22s remains)
INFO - root - 2019-11-06 18:36:28.342360: step 28900, total loss = 5.58, predict loss = 1.54 (90.2 examples/sec; 0.044 sec/batch; 1h:29m:32s remains)
INFO - root - 2019-11-06 18:36:28.796825: step 28910, total loss = 5.49, predict loss = 1.74 (96.3 examples/sec; 0.042 sec/batch; 1h:23m:49s remains)
INFO - root - 2019-11-06 18:36:29.244295: step 28920, total loss = 6.16, predict loss = 1.72 (97.0 examples/sec; 0.041 sec/batch; 1h:23m:13s remains)
INFO - root - 2019-11-06 18:36:30.567187: step 28930, total loss = 6.02, predict loss = 1.78 (66.5 examples/sec; 0.060 sec/batch; 2h:01m:19s remains)
INFO - root - 2019-11-06 18:36:31.292225: step 28940, total loss = 6.77, predict loss = 1.98 (54.5 examples/sec; 0.073 sec/batch; 2h:28m:03s remains)
INFO - root - 2019-11-06 18:36:32.031723: step 28950, total loss = 5.22, predict loss = 1.57 (55.7 examples/sec; 0.072 sec/batch; 2h:25m:00s remains)
INFO - root - 2019-11-06 18:36:32.759388: step 28960, total loss = 4.52, predict loss = 1.31 (56.9 examples/sec; 0.070 sec/batch; 2h:21m:44s remains)
INFO - root - 2019-11-06 18:36:33.491179: step 28970, total loss = 4.37, predict loss = 1.27 (67.7 examples/sec; 0.059 sec/batch; 1h:59m:10s remains)
INFO - root - 2019-11-06 18:36:34.024418: step 28980, total loss = 4.29, predict loss = 1.24 (89.6 examples/sec; 0.045 sec/batch; 1h:30m:01s remains)
INFO - root - 2019-11-06 18:36:34.476706: step 28990, total loss = 5.41, predict loss = 1.61 (94.6 examples/sec; 0.042 sec/batch; 1h:25m:16s remains)
INFO - root - 2019-11-06 18:36:35.619964: step 29000, total loss = 5.61, predict loss = 1.54 (69.3 examples/sec; 0.058 sec/batch; 1h:56m:25s remains)
INFO - root - 2019-11-06 18:36:36.337075: step 29010, total loss = 4.46, predict loss = 1.33 (60.4 examples/sec; 0.066 sec/batch; 2h:13m:30s remains)
INFO - root - 2019-11-06 18:36:37.051083: step 29020, total loss = 3.89, predict loss = 1.18 (57.9 examples/sec; 0.069 sec/batch; 2h:19m:12s remains)
INFO - root - 2019-11-06 18:36:37.815213: step 29030, total loss = 5.28, predict loss = 1.48 (56.3 examples/sec; 0.071 sec/batch; 2h:23m:10s remains)
INFO - root - 2019-11-06 18:36:38.573213: step 29040, total loss = 4.46, predict loss = 1.35 (53.5 examples/sec; 0.075 sec/batch; 2h:30m:44s remains)
INFO - root - 2019-11-06 18:36:39.232312: step 29050, total loss = 4.95, predict loss = 1.41 (96.2 examples/sec; 0.042 sec/batch; 1h:23m:50s remains)
INFO - root - 2019-11-06 18:36:39.693531: step 29060, total loss = 5.80, predict loss = 1.63 (94.7 examples/sec; 0.042 sec/batch; 1h:25m:09s remains)
INFO - root - 2019-11-06 18:36:40.145129: step 29070, total loss = 5.83, predict loss = 1.67 (94.4 examples/sec; 0.042 sec/batch; 1h:25m:24s remains)
INFO - root - 2019-11-06 18:36:41.431982: step 29080, total loss = 4.04, predict loss = 1.22 (62.1 examples/sec; 0.064 sec/batch; 2h:09m:52s remains)
INFO - root - 2019-11-06 18:36:42.213089: step 29090, total loss = 4.59, predict loss = 1.30 (66.6 examples/sec; 0.060 sec/batch; 2h:01m:03s remains)
INFO - root - 2019-11-06 18:36:42.893050: step 29100, total loss = 6.62, predict loss = 1.92 (76.4 examples/sec; 0.052 sec/batch; 1h:45m:31s remains)
INFO - root - 2019-11-06 18:36:43.543569: step 29110, total loss = 5.53, predict loss = 1.56 (69.2 examples/sec; 0.058 sec/batch; 1h:56m:31s remains)
INFO - root - 2019-11-06 18:36:44.165321: step 29120, total loss = 4.88, predict loss = 1.44 (81.6 examples/sec; 0.049 sec/batch; 1h:38m:42s remains)
INFO - root - 2019-11-06 18:36:44.656387: step 29130, total loss = 5.83, predict loss = 1.71 (96.7 examples/sec; 0.041 sec/batch; 1h:23m:19s remains)
INFO - root - 2019-11-06 18:36:45.118838: step 29140, total loss = 6.96, predict loss = 2.01 (100.2 examples/sec; 0.040 sec/batch; 1h:20m:26s remains)
INFO - root - 2019-11-06 18:36:46.242057: step 29150, total loss = 6.67, predict loss = 1.95 (69.1 examples/sec; 0.058 sec/batch; 1h:56m:32s remains)
INFO - root - 2019-11-06 18:36:46.943282: step 29160, total loss = 3.62, predict loss = 1.01 (61.8 examples/sec; 0.065 sec/batch; 2h:10m:15s remains)
INFO - root - 2019-11-06 18:36:47.688715: step 29170, total loss = 5.76, predict loss = 1.65 (52.0 examples/sec; 0.077 sec/batch; 2h:34m:57s remains)
INFO - root - 2019-11-06 18:36:48.411382: step 29180, total loss = 5.69, predict loss = 1.61 (60.2 examples/sec; 0.066 sec/batch; 2h:13m:48s remains)
INFO - root - 2019-11-06 18:36:49.137381: step 29190, total loss = 3.95, predict loss = 1.08 (57.4 examples/sec; 0.070 sec/batch; 2h:20m:24s remains)
INFO - root - 2019-11-06 18:36:49.710953: step 29200, total loss = 3.65, predict loss = 1.03 (98.5 examples/sec; 0.041 sec/batch; 1h:21m:46s remains)
INFO - root - 2019-11-06 18:36:50.209816: step 29210, total loss = 5.29, predict loss = 1.53 (85.0 examples/sec; 0.047 sec/batch; 1h:34m:44s remains)
INFO - root - 2019-11-06 18:36:50.666205: step 29220, total loss = 3.93, predict loss = 1.16 (95.9 examples/sec; 0.042 sec/batch; 1h:23m:57s remains)
INFO - root - 2019-11-06 18:36:52.045189: step 29230, total loss = 5.35, predict loss = 1.58 (54.4 examples/sec; 0.074 sec/batch; 2h:28m:06s remains)
INFO - root - 2019-11-06 18:36:52.773122: step 29240, total loss = 4.70, predict loss = 1.32 (59.2 examples/sec; 0.068 sec/batch; 2h:16m:00s remains)
INFO - root - 2019-11-06 18:36:53.558486: step 29250, total loss = 4.60, predict loss = 1.35 (57.6 examples/sec; 0.069 sec/batch; 2h:19m:40s remains)
INFO - root - 2019-11-06 18:36:54.281395: step 29260, total loss = 5.05, predict loss = 1.44 (61.2 examples/sec; 0.065 sec/batch; 2h:11m:30s remains)
INFO - root - 2019-11-06 18:36:55.026218: step 29270, total loss = 5.78, predict loss = 1.64 (66.7 examples/sec; 0.060 sec/batch; 2h:00m:44s remains)
INFO - root - 2019-11-06 18:36:55.520787: step 29280, total loss = 3.51, predict loss = 1.06 (96.7 examples/sec; 0.041 sec/batch; 1h:23m:13s remains)
INFO - root - 2019-11-06 18:36:56.011779: step 29290, total loss = 4.77, predict loss = 1.37 (85.2 examples/sec; 0.047 sec/batch; 1h:34m:27s remains)
INFO - root - 2019-11-06 18:36:57.224616: step 29300, total loss = 4.65, predict loss = 1.36 (71.2 examples/sec; 0.056 sec/batch; 1h:53m:00s remains)
INFO - root - 2019-11-06 18:36:57.963059: step 29310, total loss = 3.76, predict loss = 1.11 (56.4 examples/sec; 0.071 sec/batch; 2h:22m:35s remains)
INFO - root - 2019-11-06 18:36:58.730033: step 29320, total loss = 4.85, predict loss = 1.32 (63.6 examples/sec; 0.063 sec/batch; 2h:06m:35s remains)
INFO - root - 2019-11-06 18:36:59.486588: step 29330, total loss = 4.79, predict loss = 1.31 (57.2 examples/sec; 0.070 sec/batch; 2h:20m:34s remains)
INFO - root - 2019-11-06 18:37:00.246255: step 29340, total loss = 4.17, predict loss = 1.15 (61.7 examples/sec; 0.065 sec/batch; 2h:10m:24s remains)
INFO - root - 2019-11-06 18:37:00.827760: step 29350, total loss = 4.33, predict loss = 1.27 (86.6 examples/sec; 0.046 sec/batch; 1h:32m:51s remains)
INFO - root - 2019-11-06 18:37:01.275081: step 29360, total loss = 3.29, predict loss = 0.87 (101.6 examples/sec; 0.039 sec/batch; 1h:19m:09s remains)
INFO - root - 2019-11-06 18:37:01.741092: step 29370, total loss = 4.73, predict loss = 1.36 (112.3 examples/sec; 0.036 sec/batch; 1h:11m:38s remains)
INFO - root - 2019-11-06 18:37:03.081002: step 29380, total loss = 5.48, predict loss = 1.59 (55.7 examples/sec; 0.072 sec/batch; 2h:24m:19s remains)
INFO - root - 2019-11-06 18:37:03.820965: step 29390, total loss = 5.46, predict loss = 1.58 (54.6 examples/sec; 0.073 sec/batch; 2h:27m:09s remains)
INFO - root - 2019-11-06 18:37:04.561382: step 29400, total loss = 5.78, predict loss = 1.72 (62.5 examples/sec; 0.064 sec/batch; 2h:08m:37s remains)
INFO - root - 2019-11-06 18:37:05.320172: step 29410, total loss = 4.70, predict loss = 1.37 (61.3 examples/sec; 0.065 sec/batch; 2h:11m:04s remains)
INFO - root - 2019-11-06 18:37:05.942143: step 29420, total loss = 3.23, predict loss = 0.96 (87.0 examples/sec; 0.046 sec/batch; 1h:32m:25s remains)
INFO - root - 2019-11-06 18:37:06.389762: step 29430, total loss = 4.16, predict loss = 1.19 (96.3 examples/sec; 0.042 sec/batch; 1h:23m:29s remains)
INFO - root - 2019-11-06 18:37:06.843664: step 29440, total loss = 3.88, predict loss = 1.07 (93.1 examples/sec; 0.043 sec/batch; 1h:26m:20s remains)
INFO - root - 2019-11-06 18:37:08.076857: step 29450, total loss = 3.95, predict loss = 1.14 (61.1 examples/sec; 0.065 sec/batch; 2h:11m:31s remains)
INFO - root - 2019-11-06 18:37:08.802506: step 29460, total loss = 3.70, predict loss = 0.97 (54.6 examples/sec; 0.073 sec/batch; 2h:27m:09s remains)
INFO - root - 2019-11-06 18:37:09.635243: step 29470, total loss = 4.28, predict loss = 1.28 (57.0 examples/sec; 0.070 sec/batch; 2h:20m:55s remains)
INFO - root - 2019-11-06 18:37:10.381696: step 29480, total loss = 4.73, predict loss = 1.33 (52.6 examples/sec; 0.076 sec/batch; 2h:32m:51s remains)
INFO - root - 2019-11-06 18:37:11.161623: step 29490, total loss = 5.04, predict loss = 1.46 (60.2 examples/sec; 0.066 sec/batch; 2h:13m:31s remains)
INFO - root - 2019-11-06 18:37:11.724286: step 29500, total loss = 2.93, predict loss = 0.82 (98.6 examples/sec; 0.041 sec/batch; 1h:21m:26s remains)
INFO - root - 2019-11-06 18:37:12.175560: step 29510, total loss = 5.55, predict loss = 1.51 (97.8 examples/sec; 0.041 sec/batch; 1h:22m:06s remains)
INFO - root - 2019-11-06 18:37:13.326899: step 29520, total loss = 4.94, predict loss = 1.39 (5.4 examples/sec; 0.735 sec/batch; 24h:35m:51s remains)
INFO - root - 2019-11-06 18:37:14.035968: step 29530, total loss = 4.48, predict loss = 1.31 (57.2 examples/sec; 0.070 sec/batch; 2h:20m:17s remains)
INFO - root - 2019-11-06 18:37:14.802924: step 29540, total loss = 6.76, predict loss = 2.01 (60.6 examples/sec; 0.066 sec/batch; 2h:12m:31s remains)
INFO - root - 2019-11-06 18:37:15.569770: step 29550, total loss = 4.15, predict loss = 1.26 (57.9 examples/sec; 0.069 sec/batch; 2h:18m:39s remains)
INFO - root - 2019-11-06 18:37:16.298321: step 29560, total loss = 5.41, predict loss = 1.54 (62.8 examples/sec; 0.064 sec/batch; 2h:07m:46s remains)
INFO - root - 2019-11-06 18:37:16.932816: step 29570, total loss = 5.28, predict loss = 1.53 (93.3 examples/sec; 0.043 sec/batch; 1h:26m:02s remains)
INFO - root - 2019-11-06 18:37:17.394504: step 29580, total loss = 4.06, predict loss = 1.12 (92.5 examples/sec; 0.043 sec/batch; 1h:26m:44s remains)
INFO - root - 2019-11-06 18:37:17.850874: step 29590, total loss = 6.00, predict loss = 1.72 (98.9 examples/sec; 0.040 sec/batch; 1h:21m:08s remains)
INFO - root - 2019-11-06 18:37:19.076858: step 29600, total loss = 6.20, predict loss = 1.84 (65.3 examples/sec; 0.061 sec/batch; 2h:02m:53s remains)
INFO - root - 2019-11-06 18:37:19.873916: step 29610, total loss = 4.75, predict loss = 1.40 (56.7 examples/sec; 0.071 sec/batch; 2h:21m:34s remains)
INFO - root - 2019-11-06 18:37:20.657416: step 29620, total loss = 2.91, predict loss = 0.90 (58.3 examples/sec; 0.069 sec/batch; 2h:17m:32s remains)
INFO - root - 2019-11-06 18:37:21.470252: step 29630, total loss = 5.12, predict loss = 1.37 (54.4 examples/sec; 0.074 sec/batch; 2h:27m:34s remains)
INFO - root - 2019-11-06 18:37:22.213344: step 29640, total loss = 3.49, predict loss = 1.00 (64.8 examples/sec; 0.062 sec/batch; 2h:03m:51s remains)
INFO - root - 2019-11-06 18:37:22.732693: step 29650, total loss = 6.01, predict loss = 1.69 (101.6 examples/sec; 0.039 sec/batch; 1h:18m:59s remains)
INFO - root - 2019-11-06 18:37:23.169795: step 29660, total loss = 5.73, predict loss = 1.61 (91.6 examples/sec; 0.044 sec/batch; 1h:27m:35s remains)
INFO - root - 2019-11-06 18:37:24.299008: step 29670, total loss = 4.94, predict loss = 1.41 (69.7 examples/sec; 0.057 sec/batch; 1h:55m:03s remains)
INFO - root - 2019-11-06 18:37:24.992331: step 29680, total loss = 5.01, predict loss = 1.50 (61.6 examples/sec; 0.065 sec/batch; 2h:10m:12s remains)
INFO - root - 2019-11-06 18:37:25.715785: step 29690, total loss = 4.48, predict loss = 1.29 (61.6 examples/sec; 0.065 sec/batch; 2h:10m:10s remains)
INFO - root - 2019-11-06 18:37:26.423399: step 29700, total loss = 4.02, predict loss = 1.18 (55.8 examples/sec; 0.072 sec/batch; 2h:23m:48s remains)
INFO - root - 2019-11-06 18:37:27.201251: step 29710, total loss = 4.37, predict loss = 1.20 (55.3 examples/sec; 0.072 sec/batch; 2h:25m:00s remains)
INFO - root - 2019-11-06 18:37:27.869344: step 29720, total loss = 3.37, predict loss = 0.94 (92.0 examples/sec; 0.043 sec/batch; 1h:27m:12s remains)
INFO - root - 2019-11-06 18:37:28.340793: step 29730, total loss = 4.73, predict loss = 1.38 (95.0 examples/sec; 0.042 sec/batch; 1h:24m:21s remains)
INFO - root - 2019-11-06 18:37:28.792308: step 29740, total loss = 4.35, predict loss = 1.35 (90.7 examples/sec; 0.044 sec/batch; 1h:28m:22s remains)
INFO - root - 2019-11-06 18:37:30.077184: step 29750, total loss = 5.35, predict loss = 1.59 (61.4 examples/sec; 0.065 sec/batch; 2h:10m:32s remains)
INFO - root - 2019-11-06 18:37:30.781833: step 29760, total loss = 4.92, predict loss = 1.46 (68.7 examples/sec; 0.058 sec/batch; 1h:56m:38s remains)
INFO - root - 2019-11-06 18:37:31.552387: step 29770, total loss = 5.69, predict loss = 1.59 (60.3 examples/sec; 0.066 sec/batch; 2h:12m:52s remains)
INFO - root - 2019-11-06 18:37:32.283054: step 29780, total loss = 4.83, predict loss = 1.41 (60.9 examples/sec; 0.066 sec/batch; 2h:11m:42s remains)
INFO - root - 2019-11-06 18:37:32.967134: step 29790, total loss = 5.46, predict loss = 1.53 (60.6 examples/sec; 0.066 sec/batch; 2h:12m:09s remains)
INFO - root - 2019-11-06 18:37:33.497228: step 29800, total loss = 5.03, predict loss = 1.39 (90.8 examples/sec; 0.044 sec/batch; 1h:28m:12s remains)
INFO - root - 2019-11-06 18:37:33.978667: step 29810, total loss = 4.41, predict loss = 1.22 (93.8 examples/sec; 0.043 sec/batch; 1h:25m:25s remains)
INFO - root - 2019-11-06 18:37:35.139208: step 29820, total loss = 5.72, predict loss = 1.63 (66.7 examples/sec; 0.060 sec/batch; 2h:00m:04s remains)
INFO - root - 2019-11-06 18:37:35.830053: step 29830, total loss = 6.23, predict loss = 1.81 (58.6 examples/sec; 0.068 sec/batch; 2h:16m:41s remains)
INFO - root - 2019-11-06 18:37:36.628272: step 29840, total loss = 4.89, predict loss = 1.43 (52.4 examples/sec; 0.076 sec/batch; 2h:32m:51s remains)
INFO - root - 2019-11-06 18:37:37.334295: step 29850, total loss = 4.47, predict loss = 1.29 (61.3 examples/sec; 0.065 sec/batch; 2h:10m:41s remains)
INFO - root - 2019-11-06 18:37:38.072031: step 29860, total loss = 5.31, predict loss = 1.56 (65.7 examples/sec; 0.061 sec/batch; 2h:01m:56s remains)
INFO - root - 2019-11-06 18:37:38.690819: step 29870, total loss = 3.21, predict loss = 0.86 (96.7 examples/sec; 0.041 sec/batch; 1h:22m:48s remains)
INFO - root - 2019-11-06 18:37:39.145194: step 29880, total loss = 5.40, predict loss = 1.59 (94.7 examples/sec; 0.042 sec/batch; 1h:24m:33s remains)
INFO - root - 2019-11-06 18:37:39.637014: step 29890, total loss = 3.80, predict loss = 1.08 (94.5 examples/sec; 0.042 sec/batch; 1h:24m:45s remains)
INFO - root - 2019-11-06 18:37:40.921372: step 29900, total loss = 4.96, predict loss = 1.45 (65.6 examples/sec; 0.061 sec/batch; 2h:02m:02s remains)
INFO - root - 2019-11-06 18:37:41.674944: step 29910, total loss = 4.24, predict loss = 1.17 (57.9 examples/sec; 0.069 sec/batch; 2h:18m:11s remains)
INFO - root - 2019-11-06 18:37:42.441887: step 29920, total loss = 5.50, predict loss = 1.62 (55.0 examples/sec; 0.073 sec/batch; 2h:25m:37s remains)
INFO - root - 2019-11-06 18:37:43.193349: step 29930, total loss = 4.56, predict loss = 1.33 (58.2 examples/sec; 0.069 sec/batch; 2h:17m:34s remains)
INFO - root - 2019-11-06 18:37:43.876596: step 29940, total loss = 4.09, predict loss = 1.10 (66.1 examples/sec; 0.060 sec/batch; 2h:01m:02s remains)
INFO - root - 2019-11-06 18:37:44.404011: step 29950, total loss = 5.30, predict loss = 1.57 (91.6 examples/sec; 0.044 sec/batch; 1h:27m:24s remains)
INFO - root - 2019-11-06 18:37:44.849611: step 29960, total loss = 3.95, predict loss = 1.09 (99.6 examples/sec; 0.040 sec/batch; 1h:20m:22s remains)
INFO - root - 2019-11-06 18:37:46.071514: step 29970, total loss = 3.08, predict loss = 0.92 (65.7 examples/sec; 0.061 sec/batch; 2h:01m:52s remains)
INFO - root - 2019-11-06 18:37:46.798491: step 29980, total loss = 5.93, predict loss = 1.70 (58.4 examples/sec; 0.068 sec/batch; 2h:16m:56s remains)
INFO - root - 2019-11-06 18:37:47.524168: step 29990, total loss = 5.82, predict loss = 1.72 (56.9 examples/sec; 0.070 sec/batch; 2h:20m:42s remains)
INFO - root - 2019-11-06 18:37:48.278745: step 30000, total loss = 3.79, predict loss = 1.11 (59.7 examples/sec; 0.067 sec/batch; 2h:13m:58s remains)
INFO - root - 2019-11-06 18:37:49.594342: step 30010, total loss = 4.13, predict loss = 1.22 (66.9 examples/sec; 0.060 sec/batch; 1h:59m:30s remains)
INFO - root - 2019-11-06 18:37:50.123321: step 30020, total loss = 6.07, predict loss = 1.75 (92.1 examples/sec; 0.043 sec/batch; 1h:26m:48s remains)
INFO - root - 2019-11-06 18:37:50.606609: step 30030, total loss = 5.13, predict loss = 1.42 (84.6 examples/sec; 0.047 sec/batch; 1h:34m:34s remains)
INFO - root - 2019-11-06 18:37:51.063670: step 30040, total loss = 4.13, predict loss = 1.28 (100.6 examples/sec; 0.040 sec/batch; 1h:19m:27s remains)
INFO - root - 2019-11-06 18:37:52.462954: step 30050, total loss = 5.17, predict loss = 1.49 (61.9 examples/sec; 0.065 sec/batch; 2h:09m:07s remains)
INFO - root - 2019-11-06 18:37:53.153642: step 30060, total loss = 4.71, predict loss = 1.38 (64.3 examples/sec; 0.062 sec/batch; 2h:04m:18s remains)
INFO - root - 2019-11-06 18:37:53.931580: step 30070, total loss = 5.68, predict loss = 1.59 (56.4 examples/sec; 0.071 sec/batch; 2h:21m:39s remains)
INFO - root - 2019-11-06 18:37:54.706870: step 30080, total loss = 5.34, predict loss = 1.48 (56.8 examples/sec; 0.070 sec/batch; 2h:20m:51s remains)
INFO - root - 2019-11-06 18:37:55.397232: step 30090, total loss = 5.33, predict loss = 1.54 (72.7 examples/sec; 0.055 sec/batch; 1h:49m:53s remains)
INFO - root - 2019-11-06 18:37:55.859630: step 30100, total loss = 5.20, predict loss = 1.55 (96.8 examples/sec; 0.041 sec/batch; 1h:22m:33s remains)
INFO - root - 2019-11-06 18:37:56.309862: step 30110, total loss = 4.57, predict loss = 1.30 (97.6 examples/sec; 0.041 sec/batch; 1h:21m:53s remains)
INFO - root - 2019-11-06 18:37:57.499674: step 30120, total loss = 4.67, predict loss = 1.40 (65.4 examples/sec; 0.061 sec/batch; 2h:02m:11s remains)
INFO - root - 2019-11-06 18:37:58.229234: step 30130, total loss = 6.00, predict loss = 1.67 (58.3 examples/sec; 0.069 sec/batch; 2h:17m:09s remains)
INFO - root - 2019-11-06 18:37:58.936714: step 30140, total loss = 5.71, predict loss = 1.65 (62.2 examples/sec; 0.064 sec/batch; 2h:08m:25s remains)
INFO - root - 2019-11-06 18:37:59.700963: step 30150, total loss = 4.90, predict loss = 1.38 (62.0 examples/sec; 0.065 sec/batch; 2h:08m:54s remains)
INFO - root - 2019-11-06 18:38:00.485520: step 30160, total loss = 4.31, predict loss = 1.22 (56.0 examples/sec; 0.071 sec/batch; 2h:22m:37s remains)
INFO - root - 2019-11-06 18:38:01.060552: step 30170, total loss = 4.50, predict loss = 1.27 (100.5 examples/sec; 0.040 sec/batch; 1h:19m:30s remains)
INFO - root - 2019-11-06 18:38:01.525778: step 30180, total loss = 4.62, predict loss = 1.38 (88.1 examples/sec; 0.045 sec/batch; 1h:30m:40s remains)
INFO - root - 2019-11-06 18:38:01.979187: step 30190, total loss = 4.78, predict loss = 1.43 (120.2 examples/sec; 0.033 sec/batch; 1h:06m:26s remains)
INFO - root - 2019-11-06 18:38:03.324584: step 30200, total loss = 6.01, predict loss = 1.72 (65.8 examples/sec; 0.061 sec/batch; 2h:01m:26s remains)
INFO - root - 2019-11-06 18:38:04.121841: step 30210, total loss = 4.60, predict loss = 1.26 (56.5 examples/sec; 0.071 sec/batch; 2h:21m:25s remains)
INFO - root - 2019-11-06 18:38:04.968238: step 30220, total loss = 4.06, predict loss = 1.19 (49.4 examples/sec; 0.081 sec/batch; 2h:41m:31s remains)
INFO - root - 2019-11-06 18:38:05.754778: step 30230, total loss = 5.29, predict loss = 1.39 (59.6 examples/sec; 0.067 sec/batch; 2h:13m:59s remains)
INFO - root - 2019-11-06 18:38:06.419910: step 30240, total loss = 6.11, predict loss = 1.72 (75.2 examples/sec; 0.053 sec/batch; 1h:46m:07s remains)
INFO - root - 2019-11-06 18:38:06.928505: step 30250, total loss = 3.81, predict loss = 1.13 (99.1 examples/sec; 0.040 sec/batch; 1h:20m:32s remains)
INFO - root - 2019-11-06 18:38:07.380632: step 30260, total loss = 5.58, predict loss = 1.55 (97.5 examples/sec; 0.041 sec/batch; 1h:21m:51s remains)
INFO - root - 2019-11-06 18:38:08.643981: step 30270, total loss = 4.48, predict loss = 1.32 (65.5 examples/sec; 0.061 sec/batch; 2h:01m:46s remains)
INFO - root - 2019-11-06 18:38:09.419742: step 30280, total loss = 5.43, predict loss = 1.62 (53.5 examples/sec; 0.075 sec/batch; 2h:29m:05s remains)
INFO - root - 2019-11-06 18:38:10.167572: step 30290, total loss = 5.34, predict loss = 1.60 (60.7 examples/sec; 0.066 sec/batch; 2h:11m:26s remains)
INFO - root - 2019-11-06 18:38:10.904056: step 30300, total loss = 6.39, predict loss = 1.83 (59.5 examples/sec; 0.067 sec/batch; 2h:14m:12s remains)
INFO - root - 2019-11-06 18:38:11.677347: step 30310, total loss = 5.87, predict loss = 1.71 (68.6 examples/sec; 0.058 sec/batch; 1h:56m:19s remains)
INFO - root - 2019-11-06 18:38:12.234585: step 30320, total loss = 5.95, predict loss = 1.81 (94.1 examples/sec; 0.043 sec/batch; 1h:24m:49s remains)
INFO - root - 2019-11-06 18:38:12.716606: step 30330, total loss = 4.38, predict loss = 1.24 (93.4 examples/sec; 0.043 sec/batch; 1h:25m:25s remains)
INFO - root - 2019-11-06 18:38:13.840491: step 30340, total loss = 4.56, predict loss = 1.27 (5.5 examples/sec; 0.723 sec/batch; 24h:01m:47s remains)
INFO - root - 2019-11-06 18:38:14.547256: step 30350, total loss = 4.88, predict loss = 1.41 (56.2 examples/sec; 0.071 sec/batch; 2h:21m:58s remains)
INFO - root - 2019-11-06 18:38:15.334243: step 30360, total loss = 4.09, predict loss = 1.16 (51.0 examples/sec; 0.078 sec/batch; 2h:36m:31s remains)
INFO - root - 2019-11-06 18:38:16.061505: step 30370, total loss = 6.50, predict loss = 1.84 (65.5 examples/sec; 0.061 sec/batch; 2h:01m:45s remains)
INFO - root - 2019-11-06 18:38:16.788614: step 30380, total loss = 3.87, predict loss = 1.11 (55.9 examples/sec; 0.072 sec/batch; 2h:22m:38s remains)
INFO - root - 2019-11-06 18:38:17.426738: step 30390, total loss = 4.22, predict loss = 1.30 (99.0 examples/sec; 0.040 sec/batch; 1h:20m:31s remains)
INFO - root - 2019-11-06 18:38:17.865901: step 30400, total loss = 6.73, predict loss = 1.96 (101.6 examples/sec; 0.039 sec/batch; 1h:18m:27s remains)
INFO - root - 2019-11-06 18:38:18.334726: step 30410, total loss = 4.84, predict loss = 1.31 (93.3 examples/sec; 0.043 sec/batch; 1h:25m:24s remains)
INFO - root - 2019-11-06 18:38:19.533055: step 30420, total loss = 3.95, predict loss = 1.10 (59.5 examples/sec; 0.067 sec/batch; 2h:14m:02s remains)
INFO - root - 2019-11-06 18:38:20.242065: step 30430, total loss = 4.13, predict loss = 1.19 (58.8 examples/sec; 0.068 sec/batch; 2h:15m:35s remains)
INFO - root - 2019-11-06 18:38:20.976637: step 30440, total loss = 5.56, predict loss = 1.57 (65.2 examples/sec; 0.061 sec/batch; 2h:02m:15s remains)
INFO - root - 2019-11-06 18:38:21.718787: step 30450, total loss = 4.19, predict loss = 1.22 (63.8 examples/sec; 0.063 sec/batch; 2h:04m:58s remains)
INFO - root - 2019-11-06 18:38:22.448644: step 30460, total loss = 4.28, predict loss = 1.21 (70.6 examples/sec; 0.057 sec/batch; 1h:52m:55s remains)
INFO - root - 2019-11-06 18:38:22.966082: step 30470, total loss = 5.76, predict loss = 1.56 (93.2 examples/sec; 0.043 sec/batch; 1h:25m:30s remains)
INFO - root - 2019-11-06 18:38:23.428393: step 30480, total loss = 4.31, predict loss = 1.27 (96.1 examples/sec; 0.042 sec/batch; 1h:22m:55s remains)
INFO - root - 2019-11-06 18:38:24.613424: step 30490, total loss = 4.97, predict loss = 1.50 (68.7 examples/sec; 0.058 sec/batch; 1h:56m:00s remains)
INFO - root - 2019-11-06 18:38:25.274197: step 30500, total loss = 5.06, predict loss = 1.57 (63.4 examples/sec; 0.063 sec/batch; 2h:05m:45s remains)
INFO - root - 2019-11-06 18:38:25.999381: step 30510, total loss = 3.85, predict loss = 1.14 (56.3 examples/sec; 0.071 sec/batch; 2h:21m:36s remains)
INFO - root - 2019-11-06 18:38:26.768692: step 30520, total loss = 3.71, predict loss = 1.10 (52.2 examples/sec; 0.077 sec/batch; 2h:32m:41s remains)
INFO - root - 2019-11-06 18:38:27.512301: step 30530, total loss = 5.32, predict loss = 1.54 (61.2 examples/sec; 0.065 sec/batch; 2h:10m:08s remains)
INFO - root - 2019-11-06 18:38:28.128316: step 30540, total loss = 3.44, predict loss = 1.00 (96.5 examples/sec; 0.041 sec/batch; 1h:22m:29s remains)
INFO - root - 2019-11-06 18:38:28.577744: step 30550, total loss = 4.06, predict loss = 1.20 (96.9 examples/sec; 0.041 sec/batch; 1h:22m:11s remains)
INFO - root - 2019-11-06 18:38:29.013893: step 30560, total loss = 5.20, predict loss = 1.47 (91.7 examples/sec; 0.044 sec/batch; 1h:26m:49s remains)
INFO - root - 2019-11-06 18:38:30.318395: step 30570, total loss = 6.63, predict loss = 1.84 (61.2 examples/sec; 0.065 sec/batch; 2h:10m:11s remains)
INFO - root - 2019-11-06 18:38:31.029476: step 30580, total loss = 3.81, predict loss = 1.08 (62.8 examples/sec; 0.064 sec/batch; 2h:06m:51s remains)
INFO - root - 2019-11-06 18:38:31.842942: step 30590, total loss = 5.89, predict loss = 1.65 (58.4 examples/sec; 0.068 sec/batch; 2h:16m:15s remains)
INFO - root - 2019-11-06 18:38:32.587052: step 30600, total loss = 5.52, predict loss = 1.57 (64.0 examples/sec; 0.063 sec/batch; 2h:04m:26s remains)
INFO - root - 2019-11-06 18:38:33.325684: step 30610, total loss = 6.17, predict loss = 1.80 (66.3 examples/sec; 0.060 sec/batch; 1h:59m:59s remains)
INFO - root - 2019-11-06 18:38:33.885150: step 30620, total loss = 4.92, predict loss = 1.46 (94.7 examples/sec; 0.042 sec/batch; 1h:24m:03s remains)
INFO - root - 2019-11-06 18:38:34.334752: step 30630, total loss = 4.24, predict loss = 1.16 (94.7 examples/sec; 0.042 sec/batch; 1h:24m:04s remains)
INFO - root - 2019-11-06 18:38:35.530212: step 30640, total loss = 5.96, predict loss = 1.67 (63.6 examples/sec; 0.063 sec/batch; 2h:05m:09s remains)
INFO - root - 2019-11-06 18:38:36.257114: step 30650, total loss = 5.19, predict loss = 1.54 (59.7 examples/sec; 0.067 sec/batch; 2h:13m:18s remains)
INFO - root - 2019-11-06 18:38:37.035135: step 30660, total loss = 6.42, predict loss = 1.88 (58.5 examples/sec; 0.068 sec/batch; 2h:16m:00s remains)
INFO - root - 2019-11-06 18:38:37.740286: step 30670, total loss = 3.96, predict loss = 1.11 (69.8 examples/sec; 0.057 sec/batch; 1h:54m:02s remains)
INFO - root - 2019-11-06 18:38:38.448858: step 30680, total loss = 5.40, predict loss = 1.52 (58.4 examples/sec; 0.069 sec/batch; 2h:16m:18s remains)
INFO - root - 2019-11-06 18:38:39.059172: step 30690, total loss = 5.18, predict loss = 1.54 (93.0 examples/sec; 0.043 sec/batch; 1h:25m:33s remains)
INFO - root - 2019-11-06 18:38:39.518106: step 30700, total loss = 6.12, predict loss = 1.72 (96.9 examples/sec; 0.041 sec/batch; 1h:22m:07s remains)
INFO - root - 2019-11-06 18:38:39.971843: step 30710, total loss = 4.62, predict loss = 1.36 (96.6 examples/sec; 0.041 sec/batch; 1h:22m:20s remains)
INFO - root - 2019-11-06 18:38:41.267764: step 30720, total loss = 6.43, predict loss = 1.83 (60.4 examples/sec; 0.066 sec/batch; 2h:11m:42s remains)
INFO - root - 2019-11-06 18:38:42.040038: step 30730, total loss = 5.78, predict loss = 1.66 (59.4 examples/sec; 0.067 sec/batch; 2h:13m:51s remains)
INFO - root - 2019-11-06 18:38:42.796517: step 30740, total loss = 4.54, predict loss = 1.26 (56.9 examples/sec; 0.070 sec/batch; 2h:19m:36s remains)
INFO - root - 2019-11-06 18:38:43.577905: step 30750, total loss = 5.21, predict loss = 1.52 (56.5 examples/sec; 0.071 sec/batch; 2h:20m:42s remains)
INFO - root - 2019-11-06 18:38:44.308595: step 30760, total loss = 5.03, predict loss = 1.48 (73.8 examples/sec; 0.054 sec/batch; 1h:47m:39s remains)
INFO - root - 2019-11-06 18:38:44.833212: step 30770, total loss = 5.61, predict loss = 1.58 (96.4 examples/sec; 0.042 sec/batch; 1h:22m:28s remains)
INFO - root - 2019-11-06 18:38:45.301176: step 30780, total loss = 4.94, predict loss = 1.44 (90.4 examples/sec; 0.044 sec/batch; 1h:27m:56s remains)
INFO - root - 2019-11-06 18:38:46.488292: step 30790, total loss = 4.92, predict loss = 1.43 (74.2 examples/sec; 0.054 sec/batch; 1h:47m:10s remains)
INFO - root - 2019-11-06 18:38:47.201645: step 30800, total loss = 4.95, predict loss = 1.48 (59.2 examples/sec; 0.068 sec/batch; 2h:14m:17s remains)
INFO - root - 2019-11-06 18:38:47.976788: step 30810, total loss = 5.32, predict loss = 1.54 (60.2 examples/sec; 0.066 sec/batch; 2h:12m:02s remains)
INFO - root - 2019-11-06 18:38:48.720565: step 30820, total loss = 3.84, predict loss = 1.08 (59.7 examples/sec; 0.067 sec/batch; 2h:13m:01s remains)
INFO - root - 2019-11-06 18:38:49.461722: step 30830, total loss = 5.61, predict loss = 1.70 (61.5 examples/sec; 0.065 sec/batch; 2h:09m:11s remains)
INFO - root - 2019-11-06 18:38:50.074505: step 30840, total loss = 4.30, predict loss = 1.14 (96.2 examples/sec; 0.042 sec/batch; 1h:22m:35s remains)
INFO - root - 2019-11-06 18:38:50.556698: step 30850, total loss = 5.65, predict loss = 1.57 (93.9 examples/sec; 0.043 sec/batch; 1h:24m:33s remains)
INFO - root - 2019-11-06 18:38:50.999373: step 30860, total loss = 3.48, predict loss = 1.04 (97.3 examples/sec; 0.041 sec/batch; 1h:21m:38s remains)
INFO - root - 2019-11-06 18:38:52.404229: step 30870, total loss = 5.77, predict loss = 1.64 (62.9 examples/sec; 0.064 sec/batch; 2h:06m:15s remains)
INFO - root - 2019-11-06 18:38:53.100635: step 30880, total loss = 6.26, predict loss = 1.84 (63.1 examples/sec; 0.063 sec/batch; 2h:05m:52s remains)
INFO - root - 2019-11-06 18:38:53.850123: step 30890, total loss = 6.31, predict loss = 1.75 (65.8 examples/sec; 0.061 sec/batch; 2h:00m:35s remains)
INFO - root - 2019-11-06 18:38:54.536237: step 30900, total loss = 2.76, predict loss = 0.86 (66.5 examples/sec; 0.060 sec/batch; 1h:59m:19s remains)
INFO - root - 2019-11-06 18:38:55.196871: step 30910, total loss = 4.84, predict loss = 1.39 (75.0 examples/sec; 0.053 sec/batch; 1h:45m:55s remains)
INFO - root - 2019-11-06 18:38:55.648601: step 30920, total loss = 3.87, predict loss = 1.11 (100.0 examples/sec; 0.040 sec/batch; 1h:19m:21s remains)
INFO - root - 2019-11-06 18:38:56.112784: step 30930, total loss = 5.49, predict loss = 1.59 (100.1 examples/sec; 0.040 sec/batch; 1h:19m:17s remains)
INFO - root - 2019-11-06 18:38:57.338429: step 30940, total loss = 4.42, predict loss = 1.42 (64.4 examples/sec; 0.062 sec/batch; 2h:03m:11s remains)
INFO - root - 2019-11-06 18:38:58.052678: step 30950, total loss = 5.26, predict loss = 1.52 (57.1 examples/sec; 0.070 sec/batch; 2h:19m:06s remains)
INFO - root - 2019-11-06 18:38:58.738147: step 30960, total loss = 5.65, predict loss = 1.70 (65.4 examples/sec; 0.061 sec/batch; 2h:01m:16s remains)
INFO - root - 2019-11-06 18:38:59.446445: step 30970, total loss = 4.53, predict loss = 1.28 (65.7 examples/sec; 0.061 sec/batch; 2h:00m:43s remains)
INFO - root - 2019-11-06 18:39:00.172486: step 30980, total loss = 4.64, predict loss = 1.38 (54.3 examples/sec; 0.074 sec/batch; 2h:26m:15s remains)
INFO - root - 2019-11-06 18:39:00.749131: step 30990, total loss = 5.69, predict loss = 1.63 (100.2 examples/sec; 0.040 sec/batch; 1h:19m:08s remains)
INFO - root - 2019-11-06 18:39:01.185934: step 31000, total loss = 3.81, predict loss = 1.13 (92.9 examples/sec; 0.043 sec/batch; 1h:25m:24s remains)
INFO - root - 2019-11-06 18:39:01.656851: step 31010, total loss = 5.92, predict loss = 1.79 (125.2 examples/sec; 0.032 sec/batch; 1h:03m:21s remains)
INFO - root - 2019-11-06 18:39:02.951352: step 31020, total loss = 5.19, predict loss = 1.53 (66.9 examples/sec; 0.060 sec/batch; 1h:58m:35s remains)
INFO - root - 2019-11-06 18:39:03.695722: step 31030, total loss = 4.33, predict loss = 1.24 (56.8 examples/sec; 0.070 sec/batch; 2h:19m:31s remains)
INFO - root - 2019-11-06 18:39:04.425917: step 31040, total loss = 5.65, predict loss = 1.60 (65.3 examples/sec; 0.061 sec/batch; 2h:01m:28s remains)
INFO - root - 2019-11-06 18:39:05.171175: step 31050, total loss = 5.46, predict loss = 1.52 (53.8 examples/sec; 0.074 sec/batch; 2h:27m:30s remains)
INFO - root - 2019-11-06 18:39:05.877576: step 31060, total loss = 5.65, predict loss = 1.67 (72.3 examples/sec; 0.055 sec/batch; 1h:49m:40s remains)
INFO - root - 2019-11-06 18:39:06.351976: step 31070, total loss = 5.17, predict loss = 1.48 (93.4 examples/sec; 0.043 sec/batch; 1h:24m:54s remains)
INFO - root - 2019-11-06 18:39:06.814322: step 31080, total loss = 5.16, predict loss = 1.40 (92.9 examples/sec; 0.043 sec/batch; 1h:25m:19s remains)
INFO - root - 2019-11-06 18:39:08.072549: step 31090, total loss = 5.12, predict loss = 1.43 (62.6 examples/sec; 0.064 sec/batch; 2h:06m:39s remains)
INFO - root - 2019-11-06 18:39:08.835516: step 31100, total loss = 4.92, predict loss = 1.41 (58.6 examples/sec; 0.068 sec/batch; 2h:15m:22s remains)
INFO - root - 2019-11-06 18:39:09.646554: step 31110, total loss = 5.40, predict loss = 1.57 (53.3 examples/sec; 0.075 sec/batch; 2h:28m:36s remains)
INFO - root - 2019-11-06 18:39:10.495536: step 31120, total loss = 5.74, predict loss = 1.60 (57.6 examples/sec; 0.070 sec/batch; 2h:17m:42s remains)
INFO - root - 2019-11-06 18:39:11.210281: step 31130, total loss = 5.09, predict loss = 1.52 (77.8 examples/sec; 0.051 sec/batch; 1h:41m:52s remains)
INFO - root - 2019-11-06 18:39:11.798540: step 31140, total loss = 6.28, predict loss = 1.77 (94.6 examples/sec; 0.042 sec/batch; 1h:23m:47s remains)
INFO - root - 2019-11-06 18:39:12.270343: step 31150, total loss = 4.17, predict loss = 1.20 (93.7 examples/sec; 0.043 sec/batch; 1h:24m:35s remains)
INFO - root - 2019-11-06 18:39:13.428296: step 31160, total loss = 6.13, predict loss = 1.74 (5.3 examples/sec; 0.761 sec/batch; 25h:06m:32s remains)
INFO - root - 2019-11-06 18:39:14.126742: step 31170, total loss = 5.85, predict loss = 1.64 (59.3 examples/sec; 0.067 sec/batch; 2h:13m:38s remains)
INFO - root - 2019-11-06 18:39:14.916323: step 31180, total loss = 5.69, predict loss = 1.71 (60.6 examples/sec; 0.066 sec/batch; 2h:10m:36s remains)
INFO - root - 2019-11-06 18:39:15.678218: step 31190, total loss = 5.60, predict loss = 1.50 (69.6 examples/sec; 0.057 sec/batch; 1h:53m:46s remains)
INFO - root - 2019-11-06 18:39:16.403471: step 31200, total loss = 4.69, predict loss = 1.40 (61.2 examples/sec; 0.065 sec/batch; 2h:09m:30s remains)
INFO - root - 2019-11-06 18:39:17.065511: step 31210, total loss = 4.74, predict loss = 1.39 (89.7 examples/sec; 0.045 sec/batch; 1h:28m:19s remains)
INFO - root - 2019-11-06 18:39:17.528152: step 31220, total loss = 3.90, predict loss = 1.19 (96.5 examples/sec; 0.041 sec/batch; 1h:22m:05s remains)
INFO - root - 2019-11-06 18:39:17.995613: step 31230, total loss = 5.08, predict loss = 1.47 (91.9 examples/sec; 0.044 sec/batch; 1h:26m:11s remains)
INFO - root - 2019-11-06 18:39:19.235322: step 31240, total loss = 5.16, predict loss = 1.47 (64.3 examples/sec; 0.062 sec/batch; 2h:03m:07s remains)
INFO - root - 2019-11-06 18:39:19.998759: step 31250, total loss = 6.47, predict loss = 1.80 (56.2 examples/sec; 0.071 sec/batch; 2h:20m:49s remains)
INFO - root - 2019-11-06 18:39:20.818704: step 31260, total loss = 5.33, predict loss = 1.50 (53.6 examples/sec; 0.075 sec/batch; 2h:27m:49s remains)
INFO - root - 2019-11-06 18:39:21.608600: step 31270, total loss = 4.34, predict loss = 1.17 (56.7 examples/sec; 0.070 sec/batch; 2h:19m:29s remains)
INFO - root - 2019-11-06 18:39:22.400311: step 31280, total loss = 5.29, predict loss = 1.57 (64.6 examples/sec; 0.062 sec/batch; 2h:02m:33s remains)
INFO - root - 2019-11-06 18:39:22.974238: step 31290, total loss = 4.50, predict loss = 1.31 (94.4 examples/sec; 0.042 sec/batch; 1h:23m:48s remains)
INFO - root - 2019-11-06 18:39:23.423327: step 31300, total loss = 5.31, predict loss = 1.56 (101.4 examples/sec; 0.039 sec/batch; 1h:18m:04s remains)
INFO - root - 2019-11-06 18:39:24.580240: step 31310, total loss = 6.12, predict loss = 1.77 (70.2 examples/sec; 0.057 sec/batch; 1h:52m:46s remains)
INFO - root - 2019-11-06 18:39:25.271307: step 31320, total loss = 5.12, predict loss = 1.47 (60.2 examples/sec; 0.066 sec/batch; 2h:11m:20s remains)
INFO - root - 2019-11-06 18:39:26.049433: step 31330, total loss = 3.90, predict loss = 1.19 (64.2 examples/sec; 0.062 sec/batch; 2h:03m:12s remains)
INFO - root - 2019-11-06 18:39:26.780807: step 31340, total loss = 4.87, predict loss = 1.40 (59.4 examples/sec; 0.067 sec/batch; 2h:13m:12s remains)
INFO - root - 2019-11-06 18:39:27.602284: step 31350, total loss = 5.24, predict loss = 1.50 (48.4 examples/sec; 0.083 sec/batch; 2h:43m:35s remains)
INFO - root - 2019-11-06 18:39:28.266583: step 31360, total loss = 6.05, predict loss = 1.70 (96.6 examples/sec; 0.041 sec/batch; 1h:21m:50s remains)
INFO - root - 2019-11-06 18:39:28.749161: step 31370, total loss = 5.63, predict loss = 1.59 (89.3 examples/sec; 0.045 sec/batch; 1h:28m:34s remains)
INFO - root - 2019-11-06 18:39:29.217851: step 31380, total loss = 3.84, predict loss = 1.04 (95.7 examples/sec; 0.042 sec/batch; 1h:22m:37s remains)
INFO - root - 2019-11-06 18:39:30.501296: step 31390, total loss = 5.47, predict loss = 1.54 (64.0 examples/sec; 0.062 sec/batch; 2h:03m:29s remains)
INFO - root - 2019-11-06 18:39:31.236725: step 31400, total loss = 6.03, predict loss = 1.70 (62.4 examples/sec; 0.064 sec/batch; 2h:06m:40s remains)
INFO - root - 2019-11-06 18:39:31.980307: step 31410, total loss = 5.57, predict loss = 1.66 (67.5 examples/sec; 0.059 sec/batch; 1h:57m:12s remains)
INFO - root - 2019-11-06 18:39:32.703107: step 31420, total loss = 5.05, predict loss = 1.53 (58.0 examples/sec; 0.069 sec/batch; 2h:16m:17s remains)
INFO - root - 2019-11-06 18:39:33.432357: step 31430, total loss = 5.45, predict loss = 1.59 (64.3 examples/sec; 0.062 sec/batch; 2h:02m:52s remains)
INFO - root - 2019-11-06 18:39:33.964064: step 31440, total loss = 4.90, predict loss = 1.48 (96.1 examples/sec; 0.042 sec/batch; 1h:22m:16s remains)
INFO - root - 2019-11-06 18:39:34.457391: step 31450, total loss = 3.68, predict loss = 1.10 (97.5 examples/sec; 0.041 sec/batch; 1h:21m:02s remains)
INFO - root - 2019-11-06 18:39:35.650545: step 31460, total loss = 5.17, predict loss = 1.49 (59.7 examples/sec; 0.067 sec/batch; 2h:12m:19s remains)
INFO - root - 2019-11-06 18:39:36.373134: step 31470, total loss = 4.25, predict loss = 1.24 (60.9 examples/sec; 0.066 sec/batch; 2h:09m:45s remains)
INFO - root - 2019-11-06 18:39:37.157641: step 31480, total loss = 4.66, predict loss = 1.34 (57.0 examples/sec; 0.070 sec/batch; 2h:18m:29s remains)
INFO - root - 2019-11-06 18:39:37.908068: step 31490, total loss = 3.29, predict loss = 1.05 (59.7 examples/sec; 0.067 sec/batch; 2h:12m:14s remains)
INFO - root - 2019-11-06 18:39:38.667680: step 31500, total loss = 5.16, predict loss = 1.47 (60.6 examples/sec; 0.066 sec/batch; 2h:10m:17s remains)
INFO - root - 2019-11-06 18:39:39.331685: step 31510, total loss = 3.49, predict loss = 1.03 (90.9 examples/sec; 0.044 sec/batch; 1h:26m:54s remains)
INFO - root - 2019-11-06 18:39:39.798750: step 31520, total loss = 5.50, predict loss = 1.59 (91.1 examples/sec; 0.044 sec/batch; 1h:26m:39s remains)
INFO - root - 2019-11-06 18:39:40.281836: step 31530, total loss = 5.36, predict loss = 1.57 (96.8 examples/sec; 0.041 sec/batch; 1h:21m:36s remains)
INFO - root - 2019-11-06 18:39:41.574137: step 31540, total loss = 5.51, predict loss = 1.54 (63.3 examples/sec; 0.063 sec/batch; 2h:04m:49s remains)
INFO - root - 2019-11-06 18:39:42.295525: step 31550, total loss = 3.00, predict loss = 0.87 (54.9 examples/sec; 0.073 sec/batch; 2h:23m:50s remains)
INFO - root - 2019-11-06 18:39:43.021277: step 31560, total loss = 4.20, predict loss = 1.21 (58.6 examples/sec; 0.068 sec/batch; 2h:14m:44s remains)
INFO - root - 2019-11-06 18:39:43.774504: step 31570, total loss = 4.06, predict loss = 1.14 (57.4 examples/sec; 0.070 sec/batch; 2h:17m:35s remains)
INFO - root - 2019-11-06 18:39:44.442260: step 31580, total loss = 3.93, predict loss = 1.20 (72.9 examples/sec; 0.055 sec/batch; 1h:48m:20s remains)
INFO - root - 2019-11-06 18:39:44.949446: step 31590, total loss = 2.86, predict loss = 0.78 (96.7 examples/sec; 0.041 sec/batch; 1h:21m:37s remains)
INFO - root - 2019-11-06 18:39:45.409969: step 31600, total loss = 5.32, predict loss = 1.43 (96.1 examples/sec; 0.042 sec/batch; 1h:22m:07s remains)
INFO - root - 2019-11-06 18:39:46.609919: step 31610, total loss = 3.86, predict loss = 1.11 (70.9 examples/sec; 0.056 sec/batch; 1h:51m:20s remains)
INFO - root - 2019-11-06 18:39:47.310027: step 31620, total loss = 4.51, predict loss = 1.25 (62.9 examples/sec; 0.064 sec/batch; 2h:05m:29s remains)
INFO - root - 2019-11-06 18:39:48.087560: step 31630, total loss = 5.59, predict loss = 1.62 (56.2 examples/sec; 0.071 sec/batch; 2h:20m:28s remains)
INFO - root - 2019-11-06 18:39:48.860226: step 31640, total loss = 5.18, predict loss = 1.54 (53.3 examples/sec; 0.075 sec/batch; 2h:28m:05s remains)
INFO - root - 2019-11-06 18:39:49.625945: step 31650, total loss = 4.50, predict loss = 1.25 (64.1 examples/sec; 0.062 sec/batch; 2h:03m:08s remains)
INFO - root - 2019-11-06 18:39:50.180133: step 31660, total loss = 6.00, predict loss = 1.64 (98.1 examples/sec; 0.041 sec/batch; 1h:20m:25s remains)
INFO - root - 2019-11-06 18:39:50.636397: step 31670, total loss = 3.83, predict loss = 1.07 (96.3 examples/sec; 0.042 sec/batch; 1h:21m:56s remains)
INFO - root - 2019-11-06 18:39:51.086990: step 31680, total loss = 4.40, predict loss = 1.28 (96.2 examples/sec; 0.042 sec/batch; 1h:21m:58s remains)
INFO - root - 2019-11-06 18:39:52.472206: step 31690, total loss = 5.06, predict loss = 1.49 (61.3 examples/sec; 0.065 sec/batch; 2h:08m:35s remains)
INFO - root - 2019-11-06 18:39:53.200359: step 31700, total loss = 3.74, predict loss = 1.11 (63.8 examples/sec; 0.063 sec/batch; 2h:03m:33s remains)
INFO - root - 2019-11-06 18:39:53.902635: step 31710, total loss = 5.42, predict loss = 1.56 (63.0 examples/sec; 0.063 sec/batch; 2h:05m:07s remains)
INFO - root - 2019-11-06 18:39:54.622487: step 31720, total loss = 3.41, predict loss = 1.07 (59.3 examples/sec; 0.067 sec/batch; 2h:13m:03s remains)
INFO - root - 2019-11-06 18:39:55.326031: step 31730, total loss = 5.64, predict loss = 1.59 (67.7 examples/sec; 0.059 sec/batch; 1h:56m:27s remains)
INFO - root - 2019-11-06 18:39:55.846881: step 31740, total loss = 3.96, predict loss = 1.11 (97.2 examples/sec; 0.041 sec/batch; 1h:21m:07s remains)
INFO - root - 2019-11-06 18:39:56.291721: step 31750, total loss = 6.38, predict loss = 1.81 (94.5 examples/sec; 0.042 sec/batch; 1h:23m:24s remains)
INFO - root - 2019-11-06 18:39:57.517706: step 31760, total loss = 5.55, predict loss = 1.59 (63.0 examples/sec; 0.063 sec/batch; 2h:05m:03s remains)
INFO - root - 2019-11-06 18:39:58.256767: step 31770, total loss = 4.55, predict loss = 1.30 (61.8 examples/sec; 0.065 sec/batch; 2h:07m:34s remains)
INFO - root - 2019-11-06 18:39:59.045558: step 31780, total loss = 4.46, predict loss = 1.35 (59.4 examples/sec; 0.067 sec/batch; 2h:12m:35s remains)
INFO - root - 2019-11-06 18:39:59.832763: step 31790, total loss = 4.28, predict loss = 1.21 (61.5 examples/sec; 0.065 sec/batch; 2h:08m:08s remains)
INFO - root - 2019-11-06 18:40:00.583868: step 31800, total loss = 5.80, predict loss = 1.70 (60.3 examples/sec; 0.066 sec/batch; 2h:10m:43s remains)
INFO - root - 2019-11-06 18:40:01.181069: step 31810, total loss = 5.74, predict loss = 1.65 (93.1 examples/sec; 0.043 sec/batch; 1h:24m:36s remains)
INFO - root - 2019-11-06 18:40:01.626203: step 31820, total loss = 5.49, predict loss = 1.62 (95.8 examples/sec; 0.042 sec/batch; 1h:22m:15s remains)
INFO - root - 2019-11-06 18:40:02.071937: step 31830, total loss = 5.03, predict loss = 1.42 (125.2 examples/sec; 0.032 sec/batch; 1h:02m:56s remains)
INFO - root - 2019-11-06 18:40:03.415307: step 31840, total loss = 4.21, predict loss = 1.19 (60.3 examples/sec; 0.066 sec/batch; 2h:10m:39s remains)
INFO - root - 2019-11-06 18:40:04.164526: step 31850, total loss = 4.75, predict loss = 1.37 (48.3 examples/sec; 0.083 sec/batch; 2h:42m:54s remains)
INFO - root - 2019-11-06 18:40:04.930379: step 31860, total loss = 4.65, predict loss = 1.43 (59.4 examples/sec; 0.067 sec/batch; 2h:12m:31s remains)
INFO - root - 2019-11-06 18:40:05.640219: step 31870, total loss = 3.07, predict loss = 0.88 (63.6 examples/sec; 0.063 sec/batch; 2h:03m:54s remains)
INFO - root - 2019-11-06 18:40:06.337930: step 31880, total loss = 5.45, predict loss = 1.63 (66.6 examples/sec; 0.060 sec/batch; 1h:58m:14s remains)
INFO - root - 2019-11-06 18:40:06.835105: step 31890, total loss = 4.88, predict loss = 1.44 (99.3 examples/sec; 0.040 sec/batch; 1h:19m:19s remains)
INFO - root - 2019-11-06 18:40:07.292343: step 31900, total loss = 5.88, predict loss = 1.66 (90.4 examples/sec; 0.044 sec/batch; 1h:27m:05s remains)
INFO - root - 2019-11-06 18:40:08.503185: step 31910, total loss = 4.51, predict loss = 1.32 (65.7 examples/sec; 0.061 sec/batch; 1h:59m:47s remains)
INFO - root - 2019-11-06 18:40:09.252112: step 31920, total loss = 5.25, predict loss = 1.57 (57.5 examples/sec; 0.070 sec/batch; 2h:16m:52s remains)
INFO - root - 2019-11-06 18:40:10.006092: step 31930, total loss = 6.35, predict loss = 1.77 (60.3 examples/sec; 0.066 sec/batch; 2h:10m:33s remains)
INFO - root - 2019-11-06 18:40:10.787627: step 31940, total loss = 4.06, predict loss = 1.23 (60.5 examples/sec; 0.066 sec/batch; 2h:10m:08s remains)
INFO - root - 2019-11-06 18:40:11.537214: step 31950, total loss = 6.21, predict loss = 1.78 (62.8 examples/sec; 0.064 sec/batch; 2h:05m:21s remains)
INFO - root - 2019-11-06 18:40:12.066555: step 31960, total loss = 5.00, predict loss = 1.46 (97.8 examples/sec; 0.041 sec/batch; 1h:20m:25s remains)
INFO - root - 2019-11-06 18:40:12.547956: step 31970, total loss = 5.15, predict loss = 1.50 (95.9 examples/sec; 0.042 sec/batch; 1h:22m:02s remains)
INFO - root - 2019-11-06 18:40:13.711150: step 31980, total loss = 4.64, predict loss = 1.35 (5.3 examples/sec; 0.761 sec/batch; 24h:56m:07s remains)
INFO - root - 2019-11-06 18:40:14.395529: step 31990, total loss = 5.68, predict loss = 1.70 (61.8 examples/sec; 0.065 sec/batch; 2h:07m:22s remains)
INFO - root - 2019-11-06 18:40:15.140459: step 32000, total loss = 5.09, predict loss = 1.43 (57.8 examples/sec; 0.069 sec/batch; 2h:16m:05s remains)
INFO - root - 2019-11-06 18:40:15.906107: step 32010, total loss = 3.03, predict loss = 0.96 (53.4 examples/sec; 0.075 sec/batch; 2h:27m:10s remains)
INFO - root - 2019-11-06 18:40:16.641636: step 32020, total loss = 4.73, predict loss = 1.31 (56.9 examples/sec; 0.070 sec/batch; 2h:18m:14s remains)
INFO - root - 2019-11-06 18:40:17.320842: step 32030, total loss = 5.90, predict loss = 1.73 (89.8 examples/sec; 0.045 sec/batch; 1h:27m:32s remains)
INFO - root - 2019-11-06 18:40:17.760804: step 32040, total loss = 5.32, predict loss = 1.47 (98.9 examples/sec; 0.040 sec/batch; 1h:19m:31s remains)
INFO - root - 2019-11-06 18:40:18.248227: step 32050, total loss = 4.00, predict loss = 1.19 (88.3 examples/sec; 0.045 sec/batch; 1h:29m:01s remains)
INFO - root - 2019-11-06 18:40:19.482017: step 32060, total loss = 4.19, predict loss = 1.18 (64.2 examples/sec; 0.062 sec/batch; 2h:02m:29s remains)
INFO - root - 2019-11-06 18:40:20.227987: step 32070, total loss = 6.39, predict loss = 1.90 (56.3 examples/sec; 0.071 sec/batch; 2h:19m:43s remains)
INFO - root - 2019-11-06 18:40:20.997640: step 32080, total loss = 5.53, predict loss = 1.45 (60.3 examples/sec; 0.066 sec/batch; 2h:10m:24s remains)
INFO - root - 2019-11-06 18:40:21.759615: step 32090, total loss = 6.21, predict loss = 1.80 (56.3 examples/sec; 0.071 sec/batch; 2h:19m:38s remains)
INFO - root - 2019-11-06 18:40:22.554487: step 32100, total loss = 5.13, predict loss = 1.47 (59.2 examples/sec; 0.068 sec/batch; 2h:12m:49s remains)
INFO - root - 2019-11-06 18:40:23.083212: step 32110, total loss = 3.72, predict loss = 1.10 (98.0 examples/sec; 0.041 sec/batch; 1h:20m:13s remains)
INFO - root - 2019-11-06 18:40:23.538501: step 32120, total loss = 5.27, predict loss = 1.50 (103.4 examples/sec; 0.039 sec/batch; 1h:16m:02s remains)
INFO - root - 2019-11-06 18:40:24.694856: step 32130, total loss = 4.93, predict loss = 1.43 (72.7 examples/sec; 0.055 sec/batch; 1h:48m:00s remains)
INFO - root - 2019-11-06 18:40:25.441801: step 32140, total loss = 6.18, predict loss = 1.65 (52.4 examples/sec; 0.076 sec/batch; 2h:29m:49s remains)
INFO - root - 2019-11-06 18:40:26.160213: step 32150, total loss = 5.06, predict loss = 1.47 (60.1 examples/sec; 0.067 sec/batch; 2h:10m:41s remains)
INFO - root - 2019-11-06 18:40:27.002914: step 32160, total loss = 4.63, predict loss = 1.34 (51.0 examples/sec; 0.078 sec/batch; 2h:34m:04s remains)
INFO - root - 2019-11-06 18:40:27.843375: step 32170, total loss = 3.13, predict loss = 0.85 (59.8 examples/sec; 0.067 sec/batch; 2h:11m:28s remains)
INFO - root - 2019-11-06 18:40:28.459696: step 32180, total loss = 4.91, predict loss = 1.33 (96.7 examples/sec; 0.041 sec/batch; 1h:21m:15s remains)
INFO - root - 2019-11-06 18:40:28.897175: step 32190, total loss = 4.95, predict loss = 1.38 (95.8 examples/sec; 0.042 sec/batch; 1h:21m:57s remains)
INFO - root - 2019-11-06 18:40:29.345565: step 32200, total loss = 5.51, predict loss = 1.55 (94.4 examples/sec; 0.042 sec/batch; 1h:23m:11s remains)
INFO - root - 2019-11-06 18:40:30.636324: step 32210, total loss = 5.42, predict loss = 1.57 (65.0 examples/sec; 0.062 sec/batch; 2h:00m:45s remains)
INFO - root - 2019-11-06 18:40:31.357107: step 32220, total loss = 4.59, predict loss = 1.36 (61.2 examples/sec; 0.065 sec/batch; 2h:08m:13s remains)
INFO - root - 2019-11-06 18:40:32.081276: step 32230, total loss = 4.80, predict loss = 1.39 (59.2 examples/sec; 0.068 sec/batch; 2h:12m:40s remains)
INFO - root - 2019-11-06 18:40:32.901952: step 32240, total loss = 3.64, predict loss = 1.03 (52.4 examples/sec; 0.076 sec/batch; 2h:29m:49s remains)
INFO - root - 2019-11-06 18:40:33.622544: step 32250, total loss = 5.38, predict loss = 1.47 (85.2 examples/sec; 0.047 sec/batch; 1h:32m:08s remains)
INFO - root - 2019-11-06 18:40:34.123737: step 32260, total loss = 4.28, predict loss = 1.25 (100.3 examples/sec; 0.040 sec/batch; 1h:18m:17s remains)
INFO - root - 2019-11-06 18:40:34.625511: step 32270, total loss = 4.52, predict loss = 1.20 (85.4 examples/sec; 0.047 sec/batch; 1h:31m:57s remains)
INFO - root - 2019-11-06 18:40:36.011674: step 32280, total loss = 3.86, predict loss = 1.14 (64.8 examples/sec; 0.062 sec/batch; 2h:01m:12s remains)
INFO - root - 2019-11-06 18:40:36.747393: step 32290, total loss = 4.38, predict loss = 1.31 (68.2 examples/sec; 0.059 sec/batch; 1h:54m:59s remains)
INFO - root - 2019-11-06 18:40:37.467489: step 32300, total loss = 6.59, predict loss = 1.85 (57.7 examples/sec; 0.069 sec/batch; 2h:16m:00s remains)
INFO - root - 2019-11-06 18:40:38.140262: step 32310, total loss = 5.13, predict loss = 1.54 (65.3 examples/sec; 0.061 sec/batch; 2h:00m:09s remains)
INFO - root - 2019-11-06 18:40:38.853953: step 32320, total loss = 5.49, predict loss = 1.56 (54.2 examples/sec; 0.074 sec/batch; 2h:24m:43s remains)
INFO - root - 2019-11-06 18:40:39.494440: step 32330, total loss = 4.98, predict loss = 1.45 (97.0 examples/sec; 0.041 sec/batch; 1h:20m:53s remains)
INFO - root - 2019-11-06 18:40:39.931980: step 32340, total loss = 4.31, predict loss = 1.24 (100.7 examples/sec; 0.040 sec/batch; 1h:17m:51s remains)
INFO - root - 2019-11-06 18:40:40.400052: step 32350, total loss = 3.51, predict loss = 1.02 (97.5 examples/sec; 0.041 sec/batch; 1h:20m:27s remains)
INFO - root - 2019-11-06 18:40:41.713174: step 32360, total loss = 4.22, predict loss = 1.27 (49.1 examples/sec; 0.081 sec/batch; 2h:39m:40s remains)
INFO - root - 2019-11-06 18:40:42.479106: step 32370, total loss = 4.21, predict loss = 1.21 (57.8 examples/sec; 0.069 sec/batch; 2h:15m:42s remains)
INFO - root - 2019-11-06 18:40:43.298757: step 32380, total loss = 6.22, predict loss = 1.80 (56.1 examples/sec; 0.071 sec/batch; 2h:19m:51s remains)
INFO - root - 2019-11-06 18:40:44.050883: step 32390, total loss = 4.26, predict loss = 1.17 (58.2 examples/sec; 0.069 sec/batch; 2h:14m:42s remains)
INFO - root - 2019-11-06 18:40:44.847713: step 32400, total loss = 4.55, predict loss = 1.32 (66.8 examples/sec; 0.060 sec/batch; 1h:57m:19s remains)
INFO - root - 2019-11-06 18:40:45.389391: step 32410, total loss = 4.73, predict loss = 1.38 (96.5 examples/sec; 0.041 sec/batch; 1h:21m:13s remains)
INFO - root - 2019-11-06 18:40:45.842312: step 32420, total loss = 5.58, predict loss = 1.60 (95.7 examples/sec; 0.042 sec/batch; 1h:21m:54s remains)
INFO - root - 2019-11-06 18:40:47.101449: step 32430, total loss = 4.44, predict loss = 1.30 (66.3 examples/sec; 0.060 sec/batch; 1h:58m:16s remains)
INFO - root - 2019-11-06 18:40:47.867940: step 32440, total loss = 5.19, predict loss = 1.47 (59.9 examples/sec; 0.067 sec/batch; 2h:10m:48s remains)
INFO - root - 2019-11-06 18:40:48.807005: step 32450, total loss = 4.67, predict loss = 1.30 (41.3 examples/sec; 0.097 sec/batch; 3h:09m:31s remains)
INFO - root - 2019-11-06 18:40:49.688966: step 32460, total loss = 3.08, predict loss = 0.91 (73.4 examples/sec; 0.054 sec/batch; 1h:46m:44s remains)
INFO - root - 2019-11-06 18:40:50.379365: step 32470, total loss = 5.58, predict loss = 1.60 (56.6 examples/sec; 0.071 sec/batch; 2h:18m:32s remains)
INFO - root - 2019-11-06 18:40:50.964649: step 32480, total loss = 6.12, predict loss = 1.73 (96.5 examples/sec; 0.041 sec/batch; 1h:21m:12s remains)
INFO - root - 2019-11-06 18:40:51.445699: step 32490, total loss = 5.62, predict loss = 1.60 (93.5 examples/sec; 0.043 sec/batch; 1h:23m:44s remains)
INFO - root - 2019-11-06 18:40:51.892048: step 32500, total loss = 6.57, predict loss = 1.86 (99.0 examples/sec; 0.040 sec/batch; 1h:19m:05s remains)
INFO - root - 2019-11-06 18:40:53.263285: step 32510, total loss = 5.24, predict loss = 1.46 (58.2 examples/sec; 0.069 sec/batch; 2h:14m:29s remains)
INFO - root - 2019-11-06 18:40:54.004138: step 32520, total loss = 4.88, predict loss = 1.36 (59.8 examples/sec; 0.067 sec/batch; 2h:10m:56s remains)
INFO - root - 2019-11-06 18:40:54.789717: step 32530, total loss = 6.29, predict loss = 1.78 (56.1 examples/sec; 0.071 sec/batch; 2h:19m:37s remains)
INFO - root - 2019-11-06 18:40:55.531001: step 32540, total loss = 4.49, predict loss = 1.39 (57.6 examples/sec; 0.069 sec/batch; 2h:16m:02s remains)
INFO - root - 2019-11-06 18:40:56.234676: step 32550, total loss = 5.68, predict loss = 1.61 (81.8 examples/sec; 0.049 sec/batch; 1h:35m:42s remains)
INFO - root - 2019-11-06 18:40:56.713005: step 32560, total loss = 6.74, predict loss = 1.88 (89.9 examples/sec; 0.044 sec/batch; 1h:27m:03s remains)
INFO - root - 2019-11-06 18:40:57.180635: step 32570, total loss = 4.67, predict loss = 1.38 (97.8 examples/sec; 0.041 sec/batch; 1h:20m:00s remains)
INFO - root - 2019-11-06 18:40:58.454965: step 32580, total loss = 3.47, predict loss = 1.10 (45.9 examples/sec; 0.087 sec/batch; 2h:50m:39s remains)
INFO - root - 2019-11-06 18:40:59.222484: step 32590, total loss = 5.36, predict loss = 1.54 (59.9 examples/sec; 0.067 sec/batch; 2h:10m:40s remains)
INFO - root - 2019-11-06 18:40:59.964959: step 32600, total loss = 4.43, predict loss = 1.33 (59.1 examples/sec; 0.068 sec/batch; 2h:12m:23s remains)
INFO - root - 2019-11-06 18:41:00.762273: step 32610, total loss = 4.38, predict loss = 1.34 (54.9 examples/sec; 0.073 sec/batch; 2h:22m:32s remains)
INFO - root - 2019-11-06 18:41:01.581252: step 32620, total loss = 4.00, predict loss = 1.17 (59.5 examples/sec; 0.067 sec/batch; 2h:11m:33s remains)
INFO - root - 2019-11-06 18:41:02.169132: step 32630, total loss = 5.43, predict loss = 1.55 (93.5 examples/sec; 0.043 sec/batch; 1h:23m:41s remains)
INFO - root - 2019-11-06 18:41:02.617093: step 32640, total loss = 4.66, predict loss = 1.35 (96.2 examples/sec; 0.042 sec/batch; 1h:21m:20s remains)
INFO - root - 2019-11-06 18:41:03.091338: step 32650, total loss = 3.11, predict loss = 0.94 (124.5 examples/sec; 0.032 sec/batch; 1h:02m:50s remains)
INFO - root - 2019-11-06 18:41:04.480056: step 32660, total loss = 4.63, predict loss = 1.31 (53.1 examples/sec; 0.075 sec/batch; 2h:27m:11s remains)
INFO - root - 2019-11-06 18:41:05.252639: step 32670, total loss = 3.38, predict loss = 0.98 (69.2 examples/sec; 0.058 sec/batch; 1h:52m:57s remains)
INFO - root - 2019-11-06 18:41:05.971148: step 32680, total loss = 4.07, predict loss = 1.12 (55.8 examples/sec; 0.072 sec/batch; 2h:20m:13s remains)
INFO - root - 2019-11-06 18:41:06.679534: step 32690, total loss = 6.43, predict loss = 1.76 (67.1 examples/sec; 0.060 sec/batch; 1h:56m:34s remains)
INFO - root - 2019-11-06 18:41:07.355657: step 32700, total loss = 3.69, predict loss = 1.07 (77.7 examples/sec; 0.051 sec/batch; 1h:40m:34s remains)
INFO - root - 2019-11-06 18:41:07.814725: step 32710, total loss = 4.41, predict loss = 1.39 (99.4 examples/sec; 0.040 sec/batch; 1h:18m:38s remains)
INFO - root - 2019-11-06 18:41:08.254211: step 32720, total loss = 6.15, predict loss = 1.73 (98.0 examples/sec; 0.041 sec/batch; 1h:19m:47s remains)
INFO - root - 2019-11-06 18:41:09.495510: step 32730, total loss = 5.61, predict loss = 1.65 (65.0 examples/sec; 0.062 sec/batch; 2h:00m:13s remains)
INFO - root - 2019-11-06 18:41:10.250269: step 32740, total loss = 3.34, predict loss = 0.97 (45.4 examples/sec; 0.088 sec/batch; 2h:52m:17s remains)
INFO - root - 2019-11-06 18:41:11.088164: step 32750, total loss = 5.42, predict loss = 1.59 (56.0 examples/sec; 0.071 sec/batch; 2h:19m:32s remains)
INFO - root - 2019-11-06 18:41:11.824003: step 32760, total loss = 5.89, predict loss = 1.76 (56.4 examples/sec; 0.071 sec/batch; 2h:18m:35s remains)
INFO - root - 2019-11-06 18:41:12.606144: step 32770, total loss = 4.83, predict loss = 1.31 (64.5 examples/sec; 0.062 sec/batch; 2h:01m:07s remains)
INFO - root - 2019-11-06 18:41:13.174165: step 32780, total loss = 4.19, predict loss = 1.23 (92.8 examples/sec; 0.043 sec/batch; 1h:24m:11s remains)
INFO - root - 2019-11-06 18:41:13.622387: step 32790, total loss = 5.45, predict loss = 1.44 (97.6 examples/sec; 0.041 sec/batch; 1h:20m:01s remains)
INFO - root - 2019-11-06 18:41:14.972009: step 32800, total loss = 5.18, predict loss = 1.50 (4.4 examples/sec; 0.916 sec/batch; 29h:48m:58s remains)
INFO - root - 2019-11-06 18:41:15.693284: step 32810, total loss = 3.27, predict loss = 0.93 (70.6 examples/sec; 0.057 sec/batch; 1h:50m:42s remains)
INFO - root - 2019-11-06 18:41:16.450423: step 32820, total loss = 3.98, predict loss = 1.16 (54.2 examples/sec; 0.074 sec/batch; 2h:24m:07s remains)
INFO - root - 2019-11-06 18:41:17.223171: step 32830, total loss = 4.78, predict loss = 1.37 (65.6 examples/sec; 0.061 sec/batch; 1h:59m:01s remains)
INFO - root - 2019-11-06 18:41:17.949454: step 32840, total loss = 5.94, predict loss = 1.66 (62.5 examples/sec; 0.064 sec/batch; 2h:04m:57s remains)
INFO - root - 2019-11-06 18:41:18.592644: step 32850, total loss = 3.33, predict loss = 0.96 (93.1 examples/sec; 0.043 sec/batch; 1h:23m:55s remains)
INFO - root - 2019-11-06 18:41:19.070749: step 32860, total loss = 5.15, predict loss = 1.46 (91.5 examples/sec; 0.044 sec/batch; 1h:25m:21s remains)
INFO - root - 2019-11-06 18:41:19.521504: step 32870, total loss = 5.63, predict loss = 1.57 (99.2 examples/sec; 0.040 sec/batch; 1h:18m:43s remains)
INFO - root - 2019-11-06 18:41:20.788294: step 32880, total loss = 6.59, predict loss = 1.82 (66.5 examples/sec; 0.060 sec/batch; 1h:57m:24s remains)
INFO - root - 2019-11-06 18:41:21.536359: step 32890, total loss = 4.99, predict loss = 1.37 (59.8 examples/sec; 0.067 sec/batch; 2h:10m:29s remains)
INFO - root - 2019-11-06 18:41:22.252849: step 32900, total loss = 5.62, predict loss = 1.65 (58.6 examples/sec; 0.068 sec/batch; 2h:13m:10s remains)
INFO - root - 2019-11-06 18:41:23.009436: step 32910, total loss = 3.19, predict loss = 0.94 (68.2 examples/sec; 0.059 sec/batch; 1h:54m:28s remains)
INFO - root - 2019-11-06 18:41:23.807569: step 32920, total loss = 4.86, predict loss = 1.43 (64.0 examples/sec; 0.063 sec/batch; 2h:02m:00s remains)
INFO - root - 2019-11-06 18:41:24.372398: step 32930, total loss = 4.25, predict loss = 1.28 (102.7 examples/sec; 0.039 sec/batch; 1h:15m:58s remains)
INFO - root - 2019-11-06 18:41:24.804026: step 32940, total loss = 4.68, predict loss = 1.34 (99.1 examples/sec; 0.040 sec/batch; 1h:18m:44s remains)
INFO - root - 2019-11-06 18:41:25.999369: step 32950, total loss = 5.85, predict loss = 1.62 (58.1 examples/sec; 0.069 sec/batch; 2h:14m:24s remains)
INFO - root - 2019-11-06 18:41:26.798351: step 32960, total loss = 4.31, predict loss = 1.28 (51.3 examples/sec; 0.078 sec/batch; 2h:32m:03s remains)
INFO - root - 2019-11-06 18:41:27.595566: step 32970, total loss = 3.73, predict loss = 1.03 (56.5 examples/sec; 0.071 sec/batch; 2h:18m:08s remains)
INFO - root - 2019-11-06 18:41:28.359089: step 32980, total loss = 3.81, predict loss = 1.21 (67.1 examples/sec; 0.060 sec/batch; 1h:56m:17s remains)
INFO - root - 2019-11-06 18:41:29.164108: step 32990, total loss = 5.27, predict loss = 1.55 (63.5 examples/sec; 0.063 sec/batch; 2h:02m:49s remains)
INFO - root - 2019-11-06 18:41:29.750062: step 33000, total loss = 5.09, predict loss = 1.51 (98.5 examples/sec; 0.041 sec/batch; 1h:19m:10s remains)
INFO - root - 2019-11-06 18:41:30.220419: step 33010, total loss = 6.12, predict loss = 1.71 (97.3 examples/sec; 0.041 sec/batch; 1h:20m:07s remains)
INFO - root - 2019-11-06 18:41:30.680277: step 33020, total loss = 5.36, predict loss = 1.55 (94.6 examples/sec; 0.042 sec/batch; 1h:22m:28s remains)
INFO - root - 2019-11-06 18:41:31.994862: step 33030, total loss = 4.31, predict loss = 1.20 (58.1 examples/sec; 0.069 sec/batch; 2h:14m:16s remains)
INFO - root - 2019-11-06 18:41:32.814356: step 33040, total loss = 5.20, predict loss = 1.46 (51.0 examples/sec; 0.078 sec/batch; 2h:32m:49s remains)
INFO - root - 2019-11-06 18:41:33.562945: step 33050, total loss = 5.83, predict loss = 1.66 (61.9 examples/sec; 0.065 sec/batch; 2h:05m:57s remains)
INFO - root - 2019-11-06 18:41:34.293105: step 33060, total loss = 5.07, predict loss = 1.49 (60.0 examples/sec; 0.067 sec/batch; 2h:09m:50s remains)
INFO - root - 2019-11-06 18:41:35.014325: step 33070, total loss = 3.38, predict loss = 1.06 (76.1 examples/sec; 0.053 sec/batch; 1h:42m:29s remains)
INFO - root - 2019-11-06 18:41:35.548724: step 33080, total loss = 3.73, predict loss = 1.03 (96.7 examples/sec; 0.041 sec/batch; 1h:20m:34s remains)
INFO - root - 2019-11-06 18:41:36.026202: step 33090, total loss = 4.36, predict loss = 1.30 (91.7 examples/sec; 0.044 sec/batch; 1h:24m:59s remains)
INFO - root - 2019-11-06 18:41:37.265444: step 33100, total loss = 4.31, predict loss = 1.23 (65.9 examples/sec; 0.061 sec/batch; 1h:58m:13s remains)
INFO - root - 2019-11-06 18:41:37.972349: step 33110, total loss = 5.57, predict loss = 1.59 (53.5 examples/sec; 0.075 sec/batch; 2h:25m:34s remains)
INFO - root - 2019-11-06 18:41:38.791373: step 33120, total loss = 5.53, predict loss = 1.64 (50.7 examples/sec; 0.079 sec/batch; 2h:33m:42s remains)
