INFO - bisenet-v2 - Running command 'main'
INFO - bisenet-v2 - Started run with ID "22"
INFO - root - nvidia-ml-py is not installed, automatically select gpu is disabled!
WARNING:tensorflow:From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - tensorflow - From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:88: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:88: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:100: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:100: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
INFO - root - preproces -- augment
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:108: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:108: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:218: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map__image_mirroring, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(_image_mirroring, num_parallel_calls=threads)
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:119: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:119: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:122: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:122: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:123: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:123: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:220: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map__image_scaling, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(_image_scaling, num_parallel_calls=threads)
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:148: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:148: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.

/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:223: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map_<lambda>, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  num_parallel_calls=threads)
/home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:224: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map_<lambda>, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(lambda image, label: _apply_with_random_selector(image, lambda x, ordering: _distort_color
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:235: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/Dataset/dataset.py:235: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df89208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df89208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df89208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df89208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7df89400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7df89400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7df89400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7df89400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dfe4cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dfe4cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dfe4cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dfe4cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dfe44a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dfe44a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dfe44a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dfe44a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dedbbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dedbbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dedbbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dedbbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7df239e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7df239e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7df239e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7df239e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df60630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df60630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df60630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df60630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7eff7df70fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7eff7df70fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7eff7df70fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7eff7df70fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7de09c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7de09c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7de09c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7de09c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7de24898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7de24898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7de24898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7de24898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7de09c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7de09c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7de09c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7de09c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7de247b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7de247b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7de247b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7de247b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7df60940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7df60940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7df60940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7df60940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dddaf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dddaf28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dddaf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dddaf28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7de247b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7de247b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7de247b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7de247b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dcfea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dcfea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dcfea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dcfea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ddce518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ddce518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ddce518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ddce518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df89f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df89f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df89f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df89f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ddda898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ddda898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ddda898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ddda898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dc1f898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dc1f898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dc1f898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dc1f898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dbbe208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dbbe208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dbbe208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dbbe208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dbae390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dbae390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dbae390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7dbae390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7df60c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7df60c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7df60c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7df60c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7db17710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7db17710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7db17710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7db17710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dbae198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dbae198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dbae198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dbae198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7da976a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7da976a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7da976a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7da976a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dadf748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dadf748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dadf748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dadf748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7db17550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7db17550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7db17550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7db17550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dc1fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dc1fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dc1fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dc1fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7da976a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7da976a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7da976a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7da976a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d9fff60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d9fff60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d9fff60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d9fff60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d9274a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d9274a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d9274a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d9274a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7da66da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7da66da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7da66da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7da66da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7da97550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7da97550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7da97550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7da97550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7da66da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7da66da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7da66da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7da66da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ddce828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ddce828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ddce828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ddce828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d927160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d927160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d927160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d927160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d7deac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d7deac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d7deac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d7deac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7deef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7deef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7deef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7deef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7f6b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7f6b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7f6b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7f6b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d8f3c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7f6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7f6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7f6b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7f6b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d6020f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d6020f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d6020f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d6020f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d602b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d602b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d602b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d602b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d4b84a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d4b84a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d4b84a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d4b84a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7f6518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7f6518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7f6518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d7f6518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d69d588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d69d588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d69d588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d69d588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d567d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d567d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d567d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d567d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d37cef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d37cef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d37cef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d37cef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d4f5470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d4f5470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d4f5470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d4f5470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d3f7978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d3f7978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d3f7978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d3f7978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d3f7ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d3f7ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d3f7ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d3f7ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d37cf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d37cf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d37cf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d37cf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d337b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d337b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d337b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d337b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d468940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d468940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d468940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d468940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d429a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d429a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d429a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d429a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d2d9940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d2d9940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d2d9940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d2d9940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d468940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d468940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d468940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d468940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d2d5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d2d5048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d2d5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d2d5048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d2d9240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d2d9240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d2d9240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d2d9240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d2048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d2048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d2048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d2048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d1ce240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d1ce240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d1ce240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d1ce240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d29b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d29b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d29b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d29b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d2d5710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d2d5710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d2d5710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d2d5710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d29b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d29b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d29b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d29b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d041278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d041278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d041278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d041278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d14bb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d14bb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d14bb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d14bb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d0e7208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d0e7208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d0e7208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d0e7208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d2c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d2c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d2c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d1d2c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d1d2c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d1d2c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d1d2c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d1d2c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d77bc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d77bc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d77bc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d77bc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d77b978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d77b978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d77b978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d77b978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cebbdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cebbdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cebbdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cebbdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dff3438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dff3438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dff3438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7dff3438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cebbc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cebbc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cebbc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cebbc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ce7f898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ce7f898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ce7f898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ce7f898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cdcd748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cdcd748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cdcd748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cdcd748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ceafcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ceafcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ceafcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ceafcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ccecc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ccecc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ccecc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ccecc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ceafeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ceafeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ceafeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ceafeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cd08e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cd08e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cd08e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cd08e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d14b780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d14b780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d14b780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7d14b780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cdcdbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cdcdbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cdcdbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cdcdbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cd08940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cd08940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cd08940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cd08940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ccecb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ccecb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ccecb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ccecb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cdcd668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cdcd668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cdcd668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cdcd668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cb9a978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cb9a978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cb9a978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cb9a978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cdcd160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cdcd160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cdcd160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cdcd160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cb9a8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cb9a8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cb9a8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cb9a8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cc21160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cc21160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cc21160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cc21160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ca14cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ca14cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ca14cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ca14cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ca27a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ca27a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ca27a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ca27a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cd2aba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cd2aba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cd2aba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7cd2aba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cd2a9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cd2a9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cd2a9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7cd2a9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c91a7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c91a7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c91a7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c91a7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ce99f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ce99f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ce99f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7ce99f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ca14b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ca14b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ca14b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ca14b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c92fb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c92fb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c92fb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c92fb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c800b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c800b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c800b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c800b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c91a3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c91a3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c91a3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c91a3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c73ef28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c73ef28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c73ef28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c73ef28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c8699e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c8699e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c8699e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c8699e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c7ebf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c7ebf98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c7ebf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c7ebf98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c800fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c800fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c800fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c800fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c649518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c649518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c649518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c649518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c8209b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c8209b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c8209b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7c8209b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ca14208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ca14208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ca14208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7ca14208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c886da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c886da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c886da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c886da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c507e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c507e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c507e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c507e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:179: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:179: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c5f2d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c5f2d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c5f2d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c5f2d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df23cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df23cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df23cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7df23cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7df23ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7df23ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7df23ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7df23ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c4ee780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c4ee780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c4ee780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c4ee780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7cdcd860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7cdcd860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7cdcd860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7cdcd860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c5f2e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c5f2e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c5f2e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c5f2e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c475588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c475588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c475588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c475588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c464b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c464b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c464b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c464b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c33d5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c33d5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c33d5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c33d5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c33de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c33de10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c33de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c33de10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c3779e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c3779e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c3779e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c3779e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c2be278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c2be278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c2be278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c2be278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c2beb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c2beb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c2beb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c2beb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c2beb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c2beb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c2beb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c2beb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c2be400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c2be400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c2be400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c2be400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c287cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c287cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c287cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c287cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c287940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c287940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c287940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c287940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c1fb630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c1fb630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c1fb630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c1fb630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c281da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c281da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c281da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c281da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c1fb198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c1fb198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c1fb198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c1fb198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c1fb908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c1fb908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c1fb908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c1fb908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c1e1e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c1e1e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c1e1e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c1e1e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c13bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c13bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c13bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c13bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c0c2908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c0c2908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c0c2908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7c0c2908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c13bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c13bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c13bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c13bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c0c2358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c0c2358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c0c2358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c0c2358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77fa9518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77fa9518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77fa9518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77fa9518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:217: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:217: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:221: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:221: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:224: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:224: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:229: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:229: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:240: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/cuocduaso/bisenet-tensorflow/models/bisenet.py:240: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING - root - random_scale is not explicitly specified, using default value: False
WARNING - root - random_mirror is not explicitly specified, using default value: True
INFO - root - preproces -- None
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77cd2c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77cd2c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77cd2c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77cd2c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c9d048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c9d048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c9d048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c9d048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9dac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9dac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9dac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9dac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c9d908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c9d908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c9d908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c9d908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9dc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9dc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9dc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9dc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c9d630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c9d630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c9d630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c9d630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9d828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9d828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77fc34e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77fc34e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77fc34e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77fc34e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c0cdc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c0cdc18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c0cdc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c0cdc18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c63e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c63e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c63e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c63e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7eff77c63048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7eff77c63048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7eff77c63048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7eff77c63048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77fc3cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77fc3cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77fc3cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77fc3cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbf940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbf940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbf940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbf940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cbfda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cbfda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cbfda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cbfda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbf080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbf080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbf080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbf080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cbfa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cbfa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cbfa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cbfa20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbfb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbfb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbfb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbfb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7df60f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7df60f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7df60f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff7df60f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbfd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbfd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbfd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cbfd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c9df28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c9df28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c9df28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c9df28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cae9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cae9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cae9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77cae9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c61390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c61390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c61390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c61390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c824a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c824a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c824a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c824a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cae7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cae7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cae7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cae7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c5ec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c5ec50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c5ec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c5ec50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cd2b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cd2b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cd2b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cd2b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c63710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c63710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c63710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c63710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c5e320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c5e320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c5e320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c5e320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c83240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c83240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c83240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c83240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c5e8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c5e8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c5e8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c5e8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c82cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c82cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c82cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c82cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c82be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c82be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c82be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c82be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ce4fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ce4fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ce4fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ce4fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c63780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c63780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c63780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c63780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ce48d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ce48d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ce48d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ce48d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cff390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cff390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cff390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cff390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c16ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c16ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c16ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c16ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77ce4128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77ce4128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77ce4128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77ce4128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c31048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c31048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c31048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c31048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c16f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c16f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c16f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c16f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c31fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c31fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c31fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c31fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cffda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cffda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cffda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77cffda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c28f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c28f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c28f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c28f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c31080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c31080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c31080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c31080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c33c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c33c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c33c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c33c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c16cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c16cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c16cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c16cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b8f978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b8f978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b8f978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b8f978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c828d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c828d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c828d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c828d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b8fe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b8fe48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b8fe48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b8fe48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c332e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c332e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c332e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c332e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c288d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c288d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c288d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c288d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c288d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c288d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c288d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c288d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b8f978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b8f978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b8f978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b8f978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c288d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c288d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c288d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c288d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c30be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c30be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c30be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c30be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b8f320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b8f320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b8f320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b8f320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c30a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c30a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c30a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77c30a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b8fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b8fe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b8fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b8fe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b86e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b86e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b86e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b86e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b9b2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b9b2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b9b2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b9b2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b88748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b88748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b88748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b88748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b9b4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b9b4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b9b4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b9b4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b88ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b88ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b88ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b88ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b9bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b9bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b9bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b9bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77bcf5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77bcf5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77bcf5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77bcf5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b886a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b886a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b886a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b886a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b974e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b974e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b974e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b974e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77bcf908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77bcf908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77bcf908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77bcf908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b97860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b97860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b97860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b97860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77bcfc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77bcfc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77bcfc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77bcfc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b0c3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b0c3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b0c3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b0c3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c82860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c82860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c82860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77c82860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ba3908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ba3908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ba3908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ba3908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77ba36a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77ba36a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77ba36a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77ba36a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b7f898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b7f898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b7f898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b7f898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77ba37b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77ba37b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77ba37b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77ba37b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b2a5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b2a5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b2a5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b2a5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b47208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b47208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b47208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b47208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b2a5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b2a5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b2a5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b2a5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b97f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b97f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b97f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b97f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b47c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b47c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b47c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b47c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b7fcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b7fcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b7fcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b7fcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b0bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b0bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b0bcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b0bcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b2a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b2a710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b2a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b2a710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b36f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b36f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b36f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b36f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b0ba20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b0ba20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b0ba20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b0ba20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a85470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a85470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a85470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a85470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b0bbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b0bbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b0bbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b0bbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b362b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b362b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b362b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b362b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b36160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b36160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b36160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b36160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ab05c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ab05c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ab05c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ab05c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b36cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b36cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b36cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b36cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a85208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a85208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a85208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a85208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b0c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b0c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b0c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b0c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a0cba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a0cba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a0cba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a0cba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a85940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a85940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a85940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a85940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a455f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a455f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a455f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a455f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b7f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b7f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b7f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b7f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ae6f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ae6f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ae6f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ae6f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a453c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a453c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a453c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a453c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b3d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b3d828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b3d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77b3d828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a450f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a450f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a450f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a450f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77aad080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77aad080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77aad080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77aad080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b3dac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b3dac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b3dac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77b3dac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3b320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3b320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3b320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3b320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77aadd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77aadd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77aadd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77aadd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3b588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3b588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77aadf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77aadf98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77aadf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77aadf98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3bc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3bc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77abc4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77abc4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77abc4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77abc4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ab0be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ab0be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ab0be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77ab0be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a2d550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a2d550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a2d550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7eff77a2d550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3d978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3d978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3d978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3d978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77ab0be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77ab0be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77ab0be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77ab0be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a7b0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a7b0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a7b0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a7b0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9df60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9df60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9df60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77c9df60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d79a128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d79a128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d79a128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff7d79a128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7dfe4d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77986e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77986e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77986e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77986e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c4f5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c4f5048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c4f5048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7c4f5048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77a3bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77a46a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77a46a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77a46a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77a46a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff779a4320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff779a4320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff779a4320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff779a4320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779408d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779408d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779408d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779408d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77940710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77940710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77940710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77940710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77944400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77944400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77944400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77944400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77944e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77944e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77944e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77944e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77944e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77944e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77944e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77944e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff779446d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff779446d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff779446d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff779446d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77a7b048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77a7b048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77a7b048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77a7b048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77941f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77941f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77941f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77941f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779449b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779449b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779449b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779449b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77977b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77977b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77977b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77977b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779774a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779774a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779774a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779774a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77941be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77941be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77941be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff77941be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779cae80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779cae80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779cae80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff779cae80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff779343c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff779343c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff779343c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff779343c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7793ab70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7793ab70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7793ab70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7793ab70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff778ec358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff778ec358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff778ec358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7eff778ec358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77941be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77941be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77941be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77941be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77941ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77941ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77941ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff77941ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7793a7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7793a7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7793a7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7eff7793a7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING - tensorflow - From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING:tensorflow:From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING - tensorflow - From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING - tensorflow - From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING - tensorflow - From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

2019-11-06 18:42:56.665568: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-06 18:42:56.670405: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-11-06 18:42:56.787481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 18:42:56.787956: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bbcedfd5d0 executing computations on platform CUDA. Devices:
2019-11-06 18:42:56.787972: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-11-06 18:42:56.808755: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz
2019-11-06 18:42:56.809229: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bbced0aa50 executing computations on platform Host. Devices:
2019-11-06 18:42:56.809244: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-11-06 18:42:56.809371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 18:42:56.809756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-11-06 18:42:56.809897: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-06 18:42:56.810647: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-11-06 18:42:56.811296: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-11-06 18:42:56.811457: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-11-06 18:42:56.812266: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-11-06 18:42:56.812868: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-11-06 18:42:56.814818: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-11-06 18:42:56.814897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 18:42:56.815321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 18:42:56.815680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-11-06 18:42:56.815707: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-06 18:42:56.816447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-06 18:42:56.816461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-11-06 18:42:56.816467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-11-06 18:42:56.816671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 18:42:56.817173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-06 18:42:56.817573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6846 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
INFO - root - Restore from last checkpoint: Logs/bisenet/checkpoints/bisenet-v2/model.ckpt-30000
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from Logs/bisenet/checkpoints/bisenet-v2/model.ckpt-30000
INFO - tensorflow - Restoring parameters from Logs/bisenet/checkpoints/bisenet-v2/model.ckpt-30000
WARNING:tensorflow:From train.py:161: The name tf.train.global_step is deprecated. Please use tf.compat.v1.train.global_step instead.

WARNING - tensorflow - From train.py:161: The name tf.train.global_step is deprecated. Please use tf.compat.v1.train.global_step instead.

INFO - root - Train for 150000 steps
2019-11-06 18:43:02.985011: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
INFO - root - 2019-11-06 18:43:05.034968: step 30010, total loss = 4.04, predict loss = 1.16 (67.6 examples/sec; 0.059 sec/batch; 1h:58m:17s remains)
2019-11-06 18:43:06.304276: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
INFO - root - 2019-11-06 18:43:07.300972: step 30020, total loss = 3.95, predict loss = 1.16 (71.9 examples/sec; 0.056 sec/batch; 1h:51m:16s remains)
INFO - root - 2019-11-06 18:43:07.942849: step 30030, total loss = 4.23, predict loss = 1.26 (64.8 examples/sec; 0.062 sec/batch; 2h:03m:28s remains)
INFO - root - 2019-11-06 18:43:08.658535: step 30040, total loss = 5.46, predict loss = 1.58 (62.9 examples/sec; 0.064 sec/batch; 2h:07m:10s remains)
INFO - root - 2019-11-06 18:43:09.379891: step 30050, total loss = 4.71, predict loss = 1.38 (70.8 examples/sec; 0.057 sec/batch; 1h:52m:58s remains)
INFO - root - 2019-11-06 18:43:09.909546: step 30060, total loss = 6.08, predict loss = 1.82 (85.5 examples/sec; 0.047 sec/batch; 1h:33m:28s remains)
INFO - root - 2019-11-06 18:43:10.354882: step 30070, total loss = 5.45, predict loss = 1.59 (92.7 examples/sec; 0.043 sec/batch; 1h:26m:16s remains)
INFO - root - 2019-11-06 18:43:11.776935: step 30080, total loss = 4.72, predict loss = 1.36 (66.3 examples/sec; 0.060 sec/batch; 2h:00m:35s remains)
INFO - root - 2019-11-06 18:43:12.516919: step 30090, total loss = 4.40, predict loss = 1.24 (61.1 examples/sec; 0.065 sec/batch; 2h:10m:44s remains)
INFO - root - 2019-11-06 18:43:13.253951: step 30100, total loss = 6.55, predict loss = 1.93 (57.9 examples/sec; 0.069 sec/batch; 2h:18m:06s remains)
INFO - root - 2019-11-06 18:43:13.989196: step 30110, total loss = 6.34, predict loss = 1.92 (53.6 examples/sec; 0.075 sec/batch; 2h:29m:00s remains)
INFO - root - 2019-11-06 18:43:14.784527: step 30120, total loss = 3.30, predict loss = 0.96 (60.2 examples/sec; 0.066 sec/batch; 2h:12m:39s remains)
INFO - root - 2019-11-06 18:43:15.394053: step 30130, total loss = 5.03, predict loss = 1.35 (106.0 examples/sec; 0.038 sec/batch; 1h:15m:21s remains)
INFO - root - 2019-11-06 18:43:15.864932: step 30140, total loss = 6.08, predict loss = 1.77 (100.5 examples/sec; 0.040 sec/batch; 1h:19m:28s remains)
INFO - root - 2019-11-06 18:43:16.323808: step 30150, total loss = 5.49, predict loss = 1.55 (87.7 examples/sec; 0.046 sec/batch; 1h:31m:09s remains)
INFO - root - 2019-11-06 18:43:17.646845: step 30160, total loss = 4.67, predict loss = 1.32 (61.1 examples/sec; 0.066 sec/batch; 2h:10m:50s remains)
INFO - root - 2019-11-06 18:43:18.426146: step 30170, total loss = 3.16, predict loss = 0.90 (54.6 examples/sec; 0.073 sec/batch; 2h:26m:19s remains)
INFO - root - 2019-11-06 18:43:19.251224: step 30180, total loss = 5.99, predict loss = 1.76 (49.6 examples/sec; 0.081 sec/batch; 2h:40m:53s remains)
INFO - root - 2019-11-06 18:43:20.010811: step 30190, total loss = 5.13, predict loss = 1.52 (51.9 examples/sec; 0.077 sec/batch; 2h:33m:49s remains)
INFO - root - 2019-11-06 18:43:20.743722: step 30200, total loss = 5.87, predict loss = 1.69 (66.6 examples/sec; 0.060 sec/batch; 1h:59m:59s remains)
INFO - root - 2019-11-06 18:43:21.238117: step 30210, total loss = 4.35, predict loss = 1.25 (96.3 examples/sec; 0.042 sec/batch; 1h:22m:54s remains)
INFO - root - 2019-11-06 18:43:21.718840: step 30220, total loss = 5.48, predict loss = 1.60 (90.9 examples/sec; 0.044 sec/batch; 1h:27m:48s remains)
INFO - root - 2019-11-06 18:43:22.919919: step 30230, total loss = 4.99, predict loss = 1.40 (69.8 examples/sec; 0.057 sec/batch; 1h:54m:20s remains)
INFO - root - 2019-11-06 18:43:23.628290: step 30240, total loss = 4.70, predict loss = 1.38 (54.5 examples/sec; 0.073 sec/batch; 2h:26m:27s remains)
INFO - root - 2019-11-06 18:43:24.355974: step 30250, total loss = 3.67, predict loss = 1.11 (64.3 examples/sec; 0.062 sec/batch; 2h:04m:11s remains)
INFO - root - 2019-11-06 18:43:25.133917: step 30260, total loss = 4.99, predict loss = 1.51 (54.7 examples/sec; 0.073 sec/batch; 2h:25m:58s remains)
INFO - root - 2019-11-06 18:43:25.908678: step 30270, total loss = 5.35, predict loss = 1.49 (60.4 examples/sec; 0.066 sec/batch; 2h:12m:13s remains)
INFO - root - 2019-11-06 18:43:26.486841: step 30280, total loss = 5.71, predict loss = 1.59 (96.2 examples/sec; 0.042 sec/batch; 1h:22m:59s remains)
INFO - root - 2019-11-06 18:43:26.950968: step 30290, total loss = 4.59, predict loss = 1.27 (104.9 examples/sec; 0.038 sec/batch; 1h:16m:04s remains)
INFO - root - 2019-11-06 18:43:27.420773: step 30300, total loss = 2.11, predict loss = 0.62 (118.7 examples/sec; 0.034 sec/batch; 1h:07m:13s remains)
INFO - root - 2019-11-06 18:43:28.827081: step 30310, total loss = 4.56, predict loss = 1.26 (50.6 examples/sec; 0.079 sec/batch; 2h:37m:46s remains)
INFO - root - 2019-11-06 18:43:29.563486: step 30320, total loss = 4.66, predict loss = 1.39 (51.9 examples/sec; 0.077 sec/batch; 2h:33m:39s remains)
INFO - root - 2019-11-06 18:43:30.302974: step 30330, total loss = 5.10, predict loss = 1.44 (62.9 examples/sec; 0.064 sec/batch; 2h:06m:48s remains)
INFO - root - 2019-11-06 18:43:31.065896: step 30340, total loss = 3.05, predict loss = 0.89 (59.8 examples/sec; 0.067 sec/batch; 2h:13m:26s remains)
INFO - root - 2019-11-06 18:43:31.777092: step 30350, total loss = 5.11, predict loss = 1.41 (79.1 examples/sec; 0.051 sec/batch; 1h:40m:54s remains)
INFO - root - 2019-11-06 18:43:32.226238: step 30360, total loss = 4.10, predict loss = 1.13 (100.7 examples/sec; 0.040 sec/batch; 1h:19m:12s remains)
INFO - root - 2019-11-06 18:43:32.687758: step 30370, total loss = 6.18, predict loss = 1.71 (98.7 examples/sec; 0.041 sec/batch; 1h:20m:49s remains)
INFO - root - 2019-11-06 18:43:33.950063: step 30380, total loss = 4.58, predict loss = 1.39 (67.0 examples/sec; 0.060 sec/batch; 1h:59m:06s remains)
INFO - root - 2019-11-06 18:43:34.653300: step 30390, total loss = 4.98, predict loss = 1.48 (62.7 examples/sec; 0.064 sec/batch; 2h:07m:05s remains)
INFO - root - 2019-11-06 18:43:35.415341: step 30400, total loss = 6.09, predict loss = 1.77 (58.7 examples/sec; 0.068 sec/batch; 2h:15m:51s remains)
INFO - root - 2019-11-06 18:43:36.152728: step 30410, total loss = 4.31, predict loss = 1.28 (63.4 examples/sec; 0.063 sec/batch; 2h:05m:39s remains)
INFO - root - 2019-11-06 18:43:36.924479: step 30420, total loss = 4.61, predict loss = 1.30 (67.8 examples/sec; 0.059 sec/batch; 1h:57m:37s remains)
INFO - root - 2019-11-06 18:43:37.454943: step 30430, total loss = 5.26, predict loss = 1.42 (89.6 examples/sec; 0.045 sec/batch; 1h:28m:58s remains)
INFO - root - 2019-11-06 18:43:37.917320: step 30440, total loss = 6.32, predict loss = 1.71 (96.7 examples/sec; 0.041 sec/batch; 1h:22m:24s remains)
INFO - root - 2019-11-06 18:43:39.059846: step 30450, total loss = 6.05, predict loss = 1.78 (5.5 examples/sec; 0.727 sec/batch; 24h:08m:02s remains)
INFO - root - 2019-11-06 18:43:39.799554: step 30460, total loss = 4.59, predict loss = 1.35 (56.0 examples/sec; 0.071 sec/batch; 2h:22m:24s remains)
INFO - root - 2019-11-06 18:43:40.624789: step 30470, total loss = 5.63, predict loss = 1.53 (54.7 examples/sec; 0.073 sec/batch; 2h:25m:38s remains)
INFO - root - 2019-11-06 18:43:41.436992: step 30480, total loss = 3.43, predict loss = 0.97 (53.9 examples/sec; 0.074 sec/batch; 2h:27m:46s remains)
INFO - root - 2019-11-06 18:43:42.171216: step 30490, total loss = 3.45, predict loss = 0.94 (58.7 examples/sec; 0.068 sec/batch; 2h:15m:48s remains)
INFO - root - 2019-11-06 18:43:42.848650: step 30500, total loss = 3.28, predict loss = 1.03 (93.6 examples/sec; 0.043 sec/batch; 1h:25m:05s remains)
INFO - root - 2019-11-06 18:43:43.292361: step 30510, total loss = 4.44, predict loss = 1.18 (102.4 examples/sec; 0.039 sec/batch; 1h:17m:45s remains)
INFO - root - 2019-11-06 18:43:43.747876: step 30520, total loss = 4.41, predict loss = 1.29 (93.3 examples/sec; 0.043 sec/batch; 1h:25m:23s remains)
INFO - root - 2019-11-06 18:43:44.977063: step 30530, total loss = 4.69, predict loss = 1.31 (71.3 examples/sec; 0.056 sec/batch; 1h:51m:37s remains)
INFO - root - 2019-11-06 18:43:45.709269: step 30540, total loss = 5.66, predict loss = 1.65 (60.1 examples/sec; 0.067 sec/batch; 2h:12m:33s remains)
INFO - root - 2019-11-06 18:43:46.445028: step 30550, total loss = 5.92, predict loss = 1.73 (61.0 examples/sec; 0.066 sec/batch; 2h:10m:32s remains)
INFO - root - 2019-11-06 18:43:47.240084: step 30560, total loss = 5.71, predict loss = 1.64 (57.1 examples/sec; 0.070 sec/batch; 2h:19m:31s remains)
INFO - root - 2019-11-06 18:43:47.959974: step 30570, total loss = 3.37, predict loss = 0.99 (73.8 examples/sec; 0.054 sec/batch; 1h:47m:48s remains)
INFO - root - 2019-11-06 18:43:48.509794: step 30580, total loss = 5.66, predict loss = 1.61 (94.6 examples/sec; 0.042 sec/batch; 1h:24m:09s remains)
INFO - root - 2019-11-06 18:43:48.972582: step 30590, total loss = 5.56, predict loss = 1.59 (89.9 examples/sec; 0.044 sec/batch; 1h:28m:32s remains)
INFO - root - 2019-11-06 18:43:50.154982: step 30600, total loss = 4.64, predict loss = 1.31 (72.2 examples/sec; 0.055 sec/batch; 1h:50m:14s remains)
INFO - root - 2019-11-06 18:43:50.855908: step 30610, total loss = 5.32, predict loss = 1.56 (56.3 examples/sec; 0.071 sec/batch; 2h:21m:17s remains)
INFO - root - 2019-11-06 18:43:51.603709: step 30620, total loss = 6.23, predict loss = 1.74 (60.9 examples/sec; 0.066 sec/batch; 2h:10m:41s remains)
INFO - root - 2019-11-06 18:43:52.333376: step 30630, total loss = 2.87, predict loss = 0.85 (63.6 examples/sec; 0.063 sec/batch; 2h:05m:07s remains)
INFO - root - 2019-11-06 18:43:53.092138: step 30640, total loss = 6.46, predict loss = 1.87 (68.4 examples/sec; 0.059 sec/batch; 1h:56m:25s remains)
INFO - root - 2019-11-06 18:43:53.746542: step 30650, total loss = 3.20, predict loss = 1.03 (84.4 examples/sec; 0.047 sec/batch; 1h:34m:14s remains)
INFO - root - 2019-11-06 18:43:54.242066: step 30660, total loss = 4.53, predict loss = 1.28 (91.0 examples/sec; 0.044 sec/batch; 1h:27m:28s remains)
INFO - root - 2019-11-06 18:43:54.702765: step 30670, total loss = 4.56, predict loss = 1.36 (97.3 examples/sec; 0.041 sec/batch; 1h:21m:47s remains)
INFO - root - 2019-11-06 18:43:55.999294: step 30680, total loss = 5.43, predict loss = 1.53 (62.6 examples/sec; 0.064 sec/batch; 2h:07m:00s remains)
INFO - root - 2019-11-06 18:43:56.709127: step 30690, total loss = 5.39, predict loss = 1.56 (60.0 examples/sec; 0.067 sec/batch; 2h:12m:33s remains)
INFO - root - 2019-11-06 18:43:57.468573: step 30700, total loss = 4.46, predict loss = 1.31 (56.8 examples/sec; 0.070 sec/batch; 2h:20m:08s remains)
INFO - root - 2019-11-06 18:43:58.168857: step 30710, total loss = 5.26, predict loss = 1.54 (61.3 examples/sec; 0.065 sec/batch; 2h:09m:43s remains)
INFO - root - 2019-11-06 18:43:58.900306: step 30720, total loss = 3.59, predict loss = 0.99 (70.6 examples/sec; 0.057 sec/batch; 1h:52m:39s remains)
INFO - root - 2019-11-06 18:43:59.403141: step 30730, total loss = 4.84, predict loss = 1.35 (101.1 examples/sec; 0.040 sec/batch; 1h:18m:37s remains)
INFO - root - 2019-11-06 18:43:59.872727: step 30740, total loss = 6.41, predict loss = 1.82 (96.5 examples/sec; 0.041 sec/batch; 1h:22m:25s remains)
INFO - root - 2019-11-06 18:44:01.035474: step 30750, total loss = 3.70, predict loss = 1.07 (67.1 examples/sec; 0.060 sec/batch; 1h:58m:31s remains)
INFO - root - 2019-11-06 18:44:01.799081: step 30760, total loss = 5.45, predict loss = 1.55 (56.7 examples/sec; 0.071 sec/batch; 2h:20m:11s remains)
INFO - root - 2019-11-06 18:44:02.566000: step 30770, total loss = 3.93, predict loss = 1.14 (62.1 examples/sec; 0.064 sec/batch; 2h:07m:58s remains)
INFO - root - 2019-11-06 18:44:03.325601: step 30780, total loss = 5.02, predict loss = 1.49 (62.0 examples/sec; 0.065 sec/batch; 2h:08m:16s remains)
INFO - root - 2019-11-06 18:44:04.049919: step 30790, total loss = 4.18, predict loss = 1.25 (59.4 examples/sec; 0.067 sec/batch; 2h:13m:53s remains)
INFO - root - 2019-11-06 18:44:04.660202: step 30800, total loss = 4.89, predict loss = 1.38 (99.8 examples/sec; 0.040 sec/batch; 1h:19m:37s remains)
INFO - root - 2019-11-06 18:44:05.120605: step 30810, total loss = 4.44, predict loss = 1.27 (101.2 examples/sec; 0.040 sec/batch; 1h:18m:31s remains)
INFO - root - 2019-11-06 18:44:05.597896: step 30820, total loss = 5.52, predict loss = 1.54 (95.3 examples/sec; 0.042 sec/batch; 1h:23m:22s remains)
INFO - root - 2019-11-06 18:44:06.895658: step 30830, total loss = 6.15, predict loss = 1.74 (64.8 examples/sec; 0.062 sec/batch; 2h:02m:34s remains)
INFO - root - 2019-11-06 18:44:07.646311: step 30840, total loss = 3.13, predict loss = 0.87 (56.7 examples/sec; 0.071 sec/batch; 2h:20m:09s remains)
INFO - root - 2019-11-06 18:44:08.451726: step 30850, total loss = 4.63, predict loss = 1.34 (61.3 examples/sec; 0.065 sec/batch; 2h:09m:32s remains)
INFO - root - 2019-11-06 18:44:09.216194: step 30860, total loss = 4.30, predict loss = 1.23 (54.0 examples/sec; 0.074 sec/batch; 2h:27m:06s remains)
INFO - root - 2019-11-06 18:44:09.923294: step 30870, total loss = 4.08, predict loss = 1.17 (74.1 examples/sec; 0.054 sec/batch; 1h:47m:09s remains)
INFO - root - 2019-11-06 18:44:10.443804: step 30880, total loss = 4.94, predict loss = 1.36 (87.6 examples/sec; 0.046 sec/batch; 1h:30m:40s remains)
INFO - root - 2019-11-06 18:44:10.891051: step 30890, total loss = 4.49, predict loss = 1.25 (97.0 examples/sec; 0.041 sec/batch; 1h:21m:51s remains)
INFO - root - 2019-11-06 18:44:12.129853: step 30900, total loss = 2.74, predict loss = 0.84 (64.0 examples/sec; 0.062 sec/batch; 2h:04m:01s remains)
INFO - root - 2019-11-06 18:44:12.812868: step 30910, total loss = 5.04, predict loss = 1.48 (66.4 examples/sec; 0.060 sec/batch; 1h:59m:29s remains)
INFO - root - 2019-11-06 18:44:13.581997: step 30920, total loss = 5.20, predict loss = 1.46 (56.2 examples/sec; 0.071 sec/batch; 2h:21m:16s remains)
INFO - root - 2019-11-06 18:44:14.361684: step 30930, total loss = 5.82, predict loss = 1.67 (56.1 examples/sec; 0.071 sec/batch; 2h:21m:28s remains)
INFO - root - 2019-11-06 18:44:15.137346: step 30940, total loss = 5.03, predict loss = 1.48 (62.7 examples/sec; 0.064 sec/batch; 2h:06m:40s remains)
INFO - root - 2019-11-06 18:44:15.720980: step 30950, total loss = 3.99, predict loss = 1.13 (99.4 examples/sec; 0.040 sec/batch; 1h:19m:52s remains)
INFO - root - 2019-11-06 18:44:16.169435: step 30960, total loss = 5.44, predict loss = 1.48 (96.0 examples/sec; 0.042 sec/batch; 1h:22m:38s remains)
INFO - root - 2019-11-06 18:44:16.623569: step 30970, total loss = 4.47, predict loss = 1.30 (95.7 examples/sec; 0.042 sec/batch; 1h:22m:53s remains)
INFO - root - 2019-11-06 18:44:17.975654: step 30980, total loss = 5.70, predict loss = 1.66 (60.5 examples/sec; 0.066 sec/batch; 2h:11m:13s remains)
INFO - root - 2019-11-06 18:44:18.726130: step 30990, total loss = 4.05, predict loss = 1.16 (58.2 examples/sec; 0.069 sec/batch; 2h:16m:24s remains)
INFO - root - 2019-11-06 18:44:19.539375: step 31000, total loss = 5.06, predict loss = 1.43 (65.9 examples/sec; 0.061 sec/batch; 2h:00m:21s remains)
INFO - root - 2019-11-06 18:44:20.234490: step 31010, total loss = 5.20, predict loss = 1.50 (60.6 examples/sec; 0.066 sec/batch; 2h:10m:50s remains)
INFO - root - 2019-11-06 18:44:20.901137: step 31020, total loss = 3.48, predict loss = 1.02 (81.9 examples/sec; 0.049 sec/batch; 1h:36m:50s remains)
INFO - root - 2019-11-06 18:44:21.361938: step 31030, total loss = 5.20, predict loss = 1.43 (93.3 examples/sec; 0.043 sec/batch; 1h:25m:03s remains)
INFO - root - 2019-11-06 18:44:21.812023: step 31040, total loss = 5.94, predict loss = 1.66 (94.4 examples/sec; 0.042 sec/batch; 1h:23m:59s remains)
INFO - root - 2019-11-06 18:44:23.056959: step 31050, total loss = 4.36, predict loss = 1.20 (69.4 examples/sec; 0.058 sec/batch; 1h:54m:19s remains)
INFO - root - 2019-11-06 18:44:23.810241: step 31060, total loss = 3.48, predict loss = 1.06 (62.6 examples/sec; 0.064 sec/batch; 2h:06m:39s remains)
INFO - root - 2019-11-06 18:44:24.538334: step 31070, total loss = 4.32, predict loss = 1.26 (62.3 examples/sec; 0.064 sec/batch; 2h:07m:19s remains)
INFO - root - 2019-11-06 18:44:25.330739: step 31080, total loss = 4.60, predict loss = 1.38 (57.6 examples/sec; 0.069 sec/batch; 2h:17m:43s remains)
INFO - root - 2019-11-06 18:44:26.046803: step 31090, total loss = 5.57, predict loss = 1.62 (61.9 examples/sec; 0.065 sec/batch; 2h:08m:04s remains)
INFO - root - 2019-11-06 18:44:26.655388: step 31100, total loss = 4.55, predict loss = 1.29 (93.8 examples/sec; 0.043 sec/batch; 1h:24m:30s remains)
INFO - root - 2019-11-06 18:44:27.115493: step 31110, total loss = 6.21, predict loss = 1.72 (93.2 examples/sec; 0.043 sec/batch; 1h:25m:03s remains)
INFO - root - 2019-11-06 18:44:27.564068: step 31120, total loss = 3.80, predict loss = 1.06 (118.0 examples/sec; 0.034 sec/batch; 1h:07m:10s remains)
INFO - root - 2019-11-06 18:44:28.918956: step 31130, total loss = 6.42, predict loss = 1.73 (62.1 examples/sec; 0.064 sec/batch; 2h:07m:39s remains)
INFO - root - 2019-11-06 18:44:29.648054: step 31140, total loss = 5.39, predict loss = 1.53 (64.6 examples/sec; 0.062 sec/batch; 2h:02m:44s remains)
INFO - root - 2019-11-06 18:44:30.412331: step 31150, total loss = 5.32, predict loss = 1.50 (52.7 examples/sec; 0.076 sec/batch; 2h:30m:13s remains)
INFO - root - 2019-11-06 18:44:31.188286: step 31160, total loss = 6.26, predict loss = 1.84 (58.7 examples/sec; 0.068 sec/batch; 2h:14m:59s remains)
INFO - root - 2019-11-06 18:44:31.915059: step 31170, total loss = 5.86, predict loss = 1.70 (70.1 examples/sec; 0.057 sec/batch; 1h:53m:00s remains)
INFO - root - 2019-11-06 18:44:32.419191: step 31180, total loss = 5.72, predict loss = 1.64 (94.7 examples/sec; 0.042 sec/batch; 1h:23m:36s remains)
INFO - root - 2019-11-06 18:44:32.876766: step 31190, total loss = 5.27, predict loss = 1.53 (95.8 examples/sec; 0.042 sec/batch; 1h:22m:42s remains)
INFO - root - 2019-11-06 18:44:34.122632: step 31200, total loss = 4.82, predict loss = 1.41 (67.5 examples/sec; 0.059 sec/batch; 1h:57m:19s remains)
INFO - root - 2019-11-06 18:44:34.835393: step 31210, total loss = 4.06, predict loss = 1.12 (58.1 examples/sec; 0.069 sec/batch; 2h:16m:13s remains)
INFO - root - 2019-11-06 18:44:35.639851: step 31220, total loss = 6.09, predict loss = 1.76 (56.1 examples/sec; 0.071 sec/batch; 2h:21m:02s remains)
INFO - root - 2019-11-06 18:44:36.451534: step 31230, total loss = 3.78, predict loss = 1.08 (50.9 examples/sec; 0.079 sec/batch; 2h:35m:35s remains)
INFO - root - 2019-11-06 18:44:37.249180: step 31240, total loss = 5.16, predict loss = 1.42 (71.5 examples/sec; 0.056 sec/batch; 1h:50m:47s remains)
INFO - root - 2019-11-06 18:44:37.780231: step 31250, total loss = 4.25, predict loss = 1.26 (82.8 examples/sec; 0.048 sec/batch; 1h:35m:34s remains)
INFO - root - 2019-11-06 18:44:38.272385: step 31260, total loss = 2.93, predict loss = 0.81 (98.3 examples/sec; 0.041 sec/batch; 1h:20m:32s remains)
INFO - root - 2019-11-06 18:44:39.393317: step 31270, total loss = 4.50, predict loss = 1.38 (5.6 examples/sec; 0.711 sec/batch; 23h:26m:43s remains)
INFO - root - 2019-11-06 18:44:40.119760: step 31280, total loss = 2.82, predict loss = 0.80 (49.6 examples/sec; 0.081 sec/batch; 2h:39m:25s remains)
INFO - root - 2019-11-06 18:44:40.901411: step 31290, total loss = 5.91, predict loss = 1.74 (59.0 examples/sec; 0.068 sec/batch; 2h:14m:03s remains)
INFO - root - 2019-11-06 18:44:41.670880: step 31300, total loss = 5.20, predict loss = 1.50 (58.7 examples/sec; 0.068 sec/batch; 2h:14m:55s remains)
INFO - root - 2019-11-06 18:44:42.434650: step 31310, total loss = 3.61, predict loss = 1.02 (55.2 examples/sec; 0.072 sec/batch; 2h:23m:14s remains)
INFO - root - 2019-11-06 18:44:43.110025: step 31320, total loss = 5.47, predict loss = 1.54 (88.2 examples/sec; 0.045 sec/batch; 1h:29m:39s remains)
INFO - root - 2019-11-06 18:44:43.564893: step 31330, total loss = 4.58, predict loss = 1.33 (98.0 examples/sec; 0.041 sec/batch; 1h:20m:42s remains)
INFO - root - 2019-11-06 18:44:44.044955: step 31340, total loss = 5.12, predict loss = 1.44 (93.3 examples/sec; 0.043 sec/batch; 1h:24m:48s remains)
INFO - root - 2019-11-06 18:44:45.302470: step 31350, total loss = 4.53, predict loss = 1.43 (57.6 examples/sec; 0.069 sec/batch; 2h:17m:26s remains)
INFO - root - 2019-11-06 18:44:46.054852: step 31360, total loss = 5.30, predict loss = 1.56 (57.0 examples/sec; 0.070 sec/batch; 2h:18m:38s remains)
INFO - root - 2019-11-06 18:44:46.770044: step 31370, total loss = 4.40, predict loss = 1.32 (65.7 examples/sec; 0.061 sec/batch; 2h:00m:18s remains)
INFO - root - 2019-11-06 18:44:47.475372: step 31380, total loss = 5.34, predict loss = 1.53 (63.8 examples/sec; 0.063 sec/batch; 2h:03m:55s remains)
INFO - root - 2019-11-06 18:44:48.215831: step 31390, total loss = 4.52, predict loss = 1.23 (59.7 examples/sec; 0.067 sec/batch; 2h:12m:31s remains)
INFO - root - 2019-11-06 18:44:48.750724: step 31400, total loss = 4.22, predict loss = 1.20 (84.5 examples/sec; 0.047 sec/batch; 1h:33m:35s remains)
INFO - root - 2019-11-06 18:44:49.249087: step 31410, total loss = 4.25, predict loss = 1.15 (86.8 examples/sec; 0.046 sec/batch; 1h:31m:02s remains)
INFO - root - 2019-11-06 18:44:50.418605: step 31420, total loss = 5.33, predict loss = 1.54 (65.6 examples/sec; 0.061 sec/batch; 2h:00m:32s remains)
INFO - root - 2019-11-06 18:44:51.131557: step 31430, total loss = 4.79, predict loss = 1.47 (58.1 examples/sec; 0.069 sec/batch; 2h:16m:08s remains)
INFO - root - 2019-11-06 18:44:51.919378: step 31440, total loss = 5.36, predict loss = 1.68 (54.9 examples/sec; 0.073 sec/batch; 2h:24m:03s remains)
INFO - root - 2019-11-06 18:44:52.666211: step 31450, total loss = 4.98, predict loss = 1.36 (57.2 examples/sec; 0.070 sec/batch; 2h:18m:10s remains)
INFO - root - 2019-11-06 18:44:53.395850: step 31460, total loss = 4.37, predict loss = 1.21 (62.4 examples/sec; 0.064 sec/batch; 2h:06m:39s remains)
INFO - root - 2019-11-06 18:44:54.001236: step 31470, total loss = 4.28, predict loss = 1.20 (96.0 examples/sec; 0.042 sec/batch; 1h:22m:19s remains)
INFO - root - 2019-11-06 18:44:54.463900: step 31480, total loss = 3.04, predict loss = 0.83 (87.3 examples/sec; 0.046 sec/batch; 1h:30m:30s remains)
INFO - root - 2019-11-06 18:44:54.917921: step 31490, total loss = 4.55, predict loss = 1.25 (94.9 examples/sec; 0.042 sec/batch; 1h:23m:13s remains)
INFO - root - 2019-11-06 18:44:56.176770: step 31500, total loss = 4.14, predict loss = 1.20 (60.1 examples/sec; 0.067 sec/batch; 2h:11m:22s remains)
INFO - root - 2019-11-06 18:44:56.905385: step 31510, total loss = 5.39, predict loss = 1.54 (56.1 examples/sec; 0.071 sec/batch; 2h:20m:55s remains)
INFO - root - 2019-11-06 18:44:57.672223: step 31520, total loss = 5.25, predict loss = 1.47 (62.8 examples/sec; 0.064 sec/batch; 2h:05m:46s remains)
INFO - root - 2019-11-06 18:44:58.406790: step 31530, total loss = 3.03, predict loss = 0.84 (55.4 examples/sec; 0.072 sec/batch; 2h:22m:33s remains)
INFO - root - 2019-11-06 18:44:59.123465: step 31540, total loss = 5.37, predict loss = 1.51 (69.2 examples/sec; 0.058 sec/batch; 1h:54m:08s remains)
INFO - root - 2019-11-06 18:44:59.656804: step 31550, total loss = 4.10, predict loss = 1.11 (84.8 examples/sec; 0.047 sec/batch; 1h:33m:07s remains)
INFO - root - 2019-11-06 18:45:00.114023: step 31560, total loss = 5.25, predict loss = 1.42 (89.9 examples/sec; 0.044 sec/batch; 1h:27m:47s remains)
INFO - root - 2019-11-06 18:45:01.281788: step 31570, total loss = 3.91, predict loss = 1.11 (69.6 examples/sec; 0.058 sec/batch; 1h:53m:30s remains)
INFO - root - 2019-11-06 18:45:02.010324: step 31580, total loss = 4.79, predict loss = 1.36 (54.7 examples/sec; 0.073 sec/batch; 2h:24m:13s remains)
INFO - root - 2019-11-06 18:45:02.762609: step 31590, total loss = 4.40, predict loss = 1.21 (57.2 examples/sec; 0.070 sec/batch; 2h:17m:53s remains)
INFO - root - 2019-11-06 18:45:03.525606: step 31600, total loss = 5.51, predict loss = 1.60 (53.7 examples/sec; 0.074 sec/batch; 2h:26m:54s remains)
INFO - root - 2019-11-06 18:45:04.272118: step 31610, total loss = 3.52, predict loss = 1.03 (58.1 examples/sec; 0.069 sec/batch; 2h:15m:49s remains)
INFO - root - 2019-11-06 18:45:04.898727: step 31620, total loss = 6.01, predict loss = 1.69 (96.5 examples/sec; 0.041 sec/batch; 1h:21m:45s remains)
INFO - root - 2019-11-06 18:45:05.362385: step 31630, total loss = 5.35, predict loss = 1.53 (90.3 examples/sec; 0.044 sec/batch; 1h:27m:22s remains)
INFO - root - 2019-11-06 18:45:05.825440: step 31640, total loss = 4.36, predict loss = 1.28 (94.4 examples/sec; 0.042 sec/batch; 1h:23m:34s remains)
INFO - root - 2019-11-06 18:45:07.111174: step 31650, total loss = 4.20, predict loss = 1.23 (54.6 examples/sec; 0.073 sec/batch; 2h:24m:29s remains)
INFO - root - 2019-11-06 18:45:07.844642: step 31660, total loss = 5.99, predict loss = 1.78 (62.9 examples/sec; 0.064 sec/batch; 2h:05m:31s remains)
INFO - root - 2019-11-06 18:45:08.603261: step 31670, total loss = 5.51, predict loss = 1.55 (56.6 examples/sec; 0.071 sec/batch; 2h:19m:22s remains)
INFO - root - 2019-11-06 18:45:09.382757: step 31680, total loss = 4.80, predict loss = 1.35 (58.9 examples/sec; 0.068 sec/batch; 2h:13m:56s remains)
INFO - root - 2019-11-06 18:45:10.088710: step 31690, total loss = 4.13, predict loss = 1.16 (72.0 examples/sec; 0.056 sec/batch; 1h:49m:30s remains)
INFO - root - 2019-11-06 18:45:10.590019: step 31700, total loss = 6.40, predict loss = 1.86 (101.5 examples/sec; 0.039 sec/batch; 1h:17m:39s remains)
INFO - root - 2019-11-06 18:45:11.044787: step 31710, total loss = 4.12, predict loss = 1.14 (102.0 examples/sec; 0.039 sec/batch; 1h:17m:18s remains)
INFO - root - 2019-11-06 18:45:12.214040: step 31720, total loss = 5.02, predict loss = 1.39 (70.5 examples/sec; 0.057 sec/batch; 1h:51m:47s remains)
INFO - root - 2019-11-06 18:45:12.901137: step 31730, total loss = 4.99, predict loss = 1.43 (58.1 examples/sec; 0.069 sec/batch; 2h:15m:41s remains)
INFO - root - 2019-11-06 18:45:13.667976: step 31740, total loss = 3.66, predict loss = 1.11 (60.0 examples/sec; 0.067 sec/batch; 2h:11m:24s remains)
INFO - root - 2019-11-06 18:45:14.432798: step 31750, total loss = 5.45, predict loss = 1.48 (64.7 examples/sec; 0.062 sec/batch; 2h:01m:55s remains)
INFO - root - 2019-11-06 18:45:15.164924: step 31760, total loss = 3.56, predict loss = 1.02 (58.5 examples/sec; 0.068 sec/batch; 2h:14m:50s remains)
INFO - root - 2019-11-06 18:45:15.748220: step 31770, total loss = 4.66, predict loss = 1.29 (103.5 examples/sec; 0.039 sec/batch; 1h:16m:10s remains)
INFO - root - 2019-11-06 18:45:16.238413: step 31780, total loss = 6.04, predict loss = 1.69 (99.2 examples/sec; 0.040 sec/batch; 1h:19m:24s remains)
INFO - root - 2019-11-06 18:45:16.712958: step 31790, total loss = 4.10, predict loss = 1.14 (92.5 examples/sec; 0.043 sec/batch; 1h:25m:09s remains)
INFO - root - 2019-11-06 18:45:18.086284: step 31800, total loss = 3.96, predict loss = 1.17 (57.9 examples/sec; 0.069 sec/batch; 2h:16m:00s remains)
INFO - root - 2019-11-06 18:45:18.930066: step 31810, total loss = 3.12, predict loss = 0.94 (56.2 examples/sec; 0.071 sec/batch; 2h:20m:09s remains)
INFO - root - 2019-11-06 18:45:19.721858: step 31820, total loss = 4.36, predict loss = 1.19 (66.8 examples/sec; 0.060 sec/batch; 1h:57m:55s remains)
INFO - root - 2019-11-06 18:45:20.593280: step 31830, total loss = 5.78, predict loss = 1.56 (53.0 examples/sec; 0.075 sec/batch; 2h:28m:38s remains)
INFO - root - 2019-11-06 18:45:21.309436: step 31840, total loss = 5.16, predict loss = 1.45 (64.7 examples/sec; 0.062 sec/batch; 2h:01m:42s remains)
INFO - root - 2019-11-06 18:45:21.780896: step 31850, total loss = 4.57, predict loss = 1.22 (103.0 examples/sec; 0.039 sec/batch; 1h:16m:29s remains)
INFO - root - 2019-11-06 18:45:22.260109: step 31860, total loss = 5.88, predict loss = 1.68 (94.4 examples/sec; 0.042 sec/batch; 1h:23m:27s remains)
INFO - root - 2019-11-06 18:45:23.440464: step 31870, total loss = 4.69, predict loss = 1.36 (61.7 examples/sec; 0.065 sec/batch; 2h:07m:34s remains)
INFO - root - 2019-11-06 18:45:24.231520: step 31880, total loss = 4.19, predict loss = 1.18 (51.7 examples/sec; 0.077 sec/batch; 2h:32m:11s remains)
INFO - root - 2019-11-06 18:45:24.964143: step 31890, total loss = 4.08, predict loss = 1.17 (57.8 examples/sec; 0.069 sec/batch; 2h:16m:14s remains)
INFO - root - 2019-11-06 18:45:25.721223: step 31900, total loss = 4.85, predict loss = 1.41 (63.3 examples/sec; 0.063 sec/batch; 2h:04m:23s remains)
INFO - root - 2019-11-06 18:45:26.506799: step 31910, total loss = 4.24, predict loss = 1.21 (56.8 examples/sec; 0.070 sec/batch; 2h:18m:33s remains)
INFO - root - 2019-11-06 18:45:27.117769: step 31920, total loss = 6.24, predict loss = 1.82 (87.3 examples/sec; 0.046 sec/batch; 1h:30m:11s remains)
INFO - root - 2019-11-06 18:45:27.629710: step 31930, total loss = 3.11, predict loss = 0.88 (91.3 examples/sec; 0.044 sec/batch; 1h:26m:10s remains)
INFO - root - 2019-11-06 18:45:28.186742: step 31940, total loss = 5.83, predict loss = 1.58 (119.3 examples/sec; 0.034 sec/batch; 1h:05m:57s remains)
INFO - root - 2019-11-06 18:45:29.603169: step 31950, total loss = 5.34, predict loss = 1.54 (53.9 examples/sec; 0.074 sec/batch; 2h:25m:55s remains)
INFO - root - 2019-11-06 18:45:30.380613: step 31960, total loss = 3.46, predict loss = 0.97 (46.1 examples/sec; 0.087 sec/batch; 2h:50m:41s remains)
INFO - root - 2019-11-06 18:45:31.157550: step 31970, total loss = 4.12, predict loss = 1.24 (60.8 examples/sec; 0.066 sec/batch; 2h:09m:23s remains)
INFO - root - 2019-11-06 18:45:31.976344: step 31980, total loss = 5.88, predict loss = 1.63 (39.6 examples/sec; 0.101 sec/batch; 3h:18m:44s remains)
INFO - root - 2019-11-06 18:45:32.658273: step 31990, total loss = 4.10, predict loss = 1.11 (84.3 examples/sec; 0.047 sec/batch; 1h:33m:21s remains)
INFO - root - 2019-11-06 18:45:33.111839: step 32000, total loss = 4.19, predict loss = 1.21 (96.1 examples/sec; 0.042 sec/batch; 1h:21m:52s remains)
INFO - root - 2019-11-06 18:45:33.562186: step 32010, total loss = 4.07, predict loss = 1.11 (99.7 examples/sec; 0.040 sec/batch; 1h:18m:54s remains)
INFO - root - 2019-11-06 18:45:34.782916: step 32020, total loss = 3.17, predict loss = 0.88 (59.5 examples/sec; 0.067 sec/batch; 2h:12m:14s remains)
INFO - root - 2019-11-06 18:45:35.531598: step 32030, total loss = 6.01, predict loss = 1.73 (52.6 examples/sec; 0.076 sec/batch; 2h:29m:31s remains)
INFO - root - 2019-11-06 18:45:36.288804: step 32040, total loss = 5.53, predict loss = 1.50 (56.6 examples/sec; 0.071 sec/batch; 2h:19m:02s remains)
INFO - root - 2019-11-06 18:45:37.020777: step 32050, total loss = 4.84, predict loss = 1.33 (65.3 examples/sec; 0.061 sec/batch; 2h:00m:24s remains)
INFO - root - 2019-11-06 18:45:37.770157: step 32060, total loss = 3.55, predict loss = 1.03 (66.9 examples/sec; 0.060 sec/batch; 1h:57m:36s remains)
INFO - root - 2019-11-06 18:45:38.294967: step 32070, total loss = 4.67, predict loss = 1.28 (99.2 examples/sec; 0.040 sec/batch; 1h:19m:14s remains)
INFO - root - 2019-11-06 18:45:38.747508: step 32080, total loss = 4.17, predict loss = 1.13 (94.3 examples/sec; 0.042 sec/batch; 1h:23m:23s remains)
INFO - root - 2019-11-06 18:45:39.924683: step 32090, total loss = 5.25, predict loss = 1.49 (5.3 examples/sec; 0.754 sec/batch; 24h:41m:52s remains)
INFO - root - 2019-11-06 18:45:40.771133: step 32100, total loss = 2.95, predict loss = 0.84 (53.2 examples/sec; 0.075 sec/batch; 2h:27m:46s remains)
INFO - root - 2019-11-06 18:45:41.443004: step 32110, total loss = 5.03, predict loss = 1.43 (62.1 examples/sec; 0.064 sec/batch; 2h:06m:37s remains)
INFO - root - 2019-11-06 18:45:42.202313: step 32120, total loss = 4.59, predict loss = 1.30 (55.2 examples/sec; 0.072 sec/batch; 2h:22m:21s remains)
INFO - root - 2019-11-06 18:45:43.052613: step 32130, total loss = 5.31, predict loss = 1.47 (56.3 examples/sec; 0.071 sec/batch; 2h:19m:35s remains)
INFO - root - 2019-11-06 18:45:43.698034: step 32140, total loss = 5.50, predict loss = 1.52 (92.7 examples/sec; 0.043 sec/batch; 1h:24m:44s remains)
INFO - root - 2019-11-06 18:45:44.142618: step 32150, total loss = 4.57, predict loss = 1.29 (88.4 examples/sec; 0.045 sec/batch; 1h:28m:53s remains)
INFO - root - 2019-11-06 18:45:44.619070: step 32160, total loss = 4.24, predict loss = 1.19 (95.9 examples/sec; 0.042 sec/batch; 1h:21m:53s remains)
INFO - root - 2019-11-06 18:45:45.924518: step 32170, total loss = 5.31, predict loss = 1.49 (62.0 examples/sec; 0.064 sec/batch; 2h:06m:36s remains)
INFO - root - 2019-11-06 18:45:46.697934: step 32180, total loss = 3.62, predict loss = 1.15 (54.9 examples/sec; 0.073 sec/batch; 2h:22m:57s remains)
INFO - root - 2019-11-06 18:45:47.454440: step 32190, total loss = 5.52, predict loss = 1.55 (54.2 examples/sec; 0.074 sec/batch; 2h:24m:57s remains)
INFO - root - 2019-11-06 18:45:48.202238: step 32200, total loss = 5.77, predict loss = 1.56 (56.3 examples/sec; 0.071 sec/batch; 2h:19m:30s remains)
INFO - root - 2019-11-06 18:45:48.905453: step 32210, total loss = 4.34, predict loss = 1.23 (72.8 examples/sec; 0.055 sec/batch; 1h:47m:50s remains)
INFO - root - 2019-11-06 18:45:49.466310: step 32220, total loss = 4.66, predict loss = 1.34 (98.7 examples/sec; 0.041 sec/batch; 1h:19m:33s remains)
INFO - root - 2019-11-06 18:45:49.912660: step 32230, total loss = 3.60, predict loss = 1.05 (94.9 examples/sec; 0.042 sec/batch; 1h:22m:43s remains)
INFO - root - 2019-11-06 18:45:51.076620: step 32240, total loss = 2.30, predict loss = 0.69 (66.3 examples/sec; 0.060 sec/batch; 1h:58m:21s remains)
INFO - root - 2019-11-06 18:45:51.824189: step 32250, total loss = 3.58, predict loss = 0.98 (58.1 examples/sec; 0.069 sec/batch; 2h:15m:05s remains)
INFO - root - 2019-11-06 18:45:52.538131: step 32260, total loss = 3.27, predict loss = 0.89 (68.0 examples/sec; 0.059 sec/batch; 1h:55m:24s remains)
INFO - root - 2019-11-06 18:45:53.219913: step 32270, total loss = 4.23, predict loss = 1.19 (75.3 examples/sec; 0.053 sec/batch; 1h:44m:11s remains)
INFO - root - 2019-11-06 18:45:53.897889: step 32280, total loss = 4.97, predict loss = 1.38 (70.5 examples/sec; 0.057 sec/batch; 1h:51m:22s remains)
INFO - root - 2019-11-06 18:45:54.502894: step 32290, total loss = 5.59, predict loss = 1.53 (81.8 examples/sec; 0.049 sec/batch; 1h:35m:57s remains)
INFO - root - 2019-11-06 18:45:55.014513: step 32300, total loss = 5.47, predict loss = 1.58 (102.8 examples/sec; 0.039 sec/batch; 1h:16m:21s remains)
INFO - root - 2019-11-06 18:45:55.457336: step 32310, total loss = 4.66, predict loss = 1.28 (99.0 examples/sec; 0.040 sec/batch; 1h:19m:16s remains)
INFO - root - 2019-11-06 18:45:56.762179: step 32320, total loss = 4.47, predict loss = 1.21 (59.0 examples/sec; 0.068 sec/batch; 2h:12m:51s remains)
INFO - root - 2019-11-06 18:45:57.585536: step 32330, total loss = 5.17, predict loss = 1.45 (50.0 examples/sec; 0.080 sec/batch; 2h:36m:52s remains)
INFO - root - 2019-11-06 18:45:58.338279: step 32340, total loss = 4.41, predict loss = 1.27 (62.6 examples/sec; 0.064 sec/batch; 2h:05m:16s remains)
INFO - root - 2019-11-06 18:45:59.043299: step 32350, total loss = 5.55, predict loss = 1.57 (64.7 examples/sec; 0.062 sec/batch; 2h:01m:18s remains)
INFO - root - 2019-11-06 18:45:59.738730: step 32360, total loss = 5.19, predict loss = 1.54 (73.3 examples/sec; 0.055 sec/batch; 1h:47m:01s remains)
INFO - root - 2019-11-06 18:46:00.239005: step 32370, total loss = 3.68, predict loss = 1.12 (98.8 examples/sec; 0.040 sec/batch; 1h:19m:21s remains)
INFO - root - 2019-11-06 18:46:00.728305: step 32380, total loss = 4.73, predict loss = 1.32 (95.8 examples/sec; 0.042 sec/batch; 1h:21m:49s remains)
INFO - root - 2019-11-06 18:46:01.912732: step 32390, total loss = 3.82, predict loss = 1.03 (63.0 examples/sec; 0.063 sec/batch; 2h:04m:26s remains)
INFO - root - 2019-11-06 18:46:02.614613: step 32400, total loss = 4.87, predict loss = 1.39 (56.8 examples/sec; 0.070 sec/batch; 2h:17m:54s remains)
INFO - root - 2019-11-06 18:46:03.354643: step 32410, total loss = 4.80, predict loss = 1.33 (61.6 examples/sec; 0.065 sec/batch; 2h:07m:16s remains)
INFO - root - 2019-11-06 18:46:04.096396: step 32420, total loss = 4.73, predict loss = 1.32 (57.8 examples/sec; 0.069 sec/batch; 2h:15m:32s remains)
INFO - root - 2019-11-06 18:46:04.864989: step 32430, total loss = 4.17, predict loss = 1.24 (57.8 examples/sec; 0.069 sec/batch; 2h:15m:39s remains)
INFO - root - 2019-11-06 18:46:05.506691: step 32440, total loss = 5.83, predict loss = 1.66 (105.5 examples/sec; 0.038 sec/batch; 1h:14m:19s remains)
INFO - root - 2019-11-06 18:46:05.980218: step 32450, total loss = 3.76, predict loss = 1.04 (98.3 examples/sec; 0.041 sec/batch; 1h:19m:41s remains)
INFO - root - 2019-11-06 18:46:06.448216: step 32460, total loss = 5.31, predict loss = 1.44 (105.0 examples/sec; 0.038 sec/batch; 1h:14m:36s remains)
INFO - root - 2019-11-06 18:46:07.773429: step 32470, total loss = 3.71, predict loss = 1.11 (59.0 examples/sec; 0.068 sec/batch; 2h:12m:54s remains)
INFO - root - 2019-11-06 18:46:08.589523: step 32480, total loss = 3.45, predict loss = 0.98 (52.7 examples/sec; 0.076 sec/batch; 2h:28m:44s remains)
INFO - root - 2019-11-06 18:46:09.356230: step 32490, total loss = 4.66, predict loss = 1.32 (61.6 examples/sec; 0.065 sec/batch; 2h:07m:13s remains)
INFO - root - 2019-11-06 18:46:10.129325: step 32500, total loss = 4.93, predict loss = 1.42 (61.8 examples/sec; 0.065 sec/batch; 2h:06m:45s remains)
INFO - root - 2019-11-06 18:46:10.799761: step 32510, total loss = 5.82, predict loss = 1.72 (80.6 examples/sec; 0.050 sec/batch; 1h:37m:10s remains)
INFO - root - 2019-11-06 18:46:11.281505: step 32520, total loss = 2.52, predict loss = 0.70 (97.9 examples/sec; 0.041 sec/batch; 1h:19m:58s remains)
INFO - root - 2019-11-06 18:46:11.749075: step 32530, total loss = 4.66, predict loss = 1.31 (92.6 examples/sec; 0.043 sec/batch; 1h:24m:31s remains)
INFO - root - 2019-11-06 18:46:12.966670: step 32540, total loss = 3.83, predict loss = 1.12 (68.4 examples/sec; 0.059 sec/batch; 1h:54m:33s remains)
INFO - root - 2019-11-06 18:46:13.734265: step 32550, total loss = 3.94, predict loss = 1.07 (54.8 examples/sec; 0.073 sec/batch; 2h:22m:57s remains)
INFO - root - 2019-11-06 18:46:14.507083: step 32560, total loss = 3.09, predict loss = 0.83 (50.2 examples/sec; 0.080 sec/batch; 2h:35m:54s remains)
INFO - root - 2019-11-06 18:46:15.235251: step 32570, total loss = 5.39, predict loss = 1.54 (68.0 examples/sec; 0.059 sec/batch; 1h:55m:10s remains)
INFO - root - 2019-11-06 18:46:16.006012: step 32580, total loss = 4.47, predict loss = 1.23 (55.8 examples/sec; 0.072 sec/batch; 2h:20m:24s remains)
INFO - root - 2019-11-06 18:46:16.622322: step 32590, total loss = 5.27, predict loss = 1.44 (105.9 examples/sec; 0.038 sec/batch; 1h:13m:53s remains)
INFO - root - 2019-11-06 18:46:17.084095: step 32600, total loss = 3.84, predict loss = 1.09 (92.5 examples/sec; 0.043 sec/batch; 1h:24m:38s remains)
INFO - root - 2019-11-06 18:46:17.542535: step 32610, total loss = 4.16, predict loss = 1.18 (105.6 examples/sec; 0.038 sec/batch; 1h:14m:05s remains)
INFO - root - 2019-11-06 18:46:18.878143: step 32620, total loss = 3.73, predict loss = 1.03 (61.3 examples/sec; 0.065 sec/batch; 2h:07m:34s remains)
INFO - root - 2019-11-06 18:46:19.613192: step 32630, total loss = 4.77, predict loss = 1.31 (73.3 examples/sec; 0.055 sec/batch; 1h:46m:40s remains)
INFO - root - 2019-11-06 18:46:20.351120: step 32640, total loss = 3.54, predict loss = 1.00 (54.3 examples/sec; 0.074 sec/batch; 2h:24m:05s remains)
INFO - root - 2019-11-06 18:46:21.087650: step 32650, total loss = 3.38, predict loss = 0.95 (56.3 examples/sec; 0.071 sec/batch; 2h:18m:52s remains)
INFO - root - 2019-11-06 18:46:21.800705: step 32660, total loss = 4.13, predict loss = 1.17 (74.0 examples/sec; 0.054 sec/batch; 1h:45m:41s remains)
INFO - root - 2019-11-06 18:46:22.287822: step 32670, total loss = 5.26, predict loss = 1.46 (99.8 examples/sec; 0.040 sec/batch; 1h:18m:21s remains)
INFO - root - 2019-11-06 18:46:22.732894: step 32680, total loss = 4.94, predict loss = 1.40 (97.4 examples/sec; 0.041 sec/batch; 1h:20m:18s remains)
INFO - root - 2019-11-06 18:46:23.966528: step 32690, total loss = 2.82, predict loss = 0.80 (59.7 examples/sec; 0.067 sec/batch; 2h:10m:57s remains)
INFO - root - 2019-11-06 18:46:24.723214: step 32700, total loss = 5.27, predict loss = 1.50 (59.0 examples/sec; 0.068 sec/batch; 2h:12m:29s remains)
INFO - root - 2019-11-06 18:46:25.423170: step 32710, total loss = 5.10, predict loss = 1.34 (59.0 examples/sec; 0.068 sec/batch; 2h:12m:26s remains)
INFO - root - 2019-11-06 18:46:26.170083: step 32720, total loss = 4.21, predict loss = 1.15 (60.5 examples/sec; 0.066 sec/batch; 2h:09m:15s remains)
INFO - root - 2019-11-06 18:46:26.890324: step 32730, total loss = 4.24, predict loss = 1.20 (66.4 examples/sec; 0.060 sec/batch; 1h:57m:44s remains)
INFO - root - 2019-11-06 18:46:27.479099: step 32740, total loss = 4.16, predict loss = 1.13 (100.2 examples/sec; 0.040 sec/batch; 1h:18m:01s remains)
INFO - root - 2019-11-06 18:46:27.946580: step 32750, total loss = 5.31, predict loss = 1.54 (92.8 examples/sec; 0.043 sec/batch; 1h:24m:12s remains)
INFO - root - 2019-11-06 18:46:28.394270: step 32760, total loss = 2.12, predict loss = 0.62 (115.3 examples/sec; 0.035 sec/batch; 1h:07m:47s remains)
INFO - root - 2019-11-06 18:46:29.751208: step 32770, total loss = 4.71, predict loss = 1.30 (69.5 examples/sec; 0.058 sec/batch; 1h:52m:25s remains)
INFO - root - 2019-11-06 18:46:30.464778: step 32780, total loss = 6.17, predict loss = 1.77 (66.7 examples/sec; 0.060 sec/batch; 1h:57m:06s remains)
INFO - root - 2019-11-06 18:46:31.245539: step 32790, total loss = 4.37, predict loss = 1.23 (43.8 examples/sec; 0.091 sec/batch; 2h:58m:18s remains)
INFO - root - 2019-11-06 18:46:32.029890: step 32800, total loss = 5.45, predict loss = 1.54 (60.1 examples/sec; 0.067 sec/batch; 2h:09m:56s remains)
INFO - root - 2019-11-06 18:46:32.738386: step 32810, total loss = 3.45, predict loss = 1.00 (80.8 examples/sec; 0.050 sec/batch; 1h:36m:41s remains)
INFO - root - 2019-11-06 18:46:33.237218: step 32820, total loss = 2.71, predict loss = 0.74 (92.0 examples/sec; 0.043 sec/batch; 1h:24m:53s remains)
INFO - root - 2019-11-06 18:46:33.678245: step 32830, total loss = 4.94, predict loss = 1.38 (97.9 examples/sec; 0.041 sec/batch; 1h:19m:46s remains)
INFO - root - 2019-11-06 18:46:34.895339: step 32840, total loss = 3.89, predict loss = 1.08 (64.2 examples/sec; 0.062 sec/batch; 2h:01m:44s remains)
INFO - root - 2019-11-06 18:46:35.620950: step 32850, total loss = 4.10, predict loss = 1.12 (59.0 examples/sec; 0.068 sec/batch; 2h:12m:23s remains)
INFO - root - 2019-11-06 18:46:36.436151: step 32860, total loss = 3.01, predict loss = 0.86 (53.6 examples/sec; 0.075 sec/batch; 2h:25m:34s remains)
INFO - root - 2019-11-06 18:46:37.169382: step 32870, total loss = 2.90, predict loss = 0.79 (63.3 examples/sec; 0.063 sec/batch; 2h:03m:27s remains)
INFO - root - 2019-11-06 18:46:37.886068: step 32880, total loss = 3.49, predict loss = 0.92 (68.3 examples/sec; 0.059 sec/batch; 1h:54m:16s remains)
INFO - root - 2019-11-06 18:46:38.409646: step 32890, total loss = 4.92, predict loss = 1.36 (94.0 examples/sec; 0.043 sec/batch; 1h:23m:03s remains)
INFO - root - 2019-11-06 18:46:38.898800: step 32900, total loss = 3.10, predict loss = 0.83 (93.5 examples/sec; 0.043 sec/batch; 1h:23m:29s remains)
INFO - root - 2019-11-06 18:46:40.058721: step 32910, total loss = 3.91, predict loss = 1.06 (5.3 examples/sec; 0.750 sec/batch; 24h:22m:46s remains)
INFO - root - 2019-11-06 18:46:40.776265: step 32920, total loss = 4.41, predict loss = 1.22 (62.9 examples/sec; 0.064 sec/batch; 2h:04m:02s remains)
INFO - root - 2019-11-06 18:46:41.518536: step 32930, total loss = 4.96, predict loss = 1.35 (56.5 examples/sec; 0.071 sec/batch; 2h:18m:14s remains)
INFO - root - 2019-11-06 18:46:42.369269: step 32940, total loss = 3.99, predict loss = 1.12 (55.7 examples/sec; 0.072 sec/batch; 2h:20m:01s remains)
INFO - root - 2019-11-06 18:46:43.115924: step 32950, total loss = 4.20, predict loss = 1.18 (61.3 examples/sec; 0.065 sec/batch; 2h:07m:17s remains)
INFO - root - 2019-11-06 18:46:43.782784: step 32960, total loss = 2.87, predict loss = 0.80 (86.7 examples/sec; 0.046 sec/batch; 1h:29m:57s remains)
INFO - root - 2019-11-06 18:46:44.224203: step 32970, total loss = 3.36, predict loss = 0.93 (99.6 examples/sec; 0.040 sec/batch; 1h:18m:19s remains)
INFO - root - 2019-11-06 18:46:44.693360: step 32980, total loss = 5.14, predict loss = 1.45 (102.3 examples/sec; 0.039 sec/batch; 1h:16m:16s remains)
INFO - root - 2019-11-06 18:46:45.949006: step 32990, total loss = 3.70, predict loss = 0.96 (60.8 examples/sec; 0.066 sec/batch; 2h:08m:17s remains)
INFO - root - 2019-11-06 18:46:46.698964: step 33000, total loss = 4.35, predict loss = 1.26 (54.5 examples/sec; 0.073 sec/batch; 2h:23m:14s remains)
INFO - root - 2019-11-06 18:46:47.462916: step 33010, total loss = 3.72, predict loss = 1.04 (54.1 examples/sec; 0.074 sec/batch; 2h:24m:13s remains)
INFO - root - 2019-11-06 18:46:48.196124: step 33020, total loss = 5.23, predict loss = 1.48 (66.0 examples/sec; 0.061 sec/batch; 1h:58m:06s remains)
INFO - root - 2019-11-06 18:46:48.866962: step 33030, total loss = 3.82, predict loss = 1.05 (69.2 examples/sec; 0.058 sec/batch; 1h:52m:42s remains)
INFO - root - 2019-11-06 18:46:49.364267: step 33040, total loss = 5.36, predict loss = 1.50 (100.4 examples/sec; 0.040 sec/batch; 1h:17m:37s remains)
INFO - root - 2019-11-06 18:46:49.818688: step 33050, total loss = 3.49, predict loss = 0.93 (90.9 examples/sec; 0.044 sec/batch; 1h:25m:46s remains)
INFO - root - 2019-11-06 18:46:51.004032: step 33060, total loss = 4.65, predict loss = 1.29 (70.9 examples/sec; 0.056 sec/batch; 1h:49m:53s remains)
INFO - root - 2019-11-06 18:46:51.721652: step 33070, total loss = 3.63, predict loss = 0.93 (53.1 examples/sec; 0.075 sec/batch; 2h:26m:54s remains)
INFO - root - 2019-11-06 18:46:52.593755: step 33080, total loss = 4.32, predict loss = 1.19 (49.8 examples/sec; 0.080 sec/batch; 2h:36m:32s remains)
INFO - root - 2019-11-06 18:46:53.287350: step 33090, total loss = 3.66, predict loss = 0.97 (56.1 examples/sec; 0.071 sec/batch; 2h:18m:54s remains)
INFO - root - 2019-11-06 18:46:54.070453: step 33100, total loss = 4.70, predict loss = 1.32 (56.3 examples/sec; 0.071 sec/batch; 2h:18m:22s remains)
INFO - root - 2019-11-06 18:46:54.713506: step 33110, total loss = 3.42, predict loss = 0.88 (96.2 examples/sec; 0.042 sec/batch; 1h:21m:00s remains)
INFO - root - 2019-11-06 18:46:55.165140: step 33120, total loss = 3.16, predict loss = 0.89 (93.0 examples/sec; 0.043 sec/batch; 1h:23m:47s remains)
INFO - root - 2019-11-06 18:46:55.625505: step 33130, total loss = 5.20, predict loss = 1.50 (92.2 examples/sec; 0.043 sec/batch; 1h:24m:29s remains)
INFO - root - 2019-11-06 18:46:56.941441: step 33140, total loss = 4.76, predict loss = 1.40 (55.2 examples/sec; 0.073 sec/batch; 2h:21m:12s remains)
INFO - root - 2019-11-06 18:46:57.700316: step 33150, total loss = 3.54, predict loss = 0.97 (56.1 examples/sec; 0.071 sec/batch; 2h:18m:44s remains)
INFO - root - 2019-11-06 18:46:58.476328: step 33160, total loss = 5.01, predict loss = 1.37 (56.8 examples/sec; 0.070 sec/batch; 2h:17m:06s remains)
INFO - root - 2019-11-06 18:46:59.250327: step 33170, total loss = 5.00, predict loss = 1.42 (54.6 examples/sec; 0.073 sec/batch; 2h:22m:35s remains)
INFO - root - 2019-11-06 18:46:59.987814: step 33180, total loss = 4.26, predict loss = 1.18 (66.7 examples/sec; 0.060 sec/batch; 1h:56m:49s remains)
INFO - root - 2019-11-06 18:47:00.540721: step 33190, total loss = 4.01, predict loss = 1.09 (80.9 examples/sec; 0.049 sec/batch; 1h:36m:15s remains)
INFO - root - 2019-11-06 18:47:00.994611: step 33200, total loss = 4.69, predict loss = 1.36 (99.9 examples/sec; 0.040 sec/batch; 1h:17m:55s remains)
INFO - root - 2019-11-06 18:47:02.162979: step 33210, total loss = 4.60, predict loss = 1.32 (65.5 examples/sec; 0.061 sec/batch; 1h:58m:51s remains)
INFO - root - 2019-11-06 18:47:02.904805: step 33220, total loss = 5.16, predict loss = 1.46 (62.7 examples/sec; 0.064 sec/batch; 2h:04m:11s remains)
INFO - root - 2019-11-06 18:47:03.691936: step 33230, total loss = 3.17, predict loss = 0.88 (52.2 examples/sec; 0.077 sec/batch; 2h:29m:08s remains)
INFO - root - 2019-11-06 18:47:04.450603: step 33240, total loss = 5.17, predict loss = 1.41 (57.0 examples/sec; 0.070 sec/batch; 2h:16m:27s remains)
INFO - root - 2019-11-06 18:47:05.213137: step 33250, total loss = 3.82, predict loss = 1.02 (60.3 examples/sec; 0.066 sec/batch; 2h:08m:58s remains)
INFO - root - 2019-11-06 18:47:05.824206: step 33260, total loss = 4.62, predict loss = 1.31 (104.7 examples/sec; 0.038 sec/batch; 1h:14m:20s remains)
INFO - root - 2019-11-06 18:47:06.269332: step 33270, total loss = 6.18, predict loss = 1.72 (94.7 examples/sec; 0.042 sec/batch; 1h:22m:11s remains)
INFO - root - 2019-11-06 18:47:06.729406: step 33280, total loss = 5.30, predict loss = 1.51 (91.5 examples/sec; 0.044 sec/batch; 1h:25m:02s remains)
INFO - root - 2019-11-06 18:47:08.069184: step 33290, total loss = 3.66, predict loss = 1.03 (54.6 examples/sec; 0.073 sec/batch; 2h:22m:30s remains)
INFO - root - 2019-11-06 18:47:08.810596: step 33300, total loss = 5.29, predict loss = 1.49 (63.6 examples/sec; 0.063 sec/batch; 2h:02m:24s remains)
INFO - root - 2019-11-06 18:47:09.511413: step 33310, total loss = 5.01, predict loss = 1.40 (64.0 examples/sec; 0.062 sec/batch; 2h:01m:31s remains)
INFO - root - 2019-11-06 18:47:10.171665: step 33320, total loss = 2.96, predict loss = 0.82 (69.8 examples/sec; 0.057 sec/batch; 1h:51m:28s remains)
INFO - root - 2019-11-06 18:47:10.835023: step 33330, total loss = 4.20, predict loss = 1.11 (73.8 examples/sec; 0.054 sec/batch; 1h:45m:24s remains)
INFO - root - 2019-11-06 18:47:11.343470: step 33340, total loss = 5.85, predict loss = 1.66 (93.0 examples/sec; 0.043 sec/batch; 1h:23m:38s remains)
INFO - root - 2019-11-06 18:47:11.819771: step 33350, total loss = 4.95, predict loss = 1.35 (93.7 examples/sec; 0.043 sec/batch; 1h:23m:00s remains)
INFO - root - 2019-11-06 18:47:13.013594: step 33360, total loss = 5.22, predict loss = 1.50 (66.3 examples/sec; 0.060 sec/batch; 1h:57m:14s remains)
INFO - root - 2019-11-06 18:47:13.692550: step 33370, total loss = 5.05, predict loss = 1.48 (64.6 examples/sec; 0.062 sec/batch; 2h:00m:18s remains)
INFO - root - 2019-11-06 18:47:14.415517: step 33380, total loss = 4.63, predict loss = 1.35 (67.9 examples/sec; 0.059 sec/batch; 1h:54m:27s remains)
INFO - root - 2019-11-06 18:47:15.116998: step 33390, total loss = 3.28, predict loss = 0.88 (60.6 examples/sec; 0.066 sec/batch; 2h:08m:20s remains)
INFO - root - 2019-11-06 18:47:15.849655: step 33400, total loss = 3.69, predict loss = 0.98 (61.0 examples/sec; 0.066 sec/batch; 2h:07m:28s remains)
INFO - root - 2019-11-06 18:47:16.477427: step 33410, total loss = 4.88, predict loss = 1.35 (95.9 examples/sec; 0.042 sec/batch; 1h:21m:04s remains)
INFO - root - 2019-11-06 18:47:16.952884: step 33420, total loss = 5.55, predict loss = 1.62 (97.6 examples/sec; 0.041 sec/batch; 1h:19m:40s remains)
INFO - root - 2019-11-06 18:47:17.416101: step 33430, total loss = 3.91, predict loss = 0.99 (89.4 examples/sec; 0.045 sec/batch; 1h:26m:53s remains)
INFO - root - 2019-11-06 18:47:18.776164: step 33440, total loss = 3.06, predict loss = 0.80 (54.2 examples/sec; 0.074 sec/batch; 2h:23m:19s remains)
INFO - root - 2019-11-06 18:47:19.565194: step 33450, total loss = 4.70, predict loss = 1.30 (61.2 examples/sec; 0.065 sec/batch; 2h:07m:00s remains)
INFO - root - 2019-11-06 18:47:20.307932: step 33460, total loss = 5.05, predict loss = 1.43 (67.2 examples/sec; 0.060 sec/batch; 1h:55m:38s remains)
INFO - root - 2019-11-06 18:47:21.021548: step 33470, total loss = 5.06, predict loss = 1.37 (63.9 examples/sec; 0.063 sec/batch; 2h:01m:37s remains)
INFO - root - 2019-11-06 18:47:21.710608: step 33480, total loss = 4.24, predict loss = 1.17 (66.7 examples/sec; 0.060 sec/batch; 1h:56m:23s remains)
INFO - root - 2019-11-06 18:47:22.198433: step 33490, total loss = 5.23, predict loss = 1.47 (94.1 examples/sec; 0.043 sec/batch; 1h:22m:33s remains)
INFO - root - 2019-11-06 18:47:22.661206: step 33500, total loss = 5.16, predict loss = 1.50 (94.2 examples/sec; 0.042 sec/batch; 1h:22m:27s remains)
INFO - root - 2019-11-06 18:47:23.910823: step 33510, total loss = 5.09, predict loss = 1.39 (65.7 examples/sec; 0.061 sec/batch; 1h:58m:14s remains)
INFO - root - 2019-11-06 18:47:24.650835: step 33520, total loss = 4.63, predict loss = 1.32 (61.0 examples/sec; 0.066 sec/batch; 2h:07m:18s remains)
INFO - root - 2019-11-06 18:47:25.405608: step 33530, total loss = 5.47, predict loss = 1.49 (58.6 examples/sec; 0.068 sec/batch; 2h:12m:32s remains)
INFO - root - 2019-11-06 18:47:26.179504: step 33540, total loss = 4.30, predict loss = 1.13 (61.3 examples/sec; 0.065 sec/batch; 2h:06m:38s remains)
INFO - root - 2019-11-06 18:47:26.945610: step 33550, total loss = 4.59, predict loss = 1.33 (59.2 examples/sec; 0.068 sec/batch; 2h:11m:06s remains)
INFO - root - 2019-11-06 18:47:27.502116: step 33560, total loss = 2.96, predict loss = 0.82 (83.8 examples/sec; 0.048 sec/batch; 1h:32m:39s remains)
INFO - root - 2019-11-06 18:47:27.946035: step 33570, total loss = 3.22, predict loss = 0.83 (103.9 examples/sec; 0.038 sec/batch; 1h:14m:42s remains)
INFO - root - 2019-11-06 18:47:28.412439: step 33580, total loss = 3.81, predict loss = 1.01 (124.7 examples/sec; 0.032 sec/batch; 1h:02m:13s remains)
INFO - root - 2019-11-06 18:47:29.780544: step 33590, total loss = 3.77, predict loss = 1.00 (60.7 examples/sec; 0.066 sec/batch; 2h:07m:50s remains)
INFO - root - 2019-11-06 18:47:30.551367: step 33600, total loss = 4.21, predict loss = 1.15 (52.9 examples/sec; 0.076 sec/batch; 2h:26m:41s remains)
INFO - root - 2019-11-06 18:47:31.306970: step 33610, total loss = 5.47, predict loss = 1.53 (61.8 examples/sec; 0.065 sec/batch; 2h:05m:34s remains)
INFO - root - 2019-11-06 18:47:32.070127: step 33620, total loss = 3.70, predict loss = 1.03 (60.0 examples/sec; 0.067 sec/batch; 2h:09m:15s remains)
INFO - root - 2019-11-06 18:47:32.830517: step 33630, total loss = 1.94, predict loss = 0.53 (74.2 examples/sec; 0.054 sec/batch; 1h:44m:36s remains)
INFO - root - 2019-11-06 18:47:33.306537: step 33640, total loss = 4.30, predict loss = 1.18 (103.0 examples/sec; 0.039 sec/batch; 1h:15m:19s remains)
INFO - root - 2019-11-06 18:47:33.752296: step 33650, total loss = 4.38, predict loss = 1.15 (93.6 examples/sec; 0.043 sec/batch; 1h:22m:51s remains)
INFO - root - 2019-11-06 18:47:34.992023: step 33660, total loss = 2.73, predict loss = 0.76 (66.5 examples/sec; 0.060 sec/batch; 1h:56m:34s remains)
INFO - root - 2019-11-06 18:47:35.775107: step 33670, total loss = 5.20, predict loss = 1.49 (56.9 examples/sec; 0.070 sec/batch; 2h:16m:24s remains)
INFO - root - 2019-11-06 18:47:36.578573: step 33680, total loss = 4.28, predict loss = 1.20 (49.5 examples/sec; 0.081 sec/batch; 2h:36m:47s remains)
INFO - root - 2019-11-06 18:47:37.336702: step 33690, total loss = 5.11, predict loss = 1.45 (56.2 examples/sec; 0.071 sec/batch; 2h:18m:04s remains)
INFO - root - 2019-11-06 18:47:38.122922: step 33700, total loss = 4.35, predict loss = 1.16 (68.3 examples/sec; 0.059 sec/batch; 1h:53m:34s remains)
INFO - root - 2019-11-06 18:47:38.663341: step 33710, total loss = 3.91, predict loss = 1.12 (84.4 examples/sec; 0.047 sec/batch; 1h:31m:49s remains)
INFO - root - 2019-11-06 18:47:39.111223: step 33720, total loss = 3.17, predict loss = 0.87 (89.0 examples/sec; 0.045 sec/batch; 1h:27m:05s remains)
INFO - root - 2019-11-06 18:47:40.270029: step 33730, total loss = 3.83, predict loss = 1.03 (5.3 examples/sec; 0.758 sec/batch; 24h:29m:14s remains)
INFO - root - 2019-11-06 18:47:40.992791: step 33740, total loss = 3.70, predict loss = 0.94 (59.2 examples/sec; 0.068 sec/batch; 2h:10m:52s remains)
INFO - root - 2019-11-06 18:47:41.765717: step 33750, total loss = 4.13, predict loss = 1.13 (56.9 examples/sec; 0.070 sec/batch; 2h:16m:05s remains)
INFO - root - 2019-11-06 18:47:42.596483: step 33760, total loss = 2.90, predict loss = 0.89 (47.6 examples/sec; 0.084 sec/batch; 2h:42m:53s remains)
INFO - root - 2019-11-06 18:47:43.406866: step 33770, total loss = 4.15, predict loss = 1.19 (57.0 examples/sec; 0.070 sec/batch; 2h:15m:56s remains)
INFO - root - 2019-11-06 18:47:44.059547: step 33780, total loss = 3.50, predict loss = 0.96 (93.9 examples/sec; 0.043 sec/batch; 1h:22m:29s remains)
INFO - root - 2019-11-06 18:47:44.495625: step 33790, total loss = 3.48, predict loss = 0.86 (100.0 examples/sec; 0.040 sec/batch; 1h:17m:29s remains)
INFO - root - 2019-11-06 18:47:44.943476: step 33800, total loss = 3.62, predict loss = 1.01 (94.6 examples/sec; 0.042 sec/batch; 1h:21m:52s remains)
INFO - root - 2019-11-06 18:47:46.180189: step 33810, total loss = 4.81, predict loss = 1.42 (63.2 examples/sec; 0.063 sec/batch; 2h:02m:39s remains)
INFO - root - 2019-11-06 18:47:46.997351: step 33820, total loss = 4.90, predict loss = 1.32 (50.6 examples/sec; 0.079 sec/batch; 2h:32m:56s remains)
INFO - root - 2019-11-06 18:47:47.786239: step 33830, total loss = 2.60, predict loss = 0.74 (62.8 examples/sec; 0.064 sec/batch; 2h:03m:20s remains)
INFO - root - 2019-11-06 18:47:48.633942: step 33840, total loss = 2.40, predict loss = 0.66 (48.5 examples/sec; 0.082 sec/batch; 2h:39m:37s remains)
INFO - root - 2019-11-06 18:47:49.410132: step 33850, total loss = 4.59, predict loss = 1.22 (68.6 examples/sec; 0.058 sec/batch; 1h:52m:56s remains)
INFO - root - 2019-11-06 18:47:49.967481: step 33860, total loss = 3.98, predict loss = 1.10 (94.1 examples/sec; 0.042 sec/batch; 1h:22m:15s remains)
INFO - root - 2019-11-06 18:47:50.412743: step 33870, total loss = 3.44, predict loss = 0.88 (93.4 examples/sec; 0.043 sec/batch; 1h:22m:54s remains)
INFO - root - 2019-11-06 18:47:51.590039: step 33880, total loss = 2.22, predict loss = 0.60 (68.0 examples/sec; 0.059 sec/batch; 1h:53m:54s remains)
INFO - root - 2019-11-06 18:47:52.289598: step 33890, total loss = 3.62, predict loss = 0.94 (66.3 examples/sec; 0.060 sec/batch; 1h:56m:47s remains)
INFO - root - 2019-11-06 18:47:53.026579: step 33900, total loss = 5.14, predict loss = 1.41 (59.4 examples/sec; 0.067 sec/batch; 2h:10m:18s remains)
INFO - root - 2019-11-06 18:47:53.756549: step 33910, total loss = 3.27, predict loss = 0.90 (56.6 examples/sec; 0.071 sec/batch; 2h:16m:44s remains)
INFO - root - 2019-11-06 18:47:54.518229: step 33920, total loss = 3.38, predict loss = 0.92 (59.5 examples/sec; 0.067 sec/batch; 2h:10m:05s remains)
INFO - root - 2019-11-06 18:47:55.168505: step 33930, total loss = 4.34, predict loss = 1.15 (88.6 examples/sec; 0.045 sec/batch; 1h:27m:17s remains)
INFO - root - 2019-11-06 18:47:55.654499: step 33940, total loss = 4.33, predict loss = 1.14 (93.2 examples/sec; 0.043 sec/batch; 1h:23m:03s remains)
INFO - root - 2019-11-06 18:47:56.118005: step 33950, total loss = 4.84, predict loss = 1.34 (83.6 examples/sec; 0.048 sec/batch; 1h:32m:35s remains)
INFO - root - 2019-11-06 18:47:57.429066: step 33960, total loss = 4.69, predict loss = 1.34 (57.1 examples/sec; 0.070 sec/batch; 2h:15m:27s remains)
INFO - root - 2019-11-06 18:47:58.188020: step 33970, total loss = 2.46, predict loss = 0.64 (60.7 examples/sec; 0.066 sec/batch; 2h:07m:27s remains)
INFO - root - 2019-11-06 18:47:58.976471: step 33980, total loss = 5.06, predict loss = 1.42 (52.8 examples/sec; 0.076 sec/batch; 2h:26m:29s remains)
INFO - root - 2019-11-06 18:47:59.753517: step 33990, total loss = 2.52, predict loss = 0.67 (55.0 examples/sec; 0.073 sec/batch; 2h:20m:31s remains)
INFO - root - 2019-11-06 18:48:00.442716: step 34000, total loss = 3.21, predict loss = 0.82 (72.4 examples/sec; 0.055 sec/batch; 1h:46m:50s remains)
INFO - root - 2019-11-06 18:48:00.954360: step 34010, total loss = 4.61, predict loss = 1.24 (96.9 examples/sec; 0.041 sec/batch; 1h:19m:50s remains)
INFO - root - 2019-11-06 18:48:01.430824: step 34020, total loss = 2.65, predict loss = 0.75 (99.5 examples/sec; 0.040 sec/batch; 1h:17m:42s remains)
INFO - root - 2019-11-06 18:48:02.610882: step 34030, total loss = 3.48, predict loss = 0.89 (60.7 examples/sec; 0.066 sec/batch; 2h:07m:21s remains)
INFO - root - 2019-11-06 18:48:03.298388: step 34040, total loss = 2.66, predict loss = 0.69 (64.0 examples/sec; 0.063 sec/batch; 2h:00m:50s remains)
INFO - root - 2019-11-06 18:48:04.061058: step 34050, total loss = 3.63, predict loss = 0.92 (59.8 examples/sec; 0.067 sec/batch; 2h:09m:18s remains)
INFO - root - 2019-11-06 18:48:04.810458: step 34060, total loss = 5.00, predict loss = 1.38 (62.2 examples/sec; 0.064 sec/batch; 2h:04m:14s remains)
INFO - root - 2019-11-06 18:48:05.567530: step 34070, total loss = 4.63, predict loss = 1.21 (55.5 examples/sec; 0.072 sec/batch; 2h:19m:16s remains)
INFO - root - 2019-11-06 18:48:06.211019: step 34080, total loss = 5.87, predict loss = 1.63 (100.0 examples/sec; 0.040 sec/batch; 1h:17m:15s remains)
INFO - root - 2019-11-06 18:48:06.676879: step 34090, total loss = 4.24, predict loss = 1.17 (95.1 examples/sec; 0.042 sec/batch; 1h:21m:12s remains)
INFO - root - 2019-11-06 18:48:07.169537: step 34100, total loss = 4.58, predict loss = 1.22 (97.1 examples/sec; 0.041 sec/batch; 1h:19m:34s remains)
INFO - root - 2019-11-06 18:48:08.440581: step 34110, total loss = 2.79, predict loss = 0.74 (59.0 examples/sec; 0.068 sec/batch; 2h:10m:56s remains)
INFO - root - 2019-11-06 18:48:09.214776: step 34120, total loss = 3.04, predict loss = 0.85 (56.9 examples/sec; 0.070 sec/batch; 2h:15m:42s remains)
INFO - root - 2019-11-06 18:48:09.981276: step 34130, total loss = 3.54, predict loss = 0.97 (56.9 examples/sec; 0.070 sec/batch; 2h:15m:41s remains)
INFO - root - 2019-11-06 18:48:10.770901: step 34140, total loss = 4.31, predict loss = 1.22 (54.7 examples/sec; 0.073 sec/batch; 2h:21m:16s remains)
INFO - root - 2019-11-06 18:48:11.515856: step 34150, total loss = 3.37, predict loss = 0.98 (64.7 examples/sec; 0.062 sec/batch; 1h:59m:25s remains)
INFO - root - 2019-11-06 18:48:12.056225: step 34160, total loss = 4.11, predict loss = 1.09 (78.5 examples/sec; 0.051 sec/batch; 1h:38m:24s remains)
INFO - root - 2019-11-06 18:48:12.503591: step 34170, total loss = 3.69, predict loss = 0.96 (95.7 examples/sec; 0.042 sec/batch; 1h:20m:43s remains)
INFO - root - 2019-11-06 18:48:13.739478: step 34180, total loss = 4.26, predict loss = 1.17 (64.7 examples/sec; 0.062 sec/batch; 1h:59m:20s remains)
INFO - root - 2019-11-06 18:48:14.443234: step 34190, total loss = 3.71, predict loss = 1.03 (55.2 examples/sec; 0.072 sec/batch; 2h:19m:45s remains)
INFO - root - 2019-11-06 18:48:15.270547: step 34200, total loss = 3.15, predict loss = 0.84 (53.9 examples/sec; 0.074 sec/batch; 2h:23m:18s remains)
INFO - root - 2019-11-06 18:48:16.004766: step 34210, total loss = 2.05, predict loss = 0.54 (56.1 examples/sec; 0.071 sec/batch; 2h:17m:33s remains)
INFO - root - 2019-11-06 18:48:16.753174: step 34220, total loss = 3.43, predict loss = 0.93 (61.1 examples/sec; 0.066 sec/batch; 2h:06m:24s remains)
INFO - root - 2019-11-06 18:48:17.346730: step 34230, total loss = 4.56, predict loss = 1.21 (100.4 examples/sec; 0.040 sec/batch; 1h:16m:52s remains)
INFO - root - 2019-11-06 18:48:17.792397: step 34240, total loss = 5.39, predict loss = 1.52 (98.0 examples/sec; 0.041 sec/batch; 1h:18m:43s remains)
INFO - root - 2019-11-06 18:48:18.247824: step 34250, total loss = 3.72, predict loss = 0.97 (89.4 examples/sec; 0.045 sec/batch; 1h:26m:19s remains)
INFO - root - 2019-11-06 18:48:19.659492: step 34260, total loss = 4.03, predict loss = 1.07 (51.7 examples/sec; 0.077 sec/batch; 2h:29m:10s remains)
INFO - root - 2019-11-06 18:48:20.495984: step 34270, total loss = 5.27, predict loss = 1.41 (48.9 examples/sec; 0.082 sec/batch; 2h:37m:50s remains)
INFO - root - 2019-11-06 18:48:21.351341: step 34280, total loss = 4.07, predict loss = 1.11 (52.0 examples/sec; 0.077 sec/batch; 2h:28m:14s remains)
INFO - root - 2019-11-06 18:48:22.142429: step 34290, total loss = 4.86, predict loss = 1.44 (57.8 examples/sec; 0.069 sec/batch; 2h:13m:29s remains)
INFO - root - 2019-11-06 18:48:22.879987: step 34300, total loss = 4.54, predict loss = 1.17 (74.2 examples/sec; 0.054 sec/batch; 1h:43m:55s remains)
INFO - root - 2019-11-06 18:48:23.347441: step 34310, total loss = 3.89, predict loss = 1.04 (98.9 examples/sec; 0.040 sec/batch; 1h:18m:01s remains)
INFO - root - 2019-11-06 18:48:23.802208: step 34320, total loss = 4.73, predict loss = 1.24 (100.7 examples/sec; 0.040 sec/batch; 1h:16m:33s remains)
INFO - root - 2019-11-06 18:48:25.003287: step 34330, total loss = 5.32, predict loss = 1.62 (63.5 examples/sec; 0.063 sec/batch; 2h:01m:27s remains)
INFO - root - 2019-11-06 18:48:25.755748: step 34340, total loss = 4.99, predict loss = 1.31 (60.9 examples/sec; 0.066 sec/batch; 2h:06m:39s remains)
INFO - root - 2019-11-06 18:48:26.526939: step 34350, total loss = 4.26, predict loss = 1.12 (52.2 examples/sec; 0.077 sec/batch; 2h:27m:39s remains)
INFO - root - 2019-11-06 18:48:27.314695: step 34360, total loss = 2.39, predict loss = 0.65 (53.1 examples/sec; 0.075 sec/batch; 2h:25m:07s remains)
INFO - root - 2019-11-06 18:48:28.083108: step 34370, total loss = 4.41, predict loss = 1.16 (63.5 examples/sec; 0.063 sec/batch; 2h:01m:27s remains)
INFO - root - 2019-11-06 18:48:28.651679: step 34380, total loss = 3.14, predict loss = 0.82 (101.7 examples/sec; 0.039 sec/batch; 1h:15m:49s remains)
INFO - root - 2019-11-06 18:48:29.111028: step 34390, total loss = 3.73, predict loss = 0.98 (93.7 examples/sec; 0.043 sec/batch; 1h:22m:14s remains)
INFO - root - 2019-11-06 18:48:29.572892: step 34400, total loss = 4.70, predict loss = 1.47 (128.7 examples/sec; 0.031 sec/batch; 0h:59m:52s remains)
INFO - root - 2019-11-06 18:48:30.908732: step 34410, total loss = 4.06, predict loss = 1.05 (64.1 examples/sec; 0.062 sec/batch; 2h:00m:15s remains)
INFO - root - 2019-11-06 18:48:31.670485: step 34420, total loss = 2.31, predict loss = 0.61 (60.0 examples/sec; 0.067 sec/batch; 2h:08m:28s remains)
INFO - root - 2019-11-06 18:48:32.441042: step 34430, total loss = 3.59, predict loss = 0.95 (59.1 examples/sec; 0.068 sec/batch; 2h:10m:17s remains)
INFO - root - 2019-11-06 18:48:33.235627: step 34440, total loss = 2.11, predict loss = 0.57 (58.2 examples/sec; 0.069 sec/batch; 2h:12m:18s remains)
INFO - root - 2019-11-06 18:48:33.966784: step 34450, total loss = 4.06, predict loss = 1.10 (68.3 examples/sec; 0.059 sec/batch; 1h:52m:50s remains)
INFO - root - 2019-11-06 18:48:34.477057: step 34460, total loss = 2.94, predict loss = 0.83 (100.2 examples/sec; 0.040 sec/batch; 1h:16m:52s remains)
INFO - root - 2019-11-06 18:48:34.932116: step 34470, total loss = 4.67, predict loss = 1.20 (96.6 examples/sec; 0.041 sec/batch; 1h:19m:44s remains)
INFO - root - 2019-11-06 18:48:36.190084: step 34480, total loss = 4.19, predict loss = 1.15 (57.5 examples/sec; 0.070 sec/batch; 2h:14m:03s remains)
INFO - root - 2019-11-06 18:48:36.902036: step 34490, total loss = 3.82, predict loss = 1.04 (67.0 examples/sec; 0.060 sec/batch; 1h:54m:57s remains)
INFO - root - 2019-11-06 18:48:37.648129: step 34500, total loss = 5.04, predict loss = 1.44 (56.2 examples/sec; 0.071 sec/batch; 2h:16m:56s remains)
INFO - root - 2019-11-06 18:48:38.397598: step 34510, total loss = 3.20, predict loss = 0.79 (59.5 examples/sec; 0.067 sec/batch; 2h:09m:22s remains)
INFO - root - 2019-11-06 18:48:39.074448: step 34520, total loss = 5.05, predict loss = 1.39 (77.5 examples/sec; 0.052 sec/batch; 1h:39m:23s remains)
INFO - root - 2019-11-06 18:48:39.557881: step 34530, total loss = 5.14, predict loss = 1.44 (95.2 examples/sec; 0.042 sec/batch; 1h:20m:49s remains)
INFO - root - 2019-11-06 18:48:40.047721: step 34540, total loss = 3.77, predict loss = 1.01 (89.7 examples/sec; 0.045 sec/batch; 1h:25m:50s remains)
INFO - root - 2019-11-06 18:48:41.208240: step 34550, total loss = 3.66, predict loss = 0.97 (5.4 examples/sec; 0.744 sec/batch; 23h:51m:02s remains)
INFO - root - 2019-11-06 18:48:41.906168: step 34560, total loss = 4.24, predict loss = 1.17 (55.4 examples/sec; 0.072 sec/batch; 2h:18m:52s remains)
INFO - root - 2019-11-06 18:48:42.643983: step 34570, total loss = 4.10, predict loss = 1.09 (67.4 examples/sec; 0.059 sec/batch; 1h:54m:10s remains)
INFO - root - 2019-11-06 18:48:43.407607: step 34580, total loss = 4.11, predict loss = 1.13 (51.9 examples/sec; 0.077 sec/batch; 2h:28m:16s remains)
INFO - root - 2019-11-06 18:48:44.209175: step 34590, total loss = 3.45, predict loss = 0.90 (50.6 examples/sec; 0.079 sec/batch; 2h:32m:00s remains)
INFO - root - 2019-11-06 18:48:44.937577: step 34600, total loss = 3.98, predict loss = 1.00 (84.5 examples/sec; 0.047 sec/batch; 1h:31m:04s remains)
INFO - root - 2019-11-06 18:48:45.408617: step 34610, total loss = 4.00, predict loss = 1.02 (88.1 examples/sec; 0.045 sec/batch; 1h:27m:20s remains)
INFO - root - 2019-11-06 18:48:45.892508: step 34620, total loss = 3.97, predict loss = 1.06 (93.5 examples/sec; 0.043 sec/batch; 1h:22m:16s remains)
INFO - root - 2019-11-06 18:48:47.123575: step 34630, total loss = 4.42, predict loss = 1.24 (64.5 examples/sec; 0.062 sec/batch; 1h:59m:09s remains)
INFO - root - 2019-11-06 18:48:47.851331: step 34640, total loss = 4.95, predict loss = 1.32 (59.8 examples/sec; 0.067 sec/batch; 2h:08m:33s remains)
INFO - root - 2019-11-06 18:48:48.622197: step 34650, total loss = 3.71, predict loss = 0.99 (56.4 examples/sec; 0.071 sec/batch; 2h:16m:19s remains)
INFO - root - 2019-11-06 18:48:49.398054: step 34660, total loss = 3.73, predict loss = 1.03 (56.9 examples/sec; 0.070 sec/batch; 2h:15m:04s remains)
INFO - root - 2019-11-06 18:48:50.151807: step 34670, total loss = 2.61, predict loss = 0.67 (65.1 examples/sec; 0.061 sec/batch; 1h:58m:02s remains)
INFO - root - 2019-11-06 18:48:50.723957: step 34680, total loss = 4.45, predict loss = 1.14 (90.5 examples/sec; 0.044 sec/batch; 1h:24m:55s remains)
INFO - root - 2019-11-06 18:48:51.170285: step 34690, total loss = 4.05, predict loss = 1.09 (100.2 examples/sec; 0.040 sec/batch; 1h:16m:42s remains)
INFO - root - 2019-11-06 18:48:52.340863: step 34700, total loss = 3.52, predict loss = 0.94 (72.9 examples/sec; 0.055 sec/batch; 1h:45m:27s remains)
INFO - root - 2019-11-06 18:48:53.011073: step 34710, total loss = 4.11, predict loss = 1.12 (64.7 examples/sec; 0.062 sec/batch; 1h:58m:43s remains)
INFO - root - 2019-11-06 18:48:53.725861: step 34720, total loss = 2.64, predict loss = 0.74 (60.2 examples/sec; 0.066 sec/batch; 2h:07m:45s remains)
INFO - root - 2019-11-06 18:48:54.456962: step 34730, total loss = 3.86, predict loss = 1.04 (62.6 examples/sec; 0.064 sec/batch; 2h:02m:46s remains)
INFO - root - 2019-11-06 18:48:55.198293: step 34740, total loss = 3.83, predict loss = 1.00 (58.6 examples/sec; 0.068 sec/batch; 2h:11m:13s remains)
INFO - root - 2019-11-06 18:48:55.819484: step 34750, total loss = 2.95, predict loss = 0.76 (96.3 examples/sec; 0.042 sec/batch; 1h:19m:47s remains)
INFO - root - 2019-11-06 18:48:56.275061: step 34760, total loss = 3.18, predict loss = 0.85 (100.0 examples/sec; 0.040 sec/batch; 1h:16m:50s remains)
INFO - root - 2019-11-06 18:48:56.712968: step 34770, total loss = 3.91, predict loss = 1.03 (99.2 examples/sec; 0.040 sec/batch; 1h:17m:27s remains)
INFO - root - 2019-11-06 18:48:58.074530: step 34780, total loss = 3.92, predict loss = 1.05 (66.1 examples/sec; 0.060 sec/batch; 1h:56m:07s remains)
INFO - root - 2019-11-06 18:48:58.800312: step 34790, total loss = 3.95, predict loss = 1.06 (57.9 examples/sec; 0.069 sec/batch; 2h:12m:41s remains)
INFO - root - 2019-11-06 18:48:59.499784: step 34800, total loss = 3.50, predict loss = 0.94 (59.4 examples/sec; 0.067 sec/batch; 2h:09m:16s remains)
INFO - root - 2019-11-06 18:49:00.262028: step 34810, total loss = 3.95, predict loss = 1.06 (63.6 examples/sec; 0.063 sec/batch; 2h:00m:39s remains)
INFO - root - 2019-11-06 18:49:01.000622: step 34820, total loss = 2.61, predict loss = 0.71 (65.4 examples/sec; 0.061 sec/batch; 1h:57m:24s remains)
INFO - root - 2019-11-06 18:49:01.530979: step 34830, total loss = 3.40, predict loss = 0.93 (84.3 examples/sec; 0.047 sec/batch; 1h:31m:07s remains)
INFO - root - 2019-11-06 18:49:01.977761: step 34840, total loss = 2.96, predict loss = 0.79 (94.3 examples/sec; 0.042 sec/batch; 1h:21m:22s remains)
INFO - root - 2019-11-06 18:49:03.153465: step 34850, total loss = 4.21, predict loss = 1.12 (66.6 examples/sec; 0.060 sec/batch; 1h:55m:18s remains)
INFO - root - 2019-11-06 18:49:03.872448: step 34860, total loss = 2.77, predict loss = 0.72 (62.3 examples/sec; 0.064 sec/batch; 2h:03m:12s remains)
INFO - root - 2019-11-06 18:49:04.660879: step 34870, total loss = 3.21, predict loss = 0.83 (55.4 examples/sec; 0.072 sec/batch; 2h:18m:28s remains)
INFO - root - 2019-11-06 18:49:05.461798: step 34880, total loss = 4.39, predict loss = 1.19 (68.1 examples/sec; 0.059 sec/batch; 1h:52m:43s remains)
INFO - root - 2019-11-06 18:49:06.136389: step 34890, total loss = 2.18, predict loss = 0.59 (70.9 examples/sec; 0.056 sec/batch; 1h:48m:16s remains)
INFO - root - 2019-11-06 18:49:06.724879: step 34900, total loss = 3.30, predict loss = 0.88 (97.3 examples/sec; 0.041 sec/batch; 1h:18m:50s remains)
INFO - root - 2019-11-06 18:49:07.185139: step 34910, total loss = 3.70, predict loss = 1.04 (93.3 examples/sec; 0.043 sec/batch; 1h:22m:13s remains)
INFO - root - 2019-11-06 18:49:07.643311: step 34920, total loss = 2.56, predict loss = 0.68 (90.9 examples/sec; 0.044 sec/batch; 1h:24m:23s remains)
INFO - root - 2019-11-06 18:49:08.982746: step 34930, total loss = 4.87, predict loss = 1.36 (52.6 examples/sec; 0.076 sec/batch; 2h:25m:51s remains)
INFO - root - 2019-11-06 18:49:09.747872: step 34940, total loss = 4.75, predict loss = 1.34 (66.6 examples/sec; 0.060 sec/batch; 1h:55m:11s remains)
INFO - root - 2019-11-06 18:49:10.556839: step 34950, total loss = 5.64, predict loss = 1.58 (48.9 examples/sec; 0.082 sec/batch; 2h:36m:53s remains)
INFO - root - 2019-11-06 18:49:11.371277: step 34960, total loss = 5.06, predict loss = 1.41 (59.3 examples/sec; 0.068 sec/batch; 2h:09m:25s remains)
INFO - root - 2019-11-06 18:49:12.114355: step 34970, total loss = 2.68, predict loss = 0.73 (64.7 examples/sec; 0.062 sec/batch; 1h:58m:36s remains)
INFO - root - 2019-11-06 18:49:12.633774: step 34980, total loss = 3.59, predict loss = 0.96 (95.6 examples/sec; 0.042 sec/batch; 1h:20m:10s remains)
INFO - root - 2019-11-06 18:49:13.097811: step 34990, total loss = 3.28, predict loss = 0.84 (95.4 examples/sec; 0.042 sec/batch; 1h:20m:20s remains)
INFO - root - 2019-11-06 18:49:14.265893: step 35000, total loss = 2.33, predict loss = 0.58 (69.1 examples/sec; 0.058 sec/batch; 1h:50m:58s remains)
INFO - root - 2019-11-06 18:49:14.959441: step 35010, total loss = 4.26, predict loss = 1.13 (64.8 examples/sec; 0.062 sec/batch; 1h:58m:15s remains)
INFO - root - 2019-11-06 18:49:15.725883: step 35020, total loss = 4.50, predict loss = 1.14 (58.7 examples/sec; 0.068 sec/batch; 2h:10m:33s remains)
INFO - root - 2019-11-06 18:49:16.510350: step 35030, total loss = 3.55, predict loss = 0.90 (64.1 examples/sec; 0.062 sec/batch; 1h:59m:38s remains)
INFO - root - 2019-11-06 18:49:17.242046: step 35040, total loss = 3.26, predict loss = 0.84 (63.1 examples/sec; 0.063 sec/batch; 2h:01m:28s remains)
INFO - root - 2019-11-06 18:49:17.811210: step 35050, total loss = 3.62, predict loss = 1.11 (100.4 examples/sec; 0.040 sec/batch; 1h:16m:21s remains)
INFO - root - 2019-11-06 18:49:18.301832: step 35060, total loss = 4.71, predict loss = 1.25 (96.5 examples/sec; 0.041 sec/batch; 1h:19m:26s remains)
INFO - root - 2019-11-06 18:49:18.762713: step 35070, total loss = 2.97, predict loss = 0.76 (91.0 examples/sec; 0.044 sec/batch; 1h:24m:14s remains)
INFO - root - 2019-11-06 18:49:20.132801: step 35080, total loss = 4.88, predict loss = 1.31 (56.0 examples/sec; 0.071 sec/batch; 2h:16m:47s remains)
INFO - root - 2019-11-06 18:49:20.976051: step 35090, total loss = 3.84, predict loss = 1.00 (54.7 examples/sec; 0.073 sec/batch; 2h:19m:56s remains)
INFO - root - 2019-11-06 18:49:21.726002: step 35100, total loss = 4.23, predict loss = 1.08 (60.7 examples/sec; 0.066 sec/batch; 2h:06m:09s remains)
INFO - root - 2019-11-06 18:49:22.491471: step 35110, total loss = 2.11, predict loss = 0.54 (60.0 examples/sec; 0.067 sec/batch; 2h:07m:33s remains)
INFO - root - 2019-11-06 18:49:23.162474: step 35120, total loss = 2.71, predict loss = 0.68 (78.9 examples/sec; 0.051 sec/batch; 1h:37m:01s remains)
INFO - root - 2019-11-06 18:49:23.629774: step 35130, total loss = 4.87, predict loss = 1.36 (91.2 examples/sec; 0.044 sec/batch; 1h:24m:00s remains)
INFO - root - 2019-11-06 18:49:24.102794: step 35140, total loss = 4.18, predict loss = 1.12 (98.3 examples/sec; 0.041 sec/batch; 1h:17m:52s remains)
INFO - root - 2019-11-06 18:49:25.310802: step 35150, total loss = 2.49, predict loss = 0.64 (67.0 examples/sec; 0.060 sec/batch; 1h:54m:16s remains)
INFO - root - 2019-11-06 18:49:26.014314: step 35160, total loss = 4.63, predict loss = 1.25 (60.2 examples/sec; 0.066 sec/batch; 2h:07m:15s remains)
INFO - root - 2019-11-06 18:49:26.764472: step 35170, total loss = 1.86, predict loss = 0.49 (50.6 examples/sec; 0.079 sec/batch; 2h:31m:21s remains)
INFO - root - 2019-11-06 18:49:27.568215: step 35180, total loss = 2.95, predict loss = 0.76 (46.3 examples/sec; 0.086 sec/batch; 2h:45m:18s remains)
INFO - root - 2019-11-06 18:49:28.302527: step 35190, total loss = 2.24, predict loss = 0.57 (56.2 examples/sec; 0.071 sec/batch; 2h:16m:15s remains)
INFO - root - 2019-11-06 18:49:28.859262: step 35200, total loss = 4.12, predict loss = 1.13 (98.4 examples/sec; 0.041 sec/batch; 1h:17m:46s remains)
INFO - root - 2019-11-06 18:49:29.317330: step 35210, total loss = 3.58, predict loss = 0.94 (95.9 examples/sec; 0.042 sec/batch; 1h:19m:47s remains)
INFO - root - 2019-11-06 18:49:29.792366: step 35220, total loss = 4.31, predict loss = 1.17 (124.5 examples/sec; 0.032 sec/batch; 1h:01m:26s remains)
INFO - root - 2019-11-06 18:49:31.143053: step 35230, total loss = 3.60, predict loss = 1.00 (57.3 examples/sec; 0.070 sec/batch; 2h:13m:32s remains)
INFO - root - 2019-11-06 18:49:31.898927: step 35240, total loss = 3.66, predict loss = 1.01 (58.6 examples/sec; 0.068 sec/batch; 2h:10m:35s remains)
INFO - root - 2019-11-06 18:49:32.671914: step 35250, total loss = 3.50, predict loss = 0.94 (58.6 examples/sec; 0.068 sec/batch; 2h:10m:37s remains)
INFO - root - 2019-11-06 18:49:33.412811: step 35260, total loss = 4.21, predict loss = 1.14 (59.7 examples/sec; 0.067 sec/batch; 2h:08m:11s remains)
INFO - root - 2019-11-06 18:49:34.121874: step 35270, total loss = 3.85, predict loss = 1.05 (74.1 examples/sec; 0.054 sec/batch; 1h:43m:15s remains)
INFO - root - 2019-11-06 18:49:34.601497: step 35280, total loss = 4.31, predict loss = 1.20 (97.0 examples/sec; 0.041 sec/batch; 1h:18m:48s remains)
INFO - root - 2019-11-06 18:49:35.064986: step 35290, total loss = 2.48, predict loss = 0.64 (94.8 examples/sec; 0.042 sec/batch; 1h:20m:42s remains)
INFO - root - 2019-11-06 18:49:36.301604: step 35300, total loss = 2.92, predict loss = 0.76 (63.8 examples/sec; 0.063 sec/batch; 1h:59m:56s remains)
INFO - root - 2019-11-06 18:49:37.094317: step 35310, total loss = 2.91, predict loss = 0.75 (57.6 examples/sec; 0.069 sec/batch; 2h:12m:44s remains)
INFO - root - 2019-11-06 18:49:37.808969: step 35320, total loss = 3.70, predict loss = 0.93 (60.8 examples/sec; 0.066 sec/batch; 2h:05m:46s remains)
INFO - root - 2019-11-06 18:49:38.546395: step 35330, total loss = 3.76, predict loss = 0.99 (65.3 examples/sec; 0.061 sec/batch; 1h:57m:07s remains)
INFO - root - 2019-11-06 18:49:39.279805: step 35340, total loss = 3.72, predict loss = 1.00 (68.5 examples/sec; 0.058 sec/batch; 1h:51m:35s remains)
INFO - root - 2019-11-06 18:49:39.818396: step 35350, total loss = 3.35, predict loss = 0.90 (100.3 examples/sec; 0.040 sec/batch; 1h:16m:13s remains)
INFO - root - 2019-11-06 18:49:40.262479: step 35360, total loss = 4.00, predict loss = 1.08 (96.8 examples/sec; 0.041 sec/batch; 1h:18m:56s remains)
INFO - root - 2019-11-06 18:49:41.407988: step 35370, total loss = 5.04, predict loss = 1.34 (5.5 examples/sec; 0.733 sec/batch; 23h:20m:41s remains)
INFO - root - 2019-11-06 18:49:42.153202: step 35380, total loss = 3.82, predict loss = 1.03 (61.2 examples/sec; 0.065 sec/batch; 2h:04m:52s remains)
INFO - root - 2019-11-06 18:49:42.979316: step 35390, total loss = 4.34, predict loss = 1.19 (56.1 examples/sec; 0.071 sec/batch; 2h:16m:16s remains)
INFO - root - 2019-11-06 18:49:43.748622: step 35400, total loss = 4.04, predict loss = 1.07 (54.7 examples/sec; 0.073 sec/batch; 2h:19m:46s remains)
INFO - root - 2019-11-06 18:49:44.490151: step 35410, total loss = 2.81, predict loss = 0.78 (62.2 examples/sec; 0.064 sec/batch; 2h:02m:47s remains)
INFO - root - 2019-11-06 18:49:45.213840: step 35420, total loss = 3.23, predict loss = 0.92 (85.5 examples/sec; 0.047 sec/batch; 1h:29m:21s remains)
INFO - root - 2019-11-06 18:49:45.667474: step 35430, total loss = 4.23, predict loss = 1.12 (96.3 examples/sec; 0.042 sec/batch; 1h:19m:16s remains)
INFO - root - 2019-11-06 18:49:46.117776: step 35440, total loss = 3.31, predict loss = 0.88 (93.2 examples/sec; 0.043 sec/batch; 1h:21m:57s remains)
INFO - root - 2019-11-06 18:49:47.396382: step 35450, total loss = 4.53, predict loss = 1.17 (66.0 examples/sec; 0.061 sec/batch; 1h:55m:45s remains)
INFO - root - 2019-11-06 18:49:48.144363: step 35460, total loss = 3.95, predict loss = 1.08 (57.2 examples/sec; 0.070 sec/batch; 2h:13m:34s remains)
INFO - root - 2019-11-06 18:49:48.924092: step 35470, total loss = 5.29, predict loss = 1.53 (57.4 examples/sec; 0.070 sec/batch; 2h:12m:55s remains)
INFO - root - 2019-11-06 18:49:49.625626: step 35480, total loss = 3.64, predict loss = 0.97 (63.9 examples/sec; 0.063 sec/batch; 1h:59m:25s remains)
INFO - root - 2019-11-06 18:49:50.442854: step 35490, total loss = 3.46, predict loss = 0.88 (61.1 examples/sec; 0.065 sec/batch; 2h:04m:51s remains)
INFO - root - 2019-11-06 18:49:51.006444: step 35500, total loss = 2.84, predict loss = 0.70 (97.7 examples/sec; 0.041 sec/batch; 1h:18m:05s remains)
INFO - root - 2019-11-06 18:49:51.456449: step 35510, total loss = 4.17, predict loss = 1.18 (94.8 examples/sec; 0.042 sec/batch; 1h:20m:30s remains)
INFO - root - 2019-11-06 18:49:52.605499: step 35520, total loss = 4.42, predict loss = 1.25 (66.8 examples/sec; 0.060 sec/batch; 1h:54m:17s remains)
INFO - root - 2019-11-06 18:49:53.304944: step 35530, total loss = 3.62, predict loss = 0.93 (56.4 examples/sec; 0.071 sec/batch; 2h:15m:17s remains)
INFO - root - 2019-11-06 18:49:54.064958: step 35540, total loss = 3.80, predict loss = 1.03 (58.3 examples/sec; 0.069 sec/batch; 2h:10m:57s remains)
INFO - root - 2019-11-06 18:49:54.876676: step 35550, total loss = 3.40, predict loss = 0.88 (45.9 examples/sec; 0.087 sec/batch; 2h:46m:06s remains)
INFO - root - 2019-11-06 18:49:55.637801: step 35560, total loss = 3.95, predict loss = 1.04 (62.9 examples/sec; 0.064 sec/batch; 2h:01m:13s remains)
INFO - root - 2019-11-06 18:49:56.303409: step 35570, total loss = 3.88, predict loss = 1.02 (95.2 examples/sec; 0.042 sec/batch; 1h:20m:05s remains)
INFO - root - 2019-11-06 18:49:56.815248: step 35580, total loss = 2.07, predict loss = 0.54 (86.3 examples/sec; 0.046 sec/batch; 1h:28m:23s remains)
INFO - root - 2019-11-06 18:49:57.290696: step 35590, total loss = 3.52, predict loss = 0.91 (87.9 examples/sec; 0.046 sec/batch; 1h:26m:46s remains)
INFO - root - 2019-11-06 18:49:58.577678: step 35600, total loss = 2.82, predict loss = 0.74 (57.4 examples/sec; 0.070 sec/batch; 2h:12m:52s remains)
INFO - root - 2019-11-06 18:49:59.307141: step 35610, total loss = 4.20, predict loss = 1.14 (58.9 examples/sec; 0.068 sec/batch; 2h:09m:24s remains)
INFO - root - 2019-11-06 18:50:00.051670: step 35620, total loss = 3.38, predict loss = 0.87 (68.1 examples/sec; 0.059 sec/batch; 1h:51m:54s remains)
INFO - root - 2019-11-06 18:50:00.744891: step 35630, total loss = 5.31, predict loss = 1.53 (70.2 examples/sec; 0.057 sec/batch; 1h:48m:35s remains)
INFO - root - 2019-11-06 18:50:01.469141: step 35640, total loss = 4.24, predict loss = 1.12 (69.0 examples/sec; 0.058 sec/batch; 1h:50m:28s remains)
INFO - root - 2019-11-06 18:50:01.992446: step 35650, total loss = 4.45, predict loss = 1.29 (91.9 examples/sec; 0.044 sec/batch; 1h:22m:57s remains)
INFO - root - 2019-11-06 18:50:02.481663: step 35660, total loss = 3.26, predict loss = 0.87 (96.5 examples/sec; 0.041 sec/batch; 1h:18m:57s remains)
INFO - root - 2019-11-06 18:50:03.684592: step 35670, total loss = 2.09, predict loss = 0.53 (67.1 examples/sec; 0.060 sec/batch; 1h:53m:34s remains)
INFO - root - 2019-11-06 18:50:04.344657: step 35680, total loss = 4.34, predict loss = 1.18 (63.8 examples/sec; 0.063 sec/batch; 1h:59m:27s remains)
INFO - root - 2019-11-06 18:50:05.118518: step 35690, total loss = 3.50, predict loss = 0.93 (58.7 examples/sec; 0.068 sec/batch; 2h:09m:43s remains)
INFO - root - 2019-11-06 18:50:05.882836: step 35700, total loss = 3.19, predict loss = 0.86 (54.6 examples/sec; 0.073 sec/batch; 2h:19m:31s remains)
INFO - root - 2019-11-06 18:50:06.656311: step 35710, total loss = 3.86, predict loss = 0.98 (56.9 examples/sec; 0.070 sec/batch; 2h:13m:53s remains)
INFO - root - 2019-11-06 18:50:07.279078: step 35720, total loss = 3.44, predict loss = 0.90 (92.2 examples/sec; 0.043 sec/batch; 1h:22m:37s remains)
INFO - root - 2019-11-06 18:50:07.734890: step 35730, total loss = 4.43, predict loss = 1.18 (89.3 examples/sec; 0.045 sec/batch; 1h:25m:18s remains)
INFO - root - 2019-11-06 18:50:08.215706: step 35740, total loss = 4.11, predict loss = 1.08 (103.4 examples/sec; 0.039 sec/batch; 1h:13m:41s remains)
INFO - root - 2019-11-06 18:50:09.507524: step 35750, total loss = 4.44, predict loss = 1.24 (68.1 examples/sec; 0.059 sec/batch; 1h:51m:54s remains)
INFO - root - 2019-11-06 18:50:10.255725: step 35760, total loss = 4.56, predict loss = 1.20 (57.5 examples/sec; 0.070 sec/batch; 2h:12m:22s remains)
INFO - root - 2019-11-06 18:50:11.057383: step 35770, total loss = 5.19, predict loss = 1.52 (49.8 examples/sec; 0.080 sec/batch; 2h:33m:00s remains)
INFO - root - 2019-11-06 18:50:11.853572: step 35780, total loss = 4.68, predict loss = 1.24 (59.2 examples/sec; 0.068 sec/batch; 2h:08m:32s remains)
INFO - root - 2019-11-06 18:50:12.552225: step 35790, total loss = 5.00, predict loss = 1.40 (69.3 examples/sec; 0.058 sec/batch; 1h:49m:47s remains)
INFO - root - 2019-11-06 18:50:13.054564: step 35800, total loss = 3.16, predict loss = 0.79 (92.4 examples/sec; 0.043 sec/batch; 1h:22m:24s remains)
INFO - root - 2019-11-06 18:50:13.523578: step 35810, total loss = 4.90, predict loss = 1.35 (90.7 examples/sec; 0.044 sec/batch; 1h:23m:55s remains)
INFO - root - 2019-11-06 18:50:14.733902: step 35820, total loss = 2.03, predict loss = 0.57 (65.1 examples/sec; 0.061 sec/batch; 1h:56m:51s remains)
INFO - root - 2019-11-06 18:50:15.435698: step 35830, total loss = 3.61, predict loss = 0.94 (62.3 examples/sec; 0.064 sec/batch; 2h:02m:08s remains)
INFO - root - 2019-11-06 18:50:16.232771: step 35840, total loss = 2.36, predict loss = 0.61 (50.8 examples/sec; 0.079 sec/batch; 2h:29m:40s remains)
INFO - root - 2019-11-06 18:50:16.999741: step 35850, total loss = 4.03, predict loss = 1.15 (52.0 examples/sec; 0.077 sec/batch; 2h:26m:21s remains)
INFO - root - 2019-11-06 18:50:17.759544: step 35860, total loss = 4.31, predict loss = 1.18 (54.3 examples/sec; 0.074 sec/batch; 2h:20m:05s remains)
INFO - root - 2019-11-06 18:50:18.326702: step 35870, total loss = 3.87, predict loss = 0.97 (105.0 examples/sec; 0.038 sec/batch; 1h:12m:27s remains)
INFO - root - 2019-11-06 18:50:18.780521: step 35880, total loss = 4.28, predict loss = 1.12 (96.7 examples/sec; 0.041 sec/batch; 1h:18m:42s remains)
INFO - root - 2019-11-06 18:50:19.219035: step 35890, total loss = 1.76, predict loss = 0.45 (90.8 examples/sec; 0.044 sec/batch; 1h:23m:47s remains)
INFO - root - 2019-11-06 18:50:20.580930: step 35900, total loss = 4.28, predict loss = 1.11 (57.2 examples/sec; 0.070 sec/batch; 2h:13m:02s remains)
INFO - root - 2019-11-06 18:50:21.332685: step 35910, total loss = 4.82, predict loss = 1.32 (57.8 examples/sec; 0.069 sec/batch; 2h:11m:34s remains)
INFO - root - 2019-11-06 18:50:22.078007: step 35920, total loss = 4.01, predict loss = 1.13 (57.0 examples/sec; 0.070 sec/batch; 2h:13m:26s remains)
INFO - root - 2019-11-06 18:50:22.928002: step 35930, total loss = 3.29, predict loss = 0.82 (51.1 examples/sec; 0.078 sec/batch; 2h:28m:50s remains)
INFO - root - 2019-11-06 18:50:23.633886: step 35940, total loss = 3.45, predict loss = 0.92 (77.7 examples/sec; 0.051 sec/batch; 1h:37m:48s remains)
INFO - root - 2019-11-06 18:50:24.098566: step 35950, total loss = 2.84, predict loss = 0.76 (100.0 examples/sec; 0.040 sec/batch; 1h:16m:01s remains)
INFO - root - 2019-11-06 18:50:24.551803: step 35960, total loss = 4.21, predict loss = 1.12 (93.0 examples/sec; 0.043 sec/batch; 1h:21m:45s remains)
INFO - root - 2019-11-06 18:50:25.767408: step 35970, total loss = 3.57, predict loss = 0.93 (61.0 examples/sec; 0.066 sec/batch; 2h:04m:43s remains)
INFO - root - 2019-11-06 18:50:26.541252: step 35980, total loss = 2.64, predict loss = 0.68 (61.2 examples/sec; 0.065 sec/batch; 2h:04m:15s remains)
INFO - root - 2019-11-06 18:50:27.322290: step 35990, total loss = 3.23, predict loss = 0.83 (56.8 examples/sec; 0.070 sec/batch; 2h:13m:55s remains)
INFO - root - 2019-11-06 18:50:28.107020: step 36000, total loss = 2.71, predict loss = 0.69 (57.5 examples/sec; 0.070 sec/batch; 2h:12m:17s remains)
INFO - root - 2019-11-06 18:50:28.860791: step 36010, total loss = 4.02, predict loss = 1.11 (55.4 examples/sec; 0.072 sec/batch; 2h:17m:03s remains)
INFO - root - 2019-11-06 18:50:29.469373: step 36020, total loss = 4.48, predict loss = 1.16 (99.5 examples/sec; 0.040 sec/batch; 1h:16m:22s remains)
INFO - root - 2019-11-06 18:50:29.926640: step 36030, total loss = 2.98, predict loss = 0.76 (95.1 examples/sec; 0.042 sec/batch; 1h:19m:52s remains)
INFO - root - 2019-11-06 18:50:30.379678: step 36040, total loss = 3.08, predict loss = 0.79 (127.0 examples/sec; 0.031 sec/batch; 0h:59m:49s remains)
INFO - root - 2019-11-06 18:50:31.767790: step 36050, total loss = 4.02, predict loss = 1.05 (62.2 examples/sec; 0.064 sec/batch; 2h:02m:02s remains)
INFO - root - 2019-11-06 18:50:32.485931: step 36060, total loss = 3.73, predict loss = 1.00 (62.1 examples/sec; 0.064 sec/batch; 2h:02m:16s remains)
INFO - root - 2019-11-06 18:50:33.236681: step 36070, total loss = 3.45, predict loss = 0.91 (59.3 examples/sec; 0.067 sec/batch; 2h:08m:08s remains)
INFO - root - 2019-11-06 18:50:33.982511: step 36080, total loss = 4.26, predict loss = 1.15 (64.1 examples/sec; 0.062 sec/batch; 1h:58m:28s remains)
INFO - root - 2019-11-06 18:50:34.625498: step 36090, total loss = 3.09, predict loss = 0.85 (76.2 examples/sec; 0.053 sec/batch; 1h:39m:40s remains)
INFO - root - 2019-11-06 18:50:35.108352: step 36100, total loss = 2.93, predict loss = 0.76 (96.0 examples/sec; 0.042 sec/batch; 1h:19m:05s remains)
INFO - root - 2019-11-06 18:50:35.560142: step 36110, total loss = 3.27, predict loss = 0.85 (99.1 examples/sec; 0.040 sec/batch; 1h:16m:37s remains)
INFO - root - 2019-11-06 18:50:36.815830: step 36120, total loss = 3.32, predict loss = 0.88 (63.4 examples/sec; 0.063 sec/batch; 1h:59m:46s remains)
INFO - root - 2019-11-06 18:50:37.576657: step 36130, total loss = 2.13, predict loss = 0.54 (60.4 examples/sec; 0.066 sec/batch; 2h:05m:38s remains)
INFO - root - 2019-11-06 18:50:38.348050: step 36140, total loss = 4.21, predict loss = 1.08 (59.3 examples/sec; 0.068 sec/batch; 2h:08m:06s remains)
INFO - root - 2019-11-06 18:50:39.120892: step 36150, total loss = 3.07, predict loss = 0.80 (59.2 examples/sec; 0.068 sec/batch; 2h:08m:07s remains)
INFO - root - 2019-11-06 18:50:39.903678: step 36160, total loss = 5.30, predict loss = 1.54 (69.1 examples/sec; 0.058 sec/batch; 1h:49m:45s remains)
INFO - root - 2019-11-06 18:50:40.479167: step 36170, total loss = 3.52, predict loss = 0.95 (96.9 examples/sec; 0.041 sec/batch; 1h:18m:19s remains)
INFO - root - 2019-11-06 18:50:40.959843: step 36180, total loss = 4.02, predict loss = 1.07 (100.6 examples/sec; 0.040 sec/batch; 1h:15m:26s remains)
INFO - root - 2019-11-06 18:50:42.092313: step 36190, total loss = 4.32, predict loss = 1.17 (5.4 examples/sec; 0.741 sec/batch; 23h:25m:09s remains)
INFO - root - 2019-11-06 18:50:42.768697: step 36200, total loss = 3.59, predict loss = 1.02 (55.8 examples/sec; 0.072 sec/batch; 2h:15m:51s remains)
INFO - root - 2019-11-06 18:50:43.475589: step 36210, total loss = 4.65, predict loss = 1.28 (70.9 examples/sec; 0.056 sec/batch; 1h:47m:03s remains)
INFO - root - 2019-11-06 18:50:44.315511: step 36220, total loss = 2.94, predict loss = 0.77 (50.6 examples/sec; 0.079 sec/batch; 2h:29m:54s remains)
INFO - root - 2019-11-06 18:50:45.140681: step 36230, total loss = 4.18, predict loss = 1.11 (58.5 examples/sec; 0.068 sec/batch; 2h:09m:41s remains)
INFO - root - 2019-11-06 18:50:45.811890: step 36240, total loss = 3.92, predict loss = 0.99 (93.8 examples/sec; 0.043 sec/batch; 1h:20m:48s remains)
INFO - root - 2019-11-06 18:50:46.246689: step 36250, total loss = 3.50, predict loss = 0.94 (103.9 examples/sec; 0.039 sec/batch; 1h:13m:00s remains)
INFO - root - 2019-11-06 18:50:46.744013: step 36260, total loss = 4.30, predict loss = 1.13 (90.6 examples/sec; 0.044 sec/batch; 1h:23m:44s remains)
INFO - root - 2019-11-06 18:50:47.990932: step 36270, total loss = 4.56, predict loss = 1.22 (61.7 examples/sec; 0.065 sec/batch; 2h:02m:48s remains)
INFO - root - 2019-11-06 18:50:48.725805: step 36280, total loss = 3.05, predict loss = 0.77 (59.2 examples/sec; 0.068 sec/batch; 2h:08m:07s remains)
INFO - root - 2019-11-06 18:50:49.457175: step 36290, total loss = 3.94, predict loss = 1.06 (54.7 examples/sec; 0.073 sec/batch; 2h:18m:41s remains)
INFO - root - 2019-11-06 18:50:50.227582: step 36300, total loss = 3.88, predict loss = 0.99 (52.9 examples/sec; 0.076 sec/batch; 2h:23m:10s remains)
INFO - root - 2019-11-06 18:50:50.997864: step 36310, total loss = 2.19, predict loss = 0.57 (56.1 examples/sec; 0.071 sec/batch; 2h:15m:05s remains)
INFO - root - 2019-11-06 18:50:51.534814: step 36320, total loss = 3.65, predict loss = 0.96 (97.6 examples/sec; 0.041 sec/batch; 1h:17m:36s remains)
INFO - root - 2019-11-06 18:50:51.990500: step 36330, total loss = 4.40, predict loss = 1.16 (89.3 examples/sec; 0.045 sec/batch; 1h:24m:53s remains)
INFO - root - 2019-11-06 18:50:53.166178: step 36340, total loss = 3.15, predict loss = 0.87 (70.0 examples/sec; 0.057 sec/batch; 1h:48m:15s remains)
INFO - root - 2019-11-06 18:50:53.859443: step 36350, total loss = 2.91, predict loss = 0.70 (57.5 examples/sec; 0.070 sec/batch; 2h:11m:43s remains)
INFO - root - 2019-11-06 18:50:54.623528: step 36360, total loss = 3.64, predict loss = 0.89 (55.3 examples/sec; 0.072 sec/batch; 2h:17m:06s remains)
INFO - root - 2019-11-06 18:50:55.431475: step 36370, total loss = 4.17, predict loss = 1.15 (53.7 examples/sec; 0.075 sec/batch; 2h:21m:06s remains)
INFO - root - 2019-11-06 18:50:56.200765: step 36380, total loss = 3.90, predict loss = 1.04 (57.5 examples/sec; 0.070 sec/batch; 2h:11m:37s remains)
INFO - root - 2019-11-06 18:50:56.825560: step 36390, total loss = 3.67, predict loss = 0.89 (100.1 examples/sec; 0.040 sec/batch; 1h:15m:41s remains)
INFO - root - 2019-11-06 18:50:57.296761: step 36400, total loss = 3.49, predict loss = 0.96 (94.3 examples/sec; 0.042 sec/batch; 1h:20m:21s remains)
INFO - root - 2019-11-06 18:50:57.744742: step 36410, total loss = 2.29, predict loss = 0.57 (101.4 examples/sec; 0.039 sec/batch; 1h:14m:39s remains)
INFO - root - 2019-11-06 18:50:59.038622: step 36420, total loss = 3.57, predict loss = 0.96 (61.5 examples/sec; 0.065 sec/batch; 2h:03m:06s remains)
INFO - root - 2019-11-06 18:50:59.775062: step 36430, total loss = 2.89, predict loss = 0.80 (57.9 examples/sec; 0.069 sec/batch; 2h:10m:43s remains)
INFO - root - 2019-11-06 18:51:00.567428: step 36440, total loss = 5.30, predict loss = 1.53 (56.3 examples/sec; 0.071 sec/batch; 2h:14m:33s remains)
INFO - root - 2019-11-06 18:51:01.321032: step 36450, total loss = 3.60, predict loss = 0.93 (60.8 examples/sec; 0.066 sec/batch; 2h:04m:30s remains)
INFO - root - 2019-11-06 18:51:02.057589: step 36460, total loss = 4.17, predict loss = 1.07 (66.4 examples/sec; 0.060 sec/batch; 1h:54m:03s remains)
INFO - root - 2019-11-06 18:51:02.593981: step 36470, total loss = 3.85, predict loss = 1.06 (95.8 examples/sec; 0.042 sec/batch; 1h:18m:59s remains)
INFO - root - 2019-11-06 18:51:03.054052: step 36480, total loss = 2.69, predict loss = 0.65 (99.3 examples/sec; 0.040 sec/batch; 1h:16m:11s remains)
INFO - root - 2019-11-06 18:51:04.209929: step 36490, total loss = 3.02, predict loss = 0.84 (64.7 examples/sec; 0.062 sec/batch; 1h:57m:01s remains)
INFO - root - 2019-11-06 18:51:04.956793: step 36500, total loss = 4.85, predict loss = 1.37 (54.9 examples/sec; 0.073 sec/batch; 2h:17m:51s remains)
INFO - root - 2019-11-06 18:51:05.743333: step 36510, total loss = 2.47, predict loss = 0.60 (53.7 examples/sec; 0.074 sec/batch; 2h:20m:53s remains)
INFO - root - 2019-11-06 18:51:06.500698: step 36520, total loss = 3.90, predict loss = 1.00 (55.7 examples/sec; 0.072 sec/batch; 2h:15m:52s remains)
INFO - root - 2019-11-06 18:51:07.316077: step 36530, total loss = 2.70, predict loss = 0.72 (52.3 examples/sec; 0.076 sec/batch; 2h:24m:40s remains)
INFO - root - 2019-11-06 18:51:08.000816: step 36540, total loss = 4.14, predict loss = 1.17 (92.9 examples/sec; 0.043 sec/batch; 1h:21m:23s remains)
INFO - root - 2019-11-06 18:51:08.474946: step 36550, total loss = 1.69, predict loss = 0.48 (91.9 examples/sec; 0.044 sec/batch; 1h:22m:20s remains)
INFO - root - 2019-11-06 18:51:08.951819: step 36560, total loss = 4.23, predict loss = 1.11 (87.4 examples/sec; 0.046 sec/batch; 1h:26m:34s remains)
INFO - root - 2019-11-06 18:51:10.265453: step 36570, total loss = 3.24, predict loss = 0.80 (57.2 examples/sec; 0.070 sec/batch; 2h:12m:07s remains)
INFO - root - 2019-11-06 18:51:11.014301: step 36580, total loss = 4.29, predict loss = 1.14 (58.5 examples/sec; 0.068 sec/batch; 2h:09m:10s remains)
INFO - root - 2019-11-06 18:51:11.760275: step 36590, total loss = 3.06, predict loss = 0.77 (62.2 examples/sec; 0.064 sec/batch; 2h:01m:33s remains)
INFO - root - 2019-11-06 18:51:12.493051: step 36600, total loss = 4.76, predict loss = 1.31 (56.3 examples/sec; 0.071 sec/batch; 2h:14m:22s remains)
INFO - root - 2019-11-06 18:51:13.267642: step 36610, total loss = 4.19, predict loss = 1.14 (59.0 examples/sec; 0.068 sec/batch; 2h:08m:08s remains)
INFO - root - 2019-11-06 18:51:13.843673: step 36620, total loss = 3.08, predict loss = 0.85 (92.9 examples/sec; 0.043 sec/batch; 1h:21m:22s remains)
INFO - root - 2019-11-06 18:51:14.315637: step 36630, total loss = 4.10, predict loss = 1.10 (89.4 examples/sec; 0.045 sec/batch; 1h:24m:33s remains)
INFO - root - 2019-11-06 18:51:15.547083: step 36640, total loss = 3.16, predict loss = 0.82 (54.3 examples/sec; 0.074 sec/batch; 2h:19m:07s remains)
INFO - root - 2019-11-06 18:51:16.236661: step 36650, total loss = 3.58, predict loss = 0.93 (58.9 examples/sec; 0.068 sec/batch; 2h:08m:11s remains)
INFO - root - 2019-11-06 18:51:17.010986: step 36660, total loss = 3.98, predict loss = 1.06 (65.1 examples/sec; 0.061 sec/batch; 1h:56m:06s remains)
INFO - root - 2019-11-06 18:51:17.677373: step 36670, total loss = 3.16, predict loss = 0.82 (65.4 examples/sec; 0.061 sec/batch; 1h:55m:35s remains)
INFO - root - 2019-11-06 18:51:18.360346: step 36680, total loss = 2.75, predict loss = 0.74 (68.0 examples/sec; 0.059 sec/batch; 1h:51m:05s remains)
INFO - root - 2019-11-06 18:51:18.908932: step 36690, total loss = 4.10, predict loss = 1.09 (99.0 examples/sec; 0.040 sec/batch; 1h:16m:16s remains)
INFO - root - 2019-11-06 18:51:19.401151: step 36700, total loss = 4.67, predict loss = 1.39 (93.5 examples/sec; 0.043 sec/batch; 1h:20m:46s remains)
INFO - root - 2019-11-06 18:51:19.873220: step 36710, total loss = 3.67, predict loss = 0.87 (88.6 examples/sec; 0.045 sec/batch; 1h:25m:11s remains)
INFO - root - 2019-11-06 18:51:21.223456: step 36720, total loss = 3.94, predict loss = 1.03 (62.5 examples/sec; 0.064 sec/batch; 2h:00m:44s remains)
INFO - root - 2019-11-06 18:51:21.985934: step 36730, total loss = 3.89, predict loss = 1.00 (57.1 examples/sec; 0.070 sec/batch; 2h:12m:18s remains)
INFO - root - 2019-11-06 18:51:22.753586: step 36740, total loss = 4.35, predict loss = 1.14 (56.7 examples/sec; 0.071 sec/batch; 2h:13m:15s remains)
INFO - root - 2019-11-06 18:51:23.551135: step 36750, total loss = 2.52, predict loss = 0.63 (60.4 examples/sec; 0.066 sec/batch; 2h:04m:59s remains)
INFO - root - 2019-11-06 18:51:24.261133: step 36760, total loss = 4.20, predict loss = 1.10 (64.0 examples/sec; 0.062 sec/batch; 1h:57m:56s remains)
INFO - root - 2019-11-06 18:51:24.772775: step 36770, total loss = 3.04, predict loss = 0.81 (100.9 examples/sec; 0.040 sec/batch; 1h:14m:48s remains)
INFO - root - 2019-11-06 18:51:25.251369: step 36780, total loss = 3.17, predict loss = 0.79 (94.8 examples/sec; 0.042 sec/batch; 1h:19m:35s remains)
INFO - root - 2019-11-06 18:51:26.419000: step 36790, total loss = 2.31, predict loss = 0.59 (69.9 examples/sec; 0.057 sec/batch; 1h:48m:02s remains)
INFO - root - 2019-11-06 18:51:27.184350: step 36800, total loss = 2.90, predict loss = 0.76 (56.4 examples/sec; 0.071 sec/batch; 2h:13m:41s remains)
INFO - root - 2019-11-06 18:51:27.988728: step 36810, total loss = 3.84, predict loss = 0.95 (62.3 examples/sec; 0.064 sec/batch; 2h:01m:11s remains)
INFO - root - 2019-11-06 18:51:28.768603: step 36820, total loss = 5.12, predict loss = 1.51 (55.3 examples/sec; 0.072 sec/batch; 2h:16m:31s remains)
INFO - root - 2019-11-06 18:51:29.544800: step 36830, total loss = 3.78, predict loss = 0.98 (57.5 examples/sec; 0.070 sec/batch; 2h:11m:12s remains)
INFO - root - 2019-11-06 18:51:30.129102: step 36840, total loss = 3.81, predict loss = 0.94 (100.4 examples/sec; 0.040 sec/batch; 1h:15m:10s remains)
INFO - root - 2019-11-06 18:51:30.572615: step 36850, total loss = 3.00, predict loss = 0.74 (102.3 examples/sec; 0.039 sec/batch; 1h:13m:45s remains)
INFO - root - 2019-11-06 18:51:31.042657: step 36860, total loss = 1.22, predict loss = 0.29 (118.0 examples/sec; 0.034 sec/batch; 1h:03m:54s remains)
INFO - root - 2019-11-06 18:51:32.377946: step 36870, total loss = 3.42, predict loss = 0.91 (65.7 examples/sec; 0.061 sec/batch; 1h:54m:51s remains)
INFO - root - 2019-11-06 18:51:33.109232: step 36880, total loss = 2.61, predict loss = 0.63 (57.1 examples/sec; 0.070 sec/batch; 2h:12m:00s remains)
INFO - root - 2019-11-06 18:51:33.853221: step 36890, total loss = 2.99, predict loss = 0.80 (61.0 examples/sec; 0.066 sec/batch; 2h:03m:38s remains)
INFO - root - 2019-11-06 18:51:34.594093: step 36900, total loss = 3.44, predict loss = 0.85 (61.0 examples/sec; 0.066 sec/batch; 2h:03m:30s remains)
INFO - root - 2019-11-06 18:51:35.226422: step 36910, total loss = 2.90, predict loss = 0.83 (89.8 examples/sec; 0.045 sec/batch; 1h:23m:58s remains)
INFO - root - 2019-11-06 18:51:35.670106: step 36920, total loss = 4.75, predict loss = 1.35 (93.7 examples/sec; 0.043 sec/batch; 1h:20m:29s remains)
INFO - root - 2019-11-06 18:51:36.142414: step 36930, total loss = 4.00, predict loss = 1.11 (97.2 examples/sec; 0.041 sec/batch; 1h:17m:34s remains)
INFO - root - 2019-11-06 18:51:37.438037: step 36940, total loss = 3.74, predict loss = 0.99 (59.6 examples/sec; 0.067 sec/batch; 2h:06m:31s remains)
INFO - root - 2019-11-06 18:51:38.233646: step 36950, total loss = 4.82, predict loss = 1.34 (58.4 examples/sec; 0.069 sec/batch; 2h:09m:08s remains)
INFO - root - 2019-11-06 18:51:39.041820: step 36960, total loss = 3.71, predict loss = 0.97 (55.9 examples/sec; 0.072 sec/batch; 2h:14m:46s remains)
INFO - root - 2019-11-06 18:51:39.774255: step 36970, total loss = 3.61, predict loss = 0.95 (60.2 examples/sec; 0.066 sec/batch; 2h:05m:13s remains)
INFO - root - 2019-11-06 18:51:40.499573: step 36980, total loss = 3.81, predict loss = 1.02 (68.0 examples/sec; 0.059 sec/batch; 1h:50m:46s remains)
INFO - root - 2019-11-06 18:51:41.076197: step 36990, total loss = 5.09, predict loss = 1.58 (102.4 examples/sec; 0.039 sec/batch; 1h:13m:34s remains)
INFO - root - 2019-11-06 18:51:41.525376: step 37000, total loss = 4.21, predict loss = 1.12 (90.7 examples/sec; 0.044 sec/batch; 1h:23m:00s remains)
INFO - root - 2019-11-06 18:51:42.651692: step 37010, total loss = 2.30, predict loss = 0.56 (5.5 examples/sec; 0.729 sec/batch; 22h:53m:34s remains)
INFO - root - 2019-11-06 18:51:43.374202: step 37020, total loss = 3.63, predict loss = 1.00 (62.3 examples/sec; 0.064 sec/batch; 2h:00m:59s remains)
INFO - root - 2019-11-06 18:51:44.125993: step 37030, total loss = 3.12, predict loss = 0.80 (58.8 examples/sec; 0.068 sec/batch; 2h:08m:10s remains)
INFO - root - 2019-11-06 18:51:44.874824: step 37040, total loss = 3.84, predict loss = 1.01 (50.9 examples/sec; 0.079 sec/batch; 2h:27m:49s remains)
INFO - root - 2019-11-06 18:51:45.610761: step 37050, total loss = 3.68, predict loss = 0.96 (56.9 examples/sec; 0.070 sec/batch; 2h:12m:18s remains)
INFO - root - 2019-11-06 18:51:46.275378: step 37060, total loss = 2.09, predict loss = 0.52 (90.8 examples/sec; 0.044 sec/batch; 1h:22m:57s remains)
INFO - root - 2019-11-06 18:51:46.740776: step 37070, total loss = 2.92, predict loss = 0.76 (93.8 examples/sec; 0.043 sec/batch; 1h:20m:17s remains)
INFO - root - 2019-11-06 18:51:47.206794: step 37080, total loss = 3.67, predict loss = 1.05 (96.4 examples/sec; 0.041 sec/batch; 1h:18m:04s remains)
INFO - root - 2019-11-06 18:51:48.469992: step 37090, total loss = 4.06, predict loss = 1.05 (64.1 examples/sec; 0.062 sec/batch; 1h:57m:27s remains)
INFO - root - 2019-11-06 18:51:49.232728: step 37100, total loss = 3.79, predict loss = 0.94 (60.5 examples/sec; 0.066 sec/batch; 2h:04m:26s remains)
INFO - root - 2019-11-06 18:51:50.025562: step 37110, total loss = 3.58, predict loss = 1.01 (52.0 examples/sec; 0.077 sec/batch; 2h:24m:47s remains)
INFO - root - 2019-11-06 18:51:50.757255: step 37120, total loss = 4.51, predict loss = 1.18 (58.3 examples/sec; 0.069 sec/batch; 2h:09m:09s remains)
INFO - root - 2019-11-06 18:51:51.549828: step 37130, total loss = 3.28, predict loss = 0.88 (65.1 examples/sec; 0.061 sec/batch; 1h:55m:31s remains)
INFO - root - 2019-11-06 18:51:52.119706: step 37140, total loss = 4.04, predict loss = 1.04 (94.7 examples/sec; 0.042 sec/batch; 1h:19m:26s remains)
INFO - root - 2019-11-06 18:51:52.569095: step 37150, total loss = 4.87, predict loss = 1.41 (102.5 examples/sec; 0.039 sec/batch; 1h:13m:22s remains)
INFO - root - 2019-11-06 18:51:53.676218: step 37160, total loss = 3.39, predict loss = 0.88 (69.7 examples/sec; 0.057 sec/batch; 1h:47m:57s remains)
INFO - root - 2019-11-06 18:51:54.356120: step 37170, total loss = 4.73, predict loss = 1.32 (60.7 examples/sec; 0.066 sec/batch; 2h:03m:52s remains)
INFO - root - 2019-11-06 18:51:55.077158: step 37180, total loss = 3.74, predict loss = 1.08 (69.5 examples/sec; 0.058 sec/batch; 1h:48m:16s remains)
INFO - root - 2019-11-06 18:51:55.791318: step 37190, total loss = 2.55, predict loss = 0.62 (59.2 examples/sec; 0.068 sec/batch; 2h:07m:01s remains)
INFO - root - 2019-11-06 18:51:56.513512: step 37200, total loss = 2.41, predict loss = 0.59 (61.4 examples/sec; 0.065 sec/batch; 2h:02m:31s remains)
INFO - root - 2019-11-06 18:51:57.167526: step 37210, total loss = 2.78, predict loss = 0.69 (89.7 examples/sec; 0.045 sec/batch; 1h:23m:48s remains)
INFO - root - 2019-11-06 18:51:57.653902: step 37220, total loss = 3.99, predict loss = 1.04 (94.8 examples/sec; 0.042 sec/batch; 1h:19m:16s remains)
INFO - root - 2019-11-06 18:51:58.118760: step 37230, total loss = 4.26, predict loss = 1.09 (90.6 examples/sec; 0.044 sec/batch; 1h:23m:01s remains)
INFO - root - 2019-11-06 18:51:59.353840: step 37240, total loss = 3.80, predict loss = 1.06 (65.2 examples/sec; 0.061 sec/batch; 1h:55m:16s remains)
INFO - root - 2019-11-06 18:52:00.035017: step 37250, total loss = 2.54, predict loss = 0.61 (62.5 examples/sec; 0.064 sec/batch; 2h:00m:15s remains)
INFO - root - 2019-11-06 18:52:00.782575: step 37260, total loss = 3.93, predict loss = 1.03 (59.9 examples/sec; 0.067 sec/batch; 2h:05m:27s remains)
INFO - root - 2019-11-06 18:52:01.534093: step 37270, total loss = 3.46, predict loss = 0.89 (54.2 examples/sec; 0.074 sec/batch; 2h:18m:38s remains)
INFO - root - 2019-11-06 18:52:02.268985: step 37280, total loss = 2.87, predict loss = 0.69 (66.1 examples/sec; 0.061 sec/batch; 1h:53m:44s remains)
INFO - root - 2019-11-06 18:52:02.792061: step 37290, total loss = 2.50, predict loss = 0.67 (97.8 examples/sec; 0.041 sec/batch; 1h:16m:50s remains)
INFO - root - 2019-11-06 18:52:03.279034: step 37300, total loss = 2.81, predict loss = 0.69 (89.6 examples/sec; 0.045 sec/batch; 1h:23m:51s remains)
INFO - root - 2019-11-06 18:52:04.440989: step 37310, total loss = 2.39, predict loss = 0.64 (68.7 examples/sec; 0.058 sec/batch; 1h:49m:16s remains)
INFO - root - 2019-11-06 18:52:05.163078: step 37320, total loss = 3.74, predict loss = 0.95 (55.9 examples/sec; 0.072 sec/batch; 2h:14m:19s remains)
INFO - root - 2019-11-06 18:52:05.958729: step 37330, total loss = 3.79, predict loss = 1.03 (58.0 examples/sec; 0.069 sec/batch; 2h:09m:32s remains)
INFO - root - 2019-11-06 18:52:06.794284: step 37340, total loss = 5.11, predict loss = 1.38 (49.7 examples/sec; 0.080 sec/batch; 2h:30m:59s remains)
INFO - root - 2019-11-06 18:52:07.619373: step 37350, total loss = 3.90, predict loss = 0.96 (53.7 examples/sec; 0.074 sec/batch; 2h:19m:48s remains)
INFO - root - 2019-11-06 18:52:08.273939: step 37360, total loss = 4.55, predict loss = 1.29 (94.3 examples/sec; 0.042 sec/batch; 1h:19m:39s remains)
INFO - root - 2019-11-06 18:52:08.728501: step 37370, total loss = 3.64, predict loss = 0.94 (96.1 examples/sec; 0.042 sec/batch; 1h:18m:10s remains)
INFO - root - 2019-11-06 18:52:09.211290: step 37380, total loss = 3.68, predict loss = 0.97 (94.2 examples/sec; 0.042 sec/batch; 1h:19m:44s remains)
INFO - root - 2019-11-06 18:52:10.487249: step 37390, total loss = 2.03, predict loss = 0.53 (64.3 examples/sec; 0.062 sec/batch; 1h:56m:48s remains)
INFO - root - 2019-11-06 18:52:11.221450: step 37400, total loss = 3.13, predict loss = 0.82 (62.0 examples/sec; 0.065 sec/batch; 2h:01m:07s remains)
INFO - root - 2019-11-06 18:52:11.938629: step 37410, total loss = 2.86, predict loss = 0.72 (58.0 examples/sec; 0.069 sec/batch; 2h:09m:27s remains)
INFO - root - 2019-11-06 18:52:12.759420: step 37420, total loss = 3.63, predict loss = 0.97 (59.9 examples/sec; 0.067 sec/batch; 2h:05m:19s remains)
INFO - root - 2019-11-06 18:52:13.509315: step 37430, total loss = 3.63, predict loss = 0.89 (62.7 examples/sec; 0.064 sec/batch; 1h:59m:44s remains)
INFO - root - 2019-11-06 18:52:14.042728: step 37440, total loss = 2.27, predict loss = 0.56 (96.4 examples/sec; 0.041 sec/batch; 1h:17m:48s remains)
INFO - root - 2019-11-06 18:52:14.508956: step 37450, total loss = 3.05, predict loss = 0.77 (93.6 examples/sec; 0.043 sec/batch; 1h:20m:09s remains)
INFO - root - 2019-11-06 18:52:15.697806: step 37460, total loss = 3.96, predict loss = 1.08 (69.4 examples/sec; 0.058 sec/batch; 1h:48m:09s remains)
INFO - root - 2019-11-06 18:52:16.471615: step 37470, total loss = 2.38, predict loss = 0.58 (53.5 examples/sec; 0.075 sec/batch; 2h:20m:18s remains)
INFO - root - 2019-11-06 18:52:17.222929: step 37480, total loss = 2.13, predict loss = 0.52 (54.7 examples/sec; 0.073 sec/batch; 2h:17m:04s remains)
INFO - root - 2019-11-06 18:52:18.019992: step 37490, total loss = 3.43, predict loss = 0.90 (49.3 examples/sec; 0.081 sec/batch; 2h:32m:02s remains)
INFO - root - 2019-11-06 18:52:18.802014: step 37500, total loss = 3.37, predict loss = 0.87 (55.8 examples/sec; 0.072 sec/batch; 2h:14m:25s remains)
INFO - root - 2019-11-06 18:52:19.434766: step 37510, total loss = 2.56, predict loss = 0.59 (97.3 examples/sec; 0.041 sec/batch; 1h:17m:04s remains)
INFO - root - 2019-11-06 18:52:19.901995: step 37520, total loss = 4.28, predict loss = 1.09 (88.5 examples/sec; 0.045 sec/batch; 1h:24m:42s remains)
INFO - root - 2019-11-06 18:52:20.378101: step 37530, total loss = 3.18, predict loss = 0.83 (92.7 examples/sec; 0.043 sec/batch; 1h:20m:52s remains)
INFO - root - 2019-11-06 18:52:21.754134: step 37540, total loss = 4.31, predict loss = 1.15 (63.0 examples/sec; 0.063 sec/batch; 1h:58m:54s remains)
INFO - root - 2019-11-06 18:52:22.486417: step 37550, total loss = 4.12, predict loss = 1.30 (58.9 examples/sec; 0.068 sec/batch; 2h:07m:14s remains)
INFO - root - 2019-11-06 18:52:23.237309: step 37560, total loss = 3.79, predict loss = 1.04 (62.4 examples/sec; 0.064 sec/batch; 2h:00m:10s remains)
INFO - root - 2019-11-06 18:52:23.979049: step 37570, total loss = 2.76, predict loss = 0.69 (59.1 examples/sec; 0.068 sec/batch; 2h:06m:47s remains)
INFO - root - 2019-11-06 18:52:24.713330: step 37580, total loss = 4.42, predict loss = 1.19 (71.5 examples/sec; 0.056 sec/batch; 1h:44m:48s remains)
INFO - root - 2019-11-06 18:52:25.219442: step 37590, total loss = 3.49, predict loss = 0.93 (89.6 examples/sec; 0.045 sec/batch; 1h:23m:38s remains)
INFO - root - 2019-11-06 18:52:25.677425: step 37600, total loss = 2.94, predict loss = 0.79 (93.8 examples/sec; 0.043 sec/batch; 1h:19m:51s remains)
INFO - root - 2019-11-06 18:52:26.862586: step 37610, total loss = 3.76, predict loss = 0.99 (63.0 examples/sec; 0.063 sec/batch; 1h:58m:50s remains)
INFO - root - 2019-11-06 18:52:27.685421: step 37620, total loss = 4.40, predict loss = 1.13 (51.7 examples/sec; 0.077 sec/batch; 2h:25m:02s remains)
INFO - root - 2019-11-06 18:52:28.444971: step 37630, total loss = 4.16, predict loss = 1.19 (53.3 examples/sec; 0.075 sec/batch; 2h:20m:29s remains)
INFO - root - 2019-11-06 18:52:29.292767: step 37640, total loss = 4.46, predict loss = 1.27 (51.4 examples/sec; 0.078 sec/batch; 2h:25m:47s remains)
INFO - root - 2019-11-06 18:52:30.097555: step 37650, total loss = 3.43, predict loss = 0.88 (57.6 examples/sec; 0.069 sec/batch; 2h:09m:59s remains)
INFO - root - 2019-11-06 18:52:30.669300: step 37660, total loss = 2.88, predict loss = 0.71 (100.5 examples/sec; 0.040 sec/batch; 1h:14m:33s remains)
INFO - root - 2019-11-06 18:52:31.128075: step 37670, total loss = 4.09, predict loss = 1.12 (96.2 examples/sec; 0.042 sec/batch; 1h:17m:50s remains)
INFO - root - 2019-11-06 18:52:31.587638: step 37680, total loss = 2.97, predict loss = 0.80 (94.8 examples/sec; 0.042 sec/batch; 1h:19m:00s remains)
INFO - root - 2019-11-06 18:52:32.942438: step 37690, total loss = 2.36, predict loss = 0.58 (62.0 examples/sec; 0.064 sec/batch; 2h:00m:43s remains)
INFO - root - 2019-11-06 18:52:33.700008: step 37700, total loss = 3.43, predict loss = 0.85 (54.0 examples/sec; 0.074 sec/batch; 2h:18m:35s remains)
INFO - root - 2019-11-06 18:52:34.490787: step 37710, total loss = 3.97, predict loss = 1.06 (55.4 examples/sec; 0.072 sec/batch; 2h:15m:09s remains)
INFO - root - 2019-11-06 18:52:35.242553: step 37720, total loss = 2.86, predict loss = 0.73 (52.7 examples/sec; 0.076 sec/batch; 2h:21m:58s remains)
INFO - root - 2019-11-06 18:52:35.916200: step 37730, total loss = 3.73, predict loss = 1.08 (79.3 examples/sec; 0.050 sec/batch; 1h:34m:26s remains)
INFO - root - 2019-11-06 18:52:36.408981: step 37740, total loss = 2.55, predict loss = 0.70 (95.1 examples/sec; 0.042 sec/batch; 1h:18m:40s remains)
INFO - root - 2019-11-06 18:52:36.877952: step 37750, total loss = 4.02, predict loss = 1.04 (94.4 examples/sec; 0.042 sec/batch; 1h:19m:18s remains)
INFO - root - 2019-11-06 18:52:38.056052: step 37760, total loss = 3.27, predict loss = 0.82 (66.7 examples/sec; 0.060 sec/batch; 1h:52m:08s remains)
INFO - root - 2019-11-06 18:52:38.790750: step 37770, total loss = 2.67, predict loss = 0.66 (58.1 examples/sec; 0.069 sec/batch; 2h:08m:45s remains)
INFO - root - 2019-11-06 18:52:39.555836: step 37780, total loss = 2.51, predict loss = 0.59 (53.8 examples/sec; 0.074 sec/batch; 2h:19m:01s remains)
INFO - root - 2019-11-06 18:52:40.312396: step 37790, total loss = 3.40, predict loss = 1.00 (62.7 examples/sec; 0.064 sec/batch; 1h:59m:19s remains)
INFO - root - 2019-11-06 18:52:41.059861: step 37800, total loss = 3.48, predict loss = 0.89 (60.5 examples/sec; 0.066 sec/batch; 2h:03m:36s remains)
INFO - root - 2019-11-06 18:52:41.601879: step 37810, total loss = 2.29, predict loss = 0.55 (93.0 examples/sec; 0.043 sec/batch; 1h:20m:27s remains)
INFO - root - 2019-11-06 18:52:42.094879: step 37820, total loss = 3.27, predict loss = 0.88 (88.1 examples/sec; 0.045 sec/batch; 1h:24m:53s remains)
INFO - root - 2019-11-06 18:52:43.236639: step 37830, total loss = 5.45, predict loss = 1.54 (5.4 examples/sec; 0.737 sec/batch; 22h:56m:53s remains)
INFO - root - 2019-11-06 18:52:43.958821: step 37840, total loss = 3.05, predict loss = 0.82 (56.4 examples/sec; 0.071 sec/batch; 2h:12m:38s remains)
INFO - root - 2019-11-06 18:52:44.684812: step 37850, total loss = 3.76, predict loss = 1.01 (52.6 examples/sec; 0.076 sec/batch; 2h:22m:02s remains)
INFO - root - 2019-11-06 18:52:45.459793: step 37860, total loss = 2.98, predict loss = 0.74 (56.0 examples/sec; 0.071 sec/batch; 2h:13m:27s remains)
INFO - root - 2019-11-06 18:52:46.188004: step 37870, total loss = 3.79, predict loss = 0.99 (55.1 examples/sec; 0.073 sec/batch; 2h:15m:46s remains)
INFO - root - 2019-11-06 18:52:46.834462: step 37880, total loss = 2.16, predict loss = 0.55 (92.3 examples/sec; 0.043 sec/batch; 1h:21m:01s remains)
INFO - root - 2019-11-06 18:52:47.279786: step 37890, total loss = 3.49, predict loss = 0.83 (93.9 examples/sec; 0.043 sec/batch; 1h:19m:37s remains)
INFO - root - 2019-11-06 18:52:47.769066: step 37900, total loss = 3.94, predict loss = 1.08 (93.6 examples/sec; 0.043 sec/batch; 1h:19m:48s remains)
INFO - root - 2019-11-06 18:52:49.010597: step 37910, total loss = 4.43, predict loss = 1.22 (56.5 examples/sec; 0.071 sec/batch; 2h:12m:21s remains)
INFO - root - 2019-11-06 18:52:49.747719: step 37920, total loss = 2.20, predict loss = 0.53 (57.6 examples/sec; 0.069 sec/batch; 2h:09m:41s remains)
INFO - root - 2019-11-06 18:52:50.523580: step 37930, total loss = 2.79, predict loss = 0.71 (60.9 examples/sec; 0.066 sec/batch; 2h:02m:44s remains)
INFO - root - 2019-11-06 18:52:51.336400: step 37940, total loss = 2.62, predict loss = 0.67 (52.7 examples/sec; 0.076 sec/batch; 2h:21m:51s remains)
INFO - root - 2019-11-06 18:52:52.043046: step 37950, total loss = 4.05, predict loss = 1.11 (79.2 examples/sec; 0.050 sec/batch; 1h:34m:16s remains)
INFO - root - 2019-11-06 18:52:52.579261: step 37960, total loss = 1.70, predict loss = 0.43 (97.4 examples/sec; 0.041 sec/batch; 1h:16m:40s remains)
INFO - root - 2019-11-06 18:52:53.036699: step 37970, total loss = 3.09, predict loss = 0.75 (92.4 examples/sec; 0.043 sec/batch; 1h:20m:48s remains)
INFO - root - 2019-11-06 18:52:54.217933: step 37980, total loss = 3.20, predict loss = 0.86 (66.0 examples/sec; 0.061 sec/batch; 1h:53m:09s remains)
INFO - root - 2019-11-06 18:52:54.905701: step 37990, total loss = 3.93, predict loss = 1.08 (66.9 examples/sec; 0.060 sec/batch; 1h:51m:41s remains)
INFO - root - 2019-11-06 18:52:55.621678: step 38000, total loss = 3.65, predict loss = 0.93 (69.8 examples/sec; 0.057 sec/batch; 1h:46m:57s remains)
INFO - root - 2019-11-06 18:52:56.355426: step 38010, total loss = 2.83, predict loss = 0.78 (49.9 examples/sec; 0.080 sec/batch; 2h:29m:33s remains)
INFO - root - 2019-11-06 18:52:57.074907: step 38020, total loss = 1.95, predict loss = 0.48 (60.2 examples/sec; 0.066 sec/batch; 2h:04m:02s remains)
INFO - root - 2019-11-06 18:52:57.764056: step 38030, total loss = 3.70, predict loss = 0.97 (92.7 examples/sec; 0.043 sec/batch; 1h:20m:30s remains)
INFO - root - 2019-11-06 18:52:58.211586: step 38040, total loss = 2.44, predict loss = 0.61 (96.0 examples/sec; 0.042 sec/batch; 1h:17m:45s remains)
INFO - root - 2019-11-06 18:52:58.673236: step 38050, total loss = 2.74, predict loss = 0.63 (98.1 examples/sec; 0.041 sec/batch; 1h:16m:06s remains)
INFO - root - 2019-11-06 18:52:59.954869: step 38060, total loss = 3.08, predict loss = 0.84 (55.6 examples/sec; 0.072 sec/batch; 2h:14m:06s remains)
INFO - root - 2019-11-06 18:53:00.711701: step 38070, total loss = 4.53, predict loss = 1.19 (54.3 examples/sec; 0.074 sec/batch; 2h:17m:18s remains)
INFO - root - 2019-11-06 18:53:01.424754: step 38080, total loss = 2.48, predict loss = 0.61 (57.7 examples/sec; 0.069 sec/batch; 2h:09m:14s remains)
INFO - root - 2019-11-06 18:53:02.180248: step 38090, total loss = 2.00, predict loss = 0.54 (53.5 examples/sec; 0.075 sec/batch; 2h:19m:33s remains)
INFO - root - 2019-11-06 18:53:02.931011: step 38100, total loss = 2.88, predict loss = 0.74 (74.6 examples/sec; 0.054 sec/batch; 1h:39m:59s remains)
INFO - root - 2019-11-06 18:53:03.436788: step 38110, total loss = 1.79, predict loss = 0.46 (95.5 examples/sec; 0.042 sec/batch; 1h:18m:08s remains)
INFO - root - 2019-11-06 18:53:03.894067: step 38120, total loss = 4.11, predict loss = 1.12 (99.9 examples/sec; 0.040 sec/batch; 1h:14m:39s remains)
INFO - root - 2019-11-06 18:53:05.050383: step 38130, total loss = 4.47, predict loss = 1.22 (67.0 examples/sec; 0.060 sec/batch; 1h:51m:18s remains)
INFO - root - 2019-11-06 18:53:05.768441: step 38140, total loss = 3.95, predict loss = 1.07 (61.7 examples/sec; 0.065 sec/batch; 2h:00m:53s remains)
INFO - root - 2019-11-06 18:53:06.554964: step 38150, total loss = 3.02, predict loss = 0.77 (57.9 examples/sec; 0.069 sec/batch; 2h:08m:51s remains)
INFO - root - 2019-11-06 18:53:07.329784: step 38160, total loss = 4.04, predict loss = 1.10 (53.9 examples/sec; 0.074 sec/batch; 2h:18m:25s remains)
INFO - root - 2019-11-06 18:53:08.095916: step 38170, total loss = 2.74, predict loss = 0.65 (57.8 examples/sec; 0.069 sec/batch; 2h:08m:54s remains)
INFO - root - 2019-11-06 18:53:08.726885: step 38180, total loss = 4.68, predict loss = 1.30 (93.7 examples/sec; 0.043 sec/batch; 1h:19m:34s remains)
INFO - root - 2019-11-06 18:53:09.202656: step 38190, total loss = 4.74, predict loss = 1.36 (91.9 examples/sec; 0.044 sec/batch; 1h:21m:06s remains)
INFO - root - 2019-11-06 18:53:09.673767: step 38200, total loss = 3.76, predict loss = 0.96 (92.5 examples/sec; 0.043 sec/batch; 1h:20m:34s remains)
INFO - root - 2019-11-06 18:53:10.969578: step 38210, total loss = 1.90, predict loss = 0.47 (56.6 examples/sec; 0.071 sec/batch; 2h:11m:36s remains)
INFO - root - 2019-11-06 18:53:11.700271: step 38220, total loss = 2.97, predict loss = 0.76 (59.5 examples/sec; 0.067 sec/batch; 2h:05m:18s remains)
INFO - root - 2019-11-06 18:53:12.475096: step 38230, total loss = 4.68, predict loss = 1.29 (47.9 examples/sec; 0.084 sec/batch; 2h:35m:38s remains)
INFO - root - 2019-11-06 18:53:13.221666: step 38240, total loss = 3.31, predict loss = 0.93 (57.2 examples/sec; 0.070 sec/batch; 2h:10m:17s remains)
INFO - root - 2019-11-06 18:53:13.941176: step 38250, total loss = 4.05, predict loss = 1.22 (66.9 examples/sec; 0.060 sec/batch; 1h:51m:25s remains)
INFO - root - 2019-11-06 18:53:14.501217: step 38260, total loss = 3.21, predict loss = 0.87 (100.1 examples/sec; 0.040 sec/batch; 1h:14m:25s remains)
INFO - root - 2019-11-06 18:53:14.963853: step 38270, total loss = 3.91, predict loss = 1.10 (92.2 examples/sec; 0.043 sec/batch; 1h:20m:45s remains)
INFO - root - 2019-11-06 18:53:16.139059: step 38280, total loss = 2.15, predict loss = 0.51 (67.6 examples/sec; 0.059 sec/batch; 1h:50m:13s remains)
INFO - root - 2019-11-06 18:53:16.862036: step 38290, total loss = 4.57, predict loss = 1.21 (53.4 examples/sec; 0.075 sec/batch; 2h:19m:31s remains)
INFO - root - 2019-11-06 18:53:17.608557: step 38300, total loss = 3.73, predict loss = 0.95 (61.7 examples/sec; 0.065 sec/batch; 2h:00m:35s remains)
INFO - root - 2019-11-06 18:53:18.341981: step 38310, total loss = 2.84, predict loss = 0.71 (56.4 examples/sec; 0.071 sec/batch; 2h:12m:06s remains)
INFO - root - 2019-11-06 18:53:19.129873: step 38320, total loss = 3.20, predict loss = 0.83 (57.7 examples/sec; 0.069 sec/batch; 2h:09m:08s remains)
INFO - root - 2019-11-06 18:53:19.717594: step 38330, total loss = 3.01, predict loss = 0.80 (100.0 examples/sec; 0.040 sec/batch; 1h:14m:26s remains)
INFO - root - 2019-11-06 18:53:20.202043: step 38340, total loss = 3.86, predict loss = 1.06 (93.9 examples/sec; 0.043 sec/batch; 1h:19m:17s remains)
INFO - root - 2019-11-06 18:53:20.662190: step 38350, total loss = 2.84, predict loss = 0.72 (96.5 examples/sec; 0.041 sec/batch; 1h:17m:10s remains)
INFO - root - 2019-11-06 18:53:21.982977: step 38360, total loss = 3.33, predict loss = 0.88 (53.0 examples/sec; 0.075 sec/batch; 2h:20m:21s remains)
INFO - root - 2019-11-06 18:53:22.758837: step 38370, total loss = 4.40, predict loss = 1.34 (57.9 examples/sec; 0.069 sec/batch; 2h:08m:35s remains)
INFO - root - 2019-11-06 18:53:23.495185: step 38380, total loss = 2.73, predict loss = 0.74 (61.5 examples/sec; 0.065 sec/batch; 2h:00m:58s remains)
INFO - root - 2019-11-06 18:53:24.231410: step 38390, total loss = 4.54, predict loss = 1.28 (65.6 examples/sec; 0.061 sec/batch; 1h:53m:20s remains)
INFO - root - 2019-11-06 18:53:24.885841: step 38400, total loss = 1.79, predict loss = 0.43 (77.6 examples/sec; 0.052 sec/batch; 1h:35m:50s remains)
INFO - root - 2019-11-06 18:53:25.332607: step 38410, total loss = 3.97, predict loss = 1.03 (98.9 examples/sec; 0.040 sec/batch; 1h:15m:15s remains)
INFO - root - 2019-11-06 18:53:25.813704: step 38420, total loss = 3.82, predict loss = 1.04 (94.2 examples/sec; 0.042 sec/batch; 1h:19m:00s remains)
INFO - root - 2019-11-06 18:53:26.953190: step 38430, total loss = 1.50, predict loss = 0.38 (70.8 examples/sec; 0.056 sec/batch; 1h:45m:03s remains)
INFO - root - 2019-11-06 18:53:27.663409: step 38440, total loss = 2.75, predict loss = 0.65 (51.3 examples/sec; 0.078 sec/batch; 2h:25m:04s remains)
INFO - root - 2019-11-06 18:53:28.390752: step 38450, total loss = 4.18, predict loss = 1.15 (60.8 examples/sec; 0.066 sec/batch; 2h:02m:19s remains)
INFO - root - 2019-11-06 18:53:29.131547: step 38460, total loss = 4.33, predict loss = 1.16 (62.6 examples/sec; 0.064 sec/batch; 1h:58m:47s remains)
INFO - root - 2019-11-06 18:53:29.897260: step 38470, total loss = 2.85, predict loss = 0.73 (58.3 examples/sec; 0.069 sec/batch; 2h:07m:33s remains)
INFO - root - 2019-11-06 18:53:30.471482: step 38480, total loss = 2.87, predict loss = 0.69 (96.8 examples/sec; 0.041 sec/batch; 1h:16m:46s remains)
INFO - root - 2019-11-06 18:53:30.924206: step 38490, total loss = 2.98, predict loss = 0.72 (89.3 examples/sec; 0.045 sec/batch; 1h:23m:14s remains)
INFO - root - 2019-11-06 18:53:31.394686: step 38500, total loss = 3.66, predict loss = 0.95 (128.2 examples/sec; 0.031 sec/batch; 0h:58m:00s remains)
INFO - root - 2019-11-06 18:53:32.788891: step 38510, total loss = 3.44, predict loss = 0.89 (61.2 examples/sec; 0.065 sec/batch; 2h:01m:22s remains)
INFO - root - 2019-11-06 18:53:33.546780: step 38520, total loss = 2.91, predict loss = 0.72 (59.1 examples/sec; 0.068 sec/batch; 2h:05m:49s remains)
INFO - root - 2019-11-06 18:53:34.284979: step 38530, total loss = 3.37, predict loss = 0.89 (56.8 examples/sec; 0.070 sec/batch; 2h:10m:44s remains)
INFO - root - 2019-11-06 18:53:35.058227: step 38540, total loss = 2.54, predict loss = 0.63 (60.6 examples/sec; 0.066 sec/batch; 2h:02m:37s remains)
INFO - root - 2019-11-06 18:53:35.762488: step 38550, total loss = 3.74, predict loss = 1.01 (68.3 examples/sec; 0.059 sec/batch; 1h:48m:43s remains)
INFO - root - 2019-11-06 18:53:36.222916: step 38560, total loss = 4.07, predict loss = 1.04 (91.8 examples/sec; 0.044 sec/batch; 1h:20m:54s remains)
INFO - root - 2019-11-06 18:53:36.681472: step 38570, total loss = 3.23, predict loss = 0.82 (91.0 examples/sec; 0.044 sec/batch; 1h:21m:40s remains)
INFO - root - 2019-11-06 18:53:37.915452: step 38580, total loss = 4.67, predict loss = 1.27 (60.5 examples/sec; 0.066 sec/batch; 2h:02m:48s remains)
INFO - root - 2019-11-06 18:53:38.635254: step 38590, total loss = 1.91, predict loss = 0.47 (61.8 examples/sec; 0.065 sec/batch; 2h:00m:15s remains)
INFO - root - 2019-11-06 18:53:39.441760: step 38600, total loss = 2.96, predict loss = 0.80 (63.0 examples/sec; 0.064 sec/batch; 1h:57m:56s remains)
INFO - root - 2019-11-06 18:53:40.214677: step 38610, total loss = 4.53, predict loss = 1.29 (49.2 examples/sec; 0.081 sec/batch; 2h:30m:49s remains)
INFO - root - 2019-11-06 18:53:40.936974: step 38620, total loss = 4.44, predict loss = 1.24 (64.9 examples/sec; 0.062 sec/batch; 1h:54m:23s remains)
INFO - root - 2019-11-06 18:53:41.510256: step 38630, total loss = 3.42, predict loss = 0.87 (96.6 examples/sec; 0.041 sec/batch; 1h:16m:52s remains)
INFO - root - 2019-11-06 18:53:41.990594: step 38640, total loss = 3.05, predict loss = 0.80 (95.2 examples/sec; 0.042 sec/batch; 1h:18m:00s remains)
INFO - root - 2019-11-06 18:53:43.117887: step 38650, total loss = 2.98, predict loss = 0.82 (5.5 examples/sec; 0.722 sec/batch; 22h:20m:32s remains)
INFO - root - 2019-11-06 18:53:43.834412: step 38660, total loss = 3.22, predict loss = 0.81 (58.0 examples/sec; 0.069 sec/batch; 2h:08m:03s remains)
INFO - root - 2019-11-06 18:53:44.565800: step 38670, total loss = 4.11, predict loss = 1.09 (61.7 examples/sec; 0.065 sec/batch; 2h:00m:15s remains)
INFO - root - 2019-11-06 18:53:45.369920: step 38680, total loss = 2.84, predict loss = 0.73 (61.2 examples/sec; 0.065 sec/batch; 2h:01m:18s remains)
INFO - root - 2019-11-06 18:53:46.151323: step 38690, total loss = 2.42, predict loss = 0.64 (52.4 examples/sec; 0.076 sec/batch; 2h:21m:39s remains)
INFO - root - 2019-11-06 18:53:46.866068: step 38700, total loss = 2.51, predict loss = 0.67 (83.4 examples/sec; 0.048 sec/batch; 1h:28m:57s remains)
INFO - root - 2019-11-06 18:53:47.321800: step 38710, total loss = 3.21, predict loss = 0.86 (96.0 examples/sec; 0.042 sec/batch; 1h:17m:16s remains)
INFO - root - 2019-11-06 18:53:47.789336: step 38720, total loss = 2.20, predict loss = 0.58 (86.1 examples/sec; 0.046 sec/batch; 1h:26m:07s remains)
INFO - root - 2019-11-06 18:53:49.050399: step 38730, total loss = 2.84, predict loss = 0.71 (63.9 examples/sec; 0.063 sec/batch; 1h:56m:10s remains)
INFO - root - 2019-11-06 18:53:49.820146: step 38740, total loss = 3.94, predict loss = 1.00 (56.3 examples/sec; 0.071 sec/batch; 2h:11m:42s remains)
INFO - root - 2019-11-06 18:53:50.637864: step 38750, total loss = 3.12, predict loss = 0.76 (54.8 examples/sec; 0.073 sec/batch; 2h:15m:13s remains)
INFO - root - 2019-11-06 18:53:51.392425: step 38760, total loss = 2.48, predict loss = 0.58 (56.3 examples/sec; 0.071 sec/batch; 2h:11m:37s remains)
INFO - root - 2019-11-06 18:53:52.110535: step 38770, total loss = 3.42, predict loss = 0.82 (62.5 examples/sec; 0.064 sec/batch; 1h:58m:37s remains)
INFO - root - 2019-11-06 18:53:52.659625: step 38780, total loss = 3.70, predict loss = 0.97 (91.9 examples/sec; 0.044 sec/batch; 1h:20m:39s remains)
INFO - root - 2019-11-06 18:53:53.106394: step 38790, total loss = 3.69, predict loss = 0.96 (106.8 examples/sec; 0.037 sec/batch; 1h:09m:26s remains)
INFO - root - 2019-11-06 18:53:54.254030: step 38800, total loss = 2.21, predict loss = 0.53 (66.6 examples/sec; 0.060 sec/batch; 1h:51m:18s remains)
INFO - root - 2019-11-06 18:53:54.972822: step 38810, total loss = 3.94, predict loss = 1.10 (57.8 examples/sec; 0.069 sec/batch; 2h:08m:10s remains)
INFO - root - 2019-11-06 18:53:55.739245: step 38820, total loss = 3.49, predict loss = 0.96 (54.3 examples/sec; 0.074 sec/batch; 2h:16m:33s remains)
INFO - root - 2019-11-06 18:53:56.497455: step 38830, total loss = 3.35, predict loss = 0.85 (56.2 examples/sec; 0.071 sec/batch; 2h:11m:47s remains)
INFO - root - 2019-11-06 18:53:57.223181: step 38840, total loss = 2.13, predict loss = 0.49 (66.8 examples/sec; 0.060 sec/batch; 1h:50m:58s remains)
INFO - root - 2019-11-06 18:53:57.822104: step 38850, total loss = 3.35, predict loss = 0.85 (98.9 examples/sec; 0.040 sec/batch; 1h:14m:56s remains)
INFO - root - 2019-11-06 18:53:58.282103: step 38860, total loss = 3.84, predict loss = 1.04 (93.7 examples/sec; 0.043 sec/batch; 1h:19m:05s remains)
INFO - root - 2019-11-06 18:53:58.731190: step 38870, total loss = 2.24, predict loss = 0.54 (96.3 examples/sec; 0.042 sec/batch; 1h:16m:57s remains)
INFO - root - 2019-11-06 18:53:59.980295: step 38880, total loss = 2.83, predict loss = 0.75 (55.5 examples/sec; 0.072 sec/batch; 2h:13m:26s remains)
INFO - root - 2019-11-06 18:54:00.818459: step 38890, total loss = 2.09, predict loss = 0.49 (52.3 examples/sec; 0.076 sec/batch; 2h:21m:30s remains)
INFO - root - 2019-11-06 18:54:01.579475: step 38900, total loss = 2.02, predict loss = 0.50 (65.3 examples/sec; 0.061 sec/batch; 1h:53m:25s remains)
INFO - root - 2019-11-06 18:54:02.348944: step 38910, total loss = 2.83, predict loss = 0.71 (55.5 examples/sec; 0.072 sec/batch; 2h:13m:32s remains)
INFO - root - 2019-11-06 18:54:03.107840: step 38920, total loss = 2.39, predict loss = 0.62 (65.0 examples/sec; 0.062 sec/batch; 1h:53m:52s remains)
INFO - root - 2019-11-06 18:54:03.611674: step 38930, total loss = 3.06, predict loss = 0.74 (97.9 examples/sec; 0.041 sec/batch; 1h:15m:40s remains)
INFO - root - 2019-11-06 18:54:04.098512: step 38940, total loss = 2.75, predict loss = 0.69 (96.5 examples/sec; 0.041 sec/batch; 1h:16m:44s remains)
INFO - root - 2019-11-06 18:54:05.255235: step 38950, total loss = 1.99, predict loss = 0.51 (72.4 examples/sec; 0.055 sec/batch; 1h:42m:16s remains)
INFO - root - 2019-11-06 18:54:05.951110: step 38960, total loss = 3.83, predict loss = 1.07 (67.5 examples/sec; 0.059 sec/batch; 1h:49m:39s remains)
INFO - root - 2019-11-06 18:54:06.687419: step 38970, total loss = 4.09, predict loss = 1.10 (56.2 examples/sec; 0.071 sec/batch; 2h:11m:39s remains)
INFO - root - 2019-11-06 18:54:07.440573: step 38980, total loss = 3.84, predict loss = 1.03 (53.8 examples/sec; 0.074 sec/batch; 2h:17m:41s remains)
INFO - root - 2019-11-06 18:54:08.150749: step 38990, total loss = 1.78, predict loss = 0.52 (65.2 examples/sec; 0.061 sec/batch; 1h:53m:27s remains)
INFO - root - 2019-11-06 18:54:08.765545: step 39000, total loss = 3.45, predict loss = 0.90 (98.8 examples/sec; 0.041 sec/batch; 1h:14m:56s remains)
INFO - root - 2019-11-06 18:54:09.228316: step 39010, total loss = 3.32, predict loss = 0.81 (93.7 examples/sec; 0.043 sec/batch; 1h:18m:58s remains)
INFO - root - 2019-11-06 18:54:09.697573: step 39020, total loss = 2.78, predict loss = 0.69 (99.4 examples/sec; 0.040 sec/batch; 1h:14m:23s remains)
INFO - root - 2019-11-06 18:54:10.940952: step 39030, total loss = 2.73, predict loss = 0.71 (67.9 examples/sec; 0.059 sec/batch; 1h:48m:53s remains)
INFO - root - 2019-11-06 18:54:11.693345: step 39040, total loss = 2.01, predict loss = 0.47 (55.1 examples/sec; 0.073 sec/batch; 2h:14m:15s remains)
INFO - root - 2019-11-06 18:54:12.406209: step 39050, total loss = 3.62, predict loss = 0.93 (62.3 examples/sec; 0.064 sec/batch; 1h:58m:45s remains)
INFO - root - 2019-11-06 18:54:13.166885: step 39060, total loss = 3.20, predict loss = 0.80 (63.5 examples/sec; 0.063 sec/batch; 1h:56m:24s remains)
INFO - root - 2019-11-06 18:54:13.925379: step 39070, total loss = 3.47, predict loss = 0.82 (61.2 examples/sec; 0.065 sec/batch; 2h:00m:54s remains)
INFO - root - 2019-11-06 18:54:14.446069: step 39080, total loss = 3.80, predict loss = 0.97 (91.0 examples/sec; 0.044 sec/batch; 1h:21m:13s remains)
INFO - root - 2019-11-06 18:54:14.921771: step 39090, total loss = 3.33, predict loss = 0.90 (95.0 examples/sec; 0.042 sec/batch; 1h:17m:51s remains)
INFO - root - 2019-11-06 18:54:16.121473: step 39100, total loss = 2.84, predict loss = 0.73 (71.5 examples/sec; 0.056 sec/batch; 1h:43m:26s remains)
INFO - root - 2019-11-06 18:54:16.826146: step 39110, total loss = 2.84, predict loss = 0.75 (61.0 examples/sec; 0.066 sec/batch; 2h:01m:05s remains)
INFO - root - 2019-11-06 18:54:17.563777: step 39120, total loss = 3.86, predict loss = 0.99 (61.9 examples/sec; 0.065 sec/batch; 1h:59m:25s remains)
INFO - root - 2019-11-06 18:54:18.297074: step 39130, total loss = 3.85, predict loss = 1.05 (59.2 examples/sec; 0.068 sec/batch; 2h:04m:48s remains)
INFO - root - 2019-11-06 18:54:19.091761: step 39140, total loss = 2.68, predict loss = 0.65 (50.8 examples/sec; 0.079 sec/batch; 2h:25m:35s remains)
INFO - root - 2019-11-06 18:54:19.671987: step 39150, total loss = 4.18, predict loss = 1.10 (101.8 examples/sec; 0.039 sec/batch; 1h:12m:33s remains)
INFO - root - 2019-11-06 18:54:20.134634: step 39160, total loss = 3.02, predict loss = 0.78 (87.7 examples/sec; 0.046 sec/batch; 1h:24m:16s remains)
INFO - root - 2019-11-06 18:54:20.594222: step 39170, total loss = 3.16, predict loss = 0.72 (97.6 examples/sec; 0.041 sec/batch; 1h:15m:41s remains)
INFO - root - 2019-11-06 18:54:21.939074: step 39180, total loss = 3.38, predict loss = 0.88 (58.6 examples/sec; 0.068 sec/batch; 2h:06m:07s remains)
INFO - root - 2019-11-06 18:54:22.725495: step 39190, total loss = 3.57, predict loss = 0.90 (50.5 examples/sec; 0.079 sec/batch; 2h:26m:19s remains)
INFO - root - 2019-11-06 18:54:23.532652: step 39200, total loss = 3.43, predict loss = 0.89 (52.4 examples/sec; 0.076 sec/batch; 2h:20m:52s remains)
INFO - root - 2019-11-06 18:54:24.296442: step 39210, total loss = 3.21, predict loss = 0.90 (64.6 examples/sec; 0.062 sec/batch; 1h:54m:15s remains)
INFO - root - 2019-11-06 18:54:24.981713: step 39220, total loss = 2.44, predict loss = 0.65 (80.6 examples/sec; 0.050 sec/batch; 1h:31m:36s remains)
INFO - root - 2019-11-06 18:54:25.436733: step 39230, total loss = 3.98, predict loss = 1.10 (102.5 examples/sec; 0.039 sec/batch; 1h:12m:01s remains)
INFO - root - 2019-11-06 18:54:25.885988: step 39240, total loss = 2.02, predict loss = 0.54 (93.5 examples/sec; 0.043 sec/batch; 1h:18m:59s remains)
INFO - root - 2019-11-06 18:54:27.082694: step 39250, total loss = 2.33, predict loss = 0.59 (66.3 examples/sec; 0.060 sec/batch; 1h:51m:21s remains)
INFO - root - 2019-11-06 18:54:27.830147: step 39260, total loss = 2.66, predict loss = 0.69 (53.2 examples/sec; 0.075 sec/batch; 2h:18m:45s remains)
INFO - root - 2019-11-06 18:54:28.645930: step 39270, total loss = 4.16, predict loss = 1.23 (52.9 examples/sec; 0.076 sec/batch; 2h:19m:29s remains)
INFO - root - 2019-11-06 18:54:29.397918: step 39280, total loss = 2.85, predict loss = 0.72 (63.2 examples/sec; 0.063 sec/batch; 1h:56m:49s remains)
INFO - root - 2019-11-06 18:54:30.220565: step 39290, total loss = 3.30, predict loss = 0.86 (57.9 examples/sec; 0.069 sec/batch; 2h:07m:23s remains)
INFO - root - 2019-11-06 18:54:30.834130: step 39300, total loss = 2.03, predict loss = 0.53 (94.5 examples/sec; 0.042 sec/batch; 1h:18m:04s remains)
INFO - root - 2019-11-06 18:54:31.296274: step 39310, total loss = 2.31, predict loss = 0.58 (94.5 examples/sec; 0.042 sec/batch; 1h:18m:02s remains)
INFO - root - 2019-11-06 18:54:31.748077: step 39320, total loss = 4.33, predict loss = 1.24 (152.4 examples/sec; 0.026 sec/batch; 0h:48m:25s remains)
INFO - root - 2019-11-06 18:54:33.095512: step 39330, total loss = 2.24, predict loss = 0.58 (57.4 examples/sec; 0.070 sec/batch; 2h:08m:34s remains)
INFO - root - 2019-11-06 18:54:33.850749: step 39340, total loss = 4.26, predict loss = 1.20 (58.9 examples/sec; 0.068 sec/batch; 2h:05m:11s remains)
INFO - root - 2019-11-06 18:54:34.615856: step 39350, total loss = 2.17, predict loss = 0.57 (59.9 examples/sec; 0.067 sec/batch; 2h:03m:14s remains)
INFO - root - 2019-11-06 18:54:35.341537: step 39360, total loss = 4.24, predict loss = 1.15 (61.4 examples/sec; 0.065 sec/batch; 2h:00m:06s remains)
INFO - root - 2019-11-06 18:54:36.069349: step 39370, total loss = 3.26, predict loss = 0.97 (66.8 examples/sec; 0.060 sec/batch; 1h:50m:25s remains)
INFO - root - 2019-11-06 18:54:36.617212: step 39380, total loss = 2.53, predict loss = 0.65 (93.8 examples/sec; 0.043 sec/batch; 1h:18m:37s remains)
INFO - root - 2019-11-06 18:54:37.086031: step 39390, total loss = 2.19, predict loss = 0.56 (95.0 examples/sec; 0.042 sec/batch; 1h:17m:37s remains)
INFO - root - 2019-11-06 18:54:38.279285: step 39400, total loss = 3.91, predict loss = 1.04 (72.0 examples/sec; 0.056 sec/batch; 1h:42m:26s remains)
INFO - root - 2019-11-06 18:54:39.008759: step 39410, total loss = 3.28, predict loss = 0.87 (56.8 examples/sec; 0.070 sec/batch; 2h:09m:45s remains)
INFO - root - 2019-11-06 18:54:39.776228: step 39420, total loss = 2.75, predict loss = 0.65 (56.8 examples/sec; 0.070 sec/batch; 2h:09m:54s remains)
INFO - root - 2019-11-06 18:54:40.533993: step 39430, total loss = 2.32, predict loss = 0.67 (59.4 examples/sec; 0.067 sec/batch; 2h:04m:02s remains)
INFO - root - 2019-11-06 18:54:41.335318: step 39440, total loss = 4.17, predict loss = 1.12 (65.5 examples/sec; 0.061 sec/batch; 1h:52m:31s remains)
INFO - root - 2019-11-06 18:54:41.880605: step 39450, total loss = 3.58, predict loss = 0.96 (99.4 examples/sec; 0.040 sec/batch; 1h:14m:07s remains)
INFO - root - 2019-11-06 18:54:42.360911: step 39460, total loss = 3.24, predict loss = 0.86 (97.1 examples/sec; 0.041 sec/batch; 1h:15m:53s remains)
INFO - root - 2019-11-06 18:54:43.487487: step 39470, total loss = 4.72, predict loss = 1.32 (5.5 examples/sec; 0.734 sec/batch; 22h:32m:00s remains)
INFO - root - 2019-11-06 18:54:44.200400: step 39480, total loss = 2.18, predict loss = 0.57 (58.4 examples/sec; 0.069 sec/batch; 2h:06m:12s remains)
INFO - root - 2019-11-06 18:54:44.993758: step 39490, total loss = 3.52, predict loss = 0.92 (58.2 examples/sec; 0.069 sec/batch; 2h:06m:29s remains)
INFO - root - 2019-11-06 18:54:45.766837: step 39500, total loss = 3.84, predict loss = 1.09 (57.1 examples/sec; 0.070 sec/batch; 2h:09m:06s remains)
INFO - root - 2019-11-06 18:54:46.521096: step 39510, total loss = 2.16, predict loss = 0.50 (61.0 examples/sec; 0.066 sec/batch; 2h:00m:46s remains)
INFO - root - 2019-11-06 18:54:47.152726: step 39520, total loss = 2.93, predict loss = 0.83 (95.5 examples/sec; 0.042 sec/batch; 1h:17m:06s remains)
INFO - root - 2019-11-06 18:54:47.644123: step 39530, total loss = 3.52, predict loss = 0.91 (92.6 examples/sec; 0.043 sec/batch; 1h:19m:34s remains)
INFO - root - 2019-11-06 18:54:48.125509: step 39540, total loss = 3.95, predict loss = 1.05 (93.7 examples/sec; 0.043 sec/batch; 1h:18m:35s remains)
INFO - root - 2019-11-06 18:54:49.407162: step 39550, total loss = 3.24, predict loss = 0.78 (50.2 examples/sec; 0.080 sec/batch; 2h:26m:47s remains)
INFO - root - 2019-11-06 18:54:50.209012: step 39560, total loss = 2.92, predict loss = 0.76 (54.3 examples/sec; 0.074 sec/batch; 2h:15m:38s remains)
INFO - root - 2019-11-06 18:54:50.987049: step 39570, total loss = 3.06, predict loss = 0.79 (58.8 examples/sec; 0.068 sec/batch; 2h:05m:15s remains)
INFO - root - 2019-11-06 18:54:51.705708: step 39580, total loss = 4.11, predict loss = 1.16 (64.3 examples/sec; 0.062 sec/batch; 1h:54m:25s remains)
INFO - root - 2019-11-06 18:54:52.446195: step 39590, total loss = 2.08, predict loss = 0.50 (61.1 examples/sec; 0.065 sec/batch; 2h:00m:22s remains)
INFO - root - 2019-11-06 18:54:53.034583: step 39600, total loss = 3.94, predict loss = 1.15 (103.0 examples/sec; 0.039 sec/batch; 1h:11m:29s remains)
INFO - root - 2019-11-06 18:54:53.487428: step 39610, total loss = 1.83, predict loss = 0.42 (91.7 examples/sec; 0.044 sec/batch; 1h:20m:14s remains)
INFO - root - 2019-11-06 18:54:54.673647: step 39620, total loss = 2.34, predict loss = 0.60 (68.8 examples/sec; 0.058 sec/batch; 1h:46m:59s remains)
INFO - root - 2019-11-06 18:54:55.429682: step 39630, total loss = 3.56, predict loss = 0.90 (56.9 examples/sec; 0.070 sec/batch; 2h:09m:22s remains)
INFO - root - 2019-11-06 18:54:56.287160: step 39640, total loss = 1.94, predict loss = 0.49 (50.3 examples/sec; 0.080 sec/batch; 2h:26m:17s remains)
INFO - root - 2019-11-06 18:54:57.097965: step 39650, total loss = 2.40, predict loss = 0.58 (50.9 examples/sec; 0.079 sec/batch; 2h:24m:25s remains)
INFO - root - 2019-11-06 18:54:57.831699: step 39660, total loss = 2.77, predict loss = 0.68 (64.5 examples/sec; 0.062 sec/batch; 1h:54m:00s remains)
INFO - root - 2019-11-06 18:54:58.427189: step 39670, total loss = 1.33, predict loss = 0.33 (96.3 examples/sec; 0.042 sec/batch; 1h:16m:21s remains)
INFO - root - 2019-11-06 18:54:58.877949: step 39680, total loss = 2.95, predict loss = 0.74 (93.7 examples/sec; 0.043 sec/batch; 1h:18m:30s remains)
INFO - root - 2019-11-06 18:54:59.337522: step 39690, total loss = 2.98, predict loss = 0.76 (87.4 examples/sec; 0.046 sec/batch; 1h:24m:10s remains)
INFO - root - 2019-11-06 18:55:00.614736: step 39700, total loss = 3.42, predict loss = 0.86 (61.8 examples/sec; 0.065 sec/batch; 1h:59m:01s remains)
INFO - root - 2019-11-06 18:55:01.343478: step 39710, total loss = 3.89, predict loss = 1.13 (57.6 examples/sec; 0.069 sec/batch; 2h:07m:36s remains)
INFO - root - 2019-11-06 18:55:02.060067: step 39720, total loss = 2.08, predict loss = 0.57 (57.8 examples/sec; 0.069 sec/batch; 2h:07m:18s remains)
INFO - root - 2019-11-06 18:55:02.826185: step 39730, total loss = 4.31, predict loss = 1.24 (66.4 examples/sec; 0.060 sec/batch; 1h:50m:46s remains)
INFO - root - 2019-11-06 18:55:03.576366: step 39740, total loss = 4.29, predict loss = 1.24 (58.6 examples/sec; 0.068 sec/batch; 2h:05m:29s remains)
INFO - root - 2019-11-06 18:55:04.104384: step 39750, total loss = 2.43, predict loss = 0.59 (95.9 examples/sec; 0.042 sec/batch; 1h:16m:36s remains)
INFO - root - 2019-11-06 18:55:04.573650: step 39760, total loss = 4.25, predict loss = 1.15 (89.1 examples/sec; 0.045 sec/batch; 1h:22m:26s remains)
INFO - root - 2019-11-06 18:55:05.742577: step 39770, total loss = 3.90, predict loss = 1.10 (62.9 examples/sec; 0.064 sec/batch; 1h:56m:50s remains)
INFO - root - 2019-11-06 18:55:06.478401: step 39780, total loss = 4.38, predict loss = 1.23 (54.2 examples/sec; 0.074 sec/batch; 2h:15m:32s remains)
INFO - root - 2019-11-06 18:55:07.233911: step 39790, total loss = 3.01, predict loss = 0.78 (54.4 examples/sec; 0.074 sec/batch; 2h:15m:10s remains)
INFO - root - 2019-11-06 18:55:07.996431: step 39800, total loss = 2.70, predict loss = 0.69 (59.7 examples/sec; 0.067 sec/batch; 2h:03m:08s remains)
INFO - root - 2019-11-06 18:55:08.671602: step 39810, total loss = 4.23, predict loss = 1.19 (66.5 examples/sec; 0.060 sec/batch; 1h:50m:26s remains)
INFO - root - 2019-11-06 18:55:09.305665: step 39820, total loss = 4.33, predict loss = 1.18 (94.9 examples/sec; 0.042 sec/batch; 1h:17m:21s remains)
INFO - root - 2019-11-06 18:55:09.773506: step 39830, total loss = 2.30, predict loss = 0.59 (94.2 examples/sec; 0.042 sec/batch; 1h:17m:56s remains)
INFO - root - 2019-11-06 18:55:10.235456: step 39840, total loss = 2.18, predict loss = 0.59 (94.7 examples/sec; 0.042 sec/batch; 1h:17m:31s remains)
INFO - root - 2019-11-06 18:55:11.549280: step 39850, total loss = 2.50, predict loss = 0.65 (65.9 examples/sec; 0.061 sec/batch; 1h:51m:28s remains)
INFO - root - 2019-11-06 18:55:12.276025: step 39860, total loss = 2.74, predict loss = 0.66 (60.8 examples/sec; 0.066 sec/batch; 2h:00m:47s remains)
INFO - root - 2019-11-06 18:55:13.024226: step 39870, total loss = 3.15, predict loss = 0.86 (56.9 examples/sec; 0.070 sec/batch; 2h:09m:01s remains)
INFO - root - 2019-11-06 18:55:13.759941: step 39880, total loss = 2.95, predict loss = 0.82 (59.5 examples/sec; 0.067 sec/batch; 2h:03m:19s remains)
INFO - root - 2019-11-06 18:55:14.449139: step 39890, total loss = 3.32, predict loss = 0.85 (76.5 examples/sec; 0.052 sec/batch; 1h:35m:57s remains)
INFO - root - 2019-11-06 18:55:14.957968: step 39900, total loss = 3.27, predict loss = 0.87 (95.9 examples/sec; 0.042 sec/batch; 1h:16m:30s remains)
INFO - root - 2019-11-06 18:55:15.424091: step 39910, total loss = 2.60, predict loss = 0.69 (94.4 examples/sec; 0.042 sec/batch; 1h:17m:42s remains)
INFO - root - 2019-11-06 18:55:16.591523: step 39920, total loss = 4.35, predict loss = 1.29 (73.2 examples/sec; 0.055 sec/batch; 1h:40m:18s remains)
INFO - root - 2019-11-06 18:55:17.280230: step 39930, total loss = 2.58, predict loss = 0.63 (63.1 examples/sec; 0.063 sec/batch; 1h:56m:19s remains)
INFO - root - 2019-11-06 18:55:18.025829: step 39940, total loss = 2.18, predict loss = 0.61 (57.8 examples/sec; 0.069 sec/batch; 2h:07m:02s remains)
INFO - root - 2019-11-06 18:55:18.726343: step 39950, total loss = 3.53, predict loss = 0.91 (62.4 examples/sec; 0.064 sec/batch; 1h:57m:38s remains)
INFO - root - 2019-11-06 18:55:19.531549: step 39960, total loss = 3.98, predict loss = 1.08 (53.6 examples/sec; 0.075 sec/batch; 2h:16m:57s remains)
INFO - root - 2019-11-06 18:55:20.106834: step 39970, total loss = 3.32, predict loss = 0.91 (96.2 examples/sec; 0.042 sec/batch; 1h:16m:16s remains)
INFO - root - 2019-11-06 18:55:20.594549: step 39980, total loss = 2.37, predict loss = 0.64 (89.3 examples/sec; 0.045 sec/batch; 1h:22m:09s remains)
INFO - root - 2019-11-06 18:55:21.047652: step 39990, total loss = 3.06, predict loss = 0.81 (86.7 examples/sec; 0.046 sec/batch; 1h:24m:36s remains)
INFO - root - 2019-11-06 18:55:22.410872: step 40000, total loss = 1.80, predict loss = 0.52 (61.9 examples/sec; 0.065 sec/batch; 1h:58m:26s remains)
INFO - root - 2019-11-06 18:55:23.114380: step 40010, total loss = 3.57, predict loss = 0.94 (66.9 examples/sec; 0.060 sec/batch; 1h:49m:40s remains)
INFO - root - 2019-11-06 18:55:23.888429: step 40020, total loss = 3.29, predict loss = 0.87 (53.0 examples/sec; 0.075 sec/batch; 2h:18m:19s remains)
INFO - root - 2019-11-06 18:55:24.704498: step 40030, total loss = 3.15, predict loss = 0.78 (51.1 examples/sec; 0.078 sec/batch; 2h:23m:22s remains)
INFO - root - 2019-11-06 18:55:25.381468: step 40040, total loss = 2.79, predict loss = 0.71 (72.3 examples/sec; 0.055 sec/batch; 1h:41m:27s remains)
INFO - root - 2019-11-06 18:55:25.849834: step 40050, total loss = 2.53, predict loss = 0.63 (96.0 examples/sec; 0.042 sec/batch; 1h:16m:20s remains)
INFO - root - 2019-11-06 18:55:26.344892: step 40060, total loss = 3.12, predict loss = 0.80 (90.9 examples/sec; 0.044 sec/batch; 1h:20m:38s remains)
INFO - root - 2019-11-06 18:55:27.530754: step 40070, total loss = 3.31, predict loss = 0.87 (55.5 examples/sec; 0.072 sec/batch; 2h:11m:59s remains)
INFO - root - 2019-11-06 18:55:28.257497: step 40080, total loss = 2.25, predict loss = 0.61 (56.6 examples/sec; 0.071 sec/batch; 2h:09m:27s remains)
INFO - root - 2019-11-06 18:55:29.004988: step 40090, total loss = 3.35, predict loss = 0.89 (62.6 examples/sec; 0.064 sec/batch; 1h:57m:02s remains)
INFO - root - 2019-11-06 18:55:29.728759: step 40100, total loss = 3.39, predict loss = 0.93 (63.4 examples/sec; 0.063 sec/batch; 1h:55m:33s remains)
INFO - root - 2019-11-06 18:55:30.416747: step 40110, total loss = 3.79, predict loss = 1.10 (63.7 examples/sec; 0.063 sec/batch; 1h:55m:03s remains)
INFO - root - 2019-11-06 18:55:31.049812: step 40120, total loss = 3.03, predict loss = 0.73 (92.0 examples/sec; 0.043 sec/batch; 1h:19m:36s remains)
INFO - root - 2019-11-06 18:55:31.504518: step 40130, total loss = 2.53, predict loss = 0.64 (96.7 examples/sec; 0.041 sec/batch; 1h:15m:44s remains)
INFO - root - 2019-11-06 18:55:31.983404: step 40140, total loss = 4.41, predict loss = 1.20 (103.6 examples/sec; 0.039 sec/batch; 1h:10m:40s remains)
INFO - root - 2019-11-06 18:55:33.354936: step 40150, total loss = 3.84, predict loss = 1.08 (62.6 examples/sec; 0.064 sec/batch; 1h:57m:00s remains)
INFO - root - 2019-11-06 18:55:34.156035: step 40160, total loss = 3.70, predict loss = 1.01 (47.5 examples/sec; 0.084 sec/batch; 2h:34m:03s remains)
INFO - root - 2019-11-06 18:55:34.926525: step 40170, total loss = 2.78, predict loss = 0.69 (57.3 examples/sec; 0.070 sec/batch; 2h:07m:49s remains)
INFO - root - 2019-11-06 18:55:35.678714: step 40180, total loss = 3.58, predict loss = 1.02 (64.1 examples/sec; 0.062 sec/batch; 1h:54m:16s remains)
INFO - root - 2019-11-06 18:55:36.330615: step 40190, total loss = 3.25, predict loss = 0.88 (83.6 examples/sec; 0.048 sec/batch; 1h:27m:35s remains)
INFO - root - 2019-11-06 18:55:36.777511: step 40200, total loss = 4.14, predict loss = 1.11 (94.0 examples/sec; 0.043 sec/batch; 1h:17m:51s remains)
INFO - root - 2019-11-06 18:55:37.239471: step 40210, total loss = 2.39, predict loss = 0.55 (89.1 examples/sec; 0.045 sec/batch; 1h:22m:08s remains)
INFO - root - 2019-11-06 18:55:38.460350: step 40220, total loss = 2.92, predict loss = 0.70 (61.9 examples/sec; 0.065 sec/batch; 1h:58m:19s remains)
INFO - root - 2019-11-06 18:55:39.176822: step 40230, total loss = 3.50, predict loss = 0.93 (56.0 examples/sec; 0.071 sec/batch; 2h:10m:38s remains)
INFO - root - 2019-11-06 18:55:39.987496: step 40240, total loss = 2.76, predict loss = 0.69 (52.3 examples/sec; 0.076 sec/batch; 2h:19m:47s remains)
INFO - root - 2019-11-06 18:55:40.705043: step 40250, total loss = 2.01, predict loss = 0.53 (61.8 examples/sec; 0.065 sec/batch; 1h:58m:18s remains)
INFO - root - 2019-11-06 18:55:41.445358: step 40260, total loss = 2.22, predict loss = 0.59 (66.5 examples/sec; 0.060 sec/batch; 1h:50m:02s remains)
INFO - root - 2019-11-06 18:55:41.990674: step 40270, total loss = 3.68, predict loss = 0.98 (96.7 examples/sec; 0.041 sec/batch; 1h:15m:38s remains)
INFO - root - 2019-11-06 18:55:42.428620: step 40280, total loss = 2.22, predict loss = 0.53 (89.2 examples/sec; 0.045 sec/batch; 1h:21m:57s remains)
INFO - root - 2019-11-06 18:55:43.563924: step 40290, total loss = 2.49, predict loss = 0.64 (5.5 examples/sec; 0.732 sec/batch; 22h:18m:32s remains)
INFO - root - 2019-11-06 18:55:44.276895: step 40300, total loss = 3.95, predict loss = 1.03 (56.7 examples/sec; 0.071 sec/batch; 2h:09m:00s remains)
INFO - root - 2019-11-06 18:55:45.037625: step 40310, total loss = 3.56, predict loss = 0.90 (59.3 examples/sec; 0.067 sec/batch; 2h:03m:23s remains)
INFO - root - 2019-11-06 18:55:45.827447: step 40320, total loss = 3.67, predict loss = 0.94 (48.3 examples/sec; 0.083 sec/batch; 2h:31m:25s remains)
INFO - root - 2019-11-06 18:55:46.579151: step 40330, total loss = 3.50, predict loss = 0.93 (62.2 examples/sec; 0.064 sec/batch; 1h:57m:33s remains)
INFO - root - 2019-11-06 18:55:47.263891: step 40340, total loss = 3.92, predict loss = 1.14 (93.8 examples/sec; 0.043 sec/batch; 1h:17m:53s remains)
INFO - root - 2019-11-06 18:55:47.702750: step 40350, total loss = 2.89, predict loss = 0.76 (93.2 examples/sec; 0.043 sec/batch; 1h:18m:26s remains)
INFO - root - 2019-11-06 18:55:48.158998: step 40360, total loss = 3.19, predict loss = 0.84 (99.9 examples/sec; 0.040 sec/batch; 1h:13m:11s remains)
INFO - root - 2019-11-06 18:55:49.379803: step 40370, total loss = 3.52, predict loss = 0.90 (64.6 examples/sec; 0.062 sec/batch; 1h:53m:05s remains)
INFO - root - 2019-11-06 18:55:50.153249: step 40380, total loss = 2.81, predict loss = 0.68 (52.1 examples/sec; 0.077 sec/batch; 2h:20m:20s remains)
INFO - root - 2019-11-06 18:55:50.852115: step 40390, total loss = 2.11, predict loss = 0.53 (61.0 examples/sec; 0.066 sec/batch; 1h:59m:42s remains)
INFO - root - 2019-11-06 18:55:51.594498: step 40400, total loss = 2.17, predict loss = 0.53 (60.0 examples/sec; 0.067 sec/batch; 2h:01m:42s remains)
INFO - root - 2019-11-06 18:55:52.333251: step 40410, total loss = 3.25, predict loss = 0.85 (67.2 examples/sec; 0.060 sec/batch; 1h:48m:44s remains)
INFO - root - 2019-11-06 18:55:52.895782: step 40420, total loss = 3.24, predict loss = 0.87 (97.4 examples/sec; 0.041 sec/batch; 1h:14m:59s remains)
INFO - root - 2019-11-06 18:55:53.351684: step 40430, total loss = 3.19, predict loss = 0.82 (92.1 examples/sec; 0.043 sec/batch; 1h:19m:16s remains)
INFO - root - 2019-11-06 18:55:54.502892: step 40440, total loss = 1.84, predict loss = 0.44 (72.7 examples/sec; 0.055 sec/batch; 1h:40m:31s remains)
INFO - root - 2019-11-06 18:55:55.181198: step 40450, total loss = 3.86, predict loss = 1.05 (59.2 examples/sec; 0.068 sec/batch; 2h:03m:18s remains)
INFO - root - 2019-11-06 18:55:56.003523: step 40460, total loss = 2.29, predict loss = 0.58 (53.9 examples/sec; 0.074 sec/batch; 2h:15m:26s remains)
INFO - root - 2019-11-06 18:55:56.748921: step 40470, total loss = 2.26, predict loss = 0.54 (60.0 examples/sec; 0.067 sec/batch; 2h:01m:45s remains)
INFO - root - 2019-11-06 18:55:57.470902: step 40480, total loss = 4.01, predict loss = 1.07 (62.8 examples/sec; 0.064 sec/batch; 1h:56m:10s remains)
INFO - root - 2019-11-06 18:55:58.125417: step 40490, total loss = 2.91, predict loss = 0.73 (80.2 examples/sec; 0.050 sec/batch; 1h:31m:03s remains)
INFO - root - 2019-11-06 18:55:58.599485: step 40500, total loss = 2.62, predict loss = 0.67 (95.8 examples/sec; 0.042 sec/batch; 1h:16m:13s remains)
INFO - root - 2019-11-06 18:55:59.051361: step 40510, total loss = 2.31, predict loss = 0.55 (95.1 examples/sec; 0.042 sec/batch; 1h:16m:43s remains)
INFO - root - 2019-11-06 18:56:00.323543: step 40520, total loss = 2.41, predict loss = 0.61 (61.9 examples/sec; 0.065 sec/batch; 1h:57m:54s remains)
INFO - root - 2019-11-06 18:56:01.079501: step 40530, total loss = 1.54, predict loss = 0.38 (58.3 examples/sec; 0.069 sec/batch; 2h:05m:15s remains)
INFO - root - 2019-11-06 18:56:01.814500: step 40540, total loss = 3.71, predict loss = 1.03 (55.7 examples/sec; 0.072 sec/batch; 2h:11m:02s remains)
INFO - root - 2019-11-06 18:56:02.655329: step 40550, total loss = 2.23, predict loss = 0.57 (50.1 examples/sec; 0.080 sec/batch; 2h:25m:32s remains)
INFO - root - 2019-11-06 18:56:03.433895: step 40560, total loss = 3.97, predict loss = 1.10 (63.5 examples/sec; 0.063 sec/batch; 1h:54m:55s remains)
INFO - root - 2019-11-06 18:56:03.980829: step 40570, total loss = 3.11, predict loss = 0.78 (98.6 examples/sec; 0.041 sec/batch; 1h:13m:57s remains)
INFO - root - 2019-11-06 18:56:04.475978: step 40580, total loss = 3.69, predict loss = 1.04 (91.9 examples/sec; 0.044 sec/batch; 1h:19m:21s remains)
INFO - root - 2019-11-06 18:56:05.645175: step 40590, total loss = 3.13, predict loss = 0.82 (70.3 examples/sec; 0.057 sec/batch; 1h:43m:49s remains)
INFO - root - 2019-11-06 18:56:06.326078: step 40600, total loss = 2.76, predict loss = 0.67 (61.8 examples/sec; 0.065 sec/batch; 1h:57m:55s remains)
INFO - root - 2019-11-06 18:56:07.081305: step 40610, total loss = 1.95, predict loss = 0.53 (62.2 examples/sec; 0.064 sec/batch; 1h:57m:17s remains)
INFO - root - 2019-11-06 18:56:07.887259: step 40620, total loss = 3.79, predict loss = 1.09 (59.8 examples/sec; 0.067 sec/batch; 2h:01m:55s remains)
INFO - root - 2019-11-06 18:56:08.678079: step 40630, total loss = 2.72, predict loss = 0.73 (56.7 examples/sec; 0.071 sec/batch; 2h:08m:30s remains)
INFO - root - 2019-11-06 18:56:09.303525: step 40640, total loss = 2.76, predict loss = 0.63 (103.7 examples/sec; 0.039 sec/batch; 1h:10m:19s remains)
INFO - root - 2019-11-06 18:56:09.760960: step 40650, total loss = 3.17, predict loss = 0.87 (88.1 examples/sec; 0.045 sec/batch; 1h:22m:42s remains)
INFO - root - 2019-11-06 18:56:10.246304: step 40660, total loss = 3.10, predict loss = 0.88 (95.0 examples/sec; 0.042 sec/batch; 1h:16m:44s remains)
INFO - root - 2019-11-06 18:56:11.552916: step 40670, total loss = 2.15, predict loss = 0.50 (51.2 examples/sec; 0.078 sec/batch; 2h:22m:21s remains)
INFO - root - 2019-11-06 18:56:12.329634: step 40680, total loss = 3.01, predict loss = 0.75 (60.0 examples/sec; 0.067 sec/batch; 2h:01m:27s remains)
INFO - root - 2019-11-06 18:56:13.107723: step 40690, total loss = 3.28, predict loss = 0.78 (51.1 examples/sec; 0.078 sec/batch; 2h:22m:34s remains)
INFO - root - 2019-11-06 18:56:13.911408: step 40700, total loss = 3.34, predict loss = 0.90 (51.1 examples/sec; 0.078 sec/batch; 2h:22m:35s remains)
INFO - root - 2019-11-06 18:56:14.637495: step 40710, total loss = 4.10, predict loss = 1.11 (70.8 examples/sec; 0.056 sec/batch; 1h:42m:50s remains)
INFO - root - 2019-11-06 18:56:15.140066: step 40720, total loss = 3.63, predict loss = 0.99 (100.6 examples/sec; 0.040 sec/batch; 1h:12m:27s remains)
INFO - root - 2019-11-06 18:56:15.601445: step 40730, total loss = 4.09, predict loss = 1.04 (96.1 examples/sec; 0.042 sec/batch; 1h:15m:49s remains)
INFO - root - 2019-11-06 18:56:16.755379: step 40740, total loss = 2.30, predict loss = 0.55 (62.8 examples/sec; 0.064 sec/batch; 1h:55m:54s remains)
INFO - root - 2019-11-06 18:56:17.484585: step 40750, total loss = 2.55, predict loss = 0.67 (60.8 examples/sec; 0.066 sec/batch; 1h:59m:51s remains)
INFO - root - 2019-11-06 18:56:18.262956: step 40760, total loss = 3.76, predict loss = 1.10 (54.8 examples/sec; 0.073 sec/batch; 2h:12m:57s remains)
INFO - root - 2019-11-06 18:56:19.047543: step 40770, total loss = 3.74, predict loss = 0.98 (61.8 examples/sec; 0.065 sec/batch; 1h:57m:47s remains)
INFO - root - 2019-11-06 18:56:19.795085: step 40780, total loss = 4.03, predict loss = 1.09 (57.9 examples/sec; 0.069 sec/batch; 2h:05m:48s remains)
INFO - root - 2019-11-06 18:56:20.409921: step 40790, total loss = 2.70, predict loss = 0.73 (95.5 examples/sec; 0.042 sec/batch; 1h:16m:14s remains)
INFO - root - 2019-11-06 18:56:20.870370: step 40800, total loss = 3.16, predict loss = 0.83 (93.1 examples/sec; 0.043 sec/batch; 1h:18m:12s remains)
INFO - root - 2019-11-06 18:56:21.329425: step 40810, total loss = 4.17, predict loss = 1.24 (95.8 examples/sec; 0.042 sec/batch; 1h:15m:58s remains)
INFO - root - 2019-11-06 18:56:22.660017: step 40820, total loss = 2.40, predict loss = 0.61 (62.5 examples/sec; 0.064 sec/batch; 1h:56m:32s remains)
INFO - root - 2019-11-06 18:56:23.394344: step 40830, total loss = 2.76, predict loss = 0.70 (51.6 examples/sec; 0.077 sec/batch; 2h:20m:56s remains)
INFO - root - 2019-11-06 18:56:24.086488: step 40840, total loss = 2.79, predict loss = 0.71 (56.5 examples/sec; 0.071 sec/batch; 2h:08m:53s remains)
INFO - root - 2019-11-06 18:56:24.843088: step 40850, total loss = 3.91, predict loss = 1.15 (51.1 examples/sec; 0.078 sec/batch; 2h:22m:31s remains)
INFO - root - 2019-11-06 18:56:25.591440: step 40860, total loss = 3.95, predict loss = 1.14 (72.1 examples/sec; 0.055 sec/batch; 1h:40m:51s remains)
INFO - root - 2019-11-06 18:56:26.079802: step 40870, total loss = 2.57, predict loss = 0.65 (96.9 examples/sec; 0.041 sec/batch; 1h:15m:04s remains)
INFO - root - 2019-11-06 18:56:26.530242: step 40880, total loss = 3.01, predict loss = 0.85 (90.3 examples/sec; 0.044 sec/batch; 1h:20m:35s remains)
INFO - root - 2019-11-06 18:56:27.796061: step 40890, total loss = 2.68, predict loss = 0.66 (62.9 examples/sec; 0.064 sec/batch; 1h:55m:35s remains)
INFO - root - 2019-11-06 18:56:28.543408: step 40900, total loss = 2.52, predict loss = 0.70 (58.0 examples/sec; 0.069 sec/batch; 2h:05m:25s remains)
INFO - root - 2019-11-06 18:56:29.279650: step 40910, total loss = 2.40, predict loss = 0.62 (60.9 examples/sec; 0.066 sec/batch; 1h:59m:24s remains)
INFO - root - 2019-11-06 18:56:30.063717: step 40920, total loss = 2.92, predict loss = 0.74 (56.0 examples/sec; 0.071 sec/batch; 2h:09m:56s remains)
INFO - root - 2019-11-06 18:56:30.779218: step 40930, total loss = 3.24, predict loss = 0.92 (68.6 examples/sec; 0.058 sec/batch; 1h:45m:57s remains)
INFO - root - 2019-11-06 18:56:31.349206: step 40940, total loss = 3.42, predict loss = 0.86 (97.3 examples/sec; 0.041 sec/batch; 1h:14m:45s remains)
INFO - root - 2019-11-06 18:56:31.796689: step 40950, total loss = 3.21, predict loss = 0.86 (96.6 examples/sec; 0.041 sec/batch; 1h:15m:15s remains)
INFO - root - 2019-11-06 18:56:32.226152: step 40960, total loss = 3.76, predict loss = 1.01 (142.8 examples/sec; 0.028 sec/batch; 0h:50m:53s remains)
INFO - root - 2019-11-06 18:56:33.583645: step 40970, total loss = 3.29, predict loss = 0.87 (67.0 examples/sec; 0.060 sec/batch; 1h:48m:29s remains)
INFO - root - 2019-11-06 18:56:34.309107: step 40980, total loss = 2.57, predict loss = 0.66 (55.4 examples/sec; 0.072 sec/batch; 2h:11m:10s remains)
INFO - root - 2019-11-06 18:56:35.019703: step 40990, total loss = 1.96, predict loss = 0.48 (63.6 examples/sec; 0.063 sec/batch; 1h:54m:15s remains)
INFO - root - 2019-11-06 18:56:35.789629: step 41000, total loss = 2.24, predict loss = 0.58 (57.1 examples/sec; 0.070 sec/batch; 2h:07m:16s remains)
INFO - root - 2019-11-06 18:56:36.545631: step 41010, total loss = 1.10, predict loss = 0.26 (70.8 examples/sec; 0.057 sec/batch; 1h:42m:38s remains)
INFO - root - 2019-11-06 18:56:37.051759: step 41020, total loss = 2.56, predict loss = 0.66 (97.9 examples/sec; 0.041 sec/batch; 1h:14m:11s remains)
INFO - root - 2019-11-06 18:56:37.502635: step 41030, total loss = 3.69, predict loss = 1.06 (103.9 examples/sec; 0.038 sec/batch; 1h:09m:54s remains)
INFO - root - 2019-11-06 18:56:38.740707: step 41040, total loss = 3.03, predict loss = 0.77 (63.6 examples/sec; 0.063 sec/batch; 1h:54m:09s remains)
INFO - root - 2019-11-06 18:56:39.528903: step 41050, total loss = 3.94, predict loss = 1.13 (52.8 examples/sec; 0.076 sec/batch; 2h:17m:35s remains)
INFO - root - 2019-11-06 18:56:40.301789: step 41060, total loss = 4.01, predict loss = 1.15 (53.8 examples/sec; 0.074 sec/batch; 2h:14m:57s remains)
INFO - root - 2019-11-06 18:56:41.030274: step 41070, total loss = 2.91, predict loss = 0.87 (65.9 examples/sec; 0.061 sec/batch; 1h:50m:12s remains)
INFO - root - 2019-11-06 18:56:41.744645: step 41080, total loss = 3.88, predict loss = 1.05 (70.8 examples/sec; 0.056 sec/batch; 1h:42m:32s remains)
INFO - root - 2019-11-06 18:56:42.275870: step 41090, total loss = 2.19, predict loss = 0.53 (98.8 examples/sec; 0.040 sec/batch; 1h:13m:28s remains)
INFO - root - 2019-11-06 18:56:42.762132: step 41100, total loss = 3.48, predict loss = 0.83 (95.5 examples/sec; 0.042 sec/batch; 1h:16m:00s remains)
INFO - root - 2019-11-06 18:56:43.871549: step 41110, total loss = 3.81, predict loss = 1.02 (5.7 examples/sec; 0.702 sec/batch; 21h:13m:50s remains)
INFO - root - 2019-11-06 18:56:44.533068: step 41120, total loss = 2.94, predict loss = 0.71 (60.5 examples/sec; 0.066 sec/batch; 2h:00m:03s remains)
INFO - root - 2019-11-06 18:56:45.326160: step 41130, total loss = 2.93, predict loss = 0.80 (58.4 examples/sec; 0.068 sec/batch; 2h:04m:15s remains)
INFO - root - 2019-11-06 18:56:46.074679: step 41140, total loss = 3.61, predict loss = 0.96 (60.7 examples/sec; 0.066 sec/batch; 1h:59m:28s remains)
INFO - root - 2019-11-06 18:56:46.841603: step 41150, total loss = 1.68, predict loss = 0.40 (52.3 examples/sec; 0.077 sec/batch; 2h:18m:49s remains)
INFO - root - 2019-11-06 18:56:47.604013: step 41160, total loss = 2.27, predict loss = 0.59 (90.7 examples/sec; 0.044 sec/batch; 1h:20m:01s remains)
INFO - root - 2019-11-06 18:56:48.057221: step 41170, total loss = 3.55, predict loss = 0.94 (89.7 examples/sec; 0.045 sec/batch; 1h:20m:50s remains)
INFO - root - 2019-11-06 18:56:48.546505: step 41180, total loss = 2.36, predict loss = 0.63 (94.6 examples/sec; 0.042 sec/batch; 1h:16m:38s remains)
INFO - root - 2019-11-06 18:56:49.830894: step 41190, total loss = 1.30, predict loss = 0.34 (47.9 examples/sec; 0.083 sec/batch; 2h:31m:18s remains)
INFO - root - 2019-11-06 18:56:50.599962: step 41200, total loss = 2.20, predict loss = 0.55 (61.9 examples/sec; 0.065 sec/batch; 1h:57m:13s remains)
INFO - root - 2019-11-06 18:56:51.332195: step 41210, total loss = 3.28, predict loss = 0.91 (59.5 examples/sec; 0.067 sec/batch; 2h:01m:57s remains)
INFO - root - 2019-11-06 18:56:52.065390: step 41220, total loss = 2.61, predict loss = 0.64 (62.7 examples/sec; 0.064 sec/batch; 1h:55m:35s remains)
INFO - root - 2019-11-06 18:56:52.755409: step 41230, total loss = 2.59, predict loss = 0.65 (70.8 examples/sec; 0.056 sec/batch; 1h:42m:22s remains)
INFO - root - 2019-11-06 18:56:53.283079: step 41240, total loss = 3.75, predict loss = 1.03 (99.7 examples/sec; 0.040 sec/batch; 1h:12m:43s remains)
INFO - root - 2019-11-06 18:56:53.725548: step 41250, total loss = 2.32, predict loss = 0.65 (101.5 examples/sec; 0.039 sec/batch; 1h:11m:24s remains)
INFO - root - 2019-11-06 18:56:54.913221: step 41260, total loss = 2.25, predict loss = 0.55 (66.2 examples/sec; 0.060 sec/batch; 1h:49m:29s remains)
INFO - root - 2019-11-06 18:56:55.628204: step 41270, total loss = 1.81, predict loss = 0.43 (58.3 examples/sec; 0.069 sec/batch; 2h:04m:24s remains)
INFO - root - 2019-11-06 18:56:56.384754: step 41280, total loss = 1.95, predict loss = 0.41 (55.1 examples/sec; 0.073 sec/batch; 2h:11m:26s remains)
INFO - root - 2019-11-06 18:56:57.165815: step 41290, total loss = 2.71, predict loss = 0.67 (57.1 examples/sec; 0.070 sec/batch; 2h:06m:58s remains)
INFO - root - 2019-11-06 18:56:57.942198: step 41300, total loss = 2.87, predict loss = 0.72 (66.9 examples/sec; 0.060 sec/batch; 1h:48m:23s remains)
INFO - root - 2019-11-06 18:56:58.562050: step 41310, total loss = 3.45, predict loss = 0.96 (95.5 examples/sec; 0.042 sec/batch; 1h:15m:53s remains)
INFO - root - 2019-11-06 18:56:59.018615: step 41320, total loss = 2.46, predict loss = 0.66 (93.7 examples/sec; 0.043 sec/batch; 1h:17m:19s remains)
INFO - root - 2019-11-06 18:56:59.470871: step 41330, total loss = 3.10, predict loss = 0.79 (102.5 examples/sec; 0.039 sec/batch; 1h:10m:41s remains)
INFO - root - 2019-11-06 18:57:00.734459: step 41340, total loss = 3.12, predict loss = 0.79 (59.0 examples/sec; 0.068 sec/batch; 2h:02m:44s remains)
INFO - root - 2019-11-06 18:57:01.528508: step 41350, total loss = 4.51, predict loss = 1.27 (48.8 examples/sec; 0.082 sec/batch; 2h:28m:29s remains)
INFO - root - 2019-11-06 18:57:02.276238: step 41360, total loss = 2.77, predict loss = 0.67 (58.4 examples/sec; 0.069 sec/batch; 2h:04m:03s remains)
INFO - root - 2019-11-06 18:57:03.038601: step 41370, total loss = 1.44, predict loss = 0.37 (63.6 examples/sec; 0.063 sec/batch; 1h:53m:51s remains)
INFO - root - 2019-11-06 18:57:03.785813: step 41380, total loss = 3.74, predict loss = 1.05 (64.4 examples/sec; 0.062 sec/batch; 1h:52m:30s remains)
INFO - root - 2019-11-06 18:57:04.306528: step 41390, total loss = 2.68, predict loss = 0.64 (100.3 examples/sec; 0.040 sec/batch; 1h:12m:10s remains)
INFO - root - 2019-11-06 18:57:04.783013: step 41400, total loss = 2.29, predict loss = 0.56 (93.3 examples/sec; 0.043 sec/batch; 1h:17m:38s remains)
INFO - root - 2019-11-06 18:57:05.941212: step 41410, total loss = 2.77, predict loss = 0.74 (68.1 examples/sec; 0.059 sec/batch; 1h:46m:22s remains)
INFO - root - 2019-11-06 18:57:06.654052: step 41420, total loss = 2.26, predict loss = 0.58 (63.5 examples/sec; 0.063 sec/batch; 1h:54m:00s remains)
INFO - root - 2019-11-06 18:57:07.427198: step 41430, total loss = 4.20, predict loss = 1.24 (56.3 examples/sec; 0.071 sec/batch; 2h:08m:35s remains)
INFO - root - 2019-11-06 18:57:08.115726: step 41440, total loss = 1.80, predict loss = 0.47 (70.9 examples/sec; 0.056 sec/batch; 1h:42m:00s remains)
INFO - root - 2019-11-06 18:57:08.808882: step 41450, total loss = 2.02, predict loss = 0.51 (59.3 examples/sec; 0.067 sec/batch; 2h:01m:58s remains)
INFO - root - 2019-11-06 18:57:09.411103: step 41460, total loss = 3.07, predict loss = 0.77 (103.3 examples/sec; 0.039 sec/batch; 1h:10m:04s remains)
INFO - root - 2019-11-06 18:57:09.865487: step 41470, total loss = 1.32, predict loss = 0.33 (93.1 examples/sec; 0.043 sec/batch; 1h:17m:45s remains)
INFO - root - 2019-11-06 18:57:10.324471: step 41480, total loss = 3.13, predict loss = 0.78 (89.1 examples/sec; 0.045 sec/batch; 1h:21m:09s remains)
INFO - root - 2019-11-06 18:57:11.617766: step 41490, total loss = 2.80, predict loss = 0.69 (63.1 examples/sec; 0.063 sec/batch; 1h:54m:42s remains)
INFO - root - 2019-11-06 18:57:12.364504: step 41500, total loss = 2.90, predict loss = 0.74 (65.1 examples/sec; 0.061 sec/batch; 1h:51m:06s remains)
INFO - root - 2019-11-06 18:57:13.132695: step 41510, total loss = 2.45, predict loss = 0.68 (59.5 examples/sec; 0.067 sec/batch; 2h:01m:38s remains)
INFO - root - 2019-11-06 18:57:13.883266: step 41520, total loss = 3.30, predict loss = 0.91 (57.4 examples/sec; 0.070 sec/batch; 2h:06m:03s remains)
INFO - root - 2019-11-06 18:57:14.615098: step 41530, total loss = 2.46, predict loss = 0.70 (67.0 examples/sec; 0.060 sec/batch; 1h:47m:51s remains)
INFO - root - 2019-11-06 18:57:15.147055: step 41540, total loss = 2.69, predict loss = 0.62 (100.4 examples/sec; 0.040 sec/batch; 1h:12m:00s remains)
INFO - root - 2019-11-06 18:57:15.598020: step 41550, total loss = 3.50, predict loss = 0.94 (90.6 examples/sec; 0.044 sec/batch; 1h:19m:45s remains)
INFO - root - 2019-11-06 18:57:16.842895: step 41560, total loss = 2.34, predict loss = 0.60 (65.8 examples/sec; 0.061 sec/batch; 1h:49m:53s remains)
INFO - root - 2019-11-06 18:57:17.515970: step 41570, total loss = 2.46, predict loss = 0.60 (57.9 examples/sec; 0.069 sec/batch; 2h:04m:53s remains)
INFO - root - 2019-11-06 18:57:18.308416: step 41580, total loss = 2.59, predict loss = 0.67 (61.2 examples/sec; 0.065 sec/batch; 1h:58m:03s remains)
INFO - root - 2019-11-06 18:57:19.021444: step 41590, total loss = 2.83, predict loss = 0.70 (62.1 examples/sec; 0.064 sec/batch; 1h:56m:25s remains)
INFO - root - 2019-11-06 18:57:19.788408: step 41600, total loss = 4.67, predict loss = 1.45 (57.4 examples/sec; 0.070 sec/batch; 2h:05m:51s remains)
INFO - root - 2019-11-06 18:57:20.394016: step 41610, total loss = 2.16, predict loss = 0.55 (94.5 examples/sec; 0.042 sec/batch; 1h:16m:25s remains)
INFO - root - 2019-11-06 18:57:20.876527: step 41620, total loss = 2.67, predict loss = 0.73 (91.9 examples/sec; 0.044 sec/batch; 1h:18m:38s remains)
INFO - root - 2019-11-06 18:57:21.333067: step 41630, total loss = 2.22, predict loss = 0.54 (90.2 examples/sec; 0.044 sec/batch; 1h:20m:03s remains)
INFO - root - 2019-11-06 18:57:22.658128: step 41640, total loss = 1.81, predict loss = 0.46 (46.6 examples/sec; 0.086 sec/batch; 2h:34m:55s remains)
INFO - root - 2019-11-06 18:57:23.409880: step 41650, total loss = 3.31, predict loss = 0.85 (58.3 examples/sec; 0.069 sec/batch; 2h:03m:58s remains)
INFO - root - 2019-11-06 18:57:24.133416: step 41660, total loss = 1.66, predict loss = 0.39 (60.0 examples/sec; 0.067 sec/batch; 2h:00m:17s remains)
INFO - root - 2019-11-06 18:57:24.922380: step 41670, total loss = 2.93, predict loss = 0.73 (53.4 examples/sec; 0.075 sec/batch; 2h:15m:17s remains)
INFO - root - 2019-11-06 18:57:25.639686: step 41680, total loss = 3.42, predict loss = 0.86 (74.1 examples/sec; 0.054 sec/batch; 1h:37m:27s remains)
INFO - root - 2019-11-06 18:57:26.120661: step 41690, total loss = 2.84, predict loss = 0.70 (96.5 examples/sec; 0.041 sec/batch; 1h:14m:48s remains)
INFO - root - 2019-11-06 18:57:26.610656: step 41700, total loss = 3.56, predict loss = 1.01 (95.4 examples/sec; 0.042 sec/batch; 1h:15m:40s remains)
INFO - root - 2019-11-06 18:57:27.825123: step 41710, total loss = 4.01, predict loss = 1.20 (64.3 examples/sec; 0.062 sec/batch; 1h:52m:14s remains)
INFO - root - 2019-11-06 18:57:28.614403: step 41720, total loss = 3.85, predict loss = 1.10 (56.6 examples/sec; 0.071 sec/batch; 2h:07m:29s remains)
INFO - root - 2019-11-06 18:57:29.354462: step 41730, total loss = 1.77, predict loss = 0.42 (71.1 examples/sec; 0.056 sec/batch; 1h:41m:33s remains)
INFO - root - 2019-11-06 18:57:30.099341: step 41740, total loss = 2.43, predict loss = 0.57 (70.7 examples/sec; 0.057 sec/batch; 1h:42m:03s remains)
INFO - root - 2019-11-06 18:57:30.892326: step 41750, total loss = 2.39, predict loss = 0.63 (57.5 examples/sec; 0.070 sec/batch; 2h:05m:27s remains)
INFO - root - 2019-11-06 18:57:31.478761: step 41760, total loss = 4.40, predict loss = 1.28 (95.1 examples/sec; 0.042 sec/batch; 1h:15m:54s remains)
INFO - root - 2019-11-06 18:57:31.936233: step 41770, total loss = 3.14, predict loss = 0.81 (95.5 examples/sec; 0.042 sec/batch; 1h:15m:32s remains)
INFO - root - 2019-11-06 18:57:32.412059: step 41780, total loss = 3.05, predict loss = 0.81 (119.9 examples/sec; 0.033 sec/batch; 1h:00m:10s remains)
INFO - root - 2019-11-06 18:57:33.823219: step 41790, total loss = 3.86, predict loss = 1.07 (59.6 examples/sec; 0.067 sec/batch; 2h:01m:03s remains)
INFO - root - 2019-11-06 18:57:34.568161: step 41800, total loss = 4.01, predict loss = 1.26 (55.3 examples/sec; 0.072 sec/batch; 2h:10m:26s remains)
INFO - root - 2019-11-06 18:57:35.304400: step 41810, total loss = 2.91, predict loss = 0.73 (64.7 examples/sec; 0.062 sec/batch; 1h:51m:28s remains)
INFO - root - 2019-11-06 18:57:36.056889: step 41820, total loss = 3.83, predict loss = 1.11 (62.6 examples/sec; 0.064 sec/batch; 1h:55m:14s remains)
INFO - root - 2019-11-06 18:57:36.718324: step 41830, total loss = 1.72, predict loss = 0.41 (75.9 examples/sec; 0.053 sec/batch; 1h:35m:03s remains)
INFO - root - 2019-11-06 18:57:37.174680: step 41840, total loss = 3.60, predict loss = 0.95 (89.4 examples/sec; 0.045 sec/batch; 1h:20m:38s remains)
INFO - root - 2019-11-06 18:57:37.636749: step 41850, total loss = 1.76, predict loss = 0.46 (96.7 examples/sec; 0.041 sec/batch; 1h:14m:35s remains)
INFO - root - 2019-11-06 18:57:38.906338: step 41860, total loss = 3.37, predict loss = 0.92 (64.2 examples/sec; 0.062 sec/batch; 1h:52m:20s remains)
INFO - root - 2019-11-06 18:57:39.689950: step 41870, total loss = 2.60, predict loss = 0.66 (52.6 examples/sec; 0.076 sec/batch; 2h:17m:02s remains)
INFO - root - 2019-11-06 18:57:40.419042: step 41880, total loss = 2.10, predict loss = 0.54 (58.9 examples/sec; 0.068 sec/batch; 2h:02m:19s remains)
INFO - root - 2019-11-06 18:57:41.142428: step 41890, total loss = 2.26, predict loss = 0.55 (65.8 examples/sec; 0.061 sec/batch; 1h:49m:36s remains)
INFO - root - 2019-11-06 18:57:41.853884: step 41900, total loss = 2.23, predict loss = 0.55 (75.2 examples/sec; 0.053 sec/batch; 1h:35m:50s remains)
INFO - root - 2019-11-06 18:57:42.431169: step 41910, total loss = 2.50, predict loss = 0.63 (99.1 examples/sec; 0.040 sec/batch; 1h:12m:41s remains)
INFO - root - 2019-11-06 18:57:42.880941: step 41920, total loss = 3.13, predict loss = 0.78 (92.0 examples/sec; 0.043 sec/batch; 1h:18m:17s remains)
INFO - root - 2019-11-06 18:57:44.020866: step 41930, total loss = 2.24, predict loss = 0.55 (5.5 examples/sec; 0.722 sec/batch; 21h:40m:44s remains)
INFO - root - 2019-11-06 18:57:44.731174: step 41940, total loss = 2.66, predict loss = 0.64 (60.5 examples/sec; 0.066 sec/batch; 1h:59m:00s remains)
INFO - root - 2019-11-06 18:57:45.514001: step 41950, total loss = 1.49, predict loss = 0.37 (51.5 examples/sec; 0.078 sec/batch; 2h:20m:00s remains)
INFO - root - 2019-11-06 18:57:46.266826: step 41960, total loss = 3.70, predict loss = 1.06 (59.2 examples/sec; 0.068 sec/batch; 2h:01m:35s remains)
INFO - root - 2019-11-06 18:57:47.019753: step 41970, total loss = 1.54, predict loss = 0.37 (52.4 examples/sec; 0.076 sec/batch; 2h:17m:25s remains)
INFO - root - 2019-11-06 18:57:47.718183: step 41980, total loss = 3.49, predict loss = 0.95 (77.8 examples/sec; 0.051 sec/batch; 1h:32m:32s remains)
INFO - root - 2019-11-06 18:57:48.190086: step 41990, total loss = 2.16, predict loss = 0.53 (94.3 examples/sec; 0.042 sec/batch; 1h:16m:23s remains)
INFO - root - 2019-11-06 18:57:48.627901: step 42000, total loss = 2.88, predict loss = 0.75 (101.9 examples/sec; 0.039 sec/batch; 1h:10m:38s remains)
INFO - root - 2019-11-06 18:57:49.857007: step 42010, total loss = 2.82, predict loss = 0.73 (65.2 examples/sec; 0.061 sec/batch; 1h:50m:28s remains)
INFO - root - 2019-11-06 18:57:50.629498: step 42020, total loss = 2.89, predict loss = 0.77 (63.4 examples/sec; 0.063 sec/batch; 1h:53m:29s remains)
INFO - root - 2019-11-06 18:57:51.339640: step 42030, total loss = 3.64, predict loss = 0.95 (60.6 examples/sec; 0.066 sec/batch; 1h:58m:52s remains)
INFO - root - 2019-11-06 18:57:52.038991: step 42040, total loss = 2.75, predict loss = 0.74 (59.6 examples/sec; 0.067 sec/batch; 2h:00m:48s remains)
INFO - root - 2019-11-06 18:57:52.774408: step 42050, total loss = 4.15, predict loss = 1.18 (65.0 examples/sec; 0.062 sec/batch; 1h:50m:47s remains)
INFO - root - 2019-11-06 18:57:53.362565: step 42060, total loss = 3.21, predict loss = 0.87 (95.8 examples/sec; 0.042 sec/batch; 1h:15m:09s remains)
INFO - root - 2019-11-06 18:57:53.812246: step 42070, total loss = 3.65, predict loss = 0.99 (92.3 examples/sec; 0.043 sec/batch; 1h:17m:56s remains)
INFO - root - 2019-11-06 18:57:54.990811: step 42080, total loss = 1.95, predict loss = 0.50 (69.6 examples/sec; 0.058 sec/batch; 1h:43m:26s remains)
INFO - root - 2019-11-06 18:57:55.696095: step 42090, total loss = 4.17, predict loss = 1.21 (61.4 examples/sec; 0.065 sec/batch; 1h:57m:10s remains)
INFO - root - 2019-11-06 18:57:56.475956: step 42100, total loss = 1.58, predict loss = 0.41 (48.5 examples/sec; 0.083 sec/batch; 2h:28m:23s remains)
INFO - root - 2019-11-06 18:57:57.226463: step 42110, total loss = 2.36, predict loss = 0.63 (60.2 examples/sec; 0.066 sec/batch; 1h:59m:25s remains)
INFO - root - 2019-11-06 18:57:58.024640: step 42120, total loss = 2.44, predict loss = 0.59 (56.8 examples/sec; 0.070 sec/batch; 2h:06m:30s remains)
INFO - root - 2019-11-06 18:57:58.689673: step 42130, total loss = 3.33, predict loss = 0.87 (95.4 examples/sec; 0.042 sec/batch; 1h:15m:22s remains)
INFO - root - 2019-11-06 18:57:59.157674: step 42140, total loss = 3.57, predict loss = 0.98 (94.1 examples/sec; 0.043 sec/batch; 1h:16m:26s remains)
INFO - root - 2019-11-06 18:57:59.606348: step 42150, total loss = 2.08, predict loss = 0.51 (103.3 examples/sec; 0.039 sec/batch; 1h:09m:35s remains)
INFO - root - 2019-11-06 18:58:00.864446: step 42160, total loss = 2.76, predict loss = 0.74 (62.4 examples/sec; 0.064 sec/batch; 1h:55m:10s remains)
INFO - root - 2019-11-06 18:58:01.672568: step 42170, total loss = 1.49, predict loss = 0.42 (58.0 examples/sec; 0.069 sec/batch; 2h:03m:55s remains)
INFO - root - 2019-11-06 18:58:02.454184: step 42180, total loss = 2.45, predict loss = 0.59 (53.5 examples/sec; 0.075 sec/batch; 2h:14m:14s remains)
INFO - root - 2019-11-06 18:58:03.254957: step 42190, total loss = 3.18, predict loss = 0.92 (50.0 examples/sec; 0.080 sec/batch; 2h:23m:41s remains)
INFO - root - 2019-11-06 18:58:04.009199: step 42200, total loss = 3.67, predict loss = 0.95 (70.5 examples/sec; 0.057 sec/batch; 1h:41m:52s remains)
INFO - root - 2019-11-06 18:58:04.542048: step 42210, total loss = 3.73, predict loss = 1.02 (97.0 examples/sec; 0.041 sec/batch; 1h:14m:05s remains)
INFO - root - 2019-11-06 18:58:05.011838: step 42220, total loss = 3.66, predict loss = 1.01 (94.7 examples/sec; 0.042 sec/batch; 1h:15m:52s remains)
INFO - root - 2019-11-06 18:58:06.166786: step 42230, total loss = 3.88, predict loss = 1.07 (68.7 examples/sec; 0.058 sec/batch; 1h:44m:30s remains)
INFO - root - 2019-11-06 18:58:06.885687: step 42240, total loss = 3.09, predict loss = 0.85 (59.0 examples/sec; 0.068 sec/batch; 2h:01m:42s remains)
INFO - root - 2019-11-06 18:58:07.658029: step 42250, total loss = 2.28, predict loss = 0.61 (52.3 examples/sec; 0.076 sec/batch; 2h:17m:14s remains)
INFO - root - 2019-11-06 18:58:08.409271: step 42260, total loss = 2.32, predict loss = 0.56 (61.2 examples/sec; 0.065 sec/batch; 1h:57m:16s remains)
INFO - root - 2019-11-06 18:58:09.097125: step 42270, total loss = 2.62, predict loss = 0.68 (67.8 examples/sec; 0.059 sec/batch; 1h:45m:56s remains)
INFO - root - 2019-11-06 18:58:09.680914: step 42280, total loss = 1.90, predict loss = 0.46 (99.7 examples/sec; 0.040 sec/batch; 1h:12m:02s remains)
INFO - root - 2019-11-06 18:58:10.144910: step 42290, total loss = 2.89, predict loss = 0.69 (91.8 examples/sec; 0.044 sec/batch; 1h:18m:14s remains)
INFO - root - 2019-11-06 18:58:10.628373: step 42300, total loss = 2.58, predict loss = 0.65 (98.0 examples/sec; 0.041 sec/batch; 1h:13m:13s remains)
INFO - root - 2019-11-06 18:58:11.908554: step 42310, total loss = 3.34, predict loss = 0.86 (61.2 examples/sec; 0.065 sec/batch; 1h:57m:17s remains)
INFO - root - 2019-11-06 18:58:12.672003: step 42320, total loss = 3.94, predict loss = 1.13 (60.6 examples/sec; 0.066 sec/batch; 1h:58m:22s remains)
INFO - root - 2019-11-06 18:58:13.415848: step 42330, total loss = 1.62, predict loss = 0.41 (57.7 examples/sec; 0.069 sec/batch; 2h:04m:19s remains)
INFO - root - 2019-11-06 18:58:14.209346: step 42340, total loss = 3.32, predict loss = 0.90 (52.8 examples/sec; 0.076 sec/batch; 2h:15m:48s remains)
INFO - root - 2019-11-06 18:58:15.012692: step 42350, total loss = 2.83, predict loss = 0.81 (61.0 examples/sec; 0.066 sec/batch; 1h:57m:38s remains)
INFO - root - 2019-11-06 18:58:15.539489: step 42360, total loss = 2.96, predict loss = 0.70 (97.9 examples/sec; 0.041 sec/batch; 1h:13m:18s remains)
INFO - root - 2019-11-06 18:58:15.999200: step 42370, total loss = 1.65, predict loss = 0.42 (93.2 examples/sec; 0.043 sec/batch; 1h:17m:00s remains)
INFO - root - 2019-11-06 18:58:17.218376: step 42380, total loss = 1.44, predict loss = 0.36 (75.8 examples/sec; 0.053 sec/batch; 1h:34m:35s remains)
INFO - root - 2019-11-06 18:58:17.954559: step 42390, total loss = 3.38, predict loss = 0.85 (56.1 examples/sec; 0.071 sec/batch; 2h:07m:56s remains)
INFO - root - 2019-11-06 18:58:18.764194: step 42400, total loss = 1.43, predict loss = 0.40 (50.2 examples/sec; 0.080 sec/batch; 2h:22m:46s remains)
INFO - root - 2019-11-06 18:58:19.619831: step 42410, total loss = 2.84, predict loss = 0.79 (47.3 examples/sec; 0.085 sec/batch; 2h:31m:47s remains)
INFO - root - 2019-11-06 18:58:20.330475: step 42420, total loss = 2.06, predict loss = 0.50 (55.7 examples/sec; 0.072 sec/batch; 2h:08m:51s remains)
INFO - root - 2019-11-06 18:58:20.927794: step 42430, total loss = 2.20, predict loss = 0.59 (99.6 examples/sec; 0.040 sec/batch; 1h:12m:00s remains)
INFO - root - 2019-11-06 18:58:21.392775: step 42440, total loss = 2.81, predict loss = 0.76 (95.7 examples/sec; 0.042 sec/batch; 1h:14m:54s remains)
INFO - root - 2019-11-06 18:58:21.841692: step 42450, total loss = 1.95, predict loss = 0.46 (101.1 examples/sec; 0.040 sec/batch; 1h:10m:54s remains)
INFO - root - 2019-11-06 18:58:23.183811: step 42460, total loss = 2.45, predict loss = 0.60 (69.7 examples/sec; 0.057 sec/batch; 1h:42m:50s remains)
INFO - root - 2019-11-06 18:58:23.914780: step 42470, total loss = 3.11, predict loss = 0.82 (60.4 examples/sec; 0.066 sec/batch; 1h:58m:35s remains)
INFO - root - 2019-11-06 18:58:24.696959: step 42480, total loss = 3.30, predict loss = 0.85 (60.1 examples/sec; 0.067 sec/batch; 1h:59m:14s remains)
INFO - root - 2019-11-06 18:58:25.471717: step 42490, total loss = 3.57, predict loss = 1.04 (55.8 examples/sec; 0.072 sec/batch; 2h:08m:25s remains)
INFO - root - 2019-11-06 18:58:26.171133: step 42500, total loss = 2.40, predict loss = 0.60 (79.1 examples/sec; 0.051 sec/batch; 1h:30m:35s remains)
INFO - root - 2019-11-06 18:58:26.637854: step 42510, total loss = 2.82, predict loss = 0.68 (98.0 examples/sec; 0.041 sec/batch; 1h:13m:06s remains)
INFO - root - 2019-11-06 18:58:27.093472: step 42520, total loss = 3.05, predict loss = 0.83 (96.4 examples/sec; 0.041 sec/batch; 1h:14m:18s remains)
INFO - root - 2019-11-06 18:58:28.360487: step 42530, total loss = 2.68, predict loss = 0.70 (65.7 examples/sec; 0.061 sec/batch; 1h:49m:06s remains)
INFO - root - 2019-11-06 18:58:29.103585: step 42540, total loss = 2.11, predict loss = 0.56 (60.5 examples/sec; 0.066 sec/batch; 1h:58m:19s remains)
INFO - root - 2019-11-06 18:58:29.908378: step 42550, total loss = 3.52, predict loss = 0.97 (53.5 examples/sec; 0.075 sec/batch; 2h:13m:46s remains)
INFO - root - 2019-11-06 18:58:30.627846: step 42560, total loss = 1.99, predict loss = 0.51 (57.1 examples/sec; 0.070 sec/batch; 2h:05m:24s remains)
INFO - root - 2019-11-06 18:58:31.426673: step 42570, total loss = 3.85, predict loss = 1.04 (52.4 examples/sec; 0.076 sec/batch; 2h:16m:44s remains)
INFO - root - 2019-11-06 18:58:32.026723: step 42580, total loss = 3.99, predict loss = 1.05 (93.1 examples/sec; 0.043 sec/batch; 1h:16m:53s remains)
INFO - root - 2019-11-06 18:58:32.482009: step 42590, total loss = 3.50, predict loss = 1.05 (97.3 examples/sec; 0.041 sec/batch; 1h:13m:37s remains)
INFO - root - 2019-11-06 18:58:32.925656: step 42600, total loss = 2.64, predict loss = 0.63 (129.8 examples/sec; 0.031 sec/batch; 0h:55m:10s remains)
INFO - root - 2019-11-06 18:58:34.309033: step 42610, total loss = 1.89, predict loss = 0.46 (56.0 examples/sec; 0.071 sec/batch; 2h:07m:55s remains)
INFO - root - 2019-11-06 18:58:35.055841: step 42620, total loss = 3.14, predict loss = 0.84 (67.0 examples/sec; 0.060 sec/batch; 1h:46m:48s remains)
INFO - root - 2019-11-06 18:58:35.761322: step 42630, total loss = 1.78, predict loss = 0.48 (68.6 examples/sec; 0.058 sec/batch; 1h:44m:17s remains)
INFO - root - 2019-11-06 18:58:36.436796: step 42640, total loss = 3.66, predict loss = 1.04 (61.6 examples/sec; 0.065 sec/batch; 1h:56m:16s remains)
INFO - root - 2019-11-06 18:58:37.129759: step 42650, total loss = 3.35, predict loss = 0.85 (75.4 examples/sec; 0.053 sec/batch; 1h:34m:53s remains)
INFO - root - 2019-11-06 18:58:37.629847: step 42660, total loss = 2.93, predict loss = 0.81 (94.7 examples/sec; 0.042 sec/batch; 1h:15m:32s remains)
INFO - root - 2019-11-06 18:58:38.078027: step 42670, total loss = 2.66, predict loss = 0.70 (97.2 examples/sec; 0.041 sec/batch; 1h:13m:37s remains)
INFO - root - 2019-11-06 18:58:39.305742: step 42680, total loss = 1.67, predict loss = 0.39 (68.5 examples/sec; 0.058 sec/batch; 1h:44m:26s remains)
INFO - root - 2019-11-06 18:58:40.042938: step 42690, total loss = 2.64, predict loss = 0.67 (53.1 examples/sec; 0.075 sec/batch; 2h:14m:43s remains)
INFO - root - 2019-11-06 18:58:40.820145: step 42700, total loss = 2.78, predict loss = 0.68 (52.6 examples/sec; 0.076 sec/batch; 2h:16m:02s remains)
INFO - root - 2019-11-06 18:58:41.578079: step 42710, total loss = 1.87, predict loss = 0.49 (60.3 examples/sec; 0.066 sec/batch; 1h:58m:34s remains)
INFO - root - 2019-11-06 18:58:42.306580: step 42720, total loss = 2.79, predict loss = 0.82 (75.7 examples/sec; 0.053 sec/batch; 1h:34m:29s remains)
INFO - root - 2019-11-06 18:58:42.901246: step 42730, total loss = 1.90, predict loss = 0.45 (93.7 examples/sec; 0.043 sec/batch; 1h:16m:21s remains)
INFO - root - 2019-11-06 18:58:43.397885: step 42740, total loss = 2.81, predict loss = 0.68 (95.2 examples/sec; 0.042 sec/batch; 1h:15m:06s remains)
INFO - root - 2019-11-06 18:58:44.483273: step 42750, total loss = 2.13, predict loss = 0.56 (5.8 examples/sec; 0.689 sec/batch; 20h:31m:10s remains)
INFO - root - 2019-11-06 18:58:45.159498: step 42760, total loss = 2.24, predict loss = 0.59 (58.0 examples/sec; 0.069 sec/batch; 2h:03m:19s remains)
INFO - root - 2019-11-06 18:58:45.907990: step 42770, total loss = 2.99, predict loss = 0.75 (62.2 examples/sec; 0.064 sec/batch; 1h:54m:55s remains)
INFO - root - 2019-11-06 18:58:46.680231: step 42780, total loss = 3.00, predict loss = 0.80 (58.2 examples/sec; 0.069 sec/batch; 2h:02m:52s remains)
INFO - root - 2019-11-06 18:58:47.494819: step 42790, total loss = 1.97, predict loss = 0.46 (56.9 examples/sec; 0.070 sec/batch; 2h:05m:36s remains)
INFO - root - 2019-11-06 18:58:48.215434: step 42800, total loss = 2.96, predict loss = 0.79 (86.0 examples/sec; 0.047 sec/batch; 1h:23m:06s remains)
INFO - root - 2019-11-06 18:58:48.683713: step 42810, total loss = 3.25, predict loss = 0.93 (97.7 examples/sec; 0.041 sec/batch; 1h:13m:07s remains)
INFO - root - 2019-11-06 18:58:49.159906: step 42820, total loss = 2.00, predict loss = 0.45 (96.3 examples/sec; 0.042 sec/batch; 1h:14m:12s remains)
INFO - root - 2019-11-06 18:58:50.389797: step 42830, total loss = 4.25, predict loss = 1.23 (55.3 examples/sec; 0.072 sec/batch; 2h:09m:14s remains)
INFO - root - 2019-11-06 18:58:51.256234: step 42840, total loss = 3.71, predict loss = 1.00 (51.3 examples/sec; 0.078 sec/batch; 2h:19m:08s remains)
INFO - root - 2019-11-06 18:58:51.994931: step 42850, total loss = 4.05, predict loss = 1.12 (69.5 examples/sec; 0.058 sec/batch; 1h:42m:49s remains)
INFO - root - 2019-11-06 18:58:52.739284: step 42860, total loss = 1.78, predict loss = 0.46 (58.3 examples/sec; 0.069 sec/batch; 2h:02m:35s remains)
INFO - root - 2019-11-06 18:58:53.533552: step 42870, total loss = 2.63, predict loss = 0.66 (61.1 examples/sec; 0.065 sec/batch; 1h:56m:54s remains)
INFO - root - 2019-11-06 18:58:54.072435: step 42880, total loss = 1.77, predict loss = 0.43 (100.6 examples/sec; 0.040 sec/batch; 1h:11m:00s remains)
INFO - root - 2019-11-06 18:58:54.507954: step 42890, total loss = 1.61, predict loss = 0.41 (104.9 examples/sec; 0.038 sec/batch; 1h:08m:03s remains)
INFO - root - 2019-11-06 18:58:55.673835: step 42900, total loss = 1.81, predict loss = 0.42 (70.5 examples/sec; 0.057 sec/batch; 1h:41m:14s remains)
INFO - root - 2019-11-06 18:58:56.465225: step 42910, total loss = 3.27, predict loss = 0.87 (51.3 examples/sec; 0.078 sec/batch; 2h:19m:05s remains)
INFO - root - 2019-11-06 18:58:57.206213: step 42920, total loss = 2.36, predict loss = 0.61 (61.2 examples/sec; 0.065 sec/batch; 1h:56m:36s remains)
INFO - root - 2019-11-06 18:58:57.995367: step 42930, total loss = 1.81, predict loss = 0.48 (61.7 examples/sec; 0.065 sec/batch; 1h:55m:36s remains)
INFO - root - 2019-11-06 18:58:58.769757: step 42940, total loss = 3.10, predict loss = 0.79 (58.6 examples/sec; 0.068 sec/batch; 2h:01m:48s remains)
INFO - root - 2019-11-06 18:58:59.441650: step 42950, total loss = 2.01, predict loss = 0.53 (93.5 examples/sec; 0.043 sec/batch; 1h:16m:18s remains)
INFO - root - 2019-11-06 18:58:59.876361: step 42960, total loss = 2.56, predict loss = 0.62 (97.7 examples/sec; 0.041 sec/batch; 1h:13m:03s remains)
INFO - root - 2019-11-06 18:59:00.340745: step 42970, total loss = 2.40, predict loss = 0.70 (89.9 examples/sec; 0.044 sec/batch; 1h:19m:21s remains)
INFO - root - 2019-11-06 18:59:01.640603: step 42980, total loss = 2.94, predict loss = 0.71 (56.9 examples/sec; 0.070 sec/batch; 2h:05m:24s remains)
INFO - root - 2019-11-06 18:59:02.368245: step 42990, total loss = 2.65, predict loss = 0.65 (60.2 examples/sec; 0.066 sec/batch; 1h:58m:35s remains)
INFO - root - 2019-11-06 18:59:03.147008: step 43000, total loss = 1.43, predict loss = 0.38 (55.6 examples/sec; 0.072 sec/batch; 2h:08m:13s remains)
INFO - root - 2019-11-06 18:59:03.891026: step 43010, total loss = 3.52, predict loss = 0.92 (56.1 examples/sec; 0.071 sec/batch; 2h:07m:15s remains)
INFO - root - 2019-11-06 18:59:04.639686: step 43020, total loss = 4.07, predict loss = 1.22 (72.8 examples/sec; 0.055 sec/batch; 1h:37m:54s remains)
INFO - root - 2019-11-06 18:59:05.169798: step 43030, total loss = 1.83, predict loss = 0.41 (93.8 examples/sec; 0.043 sec/batch; 1h:15m:59s remains)
INFO - root - 2019-11-06 18:59:05.627469: step 43040, total loss = 2.53, predict loss = 0.62 (94.0 examples/sec; 0.043 sec/batch; 1h:15m:49s remains)
INFO - root - 2019-11-06 18:59:06.792061: step 43050, total loss = 2.52, predict loss = 0.68 (73.4 examples/sec; 0.054 sec/batch; 1h:37m:07s remains)
INFO - root - 2019-11-06 18:59:07.511388: step 43060, total loss = 3.12, predict loss = 0.74 (61.9 examples/sec; 0.065 sec/batch; 1h:55m:05s remains)
INFO - root - 2019-11-06 18:59:08.268434: step 43070, total loss = 1.41, predict loss = 0.37 (59.8 examples/sec; 0.067 sec/batch; 1h:59m:09s remains)
INFO - root - 2019-11-06 18:59:09.011152: step 43080, total loss = 2.24, predict loss = 0.51 (57.7 examples/sec; 0.069 sec/batch; 2h:03m:26s remains)
INFO - root - 2019-11-06 18:59:09.802883: step 43090, total loss = 2.76, predict loss = 0.69 (55.5 examples/sec; 0.072 sec/batch; 2h:08m:23s remains)
INFO - root - 2019-11-06 18:59:10.438815: step 43100, total loss = 2.74, predict loss = 0.72 (102.4 examples/sec; 0.039 sec/batch; 1h:09m:35s remains)
INFO - root - 2019-11-06 18:59:10.886514: step 43110, total loss = 4.02, predict loss = 1.08 (98.1 examples/sec; 0.041 sec/batch; 1h:12m:38s remains)
INFO - root - 2019-11-06 18:59:11.336984: step 43120, total loss = 2.80, predict loss = 0.71 (95.8 examples/sec; 0.042 sec/batch; 1h:14m:21s remains)
INFO - root - 2019-11-06 18:59:12.616924: step 43130, total loss = 2.53, predict loss = 0.61 (61.1 examples/sec; 0.065 sec/batch; 1h:56m:39s remains)
INFO - root - 2019-11-06 18:59:13.384610: step 43140, total loss = 1.73, predict loss = 0.40 (54.6 examples/sec; 0.073 sec/batch; 2h:10m:29s remains)
INFO - root - 2019-11-06 18:59:14.148938: step 43150, total loss = 3.08, predict loss = 0.84 (55.7 examples/sec; 0.072 sec/batch; 2h:07m:52s remains)
INFO - root - 2019-11-06 18:59:14.920951: step 43160, total loss = 2.36, predict loss = 0.62 (56.6 examples/sec; 0.071 sec/batch; 2h:05m:54s remains)
INFO - root - 2019-11-06 18:59:15.652096: step 43170, total loss = 1.96, predict loss = 0.49 (67.8 examples/sec; 0.059 sec/batch; 1h:44m:58s remains)
INFO - root - 2019-11-06 18:59:16.174484: step 43180, total loss = 3.09, predict loss = 0.83 (94.6 examples/sec; 0.042 sec/batch; 1h:15m:15s remains)
INFO - root - 2019-11-06 18:59:16.628510: step 43190, total loss = 2.26, predict loss = 0.58 (89.1 examples/sec; 0.045 sec/batch; 1h:19m:55s remains)
INFO - root - 2019-11-06 18:59:17.794509: step 43200, total loss = 2.77, predict loss = 0.71 (67.9 examples/sec; 0.059 sec/batch; 1h:44m:50s remains)
INFO - root - 2019-11-06 18:59:18.540775: step 43210, total loss = 2.60, predict loss = 0.64 (63.3 examples/sec; 0.063 sec/batch; 1h:52m:23s remains)
INFO - root - 2019-11-06 18:59:19.248249: step 43220, total loss = 3.52, predict loss = 1.05 (64.8 examples/sec; 0.062 sec/batch; 1h:49m:53s remains)
INFO - root - 2019-11-06 18:59:19.983785: step 43230, total loss = 3.23, predict loss = 0.92 (55.9 examples/sec; 0.072 sec/batch; 2h:07m:14s remains)
INFO - root - 2019-11-06 18:59:20.702866: step 43240, total loss = 2.85, predict loss = 0.77 (59.1 examples/sec; 0.068 sec/batch; 2h:00m:24s remains)
INFO - root - 2019-11-06 18:59:21.268405: step 43250, total loss = 1.82, predict loss = 0.48 (103.1 examples/sec; 0.039 sec/batch; 1h:09m:00s remains)
INFO - root - 2019-11-06 18:59:21.741115: step 43260, total loss = 3.66, predict loss = 1.09 (94.0 examples/sec; 0.043 sec/batch; 1h:15m:44s remains)
INFO - root - 2019-11-06 18:59:22.193913: step 43270, total loss = 2.53, predict loss = 0.67 (91.1 examples/sec; 0.044 sec/batch; 1h:18m:05s remains)
INFO - root - 2019-11-06 18:59:23.548485: step 43280, total loss = 2.42, predict loss = 0.60 (55.7 examples/sec; 0.072 sec/batch; 2h:07m:50s remains)
INFO - root - 2019-11-06 18:59:24.301567: step 43290, total loss = 1.63, predict loss = 0.38 (55.0 examples/sec; 0.073 sec/batch; 2h:09m:22s remains)
INFO - root - 2019-11-06 18:59:25.088777: step 43300, total loss = 3.95, predict loss = 1.21 (58.8 examples/sec; 0.068 sec/batch; 2h:00m:52s remains)
INFO - root - 2019-11-06 18:59:25.812815: step 43310, total loss = 2.57, predict loss = 0.70 (61.1 examples/sec; 0.065 sec/batch; 1h:56m:23s remains)
INFO - root - 2019-11-06 18:59:26.557740: step 43320, total loss = 3.69, predict loss = 1.02 (62.9 examples/sec; 0.064 sec/batch; 1h:53m:01s remains)
INFO - root - 2019-11-06 18:59:27.064910: step 43330, total loss = 3.85, predict loss = 1.09 (90.9 examples/sec; 0.044 sec/batch; 1h:18m:16s remains)
INFO - root - 2019-11-06 18:59:27.549846: step 43340, total loss = 3.76, predict loss = 1.13 (86.2 examples/sec; 0.046 sec/batch; 1h:22m:29s remains)
INFO - root - 2019-11-06 18:59:28.758726: step 43350, total loss = 2.42, predict loss = 0.57 (67.5 examples/sec; 0.059 sec/batch; 1h:45m:15s remains)
INFO - root - 2019-11-06 18:59:29.486190: step 43360, total loss = 1.96, predict loss = 0.50 (61.1 examples/sec; 0.066 sec/batch; 1h:56m:26s remains)
INFO - root - 2019-11-06 18:59:30.250654: step 43370, total loss = 3.65, predict loss = 1.05 (53.4 examples/sec; 0.075 sec/batch; 2h:13m:05s remains)
INFO - root - 2019-11-06 18:59:31.025891: step 43380, total loss = 2.18, predict loss = 0.59 (56.2 examples/sec; 0.071 sec/batch; 2h:06m:28s remains)
INFO - root - 2019-11-06 18:59:31.780564: step 43390, total loss = 2.35, predict loss = 0.56 (58.0 examples/sec; 0.069 sec/batch; 2h:02m:32s remains)
INFO - root - 2019-11-06 18:59:32.346799: step 43400, total loss = 3.04, predict loss = 0.83 (106.1 examples/sec; 0.038 sec/batch; 1h:06m:57s remains)
INFO - root - 2019-11-06 18:59:32.790630: step 43410, total loss = 3.85, predict loss = 1.07 (90.3 examples/sec; 0.044 sec/batch; 1h:18m:43s remains)
INFO - root - 2019-11-06 18:59:33.276439: step 43420, total loss = 2.41, predict loss = 0.63 (119.7 examples/sec; 0.033 sec/batch; 0h:59m:22s remains)
INFO - root - 2019-11-06 18:59:34.607620: step 43430, total loss = 1.90, predict loss = 0.47 (66.4 examples/sec; 0.060 sec/batch; 1h:47m:00s remains)
INFO - root - 2019-11-06 18:59:35.318276: step 43440, total loss = 2.83, predict loss = 0.69 (64.5 examples/sec; 0.062 sec/batch; 1h:50m:05s remains)
INFO - root - 2019-11-06 18:59:36.090899: step 43450, total loss = 1.77, predict loss = 0.43 (60.0 examples/sec; 0.067 sec/batch; 1h:58m:17s remains)
INFO - root - 2019-11-06 18:59:36.836932: step 43460, total loss = 3.25, predict loss = 0.85 (50.8 examples/sec; 0.079 sec/batch; 2h:19m:51s remains)
INFO - root - 2019-11-06 18:59:37.564155: step 43470, total loss = 4.07, predict loss = 1.19 (66.7 examples/sec; 0.060 sec/batch; 1h:46m:31s remains)
INFO - root - 2019-11-06 18:59:38.072647: step 43480, total loss = 3.85, predict loss = 1.09 (93.0 examples/sec; 0.043 sec/batch; 1h:16m:19s remains)
INFO - root - 2019-11-06 18:59:38.519385: step 43490, total loss = 3.30, predict loss = 0.95 (92.6 examples/sec; 0.043 sec/batch; 1h:16m:40s remains)
INFO - root - 2019-11-06 18:59:39.770164: step 43500, total loss = 1.77, predict loss = 0.41 (70.7 examples/sec; 0.057 sec/batch; 1h:40m:25s remains)
INFO - root - 2019-11-06 18:59:40.499523: step 43510, total loss = 2.73, predict loss = 0.66 (56.7 examples/sec; 0.071 sec/batch; 2h:05m:12s remains)
INFO - root - 2019-11-06 18:59:41.232399: step 43520, total loss = 2.31, predict loss = 0.71 (62.7 examples/sec; 0.064 sec/batch; 1h:53m:13s remains)
INFO - root - 2019-11-06 18:59:41.952342: step 43530, total loss = 3.72, predict loss = 1.08 (59.9 examples/sec; 0.067 sec/batch; 1h:58m:25s remains)
INFO - root - 2019-11-06 18:59:42.664306: step 43540, total loss = 2.32, predict loss = 0.57 (64.2 examples/sec; 0.062 sec/batch; 1h:50m:29s remains)
INFO - root - 2019-11-06 18:59:43.186265: step 43550, total loss = 1.67, predict loss = 0.47 (92.3 examples/sec; 0.043 sec/batch; 1h:16m:53s remains)
INFO - root - 2019-11-06 18:59:43.655906: step 43560, total loss = 3.37, predict loss = 0.96 (92.3 examples/sec; 0.043 sec/batch; 1h:16m:52s remains)
INFO - root - 2019-11-06 18:59:44.774156: step 43570, total loss = 2.98, predict loss = 0.74 (5.6 examples/sec; 0.716 sec/batch; 21h:10m:41s remains)
INFO - root - 2019-11-06 18:59:45.487283: step 43580, total loss = 2.26, predict loss = 0.56 (58.1 examples/sec; 0.069 sec/batch; 2h:02m:01s remains)
INFO - root - 2019-11-06 18:59:46.216601: step 43590, total loss = 2.53, predict loss = 0.63 (56.9 examples/sec; 0.070 sec/batch; 2h:04m:44s remains)
INFO - root - 2019-11-06 18:59:46.908938: step 43600, total loss = 2.21, predict loss = 0.56 (71.6 examples/sec; 0.056 sec/batch; 1h:39m:05s remains)
INFO - root - 2019-11-06 18:59:47.635650: step 43610, total loss = 3.01, predict loss = 0.76 (50.3 examples/sec; 0.079 sec/batch; 2h:20m:56s remains)
INFO - root - 2019-11-06 18:59:48.347859: step 43620, total loss = 3.11, predict loss = 0.84 (82.9 examples/sec; 0.048 sec/batch; 1h:25m:35s remains)
INFO - root - 2019-11-06 18:59:48.823356: step 43630, total loss = 2.73, predict loss = 0.74 (90.4 examples/sec; 0.044 sec/batch; 1h:18m:25s remains)
INFO - root - 2019-11-06 18:59:49.301002: step 43640, total loss = 2.64, predict loss = 0.70 (90.7 examples/sec; 0.044 sec/batch; 1h:18m:08s remains)
INFO - root - 2019-11-06 18:59:50.556631: step 43650, total loss = 2.11, predict loss = 0.59 (67.1 examples/sec; 0.060 sec/batch; 1h:45m:36s remains)
INFO - root - 2019-11-06 18:59:51.310154: step 43660, total loss = 2.50, predict loss = 0.65 (60.9 examples/sec; 0.066 sec/batch; 1h:56m:25s remains)
INFO - root - 2019-11-06 18:59:52.141052: step 43670, total loss = 2.99, predict loss = 0.80 (52.1 examples/sec; 0.077 sec/batch; 2h:15m:56s remains)
INFO - root - 2019-11-06 18:59:52.941560: step 43680, total loss = 2.60, predict loss = 0.62 (51.7 examples/sec; 0.077 sec/batch; 2h:17m:10s remains)
INFO - root - 2019-11-06 18:59:53.769764: step 43690, total loss = 3.38, predict loss = 0.90 (67.7 examples/sec; 0.059 sec/batch; 1h:44m:39s remains)
INFO - root - 2019-11-06 18:59:54.356864: step 43700, total loss = 3.65, predict loss = 1.05 (95.4 examples/sec; 0.042 sec/batch; 1h:14m:16s remains)
INFO - root - 2019-11-06 18:59:54.818109: step 43710, total loss = 2.83, predict loss = 0.79 (93.2 examples/sec; 0.043 sec/batch; 1h:16m:01s remains)
INFO - root - 2019-11-06 18:59:55.935328: step 43720, total loss = 2.63, predict loss = 0.69 (63.5 examples/sec; 0.063 sec/batch; 1h:51m:31s remains)
INFO - root - 2019-11-06 18:59:56.611365: step 43730, total loss = 2.51, predict loss = 0.65 (65.6 examples/sec; 0.061 sec/batch; 1h:47m:57s remains)
INFO - root - 2019-11-06 18:59:57.390157: step 43740, total loss = 2.97, predict loss = 0.76 (55.3 examples/sec; 0.072 sec/batch; 2h:08m:11s remains)
INFO - root - 2019-11-06 18:59:58.148655: step 43750, total loss = 2.79, predict loss = 0.72 (61.0 examples/sec; 0.066 sec/batch; 1h:56m:01s remains)
INFO - root - 2019-11-06 18:59:58.911622: step 43760, total loss = 2.93, predict loss = 0.74 (58.1 examples/sec; 0.069 sec/batch; 2h:01m:59s remains)
INFO - root - 2019-11-06 18:59:59.526620: step 43770, total loss = 1.73, predict loss = 0.43 (97.3 examples/sec; 0.041 sec/batch; 1h:12m:48s remains)
INFO - root - 2019-11-06 19:00:00.002040: step 43780, total loss = 3.40, predict loss = 0.97 (100.7 examples/sec; 0.040 sec/batch; 1h:10m:20s remains)
INFO - root - 2019-11-06 19:00:00.454977: step 43790, total loss = 2.69, predict loss = 0.71 (89.7 examples/sec; 0.045 sec/batch; 1h:18m:56s remains)
INFO - root - 2019-11-06 19:00:01.730354: step 43800, total loss = 3.83, predict loss = 1.02 (63.3 examples/sec; 0.063 sec/batch; 1h:51m:47s remains)
INFO - root - 2019-11-06 19:00:02.510167: step 43810, total loss = 3.33, predict loss = 0.92 (58.9 examples/sec; 0.068 sec/batch; 2h:00m:07s remains)
INFO - root - 2019-11-06 19:00:03.248804: step 43820, total loss = 2.56, predict loss = 0.63 (60.8 examples/sec; 0.066 sec/batch; 1h:56m:20s remains)
INFO - root - 2019-11-06 19:00:03.994587: step 43830, total loss = 1.87, predict loss = 0.49 (58.6 examples/sec; 0.068 sec/batch; 2h:00m:49s remains)
INFO - root - 2019-11-06 19:00:04.719942: step 43840, total loss = 2.42, predict loss = 0.60 (72.6 examples/sec; 0.055 sec/batch; 1h:37m:30s remains)
INFO - root - 2019-11-06 19:00:05.251676: step 43850, total loss = 1.96, predict loss = 0.48 (92.4 examples/sec; 0.043 sec/batch; 1h:16m:34s remains)
INFO - root - 2019-11-06 19:00:05.746829: step 43860, total loss = 2.14, predict loss = 0.54 (97.5 examples/sec; 0.041 sec/batch; 1h:12m:32s remains)
INFO - root - 2019-11-06 19:00:06.882015: step 43870, total loss = 3.52, predict loss = 0.95 (69.3 examples/sec; 0.058 sec/batch; 1h:42m:07s remains)
INFO - root - 2019-11-06 19:00:07.577229: step 43880, total loss = 2.84, predict loss = 0.73 (61.9 examples/sec; 0.065 sec/batch; 1h:54m:22s remains)
INFO - root - 2019-11-06 19:00:08.334117: step 43890, total loss = 1.96, predict loss = 0.46 (57.8 examples/sec; 0.069 sec/batch; 2h:02m:27s remains)
INFO - root - 2019-11-06 19:00:09.102861: step 43900, total loss = 2.04, predict loss = 0.50 (57.5 examples/sec; 0.070 sec/batch; 2h:02m:59s remains)
INFO - root - 2019-11-06 19:00:09.892284: step 43910, total loss = 3.06, predict loss = 0.78 (57.4 examples/sec; 0.070 sec/batch; 2h:03m:09s remains)
INFO - root - 2019-11-06 19:00:10.494414: step 43920, total loss = 2.91, predict loss = 0.81 (99.1 examples/sec; 0.040 sec/batch; 1h:11m:22s remains)
INFO - root - 2019-11-06 19:00:10.938936: step 43930, total loss = 2.95, predict loss = 0.78 (97.0 examples/sec; 0.041 sec/batch; 1h:12m:53s remains)
INFO - root - 2019-11-06 19:00:11.403876: step 43940, total loss = 2.12, predict loss = 0.50 (93.4 examples/sec; 0.043 sec/batch; 1h:15m:41s remains)
INFO - root - 2019-11-06 19:00:12.671170: step 43950, total loss = 3.62, predict loss = 1.06 (61.2 examples/sec; 0.065 sec/batch; 1h:55m:35s remains)
INFO - root - 2019-11-06 19:00:13.430195: step 43960, total loss = 1.88, predict loss = 0.49 (58.2 examples/sec; 0.069 sec/batch; 2h:01m:28s remains)
INFO - root - 2019-11-06 19:00:14.164440: step 43970, total loss = 3.75, predict loss = 1.12 (56.0 examples/sec; 0.071 sec/batch; 2h:06m:19s remains)
INFO - root - 2019-11-06 19:00:14.938294: step 43980, total loss = 3.39, predict loss = 0.94 (58.9 examples/sec; 0.068 sec/batch; 1h:59m:58s remains)
INFO - root - 2019-11-06 19:00:15.614303: step 43990, total loss = 2.61, predict loss = 0.67 (59.6 examples/sec; 0.067 sec/batch; 1h:58m:34s remains)
INFO - root - 2019-11-06 19:00:16.100761: step 44000, total loss = 1.88, predict loss = 0.44 (94.1 examples/sec; 0.042 sec/batch; 1h:15m:04s remains)
INFO - root - 2019-11-06 19:00:16.557656: step 44010, total loss = 2.11, predict loss = 0.52 (101.8 examples/sec; 0.039 sec/batch; 1h:09m:22s remains)
INFO - root - 2019-11-06 19:00:17.757108: step 44020, total loss = 2.84, predict loss = 0.73 (67.9 examples/sec; 0.059 sec/batch; 1h:43m:58s remains)
INFO - root - 2019-11-06 19:00:18.472386: step 44030, total loss = 3.14, predict loss = 0.87 (60.2 examples/sec; 0.066 sec/batch; 1h:57m:23s remains)
INFO - root - 2019-11-06 19:00:19.265001: step 44040, total loss = 2.52, predict loss = 0.65 (62.0 examples/sec; 0.065 sec/batch; 1h:54m:00s remains)
INFO - root - 2019-11-06 19:00:19.983122: step 44050, total loss = 3.54, predict loss = 0.99 (57.2 examples/sec; 0.070 sec/batch; 2h:03m:26s remains)
INFO - root - 2019-11-06 19:00:20.731959: step 44060, total loss = 3.21, predict loss = 0.95 (58.2 examples/sec; 0.069 sec/batch; 2h:01m:20s remains)
INFO - root - 2019-11-06 19:00:21.331766: step 44070, total loss = 2.58, predict loss = 0.63 (99.6 examples/sec; 0.040 sec/batch; 1h:10m:55s remains)
INFO - root - 2019-11-06 19:00:21.794852: step 44080, total loss = 4.29, predict loss = 1.28 (94.3 examples/sec; 0.042 sec/batch; 1h:14m:55s remains)
INFO - root - 2019-11-06 19:00:22.256635: step 44090, total loss = 2.91, predict loss = 0.75 (89.4 examples/sec; 0.045 sec/batch; 1h:19m:00s remains)
INFO - root - 2019-11-06 19:00:23.606136: step 44100, total loss = 2.89, predict loss = 0.76 (59.8 examples/sec; 0.067 sec/batch; 1h:58m:03s remains)
INFO - root - 2019-11-06 19:00:24.372230: step 44110, total loss = 2.99, predict loss = 0.82 (50.4 examples/sec; 0.079 sec/batch; 2h:19m:57s remains)
INFO - root - 2019-11-06 19:00:25.143302: step 44120, total loss = 2.89, predict loss = 0.77 (57.3 examples/sec; 0.070 sec/batch; 2h:03m:16s remains)
INFO - root - 2019-11-06 19:00:25.948566: step 44130, total loss = 2.95, predict loss = 0.93 (48.0 examples/sec; 0.083 sec/batch; 2h:26m:58s remains)
INFO - root - 2019-11-06 19:00:26.670837: step 44140, total loss = 2.89, predict loss = 0.75 (70.0 examples/sec; 0.057 sec/batch; 1h:40m:51s remains)
INFO - root - 2019-11-06 19:00:27.152084: step 44150, total loss = 3.04, predict loss = 0.77 (90.9 examples/sec; 0.044 sec/batch; 1h:17m:39s remains)
INFO - root - 2019-11-06 19:00:27.619036: step 44160, total loss = 2.43, predict loss = 0.67 (90.9 examples/sec; 0.044 sec/batch; 1h:17m:39s remains)
INFO - root - 2019-11-06 19:00:28.803842: step 44170, total loss = 3.55, predict loss = 1.06 (62.9 examples/sec; 0.064 sec/batch; 1h:52m:08s remains)
INFO - root - 2019-11-06 19:00:29.521657: step 44180, total loss = 2.79, predict loss = 0.70 (62.1 examples/sec; 0.064 sec/batch; 1h:53m:38s remains)
INFO - root - 2019-11-06 19:00:30.231659: step 44190, total loss = 2.50, predict loss = 0.71 (55.5 examples/sec; 0.072 sec/batch; 2h:07m:01s remains)
INFO - root - 2019-11-06 19:00:30.968194: step 44200, total loss = 2.40, predict loss = 0.60 (58.5 examples/sec; 0.068 sec/batch; 2h:00m:36s remains)
INFO - root - 2019-11-06 19:00:31.694652: step 44210, total loss = 2.60, predict loss = 0.68 (62.3 examples/sec; 0.064 sec/batch; 1h:53m:14s remains)
INFO - root - 2019-11-06 19:00:32.267734: step 44220, total loss = 1.73, predict loss = 0.45 (99.2 examples/sec; 0.040 sec/batch; 1h:11m:07s remains)
INFO - root - 2019-11-06 19:00:32.733293: step 44230, total loss = 3.76, predict loss = 1.11 (97.1 examples/sec; 0.041 sec/batch; 1h:12m:36s remains)
INFO - root - 2019-11-06 19:00:33.185487: step 44240, total loss = 2.82, predict loss = 0.84 (125.2 examples/sec; 0.032 sec/batch; 0h:56m:19s remains)
INFO - root - 2019-11-06 19:00:34.546760: step 44250, total loss = 3.59, predict loss = 0.97 (51.6 examples/sec; 0.078 sec/batch; 2h:16m:45s remains)
INFO - root - 2019-11-06 19:00:35.337748: step 44260, total loss = 1.85, predict loss = 0.43 (50.4 examples/sec; 0.079 sec/batch; 2h:19m:58s remains)
INFO - root - 2019-11-06 19:00:36.155730: step 44270, total loss = 3.18, predict loss = 0.85 (47.1 examples/sec; 0.085 sec/batch; 2h:29m:30s remains)
INFO - root - 2019-11-06 19:00:36.905295: step 44280, total loss = 2.56, predict loss = 0.59 (58.5 examples/sec; 0.068 sec/batch; 2h:00m:23s remains)
INFO - root - 2019-11-06 19:00:37.585555: step 44290, total loss = 2.53, predict loss = 0.65 (75.4 examples/sec; 0.053 sec/batch; 1h:33m:30s remains)
INFO - root - 2019-11-06 19:00:38.108058: step 44300, total loss = 3.99, predict loss = 1.18 (88.2 examples/sec; 0.045 sec/batch; 1h:19m:54s remains)
INFO - root - 2019-11-06 19:00:38.581962: step 44310, total loss = 3.34, predict loss = 0.89 (93.8 examples/sec; 0.043 sec/batch; 1h:15m:05s remains)
INFO - root - 2019-11-06 19:00:39.823489: step 44320, total loss = 1.93, predict loss = 0.48 (58.6 examples/sec; 0.068 sec/batch; 2h:00m:13s remains)
INFO - root - 2019-11-06 19:00:40.531020: step 44330, total loss = 2.29, predict loss = 0.58 (60.8 examples/sec; 0.066 sec/batch; 1h:55m:48s remains)
INFO - root - 2019-11-06 19:00:41.280770: step 44340, total loss = 4.20, predict loss = 1.30 (59.3 examples/sec; 0.067 sec/batch; 1h:58m:47s remains)
INFO - root - 2019-11-06 19:00:41.994207: step 44350, total loss = 2.75, predict loss = 0.79 (64.3 examples/sec; 0.062 sec/batch; 1h:49m:33s remains)
INFO - root - 2019-11-06 19:00:42.764356: step 44360, total loss = 3.03, predict loss = 0.83 (64.5 examples/sec; 0.062 sec/batch; 1h:49m:11s remains)
INFO - root - 2019-11-06 19:00:43.356296: step 44370, total loss = 4.36, predict loss = 1.28 (96.5 examples/sec; 0.041 sec/batch; 1h:13m:00s remains)
INFO - root - 2019-11-06 19:00:43.850717: step 44380, total loss = 3.16, predict loss = 0.85 (91.8 examples/sec; 0.044 sec/batch; 1h:16m:44s remains)
INFO - root - 2019-11-06 19:00:44.930474: step 44390, total loss = 2.95, predict loss = 0.78 (5.8 examples/sec; 0.686 sec/batch; 20h:06m:47s remains)
INFO - root - 2019-11-06 19:00:45.583497: step 44400, total loss = 3.30, predict loss = 0.94 (73.1 examples/sec; 0.055 sec/batch; 1h:36m:20s remains)
INFO - root - 2019-11-06 19:00:46.296383: step 44410, total loss = 1.99, predict loss = 0.47 (59.5 examples/sec; 0.067 sec/batch; 1h:58m:24s remains)
INFO - root - 2019-11-06 19:00:47.095811: step 44420, total loss = 2.63, predict loss = 0.72 (55.6 examples/sec; 0.072 sec/batch; 2h:06m:29s remains)
INFO - root - 2019-11-06 19:00:47.799238: step 44430, total loss = 1.80, predict loss = 0.46 (59.8 examples/sec; 0.067 sec/batch; 1h:57m:43s remains)
INFO - root - 2019-11-06 19:00:48.450549: step 44440, total loss = 2.42, predict loss = 0.63 (89.7 examples/sec; 0.045 sec/batch; 1h:18m:25s remains)
INFO - root - 2019-11-06 19:00:48.906377: step 44450, total loss = 3.61, predict loss = 0.94 (93.8 examples/sec; 0.043 sec/batch; 1h:15m:02s remains)
INFO - root - 2019-11-06 19:00:49.380766: step 44460, total loss = 2.38, predict loss = 0.60 (93.4 examples/sec; 0.043 sec/batch; 1h:15m:18s remains)
INFO - root - 2019-11-06 19:00:50.609916: step 44470, total loss = 2.90, predict loss = 0.76 (61.6 examples/sec; 0.065 sec/batch; 1h:54m:13s remains)
INFO - root - 2019-11-06 19:00:51.437948: step 44480, total loss = 3.07, predict loss = 0.81 (56.7 examples/sec; 0.071 sec/batch; 2h:04m:04s remains)
INFO - root - 2019-11-06 19:00:52.144323: step 44490, total loss = 1.89, predict loss = 0.48 (70.0 examples/sec; 0.057 sec/batch; 1h:40m:32s remains)
INFO - root - 2019-11-06 19:00:52.913329: step 44500, total loss = 2.92, predict loss = 0.72 (53.5 examples/sec; 0.075 sec/batch; 2h:11m:34s remains)
INFO - root - 2019-11-06 19:00:53.693709: step 44510, total loss = 1.50, predict loss = 0.37 (70.3 examples/sec; 0.057 sec/batch; 1h:39m:58s remains)
INFO - root - 2019-11-06 19:00:54.224596: step 44520, total loss = 2.10, predict loss = 0.52 (97.9 examples/sec; 0.041 sec/batch; 1h:11m:48s remains)
INFO - root - 2019-11-06 19:00:54.689059: step 44530, total loss = 2.41, predict loss = 0.63 (85.6 examples/sec; 0.047 sec/batch; 1h:22m:06s remains)
INFO - root - 2019-11-06 19:00:55.890654: step 44540, total loss = 2.11, predict loss = 0.53 (70.0 examples/sec; 0.057 sec/batch; 1h:40m:28s remains)
INFO - root - 2019-11-06 19:00:56.622115: step 44550, total loss = 2.82, predict loss = 0.75 (54.0 examples/sec; 0.074 sec/batch; 2h:10m:11s remains)
INFO - root - 2019-11-06 19:00:57.467383: step 44560, total loss = 1.75, predict loss = 0.45 (48.2 examples/sec; 0.083 sec/batch; 2h:25m:57s remains)
INFO - root - 2019-11-06 19:00:58.257333: step 44570, total loss = 2.49, predict loss = 0.61 (56.5 examples/sec; 0.071 sec/batch; 2h:04m:22s remains)
INFO - root - 2019-11-06 19:00:58.996122: step 44580, total loss = 2.89, predict loss = 0.73 (58.5 examples/sec; 0.068 sec/batch; 2h:00m:13s remains)
INFO - root - 2019-11-06 19:00:59.682255: step 44590, total loss = 2.57, predict loss = 0.65 (91.9 examples/sec; 0.044 sec/batch; 1h:16m:27s remains)
INFO - root - 2019-11-06 19:01:00.137943: step 44600, total loss = 3.03, predict loss = 0.81 (95.7 examples/sec; 0.042 sec/batch; 1h:13m:25s remains)
INFO - root - 2019-11-06 19:01:00.593007: step 44610, total loss = 3.03, predict loss = 0.81 (91.6 examples/sec; 0.044 sec/batch; 1h:16m:40s remains)
INFO - root - 2019-11-06 19:01:01.880479: step 44620, total loss = 1.92, predict loss = 0.45 (60.8 examples/sec; 0.066 sec/batch; 1h:55m:37s remains)
INFO - root - 2019-11-06 19:01:02.730964: step 44630, total loss = 2.43, predict loss = 0.68 (49.6 examples/sec; 0.081 sec/batch; 2h:21m:39s remains)
INFO - root - 2019-11-06 19:01:03.485039: step 44640, total loss = 2.69, predict loss = 0.73 (58.8 examples/sec; 0.068 sec/batch; 1h:59m:32s remains)
INFO - root - 2019-11-06 19:01:04.252549: step 44650, total loss = 3.19, predict loss = 0.85 (59.1 examples/sec; 0.068 sec/batch; 1h:58m:49s remains)
INFO - root - 2019-11-06 19:01:04.972289: step 44660, total loss = 2.28, predict loss = 0.60 (75.6 examples/sec; 0.053 sec/batch; 1h:32m:50s remains)
INFO - root - 2019-11-06 19:01:05.486444: step 44670, total loss = 2.47, predict loss = 0.64 (94.0 examples/sec; 0.043 sec/batch; 1h:14m:40s remains)
INFO - root - 2019-11-06 19:01:05.929851: step 44680, total loss = 2.02, predict loss = 0.51 (94.6 examples/sec; 0.042 sec/batch; 1h:14m:12s remains)
INFO - root - 2019-11-06 19:01:07.123446: step 44690, total loss = 2.02, predict loss = 0.53 (67.2 examples/sec; 0.060 sec/batch; 1h:44m:27s remains)
INFO - root - 2019-11-06 19:01:07.845324: step 44700, total loss = 1.54, predict loss = 0.43 (61.1 examples/sec; 0.065 sec/batch; 1h:54m:55s remains)
INFO - root - 2019-11-06 19:01:08.587727: step 44710, total loss = 2.88, predict loss = 0.81 (60.7 examples/sec; 0.066 sec/batch; 1h:55m:35s remains)
INFO - root - 2019-11-06 19:01:09.317585: step 44720, total loss = 2.71, predict loss = 0.70 (57.5 examples/sec; 0.070 sec/batch; 2h:01m:59s remains)
INFO - root - 2019-11-06 19:01:10.095740: step 44730, total loss = 1.85, predict loss = 0.48 (51.9 examples/sec; 0.077 sec/batch; 2h:15m:07s remains)
INFO - root - 2019-11-06 19:01:10.707846: step 44740, total loss = 2.07, predict loss = 0.56 (105.0 examples/sec; 0.038 sec/batch; 1h:06m:48s remains)
INFO - root - 2019-11-06 19:01:11.173744: step 44750, total loss = 2.98, predict loss = 0.83 (94.0 examples/sec; 0.043 sec/batch; 1h:14m:39s remains)
INFO - root - 2019-11-06 19:01:11.632846: step 44760, total loss = 1.89, predict loss = 0.49 (96.3 examples/sec; 0.042 sec/batch; 1h:12m:51s remains)
INFO - root - 2019-11-06 19:01:12.920032: step 44770, total loss = 1.91, predict loss = 0.48 (53.3 examples/sec; 0.075 sec/batch; 2h:11m:41s remains)
INFO - root - 2019-11-06 19:01:13.708574: step 44780, total loss = 2.22, predict loss = 0.52 (61.7 examples/sec; 0.065 sec/batch; 1h:53m:46s remains)
INFO - root - 2019-11-06 19:01:14.507974: step 44790, total loss = 2.74, predict loss = 0.68 (55.9 examples/sec; 0.072 sec/batch; 2h:05m:24s remains)
INFO - root - 2019-11-06 19:01:15.276367: step 44800, total loss = 2.58, predict loss = 0.69 (62.0 examples/sec; 0.065 sec/batch; 1h:53m:07s remains)
INFO - root - 2019-11-06 19:01:15.980097: step 44810, total loss = 2.60, predict loss = 0.62 (65.3 examples/sec; 0.061 sec/batch; 1h:47m:23s remains)
INFO - root - 2019-11-06 19:01:16.516254: step 44820, total loss = 1.48, predict loss = 0.37 (90.0 examples/sec; 0.044 sec/batch; 1h:17m:53s remains)
INFO - root - 2019-11-06 19:01:16.972888: step 44830, total loss = 2.16, predict loss = 0.51 (93.7 examples/sec; 0.043 sec/batch; 1h:14m:50s remains)
INFO - root - 2019-11-06 19:01:18.137536: step 44840, total loss = 1.84, predict loss = 0.46 (71.2 examples/sec; 0.056 sec/batch; 1h:38m:25s remains)
INFO - root - 2019-11-06 19:01:18.827199: step 44850, total loss = 3.11, predict loss = 0.83 (64.8 examples/sec; 0.062 sec/batch; 1h:48m:08s remains)
INFO - root - 2019-11-06 19:01:19.553203: step 44860, total loss = 3.46, predict loss = 0.95 (66.3 examples/sec; 0.060 sec/batch; 1h:45m:47s remains)
INFO - root - 2019-11-06 19:01:20.364138: step 44870, total loss = 2.03, predict loss = 0.50 (54.7 examples/sec; 0.073 sec/batch; 2h:08m:11s remains)
INFO - root - 2019-11-06 19:01:21.123271: step 44880, total loss = 1.83, predict loss = 0.44 (59.0 examples/sec; 0.068 sec/batch; 1h:58m:41s remains)
INFO - root - 2019-11-06 19:01:21.696833: step 44890, total loss = 2.66, predict loss = 0.72 (100.5 examples/sec; 0.040 sec/batch; 1h:09m:44s remains)
INFO - root - 2019-11-06 19:01:22.173586: step 44900, total loss = 2.37, predict loss = 0.57 (95.2 examples/sec; 0.042 sec/batch; 1h:13m:36s remains)
INFO - root - 2019-11-06 19:01:22.627942: step 44910, total loss = 2.47, predict loss = 0.61 (87.1 examples/sec; 0.046 sec/batch; 1h:20m:27s remains)
INFO - root - 2019-11-06 19:01:23.996019: step 44920, total loss = 1.66, predict loss = 0.39 (62.0 examples/sec; 0.065 sec/batch; 1h:53m:04s remains)
INFO - root - 2019-11-06 19:01:24.799361: step 44930, total loss = 1.90, predict loss = 0.47 (50.3 examples/sec; 0.080 sec/batch; 2h:19m:18s remains)
INFO - root - 2019-11-06 19:01:25.558740: step 44940, total loss = 3.57, predict loss = 0.99 (58.0 examples/sec; 0.069 sec/batch; 2h:00m:47s remains)
INFO - root - 2019-11-06 19:01:26.332897: step 44950, total loss = 1.40, predict loss = 0.34 (57.6 examples/sec; 0.069 sec/batch; 2h:01m:33s remains)
INFO - root - 2019-11-06 19:01:27.084749: step 44960, total loss = 3.33, predict loss = 0.93 (67.6 examples/sec; 0.059 sec/batch; 1h:43m:36s remains)
INFO - root - 2019-11-06 19:01:27.582387: step 44970, total loss = 1.73, predict loss = 0.43 (91.0 examples/sec; 0.044 sec/batch; 1h:16m:54s remains)
INFO - root - 2019-11-06 19:01:28.076748: step 44980, total loss = 2.31, predict loss = 0.71 (91.1 examples/sec; 0.044 sec/batch; 1h:16m:51s remains)
INFO - root - 2019-11-06 19:01:29.279378: step 44990, total loss = 2.52, predict loss = 0.69 (60.8 examples/sec; 0.066 sec/batch; 1h:55m:06s remains)
INFO - root - 2019-11-06 19:01:30.003470: step 45000, total loss = 2.89, predict loss = 0.74 (53.5 examples/sec; 0.075 sec/batch; 2h:10m:46s remains)
INFO - root - 2019-11-06 19:01:31.781221: step 45010, total loss = 3.25, predict loss = 0.88 (69.3 examples/sec; 0.058 sec/batch; 1h:41m:02s remains)
INFO - root - 2019-11-06 19:01:32.475291: step 45020, total loss = 3.19, predict loss = 0.86 (61.5 examples/sec; 0.065 sec/batch; 1h:53m:46s remains)
INFO - root - 2019-11-06 19:01:33.183607: step 45030, total loss = 1.99, predict loss = 0.52 (62.8 examples/sec; 0.064 sec/batch; 1h:51m:21s remains)
INFO - root - 2019-11-06 19:01:33.781163: step 45040, total loss = 2.34, predict loss = 0.64 (96.7 examples/sec; 0.041 sec/batch; 1h:12m:21s remains)
INFO - root - 2019-11-06 19:01:34.233530: step 45050, total loss = 3.49, predict loss = 1.01 (102.3 examples/sec; 0.039 sec/batch; 1h:08m:24s remains)
INFO - root - 2019-11-06 19:01:34.723756: step 45060, total loss = 3.23, predict loss = 0.90 (117.5 examples/sec; 0.034 sec/batch; 0h:59m:31s remains)
INFO - root - 2019-11-06 19:01:36.090641: step 45070, total loss = 1.96, predict loss = 0.45 (59.1 examples/sec; 0.068 sec/batch; 1h:58m:17s remains)
INFO - root - 2019-11-06 19:01:36.853350: step 45080, total loss = 1.93, predict loss = 0.46 (56.8 examples/sec; 0.070 sec/batch; 2h:03m:13s remains)
INFO - root - 2019-11-06 19:01:37.653473: step 45090, total loss = 3.51, predict loss = 0.99 (58.7 examples/sec; 0.068 sec/batch; 1h:59m:03s remains)
INFO - root - 2019-11-06 19:01:38.404543: step 45100, total loss = 3.65, predict loss = 1.02 (61.5 examples/sec; 0.065 sec/batch; 1h:53m:47s remains)
INFO - root - 2019-11-06 19:01:39.078655: step 45110, total loss = 2.42, predict loss = 0.69 (72.0 examples/sec; 0.056 sec/batch; 1h:37m:05s remains)
INFO - root - 2019-11-06 19:01:39.546080: step 45120, total loss = 4.04, predict loss = 1.23 (97.2 examples/sec; 0.041 sec/batch; 1h:11m:57s remains)
INFO - root - 2019-11-06 19:01:40.007349: step 45130, total loss = 1.70, predict loss = 0.41 (100.3 examples/sec; 0.040 sec/batch; 1h:09m:40s remains)
INFO - root - 2019-11-06 19:01:41.248208: step 45140, total loss = 2.98, predict loss = 0.76 (65.5 examples/sec; 0.061 sec/batch; 1h:46m:41s remains)
INFO - root - 2019-11-06 19:01:41.973260: step 45150, total loss = 2.08, predict loss = 0.58 (59.4 examples/sec; 0.067 sec/batch; 1h:57m:45s remains)
INFO - root - 2019-11-06 19:01:42.726489: step 45160, total loss = 2.20, predict loss = 0.65 (60.9 examples/sec; 0.066 sec/batch; 1h:54m:45s remains)
INFO - root - 2019-11-06 19:01:43.483339: step 45170, total loss = 1.58, predict loss = 0.39 (58.5 examples/sec; 0.068 sec/batch; 1h:59m:31s remains)
INFO - root - 2019-11-06 19:01:44.235549: step 45180, total loss = 1.81, predict loss = 0.46 (67.5 examples/sec; 0.059 sec/batch; 1h:43m:34s remains)
INFO - root - 2019-11-06 19:01:44.829345: step 45190, total loss = 1.77, predict loss = 0.50 (94.4 examples/sec; 0.042 sec/batch; 1h:14m:03s remains)
INFO - root - 2019-11-06 19:01:45.311786: step 45200, total loss = 3.44, predict loss = 0.94 (89.8 examples/sec; 0.045 sec/batch; 1h:17m:47s remains)
INFO - root - 2019-11-06 19:01:46.456318: step 45210, total loss = 2.62, predict loss = 0.67 (5.4 examples/sec; 0.735 sec/batch; 21h:24m:07s remains)
INFO - root - 2019-11-06 19:01:47.177584: step 45220, total loss = 3.14, predict loss = 0.89 (60.2 examples/sec; 0.066 sec/batch; 1h:55m:57s remains)
INFO - root - 2019-11-06 19:01:47.935278: step 45230, total loss = 1.55, predict loss = 0.43 (54.5 examples/sec; 0.073 sec/batch; 2h:08m:12s remains)
INFO - root - 2019-11-06 19:01:48.644751: step 45240, total loss = 2.62, predict loss = 0.62 (76.2 examples/sec; 0.052 sec/batch; 1h:31m:38s remains)
INFO - root - 2019-11-06 19:01:49.370583: step 45250, total loss = 2.27, predict loss = 0.56 (64.3 examples/sec; 0.062 sec/batch; 1h:48m:36s remains)
INFO - root - 2019-11-06 19:01:49.995764: step 45260, total loss = 3.52, predict loss = 0.99 (99.0 examples/sec; 0.040 sec/batch; 1h:10m:33s remains)
INFO - root - 2019-11-06 19:01:50.433218: step 45270, total loss = 1.76, predict loss = 0.46 (95.2 examples/sec; 0.042 sec/batch; 1h:13m:18s remains)
INFO - root - 2019-11-06 19:01:50.892847: step 45280, total loss = 2.69, predict loss = 0.73 (101.7 examples/sec; 0.039 sec/batch; 1h:08m:40s remains)
INFO - root - 2019-11-06 19:01:52.148743: step 45290, total loss = 1.52, predict loss = 0.38 (63.2 examples/sec; 0.063 sec/batch; 1h:50m:29s remains)
INFO - root - 2019-11-06 19:01:52.910191: step 45300, total loss = 3.89, predict loss = 1.15 (55.0 examples/sec; 0.073 sec/batch; 2h:06m:59s remains)
INFO - root - 2019-11-06 19:01:53.670100: step 45310, total loss = 2.06, predict loss = 0.53 (54.3 examples/sec; 0.074 sec/batch; 2h:08m:38s remains)
INFO - root - 2019-11-06 19:01:54.542693: step 45320, total loss = 2.02, predict loss = 0.47 (44.2 examples/sec; 0.090 sec/batch; 2h:37m:44s remains)
INFO - root - 2019-11-06 19:01:55.284224: step 45330, total loss = 3.68, predict loss = 1.04 (71.5 examples/sec; 0.056 sec/batch; 1h:37m:35s remains)
INFO - root - 2019-11-06 19:01:55.849492: step 45340, total loss = 2.48, predict loss = 0.63 (96.5 examples/sec; 0.041 sec/batch; 1h:12m:19s remains)
INFO - root - 2019-11-06 19:01:56.292414: step 45350, total loss = 2.47, predict loss = 0.62 (96.3 examples/sec; 0.042 sec/batch; 1h:12m:24s remains)
INFO - root - 2019-11-06 19:01:57.389086: step 45360, total loss = 2.15, predict loss = 0.50 (69.5 examples/sec; 0.058 sec/batch; 1h:40m:19s remains)
INFO - root - 2019-11-06 19:01:58.065484: step 45370, total loss = 2.19, predict loss = 0.59 (69.7 examples/sec; 0.057 sec/batch; 1h:40m:04s remains)
INFO - root - 2019-11-06 19:01:58.814272: step 45380, total loss = 2.65, predict loss = 0.72 (56.6 examples/sec; 0.071 sec/batch; 2h:03m:07s remains)
INFO - root - 2019-11-06 19:01:59.552200: step 45390, total loss = 1.70, predict loss = 0.42 (64.7 examples/sec; 0.062 sec/batch; 1h:47m:46s remains)
INFO - root - 2019-11-06 19:02:00.300098: step 45400, total loss = 1.40, predict loss = 0.35 (63.6 examples/sec; 0.063 sec/batch; 1h:49m:34s remains)
INFO - root - 2019-11-06 19:02:00.936878: step 45410, total loss = 2.15, predict loss = 0.55 (93.6 examples/sec; 0.043 sec/batch; 1h:14m:31s remains)
INFO - root - 2019-11-06 19:02:01.413031: step 45420, total loss = 3.02, predict loss = 0.75 (91.4 examples/sec; 0.044 sec/batch; 1h:16m:18s remains)
INFO - root - 2019-11-06 19:02:01.880382: step 45430, total loss = 2.24, predict loss = 0.60 (91.5 examples/sec; 0.044 sec/batch; 1h:16m:13s remains)
INFO - root - 2019-11-06 19:02:03.219662: step 45440, total loss = 2.71, predict loss = 0.74 (52.2 examples/sec; 0.077 sec/batch; 2h:13m:33s remains)
INFO - root - 2019-11-06 19:02:04.014427: step 45450, total loss = 3.30, predict loss = 0.88 (54.7 examples/sec; 0.073 sec/batch; 2h:07m:29s remains)
INFO - root - 2019-11-06 19:02:04.804126: step 45460, total loss = 1.79, predict loss = 0.44 (54.0 examples/sec; 0.074 sec/batch; 2h:09m:10s remains)
INFO - root - 2019-11-06 19:02:05.592987: step 45470, total loss = 3.46, predict loss = 0.96 (55.2 examples/sec; 0.072 sec/batch; 2h:06m:16s remains)
INFO - root - 2019-11-06 19:02:06.301634: step 45480, total loss = 2.85, predict loss = 0.74 (65.5 examples/sec; 0.061 sec/batch; 1h:46m:26s remains)
INFO - root - 2019-11-06 19:02:06.815534: step 45490, total loss = 2.48, predict loss = 0.59 (96.9 examples/sec; 0.041 sec/batch; 1h:11m:53s remains)
INFO - root - 2019-11-06 19:02:07.310208: step 45500, total loss = 2.96, predict loss = 0.80 (97.4 examples/sec; 0.041 sec/batch; 1h:11m:32s remains)
INFO - root - 2019-11-06 19:02:08.503554: step 45510, total loss = 1.43, predict loss = 0.35 (68.4 examples/sec; 0.058 sec/batch; 1h:41m:50s remains)
INFO - root - 2019-11-06 19:02:09.244001: step 45520, total loss = 3.44, predict loss = 0.99 (50.7 examples/sec; 0.079 sec/batch; 2h:17m:19s remains)
INFO - root - 2019-11-06 19:02:10.103440: step 45530, total loss = 1.78, predict loss = 0.46 (54.6 examples/sec; 0.073 sec/batch; 2h:07m:36s remains)
INFO - root - 2019-11-06 19:02:10.839773: step 45540, total loss = 3.56, predict loss = 0.96 (60.8 examples/sec; 0.066 sec/batch; 1h:54m:30s remains)
INFO - root - 2019-11-06 19:02:11.565890: step 45550, total loss = 3.13, predict loss = 0.89 (60.3 examples/sec; 0.066 sec/batch; 1h:55m:29s remains)
INFO - root - 2019-11-06 19:02:12.245279: step 45560, total loss = 3.20, predict loss = 0.95 (102.9 examples/sec; 0.039 sec/batch; 1h:07m:38s remains)
INFO - root - 2019-11-06 19:02:12.690563: step 45570, total loss = 2.27, predict loss = 0.54 (97.3 examples/sec; 0.041 sec/batch; 1h:11m:34s remains)
INFO - root - 2019-11-06 19:02:13.161689: step 45580, total loss = 2.18, predict loss = 0.56 (94.7 examples/sec; 0.042 sec/batch; 1h:13m:29s remains)
INFO - root - 2019-11-06 19:02:14.438007: step 45590, total loss = 2.49, predict loss = 0.63 (54.5 examples/sec; 0.073 sec/batch; 2h:07m:37s remains)
INFO - root - 2019-11-06 19:02:15.202855: step 45600, total loss = 2.97, predict loss = 0.78 (57.3 examples/sec; 0.070 sec/batch; 2h:01m:26s remains)
INFO - root - 2019-11-06 19:02:15.962805: step 45610, total loss = 1.49, predict loss = 0.45 (57.5 examples/sec; 0.070 sec/batch; 2h:00m:58s remains)
INFO - root - 2019-11-06 19:02:16.719548: step 45620, total loss = 2.49, predict loss = 0.59 (59.1 examples/sec; 0.068 sec/batch; 1h:57m:47s remains)
INFO - root - 2019-11-06 19:02:17.423340: step 45630, total loss = 2.24, predict loss = 0.57 (73.0 examples/sec; 0.055 sec/batch; 1h:35m:22s remains)
INFO - root - 2019-11-06 19:02:17.913900: step 45640, total loss = 2.96, predict loss = 0.96 (95.7 examples/sec; 0.042 sec/batch; 1h:12m:41s remains)
INFO - root - 2019-11-06 19:02:18.366910: step 45650, total loss = 2.52, predict loss = 0.60 (97.4 examples/sec; 0.041 sec/batch; 1h:11m:26s remains)
INFO - root - 2019-11-06 19:02:19.555753: step 45660, total loss = 2.66, predict loss = 0.74 (67.1 examples/sec; 0.060 sec/batch; 1h:43m:42s remains)
INFO - root - 2019-11-06 19:02:20.277593: step 45670, total loss = 3.14, predict loss = 0.86 (66.5 examples/sec; 0.060 sec/batch; 1h:44m:38s remains)
INFO - root - 2019-11-06 19:02:21.007058: step 45680, total loss = 2.80, predict loss = 0.76 (60.3 examples/sec; 0.066 sec/batch; 1h:55m:23s remains)
INFO - root - 2019-11-06 19:02:21.759835: step 45690, total loss = 2.44, predict loss = 0.64 (57.2 examples/sec; 0.070 sec/batch; 2h:01m:39s remains)
INFO - root - 2019-11-06 19:02:22.510720: step 45700, total loss = 2.38, predict loss = 0.63 (64.6 examples/sec; 0.062 sec/batch; 1h:47m:33s remains)
INFO - root - 2019-11-06 19:02:23.091988: step 45710, total loss = 3.45, predict loss = 0.90 (105.2 examples/sec; 0.038 sec/batch; 1h:06m:05s remains)
INFO - root - 2019-11-06 19:02:23.530919: step 45720, total loss = 2.71, predict loss = 0.81 (97.2 examples/sec; 0.041 sec/batch; 1h:11m:31s remains)
INFO - root - 2019-11-06 19:02:23.984227: step 45730, total loss = 1.61, predict loss = 0.40 (95.9 examples/sec; 0.042 sec/batch; 1h:12m:30s remains)
INFO - root - 2019-11-06 19:02:25.318935: step 45740, total loss = 2.52, predict loss = 0.64 (60.1 examples/sec; 0.067 sec/batch; 1h:55m:40s remains)
INFO - root - 2019-11-06 19:02:26.070579: step 45750, total loss = 1.59, predict loss = 0.41 (61.0 examples/sec; 0.066 sec/batch; 1h:53m:51s remains)
INFO - root - 2019-11-06 19:02:26.822252: step 45760, total loss = 2.68, predict loss = 0.73 (52.9 examples/sec; 0.076 sec/batch; 2h:11m:21s remains)
INFO - root - 2019-11-06 19:02:27.671868: step 45770, total loss = 4.27, predict loss = 1.31 (42.2 examples/sec; 0.095 sec/batch; 2h:44m:48s remains)
INFO - root - 2019-11-06 19:02:28.374359: step 45780, total loss = 1.47, predict loss = 0.35 (77.6 examples/sec; 0.052 sec/batch; 1h:29m:30s remains)
INFO - root - 2019-11-06 19:02:28.863656: step 45790, total loss = 2.27, predict loss = 0.56 (97.1 examples/sec; 0.041 sec/batch; 1h:11m:34s remains)
INFO - root - 2019-11-06 19:02:29.325408: step 45800, total loss = 2.89, predict loss = 0.76 (96.5 examples/sec; 0.041 sec/batch; 1h:11m:58s remains)
INFO - root - 2019-11-06 19:02:30.523952: step 45810, total loss = 2.81, predict loss = 0.72 (63.5 examples/sec; 0.063 sec/batch; 1h:49m:20s remains)
INFO - root - 2019-11-06 19:02:31.243187: step 45820, total loss = 3.12, predict loss = 0.87 (63.8 examples/sec; 0.063 sec/batch; 1h:48m:50s remains)
INFO - root - 2019-11-06 19:02:31.984088: step 45830, total loss = 3.17, predict loss = 0.89 (63.7 examples/sec; 0.063 sec/batch; 1h:48m:57s remains)
INFO - root - 2019-11-06 19:02:32.752623: step 45840, total loss = 1.99, predict loss = 0.50 (55.6 examples/sec; 0.072 sec/batch; 2h:04m:48s remains)
INFO - root - 2019-11-06 19:02:33.529173: step 45850, total loss = 2.16, predict loss = 0.54 (56.8 examples/sec; 0.070 sec/batch; 2h:02m:16s remains)
INFO - root - 2019-11-06 19:02:34.096476: step 45860, total loss = 2.20, predict loss = 0.54 (96.5 examples/sec; 0.041 sec/batch; 1h:11m:57s remains)
INFO - root - 2019-11-06 19:02:34.576112: step 45870, total loss = 2.77, predict loss = 0.75 (101.2 examples/sec; 0.040 sec/batch; 1h:08m:33s remains)
INFO - root - 2019-11-06 19:02:35.010916: step 45880, total loss = 2.25, predict loss = 0.59 (128.5 examples/sec; 0.031 sec/batch; 0h:54m:00s remains)
INFO - root - 2019-11-06 19:02:36.415942: step 45890, total loss = 3.30, predict loss = 0.91 (58.5 examples/sec; 0.068 sec/batch; 1h:58m:32s remains)
INFO - root - 2019-11-06 19:02:37.197046: step 45900, total loss = 3.04, predict loss = 0.81 (63.9 examples/sec; 0.063 sec/batch; 1h:48m:40s remains)
INFO - root - 2019-11-06 19:02:37.981906: step 45910, total loss = 1.08, predict loss = 0.29 (53.2 examples/sec; 0.075 sec/batch; 2h:10m:23s remains)
INFO - root - 2019-11-06 19:02:38.711028: step 45920, total loss = 1.26, predict loss = 0.33 (54.8 examples/sec; 0.073 sec/batch; 2h:06m:34s remains)
INFO - root - 2019-11-06 19:02:39.428710: step 45930, total loss = 2.14, predict loss = 0.52 (69.4 examples/sec; 0.058 sec/batch; 1h:39m:56s remains)
INFO - root - 2019-11-06 19:02:39.935803: step 45940, total loss = 3.49, predict loss = 1.00 (93.4 examples/sec; 0.043 sec/batch; 1h:14m:15s remains)
INFO - root - 2019-11-06 19:02:40.383635: step 45950, total loss = 2.15, predict loss = 0.54 (99.0 examples/sec; 0.040 sec/batch; 1h:10m:03s remains)
INFO - root - 2019-11-06 19:02:41.620393: step 45960, total loss = 2.48, predict loss = 0.64 (66.2 examples/sec; 0.060 sec/batch; 1h:44m:48s remains)
INFO - root - 2019-11-06 19:02:42.309222: step 45970, total loss = 1.87, predict loss = 0.53 (58.5 examples/sec; 0.068 sec/batch; 1h:58m:37s remains)
INFO - root - 2019-11-06 19:02:43.083629: step 45980, total loss = 2.95, predict loss = 0.78 (52.6 examples/sec; 0.076 sec/batch; 2h:11m:45s remains)
INFO - root - 2019-11-06 19:02:43.846185: step 45990, total loss = 1.78, predict loss = 0.47 (58.2 examples/sec; 0.069 sec/batch; 1h:59m:08s remains)
INFO - root - 2019-11-06 19:02:44.589840: step 46000, total loss = 2.45, predict loss = 0.58 (67.2 examples/sec; 0.060 sec/batch; 1h:43m:12s remains)
INFO - root - 2019-11-06 19:02:45.145912: step 46010, total loss = 2.86, predict loss = 0.78 (99.9 examples/sec; 0.040 sec/batch; 1h:09m:24s remains)
INFO - root - 2019-11-06 19:02:45.616461: step 46020, total loss = 1.94, predict loss = 0.47 (96.1 examples/sec; 0.042 sec/batch; 1h:12m:06s remains)
INFO - root - 2019-11-06 19:02:46.783672: step 46030, total loss = 3.02, predict loss = 0.84 (5.4 examples/sec; 0.743 sec/batch; 21h:28m:10s remains)
INFO - root - 2019-11-06 19:02:47.464281: step 46040, total loss = 1.74, predict loss = 0.42 (52.6 examples/sec; 0.076 sec/batch; 2h:11m:46s remains)
INFO - root - 2019-11-06 19:02:48.174997: step 46050, total loss = 2.89, predict loss = 0.73 (63.3 examples/sec; 0.063 sec/batch; 1h:49m:32s remains)
INFO - root - 2019-11-06 19:02:48.961438: step 46060, total loss = 2.71, predict loss = 0.76 (59.6 examples/sec; 0.067 sec/batch; 1h:56m:14s remains)
INFO - root - 2019-11-06 19:02:49.698558: step 46070, total loss = 2.70, predict loss = 0.67 (59.9 examples/sec; 0.067 sec/batch; 1h:55m:41s remains)
INFO - root - 2019-11-06 19:02:50.382360: step 46080, total loss = 1.83, predict loss = 0.45 (87.3 examples/sec; 0.046 sec/batch; 1h:19m:18s remains)
INFO - root - 2019-11-06 19:02:50.828021: step 46090, total loss = 3.41, predict loss = 0.93 (99.7 examples/sec; 0.040 sec/batch; 1h:09m:29s remains)
INFO - root - 2019-11-06 19:02:51.310075: step 46100, total loss = 3.19, predict loss = 0.87 (96.5 examples/sec; 0.041 sec/batch; 1h:11m:48s remains)
INFO - root - 2019-11-06 19:02:52.544043: step 46110, total loss = 3.11, predict loss = 0.83 (63.7 examples/sec; 0.063 sec/batch; 1h:48m:43s remains)
INFO - root - 2019-11-06 19:02:53.254446: step 46120, total loss = 2.38, predict loss = 0.63 (60.9 examples/sec; 0.066 sec/batch; 1h:53m:44s remains)
INFO - root - 2019-11-06 19:02:54.088350: step 46130, total loss = 1.81, predict loss = 0.45 (51.9 examples/sec; 0.077 sec/batch; 2h:13m:19s remains)
INFO - root - 2019-11-06 19:02:54.874346: step 46140, total loss = 1.91, predict loss = 0.45 (56.9 examples/sec; 0.070 sec/batch; 2h:01m:42s remains)
INFO - root - 2019-11-06 19:02:55.641133: step 46150, total loss = 2.60, predict loss = 0.67 (67.9 examples/sec; 0.059 sec/batch; 1h:42m:00s remains)
INFO - root - 2019-11-06 19:02:56.200007: step 46160, total loss = 1.85, predict loss = 0.47 (99.4 examples/sec; 0.040 sec/batch; 1h:09m:39s remains)
INFO - root - 2019-11-06 19:02:56.675303: step 46170, total loss = 2.91, predict loss = 0.86 (89.5 examples/sec; 0.045 sec/batch; 1h:17m:22s remains)
INFO - root - 2019-11-06 19:02:57.915594: step 46180, total loss = 2.32, predict loss = 0.67 (65.2 examples/sec; 0.061 sec/batch; 1h:46m:12s remains)
INFO - root - 2019-11-06 19:02:58.647559: step 46190, total loss = 2.42, predict loss = 0.60 (55.6 examples/sec; 0.072 sec/batch; 2h:04m:29s remains)
INFO - root - 2019-11-06 19:02:59.366095: step 46200, total loss = 2.52, predict loss = 0.76 (69.4 examples/sec; 0.058 sec/batch; 1h:39m:42s remains)
INFO - root - 2019-11-06 19:03:00.089958: step 46210, total loss = 1.28, predict loss = 0.31 (54.9 examples/sec; 0.073 sec/batch; 2h:06m:04s remains)
INFO - root - 2019-11-06 19:03:00.909416: step 46220, total loss = 2.94, predict loss = 0.75 (50.5 examples/sec; 0.079 sec/batch; 2h:16m:53s remains)
INFO - root - 2019-11-06 19:03:01.569724: step 46230, total loss = 2.68, predict loss = 0.70 (88.4 examples/sec; 0.045 sec/batch; 1h:18m:16s remains)
INFO - root - 2019-11-06 19:03:02.004510: step 46240, total loss = 1.97, predict loss = 0.54 (96.0 examples/sec; 0.042 sec/batch; 1h:12m:01s remains)
INFO - root - 2019-11-06 19:03:02.466543: step 46250, total loss = 2.17, predict loss = 0.51 (96.1 examples/sec; 0.042 sec/batch; 1h:11m:59s remains)
INFO - root - 2019-11-06 19:03:03.782972: step 46260, total loss = 3.52, predict loss = 0.95 (60.8 examples/sec; 0.066 sec/batch; 1h:53m:46s remains)
INFO - root - 2019-11-06 19:03:04.509829: step 46270, total loss = 3.69, predict loss = 1.15 (58.5 examples/sec; 0.068 sec/batch; 1h:58m:18s remains)
INFO - root - 2019-11-06 19:03:05.292045: step 46280, total loss = 2.49, predict loss = 0.65 (55.8 examples/sec; 0.072 sec/batch; 2h:03m:53s remains)
INFO - root - 2019-11-06 19:03:06.030823: step 46290, total loss = 3.26, predict loss = 0.94 (55.9 examples/sec; 0.072 sec/batch; 2h:03m:45s remains)
INFO - root - 2019-11-06 19:03:06.759720: step 46300, total loss = 2.47, predict loss = 0.62 (71.5 examples/sec; 0.056 sec/batch; 1h:36m:37s remains)
INFO - root - 2019-11-06 19:03:07.281848: step 46310, total loss = 2.12, predict loss = 0.55 (94.4 examples/sec; 0.042 sec/batch; 1h:13m:13s remains)
INFO - root - 2019-11-06 19:03:07.749393: step 46320, total loss = 2.65, predict loss = 0.65 (91.9 examples/sec; 0.044 sec/batch; 1h:15m:14s remains)
INFO - root - 2019-11-06 19:03:08.940675: step 46330, total loss = 2.55, predict loss = 0.64 (65.9 examples/sec; 0.061 sec/batch; 1h:44m:53s remains)
INFO - root - 2019-11-06 19:03:09.701821: step 46340, total loss = 1.71, predict loss = 0.44 (53.9 examples/sec; 0.074 sec/batch; 2h:08m:16s remains)
INFO - root - 2019-11-06 19:03:10.463061: step 46350, total loss = 2.01, predict loss = 0.48 (55.1 examples/sec; 0.073 sec/batch; 2h:05m:28s remains)
INFO - root - 2019-11-06 19:03:11.257928: step 46360, total loss = 2.65, predict loss = 0.69 (58.0 examples/sec; 0.069 sec/batch; 1h:59m:04s remains)
INFO - root - 2019-11-06 19:03:12.065412: step 46370, total loss = 1.82, predict loss = 0.42 (57.8 examples/sec; 0.069 sec/batch; 1h:59m:31s remains)
INFO - root - 2019-11-06 19:03:12.688030: step 46380, total loss = 1.62, predict loss = 0.45 (101.8 examples/sec; 0.039 sec/batch; 1h:07m:51s remains)
INFO - root - 2019-11-06 19:03:13.146895: step 46390, total loss = 2.91, predict loss = 0.73 (93.7 examples/sec; 0.043 sec/batch; 1h:13m:42s remains)
INFO - root - 2019-11-06 19:03:13.601388: step 46400, total loss = 1.84, predict loss = 0.49 (89.8 examples/sec; 0.045 sec/batch; 1h:16m:52s remains)
INFO - root - 2019-11-06 19:03:14.925795: step 46410, total loss = 2.02, predict loss = 0.52 (55.0 examples/sec; 0.073 sec/batch; 2h:05m:33s remains)
INFO - root - 2019-11-06 19:03:15.658582: step 46420, total loss = 1.95, predict loss = 0.48 (61.5 examples/sec; 0.065 sec/batch; 1h:52m:21s remains)
INFO - root - 2019-11-06 19:03:16.380256: step 46430, total loss = 1.70, predict loss = 0.40 (64.1 examples/sec; 0.062 sec/batch; 1h:47m:45s remains)
INFO - root - 2019-11-06 19:03:17.148393: step 46440, total loss = 1.88, predict loss = 0.43 (57.8 examples/sec; 0.069 sec/batch; 1h:59m:25s remains)
INFO - root - 2019-11-06 19:03:17.835572: step 46450, total loss = 2.73, predict loss = 0.76 (76.7 examples/sec; 0.052 sec/batch; 1h:30m:02s remains)
INFO - root - 2019-11-06 19:03:18.360871: step 46460, total loss = 1.33, predict loss = 0.33 (93.3 examples/sec; 0.043 sec/batch; 1h:14m:00s remains)
INFO - root - 2019-11-06 19:03:18.829381: step 46470, total loss = 2.39, predict loss = 0.64 (94.1 examples/sec; 0.042 sec/batch; 1h:13m:19s remains)
INFO - root - 2019-11-06 19:03:19.988577: step 46480, total loss = 2.48, predict loss = 0.63 (65.6 examples/sec; 0.061 sec/batch; 1h:45m:08s remains)
INFO - root - 2019-11-06 19:03:20.715343: step 46490, total loss = 3.05, predict loss = 0.80 (57.8 examples/sec; 0.069 sec/batch; 1h:59m:21s remains)
INFO - root - 2019-11-06 19:03:21.480972: step 46500, total loss = 2.58, predict loss = 0.63 (53.4 examples/sec; 0.075 sec/batch; 2h:09m:07s remains)
INFO - root - 2019-11-06 19:03:22.224724: step 46510, total loss = 2.28, predict loss = 0.57 (62.5 examples/sec; 0.064 sec/batch; 1h:50m:18s remains)
INFO - root - 2019-11-06 19:03:22.974244: step 46520, total loss = 1.78, predict loss = 0.49 (63.1 examples/sec; 0.063 sec/batch; 1h:49m:18s remains)
INFO - root - 2019-11-06 19:03:23.575096: step 46530, total loss = 3.05, predict loss = 0.84 (100.0 examples/sec; 0.040 sec/batch; 1h:08m:58s remains)
INFO - root - 2019-11-06 19:03:24.055301: step 46540, total loss = 2.27, predict loss = 0.57 (103.2 examples/sec; 0.039 sec/batch; 1h:06m:48s remains)
INFO - root - 2019-11-06 19:03:24.511245: step 46550, total loss = 3.00, predict loss = 0.80 (86.7 examples/sec; 0.046 sec/batch; 1h:19m:30s remains)
INFO - root - 2019-11-06 19:03:25.840582: step 46560, total loss = 2.45, predict loss = 0.65 (54.4 examples/sec; 0.074 sec/batch; 2h:06m:43s remains)
INFO - root - 2019-11-06 19:03:26.602175: step 46570, total loss = 2.40, predict loss = 0.65 (65.2 examples/sec; 0.061 sec/batch; 1h:45m:45s remains)
INFO - root - 2019-11-06 19:03:27.392768: step 46580, total loss = 2.30, predict loss = 0.65 (58.6 examples/sec; 0.068 sec/batch; 1h:57m:41s remains)
INFO - root - 2019-11-06 19:03:28.223045: step 46590, total loss = 1.99, predict loss = 0.49 (54.6 examples/sec; 0.073 sec/batch; 2h:06m:13s remains)
INFO - root - 2019-11-06 19:03:28.917412: step 46600, total loss = 1.58, predict loss = 0.37 (76.8 examples/sec; 0.052 sec/batch; 1h:29m:43s remains)
INFO - root - 2019-11-06 19:03:29.411964: step 46610, total loss = 2.28, predict loss = 0.58 (93.6 examples/sec; 0.043 sec/batch; 1h:13m:38s remains)
INFO - root - 2019-11-06 19:03:29.884964: step 46620, total loss = 2.70, predict loss = 0.75 (97.3 examples/sec; 0.041 sec/batch; 1h:10m:50s remains)
INFO - root - 2019-11-06 19:03:31.108469: step 46630, total loss = 2.25, predict loss = 0.55 (77.4 examples/sec; 0.052 sec/batch; 1h:29m:02s remains)
INFO - root - 2019-11-06 19:03:31.843670: step 46640, total loss = 2.17, predict loss = 0.55 (56.6 examples/sec; 0.071 sec/batch; 2h:01m:47s remains)
INFO - root - 2019-11-06 19:03:32.601573: step 46650, total loss = 2.25, predict loss = 0.56 (59.9 examples/sec; 0.067 sec/batch; 1h:55m:00s remains)
INFO - root - 2019-11-06 19:03:33.349548: step 46660, total loss = 1.81, predict loss = 0.49 (66.5 examples/sec; 0.060 sec/batch; 1h:43m:32s remains)
INFO - root - 2019-11-06 19:03:34.097344: step 46670, total loss = 3.42, predict loss = 1.00 (57.1 examples/sec; 0.070 sec/batch; 2h:00m:33s remains)
INFO - root - 2019-11-06 19:03:34.722174: step 46680, total loss = 2.29, predict loss = 0.53 (95.4 examples/sec; 0.042 sec/batch; 1h:12m:12s remains)
INFO - root - 2019-11-06 19:03:35.175129: step 46690, total loss = 1.54, predict loss = 0.43 (102.2 examples/sec; 0.039 sec/batch; 1h:07m:21s remains)
INFO - root - 2019-11-06 19:03:35.634123: step 46700, total loss = 1.74, predict loss = 0.46 (136.3 examples/sec; 0.029 sec/batch; 0h:50m:31s remains)
INFO - root - 2019-11-06 19:03:36.993426: step 46710, total loss = 2.77, predict loss = 0.74 (56.6 examples/sec; 0.071 sec/batch; 2h:01m:35s remains)
INFO - root - 2019-11-06 19:03:37.748153: step 46720, total loss = 1.90, predict loss = 0.46 (55.6 examples/sec; 0.072 sec/batch; 2h:03m:53s remains)
INFO - root - 2019-11-06 19:03:38.551766: step 46730, total loss = 2.89, predict loss = 0.78 (56.0 examples/sec; 0.071 sec/batch; 2h:03m:00s remains)
INFO - root - 2019-11-06 19:03:39.357920: step 46740, total loss = 3.18, predict loss = 0.88 (56.1 examples/sec; 0.071 sec/batch; 2h:02m:45s remains)
INFO - root - 2019-11-06 19:03:40.043066: step 46750, total loss = 2.05, predict loss = 0.50 (74.4 examples/sec; 0.054 sec/batch; 1h:32m:27s remains)
INFO - root - 2019-11-06 19:03:40.487016: step 46760, total loss = 2.64, predict loss = 0.74 (95.1 examples/sec; 0.042 sec/batch; 1h:12m:23s remains)
INFO - root - 2019-11-06 19:03:40.943007: step 46770, total loss = 2.14, predict loss = 0.49 (103.8 examples/sec; 0.039 sec/batch; 1h:06m:17s remains)
INFO - root - 2019-11-06 19:03:42.206794: step 46780, total loss = 2.35, predict loss = 0.61 (65.7 examples/sec; 0.061 sec/batch; 1h:44m:42s remains)
INFO - root - 2019-11-06 19:03:42.971968: step 46790, total loss = 2.51, predict loss = 0.77 (61.3 examples/sec; 0.065 sec/batch; 1h:52m:20s remains)
INFO - root - 2019-11-06 19:03:43.741004: step 46800, total loss = 2.72, predict loss = 0.82 (55.2 examples/sec; 0.073 sec/batch; 2h:04m:43s remains)
INFO - root - 2019-11-06 19:03:44.540218: step 46810, total loss = 3.51, predict loss = 1.12 (57.3 examples/sec; 0.070 sec/batch; 2h:00m:08s remains)
INFO - root - 2019-11-06 19:03:45.269343: step 46820, total loss = 1.83, predict loss = 0.48 (74.3 examples/sec; 0.054 sec/batch; 1h:32m:33s remains)
INFO - root - 2019-11-06 19:03:45.823321: step 46830, total loss = 2.37, predict loss = 0.57 (91.8 examples/sec; 0.044 sec/batch; 1h:14m:57s remains)
INFO - root - 2019-11-06 19:03:46.289727: step 46840, total loss = 3.67, predict loss = 1.06 (98.4 examples/sec; 0.041 sec/batch; 1h:09m:53s remains)
INFO - root - 2019-11-06 19:03:47.428904: step 46850, total loss = 2.66, predict loss = 0.71 (5.6 examples/sec; 0.716 sec/batch; 20h:30m:55s remains)
INFO - root - 2019-11-06 19:03:48.172091: step 46860, total loss = 2.43, predict loss = 0.63 (50.9 examples/sec; 0.079 sec/batch; 2h:15m:01s remains)
INFO - root - 2019-11-06 19:03:48.990915: step 46870, total loss = 2.60, predict loss = 0.67 (56.5 examples/sec; 0.071 sec/batch; 2h:01m:35s remains)
INFO - root - 2019-11-06 19:03:49.722634: step 46880, total loss = 3.32, predict loss = 0.89 (63.5 examples/sec; 0.063 sec/batch; 1h:48m:17s remains)
INFO - root - 2019-11-06 19:03:50.465654: step 46890, total loss = 3.22, predict loss = 0.94 (62.2 examples/sec; 0.064 sec/batch; 1h:50m:30s remains)
INFO - root - 2019-11-06 19:03:51.168187: step 46900, total loss = 2.30, predict loss = 0.56 (88.5 examples/sec; 0.045 sec/batch; 1h:17m:37s remains)
INFO - root - 2019-11-06 19:03:51.601182: step 46910, total loss = 2.19, predict loss = 0.64 (104.9 examples/sec; 0.038 sec/batch; 1h:05m:29s remains)
INFO - root - 2019-11-06 19:03:52.029037: step 46920, total loss = 1.88, predict loss = 0.49 (98.3 examples/sec; 0.041 sec/batch; 1h:09m:54s remains)
INFO - root - 2019-11-06 19:03:53.217605: step 46930, total loss = 2.40, predict loss = 0.65 (65.4 examples/sec; 0.061 sec/batch; 1h:45m:07s remains)
INFO - root - 2019-11-06 19:03:54.006587: step 46940, total loss = 2.90, predict loss = 0.82 (55.9 examples/sec; 0.072 sec/batch; 2h:02m:57s remains)
INFO - root - 2019-11-06 19:03:54.807655: step 46950, total loss = 2.58, predict loss = 0.67 (51.0 examples/sec; 0.078 sec/batch; 2h:14m:46s remains)
INFO - root - 2019-11-06 19:03:55.581823: step 46960, total loss = 3.88, predict loss = 1.14 (63.2 examples/sec; 0.063 sec/batch; 1h:48m:41s remains)
INFO - root - 2019-11-06 19:03:56.301410: step 46970, total loss = 1.68, predict loss = 0.43 (70.5 examples/sec; 0.057 sec/batch; 1h:37m:28s remains)
INFO - root - 2019-11-06 19:03:56.841699: step 46980, total loss = 3.14, predict loss = 0.92 (95.5 examples/sec; 0.042 sec/batch; 1h:11m:53s remains)
INFO - root - 2019-11-06 19:03:57.308271: step 46990, total loss = 2.48, predict loss = 0.81 (96.7 examples/sec; 0.041 sec/batch; 1h:10m:59s remains)
INFO - root - 2019-11-06 19:03:58.446493: step 47000, total loss = 1.59, predict loss = 0.41 (70.4 examples/sec; 0.057 sec/batch; 1h:37m:35s remains)
INFO - root - 2019-11-06 19:03:59.220331: step 47010, total loss = 2.42, predict loss = 0.76 (56.0 examples/sec; 0.071 sec/batch; 2h:02m:32s remains)
INFO - root - 2019-11-06 19:03:59.995533: step 47020, total loss = 2.33, predict loss = 0.57 (57.1 examples/sec; 0.070 sec/batch; 2h:00m:11s remains)
INFO - root - 2019-11-06 19:04:00.738594: step 47030, total loss = 1.75, predict loss = 0.46 (57.8 examples/sec; 0.069 sec/batch; 1h:58m:43s remains)
INFO - root - 2019-11-06 19:04:01.454367: step 47040, total loss = 1.26, predict loss = 0.32 (60.1 examples/sec; 0.067 sec/batch; 1h:54m:15s remains)
INFO - root - 2019-11-06 19:04:02.125346: step 47050, total loss = 2.24, predict loss = 0.53 (85.2 examples/sec; 0.047 sec/batch; 1h:20m:31s remains)
INFO - root - 2019-11-06 19:04:02.608092: step 47060, total loss = 3.40, predict loss = 0.98 (97.8 examples/sec; 0.041 sec/batch; 1h:10m:12s remains)
INFO - root - 2019-11-06 19:04:03.059901: step 47070, total loss = 2.66, predict loss = 0.66 (94.4 examples/sec; 0.042 sec/batch; 1h:12m:42s remains)
INFO - root - 2019-11-06 19:04:04.342570: step 47080, total loss = 2.75, predict loss = 0.75 (61.9 examples/sec; 0.065 sec/batch; 1h:50m:45s remains)
INFO - root - 2019-11-06 19:04:05.076729: step 47090, total loss = 2.99, predict loss = 0.84 (62.0 examples/sec; 0.065 sec/batch; 1h:50m:43s remains)
INFO - root - 2019-11-06 19:04:05.856267: step 47100, total loss = 2.63, predict loss = 0.78 (62.1 examples/sec; 0.064 sec/batch; 1h:50m:27s remains)
INFO - root - 2019-11-06 19:04:06.659590: step 47110, total loss = 1.38, predict loss = 0.34 (58.3 examples/sec; 0.069 sec/batch; 1h:57m:39s remains)
INFO - root - 2019-11-06 19:04:07.445382: step 47120, total loss = 3.00, predict loss = 0.82 (64.3 examples/sec; 0.062 sec/batch; 1h:46m:35s remains)
INFO - root - 2019-11-06 19:04:07.972776: step 47130, total loss = 3.34, predict loss = 0.94 (91.6 examples/sec; 0.044 sec/batch; 1h:14m:51s remains)
INFO - root - 2019-11-06 19:04:08.471314: step 47140, total loss = 2.02, predict loss = 0.48 (92.0 examples/sec; 0.043 sec/batch; 1h:14m:32s remains)
INFO - root - 2019-11-06 19:04:09.635232: step 47150, total loss = 2.05, predict loss = 0.58 (65.3 examples/sec; 0.061 sec/batch; 1h:45m:00s remains)
INFO - root - 2019-11-06 19:04:10.313597: step 47160, total loss = 1.41, predict loss = 0.36 (55.2 examples/sec; 0.072 sec/batch; 2h:04m:06s remains)
INFO - root - 2019-11-06 19:04:11.072324: step 47170, total loss = 1.88, predict loss = 0.49 (55.5 examples/sec; 0.072 sec/batch; 2h:03m:30s remains)
INFO - root - 2019-11-06 19:04:11.887186: step 47180, total loss = 1.94, predict loss = 0.51 (61.0 examples/sec; 0.066 sec/batch; 1h:52m:24s remains)
INFO - root - 2019-11-06 19:04:12.650600: step 47190, total loss = 2.82, predict loss = 0.73 (57.7 examples/sec; 0.069 sec/batch; 1h:58m:51s remains)
INFO - root - 2019-11-06 19:04:13.286990: step 47200, total loss = 3.32, predict loss = 1.00 (102.7 examples/sec; 0.039 sec/batch; 1h:06m:44s remains)
INFO - root - 2019-11-06 19:04:13.735407: step 47210, total loss = 2.53, predict loss = 0.64 (97.1 examples/sec; 0.041 sec/batch; 1h:10m:35s remains)
INFO - root - 2019-11-06 19:04:14.210567: step 47220, total loss = 3.68, predict loss = 1.06 (89.0 examples/sec; 0.045 sec/batch; 1h:16m:57s remains)
INFO - root - 2019-11-06 19:04:15.499163: step 47230, total loss = 1.91, predict loss = 0.49 (57.6 examples/sec; 0.069 sec/batch; 1h:59m:01s remains)
INFO - root - 2019-11-06 19:04:16.249502: step 47240, total loss = 1.59, predict loss = 0.40 (53.7 examples/sec; 0.075 sec/batch; 2h:07m:36s remains)
INFO - root - 2019-11-06 19:04:16.983876: step 47250, total loss = 1.75, predict loss = 0.44 (56.3 examples/sec; 0.071 sec/batch; 2h:01m:46s remains)
INFO - root - 2019-11-06 19:04:17.759388: step 47260, total loss = 2.00, predict loss = 0.52 (54.0 examples/sec; 0.074 sec/batch; 2h:06m:55s remains)
INFO - root - 2019-11-06 19:04:18.482901: step 47270, total loss = 3.46, predict loss = 0.96 (64.9 examples/sec; 0.062 sec/batch; 1h:45m:32s remains)
INFO - root - 2019-11-06 19:04:18.988912: step 47280, total loss = 2.96, predict loss = 0.84 (99.4 examples/sec; 0.040 sec/batch; 1h:08m:54s remains)
INFO - root - 2019-11-06 19:04:19.442144: step 47290, total loss = 1.91, predict loss = 0.54 (100.3 examples/sec; 0.040 sec/batch; 1h:08m:17s remains)
INFO - root - 2019-11-06 19:04:20.682348: step 47300, total loss = 2.23, predict loss = 0.58 (61.9 examples/sec; 0.065 sec/batch; 1h:50m:37s remains)
INFO - root - 2019-11-06 19:04:21.381055: step 47310, total loss = 2.51, predict loss = 0.72 (53.6 examples/sec; 0.075 sec/batch; 2h:07m:48s remains)
INFO - root - 2019-11-06 19:04:22.242424: step 47320, total loss = 2.37, predict loss = 0.60 (49.6 examples/sec; 0.081 sec/batch; 2h:17m:52s remains)
INFO - root - 2019-11-06 19:04:22.998021: step 47330, total loss = 2.47, predict loss = 0.62 (59.1 examples/sec; 0.068 sec/batch; 1h:55m:52s remains)
INFO - root - 2019-11-06 19:04:23.762624: step 47340, total loss = 2.29, predict loss = 0.59 (58.8 examples/sec; 0.068 sec/batch; 1h:56m:26s remains)
INFO - root - 2019-11-06 19:04:24.345033: step 47350, total loss = 3.30, predict loss = 0.93 (98.3 examples/sec; 0.041 sec/batch; 1h:09m:35s remains)
INFO - root - 2019-11-06 19:04:24.805799: step 47360, total loss = 2.66, predict loss = 0.74 (94.6 examples/sec; 0.042 sec/batch; 1h:12m:22s remains)
INFO - root - 2019-11-06 19:04:25.273281: step 47370, total loss = 3.30, predict loss = 0.94 (84.9 examples/sec; 0.047 sec/batch; 1h:20m:35s remains)
INFO - root - 2019-11-06 19:04:26.597850: step 47380, total loss = 1.96, predict loss = 0.54 (54.1 examples/sec; 0.074 sec/batch; 2h:06m:22s remains)
INFO - root - 2019-11-06 19:04:27.333022: step 47390, total loss = 2.62, predict loss = 0.69 (52.7 examples/sec; 0.076 sec/batch; 2h:09m:41s remains)
INFO - root - 2019-11-06 19:04:28.095213: step 47400, total loss = 2.17, predict loss = 0.61 (66.1 examples/sec; 0.060 sec/batch; 1h:43m:26s remains)
INFO - root - 2019-11-06 19:04:28.823998: step 47410, total loss = 2.67, predict loss = 0.72 (60.5 examples/sec; 0.066 sec/batch; 1h:52m:57s remains)
INFO - root - 2019-11-06 19:04:29.554132: step 47420, total loss = 2.91, predict loss = 0.77 (69.2 examples/sec; 0.058 sec/batch; 1h:38m:46s remains)
INFO - root - 2019-11-06 19:04:30.052239: step 47430, total loss = 2.73, predict loss = 0.71 (95.8 examples/sec; 0.042 sec/batch; 1h:11m:24s remains)
INFO - root - 2019-11-06 19:04:30.494115: step 47440, total loss = 2.58, predict loss = 0.68 (93.3 examples/sec; 0.043 sec/batch; 1h:13m:16s remains)
INFO - root - 2019-11-06 19:04:31.719366: step 47450, total loss = 1.81, predict loss = 0.47 (66.7 examples/sec; 0.060 sec/batch; 1h:42m:34s remains)
INFO - root - 2019-11-06 19:04:32.448213: step 47460, total loss = 3.68, predict loss = 1.15 (61.3 examples/sec; 0.065 sec/batch; 1h:51m:33s remains)
INFO - root - 2019-11-06 19:04:33.189248: step 47470, total loss = 1.10, predict loss = 0.30 (60.0 examples/sec; 0.067 sec/batch; 1h:53m:56s remains)
INFO - root - 2019-11-06 19:04:33.890862: step 47480, total loss = 2.47, predict loss = 0.63 (54.5 examples/sec; 0.073 sec/batch; 2h:05m:30s remains)
INFO - root - 2019-11-06 19:04:34.642897: step 47490, total loss = 1.82, predict loss = 0.49 (60.5 examples/sec; 0.066 sec/batch; 1h:52m:52s remains)
INFO - root - 2019-11-06 19:04:35.248093: step 47500, total loss = 1.92, predict loss = 0.50 (99.2 examples/sec; 0.040 sec/batch; 1h:08m:53s remains)
INFO - root - 2019-11-06 19:04:35.708094: step 47510, total loss = 1.61, predict loss = 0.40 (93.5 examples/sec; 0.043 sec/batch; 1h:13m:04s remains)
INFO - root - 2019-11-06 19:04:36.153709: step 47520, total loss = 1.80, predict loss = 0.43 (126.6 examples/sec; 0.032 sec/batch; 0h:53m:57s remains)
INFO - root - 2019-11-06 19:04:37.537788: step 47530, total loss = 1.28, predict loss = 0.32 (59.4 examples/sec; 0.067 sec/batch; 1h:54m:56s remains)
INFO - root - 2019-11-06 19:04:38.306685: step 47540, total loss = 2.63, predict loss = 0.67 (62.1 examples/sec; 0.064 sec/batch; 1h:49m:54s remains)
INFO - root - 2019-11-06 19:04:39.052227: step 47550, total loss = 1.22, predict loss = 0.32 (54.2 examples/sec; 0.074 sec/batch; 2h:06m:01s remains)
INFO - root - 2019-11-06 19:04:39.768430: step 47560, total loss = 1.30, predict loss = 0.29 (63.8 examples/sec; 0.063 sec/batch; 1h:47m:07s remains)
INFO - root - 2019-11-06 19:04:40.429500: step 47570, total loss = 1.71, predict loss = 0.43 (86.8 examples/sec; 0.046 sec/batch; 1h:18m:42s remains)
INFO - root - 2019-11-06 19:04:40.900244: step 47580, total loss = 2.06, predict loss = 0.52 (102.6 examples/sec; 0.039 sec/batch; 1h:06m:32s remains)
INFO - root - 2019-11-06 19:04:41.367795: step 47590, total loss = 3.57, predict loss = 1.02 (88.5 examples/sec; 0.045 sec/batch; 1h:17m:07s remains)
INFO - root - 2019-11-06 19:04:42.577594: step 47600, total loss = 2.84, predict loss = 0.92 (64.7 examples/sec; 0.062 sec/batch; 1h:45m:28s remains)
INFO - root - 2019-11-06 19:04:43.284896: step 47610, total loss = 1.61, predict loss = 0.44 (60.8 examples/sec; 0.066 sec/batch; 1h:52m:11s remains)
INFO - root - 2019-11-06 19:04:44.067458: step 47620, total loss = 1.51, predict loss = 0.37 (60.2 examples/sec; 0.066 sec/batch; 1h:53m:24s remains)
INFO - root - 2019-11-06 19:04:44.833554: step 47630, total loss = 2.64, predict loss = 0.64 (59.4 examples/sec; 0.067 sec/batch; 1h:54m:54s remains)
INFO - root - 2019-11-06 19:04:45.555587: step 47640, total loss = 1.79, predict loss = 0.45 (68.0 examples/sec; 0.059 sec/batch; 1h:40m:22s remains)
INFO - root - 2019-11-06 19:04:46.108227: step 47650, total loss = 2.27, predict loss = 0.56 (95.7 examples/sec; 0.042 sec/batch; 1h:11m:15s remains)
INFO - root - 2019-11-06 19:04:46.591797: step 47660, total loss = 1.74, predict loss = 0.43 (93.4 examples/sec; 0.043 sec/batch; 1h:13m:02s remains)
INFO - root - 2019-11-06 19:04:47.740626: step 47670, total loss = 2.27, predict loss = 0.64 (5.4 examples/sec; 0.739 sec/batch; 21h:00m:32s remains)
INFO - root - 2019-11-06 19:04:48.424315: step 47680, total loss = 3.13, predict loss = 0.87 (53.5 examples/sec; 0.075 sec/batch; 2h:07m:24s remains)
INFO - root - 2019-11-06 19:04:49.174884: step 47690, total loss = 3.04, predict loss = 0.83 (57.8 examples/sec; 0.069 sec/batch; 1h:57m:59s remains)
INFO - root - 2019-11-06 19:04:49.986282: step 47700, total loss = 2.33, predict loss = 0.67 (53.6 examples/sec; 0.075 sec/batch; 2h:07m:08s remains)
INFO - root - 2019-11-06 19:04:50.685131: step 47710, total loss = 1.99, predict loss = 0.49 (66.5 examples/sec; 0.060 sec/batch; 1h:42m:29s remains)
INFO - root - 2019-11-06 19:04:51.275068: step 47720, total loss = 2.75, predict loss = 0.79 (96.2 examples/sec; 0.042 sec/batch; 1h:10m:53s remains)
INFO - root - 2019-11-06 19:04:51.738115: step 47730, total loss = 2.38, predict loss = 0.65 (95.7 examples/sec; 0.042 sec/batch; 1h:11m:12s remains)
INFO - root - 2019-11-06 19:04:52.245246: step 47740, total loss = 2.72, predict loss = 0.69 (87.4 examples/sec; 0.046 sec/batch; 1h:17m:57s remains)
INFO - root - 2019-11-06 19:04:53.467582: step 47750, total loss = 1.86, predict loss = 0.47 (60.7 examples/sec; 0.066 sec/batch; 1h:52m:18s remains)
INFO - root - 2019-11-06 19:04:54.205825: step 47760, total loss = 1.66, predict loss = 0.42 (53.7 examples/sec; 0.075 sec/batch; 2h:06m:57s remains)
INFO - root - 2019-11-06 19:04:54.966062: step 47770, total loss = 2.67, predict loss = 0.72 (56.2 examples/sec; 0.071 sec/batch; 2h:01m:16s remains)
INFO - root - 2019-11-06 19:04:55.723324: step 47780, total loss = 1.56, predict loss = 0.39 (60.9 examples/sec; 0.066 sec/batch; 1h:51m:53s remains)
INFO - root - 2019-11-06 19:04:56.498572: step 47790, total loss = 3.62, predict loss = 1.06 (57.0 examples/sec; 0.070 sec/batch; 1h:59m:26s remains)
INFO - root - 2019-11-06 19:04:57.047360: step 47800, total loss = 2.91, predict loss = 0.84 (100.4 examples/sec; 0.040 sec/batch; 1h:07m:51s remains)
INFO - root - 2019-11-06 19:04:57.494228: step 47810, total loss = 2.19, predict loss = 0.61 (96.7 examples/sec; 0.041 sec/batch; 1h:10m:25s remains)
INFO - root - 2019-11-06 19:04:58.713765: step 47820, total loss = 2.08, predict loss = 0.52 (69.1 examples/sec; 0.058 sec/batch; 1h:38m:34s remains)
INFO - root - 2019-11-06 19:04:59.379324: step 47830, total loss = 2.03, predict loss = 0.50 (59.6 examples/sec; 0.067 sec/batch; 1h:54m:22s remains)
INFO - root - 2019-11-06 19:05:00.125077: step 47840, total loss = 3.15, predict loss = 0.88 (53.0 examples/sec; 0.075 sec/batch; 2h:08m:23s remains)
INFO - root - 2019-11-06 19:05:00.934148: step 47850, total loss = 2.68, predict loss = 0.68 (67.8 examples/sec; 0.059 sec/batch; 1h:40m:28s remains)
INFO - root - 2019-11-06 19:05:01.686787: step 47860, total loss = 2.73, predict loss = 0.74 (68.3 examples/sec; 0.059 sec/batch; 1h:39m:37s remains)
INFO - root - 2019-11-06 19:05:02.338237: step 47870, total loss = 2.50, predict loss = 0.70 (90.7 examples/sec; 0.044 sec/batch; 1h:15m:04s remains)
INFO - root - 2019-11-06 19:05:02.793677: step 47880, total loss = 2.83, predict loss = 0.76 (93.5 examples/sec; 0.043 sec/batch; 1h:12m:49s remains)
INFO - root - 2019-11-06 19:05:03.255953: step 47890, total loss = 3.07, predict loss = 0.83 (100.6 examples/sec; 0.040 sec/batch; 1h:07m:38s remains)
INFO - root - 2019-11-06 19:05:04.548626: step 47900, total loss = 2.46, predict loss = 0.67 (59.0 examples/sec; 0.068 sec/batch; 1h:55m:16s remains)
INFO - root - 2019-11-06 19:05:05.243554: step 47910, total loss = 2.87, predict loss = 0.78 (72.5 examples/sec; 0.055 sec/batch; 1h:33m:53s remains)
INFO - root - 2019-11-06 19:05:05.976409: step 47920, total loss = 2.71, predict loss = 0.73 (57.0 examples/sec; 0.070 sec/batch; 1h:59m:23s remains)
INFO - root - 2019-11-06 19:05:06.737686: step 47930, total loss = 1.62, predict loss = 0.40 (50.2 examples/sec; 0.080 sec/batch; 2h:15m:26s remains)
INFO - root - 2019-11-06 19:05:07.453315: step 47940, total loss = 2.35, predict loss = 0.60 (72.0 examples/sec; 0.056 sec/batch; 1h:34m:28s remains)
INFO - root - 2019-11-06 19:05:07.957172: step 47950, total loss = 1.97, predict loss = 0.51 (101.5 examples/sec; 0.039 sec/batch; 1h:07m:00s remains)
INFO - root - 2019-11-06 19:05:08.417445: step 47960, total loss = 2.08, predict loss = 0.53 (92.4 examples/sec; 0.043 sec/batch; 1h:13m:35s remains)
INFO - root - 2019-11-06 19:05:09.614900: step 47970, total loss = 1.97, predict loss = 0.48 (60.0 examples/sec; 0.067 sec/batch; 1h:53m:17s remains)
INFO - root - 2019-11-06 19:05:10.316740: step 47980, total loss = 2.20, predict loss = 0.67 (62.6 examples/sec; 0.064 sec/batch; 1h:48m:39s remains)
INFO - root - 2019-11-06 19:05:11.044488: step 47990, total loss = 2.61, predict loss = 0.69 (58.8 examples/sec; 0.068 sec/batch; 1h:55m:40s remains)
INFO - root - 2019-11-06 19:05:11.807646: step 48000, total loss = 3.59, predict loss = 1.06 (55.2 examples/sec; 0.072 sec/batch; 2h:03m:13s remains)
INFO - root - 2019-11-06 19:05:12.552968: step 48010, total loss = 2.82, predict loss = 0.69 (56.2 examples/sec; 0.071 sec/batch; 2h:00m:57s remains)
INFO - root - 2019-11-06 19:05:13.209881: step 48020, total loss = 2.20, predict loss = 0.57 (100.2 examples/sec; 0.040 sec/batch; 1h:07m:49s remains)
INFO - root - 2019-11-06 19:05:13.663775: step 48030, total loss = 1.58, predict loss = 0.41 (103.1 examples/sec; 0.039 sec/batch; 1h:05m:56s remains)
INFO - root - 2019-11-06 19:05:14.128895: step 48040, total loss = 1.64, predict loss = 0.48 (90.8 examples/sec; 0.044 sec/batch; 1h:14m:51s remains)
INFO - root - 2019-11-06 19:05:15.451536: step 48050, total loss = 2.30, predict loss = 0.62 (61.2 examples/sec; 0.065 sec/batch; 1h:51m:04s remains)
INFO - root - 2019-11-06 19:05:16.212423: step 48060, total loss = 3.29, predict loss = 0.97 (57.0 examples/sec; 0.070 sec/batch; 1h:59m:14s remains)
INFO - root - 2019-11-06 19:05:16.954681: step 48070, total loss = 2.24, predict loss = 0.54 (55.5 examples/sec; 0.072 sec/batch; 2h:02m:20s remains)
INFO - root - 2019-11-06 19:05:17.711165: step 48080, total loss = 1.72, predict loss = 0.43 (55.1 examples/sec; 0.073 sec/batch; 2h:03m:19s remains)
INFO - root - 2019-11-06 19:05:18.480250: step 48090, total loss = 2.37, predict loss = 0.72 (67.5 examples/sec; 0.059 sec/batch; 1h:40m:37s remains)
INFO - root - 2019-11-06 19:05:19.020320: step 48100, total loss = 3.02, predict loss = 0.83 (96.3 examples/sec; 0.042 sec/batch; 1h:10m:32s remains)
INFO - root - 2019-11-06 19:05:19.478341: step 48110, total loss = 2.58, predict loss = 0.69 (93.5 examples/sec; 0.043 sec/batch; 1h:12m:36s remains)
INFO - root - 2019-11-06 19:05:20.649041: step 48120, total loss = 2.31, predict loss = 0.56 (67.1 examples/sec; 0.060 sec/batch; 1h:41m:12s remains)
INFO - root - 2019-11-06 19:05:21.341113: step 48130, total loss = 2.98, predict loss = 0.83 (65.0 examples/sec; 0.062 sec/batch; 1h:44m:33s remains)
INFO - root - 2019-11-06 19:05:22.070561: step 48140, total loss = 3.55, predict loss = 1.01 (56.4 examples/sec; 0.071 sec/batch; 2h:00m:21s remains)
INFO - root - 2019-11-06 19:05:22.854446: step 48150, total loss = 1.57, predict loss = 0.39 (50.5 examples/sec; 0.079 sec/batch; 2h:14m:23s remains)
INFO - root - 2019-11-06 19:05:23.581091: step 48160, total loss = 2.62, predict loss = 0.66 (59.7 examples/sec; 0.067 sec/batch; 1h:53m:48s remains)
INFO - root - 2019-11-06 19:05:24.160764: step 48170, total loss = 2.44, predict loss = 0.72 (101.9 examples/sec; 0.039 sec/batch; 1h:06m:38s remains)
INFO - root - 2019-11-06 19:05:24.652475: step 48180, total loss = 2.74, predict loss = 0.75 (89.7 examples/sec; 0.045 sec/batch; 1h:15m:42s remains)
INFO - root - 2019-11-06 19:05:25.104959: step 48190, total loss = 2.21, predict loss = 0.64 (86.7 examples/sec; 0.046 sec/batch; 1h:18m:16s remains)
INFO - root - 2019-11-06 19:05:26.477131: step 48200, total loss = 1.79, predict loss = 0.43 (60.8 examples/sec; 0.066 sec/batch; 1h:51m:39s remains)
INFO - root - 2019-11-06 19:05:27.256563: step 48210, total loss = 2.37, predict loss = 0.63 (51.9 examples/sec; 0.077 sec/batch; 2h:10m:51s remains)
INFO - root - 2019-11-06 19:05:28.004645: step 48220, total loss = 2.38, predict loss = 0.62 (61.4 examples/sec; 0.065 sec/batch; 1h:50m:33s remains)
INFO - root - 2019-11-06 19:05:28.792745: step 48230, total loss = 3.07, predict loss = 0.87 (59.4 examples/sec; 0.067 sec/batch; 1h:54m:15s remains)
INFO - root - 2019-11-06 19:05:29.504045: step 48240, total loss = 1.75, predict loss = 0.46 (64.2 examples/sec; 0.062 sec/batch; 1h:45m:44s remains)
INFO - root - 2019-11-06 19:05:29.998263: step 48250, total loss = 3.26, predict loss = 0.86 (99.3 examples/sec; 0.040 sec/batch; 1h:08m:19s remains)
INFO - root - 2019-11-06 19:05:30.469801: step 48260, total loss = 4.41, predict loss = 1.51 (97.4 examples/sec; 0.041 sec/batch; 1h:09m:39s remains)
INFO - root - 2019-11-06 19:05:31.690534: step 48270, total loss = 2.96, predict loss = 0.79 (57.3 examples/sec; 0.070 sec/batch; 1h:58m:19s remains)
INFO - root - 2019-11-06 19:05:32.500967: step 48280, total loss = 2.88, predict loss = 0.74 (55.9 examples/sec; 0.072 sec/batch; 2h:01m:19s remains)
INFO - root - 2019-11-06 19:05:33.335440: step 48290, total loss = 3.03, predict loss = 0.87 (53.7 examples/sec; 0.075 sec/batch; 2h:06m:21s remains)
INFO - root - 2019-11-06 19:05:34.078121: step 48300, total loss = 2.42, predict loss = 0.64 (60.9 examples/sec; 0.066 sec/batch; 1h:51m:15s remains)
INFO - root - 2019-11-06 19:05:34.876591: step 48310, total loss = 1.44, predict loss = 0.36 (63.0 examples/sec; 0.064 sec/batch; 1h:47m:37s remains)
INFO - root - 2019-11-06 19:05:35.434056: step 48320, total loss = 1.32, predict loss = 0.35 (96.8 examples/sec; 0.041 sec/batch; 1h:10m:02s remains)
INFO - root - 2019-11-06 19:05:35.905458: step 48330, total loss = 2.60, predict loss = 0.64 (96.8 examples/sec; 0.041 sec/batch; 1h:09m:59s remains)
INFO - root - 2019-11-06 19:05:36.393306: step 48340, total loss = 3.65, predict loss = 1.07 (126.0 examples/sec; 0.032 sec/batch; 0h:53m:47s remains)
INFO - root - 2019-11-06 19:05:37.782525: step 48350, total loss = 2.69, predict loss = 0.73 (63.2 examples/sec; 0.063 sec/batch; 1h:47m:12s remains)
INFO - root - 2019-11-06 19:05:38.494136: step 48360, total loss = 1.96, predict loss = 0.48 (71.5 examples/sec; 0.056 sec/batch; 1h:34m:49s remains)
INFO - root - 2019-11-06 19:05:39.267407: step 48370, total loss = 2.12, predict loss = 0.52 (57.8 examples/sec; 0.069 sec/batch; 1h:57m:17s remains)
INFO - root - 2019-11-06 19:05:40.044903: step 48380, total loss = 2.28, predict loss = 0.61 (59.9 examples/sec; 0.067 sec/batch; 1h:53m:07s remains)
INFO - root - 2019-11-06 19:05:40.740099: step 48390, total loss = 3.60, predict loss = 1.13 (79.1 examples/sec; 0.051 sec/batch; 1h:25m:35s remains)
INFO - root - 2019-11-06 19:05:41.199452: step 48400, total loss = 1.69, predict loss = 0.44 (96.9 examples/sec; 0.041 sec/batch; 1h:09m:54s remains)
INFO - root - 2019-11-06 19:05:41.659129: step 48410, total loss = 3.16, predict loss = 0.91 (93.8 examples/sec; 0.043 sec/batch; 1h:12m:10s remains)
INFO - root - 2019-11-06 19:05:42.915698: step 48420, total loss = 2.33, predict loss = 0.70 (68.9 examples/sec; 0.058 sec/batch; 1h:38m:17s remains)
INFO - root - 2019-11-06 19:05:43.632328: step 48430, total loss = 2.34, predict loss = 0.61 (53.1 examples/sec; 0.075 sec/batch; 2h:07m:34s remains)
INFO - root - 2019-11-06 19:05:44.447412: step 48440, total loss = 1.79, predict loss = 0.44 (47.4 examples/sec; 0.084 sec/batch; 2h:22m:52s remains)
INFO - root - 2019-11-06 19:05:45.195827: step 48450, total loss = 2.48, predict loss = 0.67 (56.4 examples/sec; 0.071 sec/batch; 2h:00m:02s remains)
INFO - root - 2019-11-06 19:05:45.933865: step 48460, total loss = 2.18, predict loss = 0.56 (61.1 examples/sec; 0.065 sec/batch; 1h:50m:48s remains)
INFO - root - 2019-11-06 19:05:46.511898: step 48470, total loss = 2.17, predict loss = 0.61 (101.2 examples/sec; 0.040 sec/batch; 1h:06m:51s remains)
INFO - root - 2019-11-06 19:05:46.953550: step 48480, total loss = 3.66, predict loss = 1.10 (96.5 examples/sec; 0.041 sec/batch; 1h:10m:09s remains)
INFO - root - 2019-11-06 19:05:48.098029: step 48490, total loss = 2.37, predict loss = 0.66 (5.4 examples/sec; 0.743 sec/batch; 20h:56m:46s remains)
INFO - root - 2019-11-06 19:05:48.816969: step 48500, total loss = 2.75, predict loss = 0.78 (59.8 examples/sec; 0.067 sec/batch; 1h:53m:11s remains)
INFO - root - 2019-11-06 19:05:49.535801: step 48510, total loss = 3.42, predict loss = 1.01 (50.8 examples/sec; 0.079 sec/batch; 2h:13m:09s remains)
INFO - root - 2019-11-06 19:05:50.262176: step 48520, total loss = 3.13, predict loss = 0.89 (55.1 examples/sec; 0.073 sec/batch; 2h:02m:45s remains)
INFO - root - 2019-11-06 19:05:51.004773: step 48530, total loss = 1.94, predict loss = 0.47 (58.5 examples/sec; 0.068 sec/batch; 1h:55m:33s remains)
INFO - root - 2019-11-06 19:05:51.683555: step 48540, total loss = 2.15, predict loss = 0.58 (91.4 examples/sec; 0.044 sec/batch; 1h:13m:58s remains)
INFO - root - 2019-11-06 19:05:52.153823: step 48550, total loss = 1.69, predict loss = 0.40 (96.8 examples/sec; 0.041 sec/batch; 1h:09m:53s remains)
INFO - root - 2019-11-06 19:05:52.598461: step 48560, total loss = 1.47, predict loss = 0.39 (98.0 examples/sec; 0.041 sec/batch; 1h:08m:58s remains)
INFO - root - 2019-11-06 19:05:53.822035: step 48570, total loss = 3.04, predict loss = 0.86 (70.1 examples/sec; 0.057 sec/batch; 1h:36m:30s remains)
INFO - root - 2019-11-06 19:05:54.588586: step 48580, total loss = 2.67, predict loss = 0.72 (53.6 examples/sec; 0.075 sec/batch; 2h:06m:09s remains)
INFO - root - 2019-11-06 19:05:55.339109: step 48590, total loss = 2.07, predict loss = 0.55 (58.2 examples/sec; 0.069 sec/batch; 1h:56m:14s remains)
INFO - root - 2019-11-06 19:05:56.143572: step 48600, total loss = 2.40, predict loss = 0.67 (55.7 examples/sec; 0.072 sec/batch; 2h:01m:27s remains)
INFO - root - 2019-11-06 19:05:56.846482: step 48610, total loss = 1.48, predict loss = 0.35 (72.2 examples/sec; 0.055 sec/batch; 1h:33m:39s remains)
INFO - root - 2019-11-06 19:05:57.408872: step 48620, total loss = 2.38, predict loss = 0.61 (98.4 examples/sec; 0.041 sec/batch; 1h:08m:41s remains)
INFO - root - 2019-11-06 19:05:57.868418: step 48630, total loss = 2.58, predict loss = 0.68 (94.2 examples/sec; 0.042 sec/batch; 1h:11m:44s remains)
INFO - root - 2019-11-06 19:05:58.988763: step 48640, total loss = 3.54, predict loss = 1.11 (76.4 examples/sec; 0.052 sec/batch; 1h:28m:29s remains)
INFO - root - 2019-11-06 19:05:59.693553: step 48650, total loss = 1.68, predict loss = 0.44 (59.4 examples/sec; 0.067 sec/batch; 1h:53m:44s remains)
INFO - root - 2019-11-06 19:06:00.458553: step 48660, total loss = 2.30, predict loss = 0.59 (59.3 examples/sec; 0.067 sec/batch; 1h:53m:58s remains)
INFO - root - 2019-11-06 19:06:01.188608: step 48670, total loss = 0.94, predict loss = 0.27 (63.4 examples/sec; 0.063 sec/batch; 1h:46m:29s remains)
INFO - root - 2019-11-06 19:06:01.991642: step 48680, total loss = 1.77, predict loss = 0.40 (52.0 examples/sec; 0.077 sec/batch; 2h:09m:52s remains)
INFO - root - 2019-11-06 19:06:02.622788: step 48690, total loss = 2.69, predict loss = 0.71 (87.0 examples/sec; 0.046 sec/batch; 1h:17m:36s remains)
INFO - root - 2019-11-06 19:06:03.107940: step 48700, total loss = 1.46, predict loss = 0.37 (94.9 examples/sec; 0.042 sec/batch; 1h:11m:11s remains)
INFO - root - 2019-11-06 19:06:03.567616: step 48710, total loss = 2.13, predict loss = 0.54 (98.8 examples/sec; 0.041 sec/batch; 1h:08m:22s remains)
INFO - root - 2019-11-06 19:06:04.831744: step 48720, total loss = 2.24, predict loss = 0.55 (52.7 examples/sec; 0.076 sec/batch; 2h:08m:03s remains)
INFO - root - 2019-11-06 19:06:05.548903: step 48730, total loss = 2.13, predict loss = 0.54 (61.4 examples/sec; 0.065 sec/batch; 1h:49m:58s remains)
INFO - root - 2019-11-06 19:06:06.287058: step 48740, total loss = 2.70, predict loss = 0.70 (60.5 examples/sec; 0.066 sec/batch; 1h:51m:33s remains)
INFO - root - 2019-11-06 19:06:07.083703: step 48750, total loss = 2.52, predict loss = 0.72 (55.5 examples/sec; 0.072 sec/batch; 2h:01m:37s remains)
INFO - root - 2019-11-06 19:06:07.813309: step 48760, total loss = 2.05, predict loss = 0.54 (65.0 examples/sec; 0.062 sec/batch; 1h:43m:51s remains)
INFO - root - 2019-11-06 19:06:08.352699: step 48770, total loss = 2.19, predict loss = 0.56 (98.7 examples/sec; 0.041 sec/batch; 1h:08m:21s remains)
INFO - root - 2019-11-06 19:06:08.827993: step 48780, total loss = 3.47, predict loss = 1.10 (95.4 examples/sec; 0.042 sec/batch; 1h:10m:44s remains)
INFO - root - 2019-11-06 19:06:09.956041: step 48790, total loss = 3.09, predict loss = 0.86 (61.6 examples/sec; 0.065 sec/batch; 1h:49m:30s remains)
INFO - root - 2019-11-06 19:06:10.607061: step 48800, total loss = 1.22, predict loss = 0.33 (70.5 examples/sec; 0.057 sec/batch; 1h:35m:38s remains)
INFO - root - 2019-11-06 19:06:11.307697: step 48810, total loss = 1.35, predict loss = 0.36 (65.5 examples/sec; 0.061 sec/batch; 1h:43m:03s remains)
INFO - root - 2019-11-06 19:06:12.033799: step 48820, total loss = 1.37, predict loss = 0.35 (59.2 examples/sec; 0.068 sec/batch; 1h:53m:51s remains)
INFO - root - 2019-11-06 19:06:12.791393: step 48830, total loss = 2.59, predict loss = 0.67 (59.8 examples/sec; 0.067 sec/batch; 1h:52m:48s remains)
INFO - root - 2019-11-06 19:06:13.392588: step 48840, total loss = 2.35, predict loss = 0.72 (96.6 examples/sec; 0.041 sec/batch; 1h:09m:47s remains)
INFO - root - 2019-11-06 19:06:13.846310: step 48850, total loss = 2.00, predict loss = 0.50 (91.1 examples/sec; 0.044 sec/batch; 1h:14m:02s remains)
INFO - root - 2019-11-06 19:06:14.323015: step 48860, total loss = 1.84, predict loss = 0.47 (94.8 examples/sec; 0.042 sec/batch; 1h:11m:06s remains)
INFO - root - 2019-11-06 19:06:15.585643: step 48870, total loss = 2.07, predict loss = 0.57 (58.2 examples/sec; 0.069 sec/batch; 1h:55m:55s remains)
INFO - root - 2019-11-06 19:06:16.288405: step 48880, total loss = 1.78, predict loss = 0.53 (66.5 examples/sec; 0.060 sec/batch; 1h:41m:18s remains)
INFO - root - 2019-11-06 19:06:17.016157: step 48890, total loss = 3.41, predict loss = 0.97 (63.4 examples/sec; 0.063 sec/batch; 1h:46m:19s remains)
INFO - root - 2019-11-06 19:06:17.816081: step 48900, total loss = 2.88, predict loss = 0.77 (57.8 examples/sec; 0.069 sec/batch; 1h:56m:34s remains)
INFO - root - 2019-11-06 19:06:18.583289: step 48910, total loss = 2.27, predict loss = 0.56 (65.2 examples/sec; 0.061 sec/batch; 1h:43m:17s remains)
INFO - root - 2019-11-06 19:06:19.110731: step 48920, total loss = 2.03, predict loss = 0.51 (93.8 examples/sec; 0.043 sec/batch; 1h:11m:51s remains)
INFO - root - 2019-11-06 19:06:19.563309: step 48930, total loss = 2.02, predict loss = 0.55 (90.8 examples/sec; 0.044 sec/batch; 1h:14m:12s remains)
INFO - root - 2019-11-06 19:06:20.734921: step 48940, total loss = 2.06, predict loss = 0.60 (68.3 examples/sec; 0.059 sec/batch; 1h:38m:35s remains)
INFO - root - 2019-11-06 19:06:21.447381: step 48950, total loss = 0.91, predict loss = 0.24 (60.8 examples/sec; 0.066 sec/batch; 1h:50m:49s remains)
INFO - root - 2019-11-06 19:06:22.276282: step 48960, total loss = 1.94, predict loss = 0.47 (57.4 examples/sec; 0.070 sec/batch; 1h:57m:15s remains)
INFO - root - 2019-11-06 19:06:23.108875: step 48970, total loss = 2.89, predict loss = 0.76 (58.3 examples/sec; 0.069 sec/batch; 1h:55m:31s remains)
INFO - root - 2019-11-06 19:06:23.888351: step 48980, total loss = 2.15, predict loss = 0.52 (56.3 examples/sec; 0.071 sec/batch; 1h:59m:41s remains)
INFO - root - 2019-11-06 19:06:24.493468: step 48990, total loss = 2.48, predict loss = 0.65 (95.8 examples/sec; 0.042 sec/batch; 1h:10m:19s remains)
INFO - root - 2019-11-06 19:06:24.955908: step 49000, total loss = 2.50, predict loss = 0.70 (94.4 examples/sec; 0.042 sec/batch; 1h:11m:17s remains)
INFO - root - 2019-11-06 19:06:25.413206: step 49010, total loss = 2.05, predict loss = 0.51 (88.0 examples/sec; 0.045 sec/batch; 1h:16m:28s remains)
INFO - root - 2019-11-06 19:06:26.800031: step 49020, total loss = 2.23, predict loss = 0.59 (55.9 examples/sec; 0.072 sec/batch; 2h:00m:32s remains)
INFO - root - 2019-11-06 19:06:27.549069: step 49030, total loss = 2.57, predict loss = 0.77 (50.6 examples/sec; 0.079 sec/batch; 2h:12m:56s remains)
INFO - root - 2019-11-06 19:06:28.300298: step 49040, total loss = 1.97, predict loss = 0.51 (61.9 examples/sec; 0.065 sec/batch; 1h:48m:45s remains)
INFO - root - 2019-11-06 19:06:29.052323: step 49050, total loss = 1.64, predict loss = 0.41 (57.2 examples/sec; 0.070 sec/batch; 1h:57m:44s remains)
INFO - root - 2019-11-06 19:06:29.743145: step 49060, total loss = 2.31, predict loss = 0.55 (76.2 examples/sec; 0.052 sec/batch; 1h:28m:16s remains)
INFO - root - 2019-11-06 19:06:30.208120: step 49070, total loss = 2.21, predict loss = 0.59 (99.0 examples/sec; 0.040 sec/batch; 1h:07m:56s remains)
INFO - root - 2019-11-06 19:06:30.663572: step 49080, total loss = 2.34, predict loss = 0.65 (99.7 examples/sec; 0.040 sec/batch; 1h:07m:29s remains)
INFO - root - 2019-11-06 19:06:31.867457: step 49090, total loss = 2.62, predict loss = 0.67 (63.9 examples/sec; 0.063 sec/batch; 1h:45m:15s remains)
INFO - root - 2019-11-06 19:06:32.603522: step 49100, total loss = 1.65, predict loss = 0.45 (62.7 examples/sec; 0.064 sec/batch; 1h:47m:12s remains)
INFO - root - 2019-11-06 19:06:33.398335: step 49110, total loss = 2.66, predict loss = 0.69 (56.2 examples/sec; 0.071 sec/batch; 1h:59m:44s remains)
INFO - root - 2019-11-06 19:06:34.233983: step 49120, total loss = 1.76, predict loss = 0.45 (50.3 examples/sec; 0.080 sec/batch; 2h:13m:47s remains)
INFO - root - 2019-11-06 19:06:34.961786: step 49130, total loss = 2.22, predict loss = 0.57 (64.0 examples/sec; 0.062 sec/batch; 1h:45m:03s remains)
INFO - root - 2019-11-06 19:06:35.527920: step 49140, total loss = 1.37, predict loss = 0.34 (101.1 examples/sec; 0.040 sec/batch; 1h:06m:32s remains)
INFO - root - 2019-11-06 19:06:35.987669: step 49150, total loss = 2.28, predict loss = 0.55 (91.4 examples/sec; 0.044 sec/batch; 1h:13m:35s remains)
INFO - root - 2019-11-06 19:06:36.431096: step 49160, total loss = 2.61, predict loss = 0.66 (122.0 examples/sec; 0.033 sec/batch; 0h:55m:06s remains)
INFO - root - 2019-11-06 19:06:37.781538: step 49170, total loss = 2.37, predict loss = 0.61 (54.6 examples/sec; 0.073 sec/batch; 2h:03m:08s remains)
INFO - root - 2019-11-06 19:06:38.547866: step 49180, total loss = 2.87, predict loss = 0.73 (56.4 examples/sec; 0.071 sec/batch; 1h:59m:16s remains)
INFO - root - 2019-11-06 19:06:39.366312: step 49190, total loss = 2.42, predict loss = 0.60 (59.4 examples/sec; 0.067 sec/batch; 1h:53m:07s remains)
INFO - root - 2019-11-06 19:06:40.139024: step 49200, total loss = 2.04, predict loss = 0.49 (56.5 examples/sec; 0.071 sec/batch; 1h:59m:01s remains)
INFO - root - 2019-11-06 19:06:40.854333: step 49210, total loss = 2.33, predict loss = 0.64 (76.3 examples/sec; 0.052 sec/batch; 1h:28m:01s remains)
INFO - root - 2019-11-06 19:06:41.356833: step 49220, total loss = 3.17, predict loss = 0.89 (95.2 examples/sec; 0.042 sec/batch; 1h:10m:35s remains)
INFO - root - 2019-11-06 19:06:41.805909: step 49230, total loss = 2.35, predict loss = 0.63 (95.6 examples/sec; 0.042 sec/batch; 1h:10m:18s remains)
INFO - root - 2019-11-06 19:06:43.012352: step 49240, total loss = 1.29, predict loss = 0.30 (63.5 examples/sec; 0.063 sec/batch; 1h:45m:50s remains)
INFO - root - 2019-11-06 19:06:43.827218: step 49250, total loss = 2.64, predict loss = 0.67 (50.3 examples/sec; 0.080 sec/batch; 2h:13m:33s remains)
INFO - root - 2019-11-06 19:06:44.645068: step 49260, total loss = 2.64, predict loss = 0.73 (53.8 examples/sec; 0.074 sec/batch; 2h:04m:55s remains)
INFO - root - 2019-11-06 19:06:45.402762: step 49270, total loss = 1.68, predict loss = 0.50 (61.5 examples/sec; 0.065 sec/batch; 1h:49m:14s remains)
INFO - root - 2019-11-06 19:06:46.137355: step 49280, total loss = 1.93, predict loss = 0.49 (61.2 examples/sec; 0.065 sec/batch; 1h:49m:47s remains)
INFO - root - 2019-11-06 19:06:46.687618: step 49290, total loss = 2.59, predict loss = 0.77 (101.1 examples/sec; 0.040 sec/batch; 1h:06m:25s remains)
INFO - root - 2019-11-06 19:06:47.169751: step 49300, total loss = 1.90, predict loss = 0.49 (98.0 examples/sec; 0.041 sec/batch; 1h:08m:31s remains)
INFO - root - 2019-11-06 19:06:48.295361: step 49310, total loss = 2.09, predict loss = 0.50 (5.5 examples/sec; 0.729 sec/batch; 20h:22m:40s remains)
INFO - root - 2019-11-06 19:06:49.024660: step 49320, total loss = 1.83, predict loss = 0.48 (62.1 examples/sec; 0.064 sec/batch; 1h:48m:01s remains)
INFO - root - 2019-11-06 19:06:49.812163: step 49330, total loss = 2.31, predict loss = 0.57 (57.4 examples/sec; 0.070 sec/batch; 1h:56m:55s remains)
INFO - root - 2019-11-06 19:06:50.624622: step 49340, total loss = 3.17, predict loss = 0.87 (55.0 examples/sec; 0.073 sec/batch; 2h:02m:04s remains)
INFO - root - 2019-11-06 19:06:51.389369: step 49350, total loss = 2.32, predict loss = 0.59 (62.1 examples/sec; 0.064 sec/batch; 1h:47m:59s remains)
INFO - root - 2019-11-06 19:06:52.006556: step 49360, total loss = 2.72, predict loss = 0.73 (99.3 examples/sec; 0.040 sec/batch; 1h:07m:32s remains)
INFO - root - 2019-11-06 19:06:52.456697: step 49370, total loss = 2.10, predict loss = 0.57 (98.3 examples/sec; 0.041 sec/batch; 1h:08m:14s remains)
INFO - root - 2019-11-06 19:06:52.939261: step 49380, total loss = 2.59, predict loss = 0.70 (100.5 examples/sec; 0.040 sec/batch; 1h:06m:45s remains)
INFO - root - 2019-11-06 19:06:54.216767: step 49390, total loss = 2.58, predict loss = 0.70 (59.1 examples/sec; 0.068 sec/batch; 1h:53m:27s remains)
INFO - root - 2019-11-06 19:06:54.951975: step 49400, total loss = 3.00, predict loss = 0.87 (58.0 examples/sec; 0.069 sec/batch; 1h:55m:43s remains)
INFO - root - 2019-11-06 19:06:55.774590: step 49410, total loss = 3.62, predict loss = 1.14 (51.5 examples/sec; 0.078 sec/batch; 2h:10m:09s remains)
INFO - root - 2019-11-06 19:06:56.530487: step 49420, total loss = 2.18, predict loss = 0.54 (59.7 examples/sec; 0.067 sec/batch; 1h:52m:17s remains)
INFO - root - 2019-11-06 19:06:57.234814: step 49430, total loss = 2.94, predict loss = 0.87 (72.3 examples/sec; 0.055 sec/batch; 1h:32m:42s remains)
INFO - root - 2019-11-06 19:06:57.795930: step 49440, total loss = 1.83, predict loss = 0.45 (102.3 examples/sec; 0.039 sec/batch; 1h:05m:31s remains)
INFO - root - 2019-11-06 19:06:58.262464: step 49450, total loss = 2.62, predict loss = 0.72 (94.6 examples/sec; 0.042 sec/batch; 1h:10m:50s remains)
INFO - root - 2019-11-06 19:06:59.401050: step 49460, total loss = 1.64, predict loss = 0.42 (62.9 examples/sec; 0.064 sec/batch; 1h:46m:29s remains)
INFO - root - 2019-11-06 19:07:00.148395: step 49470, total loss = 1.35, predict loss = 0.38 (45.8 examples/sec; 0.087 sec/batch; 2h:26m:16s remains)
INFO - root - 2019-11-06 19:07:00.922058: step 49480, total loss = 1.46, predict loss = 0.40 (58.0 examples/sec; 0.069 sec/batch; 1h:55m:35s remains)
INFO - root - 2019-11-06 19:07:01.672952: step 49490, total loss = 2.82, predict loss = 0.74 (54.4 examples/sec; 0.073 sec/batch; 2h:03m:05s remains)
INFO - root - 2019-11-06 19:07:02.403422: step 49500, total loss = 2.45, predict loss = 0.62 (63.8 examples/sec; 0.063 sec/batch; 1h:44m:58s remains)
INFO - root - 2019-11-06 19:07:03.017371: step 49510, total loss = 3.26, predict loss = 0.96 (93.6 examples/sec; 0.043 sec/batch; 1h:11m:35s remains)
INFO - root - 2019-11-06 19:07:03.468139: step 49520, total loss = 1.74, predict loss = 0.46 (97.8 examples/sec; 0.041 sec/batch; 1h:08m:27s remains)
INFO - root - 2019-11-06 19:07:03.918195: step 49530, total loss = 3.14, predict loss = 0.92 (90.1 examples/sec; 0.044 sec/batch; 1h:14m:21s remains)
INFO - root - 2019-11-06 19:07:05.224887: step 49540, total loss = 2.22, predict loss = 0.60 (53.9 examples/sec; 0.074 sec/batch; 2h:04m:13s remains)
INFO - root - 2019-11-06 19:07:05.988576: step 49550, total loss = 2.84, predict loss = 0.74 (59.7 examples/sec; 0.067 sec/batch; 1h:52m:15s remains)
INFO - root - 2019-11-06 19:07:06.771527: step 49560, total loss = 3.17, predict loss = 0.98 (58.9 examples/sec; 0.068 sec/batch; 1h:53m:40s remains)
INFO - root - 2019-11-06 19:07:07.525166: step 49570, total loss = 2.00, predict loss = 0.56 (56.1 examples/sec; 0.071 sec/batch; 1h:59m:24s remains)
INFO - root - 2019-11-06 19:07:08.304173: step 49580, total loss = 2.63, predict loss = 0.67 (75.9 examples/sec; 0.053 sec/batch; 1h:28m:13s remains)
INFO - root - 2019-11-06 19:07:08.840905: step 49590, total loss = 2.02, predict loss = 0.54 (91.3 examples/sec; 0.044 sec/batch; 1h:13m:18s remains)
INFO - root - 2019-11-06 19:07:09.292111: step 49600, total loss = 2.35, predict loss = 0.66 (100.0 examples/sec; 0.040 sec/batch; 1h:06m:54s remains)
INFO - root - 2019-11-06 19:07:10.459785: step 49610, total loss = 2.29, predict loss = 0.57 (69.8 examples/sec; 0.057 sec/batch; 1h:35m:50s remains)
INFO - root - 2019-11-06 19:07:11.219839: step 49620, total loss = 2.99, predict loss = 0.84 (54.7 examples/sec; 0.073 sec/batch; 2h:02m:17s remains)
INFO - root - 2019-11-06 19:07:11.996094: step 49630, total loss = 2.65, predict loss = 0.76 (51.0 examples/sec; 0.078 sec/batch; 2h:11m:13s remains)
INFO - root - 2019-11-06 19:07:12.819838: step 49640, total loss = 2.29, predict loss = 0.56 (56.2 examples/sec; 0.071 sec/batch; 1h:59m:07s remains)
INFO - root - 2019-11-06 19:07:13.635395: step 49650, total loss = 2.57, predict loss = 0.68 (62.1 examples/sec; 0.064 sec/batch; 1h:47m:43s remains)
INFO - root - 2019-11-06 19:07:14.254757: step 49660, total loss = 2.58, predict loss = 0.65 (101.7 examples/sec; 0.039 sec/batch; 1h:05m:46s remains)
INFO - root - 2019-11-06 19:07:14.708475: step 49670, total loss = 3.51, predict loss = 1.10 (104.8 examples/sec; 0.038 sec/batch; 1h:03m:51s remains)
INFO - root - 2019-11-06 19:07:15.172989: step 49680, total loss = 1.65, predict loss = 0.50 (91.9 examples/sec; 0.044 sec/batch; 1h:12m:44s remains)
INFO - root - 2019-11-06 19:07:16.468044: step 49690, total loss = 1.71, predict loss = 0.43 (60.4 examples/sec; 0.066 sec/batch; 1h:50m:37s remains)
INFO - root - 2019-11-06 19:07:17.262888: step 49700, total loss = 1.62, predict loss = 0.43 (55.3 examples/sec; 0.072 sec/batch; 2h:01m:00s remains)
INFO - root - 2019-11-06 19:07:18.024649: step 49710, total loss = 1.56, predict loss = 0.40 (60.9 examples/sec; 0.066 sec/batch; 1h:49m:48s remains)
INFO - root - 2019-11-06 19:07:18.807269: step 49720, total loss = 3.16, predict loss = 0.83 (58.2 examples/sec; 0.069 sec/batch; 1h:54m:56s remains)
INFO - root - 2019-11-06 19:07:19.500909: step 49730, total loss = 3.68, predict loss = 1.04 (70.9 examples/sec; 0.056 sec/batch; 1h:34m:13s remains)
INFO - root - 2019-11-06 19:07:20.013153: step 49740, total loss = 1.90, predict loss = 0.54 (100.8 examples/sec; 0.040 sec/batch; 1h:06m:18s remains)
INFO - root - 2019-11-06 19:07:20.464098: step 49750, total loss = 3.52, predict loss = 1.02 (99.6 examples/sec; 0.040 sec/batch; 1h:07m:07s remains)
INFO - root - 2019-11-06 19:07:21.646429: step 49760, total loss = 1.46, predict loss = 0.38 (71.0 examples/sec; 0.056 sec/batch; 1h:34m:08s remains)
INFO - root - 2019-11-06 19:07:22.422843: step 49770, total loss = 2.50, predict loss = 0.72 (50.7 examples/sec; 0.079 sec/batch; 2h:11m:41s remains)
INFO - root - 2019-11-06 19:07:23.165618: step 49780, total loss = 2.77, predict loss = 0.75 (62.5 examples/sec; 0.064 sec/batch; 1h:46m:56s remains)
INFO - root - 2019-11-06 19:07:23.953608: step 49790, total loss = 1.72, predict loss = 0.43 (55.8 examples/sec; 0.072 sec/batch; 1h:59m:38s remains)
INFO - root - 2019-11-06 19:07:24.709211: step 49800, total loss = 1.58, predict loss = 0.35 (58.1 examples/sec; 0.069 sec/batch; 1h:54m:53s remains)
INFO - root - 2019-11-06 19:07:25.314119: step 49810, total loss = 1.83, predict loss = 0.44 (100.6 examples/sec; 0.040 sec/batch; 1h:06m:23s remains)
INFO - root - 2019-11-06 19:07:25.796325: step 49820, total loss = 2.14, predict loss = 0.54 (95.2 examples/sec; 0.042 sec/batch; 1h:10m:09s remains)
INFO - root - 2019-11-06 19:07:26.246803: step 49830, total loss = 2.69, predict loss = 0.76 (101.9 examples/sec; 0.039 sec/batch; 1h:05m:30s remains)
INFO - root - 2019-11-06 19:07:27.607186: step 49840, total loss = 3.23, predict loss = 0.92 (50.3 examples/sec; 0.079 sec/batch; 2h:12m:37s remains)
INFO - root - 2019-11-06 19:07:28.366691: step 49850, total loss = 1.73, predict loss = 0.43 (62.8 examples/sec; 0.064 sec/batch; 1h:46m:23s remains)
INFO - root - 2019-11-06 19:07:29.104483: step 49860, total loss = 2.04, predict loss = 0.54 (57.7 examples/sec; 0.069 sec/batch; 1h:55m:47s remains)
INFO - root - 2019-11-06 19:07:29.860617: step 49870, total loss = 1.84, predict loss = 0.49 (56.3 examples/sec; 0.071 sec/batch; 1h:58m:28s remains)
INFO - root - 2019-11-06 19:07:30.585676: step 49880, total loss = 1.79, predict loss = 0.50 (71.0 examples/sec; 0.056 sec/batch; 1h:33m:57s remains)
INFO - root - 2019-11-06 19:07:31.080106: step 49890, total loss = 2.25, predict loss = 0.56 (96.9 examples/sec; 0.041 sec/batch; 1h:08m:52s remains)
INFO - root - 2019-11-06 19:07:31.572950: step 49900, total loss = 1.94, predict loss = 0.50 (86.9 examples/sec; 0.046 sec/batch; 1h:16m:47s remains)
INFO - root - 2019-11-06 19:07:32.790990: step 49910, total loss = 2.23, predict loss = 0.65 (66.3 examples/sec; 0.060 sec/batch; 1h:40m:40s remains)
INFO - root - 2019-11-06 19:07:33.560789: step 49920, total loss = 1.68, predict loss = 0.41 (59.2 examples/sec; 0.068 sec/batch; 1h:52m:39s remains)
INFO - root - 2019-11-06 19:07:34.311155: step 49930, total loss = 1.40, predict loss = 0.37 (58.0 examples/sec; 0.069 sec/batch; 1h:55m:07s remains)
INFO - root - 2019-11-06 19:07:35.116637: step 49940, total loss = 1.43, predict loss = 0.35 (59.6 examples/sec; 0.067 sec/batch; 1h:52m:00s remains)
INFO - root - 2019-11-06 19:07:35.868622: step 49950, total loss = 2.76, predict loss = 0.68 (55.6 examples/sec; 0.072 sec/batch; 1h:59m:52s remains)
INFO - root - 2019-11-06 19:07:36.450041: step 49960, total loss = 2.65, predict loss = 0.74 (99.7 examples/sec; 0.040 sec/batch; 1h:06m:53s remains)
INFO - root - 2019-11-06 19:07:36.908683: step 49970, total loss = 2.92, predict loss = 0.78 (93.8 examples/sec; 0.043 sec/batch; 1h:11m:07s remains)
INFO - root - 2019-11-06 19:07:37.375184: step 49980, total loss = 2.41, predict loss = 0.59 (130.3 examples/sec; 0.031 sec/batch; 0h:51m:10s remains)
INFO - root - 2019-11-06 19:07:38.718093: step 49990, total loss = 2.19, predict loss = 0.59 (56.5 examples/sec; 0.071 sec/batch; 1h:57m:55s remains)
INFO - root - 2019-11-06 19:07:39.454235: step 50000, total loss = 2.53, predict loss = 0.75 (53.8 examples/sec; 0.074 sec/batch; 2h:03m:52s remains)
INFO - root - 2019-11-06 19:07:40.171071: step 50010, total loss = 2.07, predict loss = 0.52 (68.9 examples/sec; 0.058 sec/batch; 1h:36m:47s remains)
INFO - root - 2019-11-06 19:07:40.966750: step 50020, total loss = 1.08, predict loss = 0.30 (52.8 examples/sec; 0.076 sec/batch; 2h:06m:16s remains)
INFO - root - 2019-11-06 19:07:41.697809: step 50030, total loss = 2.26, predict loss = 0.55 (64.1 examples/sec; 0.062 sec/batch; 1h:43m:54s remains)
INFO - root - 2019-11-06 19:07:42.174145: step 50040, total loss = 2.65, predict loss = 0.81 (90.2 examples/sec; 0.044 sec/batch; 1h:13m:52s remains)
INFO - root - 2019-11-06 19:07:42.654429: step 50050, total loss = 1.63, predict loss = 0.44 (92.3 examples/sec; 0.043 sec/batch; 1h:12m:10s remains)
INFO - root - 2019-11-06 19:07:43.897706: step 50060, total loss = 3.00, predict loss = 0.87 (62.3 examples/sec; 0.064 sec/batch; 1h:46m:54s remains)
INFO - root - 2019-11-06 19:07:44.598412: step 50070, total loss = 1.78, predict loss = 0.46 (66.6 examples/sec; 0.060 sec/batch; 1h:39m:59s remains)
INFO - root - 2019-11-06 19:07:45.357078: step 50080, total loss = 2.08, predict loss = 0.50 (59.9 examples/sec; 0.067 sec/batch; 1h:51m:16s remains)
INFO - root - 2019-11-06 19:07:46.083417: step 50090, total loss = 3.12, predict loss = 0.85 (57.3 examples/sec; 0.070 sec/batch; 1h:56m:10s remains)
INFO - root - 2019-11-06 19:07:46.833881: step 50100, total loss = 2.12, predict loss = 0.56 (71.7 examples/sec; 0.056 sec/batch; 1h:32m:51s remains)
INFO - root - 2019-11-06 19:07:47.416232: step 50110, total loss = 2.97, predict loss = 0.81 (93.0 examples/sec; 0.043 sec/batch; 1h:11m:35s remains)
INFO - root - 2019-11-06 19:07:47.879710: step 50120, total loss = 2.13, predict loss = 0.55 (93.3 examples/sec; 0.043 sec/batch; 1h:11m:21s remains)
INFO - root - 2019-11-06 19:07:49.016921: step 50130, total loss = 2.20, predict loss = 0.58 (5.5 examples/sec; 0.731 sec/batch; 20h:16m:07s remains)
INFO - root - 2019-11-06 19:07:49.731447: step 50140, total loss = 1.23, predict loss = 0.33 (61.3 examples/sec; 0.065 sec/batch; 1h:48m:34s remains)
INFO - root - 2019-11-06 19:07:50.517383: step 50150, total loss = 2.19, predict loss = 0.56 (55.5 examples/sec; 0.072 sec/batch; 2h:00m:01s remains)
INFO - root - 2019-11-06 19:07:51.314592: step 50160, total loss = 2.09, predict loss = 0.52 (53.7 examples/sec; 0.075 sec/batch; 2h:03m:58s remains)
INFO - root - 2019-11-06 19:07:52.029015: step 50170, total loss = 1.86, predict loss = 0.45 (61.7 examples/sec; 0.065 sec/batch; 1h:47m:55s remains)
INFO - root - 2019-11-06 19:07:52.673785: step 50180, total loss = 1.77, predict loss = 0.49 (87.3 examples/sec; 0.046 sec/batch; 1h:16m:11s remains)
INFO - root - 2019-11-06 19:07:53.143785: step 50190, total loss = 3.29, predict loss = 0.89 (92.7 examples/sec; 0.043 sec/batch; 1h:11m:48s remains)
INFO - root - 2019-11-06 19:07:53.596001: step 50200, total loss = 2.28, predict loss = 0.60 (99.5 examples/sec; 0.040 sec/batch; 1h:06m:53s remains)
INFO - root - 2019-11-06 19:07:54.821880: step 50210, total loss = 1.90, predict loss = 0.48 (65.3 examples/sec; 0.061 sec/batch; 1h:41m:51s remains)
INFO - root - 2019-11-06 19:07:55.585034: step 50220, total loss = 2.72, predict loss = 0.69 (55.9 examples/sec; 0.072 sec/batch; 1h:59m:02s remains)
INFO - root - 2019-11-06 19:07:56.311831: step 50230, total loss = 1.73, predict loss = 0.43 (62.3 examples/sec; 0.064 sec/batch; 1h:46m:43s remains)
INFO - root - 2019-11-06 19:07:57.047589: step 50240, total loss = 2.66, predict loss = 0.73 (59.2 examples/sec; 0.068 sec/batch; 1h:52m:19s remains)
INFO - root - 2019-11-06 19:07:57.800387: step 50250, total loss = 2.40, predict loss = 0.67 (65.2 examples/sec; 0.061 sec/batch; 1h:42m:03s remains)
INFO - root - 2019-11-06 19:07:58.371339: step 50260, total loss = 2.51, predict loss = 0.65 (103.9 examples/sec; 0.038 sec/batch; 1h:03m:58s remains)
INFO - root - 2019-11-06 19:07:58.831651: step 50270, total loss = 2.15, predict loss = 0.59 (96.8 examples/sec; 0.041 sec/batch; 1h:08m:41s remains)
INFO - root - 2019-11-06 19:07:59.967308: step 50280, total loss = 3.22, predict loss = 0.87 (71.6 examples/sec; 0.056 sec/batch; 1h:32m:50s remains)
INFO - root - 2019-11-06 19:08:00.682293: step 50290, total loss = 2.15, predict loss = 0.57 (56.3 examples/sec; 0.071 sec/batch; 1h:58m:08s remains)
INFO - root - 2019-11-06 19:08:01.493334: step 50300, total loss = 2.91, predict loss = 0.81 (60.1 examples/sec; 0.067 sec/batch; 1h:50m:34s remains)
INFO - root - 2019-11-06 19:08:02.235907: step 50310, total loss = 2.50, predict loss = 0.70 (64.7 examples/sec; 0.062 sec/batch; 1h:42m:45s remains)
INFO - root - 2019-11-06 19:08:02.927283: step 50320, total loss = 1.49, predict loss = 0.38 (53.0 examples/sec; 0.075 sec/batch; 2h:05m:21s remains)
INFO - root - 2019-11-06 19:08:03.613612: step 50330, total loss = 3.57, predict loss = 1.07 (85.3 examples/sec; 0.047 sec/batch; 1h:17m:54s remains)
INFO - root - 2019-11-06 19:08:04.080791: step 50340, total loss = 2.13, predict loss = 0.53 (100.3 examples/sec; 0.040 sec/batch; 1h:06m:14s remains)
INFO - root - 2019-11-06 19:08:04.525028: step 50350, total loss = 1.73, predict loss = 0.44 (100.1 examples/sec; 0.040 sec/batch; 1h:06m:21s remains)
INFO - root - 2019-11-06 19:08:05.784845: step 50360, total loss = 3.25, predict loss = 1.01 (64.4 examples/sec; 0.062 sec/batch; 1h:43m:06s remains)
INFO - root - 2019-11-06 19:08:06.513744: step 50370, total loss = 1.60, predict loss = 0.41 (61.0 examples/sec; 0.066 sec/batch; 1h:48m:53s remains)
INFO - root - 2019-11-06 19:08:07.257678: step 50380, total loss = 3.60, predict loss = 1.07 (56.4 examples/sec; 0.071 sec/batch; 1h:57m:42s remains)
INFO - root - 2019-11-06 19:08:08.034515: step 50390, total loss = 2.18, predict loss = 0.61 (56.0 examples/sec; 0.071 sec/batch; 1h:58m:35s remains)
INFO - root - 2019-11-06 19:08:08.794211: step 50400, total loss = 1.97, predict loss = 0.45 (64.0 examples/sec; 0.063 sec/batch; 1h:43m:49s remains)
INFO - root - 2019-11-06 19:08:09.313592: step 50410, total loss = 1.29, predict loss = 0.30 (104.8 examples/sec; 0.038 sec/batch; 1h:03m:21s remains)
INFO - root - 2019-11-06 19:08:09.797683: step 50420, total loss = 2.54, predict loss = 0.66 (92.2 examples/sec; 0.043 sec/batch; 1h:12m:01s remains)
INFO - root - 2019-11-06 19:08:10.960872: step 50430, total loss = 2.94, predict loss = 0.84 (69.4 examples/sec; 0.058 sec/batch; 1h:35m:37s remains)
INFO - root - 2019-11-06 19:08:11.652995: step 50440, total loss = 1.80, predict loss = 0.48 (61.6 examples/sec; 0.065 sec/batch; 1h:47m:44s remains)
INFO - root - 2019-11-06 19:08:12.383183: step 50450, total loss = 2.05, predict loss = 0.48 (50.7 examples/sec; 0.079 sec/batch; 2h:10m:51s remains)
INFO - root - 2019-11-06 19:08:13.140736: step 50460, total loss = 2.30, predict loss = 0.68 (64.8 examples/sec; 0.062 sec/batch; 1h:42m:22s remains)
INFO - root - 2019-11-06 19:08:13.872218: step 50470, total loss = 2.30, predict loss = 0.58 (55.7 examples/sec; 0.072 sec/batch; 1h:59m:02s remains)
INFO - root - 2019-11-06 19:08:14.528450: step 50480, total loss = 3.49, predict loss = 1.12 (92.4 examples/sec; 0.043 sec/batch; 1h:11m:49s remains)
INFO - root - 2019-11-06 19:08:14.993431: step 50490, total loss = 1.82, predict loss = 0.48 (89.5 examples/sec; 0.045 sec/batch; 1h:14m:07s remains)
INFO - root - 2019-11-06 19:08:15.474211: step 50500, total loss = 1.80, predict loss = 0.43 (91.9 examples/sec; 0.044 sec/batch; 1h:12m:12s remains)
INFO - root - 2019-11-06 19:08:16.731353: step 50510, total loss = 1.67, predict loss = 0.41 (66.9 examples/sec; 0.060 sec/batch; 1h:39m:10s remains)
INFO - root - 2019-11-06 19:08:17.468925: step 50520, total loss = 2.60, predict loss = 0.74 (58.6 examples/sec; 0.068 sec/batch; 1h:53m:11s remains)
INFO - root - 2019-11-06 19:08:18.236600: step 50530, total loss = 2.19, predict loss = 0.56 (63.5 examples/sec; 0.063 sec/batch; 1h:44m:21s remains)
INFO - root - 2019-11-06 19:08:19.033690: step 50540, total loss = 1.50, predict loss = 0.40 (51.1 examples/sec; 0.078 sec/batch; 2h:09m:48s remains)
INFO - root - 2019-11-06 19:08:19.730752: step 50550, total loss = 3.71, predict loss = 1.17 (68.2 examples/sec; 0.059 sec/batch; 1h:37m:14s remains)
INFO - root - 2019-11-06 19:08:20.216754: step 50560, total loss = 2.91, predict loss = 0.76 (99.7 examples/sec; 0.040 sec/batch; 1h:06m:29s remains)
INFO - root - 2019-11-06 19:08:20.691704: step 50570, total loss = 2.19, predict loss = 0.55 (90.9 examples/sec; 0.044 sec/batch; 1h:12m:55s remains)
INFO - root - 2019-11-06 19:08:21.908227: step 50580, total loss = 2.11, predict loss = 0.61 (68.1 examples/sec; 0.059 sec/batch; 1h:37m:15s remains)
INFO - root - 2019-11-06 19:08:22.594316: step 50590, total loss = 2.48, predict loss = 0.68 (63.4 examples/sec; 0.063 sec/batch; 1h:44m:32s remains)
INFO - root - 2019-11-06 19:08:23.332257: step 50600, total loss = 1.70, predict loss = 0.48 (60.0 examples/sec; 0.067 sec/batch; 1h:50m:27s remains)
INFO - root - 2019-11-06 19:08:24.082243: step 50610, total loss = 1.37, predict loss = 0.32 (56.0 examples/sec; 0.071 sec/batch; 1h:58m:17s remains)
INFO - root - 2019-11-06 19:08:24.857617: step 50620, total loss = 2.24, predict loss = 0.55 (55.8 examples/sec; 0.072 sec/batch; 1h:58m:49s remains)
INFO - root - 2019-11-06 19:08:25.451299: step 50630, total loss = 1.30, predict loss = 0.35 (100.7 examples/sec; 0.040 sec/batch; 1h:05m:47s remains)
INFO - root - 2019-11-06 19:08:25.906191: step 50640, total loss = 3.58, predict loss = 1.17 (93.4 examples/sec; 0.043 sec/batch; 1h:10m:54s remains)
INFO - root - 2019-11-06 19:08:26.368235: step 50650, total loss = 2.26, predict loss = 0.64 (90.7 examples/sec; 0.044 sec/batch; 1h:13m:01s remains)
INFO - root - 2019-11-06 19:08:27.715618: step 50660, total loss = 1.55, predict loss = 0.41 (61.2 examples/sec; 0.065 sec/batch; 1h:48m:17s remains)
INFO - root - 2019-11-06 19:08:28.443566: step 50670, total loss = 3.16, predict loss = 0.96 (61.8 examples/sec; 0.065 sec/batch; 1h:47m:05s remains)
INFO - root - 2019-11-06 19:08:29.192218: step 50680, total loss = 1.85, predict loss = 0.48 (52.2 examples/sec; 0.077 sec/batch; 2h:06m:47s remains)
INFO - root - 2019-11-06 19:08:29.904822: step 50690, total loss = 2.75, predict loss = 0.74 (61.9 examples/sec; 0.065 sec/batch; 1h:46m:56s remains)
INFO - root - 2019-11-06 19:08:30.593799: step 50700, total loss = 2.15, predict loss = 0.57 (79.8 examples/sec; 0.050 sec/batch; 1h:22m:55s remains)
INFO - root - 2019-11-06 19:08:31.075600: step 50710, total loss = 3.28, predict loss = 0.88 (89.5 examples/sec; 0.045 sec/batch; 1h:13m:57s remains)
INFO - root - 2019-11-06 19:08:31.519779: step 50720, total loss = 2.29, predict loss = 0.64 (92.8 examples/sec; 0.043 sec/batch; 1h:11m:19s remains)
INFO - root - 2019-11-06 19:08:32.716824: step 50730, total loss = 2.37, predict loss = 0.63 (63.1 examples/sec; 0.063 sec/batch; 1h:44m:55s remains)
INFO - root - 2019-11-06 19:08:33.452597: step 50740, total loss = 2.20, predict loss = 0.57 (55.2 examples/sec; 0.072 sec/batch; 1h:59m:50s remains)
INFO - root - 2019-11-06 19:08:34.179510: step 50750, total loss = 2.15, predict loss = 0.54 (55.2 examples/sec; 0.072 sec/batch; 1h:59m:50s remains)
INFO - root - 2019-11-06 19:08:34.896832: step 50760, total loss = 1.62, predict loss = 0.44 (61.4 examples/sec; 0.065 sec/batch; 1h:47m:44s remains)
INFO - root - 2019-11-06 19:08:35.678410: step 50770, total loss = 3.43, predict loss = 0.97 (54.8 examples/sec; 0.073 sec/batch; 2h:00m:38s remains)
INFO - root - 2019-11-06 19:08:36.270633: step 50780, total loss = 1.54, predict loss = 0.45 (86.9 examples/sec; 0.046 sec/batch; 1h:16m:04s remains)
INFO - root - 2019-11-06 19:08:36.706424: step 50790, total loss = 2.76, predict loss = 0.74 (97.8 examples/sec; 0.041 sec/batch; 1h:07m:36s remains)
INFO - root - 2019-11-06 19:08:37.146231: step 50800, total loss = 1.45, predict loss = 0.48 (135.8 examples/sec; 0.029 sec/batch; 0h:48m:40s remains)
INFO - root - 2019-11-06 19:08:38.529832: step 50810, total loss = 1.97, predict loss = 0.55 (55.3 examples/sec; 0.072 sec/batch; 1h:59m:29s remains)
INFO - root - 2019-11-06 19:08:39.297971: step 50820, total loss = 2.44, predict loss = 0.70 (56.3 examples/sec; 0.071 sec/batch; 1h:57m:22s remains)
INFO - root - 2019-11-06 19:08:40.064817: step 50830, total loss = 1.64, predict loss = 0.42 (49.9 examples/sec; 0.080 sec/batch; 2h:12m:34s remains)
INFO - root - 2019-11-06 19:08:40.833547: step 50840, total loss = 2.60, predict loss = 0.73 (55.7 examples/sec; 0.072 sec/batch; 1h:58m:35s remains)
INFO - root - 2019-11-06 19:08:41.504004: step 50850, total loss = 1.91, predict loss = 0.50 (84.7 examples/sec; 0.047 sec/batch; 1h:18m:04s remains)
INFO - root - 2019-11-06 19:08:42.008947: step 50860, total loss = 1.88, predict loss = 0.48 (91.1 examples/sec; 0.044 sec/batch; 1h:12m:34s remains)
INFO - root - 2019-11-06 19:08:42.468071: step 50870, total loss = 2.78, predict loss = 0.76 (98.1 examples/sec; 0.041 sec/batch; 1h:07m:21s remains)
INFO - root - 2019-11-06 19:08:43.688782: step 50880, total loss = 1.40, predict loss = 0.30 (60.8 examples/sec; 0.066 sec/batch; 1h:48m:41s remains)
INFO - root - 2019-11-06 19:08:44.438992: step 50890, total loss = 1.46, predict loss = 0.35 (54.2 examples/sec; 0.074 sec/batch; 2h:01m:52s remains)
INFO - root - 2019-11-06 19:08:45.198001: step 50900, total loss = 2.12, predict loss = 0.56 (56.0 examples/sec; 0.071 sec/batch; 1h:57m:53s remains)
INFO - root - 2019-11-06 19:08:45.932773: step 50910, total loss = 2.07, predict loss = 0.61 (60.1 examples/sec; 0.067 sec/batch; 1h:49m:50s remains)
INFO - root - 2019-11-06 19:08:46.642395: step 50920, total loss = 1.42, predict loss = 0.34 (64.0 examples/sec; 0.062 sec/batch; 1h:43m:07s remains)
INFO - root - 2019-11-06 19:08:47.197122: step 50930, total loss = 2.56, predict loss = 0.66 (97.9 examples/sec; 0.041 sec/batch; 1h:07m:26s remains)
INFO - root - 2019-11-06 19:08:47.672807: step 50940, total loss = 2.86, predict loss = 0.79 (95.1 examples/sec; 0.042 sec/batch; 1h:09m:24s remains)
INFO - root - 2019-11-06 19:08:48.791651: step 50950, total loss = 1.23, predict loss = 0.36 (5.5 examples/sec; 0.727 sec/batch; 19h:59m:44s remains)
INFO - root - 2019-11-06 19:08:49.482281: step 50960, total loss = 2.83, predict loss = 0.80 (58.9 examples/sec; 0.068 sec/batch; 1h:52m:10s remains)
INFO - root - 2019-11-06 19:08:50.185689: step 50970, total loss = 1.91, predict loss = 0.49 (61.1 examples/sec; 0.065 sec/batch; 1h:48m:02s remains)
INFO - root - 2019-11-06 19:08:50.907602: step 50980, total loss = 2.74, predict loss = 0.81 (60.1 examples/sec; 0.067 sec/batch; 1h:49m:47s remains)
INFO - root - 2019-11-06 19:08:51.673988: step 50990, total loss = 2.12, predict loss = 0.56 (57.3 examples/sec; 0.070 sec/batch; 1h:55m:06s remains)
INFO - root - 2019-11-06 19:08:52.332824: step 51000, total loss = 2.52, predict loss = 0.68 (92.9 examples/sec; 0.043 sec/batch; 1h:11m:03s remains)
INFO - root - 2019-11-06 19:08:52.810222: step 51010, total loss = 1.90, predict loss = 0.49 (87.6 examples/sec; 0.046 sec/batch; 1h:15m:17s remains)
INFO - root - 2019-11-06 19:08:53.299915: step 51020, total loss = 2.82, predict loss = 0.80 (96.5 examples/sec; 0.041 sec/batch; 1h:08m:22s remains)
INFO - root - 2019-11-06 19:08:54.515018: step 51030, total loss = 2.30, predict loss = 0.61 (60.1 examples/sec; 0.067 sec/batch; 1h:49m:48s remains)
INFO - root - 2019-11-06 19:08:55.283134: step 51040, total loss = 2.10, predict loss = 0.50 (54.9 examples/sec; 0.073 sec/batch; 2h:00m:10s remains)
INFO - root - 2019-11-06 19:08:56.042375: step 51050, total loss = 1.96, predict loss = 0.54 (62.1 examples/sec; 0.064 sec/batch; 1h:46m:10s remains)
INFO - root - 2019-11-06 19:08:56.854492: step 51060, total loss = 2.53, predict loss = 0.72 (57.4 examples/sec; 0.070 sec/batch; 1h:54m:55s remains)
INFO - root - 2019-11-06 19:08:57.577249: step 51070, total loss = 1.67, predict loss = 0.46 (70.2 examples/sec; 0.057 sec/batch; 1h:33m:54s remains)
INFO - root - 2019-11-06 19:08:58.101080: step 51080, total loss = 1.82, predict loss = 0.45 (94.0 examples/sec; 0.043 sec/batch; 1h:10m:07s remains)
INFO - root - 2019-11-06 19:08:58.555942: step 51090, total loss = 3.50, predict loss = 1.07 (96.8 examples/sec; 0.041 sec/batch; 1h:08m:06s remains)
INFO - root - 2019-11-06 19:08:59.712936: step 51100, total loss = 2.31, predict loss = 0.67 (63.7 examples/sec; 0.063 sec/batch; 1h:43m:29s remains)
INFO - root - 2019-11-06 19:09:00.430970: step 51110, total loss = 2.18, predict loss = 0.54 (62.1 examples/sec; 0.064 sec/batch; 1h:46m:11s remains)
INFO - root - 2019-11-06 19:09:01.198909: step 51120, total loss = 1.80, predict loss = 0.47 (60.3 examples/sec; 0.066 sec/batch; 1h:49m:17s remains)
INFO - root - 2019-11-06 19:09:02.021046: step 51130, total loss = 1.76, predict loss = 0.51 (52.8 examples/sec; 0.076 sec/batch; 2h:04m:46s remains)
INFO - root - 2019-11-06 19:09:02.743389: step 51140, total loss = 1.40, predict loss = 0.39 (66.7 examples/sec; 0.060 sec/batch; 1h:38m:50s remains)
INFO - root - 2019-11-06 19:09:03.406252: step 51150, total loss = 1.99, predict loss = 0.54 (94.5 examples/sec; 0.042 sec/batch; 1h:09m:45s remains)
INFO - root - 2019-11-06 19:09:03.866358: step 51160, total loss = 1.79, predict loss = 0.52 (96.1 examples/sec; 0.042 sec/batch; 1h:08m:35s remains)
INFO - root - 2019-11-06 19:09:04.330341: step 51170, total loss = 1.46, predict loss = 0.37 (96.5 examples/sec; 0.041 sec/batch; 1h:08m:16s remains)
INFO - root - 2019-11-06 19:09:05.640261: step 51180, total loss = 2.66, predict loss = 0.74 (56.1 examples/sec; 0.071 sec/batch; 1h:57m:26s remains)
INFO - root - 2019-11-06 19:09:06.382933: step 51190, total loss = 1.67, predict loss = 0.44 (57.8 examples/sec; 0.069 sec/batch; 1h:53m:55s remains)
INFO - root - 2019-11-06 19:09:07.082642: step 51200, total loss = 1.66, predict loss = 0.40 (63.4 examples/sec; 0.063 sec/batch; 1h:43m:57s remains)
INFO - root - 2019-11-06 19:09:07.820555: step 51210, total loss = 2.42, predict loss = 0.67 (58.7 examples/sec; 0.068 sec/batch; 1h:52m:11s remains)
INFO - root - 2019-11-06 19:09:08.561755: step 51220, total loss = 2.01, predict loss = 0.62 (72.3 examples/sec; 0.055 sec/batch; 1h:31m:05s remains)
INFO - root - 2019-11-06 19:09:09.094196: step 51230, total loss = 1.31, predict loss = 0.35 (93.8 examples/sec; 0.043 sec/batch; 1h:10m:11s remains)
INFO - root - 2019-11-06 19:09:09.560656: step 51240, total loss = 2.14, predict loss = 0.51 (89.3 examples/sec; 0.045 sec/batch; 1h:13m:42s remains)
INFO - root - 2019-11-06 19:09:10.728100: step 51250, total loss = 2.79, predict loss = 0.71 (68.0 examples/sec; 0.059 sec/batch; 1h:36m:44s remains)
INFO - root - 2019-11-06 19:09:11.481167: step 51260, total loss = 2.51, predict loss = 0.70 (55.4 examples/sec; 0.072 sec/batch; 1h:58m:47s remains)
INFO - root - 2019-11-06 19:09:12.238508: step 51270, total loss = 1.82, predict loss = 0.45 (56.8 examples/sec; 0.070 sec/batch; 1h:55m:57s remains)
INFO - root - 2019-11-06 19:09:12.979310: step 51280, total loss = 2.50, predict loss = 0.65 (64.2 examples/sec; 0.062 sec/batch; 1h:42m:28s remains)
INFO - root - 2019-11-06 19:09:13.711064: step 51290, total loss = 1.63, predict loss = 0.41 (57.8 examples/sec; 0.069 sec/batch; 1h:53m:55s remains)
INFO - root - 2019-11-06 19:09:14.324978: step 51300, total loss = 2.18, predict loss = 0.73 (104.5 examples/sec; 0.038 sec/batch; 1h:02m:57s remains)
INFO - root - 2019-11-06 19:09:14.773635: step 51310, total loss = 3.21, predict loss = 0.94 (96.4 examples/sec; 0.041 sec/batch; 1h:08m:14s remains)
INFO - root - 2019-11-06 19:09:15.225603: step 51320, total loss = 1.40, predict loss = 0.33 (100.0 examples/sec; 0.040 sec/batch; 1h:05m:47s remains)
INFO - root - 2019-11-06 19:09:16.490457: step 51330, total loss = 1.75, predict loss = 0.46 (62.9 examples/sec; 0.064 sec/batch; 1h:44m:30s remains)
INFO - root - 2019-11-06 19:09:17.213176: step 51340, total loss = 1.99, predict loss = 0.50 (63.2 examples/sec; 0.063 sec/batch; 1h:44m:01s remains)
INFO - root - 2019-11-06 19:09:17.945133: step 51350, total loss = 2.26, predict loss = 0.61 (61.9 examples/sec; 0.065 sec/batch; 1h:46m:16s remains)
INFO - root - 2019-11-06 19:09:18.693408: step 51360, total loss = 1.90, predict loss = 0.48 (53.5 examples/sec; 0.075 sec/batch; 2h:02m:49s remains)
INFO - root - 2019-11-06 19:09:19.456406: step 51370, total loss = 2.55, predict loss = 0.73 (63.2 examples/sec; 0.063 sec/batch; 1h:44m:06s remains)
INFO - root - 2019-11-06 19:09:19.993296: step 51380, total loss = 3.08, predict loss = 0.85 (98.6 examples/sec; 0.041 sec/batch; 1h:06m:42s remains)
INFO - root - 2019-11-06 19:09:20.456482: step 51390, total loss = 2.02, predict loss = 0.49 (93.9 examples/sec; 0.043 sec/batch; 1h:09m:59s remains)
INFO - root - 2019-11-06 19:09:21.627316: step 51400, total loss = 1.74, predict loss = 0.43 (67.6 examples/sec; 0.059 sec/batch; 1h:37m:13s remains)
INFO - root - 2019-11-06 19:09:22.340184: step 51410, total loss = 2.25, predict loss = 0.62 (61.5 examples/sec; 0.065 sec/batch; 1h:46m:51s remains)
INFO - root - 2019-11-06 19:09:23.059525: step 51420, total loss = 2.42, predict loss = 0.67 (64.1 examples/sec; 0.062 sec/batch; 1h:42m:33s remains)
INFO - root - 2019-11-06 19:09:23.815451: step 51430, total loss = 1.76, predict loss = 0.42 (53.4 examples/sec; 0.075 sec/batch; 2h:03m:00s remains)
INFO - root - 2019-11-06 19:09:24.554700: step 51440, total loss = 2.41, predict loss = 0.63 (56.0 examples/sec; 0.071 sec/batch; 1h:57m:17s remains)
INFO - root - 2019-11-06 19:09:25.152667: step 51450, total loss = 2.24, predict loss = 0.59 (92.7 examples/sec; 0.043 sec/batch; 1h:10m:52s remains)
INFO - root - 2019-11-06 19:09:25.632402: step 51460, total loss = 2.60, predict loss = 0.71 (97.8 examples/sec; 0.041 sec/batch; 1h:07m:09s remains)
INFO - root - 2019-11-06 19:09:26.077349: step 51470, total loss = 2.42, predict loss = 0.62 (90.9 examples/sec; 0.044 sec/batch; 1h:12m:16s remains)
INFO - root - 2019-11-06 19:09:27.406241: step 51480, total loss = 2.40, predict loss = 0.62 (53.8 examples/sec; 0.074 sec/batch; 2h:02m:07s remains)
INFO - root - 2019-11-06 19:09:28.125266: step 51490, total loss = 2.32, predict loss = 0.69 (60.4 examples/sec; 0.066 sec/batch; 1h:48m:42s remains)
INFO - root - 2019-11-06 19:09:28.911859: step 51500, total loss = 3.09, predict loss = 0.78 (59.4 examples/sec; 0.067 sec/batch; 1h:50m:32s remains)
INFO - root - 2019-11-06 19:09:29.640285: step 51510, total loss = 1.76, predict loss = 0.46 (59.8 examples/sec; 0.067 sec/batch; 1h:49m:43s remains)
INFO - root - 2019-11-06 19:09:30.308812: step 51520, total loss = 1.82, predict loss = 0.47 (78.0 examples/sec; 0.051 sec/batch; 1h:24m:13s remains)
INFO - root - 2019-11-06 19:09:30.774317: step 51530, total loss = 1.65, predict loss = 0.43 (96.6 examples/sec; 0.041 sec/batch; 1h:07m:59s remains)
INFO - root - 2019-11-06 19:09:31.257214: step 51540, total loss = 2.85, predict loss = 0.78 (95.9 examples/sec; 0.042 sec/batch; 1h:08m:25s remains)
INFO - root - 2019-11-06 19:09:32.462093: step 51550, total loss = 2.10, predict loss = 0.52 (63.9 examples/sec; 0.063 sec/batch; 1h:42m:45s remains)
INFO - root - 2019-11-06 19:09:33.177465: step 51560, total loss = 2.26, predict loss = 0.59 (60.8 examples/sec; 0.066 sec/batch; 1h:47m:58s remains)
INFO - root - 2019-11-06 19:09:33.922394: step 51570, total loss = 1.93, predict loss = 0.52 (60.7 examples/sec; 0.066 sec/batch; 1h:48m:10s remains)
INFO - root - 2019-11-06 19:09:34.636919: step 51580, total loss = 2.13, predict loss = 0.56 (61.3 examples/sec; 0.065 sec/batch; 1h:47m:02s remains)
INFO - root - 2019-11-06 19:09:35.382168: step 51590, total loss = 1.49, predict loss = 0.36 (56.6 examples/sec; 0.071 sec/batch; 1h:55m:52s remains)
INFO - root - 2019-11-06 19:09:35.950495: step 51600, total loss = 1.46, predict loss = 0.35 (100.4 examples/sec; 0.040 sec/batch; 1h:05m:22s remains)
INFO - root - 2019-11-06 19:09:36.407035: step 51610, total loss = 2.34, predict loss = 0.65 (94.5 examples/sec; 0.042 sec/batch; 1h:09m:22s remains)
INFO - root - 2019-11-06 19:09:36.906938: step 51620, total loss = 1.53, predict loss = 0.38 (108.4 examples/sec; 0.037 sec/batch; 1h:00m:30s remains)
INFO - root - 2019-11-06 19:09:38.288508: step 51630, total loss = 1.82, predict loss = 0.48 (46.6 examples/sec; 0.086 sec/batch; 2h:20m:45s remains)
INFO - root - 2019-11-06 19:09:39.043111: step 51640, total loss = 1.68, predict loss = 0.46 (60.2 examples/sec; 0.066 sec/batch; 1h:48m:57s remains)
INFO - root - 2019-11-06 19:09:39.816165: step 51650, total loss = 2.38, predict loss = 0.67 (59.4 examples/sec; 0.067 sec/batch; 1h:50m:18s remains)
INFO - root - 2019-11-06 19:09:40.586310: step 51660, total loss = 2.12, predict loss = 0.58 (60.6 examples/sec; 0.066 sec/batch; 1h:48m:12s remains)
INFO - root - 2019-11-06 19:09:41.258280: step 51670, total loss = 2.35, predict loss = 0.71 (79.1 examples/sec; 0.051 sec/batch; 1h:22m:51s remains)
INFO - root - 2019-11-06 19:09:41.730681: step 51680, total loss = 1.46, predict loss = 0.37 (91.6 examples/sec; 0.044 sec/batch; 1h:11m:32s remains)
INFO - root - 2019-11-06 19:09:42.193202: step 51690, total loss = 1.74, predict loss = 0.48 (96.0 examples/sec; 0.042 sec/batch; 1h:08m:17s remains)
INFO - root - 2019-11-06 19:09:43.438635: step 51700, total loss = 2.77, predict loss = 0.77 (65.4 examples/sec; 0.061 sec/batch; 1h:40m:08s remains)
INFO - root - 2019-11-06 19:09:44.168954: step 51710, total loss = 2.09, predict loss = 0.58 (58.3 examples/sec; 0.069 sec/batch; 1h:52m:27s remains)
INFO - root - 2019-11-06 19:09:44.917629: step 51720, total loss = 2.75, predict loss = 0.77 (58.1 examples/sec; 0.069 sec/batch; 1h:52m:44s remains)
INFO - root - 2019-11-06 19:09:45.679346: step 51730, total loss = 2.02, predict loss = 0.59 (51.4 examples/sec; 0.078 sec/batch; 2h:07m:29s remains)
INFO - root - 2019-11-06 19:09:46.405519: step 51740, total loss = 2.74, predict loss = 0.76 (62.0 examples/sec; 0.065 sec/batch; 1h:45m:39s remains)
INFO - root - 2019-11-06 19:09:46.936492: step 51750, total loss = 2.45, predict loss = 0.65 (104.3 examples/sec; 0.038 sec/batch; 1h:02m:47s remains)
INFO - root - 2019-11-06 19:09:47.390109: step 51760, total loss = 1.79, predict loss = 0.45 (89.5 examples/sec; 0.045 sec/batch; 1h:13m:11s remains)
INFO - root - 2019-11-06 19:09:48.540766: step 51770, total loss = 1.06, predict loss = 0.31 (5.3 examples/sec; 0.752 sec/batch; 20h:30m:23s remains)
INFO - root - 2019-11-06 19:09:49.240166: step 51780, total loss = 2.33, predict loss = 0.60 (57.0 examples/sec; 0.070 sec/batch; 1h:54m:52s remains)
INFO - root - 2019-11-06 19:09:49.986452: step 51790, total loss = 1.92, predict loss = 0.48 (63.2 examples/sec; 0.063 sec/batch; 1h:43m:38s remains)
INFO - root - 2019-11-06 19:09:50.773703: step 51800, total loss = 2.62, predict loss = 0.82 (61.0 examples/sec; 0.066 sec/batch; 1h:47m:22s remains)
INFO - root - 2019-11-06 19:09:51.537760: step 51810, total loss = 1.94, predict loss = 0.48 (57.7 examples/sec; 0.069 sec/batch; 1h:53m:25s remains)
INFO - root - 2019-11-06 19:09:52.254328: step 51820, total loss = 2.20, predict loss = 0.59 (82.4 examples/sec; 0.049 sec/batch; 1h:19m:28s remains)
INFO - root - 2019-11-06 19:09:52.703834: step 51830, total loss = 1.97, predict loss = 0.48 (96.4 examples/sec; 0.041 sec/batch; 1h:07m:52s remains)
INFO - root - 2019-11-06 19:09:53.150401: step 51840, total loss = 3.39, predict loss = 1.03 (95.2 examples/sec; 0.042 sec/batch; 1h:08m:46s remains)
INFO - root - 2019-11-06 19:09:54.380858: step 51850, total loss = 2.82, predict loss = 0.82 (63.9 examples/sec; 0.063 sec/batch; 1h:42m:28s remains)
INFO - root - 2019-11-06 19:09:55.133439: step 51860, total loss = 1.66, predict loss = 0.44 (60.0 examples/sec; 0.067 sec/batch; 1h:49m:03s remains)
INFO - root - 2019-11-06 19:09:55.844674: step 51870, total loss = 2.36, predict loss = 0.67 (68.5 examples/sec; 0.058 sec/batch; 1h:35m:26s remains)
INFO - root - 2019-11-06 19:09:56.596038: step 51880, total loss = 2.99, predict loss = 0.84 (57.7 examples/sec; 0.069 sec/batch; 1h:53m:18s remains)
INFO - root - 2019-11-06 19:09:57.350279: step 51890, total loss = 2.85, predict loss = 0.82 (67.6 examples/sec; 0.059 sec/batch; 1h:36m:46s remains)
INFO - root - 2019-11-06 19:09:57.905585: step 51900, total loss = 1.98, predict loss = 0.54 (103.6 examples/sec; 0.039 sec/batch; 1h:03m:07s remains)
INFO - root - 2019-11-06 19:09:58.363302: step 51910, total loss = 1.78, predict loss = 0.48 (88.8 examples/sec; 0.045 sec/batch; 1h:13m:38s remains)
INFO - root - 2019-11-06 19:09:59.529854: step 51920, total loss = 2.31, predict loss = 0.54 (72.4 examples/sec; 0.055 sec/batch; 1h:30m:20s remains)
INFO - root - 2019-11-06 19:10:00.224760: step 51930, total loss = 2.43, predict loss = 0.69 (64.0 examples/sec; 0.062 sec/batch; 1h:42m:07s remains)
INFO - root - 2019-11-06 19:10:01.018131: step 51940, total loss = 3.03, predict loss = 0.82 (51.2 examples/sec; 0.078 sec/batch; 2h:07m:42s remains)
INFO - root - 2019-11-06 19:10:01.738839: step 51950, total loss = 3.37, predict loss = 1.00 (57.9 examples/sec; 0.069 sec/batch; 1h:52m:49s remains)
INFO - root - 2019-11-06 19:10:02.464136: step 51960, total loss = 2.80, predict loss = 0.86 (56.4 examples/sec; 0.071 sec/batch; 1h:55m:49s remains)
INFO - root - 2019-11-06 19:10:03.114204: step 51970, total loss = 2.51, predict loss = 0.66 (90.6 examples/sec; 0.044 sec/batch; 1h:12m:07s remains)
INFO - root - 2019-11-06 19:10:03.581977: step 51980, total loss = 2.44, predict loss = 0.66 (94.1 examples/sec; 0.043 sec/batch; 1h:09m:26s remains)
INFO - root - 2019-11-06 19:10:04.055104: step 51990, total loss = 1.55, predict loss = 0.39 (92.6 examples/sec; 0.043 sec/batch; 1h:10m:33s remains)
INFO - root - 2019-11-06 19:10:05.319179: step 52000, total loss = 2.27, predict loss = 0.58 (64.3 examples/sec; 0.062 sec/batch; 1h:41m:37s remains)
INFO - root - 2019-11-06 19:10:06.078674: step 52010, total loss = 2.28, predict loss = 0.62 (64.4 examples/sec; 0.062 sec/batch; 1h:41m:26s remains)
INFO - root - 2019-11-06 19:10:06.810768: step 52020, total loss = 2.61, predict loss = 0.77 (65.1 examples/sec; 0.061 sec/batch; 1h:40m:21s remains)
INFO - root - 2019-11-06 19:10:07.545759: step 52030, total loss = 2.19, predict loss = 0.56 (69.2 examples/sec; 0.058 sec/batch; 1h:34m:21s remains)
INFO - root - 2019-11-06 19:10:08.256108: step 52040, total loss = 1.89, predict loss = 0.47 (69.4 examples/sec; 0.058 sec/batch; 1h:34m:03s remains)
INFO - root - 2019-11-06 19:10:08.778859: step 52050, total loss = 1.57, predict loss = 0.41 (95.4 examples/sec; 0.042 sec/batch; 1h:08m:28s remains)
INFO - root - 2019-11-06 19:10:09.277018: step 52060, total loss = 1.93, predict loss = 0.50 (86.6 examples/sec; 0.046 sec/batch; 1h:15m:25s remains)
INFO - root - 2019-11-06 19:10:10.432633: step 52070, total loss = 3.04, predict loss = 0.93 (71.8 examples/sec; 0.056 sec/batch; 1h:30m:55s remains)
INFO - root - 2019-11-06 19:10:11.170721: step 52080, total loss = 2.18, predict loss = 0.58 (53.8 examples/sec; 0.074 sec/batch; 2h:01m:16s remains)
INFO - root - 2019-11-06 19:10:11.888902: step 52090, total loss = 1.51, predict loss = 0.39 (64.9 examples/sec; 0.062 sec/batch; 1h:40m:39s remains)
INFO - root - 2019-11-06 19:10:12.630504: step 52100, total loss = 1.92, predict loss = 0.50 (63.2 examples/sec; 0.063 sec/batch; 1h:43m:12s remains)
INFO - root - 2019-11-06 19:10:13.397746: step 52110, total loss = 1.29, predict loss = 0.32 (56.1 examples/sec; 0.071 sec/batch; 1h:56m:25s remains)
INFO - root - 2019-11-06 19:10:14.052596: step 52120, total loss = 2.50, predict loss = 0.75 (95.3 examples/sec; 0.042 sec/batch; 1h:08m:28s remains)
INFO - root - 2019-11-06 19:10:14.492466: step 52130, total loss = 1.44, predict loss = 0.36 (95.4 examples/sec; 0.042 sec/batch; 1h:08m:24s remains)
INFO - root - 2019-11-06 19:10:14.982447: step 52140, total loss = 1.60, predict loss = 0.40 (95.3 examples/sec; 0.042 sec/batch; 1h:08m:27s remains)
INFO - root - 2019-11-06 19:10:16.254927: step 52150, total loss = 1.87, predict loss = 0.51 (62.8 examples/sec; 0.064 sec/batch; 1h:43m:49s remains)
INFO - root - 2019-11-06 19:10:17.045699: step 52160, total loss = 1.99, predict loss = 0.50 (56.1 examples/sec; 0.071 sec/batch; 1h:56m:21s remains)
INFO - root - 2019-11-06 19:10:17.794013: step 52170, total loss = 2.27, predict loss = 0.66 (55.7 examples/sec; 0.072 sec/batch; 1h:57m:04s remains)
INFO - root - 2019-11-06 19:10:18.563216: step 52180, total loss = 1.49, predict loss = 0.37 (52.1 examples/sec; 0.077 sec/batch; 2h:05m:05s remains)
INFO - root - 2019-11-06 19:10:19.305912: step 52190, total loss = 2.09, predict loss = 0.56 (69.6 examples/sec; 0.057 sec/batch; 1h:33m:37s remains)
INFO - root - 2019-11-06 19:10:19.819725: step 52200, total loss = 2.30, predict loss = 0.64 (98.5 examples/sec; 0.041 sec/batch; 1h:06m:12s remains)
INFO - root - 2019-11-06 19:10:20.290114: step 52210, total loss = 2.32, predict loss = 0.72 (89.8 examples/sec; 0.045 sec/batch; 1h:12m:34s remains)
INFO - root - 2019-11-06 19:10:21.485854: step 52220, total loss = 1.66, predict loss = 0.41 (67.6 examples/sec; 0.059 sec/batch; 1h:36m:27s remains)
INFO - root - 2019-11-06 19:10:22.192236: step 52230, total loss = 3.12, predict loss = 0.95 (58.2 examples/sec; 0.069 sec/batch; 1h:52m:02s remains)
INFO - root - 2019-11-06 19:10:22.947611: step 52240, total loss = 3.34, predict loss = 1.02 (54.6 examples/sec; 0.073 sec/batch; 1h:59m:18s remains)
INFO - root - 2019-11-06 19:10:23.757027: step 52250, total loss = 1.20, predict loss = 0.30 (58.6 examples/sec; 0.068 sec/batch; 1h:51m:10s remains)
INFO - root - 2019-11-06 19:10:24.529683: step 52260, total loss = 1.62, predict loss = 0.40 (57.3 examples/sec; 0.070 sec/batch; 1h:53m:37s remains)
INFO - root - 2019-11-06 19:10:25.096178: step 52270, total loss = 1.28, predict loss = 0.35 (95.6 examples/sec; 0.042 sec/batch; 1h:08m:10s remains)
INFO - root - 2019-11-06 19:10:25.588640: step 52280, total loss = 1.17, predict loss = 0.37 (93.6 examples/sec; 0.043 sec/batch; 1h:09m:36s remains)
INFO - root - 2019-11-06 19:10:26.060459: step 52290, total loss = 3.25, predict loss = 0.98 (94.4 examples/sec; 0.042 sec/batch; 1h:09m:00s remains)
INFO - root - 2019-11-06 19:10:27.414306: step 52300, total loss = 2.58, predict loss = 0.69 (64.2 examples/sec; 0.062 sec/batch; 1h:41m:22s remains)
INFO - root - 2019-11-06 19:10:28.161751: step 52310, total loss = 2.28, predict loss = 0.62 (58.5 examples/sec; 0.068 sec/batch; 1h:51m:17s remains)
INFO - root - 2019-11-06 19:10:28.919443: step 52320, total loss = 1.68, predict loss = 0.48 (58.2 examples/sec; 0.069 sec/batch; 1h:51m:57s remains)
INFO - root - 2019-11-06 19:10:29.718317: step 52330, total loss = 1.66, predict loss = 0.40 (50.1 examples/sec; 0.080 sec/batch; 2h:09m:58s remains)
INFO - root - 2019-11-06 19:10:30.403885: step 52340, total loss = 1.96, predict loss = 0.55 (73.1 examples/sec; 0.055 sec/batch; 1h:29m:05s remains)
INFO - root - 2019-11-06 19:10:30.885453: step 52350, total loss = 2.32, predict loss = 0.67 (98.2 examples/sec; 0.041 sec/batch; 1h:06m:18s remains)
INFO - root - 2019-11-06 19:10:31.349168: step 52360, total loss = 2.07, predict loss = 0.60 (95.8 examples/sec; 0.042 sec/batch; 1h:07m:55s remains)
INFO - root - 2019-11-06 19:10:32.527903: step 52370, total loss = 3.32, predict loss = 0.98 (61.1 examples/sec; 0.065 sec/batch; 1h:46m:27s remains)
INFO - root - 2019-11-06 19:10:33.269166: step 52380, total loss = 1.75, predict loss = 0.44 (59.2 examples/sec; 0.068 sec/batch; 1h:49m:55s remains)
INFO - root - 2019-11-06 19:10:34.042554: step 52390, total loss = 1.42, predict loss = 0.34 (58.6 examples/sec; 0.068 sec/batch; 1h:50m:57s remains)
INFO - root - 2019-11-06 19:10:34.962775: step 52400, total loss = 2.22, predict loss = 0.60 (46.5 examples/sec; 0.086 sec/batch; 2h:19m:46s remains)
INFO - root - 2019-11-06 19:10:35.714478: step 52410, total loss = 1.55, predict loss = 0.43 (61.2 examples/sec; 0.065 sec/batch; 1h:46m:19s remains)
INFO - root - 2019-11-06 19:10:36.310209: step 52420, total loss = 2.08, predict loss = 0.58 (91.3 examples/sec; 0.044 sec/batch; 1h:11m:14s remains)
INFO - root - 2019-11-06 19:10:36.778549: step 52430, total loss = 2.27, predict loss = 0.61 (92.2 examples/sec; 0.043 sec/batch; 1h:10m:32s remains)
INFO - root - 2019-11-06 19:10:37.214875: step 52440, total loss = 1.58, predict loss = 0.40 (138.8 examples/sec; 0.029 sec/batch; 0h:46m:50s remains)
INFO - root - 2019-11-06 19:10:38.605711: step 52450, total loss = 1.51, predict loss = 0.39 (59.9 examples/sec; 0.067 sec/batch; 1h:48m:30s remains)
INFO - root - 2019-11-06 19:10:39.404218: step 52460, total loss = 1.44, predict loss = 0.37 (59.4 examples/sec; 0.067 sec/batch; 1h:49m:33s remains)
INFO - root - 2019-11-06 19:10:40.134792: step 52470, total loss = 2.13, predict loss = 0.56 (65.8 examples/sec; 0.061 sec/batch; 1h:38m:45s remains)
INFO - root - 2019-11-06 19:10:40.901783: step 52480, total loss = 2.87, predict loss = 0.82 (55.4 examples/sec; 0.072 sec/batch; 1h:57m:20s remains)
INFO - root - 2019-11-06 19:10:41.587794: step 52490, total loss = 2.34, predict loss = 0.60 (66.9 examples/sec; 0.060 sec/batch; 1h:37m:14s remains)
INFO - root - 2019-11-06 19:10:42.100915: step 52500, total loss = 1.75, predict loss = 0.43 (94.5 examples/sec; 0.042 sec/batch; 1h:08m:47s remains)
INFO - root - 2019-11-06 19:10:42.563427: step 52510, total loss = 3.33, predict loss = 1.10 (101.0 examples/sec; 0.040 sec/batch; 1h:04m:19s remains)
INFO - root - 2019-11-06 19:10:43.781544: step 52520, total loss = 0.99, predict loss = 0.25 (62.4 examples/sec; 0.064 sec/batch; 1h:44m:09s remains)
INFO - root - 2019-11-06 19:10:44.525827: step 52530, total loss = 2.48, predict loss = 0.65 (51.1 examples/sec; 0.078 sec/batch; 2h:07m:16s remains)
INFO - root - 2019-11-06 19:10:45.286830: step 52540, total loss = 2.61, predict loss = 0.71 (54.1 examples/sec; 0.074 sec/batch; 2h:00m:04s remains)
INFO - root - 2019-11-06 19:10:46.000878: step 52550, total loss = 1.55, predict loss = 0.40 (57.9 examples/sec; 0.069 sec/batch; 1h:52m:15s remains)
INFO - root - 2019-11-06 19:10:46.704839: step 52560, total loss = 2.39, predict loss = 0.61 (69.4 examples/sec; 0.058 sec/batch; 1h:33m:38s remains)
INFO - root - 2019-11-06 19:10:47.258727: step 52570, total loss = 2.35, predict loss = 0.65 (102.8 examples/sec; 0.039 sec/batch; 1h:03m:09s remains)
INFO - root - 2019-11-06 19:10:47.752493: step 52580, total loss = 1.61, predict loss = 0.45 (84.9 examples/sec; 0.047 sec/batch; 1h:16m:27s remains)
INFO - root - 2019-11-06 19:10:48.865772: step 52590, total loss = 1.91, predict loss = 0.49 (5.7 examples/sec; 0.699 sec/batch; 18h:54m:12s remains)
INFO - root - 2019-11-06 19:10:49.673010: step 52600, total loss = 2.50, predict loss = 0.66 (51.3 examples/sec; 0.078 sec/batch; 2h:06m:28s remains)
INFO - root - 2019-11-06 19:10:50.432836: step 52610, total loss = 1.78, predict loss = 0.47 (57.2 examples/sec; 0.070 sec/batch; 1h:53m:32s remains)
INFO - root - 2019-11-06 19:10:51.204140: step 52620, total loss = 1.32, predict loss = 0.33 (59.6 examples/sec; 0.067 sec/batch; 1h:48m:58s remains)
INFO - root - 2019-11-06 19:10:51.920483: step 52630, total loss = 2.62, predict loss = 0.80 (60.1 examples/sec; 0.067 sec/batch; 1h:48m:00s remains)
INFO - root - 2019-11-06 19:10:52.539726: step 52640, total loss = 2.72, predict loss = 0.74 (92.8 examples/sec; 0.043 sec/batch; 1h:09m:54s remains)
INFO - root - 2019-11-06 19:10:52.979737: step 52650, total loss = 1.71, predict loss = 0.43 (94.7 examples/sec; 0.042 sec/batch; 1h:08m:30s remains)
INFO - root - 2019-11-06 19:10:53.471093: step 52660, total loss = 2.38, predict loss = 0.71 (91.2 examples/sec; 0.044 sec/batch; 1h:11m:11s remains)
INFO - root - 2019-11-06 19:10:54.751918: step 52670, total loss = 1.11, predict loss = 0.27 (59.0 examples/sec; 0.068 sec/batch; 1h:49m:56s remains)
INFO - root - 2019-11-06 19:10:55.490939: step 52680, total loss = 2.35, predict loss = 0.68 (54.3 examples/sec; 0.074 sec/batch; 1h:59m:35s remains)
INFO - root - 2019-11-06 19:10:56.229488: step 52690, total loss = 1.99, predict loss = 0.53 (59.3 examples/sec; 0.067 sec/batch; 1h:49m:27s remains)
INFO - root - 2019-11-06 19:10:56.985633: step 52700, total loss = 2.64, predict loss = 0.83 (55.6 examples/sec; 0.072 sec/batch; 1h:56m:41s remains)
INFO - root - 2019-11-06 19:10:57.719339: step 52710, total loss = 2.77, predict loss = 0.91 (69.5 examples/sec; 0.058 sec/batch; 1h:33m:17s remains)
INFO - root - 2019-11-06 19:10:58.219284: step 52720, total loss = 2.43, predict loss = 0.66 (96.3 examples/sec; 0.042 sec/batch; 1h:07m:19s remains)
INFO - root - 2019-11-06 19:10:58.687092: step 52730, total loss = 1.29, predict loss = 0.31 (90.3 examples/sec; 0.044 sec/batch; 1h:11m:46s remains)
INFO - root - 2019-11-06 19:10:59.850846: step 52740, total loss = 2.12, predict loss = 0.55 (70.3 examples/sec; 0.057 sec/batch; 1h:32m:12s remains)
INFO - root - 2019-11-06 19:11:00.590262: step 52750, total loss = 1.69, predict loss = 0.41 (51.8 examples/sec; 0.077 sec/batch; 2h:05m:04s remains)
INFO - root - 2019-11-06 19:11:01.318671: step 52760, total loss = 2.85, predict loss = 0.81 (58.3 examples/sec; 0.069 sec/batch; 1h:51m:08s remains)
INFO - root - 2019-11-06 19:11:02.134652: step 52770, total loss = 1.17, predict loss = 0.32 (59.4 examples/sec; 0.067 sec/batch; 1h:49m:08s remains)
INFO - root - 2019-11-06 19:11:02.880267: step 52780, total loss = 1.86, predict loss = 0.48 (63.3 examples/sec; 0.063 sec/batch; 1h:42m:22s remains)
INFO - root - 2019-11-06 19:11:03.522871: step 52790, total loss = 1.41, predict loss = 0.34 (92.8 examples/sec; 0.043 sec/batch; 1h:09m:50s remains)
INFO - root - 2019-11-06 19:11:03.977030: step 52800, total loss = 3.31, predict loss = 1.02 (96.7 examples/sec; 0.041 sec/batch; 1h:06m:59s remains)
INFO - root - 2019-11-06 19:11:04.418738: step 52810, total loss = 2.85, predict loss = 0.78 (104.2 examples/sec; 0.038 sec/batch; 1h:02m:12s remains)
INFO - root - 2019-11-06 19:11:05.740023: step 52820, total loss = 2.40, predict loss = 0.70 (60.3 examples/sec; 0.066 sec/batch; 1h:47m:30s remains)
INFO - root - 2019-11-06 19:11:06.528021: step 52830, total loss = 1.34, predict loss = 0.30 (58.3 examples/sec; 0.069 sec/batch; 1h:51m:07s remains)
INFO - root - 2019-11-06 19:11:07.259137: step 52840, total loss = 1.50, predict loss = 0.36 (52.9 examples/sec; 0.076 sec/batch; 2h:02m:24s remains)
INFO - root - 2019-11-06 19:11:08.047757: step 52850, total loss = 2.29, predict loss = 0.69 (60.8 examples/sec; 0.066 sec/batch; 1h:46m:32s remains)
INFO - root - 2019-11-06 19:11:08.801681: step 52860, total loss = 2.52, predict loss = 0.71 (65.8 examples/sec; 0.061 sec/batch; 1h:38m:29s remains)
INFO - root - 2019-11-06 19:11:09.334818: step 52870, total loss = 1.05, predict loss = 0.27 (86.6 examples/sec; 0.046 sec/batch; 1h:14m:44s remains)
INFO - root - 2019-11-06 19:11:09.788111: step 52880, total loss = 1.99, predict loss = 0.51 (96.5 examples/sec; 0.041 sec/batch; 1h:07m:06s remains)
INFO - root - 2019-11-06 19:11:10.987973: step 52890, total loss = 2.08, predict loss = 0.53 (61.9 examples/sec; 0.065 sec/batch; 1h:44m:32s remains)
INFO - root - 2019-11-06 19:11:11.679585: step 52900, total loss = 1.68, predict loss = 0.41 (59.4 examples/sec; 0.067 sec/batch; 1h:48m:54s remains)
INFO - root - 2019-11-06 19:11:12.450612: step 52910, total loss = 1.70, predict loss = 0.41 (61.1 examples/sec; 0.065 sec/batch; 1h:45m:51s remains)
INFO - root - 2019-11-06 19:11:13.247370: step 52920, total loss = 3.51, predict loss = 1.07 (54.2 examples/sec; 0.074 sec/batch; 1h:59m:28s remains)
INFO - root - 2019-11-06 19:11:14.050155: step 52930, total loss = 1.27, predict loss = 0.32 (58.7 examples/sec; 0.068 sec/batch; 1h:50m:16s remains)
INFO - root - 2019-11-06 19:11:14.715496: step 52940, total loss = 1.43, predict loss = 0.41 (102.8 examples/sec; 0.039 sec/batch; 1h:02m:56s remains)
INFO - root - 2019-11-06 19:11:15.175018: step 52950, total loss = 1.60, predict loss = 0.37 (91.6 examples/sec; 0.044 sec/batch; 1h:10m:35s remains)
INFO - root - 2019-11-06 19:11:15.630102: step 52960, total loss = 1.12, predict loss = 0.29 (96.9 examples/sec; 0.041 sec/batch; 1h:06m:45s remains)
INFO - root - 2019-11-06 19:11:16.907200: step 52970, total loss = 3.09, predict loss = 0.92 (59.6 examples/sec; 0.067 sec/batch; 1h:48m:33s remains)
INFO - root - 2019-11-06 19:11:17.644337: step 52980, total loss = 2.11, predict loss = 0.54 (57.9 examples/sec; 0.069 sec/batch; 1h:51m:38s remains)
INFO - root - 2019-11-06 19:11:18.359940: step 52990, total loss = 2.92, predict loss = 0.84 (59.1 examples/sec; 0.068 sec/batch; 1h:49m:25s remains)
INFO - root - 2019-11-06 19:11:19.139595: step 53000, total loss = 2.56, predict loss = 0.70 (62.8 examples/sec; 0.064 sec/batch; 1h:42m:59s remains)
INFO - root - 2019-11-06 19:11:19.827207: step 53010, total loss = 2.05, predict loss = 0.53 (80.9 examples/sec; 0.049 sec/batch; 1h:19m:54s remains)
INFO - root - 2019-11-06 19:11:20.337371: step 53020, total loss = 1.07, predict loss = 0.27 (101.7 examples/sec; 0.039 sec/batch; 1h:03m:35s remains)
INFO - root - 2019-11-06 19:11:20.808118: step 53030, total loss = 1.99, predict loss = 0.52 (87.8 examples/sec; 0.046 sec/batch; 1h:13m:38s remains)
INFO - root - 2019-11-06 19:11:22.001345: step 53040, total loss = 1.56, predict loss = 0.36 (60.7 examples/sec; 0.066 sec/batch; 1h:46m:25s remains)
INFO - root - 2019-11-06 19:11:22.732643: step 53050, total loss = 1.46, predict loss = 0.38 (55.8 examples/sec; 0.072 sec/batch; 1h:55m:55s remains)
INFO - root - 2019-11-06 19:11:23.497475: step 53060, total loss = 1.97, predict loss = 0.49 (61.8 examples/sec; 0.065 sec/batch; 1h:44m:36s remains)
INFO - root - 2019-11-06 19:11:24.213782: step 53070, total loss = 1.87, predict loss = 0.47 (59.4 examples/sec; 0.067 sec/batch; 1h:48m:42s remains)
INFO - root - 2019-11-06 19:11:24.989268: step 53080, total loss = 2.15, predict loss = 0.59 (55.5 examples/sec; 0.072 sec/batch; 1h:56m:26s remains)
INFO - root - 2019-11-06 19:11:25.587904: step 53090, total loss = 1.46, predict loss = 0.34 (98.8 examples/sec; 0.040 sec/batch; 1h:05m:21s remains)
INFO - root - 2019-11-06 19:11:26.067533: step 53100, total loss = 1.03, predict loss = 0.29 (89.8 examples/sec; 0.045 sec/batch; 1h:11m:57s remains)
INFO - root - 2019-11-06 19:11:26.538135: step 53110, total loss = 1.57, predict loss = 0.44 (93.5 examples/sec; 0.043 sec/batch; 1h:09m:03s remains)
INFO - root - 2019-11-06 19:11:27.875383: step 53120, total loss = 2.02, predict loss = 0.53 (63.1 examples/sec; 0.063 sec/batch; 1h:42m:24s remains)
INFO - root - 2019-11-06 19:11:28.648500: step 53130, total loss = 1.35, predict loss = 0.34 (50.2 examples/sec; 0.080 sec/batch; 2h:08m:40s remains)
INFO - root - 2019-11-06 19:11:29.396650: step 53140, total loss = 2.87, predict loss = 0.76 (63.2 examples/sec; 0.063 sec/batch; 1h:42m:13s remains)
INFO - root - 2019-11-06 19:11:30.147586: step 53150, total loss = 1.76, predict loss = 0.47 (61.8 examples/sec; 0.065 sec/batch; 1h:44m:33s remains)
INFO - root - 2019-11-06 19:11:30.849139: step 53160, total loss = 1.60, predict loss = 0.43 (64.0 examples/sec; 0.063 sec/batch; 1h:40m:55s remains)
INFO - root - 2019-11-06 19:11:31.341913: step 53170, total loss = 2.08, predict loss = 0.52 (95.8 examples/sec; 0.042 sec/batch; 1h:07m:23s remains)
INFO - root - 2019-11-06 19:11:31.822685: step 53180, total loss = 2.80, predict loss = 0.86 (93.3 examples/sec; 0.043 sec/batch; 1h:09m:09s remains)
INFO - root - 2019-11-06 19:11:33.027303: step 53190, total loss = 2.85, predict loss = 0.83 (67.0 examples/sec; 0.060 sec/batch; 1h:36m:20s remains)
INFO - root - 2019-11-06 19:11:33.742298: step 53200, total loss = 1.92, predict loss = 0.54 (60.3 examples/sec; 0.066 sec/batch; 1h:46m:57s remains)
INFO - root - 2019-11-06 19:11:34.489466: step 53210, total loss = 1.61, predict loss = 0.39 (60.7 examples/sec; 0.066 sec/batch; 1h:46m:13s remains)
INFO - root - 2019-11-06 19:11:35.225254: step 53220, total loss = 1.50, predict loss = 0.34 (59.3 examples/sec; 0.067 sec/batch; 1h:48m:47s remains)
INFO - root - 2019-11-06 19:11:35.939639: step 53230, total loss = 2.94, predict loss = 0.87 (62.4 examples/sec; 0.064 sec/batch; 1h:43m:23s remains)
INFO - root - 2019-11-06 19:11:36.484618: step 53240, total loss = 2.39, predict loss = 0.65 (102.7 examples/sec; 0.039 sec/batch; 1h:02m:49s remains)
INFO - root - 2019-11-06 19:11:36.934617: step 53250, total loss = 3.45, predict loss = 1.03 (95.2 examples/sec; 0.042 sec/batch; 1h:07m:46s remains)
INFO - root - 2019-11-06 19:11:37.407526: step 53260, total loss = 1.90, predict loss = 0.47 (124.9 examples/sec; 0.032 sec/batch; 0h:51m:37s remains)
INFO - root - 2019-11-06 19:11:38.779789: step 53270, total loss = 1.63, predict loss = 0.43 (51.9 examples/sec; 0.077 sec/batch; 2h:04m:19s remains)
INFO - root - 2019-11-06 19:11:39.643751: step 53280, total loss = 2.41, predict loss = 0.70 (49.1 examples/sec; 0.081 sec/batch; 2h:11m:15s remains)
INFO - root - 2019-11-06 19:11:40.401030: step 53290, total loss = 3.31, predict loss = 1.04 (50.0 examples/sec; 0.080 sec/batch; 2h:09m:01s remains)
INFO - root - 2019-11-06 19:11:41.161574: step 53300, total loss = 2.66, predict loss = 0.72 (65.8 examples/sec; 0.061 sec/batch; 1h:37m:56s remains)
INFO - root - 2019-11-06 19:11:41.832920: step 53310, total loss = 1.88, predict loss = 0.56 (73.4 examples/sec; 0.054 sec/batch; 1h:27m:47s remains)
INFO - root - 2019-11-06 19:11:42.294139: step 53320, total loss = 2.75, predict loss = 0.71 (106.4 examples/sec; 0.038 sec/batch; 1h:00m:33s remains)
INFO - root - 2019-11-06 19:11:42.743302: step 53330, total loss = 2.38, predict loss = 0.61 (93.3 examples/sec; 0.043 sec/batch; 1h:09m:03s remains)
INFO - root - 2019-11-06 19:11:44.006365: step 53340, total loss = 0.93, predict loss = 0.28 (62.2 examples/sec; 0.064 sec/batch; 1h:43m:32s remains)
INFO - root - 2019-11-06 19:11:44.791091: step 53350, total loss = 1.98, predict loss = 0.51 (57.9 examples/sec; 0.069 sec/batch; 1h:51m:17s remains)
INFO - root - 2019-11-06 19:11:45.504741: step 53360, total loss = 2.41, predict loss = 0.63 (66.3 examples/sec; 0.060 sec/batch; 1h:37m:09s remains)
INFO - root - 2019-11-06 19:11:46.176642: step 53370, total loss = 2.06, predict loss = 0.54 (69.0 examples/sec; 0.058 sec/batch; 1h:33m:23s remains)
INFO - root - 2019-11-06 19:11:46.924922: step 53380, total loss = 1.61, predict loss = 0.44 (67.1 examples/sec; 0.060 sec/batch; 1h:35m:56s remains)
INFO - root - 2019-11-06 19:11:47.455471: step 53390, total loss = 2.00, predict loss = 0.50 (98.6 examples/sec; 0.041 sec/batch; 1h:05m:18s remains)
INFO - root - 2019-11-06 19:11:47.911637: step 53400, total loss = 1.77, predict loss = 0.43 (96.7 examples/sec; 0.041 sec/batch; 1h:06m:34s remains)
INFO - root - 2019-11-06 19:11:49.026936: step 53410, total loss = 2.30, predict loss = 0.58 (5.6 examples/sec; 0.710 sec/batch; 19h:03m:27s remains)
INFO - root - 2019-11-06 19:11:49.742152: step 53420, total loss = 2.00, predict loss = 0.51 (59.9 examples/sec; 0.067 sec/batch; 1h:47m:25s remains)
INFO - root - 2019-11-06 19:11:50.505372: step 53430, total loss = 2.10, predict loss = 0.55 (65.9 examples/sec; 0.061 sec/batch; 1h:37m:39s remains)
INFO - root - 2019-11-06 19:11:51.241194: step 53440, total loss = 2.80, predict loss = 0.81 (58.2 examples/sec; 0.069 sec/batch; 1h:50m:33s remains)
INFO - root - 2019-11-06 19:11:51.981499: step 53450, total loss = 2.89, predict loss = 0.86 (61.3 examples/sec; 0.065 sec/batch; 1h:45m:00s remains)
INFO - root - 2019-11-06 19:11:52.645568: step 53460, total loss = 1.82, predict loss = 0.46 (97.9 examples/sec; 0.041 sec/batch; 1h:05m:44s remains)
INFO - root - 2019-11-06 19:11:53.083318: step 53470, total loss = 2.25, predict loss = 0.56 (95.8 examples/sec; 0.042 sec/batch; 1h:07m:12s remains)
INFO - root - 2019-11-06 19:11:53.538879: step 53480, total loss = 1.41, predict loss = 0.34 (95.2 examples/sec; 0.042 sec/batch; 1h:07m:34s remains)
INFO - root - 2019-11-06 19:11:54.767419: step 53490, total loss = 1.98, predict loss = 0.56 (65.7 examples/sec; 0.061 sec/batch; 1h:37m:55s remains)
INFO - root - 2019-11-06 19:11:55.508298: step 53500, total loss = 1.29, predict loss = 0.33 (57.8 examples/sec; 0.069 sec/batch; 1h:51m:21s remains)
INFO - root - 2019-11-06 19:11:56.284286: step 53510, total loss = 2.56, predict loss = 0.73 (60.2 examples/sec; 0.066 sec/batch; 1h:46m:53s remains)
INFO - root - 2019-11-06 19:11:57.027946: step 53520, total loss = 1.94, predict loss = 0.51 (60.1 examples/sec; 0.067 sec/batch; 1h:47m:02s remains)
INFO - root - 2019-11-06 19:11:57.773457: step 53530, total loss = 2.60, predict loss = 0.68 (74.1 examples/sec; 0.054 sec/batch; 1h:26m:47s remains)
INFO - root - 2019-11-06 19:11:58.326486: step 53540, total loss = 1.67, predict loss = 0.46 (100.6 examples/sec; 0.040 sec/batch; 1h:03m:53s remains)
INFO - root - 2019-11-06 19:11:58.772368: step 53550, total loss = 1.79, predict loss = 0.39 (94.7 examples/sec; 0.042 sec/batch; 1h:07m:52s remains)
INFO - root - 2019-11-06 19:11:59.937756: step 53560, total loss = 2.03, predict loss = 0.53 (66.1 examples/sec; 0.061 sec/batch; 1h:37m:16s remains)
INFO - root - 2019-11-06 19:12:00.668436: step 53570, total loss = 1.66, predict loss = 0.41 (60.6 examples/sec; 0.066 sec/batch; 1h:46m:09s remains)
INFO - root - 2019-11-06 19:12:01.436268: step 53580, total loss = 3.33, predict loss = 0.93 (53.5 examples/sec; 0.075 sec/batch; 2h:00m:02s remains)
INFO - root - 2019-11-06 19:12:02.240865: step 53590, total loss = 1.80, predict loss = 0.47 (58.8 examples/sec; 0.068 sec/batch; 1h:49m:17s remains)
INFO - root - 2019-11-06 19:12:02.950160: step 53600, total loss = 2.36, predict loss = 0.61 (63.8 examples/sec; 0.063 sec/batch; 1h:40m:45s remains)
INFO - root - 2019-11-06 19:12:03.552121: step 53610, total loss = 1.72, predict loss = 0.49 (98.8 examples/sec; 0.040 sec/batch; 1h:05m:03s remains)
INFO - root - 2019-11-06 19:12:04.021661: step 53620, total loss = 2.24, predict loss = 0.58 (95.7 examples/sec; 0.042 sec/batch; 1h:07m:07s remains)
INFO - root - 2019-11-06 19:12:04.492179: step 53630, total loss = 2.68, predict loss = 0.79 (96.5 examples/sec; 0.041 sec/batch; 1h:06m:35s remains)
INFO - root - 2019-11-06 19:12:05.766908: step 53640, total loss = 2.11, predict loss = 0.53 (60.5 examples/sec; 0.066 sec/batch; 1h:46m:05s remains)
INFO - root - 2019-11-06 19:12:06.491942: step 53650, total loss = 3.71, predict loss = 1.16 (63.6 examples/sec; 0.063 sec/batch; 1h:41m:01s remains)
INFO - root - 2019-11-06 19:12:07.214754: step 53660, total loss = 1.65, predict loss = 0.43 (62.0 examples/sec; 0.064 sec/batch; 1h:43m:32s remains)
INFO - root - 2019-11-06 19:12:07.949290: step 53670, total loss = 1.82, predict loss = 0.46 (61.9 examples/sec; 0.065 sec/batch; 1h:43m:43s remains)
INFO - root - 2019-11-06 19:12:08.719764: step 53680, total loss = 1.44, predict loss = 0.39 (59.3 examples/sec; 0.067 sec/batch; 1h:48m:14s remains)
INFO - root - 2019-11-06 19:12:09.242594: step 53690, total loss = 1.23, predict loss = 0.33 (96.4 examples/sec; 0.042 sec/batch; 1h:06m:38s remains)
INFO - root - 2019-11-06 19:12:09.724363: step 53700, total loss = 1.56, predict loss = 0.39 (96.4 examples/sec; 0.042 sec/batch; 1h:06m:37s remains)
INFO - root - 2019-11-06 19:12:10.874163: step 53710, total loss = 1.37, predict loss = 0.35 (66.8 examples/sec; 0.060 sec/batch; 1h:36m:05s remains)
INFO - root - 2019-11-06 19:12:11.534185: step 53720, total loss = 1.39, predict loss = 0.33 (61.7 examples/sec; 0.065 sec/batch; 1h:44m:04s remains)
INFO - root - 2019-11-06 19:12:12.292157: step 53730, total loss = 2.29, predict loss = 0.61 (57.8 examples/sec; 0.069 sec/batch; 1h:50m:57s remains)
INFO - root - 2019-11-06 19:12:13.125296: step 53740, total loss = 2.39, predict loss = 0.66 (56.7 examples/sec; 0.070 sec/batch; 1h:53m:05s remains)
INFO - root - 2019-11-06 19:12:13.907634: step 53750, total loss = 1.94, predict loss = 0.56 (52.4 examples/sec; 0.076 sec/batch; 2h:02m:28s remains)
INFO - root - 2019-11-06 19:12:14.550704: step 53760, total loss = 2.82, predict loss = 0.78 (95.8 examples/sec; 0.042 sec/batch; 1h:06m:57s remains)
INFO - root - 2019-11-06 19:12:15.012977: step 53770, total loss = 1.43, predict loss = 0.37 (92.7 examples/sec; 0.043 sec/batch; 1h:09m:13s remains)
INFO - root - 2019-11-06 19:12:15.507939: step 53780, total loss = 3.36, predict loss = 1.14 (91.3 examples/sec; 0.044 sec/batch; 1h:10m:15s remains)
INFO - root - 2019-11-06 19:12:16.785286: step 53790, total loss = 2.46, predict loss = 0.68 (60.8 examples/sec; 0.066 sec/batch; 1h:45m:25s remains)
INFO - root - 2019-11-06 19:12:17.560165: step 53800, total loss = 2.61, predict loss = 0.70 (61.0 examples/sec; 0.066 sec/batch; 1h:45m:12s remains)
INFO - root - 2019-11-06 19:12:18.324885: step 53810, total loss = 1.88, predict loss = 0.57 (48.7 examples/sec; 0.082 sec/batch; 2h:11m:34s remains)
INFO - root - 2019-11-06 19:12:19.105749: step 53820, total loss = 1.57, predict loss = 0.41 (55.3 examples/sec; 0.072 sec/batch; 1h:55m:53s remains)
INFO - root - 2019-11-06 19:12:19.865342: step 53830, total loss = 1.78, predict loss = 0.46 (61.2 examples/sec; 0.065 sec/batch; 1h:44m:42s remains)
INFO - root - 2019-11-06 19:12:20.373581: step 53840, total loss = 1.42, predict loss = 0.42 (97.4 examples/sec; 0.041 sec/batch; 1h:05m:47s remains)
INFO - root - 2019-11-06 19:12:20.836272: step 53850, total loss = 2.67, predict loss = 0.72 (94.9 examples/sec; 0.042 sec/batch; 1h:07m:30s remains)
INFO - root - 2019-11-06 19:12:22.049844: step 53860, total loss = 2.56, predict loss = 0.74 (66.6 examples/sec; 0.060 sec/batch; 1h:36m:11s remains)
INFO - root - 2019-11-06 19:12:22.758328: step 53870, total loss = 3.11, predict loss = 0.97 (53.4 examples/sec; 0.075 sec/batch; 1h:59m:58s remains)
INFO - root - 2019-11-06 19:12:23.538700: step 53880, total loss = 1.67, predict loss = 0.43 (54.9 examples/sec; 0.073 sec/batch; 1h:56m:49s remains)
INFO - root - 2019-11-06 19:12:24.259658: step 53890, total loss = 2.97, predict loss = 0.88 (59.3 examples/sec; 0.067 sec/batch; 1h:48m:05s remains)
INFO - root - 2019-11-06 19:12:24.987742: step 53900, total loss = 2.20, predict loss = 0.62 (66.9 examples/sec; 0.060 sec/batch; 1h:35m:43s remains)
INFO - root - 2019-11-06 19:12:25.553757: step 53910, total loss = 1.46, predict loss = 0.35 (99.3 examples/sec; 0.040 sec/batch; 1h:04m:29s remains)
INFO - root - 2019-11-06 19:12:26.004748: step 53920, total loss = 1.68, predict loss = 0.53 (98.8 examples/sec; 0.040 sec/batch; 1h:04m:48s remains)
INFO - root - 2019-11-06 19:12:26.459672: step 53930, total loss = 1.36, predict loss = 0.37 (87.6 examples/sec; 0.046 sec/batch; 1h:13m:06s remains)
INFO - root - 2019-11-06 19:12:27.828253: step 53940, total loss = 3.09, predict loss = 0.92 (58.6 examples/sec; 0.068 sec/batch; 1h:49m:16s remains)
INFO - root - 2019-11-06 19:12:28.565244: step 53950, total loss = 1.90, predict loss = 0.53 (54.5 examples/sec; 0.073 sec/batch; 1h:57m:34s remains)
INFO - root - 2019-11-06 19:12:29.314540: step 53960, total loss = 1.48, predict loss = 0.39 (60.3 examples/sec; 0.066 sec/batch; 1h:46m:09s remains)
INFO - root - 2019-11-06 19:12:30.081331: step 53970, total loss = 2.83, predict loss = 0.76 (57.5 examples/sec; 0.070 sec/batch; 1h:51m:24s remains)
INFO - root - 2019-11-06 19:12:30.811253: step 53980, total loss = 3.17, predict loss = 1.04 (67.8 examples/sec; 0.059 sec/batch; 1h:34m:22s remains)
INFO - root - 2019-11-06 19:12:31.306953: step 53990, total loss = 2.38, predict loss = 0.65 (90.6 examples/sec; 0.044 sec/batch; 1h:10m:37s remains)
INFO - root - 2019-11-06 19:12:31.779702: step 54000, total loss = 1.83, predict loss = 0.48 (96.3 examples/sec; 0.042 sec/batch; 1h:06m:26s remains)
INFO - root - 2019-11-06 19:12:33.004589: step 54010, total loss = 1.30, predict loss = 0.31 (56.7 examples/sec; 0.071 sec/batch; 1h:52m:56s remains)
INFO - root - 2019-11-06 19:12:33.760581: step 54020, total loss = 2.13, predict loss = 0.67 (60.9 examples/sec; 0.066 sec/batch; 1h:44m:59s remains)
INFO - root - 2019-11-06 19:12:34.565128: step 54030, total loss = 2.98, predict loss = 0.87 (58.1 examples/sec; 0.069 sec/batch; 1h:50m:07s remains)
INFO - root - 2019-11-06 19:12:35.304769: step 54040, total loss = 1.81, predict loss = 0.49 (60.3 examples/sec; 0.066 sec/batch; 1h:46m:08s remains)
INFO - root - 2019-11-06 19:12:36.060968: step 54050, total loss = 1.60, predict loss = 0.45 (66.5 examples/sec; 0.060 sec/batch; 1h:36m:07s remains)
INFO - root - 2019-11-06 19:12:36.671544: step 54060, total loss = 1.75, predict loss = 0.47 (93.9 examples/sec; 0.043 sec/batch; 1h:08m:07s remains)
INFO - root - 2019-11-06 19:12:37.137116: step 54070, total loss = 2.34, predict loss = 0.63 (96.4 examples/sec; 0.041 sec/batch; 1h:06m:20s remains)
INFO - root - 2019-11-06 19:12:37.587046: step 54080, total loss = 1.66, predict loss = 0.47 (124.1 examples/sec; 0.032 sec/batch; 0h:51m:32s remains)
INFO - root - 2019-11-06 19:12:38.977269: step 54090, total loss = 2.11, predict loss = 0.57 (58.1 examples/sec; 0.069 sec/batch; 1h:50m:00s remains)
INFO - root - 2019-11-06 19:12:39.764278: step 54100, total loss = 1.75, predict loss = 0.45 (56.0 examples/sec; 0.071 sec/batch; 1h:54m:12s remains)
INFO - root - 2019-11-06 19:12:40.528175: step 54110, total loss = 1.83, predict loss = 0.46 (54.0 examples/sec; 0.074 sec/batch; 1h:58m:18s remains)
INFO - root - 2019-11-06 19:12:41.244161: step 54120, total loss = 2.30, predict loss = 0.62 (68.9 examples/sec; 0.058 sec/batch; 1h:32m:47s remains)
INFO - root - 2019-11-06 19:12:41.879659: step 54130, total loss = 2.29, predict loss = 0.61 (77.2 examples/sec; 0.052 sec/batch; 1h:22m:49s remains)
INFO - root - 2019-11-06 19:12:42.378058: step 54140, total loss = 2.05, predict loss = 0.58 (95.9 examples/sec; 0.042 sec/batch; 1h:06m:36s remains)
INFO - root - 2019-11-06 19:12:42.816800: step 54150, total loss = 2.53, predict loss = 0.68 (101.5 examples/sec; 0.039 sec/batch; 1h:02m:55s remains)
INFO - root - 2019-11-06 19:12:44.066636: step 54160, total loss = 1.75, predict loss = 0.46 (68.0 examples/sec; 0.059 sec/batch; 1h:33m:56s remains)
INFO - root - 2019-11-06 19:12:44.819343: step 54170, total loss = 2.26, predict loss = 0.59 (55.5 examples/sec; 0.072 sec/batch; 1h:55m:05s remains)
INFO - root - 2019-11-06 19:12:45.626840: step 54180, total loss = 1.59, predict loss = 0.43 (58.5 examples/sec; 0.068 sec/batch; 1h:49m:13s remains)
INFO - root - 2019-11-06 19:12:46.365585: step 54190, total loss = 2.67, predict loss = 0.74 (65.1 examples/sec; 0.061 sec/batch; 1h:38m:10s remains)
INFO - root - 2019-11-06 19:12:47.085329: step 54200, total loss = 2.52, predict loss = 0.71 (73.4 examples/sec; 0.054 sec/batch; 1h:26m:57s remains)
INFO - root - 2019-11-06 19:12:47.656791: step 54210, total loss = 2.67, predict loss = 0.71 (91.6 examples/sec; 0.044 sec/batch; 1h:09m:43s remains)
INFO - root - 2019-11-06 19:12:48.143667: step 54220, total loss = 1.80, predict loss = 0.47 (93.6 examples/sec; 0.043 sec/batch; 1h:08m:11s remains)
INFO - root - 2019-11-06 19:12:49.289627: step 54230, total loss = 2.47, predict loss = 0.64 (5.4 examples/sec; 0.735 sec/batch; 19h:33m:56s remains)
INFO - root - 2019-11-06 19:12:50.005803: step 54240, total loss = 1.52, predict loss = 0.38 (60.4 examples/sec; 0.066 sec/batch; 1h:45m:44s remains)
INFO - root - 2019-11-06 19:12:50.760354: step 54250, total loss = 1.63, predict loss = 0.39 (61.1 examples/sec; 0.065 sec/batch; 1h:44m:25s remains)
INFO - root - 2019-11-06 19:12:51.479738: step 54260, total loss = 1.84, predict loss = 0.48 (63.7 examples/sec; 0.063 sec/batch; 1h:40m:11s remains)
INFO - root - 2019-11-06 19:12:52.233781: step 54270, total loss = 1.07, predict loss = 0.28 (57.3 examples/sec; 0.070 sec/batch; 1h:51m:28s remains)
INFO - root - 2019-11-06 19:12:52.950562: step 54280, total loss = 2.99, predict loss = 0.86 (84.2 examples/sec; 0.048 sec/batch; 1h:15m:46s remains)
INFO - root - 2019-11-06 19:12:53.437254: step 54290, total loss = 2.34, predict loss = 0.63 (96.1 examples/sec; 0.042 sec/batch; 1h:06m:24s remains)
INFO - root - 2019-11-06 19:12:53.926328: step 54300, total loss = 2.13, predict loss = 0.53 (91.1 examples/sec; 0.044 sec/batch; 1h:10m:00s remains)
INFO - root - 2019-11-06 19:12:55.179747: step 54310, total loss = 2.54, predict loss = 0.72 (64.3 examples/sec; 0.062 sec/batch; 1h:39m:16s remains)
INFO - root - 2019-11-06 19:12:55.878695: step 54320, total loss = 1.54, predict loss = 0.41 (59.0 examples/sec; 0.068 sec/batch; 1h:48m:10s remains)
INFO - root - 2019-11-06 19:12:56.613242: step 54330, total loss = 1.17, predict loss = 0.33 (61.3 examples/sec; 0.065 sec/batch; 1h:44m:03s remains)
INFO - root - 2019-11-06 19:12:57.363321: step 54340, total loss = 2.18, predict loss = 0.57 (62.8 examples/sec; 0.064 sec/batch; 1h:41m:35s remains)
INFO - root - 2019-11-06 19:12:58.101682: step 54350, total loss = 2.94, predict loss = 0.82 (64.7 examples/sec; 0.062 sec/batch; 1h:38m:35s remains)
INFO - root - 2019-11-06 19:12:58.637586: step 54360, total loss = 3.35, predict loss = 0.97 (92.6 examples/sec; 0.043 sec/batch; 1h:08m:50s remains)
INFO - root - 2019-11-06 19:12:59.105763: step 54370, total loss = 2.02, predict loss = 0.52 (97.0 examples/sec; 0.041 sec/batch; 1h:05m:43s remains)
INFO - root - 2019-11-06 19:13:00.278861: step 54380, total loss = 1.53, predict loss = 0.36 (77.6 examples/sec; 0.052 sec/batch; 1h:22m:07s remains)
INFO - root - 2019-11-06 19:13:00.966043: step 54390, total loss = 1.76, predict loss = 0.47 (58.6 examples/sec; 0.068 sec/batch; 1h:48m:50s remains)
INFO - root - 2019-11-06 19:13:01.720041: step 54400, total loss = 2.66, predict loss = 0.75 (59.2 examples/sec; 0.068 sec/batch; 1h:47m:39s remains)
INFO - root - 2019-11-06 19:13:02.435243: step 54410, total loss = 2.36, predict loss = 0.64 (60.5 examples/sec; 0.066 sec/batch; 1h:45m:17s remains)
INFO - root - 2019-11-06 19:13:03.222147: step 54420, total loss = 1.36, predict loss = 0.36 (60.0 examples/sec; 0.067 sec/batch; 1h:46m:07s remains)
INFO - root - 2019-11-06 19:13:03.939653: step 54430, total loss = 2.81, predict loss = 0.72 (92.1 examples/sec; 0.043 sec/batch; 1h:09m:12s remains)
INFO - root - 2019-11-06 19:13:04.388999: step 54440, total loss = 2.24, predict loss = 0.59 (91.6 examples/sec; 0.044 sec/batch; 1h:09m:33s remains)
INFO - root - 2019-11-06 19:13:04.842496: step 54450, total loss = 2.27, predict loss = 0.62 (93.8 examples/sec; 0.043 sec/batch; 1h:07m:54s remains)
INFO - root - 2019-11-06 19:13:06.136170: step 54460, total loss = 2.47, predict loss = 0.74 (52.8 examples/sec; 0.076 sec/batch; 2h:00m:31s remains)
INFO - root - 2019-11-06 19:13:06.906902: step 54470, total loss = 2.99, predict loss = 0.86 (54.1 examples/sec; 0.074 sec/batch; 1h:57m:49s remains)
INFO - root - 2019-11-06 19:13:07.593086: step 54480, total loss = 3.17, predict loss = 1.00 (69.3 examples/sec; 0.058 sec/batch; 1h:31m:53s remains)
INFO - root - 2019-11-06 19:13:08.291478: step 54490, total loss = 1.83, predict loss = 0.48 (60.6 examples/sec; 0.066 sec/batch; 1h:45m:03s remains)
INFO - root - 2019-11-06 19:13:08.983020: step 54500, total loss = 1.98, predict loss = 0.60 (73.8 examples/sec; 0.054 sec/batch; 1h:26m:13s remains)
INFO - root - 2019-11-06 19:13:09.466609: step 54510, total loss = 1.53, predict loss = 0.37 (94.0 examples/sec; 0.043 sec/batch; 1h:07m:45s remains)
INFO - root - 2019-11-06 19:13:09.923044: step 54520, total loss = 1.27, predict loss = 0.34 (102.0 examples/sec; 0.039 sec/batch; 1h:02m:24s remains)
INFO - root - 2019-11-06 19:13:11.106080: step 54530, total loss = 2.24, predict loss = 0.67 (62.7 examples/sec; 0.064 sec/batch; 1h:41m:26s remains)
INFO - root - 2019-11-06 19:13:11.843004: step 54540, total loss = 1.76, predict loss = 0.49 (55.4 examples/sec; 0.072 sec/batch; 1h:54m:53s remains)
INFO - root - 2019-11-06 19:13:12.651478: step 54550, total loss = 2.58, predict loss = 0.67 (55.3 examples/sec; 0.072 sec/batch; 1h:55m:09s remains)
INFO - root - 2019-11-06 19:13:13.428234: step 54560, total loss = 2.28, predict loss = 0.61 (59.3 examples/sec; 0.067 sec/batch; 1h:47m:16s remains)
INFO - root - 2019-11-06 19:13:14.160226: step 54570, total loss = 2.22, predict loss = 0.59 (57.7 examples/sec; 0.069 sec/batch; 1h:50m:17s remains)
INFO - root - 2019-11-06 19:13:14.770052: step 54580, total loss = 1.91, predict loss = 0.56 (97.9 examples/sec; 0.041 sec/batch; 1h:04m:59s remains)
INFO - root - 2019-11-06 19:13:15.224267: step 54590, total loss = 3.06, predict loss = 0.97 (99.7 examples/sec; 0.040 sec/batch; 1h:03m:49s remains)
INFO - root - 2019-11-06 19:13:15.682641: step 54600, total loss = 1.85, predict loss = 0.48 (104.2 examples/sec; 0.038 sec/batch; 1h:01m:02s remains)
INFO - root - 2019-11-06 19:13:16.990325: step 54610, total loss = 1.76, predict loss = 0.45 (52.8 examples/sec; 0.076 sec/batch; 2h:00m:31s remains)
INFO - root - 2019-11-06 19:13:17.780027: step 54620, total loss = 1.48, predict loss = 0.37 (55.3 examples/sec; 0.072 sec/batch; 1h:54m:54s remains)
INFO - root - 2019-11-06 19:13:18.509044: step 54630, total loss = 1.63, predict loss = 0.48 (62.3 examples/sec; 0.064 sec/batch; 1h:41m:59s remains)
INFO - root - 2019-11-06 19:13:19.241342: step 54640, total loss = 1.79, predict loss = 0.44 (54.6 examples/sec; 0.073 sec/batch; 1h:56m:26s remains)
INFO - root - 2019-11-06 19:13:19.948013: step 54650, total loss = 1.63, predict loss = 0.40 (68.4 examples/sec; 0.058 sec/batch; 1h:32m:55s remains)
INFO - root - 2019-11-06 19:13:20.483412: step 54660, total loss = 3.10, predict loss = 0.94 (99.1 examples/sec; 0.040 sec/batch; 1h:04m:09s remains)
INFO - root - 2019-11-06 19:13:20.954231: step 54670, total loss = 1.46, predict loss = 0.36 (93.5 examples/sec; 0.043 sec/batch; 1h:07m:58s remains)
INFO - root - 2019-11-06 19:13:22.146382: step 54680, total loss = 1.80, predict loss = 0.47 (66.6 examples/sec; 0.060 sec/batch; 1h:35m:20s remains)
INFO - root - 2019-11-06 19:13:22.870088: step 54690, total loss = 1.52, predict loss = 0.41 (51.8 examples/sec; 0.077 sec/batch; 2h:02m:39s remains)
INFO - root - 2019-11-06 19:13:23.688069: step 54700, total loss = 1.78, predict loss = 0.51 (56.6 examples/sec; 0.071 sec/batch; 1h:52m:09s remains)
INFO - root - 2019-11-06 19:13:24.448398: step 54710, total loss = 2.05, predict loss = 0.59 (60.4 examples/sec; 0.066 sec/batch; 1h:45m:11s remains)
INFO - root - 2019-11-06 19:13:25.171395: step 54720, total loss = 3.47, predict loss = 1.09 (63.8 examples/sec; 0.063 sec/batch; 1h:39m:31s remains)
INFO - root - 2019-11-06 19:13:25.743800: step 54730, total loss = 1.87, predict loss = 0.49 (96.3 examples/sec; 0.042 sec/batch; 1h:05m:57s remains)
INFO - root - 2019-11-06 19:13:26.218769: step 54740, total loss = 1.88, predict loss = 0.58 (93.6 examples/sec; 0.043 sec/batch; 1h:07m:49s remains)
INFO - root - 2019-11-06 19:13:26.671763: step 54750, total loss = 1.70, predict loss = 0.50 (98.5 examples/sec; 0.041 sec/batch; 1h:04m:27s remains)
INFO - root - 2019-11-06 19:13:28.004431: step 54760, total loss = 1.65, predict loss = 0.42 (68.3 examples/sec; 0.059 sec/batch; 1h:32m:53s remains)
INFO - root - 2019-11-06 19:13:28.771928: step 54770, total loss = 3.08, predict loss = 0.96 (61.8 examples/sec; 0.065 sec/batch; 1h:42m:48s remains)
INFO - root - 2019-11-06 19:13:29.560903: step 54780, total loss = 1.74, predict loss = 0.43 (51.0 examples/sec; 0.078 sec/batch; 2h:04m:28s remains)
INFO - root - 2019-11-06 19:13:30.374896: step 54790, total loss = 2.30, predict loss = 0.66 (58.1 examples/sec; 0.069 sec/batch; 1h:49m:09s remains)
INFO - root - 2019-11-06 19:13:31.060708: step 54800, total loss = 1.86, predict loss = 0.45 (78.0 examples/sec; 0.051 sec/batch; 1h:21m:19s remains)
INFO - root - 2019-11-06 19:13:31.527466: step 54810, total loss = 1.03, predict loss = 0.28 (100.2 examples/sec; 0.040 sec/batch; 1h:03m:20s remains)
INFO - root - 2019-11-06 19:13:32.014146: step 54820, total loss = 2.01, predict loss = 0.53 (93.4 examples/sec; 0.043 sec/batch; 1h:07m:57s remains)
INFO - root - 2019-11-06 19:13:33.186893: step 54830, total loss = 0.98, predict loss = 0.25 (68.8 examples/sec; 0.058 sec/batch; 1h:32m:13s remains)
INFO - root - 2019-11-06 19:13:33.919400: step 54840, total loss = 2.52, predict loss = 0.69 (58.9 examples/sec; 0.068 sec/batch; 1h:47m:41s remains)
INFO - root - 2019-11-06 19:13:34.685175: step 54850, total loss = 2.37, predict loss = 0.61 (60.3 examples/sec; 0.066 sec/batch; 1h:45m:09s remains)
INFO - root - 2019-11-06 19:13:35.472904: step 54860, total loss = 1.16, predict loss = 0.27 (51.8 examples/sec; 0.077 sec/batch; 2h:02m:28s remains)
INFO - root - 2019-11-06 19:13:36.213780: step 54870, total loss = 1.68, predict loss = 0.43 (62.0 examples/sec; 0.065 sec/batch; 1h:42m:17s remains)
INFO - root - 2019-11-06 19:13:36.755179: step 54880, total loss = 2.49, predict loss = 0.68 (89.8 examples/sec; 0.045 sec/batch; 1h:10m:37s remains)
INFO - root - 2019-11-06 19:13:37.205100: step 54890, total loss = 1.66, predict loss = 0.51 (95.8 examples/sec; 0.042 sec/batch; 1h:06m:10s remains)
INFO - root - 2019-11-06 19:13:37.684040: step 54900, total loss = 2.14, predict loss = 0.59 (114.1 examples/sec; 0.035 sec/batch; 0h:55m:33s remains)
INFO - root - 2019-11-06 19:13:39.048397: step 54910, total loss = 3.08, predict loss = 0.95 (50.3 examples/sec; 0.080 sec/batch; 2h:06m:08s remains)
INFO - root - 2019-11-06 19:13:39.819445: step 54920, total loss = 1.94, predict loss = 0.51 (58.0 examples/sec; 0.069 sec/batch; 1h:49m:13s remains)
INFO - root - 2019-11-06 19:13:40.587575: step 54930, total loss = 1.49, predict loss = 0.36 (59.1 examples/sec; 0.068 sec/batch; 1h:47m:09s remains)
INFO - root - 2019-11-06 19:13:41.340452: step 54940, total loss = 1.28, predict loss = 0.31 (60.3 examples/sec; 0.066 sec/batch; 1h:45m:00s remains)
INFO - root - 2019-11-06 19:13:42.008648: step 54950, total loss = 2.84, predict loss = 0.88 (73.7 examples/sec; 0.054 sec/batch; 1h:25m:55s remains)
INFO - root - 2019-11-06 19:13:42.473491: step 54960, total loss = 1.64, predict loss = 0.40 (94.8 examples/sec; 0.042 sec/batch; 1h:06m:51s remains)
INFO - root - 2019-11-06 19:13:42.935360: step 54970, total loss = 1.54, predict loss = 0.38 (98.0 examples/sec; 0.041 sec/batch; 1h:04m:38s remains)
INFO - root - 2019-11-06 19:13:44.203439: step 54980, total loss = 0.89, predict loss = 0.25 (58.3 examples/sec; 0.069 sec/batch; 1h:48m:44s remains)
INFO - root - 2019-11-06 19:13:44.987268: step 54990, total loss = 1.44, predict loss = 0.36 (57.1 examples/sec; 0.070 sec/batch; 1h:50m:58s remains)
INFO - root - 2019-11-06 19:13:45.738611: step 55000, total loss = 2.27, predict loss = 0.63 (55.7 examples/sec; 0.072 sec/batch; 1h:53m:40s remains)
INFO - root - 2019-11-06 19:13:46.456093: step 55010, total loss = 2.23, predict loss = 0.58 (63.7 examples/sec; 0.063 sec/batch; 1h:39m:27s remains)
INFO - root - 2019-11-06 19:13:47.194552: step 55020, total loss = 2.41, predict loss = 0.66 (63.7 examples/sec; 0.063 sec/batch; 1h:39m:22s remains)
INFO - root - 2019-11-06 19:13:47.737831: step 55030, total loss = 1.58, predict loss = 0.38 (96.6 examples/sec; 0.041 sec/batch; 1h:05m:31s remains)
INFO - root - 2019-11-06 19:13:48.194220: step 55040, total loss = 2.61, predict loss = 0.80 (93.1 examples/sec; 0.043 sec/batch; 1h:07m:58s remains)
INFO - root - 2019-11-06 19:13:49.297814: step 55050, total loss = 1.11, predict loss = 0.27 (5.6 examples/sec; 0.714 sec/batch; 18h:50m:08s remains)
INFO - root - 2019-11-06 19:13:50.026811: step 55060, total loss = 2.85, predict loss = 0.84 (54.5 examples/sec; 0.073 sec/batch; 1h:56m:04s remains)
INFO - root - 2019-11-06 19:13:50.769255: step 55070, total loss = 1.15, predict loss = 0.30 (55.7 examples/sec; 0.072 sec/batch; 1h:53m:41s remains)
INFO - root - 2019-11-06 19:13:51.505401: step 55080, total loss = 1.21, predict loss = 0.31 (62.1 examples/sec; 0.064 sec/batch; 1h:41m:55s remains)
INFO - root - 2019-11-06 19:13:52.227448: step 55090, total loss = 1.21, predict loss = 0.32 (57.2 examples/sec; 0.070 sec/batch; 1h:50m:36s remains)
INFO - root - 2019-11-06 19:13:52.917813: step 55100, total loss = 3.00, predict loss = 0.84 (94.5 examples/sec; 0.042 sec/batch; 1h:06m:58s remains)
INFO - root - 2019-11-06 19:13:53.352116: step 55110, total loss = 1.31, predict loss = 0.35 (94.8 examples/sec; 0.042 sec/batch; 1h:06m:45s remains)
INFO - root - 2019-11-06 19:13:53.803681: step 55120, total loss = 1.48, predict loss = 0.37 (97.1 examples/sec; 0.041 sec/batch; 1h:05m:10s remains)
INFO - root - 2019-11-06 19:13:55.060781: step 55130, total loss = 3.05, predict loss = 0.94 (60.0 examples/sec; 0.067 sec/batch; 1h:45m:23s remains)
INFO - root - 2019-11-06 19:13:55.806441: step 55140, total loss = 1.44, predict loss = 0.39 (59.1 examples/sec; 0.068 sec/batch; 1h:47m:02s remains)
INFO - root - 2019-11-06 19:13:56.554660: step 55150, total loss = 2.45, predict loss = 0.72 (60.0 examples/sec; 0.067 sec/batch; 1h:45m:25s remains)
INFO - root - 2019-11-06 19:13:57.319321: step 55160, total loss = 2.12, predict loss = 0.59 (65.2 examples/sec; 0.061 sec/batch; 1h:36m:54s remains)
INFO - root - 2019-11-06 19:13:58.048608: step 55170, total loss = 2.18, predict loss = 0.60 (67.6 examples/sec; 0.059 sec/batch; 1h:33m:28s remains)
INFO - root - 2019-11-06 19:13:58.605318: step 55180, total loss = 2.94, predict loss = 0.82 (95.6 examples/sec; 0.042 sec/batch; 1h:06m:07s remains)
INFO - root - 2019-11-06 19:13:59.065808: step 55190, total loss = 2.04, predict loss = 0.51 (95.7 examples/sec; 0.042 sec/batch; 1h:06m:04s remains)
INFO - root - 2019-11-06 19:14:00.211196: step 55200, total loss = 2.04, predict loss = 0.57 (68.0 examples/sec; 0.059 sec/batch; 1h:33m:00s remains)
INFO - root - 2019-11-06 19:14:00.932788: step 55210, total loss = 1.65, predict loss = 0.41 (54.2 examples/sec; 0.074 sec/batch; 1h:56m:39s remains)
INFO - root - 2019-11-06 19:14:01.724150: step 55220, total loss = 2.17, predict loss = 0.58 (57.2 examples/sec; 0.070 sec/batch; 1h:50m:27s remains)
INFO - root - 2019-11-06 19:14:02.457424: step 55230, total loss = 2.09, predict loss = 0.63 (63.8 examples/sec; 0.063 sec/batch; 1h:38m:57s remains)
INFO - root - 2019-11-06 19:14:03.228623: step 55240, total loss = 2.81, predict loss = 0.84 (56.5 examples/sec; 0.071 sec/batch; 1h:51m:46s remains)
INFO - root - 2019-11-06 19:14:03.920772: step 55250, total loss = 2.83, predict loss = 0.95 (80.2 examples/sec; 0.050 sec/batch; 1h:18m:45s remains)
INFO - root - 2019-11-06 19:14:04.386407: step 55260, total loss = 3.23, predict loss = 1.09 (93.8 examples/sec; 0.043 sec/batch; 1h:07m:18s remains)
INFO - root - 2019-11-06 19:14:04.844811: step 55270, total loss = 2.16, predict loss = 0.59 (93.7 examples/sec; 0.043 sec/batch; 1h:07m:25s remains)
INFO - root - 2019-11-06 19:14:06.193282: step 55280, total loss = 2.53, predict loss = 0.70 (63.7 examples/sec; 0.063 sec/batch; 1h:39m:07s remains)
INFO - root - 2019-11-06 19:14:06.966086: step 55290, total loss = 2.25, predict loss = 0.62 (58.3 examples/sec; 0.069 sec/batch; 1h:48m:23s remains)
INFO - root - 2019-11-06 19:14:07.705291: step 55300, total loss = 1.73, predict loss = 0.44 (60.0 examples/sec; 0.067 sec/batch; 1h:45m:13s remains)
INFO - root - 2019-11-06 19:14:08.425594: step 55310, total loss = 2.79, predict loss = 0.81 (57.0 examples/sec; 0.070 sec/batch; 1h:50m:44s remains)
INFO - root - 2019-11-06 19:14:09.200633: step 55320, total loss = 2.49, predict loss = 0.67 (54.6 examples/sec; 0.073 sec/batch; 1h:55m:30s remains)
INFO - root - 2019-11-06 19:14:09.757536: step 55330, total loss = 1.88, predict loss = 0.51 (97.4 examples/sec; 0.041 sec/batch; 1h:04m:46s remains)
INFO - root - 2019-11-06 19:14:10.238520: step 55340, total loss = 2.34, predict loss = 0.64 (103.4 examples/sec; 0.039 sec/batch; 1h:01m:02s remains)
INFO - root - 2019-11-06 19:14:11.396835: step 55350, total loss = 1.28, predict loss = 0.32 (70.8 examples/sec; 0.056 sec/batch; 1h:29m:06s remains)
INFO - root - 2019-11-06 19:14:12.162639: step 55360, total loss = 1.80, predict loss = 0.48 (50.2 examples/sec; 0.080 sec/batch; 2h:05m:43s remains)
INFO - root - 2019-11-06 19:14:12.948544: step 55370, total loss = 1.33, predict loss = 0.33 (57.0 examples/sec; 0.070 sec/batch; 1h:50m:39s remains)
INFO - root - 2019-11-06 19:14:13.704720: step 55380, total loss = 2.15, predict loss = 0.59 (58.9 examples/sec; 0.068 sec/batch; 1h:47m:09s remains)
INFO - root - 2019-11-06 19:14:14.483755: step 55390, total loss = 1.28, predict loss = 0.33 (58.5 examples/sec; 0.068 sec/batch; 1h:47m:46s remains)
INFO - root - 2019-11-06 19:14:15.115920: step 55400, total loss = 1.86, predict loss = 0.52 (100.5 examples/sec; 0.040 sec/batch; 1h:02m:45s remains)
INFO - root - 2019-11-06 19:14:15.557063: step 55410, total loss = 2.36, predict loss = 0.69 (99.0 examples/sec; 0.040 sec/batch; 1h:03m:42s remains)
INFO - root - 2019-11-06 19:14:16.045022: step 55420, total loss = 1.75, predict loss = 0.47 (98.9 examples/sec; 0.040 sec/batch; 1h:03m:45s remains)
INFO - root - 2019-11-06 19:14:17.361866: step 55430, total loss = 1.93, predict loss = 0.51 (58.1 examples/sec; 0.069 sec/batch; 1h:48m:34s remains)
INFO - root - 2019-11-06 19:14:18.149660: step 55440, total loss = 1.10, predict loss = 0.31 (56.6 examples/sec; 0.071 sec/batch; 1h:51m:27s remains)
INFO - root - 2019-11-06 19:14:18.860459: step 55450, total loss = 1.33, predict loss = 0.34 (62.5 examples/sec; 0.064 sec/batch; 1h:40m:50s remains)
INFO - root - 2019-11-06 19:14:19.606219: step 55460, total loss = 2.87, predict loss = 0.96 (59.9 examples/sec; 0.067 sec/batch; 1h:45m:16s remains)
INFO - root - 2019-11-06 19:14:20.352069: step 55470, total loss = 2.05, predict loss = 0.54 (63.8 examples/sec; 0.063 sec/batch; 1h:38m:45s remains)
INFO - root - 2019-11-06 19:14:20.864426: step 55480, total loss = 1.84, predict loss = 0.46 (89.2 examples/sec; 0.045 sec/batch; 1h:10m:40s remains)
INFO - root - 2019-11-06 19:14:21.327152: step 55490, total loss = 1.95, predict loss = 0.64 (92.2 examples/sec; 0.043 sec/batch; 1h:08m:18s remains)
INFO - root - 2019-11-06 19:14:22.515799: step 55500, total loss = 2.28, predict loss = 0.62 (72.3 examples/sec; 0.055 sec/batch; 1h:27m:08s remains)
INFO - root - 2019-11-06 19:14:23.259756: step 55510, total loss = 1.38, predict loss = 0.38 (60.2 examples/sec; 0.066 sec/batch; 1h:44m:40s remains)
INFO - root - 2019-11-06 19:14:24.004601: step 55520, total loss = 2.24, predict loss = 0.59 (64.6 examples/sec; 0.062 sec/batch; 1h:37m:34s remains)
INFO - root - 2019-11-06 19:14:24.737784: step 55530, total loss = 1.38, predict loss = 0.35 (59.5 examples/sec; 0.067 sec/batch; 1h:45m:47s remains)
INFO - root - 2019-11-06 19:14:25.522781: step 55540, total loss = 2.07, predict loss = 0.56 (57.1 examples/sec; 0.070 sec/batch; 1h:50m:20s remains)
INFO - root - 2019-11-06 19:14:26.098142: step 55550, total loss = 2.37, predict loss = 0.67 (99.5 examples/sec; 0.040 sec/batch; 1h:03m:16s remains)
INFO - root - 2019-11-06 19:14:26.551261: step 55560, total loss = 1.67, predict loss = 0.40 (95.6 examples/sec; 0.042 sec/batch; 1h:05m:50s remains)
INFO - root - 2019-11-06 19:14:27.014796: step 55570, total loss = 3.19, predict loss = 0.99 (88.3 examples/sec; 0.045 sec/batch; 1h:11m:15s remains)
INFO - root - 2019-11-06 19:14:28.395481: step 55580, total loss = 2.61, predict loss = 0.73 (64.7 examples/sec; 0.062 sec/batch; 1h:37m:14s remains)
INFO - root - 2019-11-06 19:14:29.199544: step 55590, total loss = 2.39, predict loss = 0.65 (53.3 examples/sec; 0.075 sec/batch; 1h:58m:10s remains)
INFO - root - 2019-11-06 19:14:29.900332: step 55600, total loss = 1.08, predict loss = 0.28 (65.7 examples/sec; 0.061 sec/batch; 1h:35m:48s remains)
INFO - root - 2019-11-06 19:14:30.615996: step 55610, total loss = 2.45, predict loss = 0.65 (56.4 examples/sec; 0.071 sec/batch; 1h:51m:29s remains)
INFO - root - 2019-11-06 19:14:31.331211: step 55620, total loss = 1.54, predict loss = 0.38 (72.4 examples/sec; 0.055 sec/batch; 1h:26m:55s remains)
INFO - root - 2019-11-06 19:14:31.807795: step 55630, total loss = 2.52, predict loss = 0.69 (106.2 examples/sec; 0.038 sec/batch; 0h:59m:15s remains)
INFO - root - 2019-11-06 19:14:32.259085: step 55640, total loss = 1.95, predict loss = 0.49 (96.3 examples/sec; 0.042 sec/batch; 1h:05m:20s remains)
INFO - root - 2019-11-06 19:14:33.469694: step 55650, total loss = 2.07, predict loss = 0.64 (70.8 examples/sec; 0.056 sec/batch; 1h:28m:49s remains)
INFO - root - 2019-11-06 19:14:34.206890: step 55660, total loss = 2.53, predict loss = 0.67 (68.7 examples/sec; 0.058 sec/batch; 1h:31m:29s remains)
INFO - root - 2019-11-06 19:14:34.909022: step 55670, total loss = 1.45, predict loss = 0.38 (71.9 examples/sec; 0.056 sec/batch; 1h:27m:30s remains)
INFO - root - 2019-11-06 19:14:35.692998: step 55680, total loss = 1.80, predict loss = 0.53 (55.4 examples/sec; 0.072 sec/batch; 1h:53m:35s remains)
INFO - root - 2019-11-06 19:14:36.461771: step 55690, total loss = 1.65, predict loss = 0.50 (59.4 examples/sec; 0.067 sec/batch; 1h:45m:54s remains)
INFO - root - 2019-11-06 19:14:37.025740: step 55700, total loss = 3.03, predict loss = 0.96 (98.9 examples/sec; 0.040 sec/batch; 1h:03m:32s remains)
INFO - root - 2019-11-06 19:14:37.479369: step 55710, total loss = 3.34, predict loss = 1.08 (90.6 examples/sec; 0.044 sec/batch; 1h:09m:21s remains)
INFO - root - 2019-11-06 19:14:37.928099: step 55720, total loss = 2.36, predict loss = 0.66 (123.7 examples/sec; 0.032 sec/batch; 0h:50m:48s remains)
INFO - root - 2019-11-06 19:14:39.255815: step 55730, total loss = 1.29, predict loss = 0.34 (64.9 examples/sec; 0.062 sec/batch; 1h:36m:46s remains)
INFO - root - 2019-11-06 19:14:40.011757: step 55740, total loss = 1.02, predict loss = 0.26 (59.1 examples/sec; 0.068 sec/batch; 1h:46m:20s remains)
INFO - root - 2019-11-06 19:14:40.789395: step 55750, total loss = 2.20, predict loss = 0.60 (55.8 examples/sec; 0.072 sec/batch; 1h:52m:32s remains)
INFO - root - 2019-11-06 19:14:41.536234: step 55760, total loss = 2.53, predict loss = 0.75 (57.4 examples/sec; 0.070 sec/batch; 1h:49m:22s remains)
INFO - root - 2019-11-06 19:14:42.206523: step 55770, total loss = 2.25, predict loss = 0.63 (72.3 examples/sec; 0.055 sec/batch; 1h:26m:55s remains)
INFO - root - 2019-11-06 19:14:42.701796: step 55780, total loss = 2.91, predict loss = 0.87 (101.0 examples/sec; 0.040 sec/batch; 1h:02m:10s remains)
INFO - root - 2019-11-06 19:14:43.161230: step 55790, total loss = 3.24, predict loss = 1.02 (89.3 examples/sec; 0.045 sec/batch; 1h:10m:18s remains)
INFO - root - 2019-11-06 19:14:44.363346: step 55800, total loss = 0.83, predict loss = 0.24 (61.5 examples/sec; 0.065 sec/batch; 1h:42m:06s remains)
INFO - root - 2019-11-06 19:14:45.145627: step 55810, total loss = 1.60, predict loss = 0.41 (55.9 examples/sec; 0.072 sec/batch; 1h:52m:20s remains)
INFO - root - 2019-11-06 19:14:45.893549: step 55820, total loss = 2.33, predict loss = 0.61 (55.3 examples/sec; 0.072 sec/batch; 1h:53m:31s remains)
INFO - root - 2019-11-06 19:14:46.614756: step 55830, total loss = 2.26, predict loss = 0.61 (55.2 examples/sec; 0.072 sec/batch; 1h:53m:39s remains)
INFO - root - 2019-11-06 19:14:47.383564: step 55840, total loss = 2.53, predict loss = 0.71 (65.9 examples/sec; 0.061 sec/batch; 1h:35m:13s remains)
INFO - root - 2019-11-06 19:14:47.970644: step 55850, total loss = 1.66, predict loss = 0.41 (97.6 examples/sec; 0.041 sec/batch; 1h:04m:18s remains)
INFO - root - 2019-11-06 19:14:48.445774: step 55860, total loss = 1.55, predict loss = 0.40 (94.5 examples/sec; 0.042 sec/batch; 1h:06m:26s remains)
INFO - root - 2019-11-06 19:14:49.590424: step 55870, total loss = 1.31, predict loss = 0.34 (5.5 examples/sec; 0.730 sec/batch; 19h:04m:56s remains)
INFO - root - 2019-11-06 19:14:50.304061: step 55880, total loss = 2.05, predict loss = 0.54 (63.7 examples/sec; 0.063 sec/batch; 1h:38m:30s remains)
INFO - root - 2019-11-06 19:14:51.101387: step 55890, total loss = 1.48, predict loss = 0.40 (52.4 examples/sec; 0.076 sec/batch; 1h:59m:49s remains)
INFO - root - 2019-11-06 19:14:51.886774: step 55900, total loss = 1.81, predict loss = 0.49 (52.6 examples/sec; 0.076 sec/batch; 1h:59m:22s remains)
INFO - root - 2019-11-06 19:14:52.640210: step 55910, total loss = 2.42, predict loss = 0.67 (58.7 examples/sec; 0.068 sec/batch; 1h:46m:50s remains)
INFO - root - 2019-11-06 19:14:53.286194: step 55920, total loss = 2.03, predict loss = 0.50 (94.8 examples/sec; 0.042 sec/batch; 1h:06m:10s remains)
INFO - root - 2019-11-06 19:14:53.741085: step 55930, total loss = 2.39, predict loss = 0.67 (90.9 examples/sec; 0.044 sec/batch; 1h:08m:59s remains)
INFO - root - 2019-11-06 19:14:54.219361: step 55940, total loss = 2.04, predict loss = 0.55 (93.4 examples/sec; 0.043 sec/batch; 1h:07m:09s remains)
INFO - root - 2019-11-06 19:14:55.446926: step 55950, total loss = 1.58, predict loss = 0.40 (67.8 examples/sec; 0.059 sec/batch; 1h:32m:31s remains)
INFO - root - 2019-11-06 19:14:56.246351: step 55960, total loss = 2.07, predict loss = 0.55 (56.4 examples/sec; 0.071 sec/batch; 1h:51m:14s remains)
INFO - root - 2019-11-06 19:14:56.957762: step 55970, total loss = 2.00, predict loss = 0.51 (61.6 examples/sec; 0.065 sec/batch; 1h:41m:45s remains)
INFO - root - 2019-11-06 19:14:57.736115: step 55980, total loss = 2.73, predict loss = 0.81 (56.5 examples/sec; 0.071 sec/batch; 1h:50m:55s remains)
INFO - root - 2019-11-06 19:14:58.446249: step 55990, total loss = 1.33, predict loss = 0.36 (70.8 examples/sec; 0.056 sec/batch; 1h:28m:29s remains)
INFO - root - 2019-11-06 19:14:58.965405: step 56000, total loss = 2.12, predict loss = 0.55 (98.0 examples/sec; 0.041 sec/batch; 1h:03m:57s remains)
INFO - root - 2019-11-06 19:14:59.429917: step 56010, total loss = 1.76, predict loss = 0.44 (90.4 examples/sec; 0.044 sec/batch; 1h:09m:20s remains)
INFO - root - 2019-11-06 19:15:00.645642: step 56020, total loss = 1.67, predict loss = 0.44 (64.7 examples/sec; 0.062 sec/batch; 1h:36m:51s remains)
INFO - root - 2019-11-06 19:15:01.360904: step 56030, total loss = 2.64, predict loss = 0.71 (56.8 examples/sec; 0.070 sec/batch; 1h:50m:13s remains)
INFO - root - 2019-11-06 19:15:02.078522: step 56040, total loss = 1.31, predict loss = 0.31 (61.0 examples/sec; 0.066 sec/batch; 1h:42m:40s remains)
INFO - root - 2019-11-06 19:15:02.791195: step 56050, total loss = 3.49, predict loss = 1.12 (63.0 examples/sec; 0.064 sec/batch; 1h:39m:27s remains)
INFO - root - 2019-11-06 19:15:03.558909: step 56060, total loss = 2.66, predict loss = 0.77 (65.5 examples/sec; 0.061 sec/batch; 1h:35m:41s remains)
INFO - root - 2019-11-06 19:15:04.260154: step 56070, total loss = 1.75, predict loss = 0.49 (98.6 examples/sec; 0.041 sec/batch; 1h:03m:28s remains)
INFO - root - 2019-11-06 19:15:04.716438: step 56080, total loss = 1.48, predict loss = 0.38 (92.0 examples/sec; 0.043 sec/batch; 1h:08m:04s remains)
INFO - root - 2019-11-06 19:15:05.164132: step 56090, total loss = 2.03, predict loss = 0.50 (91.8 examples/sec; 0.044 sec/batch; 1h:08m:10s remains)
INFO - root - 2019-11-06 19:15:06.492335: step 56100, total loss = 1.57, predict loss = 0.44 (59.3 examples/sec; 0.067 sec/batch; 1h:45m:30s remains)
INFO - root - 2019-11-06 19:15:07.281708: step 56110, total loss = 1.61, predict loss = 0.40 (50.0 examples/sec; 0.080 sec/batch; 2h:05m:12s remains)
INFO - root - 2019-11-06 19:15:08.033990: step 56120, total loss = 1.62, predict loss = 0.43 (55.9 examples/sec; 0.072 sec/batch; 1h:51m:53s remains)
INFO - root - 2019-11-06 19:15:08.757598: step 56130, total loss = 2.44, predict loss = 0.64 (63.8 examples/sec; 0.063 sec/batch; 1h:38m:01s remains)
INFO - root - 2019-11-06 19:15:09.488365: step 56140, total loss = 3.08, predict loss = 1.02 (62.7 examples/sec; 0.064 sec/batch; 1h:39m:43s remains)
INFO - root - 2019-11-06 19:15:10.044044: step 56150, total loss = 2.65, predict loss = 0.81 (90.3 examples/sec; 0.044 sec/batch; 1h:09m:15s remains)
INFO - root - 2019-11-06 19:15:10.501921: step 56160, total loss = 1.76, predict loss = 0.49 (94.6 examples/sec; 0.042 sec/batch; 1h:06m:08s remains)
INFO - root - 2019-11-06 19:15:11.638582: step 56170, total loss = 2.26, predict loss = 0.62 (70.0 examples/sec; 0.057 sec/batch; 1h:29m:19s remains)
INFO - root - 2019-11-06 19:15:12.331915: step 56180, total loss = 1.21, predict loss = 0.28 (61.8 examples/sec; 0.065 sec/batch; 1h:41m:16s remains)
INFO - root - 2019-11-06 19:15:13.068523: step 56190, total loss = 2.56, predict loss = 0.71 (48.2 examples/sec; 0.083 sec/batch; 2h:09m:50s remains)
INFO - root - 2019-11-06 19:15:13.805665: step 56200, total loss = 1.91, predict loss = 0.49 (58.5 examples/sec; 0.068 sec/batch; 1h:46m:53s remains)
INFO - root - 2019-11-06 19:15:14.571563: step 56210, total loss = 1.10, predict loss = 0.27 (55.3 examples/sec; 0.072 sec/batch; 1h:53m:05s remains)
INFO - root - 2019-11-06 19:15:15.185003: step 56220, total loss = 1.68, predict loss = 0.48 (98.3 examples/sec; 0.041 sec/batch; 1h:03m:35s remains)
INFO - root - 2019-11-06 19:15:15.640025: step 56230, total loss = 1.38, predict loss = 0.34 (97.9 examples/sec; 0.041 sec/batch; 1h:03m:50s remains)
INFO - root - 2019-11-06 19:15:16.088045: step 56240, total loss = 2.42, predict loss = 0.68 (98.5 examples/sec; 0.041 sec/batch; 1h:03m:29s remains)
INFO - root - 2019-11-06 19:15:17.372866: step 56250, total loss = 1.55, predict loss = 0.40 (63.7 examples/sec; 0.063 sec/batch; 1h:38m:03s remains)
INFO - root - 2019-11-06 19:15:18.117937: step 56260, total loss = 1.10, predict loss = 0.27 (57.8 examples/sec; 0.069 sec/batch; 1h:48m:10s remains)
INFO - root - 2019-11-06 19:15:18.842953: step 56270, total loss = 2.32, predict loss = 0.68 (60.0 examples/sec; 0.067 sec/batch; 1h:44m:06s remains)
INFO - root - 2019-11-06 19:15:19.584058: step 56280, total loss = 1.34, predict loss = 0.37 (60.5 examples/sec; 0.066 sec/batch; 1h:43m:16s remains)
INFO - root - 2019-11-06 19:15:20.255490: step 56290, total loss = 1.46, predict loss = 0.36 (68.2 examples/sec; 0.059 sec/batch; 1h:31m:36s remains)
INFO - root - 2019-11-06 19:15:20.793748: step 56300, total loss = 2.19, predict loss = 0.63 (96.1 examples/sec; 0.042 sec/batch; 1h:04m:59s remains)
INFO - root - 2019-11-06 19:15:21.232336: step 56310, total loss = 2.63, predict loss = 0.82 (95.9 examples/sec; 0.042 sec/batch; 1h:05m:08s remains)
INFO - root - 2019-11-06 19:15:22.397344: step 56320, total loss = 2.89, predict loss = 0.86 (68.4 examples/sec; 0.058 sec/batch; 1h:31m:15s remains)
INFO - root - 2019-11-06 19:15:23.105728: step 56330, total loss = 3.03, predict loss = 1.00 (58.4 examples/sec; 0.068 sec/batch; 1h:46m:54s remains)
INFO - root - 2019-11-06 19:15:23.832416: step 56340, total loss = 2.22, predict loss = 0.61 (64.3 examples/sec; 0.062 sec/batch; 1h:37m:10s remains)
INFO - root - 2019-11-06 19:15:24.531775: step 56350, total loss = 2.00, predict loss = 0.53 (63.1 examples/sec; 0.063 sec/batch; 1h:39m:00s remains)
INFO - root - 2019-11-06 19:15:25.229942: step 56360, total loss = 2.16, predict loss = 0.53 (64.7 examples/sec; 0.062 sec/batch; 1h:36m:30s remains)
INFO - root - 2019-11-06 19:15:25.774084: step 56370, total loss = 1.95, predict loss = 0.51 (104.2 examples/sec; 0.038 sec/batch; 0h:59m:52s remains)
INFO - root - 2019-11-06 19:15:26.239119: step 56380, total loss = 2.69, predict loss = 0.71 (95.9 examples/sec; 0.042 sec/batch; 1h:05m:02s remains)
INFO - root - 2019-11-06 19:15:26.688499: step 56390, total loss = 2.83, predict loss = 0.87 (99.7 examples/sec; 0.040 sec/batch; 1h:02m:36s remains)
INFO - root - 2019-11-06 19:15:28.043343: step 56400, total loss = 1.24, predict loss = 0.35 (54.4 examples/sec; 0.074 sec/batch; 1h:54m:41s remains)
INFO - root - 2019-11-06 19:15:28.835399: step 56410, total loss = 1.16, predict loss = 0.34 (59.4 examples/sec; 0.067 sec/batch; 1h:45m:03s remains)
INFO - root - 2019-11-06 19:15:29.591079: step 56420, total loss = 2.18, predict loss = 0.70 (49.8 examples/sec; 0.080 sec/batch; 2h:05m:21s remains)
INFO - root - 2019-11-06 19:15:30.468495: step 56430, total loss = 1.45, predict loss = 0.34 (49.5 examples/sec; 0.081 sec/batch; 2h:06m:01s remains)
INFO - root - 2019-11-06 19:15:31.191945: step 56440, total loss = 2.31, predict loss = 0.62 (76.8 examples/sec; 0.052 sec/batch; 1h:21m:12s remains)
INFO - root - 2019-11-06 19:15:31.689527: step 56450, total loss = 2.36, predict loss = 0.73 (98.0 examples/sec; 0.041 sec/batch; 1h:03m:38s remains)
INFO - root - 2019-11-06 19:15:32.178425: step 56460, total loss = 2.37, predict loss = 0.58 (93.8 examples/sec; 0.043 sec/batch; 1h:06m:28s remains)
INFO - root - 2019-11-06 19:15:33.337815: step 56470, total loss = 1.10, predict loss = 0.30 (69.4 examples/sec; 0.058 sec/batch; 1h:29m:51s remains)
INFO - root - 2019-11-06 19:15:34.065986: step 56480, total loss = 1.13, predict loss = 0.33 (58.6 examples/sec; 0.068 sec/batch; 1h:46m:21s remains)
INFO - root - 2019-11-06 19:15:34.858720: step 56490, total loss = 2.52, predict loss = 0.73 (58.4 examples/sec; 0.069 sec/batch; 1h:46m:49s remains)
INFO - root - 2019-11-06 19:15:35.647861: step 56500, total loss = 2.19, predict loss = 0.57 (59.0 examples/sec; 0.068 sec/batch; 1h:45m:35s remains)
INFO - root - 2019-11-06 19:15:36.399854: step 56510, total loss = 1.97, predict loss = 0.56 (64.4 examples/sec; 0.062 sec/batch; 1h:36m:43s remains)
INFO - root - 2019-11-06 19:15:36.944078: step 56520, total loss = 1.61, predict loss = 0.42 (102.9 examples/sec; 0.039 sec/batch; 1h:00m:33s remains)
INFO - root - 2019-11-06 19:15:37.410004: step 56530, total loss = 1.90, predict loss = 0.52 (91.6 examples/sec; 0.044 sec/batch; 1h:08m:03s remains)
INFO - root - 2019-11-06 19:15:37.885801: step 56540, total loss = 2.97, predict loss = 0.86 (123.7 examples/sec; 0.032 sec/batch; 0h:50m:21s remains)
INFO - root - 2019-11-06 19:15:39.230630: step 56550, total loss = 1.52, predict loss = 0.38 (59.1 examples/sec; 0.068 sec/batch; 1h:45m:26s remains)
INFO - root - 2019-11-06 19:15:39.934850: step 56560, total loss = 1.20, predict loss = 0.31 (58.1 examples/sec; 0.069 sec/batch; 1h:47m:10s remains)
INFO - root - 2019-11-06 19:15:40.695474: step 56570, total loss = 2.14, predict loss = 0.56 (56.9 examples/sec; 0.070 sec/batch; 1h:49m:29s remains)
INFO - root - 2019-11-06 19:15:41.477616: step 56580, total loss = 1.94, predict loss = 0.55 (53.3 examples/sec; 0.075 sec/batch; 1h:56m:54s remains)
INFO - root - 2019-11-06 19:15:42.210928: step 56590, total loss = 2.52, predict loss = 0.73 (70.7 examples/sec; 0.057 sec/batch; 1h:28m:03s remains)
INFO - root - 2019-11-06 19:15:42.693629: step 56600, total loss = 2.54, predict loss = 0.76 (91.0 examples/sec; 0.044 sec/batch; 1h:08m:26s remains)
INFO - root - 2019-11-06 19:15:43.161920: step 56610, total loss = 1.60, predict loss = 0.42 (93.1 examples/sec; 0.043 sec/batch; 1h:06m:51s remains)
INFO - root - 2019-11-06 19:15:44.428484: step 56620, total loss = 1.71, predict loss = 0.45 (57.9 examples/sec; 0.069 sec/batch; 1h:47m:35s remains)
INFO - root - 2019-11-06 19:15:45.183504: step 56630, total loss = 2.55, predict loss = 0.65 (59.4 examples/sec; 0.067 sec/batch; 1h:44m:44s remains)
INFO - root - 2019-11-06 19:15:45.928923: step 56640, total loss = 1.88, predict loss = 0.47 (60.0 examples/sec; 0.067 sec/batch; 1h:43m:43s remains)
INFO - root - 2019-11-06 19:15:46.697515: step 56650, total loss = 2.39, predict loss = 0.73 (60.2 examples/sec; 0.066 sec/batch; 1h:43m:23s remains)
INFO - root - 2019-11-06 19:15:47.449284: step 56660, total loss = 1.60, predict loss = 0.41 (66.5 examples/sec; 0.060 sec/batch; 1h:33m:32s remains)
INFO - root - 2019-11-06 19:15:47.993916: step 56670, total loss = 2.40, predict loss = 0.75 (100.3 examples/sec; 0.040 sec/batch; 1h:02m:03s remains)
INFO - root - 2019-11-06 19:15:48.463357: step 56680, total loss = 1.88, predict loss = 0.50 (92.4 examples/sec; 0.043 sec/batch; 1h:07m:18s remains)
INFO - root - 2019-11-06 19:15:49.613654: step 56690, total loss = 2.74, predict loss = 0.84 (5.5 examples/sec; 0.727 sec/batch; 18h:50m:19s remains)
INFO - root - 2019-11-06 19:15:50.304596: step 56700, total loss = 2.48, predict loss = 0.72 (66.0 examples/sec; 0.061 sec/batch; 1h:34m:10s remains)
INFO - root - 2019-11-06 19:15:51.056660: step 56710, total loss = 1.90, predict loss = 0.53 (58.6 examples/sec; 0.068 sec/batch; 1h:46m:03s remains)
INFO - root - 2019-11-06 19:15:51.813294: step 56720, total loss = 1.88, predict loss = 0.51 (59.5 examples/sec; 0.067 sec/batch; 1h:44m:32s remains)
INFO - root - 2019-11-06 19:15:52.602994: step 56730, total loss = 2.17, predict loss = 0.64 (57.8 examples/sec; 0.069 sec/batch; 1h:47m:32s remains)
INFO - root - 2019-11-06 19:15:53.303996: step 56740, total loss = 1.83, predict loss = 0.49 (88.3 examples/sec; 0.045 sec/batch; 1h:10m:23s remains)
INFO - root - 2019-11-06 19:15:53.762023: step 56750, total loss = 2.38, predict loss = 0.65 (95.7 examples/sec; 0.042 sec/batch; 1h:04m:58s remains)
INFO - root - 2019-11-06 19:15:54.225909: step 56760, total loss = 1.64, predict loss = 0.43 (89.7 examples/sec; 0.045 sec/batch; 1h:09m:15s remains)
INFO - root - 2019-11-06 19:15:55.514410: step 56770, total loss = 3.17, predict loss = 0.98 (55.1 examples/sec; 0.073 sec/batch; 1h:52m:50s remains)
INFO - root - 2019-11-06 19:15:56.261002: step 56780, total loss = 1.55, predict loss = 0.39 (60.1 examples/sec; 0.067 sec/batch; 1h:43m:22s remains)
INFO - root - 2019-11-06 19:15:57.018687: step 56790, total loss = 1.80, predict loss = 0.46 (54.5 examples/sec; 0.073 sec/batch; 1h:54m:03s remains)
INFO - root - 2019-11-06 19:15:57.811043: step 56800, total loss = 2.55, predict loss = 0.70 (62.1 examples/sec; 0.064 sec/batch; 1h:40m:04s remains)
INFO - root - 2019-11-06 19:15:58.536504: step 56810, total loss = 1.73, predict loss = 0.44 (50.9 examples/sec; 0.079 sec/batch; 2h:01m:58s remains)
INFO - root - 2019-11-06 19:15:59.130686: step 56820, total loss = 1.74, predict loss = 0.40 (91.4 examples/sec; 0.044 sec/batch; 1h:07m:58s remains)
INFO - root - 2019-11-06 19:15:59.600651: step 56830, total loss = 2.14, predict loss = 0.57 (91.0 examples/sec; 0.044 sec/batch; 1h:08m:16s remains)
INFO - root - 2019-11-06 19:16:00.781738: step 56840, total loss = 2.32, predict loss = 0.71 (67.9 examples/sec; 0.059 sec/batch; 1h:31m:27s remains)
INFO - root - 2019-11-06 19:16:01.495775: step 56850, total loss = 2.01, predict loss = 0.68 (51.6 examples/sec; 0.077 sec/batch; 2h:00m:17s remains)
INFO - root - 2019-11-06 19:16:02.263996: step 56860, total loss = 2.65, predict loss = 0.76 (56.3 examples/sec; 0.071 sec/batch; 1h:50m:19s remains)
INFO - root - 2019-11-06 19:16:03.014733: step 56870, total loss = 2.77, predict loss = 0.83 (56.7 examples/sec; 0.071 sec/batch; 1h:49m:30s remains)
INFO - root - 2019-11-06 19:16:03.785358: step 56880, total loss = 2.81, predict loss = 0.87 (58.6 examples/sec; 0.068 sec/batch; 1h:45m:53s remains)
INFO - root - 2019-11-06 19:16:04.446082: step 56890, total loss = 2.29, predict loss = 0.71 (89.3 examples/sec; 0.045 sec/batch; 1h:09m:32s remains)
INFO - root - 2019-11-06 19:16:04.938406: step 56900, total loss = 2.73, predict loss = 0.78 (89.7 examples/sec; 0.045 sec/batch; 1h:09m:13s remains)
INFO - root - 2019-11-06 19:16:05.391449: step 56910, total loss = 1.39, predict loss = 0.34 (92.0 examples/sec; 0.043 sec/batch; 1h:07m:27s remains)
INFO - root - 2019-11-06 19:16:06.679771: step 56920, total loss = 1.94, predict loss = 0.56 (58.3 examples/sec; 0.069 sec/batch; 1h:46m:26s remains)
INFO - root - 2019-11-06 19:16:07.455534: step 56930, total loss = 1.98, predict loss = 0.58 (53.1 examples/sec; 0.075 sec/batch; 1h:56m:49s remains)
INFO - root - 2019-11-06 19:16:08.228720: step 56940, total loss = 3.50, predict loss = 1.13 (59.6 examples/sec; 0.067 sec/batch; 1h:44m:03s remains)
INFO - root - 2019-11-06 19:16:09.014808: step 56950, total loss = 1.91, predict loss = 0.52 (52.4 examples/sec; 0.076 sec/batch; 1h:58m:29s remains)
INFO - root - 2019-11-06 19:16:09.730979: step 56960, total loss = 2.28, predict loss = 0.65 (69.7 examples/sec; 0.057 sec/batch; 1h:29m:01s remains)
INFO - root - 2019-11-06 19:16:10.291742: step 56970, total loss = 2.30, predict loss = 0.67 (86.8 examples/sec; 0.046 sec/batch; 1h:11m:27s remains)
INFO - root - 2019-11-06 19:16:10.758667: step 56980, total loss = 2.47, predict loss = 0.70 (94.3 examples/sec; 0.042 sec/batch; 1h:05m:45s remains)
INFO - root - 2019-11-06 19:16:11.913699: step 56990, total loss = 2.20, predict loss = 0.61 (67.4 examples/sec; 0.059 sec/batch; 1h:32m:03s remains)
INFO - root - 2019-11-06 19:16:12.626277: step 57000, total loss = 3.26, predict loss = 1.00 (64.4 examples/sec; 0.062 sec/batch; 1h:36m:18s remains)
INFO - root - 2019-11-06 19:16:13.376459: step 57010, total loss = 2.45, predict loss = 0.69 (58.3 examples/sec; 0.069 sec/batch; 1h:46m:16s remains)
INFO - root - 2019-11-06 19:16:14.147305: step 57020, total loss = 1.38, predict loss = 0.36 (61.6 examples/sec; 0.065 sec/batch; 1h:40m:37s remains)
INFO - root - 2019-11-06 19:16:14.977641: step 57030, total loss = 2.19, predict loss = 0.61 (58.0 examples/sec; 0.069 sec/batch; 1h:46m:54s remains)
INFO - root - 2019-11-06 19:16:15.574360: step 57040, total loss = 2.29, predict loss = 0.60 (102.4 examples/sec; 0.039 sec/batch; 1h:00m:30s remains)
INFO - root - 2019-11-06 19:16:16.035749: step 57050, total loss = 2.37, predict loss = 0.72 (88.5 examples/sec; 0.045 sec/batch; 1h:10m:01s remains)
INFO - root - 2019-11-06 19:16:16.512869: step 57060, total loss = 1.59, predict loss = 0.35 (100.2 examples/sec; 0.040 sec/batch; 1h:01m:51s remains)
INFO - root - 2019-11-06 19:16:17.748210: step 57070, total loss = 1.54, predict loss = 0.38 (67.3 examples/sec; 0.059 sec/batch; 1h:32m:06s remains)
INFO - root - 2019-11-06 19:16:18.523840: step 57080, total loss = 2.05, predict loss = 0.54 (49.9 examples/sec; 0.080 sec/batch; 2h:04m:11s remains)
INFO - root - 2019-11-06 19:16:19.231002: step 57090, total loss = 1.57, predict loss = 0.40 (62.9 examples/sec; 0.064 sec/batch; 1h:38m:31s remains)
INFO - root - 2019-11-06 19:16:20.049202: step 57100, total loss = 2.41, predict loss = 0.63 (58.8 examples/sec; 0.068 sec/batch; 1h:45m:19s remains)
INFO - root - 2019-11-06 19:16:20.745196: step 57110, total loss = 2.38, predict loss = 0.74 (64.1 examples/sec; 0.062 sec/batch; 1h:36m:37s remains)
INFO - root - 2019-11-06 19:16:21.234751: step 57120, total loss = 3.80, predict loss = 1.35 (91.0 examples/sec; 0.044 sec/batch; 1h:08m:03s remains)
INFO - root - 2019-11-06 19:16:21.702120: step 57130, total loss = 1.67, predict loss = 0.47 (94.2 examples/sec; 0.042 sec/batch; 1h:05m:42s remains)
INFO - root - 2019-11-06 19:16:22.918667: step 57140, total loss = 1.88, predict loss = 0.49 (66.0 examples/sec; 0.061 sec/batch; 1h:33m:50s remains)
INFO - root - 2019-11-06 19:16:23.638298: step 57150, total loss = 1.46, predict loss = 0.41 (61.4 examples/sec; 0.065 sec/batch; 1h:40m:44s remains)
INFO - root - 2019-11-06 19:16:24.408966: step 57160, total loss = 1.66, predict loss = 0.49 (52.2 examples/sec; 0.077 sec/batch; 1h:58m:40s remains)
INFO - root - 2019-11-06 19:16:25.185740: step 57170, total loss = 2.97, predict loss = 0.96 (56.6 examples/sec; 0.071 sec/batch; 1h:49m:22s remains)
INFO - root - 2019-11-06 19:16:25.935290: step 57180, total loss = 1.00, predict loss = 0.26 (56.8 examples/sec; 0.070 sec/batch; 1h:48m:54s remains)
INFO - root - 2019-11-06 19:16:26.566940: step 57190, total loss = 1.61, predict loss = 0.43 (98.8 examples/sec; 0.040 sec/batch; 1h:02m:37s remains)
INFO - root - 2019-11-06 19:16:27.032584: step 57200, total loss = 1.90, predict loss = 0.49 (93.0 examples/sec; 0.043 sec/batch; 1h:06m:31s remains)
INFO - root - 2019-11-06 19:16:27.497657: step 57210, total loss = 2.81, predict loss = 0.90 (88.9 examples/sec; 0.045 sec/batch; 1h:09m:35s remains)
INFO - root - 2019-11-06 19:16:28.837545: step 57220, total loss = 1.46, predict loss = 0.41 (62.8 examples/sec; 0.064 sec/batch; 1h:38m:26s remains)
INFO - root - 2019-11-06 19:16:29.577935: step 57230, total loss = 2.37, predict loss = 0.67 (59.5 examples/sec; 0.067 sec/batch; 1h:43m:54s remains)
INFO - root - 2019-11-06 19:16:30.343877: step 57240, total loss = 1.77, predict loss = 0.45 (54.9 examples/sec; 0.073 sec/batch; 1h:52m:37s remains)
INFO - root - 2019-11-06 19:16:31.095655: step 57250, total loss = 1.00, predict loss = 0.24 (60.3 examples/sec; 0.066 sec/batch; 1h:42m:28s remains)
INFO - root - 2019-11-06 19:16:31.798193: step 57260, total loss = 2.15, predict loss = 0.60 (68.3 examples/sec; 0.059 sec/batch; 1h:30m:31s remains)
INFO - root - 2019-11-06 19:16:32.280456: step 57270, total loss = 1.13, predict loss = 0.29 (99.9 examples/sec; 0.040 sec/batch; 1h:01m:52s remains)
INFO - root - 2019-11-06 19:16:32.741465: step 57280, total loss = 1.61, predict loss = 0.41 (99.6 examples/sec; 0.040 sec/batch; 1h:02m:04s remains)
INFO - root - 2019-11-06 19:16:33.935291: step 57290, total loss = 1.53, predict loss = 0.39 (69.4 examples/sec; 0.058 sec/batch; 1h:29m:02s remains)
INFO - root - 2019-11-06 19:16:34.701381: step 57300, total loss = 2.54, predict loss = 0.70 (53.1 examples/sec; 0.075 sec/batch; 1h:56m:21s remains)
INFO - root - 2019-11-06 19:16:35.529837: step 57310, total loss = 0.96, predict loss = 0.25 (52.1 examples/sec; 0.077 sec/batch; 1h:58m:30s remains)
INFO - root - 2019-11-06 19:16:36.343745: step 57320, total loss = 1.08, predict loss = 0.32 (59.6 examples/sec; 0.067 sec/batch; 1h:43m:39s remains)
INFO - root - 2019-11-06 19:16:37.156633: step 57330, total loss = 2.08, predict loss = 0.52 (59.2 examples/sec; 0.068 sec/batch; 1h:44m:25s remains)
INFO - root - 2019-11-06 19:16:37.715579: step 57340, total loss = 1.55, predict loss = 0.39 (101.2 examples/sec; 0.040 sec/batch; 1h:01m:04s remains)
INFO - root - 2019-11-06 19:16:38.167473: step 57350, total loss = 2.22, predict loss = 0.62 (92.1 examples/sec; 0.043 sec/batch; 1h:07m:02s remains)
INFO - root - 2019-11-06 19:16:38.601989: step 57360, total loss = 2.64, predict loss = 0.80 (130.0 examples/sec; 0.031 sec/batch; 0h:47m:29s remains)
INFO - root - 2019-11-06 19:16:39.985557: step 57370, total loss = 2.53, predict loss = 0.71 (58.4 examples/sec; 0.068 sec/batch; 1h:45m:42s remains)
INFO - root - 2019-11-06 19:16:40.788008: step 57380, total loss = 1.42, predict loss = 0.38 (58.0 examples/sec; 0.069 sec/batch; 1h:46m:31s remains)
INFO - root - 2019-11-06 19:16:41.558681: step 57390, total loss = 2.39, predict loss = 0.68 (56.0 examples/sec; 0.071 sec/batch; 1h:50m:11s remains)
INFO - root - 2019-11-06 19:16:42.337084: step 57400, total loss = 2.89, predict loss = 0.93 (60.7 examples/sec; 0.066 sec/batch; 1h:41m:46s remains)
INFO - root - 2019-11-06 19:16:43.023543: step 57410, total loss = 1.87, predict loss = 0.46 (74.3 examples/sec; 0.054 sec/batch; 1h:23m:02s remains)
INFO - root - 2019-11-06 19:16:43.525025: step 57420, total loss = 2.51, predict loss = 0.76 (99.1 examples/sec; 0.040 sec/batch; 1h:02m:17s remains)
INFO - root - 2019-11-06 19:16:43.989445: step 57430, total loss = 1.36, predict loss = 0.36 (100.4 examples/sec; 0.040 sec/batch; 1h:01m:28s remains)
INFO - root - 2019-11-06 19:16:45.211830: step 57440, total loss = 2.59, predict loss = 0.81 (63.5 examples/sec; 0.063 sec/batch; 1h:37m:13s remains)
INFO - root - 2019-11-06 19:16:45.977377: step 57450, total loss = 2.22, predict loss = 0.56 (61.9 examples/sec; 0.065 sec/batch; 1h:39m:38s remains)
INFO - root - 2019-11-06 19:16:46.722322: step 57460, total loss = 1.85, predict loss = 0.48 (61.0 examples/sec; 0.066 sec/batch; 1h:41m:05s remains)
INFO - root - 2019-11-06 19:16:47.501343: step 57470, total loss = 2.11, predict loss = 0.52 (64.5 examples/sec; 0.062 sec/batch; 1h:35m:37s remains)
INFO - root - 2019-11-06 19:16:48.234438: step 57480, total loss = 0.82, predict loss = 0.25 (73.3 examples/sec; 0.055 sec/batch; 1h:24m:09s remains)
INFO - root - 2019-11-06 19:16:48.800014: step 57490, total loss = 1.61, predict loss = 0.50 (96.4 examples/sec; 0.041 sec/batch; 1h:03m:57s remains)
INFO - root - 2019-11-06 19:16:49.276072: step 57500, total loss = 2.66, predict loss = 0.76 (96.3 examples/sec; 0.042 sec/batch; 1h:04m:03s remains)
INFO - root - 2019-11-06 19:16:50.418523: step 57510, total loss = 1.24, predict loss = 0.33 (5.4 examples/sec; 0.738 sec/batch; 18h:57m:43s remains)
INFO - root - 2019-11-06 19:16:51.088672: step 57520, total loss = 1.32, predict loss = 0.38 (58.0 examples/sec; 0.069 sec/batch; 1h:46m:13s remains)
INFO - root - 2019-11-06 19:16:51.915455: step 57530, total loss = 2.17, predict loss = 0.57 (53.5 examples/sec; 0.075 sec/batch; 1h:55m:08s remains)
INFO - root - 2019-11-06 19:16:52.669291: step 57540, total loss = 3.20, predict loss = 0.96 (57.1 examples/sec; 0.070 sec/batch; 1h:47m:56s remains)
INFO - root - 2019-11-06 19:16:53.394282: step 57550, total loss = 1.78, predict loss = 0.45 (54.5 examples/sec; 0.073 sec/batch; 1h:53m:04s remains)
INFO - root - 2019-11-06 19:16:54.079627: step 57560, total loss = 2.93, predict loss = 0.87 (87.6 examples/sec; 0.046 sec/batch; 1h:10m:19s remains)
INFO - root - 2019-11-06 19:16:54.532546: step 57570, total loss = 1.72, predict loss = 0.47 (89.9 examples/sec; 0.045 sec/batch; 1h:08m:34s remains)
INFO - root - 2019-11-06 19:16:55.014583: step 57580, total loss = 2.02, predict loss = 0.52 (94.1 examples/sec; 0.043 sec/batch; 1h:05m:29s remains)
INFO - root - 2019-11-06 19:16:56.226216: step 57590, total loss = 2.09, predict loss = 0.53 (61.7 examples/sec; 0.065 sec/batch; 1h:39m:46s remains)
INFO - root - 2019-11-06 19:16:57.020897: step 57600, total loss = 2.91, predict loss = 0.89 (58.2 examples/sec; 0.069 sec/batch; 1h:45m:45s remains)
INFO - root - 2019-11-06 19:16:57.902325: step 57610, total loss = 2.08, predict loss = 0.55 (54.7 examples/sec; 0.073 sec/batch; 1h:52m:36s remains)
INFO - root - 2019-11-06 19:16:58.745634: step 57620, total loss = 1.77, predict loss = 0.47 (46.5 examples/sec; 0.086 sec/batch; 2h:12m:32s remains)
INFO - root - 2019-11-06 19:16:59.512571: step 57630, total loss = 2.78, predict loss = 0.89 (70.1 examples/sec; 0.057 sec/batch; 1h:27m:47s remains)
INFO - root - 2019-11-06 19:17:00.041206: step 57640, total loss = 2.27, predict loss = 0.63 (95.4 examples/sec; 0.042 sec/batch; 1h:04m:30s remains)
INFO - root - 2019-11-06 19:17:00.496997: step 57650, total loss = 1.46, predict loss = 0.35 (97.7 examples/sec; 0.041 sec/batch; 1h:03m:00s remains)
INFO - root - 2019-11-06 19:17:01.695284: step 57660, total loss = 1.02, predict loss = 0.30 (64.5 examples/sec; 0.062 sec/batch; 1h:35m:24s remains)
INFO - root - 2019-11-06 19:17:02.443107: step 57670, total loss = 1.96, predict loss = 0.54 (51.8 examples/sec; 0.077 sec/batch; 1h:58m:53s remains)
INFO - root - 2019-11-06 19:17:03.197154: step 57680, total loss = 2.65, predict loss = 0.73 (60.8 examples/sec; 0.066 sec/batch; 1h:41m:18s remains)
INFO - root - 2019-11-06 19:17:03.942038: step 57690, total loss = 1.90, predict loss = 0.57 (59.0 examples/sec; 0.068 sec/batch; 1h:44m:13s remains)
INFO - root - 2019-11-06 19:17:04.759026: step 57700, total loss = 1.20, predict loss = 0.29 (55.0 examples/sec; 0.073 sec/batch; 1h:51m:53s remains)
INFO - root - 2019-11-06 19:17:05.403071: step 57710, total loss = 2.29, predict loss = 0.62 (93.0 examples/sec; 0.043 sec/batch; 1h:06m:08s remains)
INFO - root - 2019-11-06 19:17:05.847307: step 57720, total loss = 1.71, predict loss = 0.53 (97.3 examples/sec; 0.041 sec/batch; 1h:03m:15s remains)
INFO - root - 2019-11-06 19:17:06.302817: step 57730, total loss = 2.36, predict loss = 0.61 (87.4 examples/sec; 0.046 sec/batch; 1h:10m:24s remains)
INFO - root - 2019-11-06 19:17:07.626250: step 57740, total loss = 2.68, predict loss = 0.87 (53.7 examples/sec; 0.075 sec/batch; 1h:54m:34s remains)
INFO - root - 2019-11-06 19:17:08.327218: step 57750, total loss = 2.61, predict loss = 0.84 (71.8 examples/sec; 0.056 sec/batch; 1h:25m:35s remains)
INFO - root - 2019-11-06 19:17:09.032396: step 57760, total loss = 1.26, predict loss = 0.31 (67.8 examples/sec; 0.059 sec/batch; 1h:30m:39s remains)
INFO - root - 2019-11-06 19:17:09.767876: step 57770, total loss = 1.19, predict loss = 0.32 (59.9 examples/sec; 0.067 sec/batch; 1h:42m:37s remains)
INFO - root - 2019-11-06 19:17:10.501556: step 57780, total loss = 1.14, predict loss = 0.29 (59.8 examples/sec; 0.067 sec/batch; 1h:42m:53s remains)
INFO - root - 2019-11-06 19:17:11.008663: step 57790, total loss = 1.28, predict loss = 0.33 (93.3 examples/sec; 0.043 sec/batch; 1h:05m:53s remains)
INFO - root - 2019-11-06 19:17:11.470182: step 57800, total loss = 1.97, predict loss = 0.50 (89.5 examples/sec; 0.045 sec/batch; 1h:08m:41s remains)
INFO - root - 2019-11-06 19:17:12.592038: step 57810, total loss = 2.71, predict loss = 0.78 (65.9 examples/sec; 0.061 sec/batch; 1h:33m:14s remains)
INFO - root - 2019-11-06 19:17:13.323015: step 57820, total loss = 3.10, predict loss = 0.93 (56.5 examples/sec; 0.071 sec/batch; 1h:48m:50s remains)
INFO - root - 2019-11-06 19:17:14.137916: step 57830, total loss = 1.44, predict loss = 0.34 (50.9 examples/sec; 0.079 sec/batch; 2h:00m:41s remains)
INFO - root - 2019-11-06 19:17:14.903620: step 57840, total loss = 1.34, predict loss = 0.33 (53.7 examples/sec; 0.075 sec/batch; 1h:54m:30s remains)
INFO - root - 2019-11-06 19:17:15.609141: step 57850, total loss = 2.15, predict loss = 0.56 (65.8 examples/sec; 0.061 sec/batch; 1h:33m:18s remains)
INFO - root - 2019-11-06 19:17:16.209984: step 57860, total loss = 1.72, predict loss = 0.43 (99.6 examples/sec; 0.040 sec/batch; 1h:01m:38s remains)
INFO - root - 2019-11-06 19:17:16.654054: step 57870, total loss = 1.24, predict loss = 0.31 (96.6 examples/sec; 0.041 sec/batch; 1h:03m:36s remains)
INFO - root - 2019-11-06 19:17:17.115460: step 57880, total loss = 2.64, predict loss = 0.83 (90.9 examples/sec; 0.044 sec/batch; 1h:07m:32s remains)
INFO - root - 2019-11-06 19:17:18.372729: step 57890, total loss = 1.91, predict loss = 0.48 (58.9 examples/sec; 0.068 sec/batch; 1h:44m:10s remains)
INFO - root - 2019-11-06 19:17:19.122054: step 57900, total loss = 2.32, predict loss = 0.60 (60.5 examples/sec; 0.066 sec/batch; 1h:41m:29s remains)
INFO - root - 2019-11-06 19:17:19.813342: step 57910, total loss = 2.17, predict loss = 0.57 (63.9 examples/sec; 0.063 sec/batch; 1h:36m:08s remains)
INFO - root - 2019-11-06 19:17:20.516232: step 57920, total loss = 2.01, predict loss = 0.59 (63.6 examples/sec; 0.063 sec/batch; 1h:36m:27s remains)
INFO - root - 2019-11-06 19:17:21.215641: step 57930, total loss = 1.21, predict loss = 0.30 (70.3 examples/sec; 0.057 sec/batch; 1h:27m:20s remains)
INFO - root - 2019-11-06 19:17:21.725109: step 57940, total loss = 1.72, predict loss = 0.44 (99.7 examples/sec; 0.040 sec/batch; 1h:01m:33s remains)
INFO - root - 2019-11-06 19:17:22.187528: step 57950, total loss = 0.90, predict loss = 0.26 (97.6 examples/sec; 0.041 sec/batch; 1h:02m:51s remains)
INFO - root - 2019-11-06 19:17:23.427779: step 57960, total loss = 2.13, predict loss = 0.61 (72.0 examples/sec; 0.056 sec/batch; 1h:25m:10s remains)
INFO - root - 2019-11-06 19:17:24.159574: step 57970, total loss = 2.58, predict loss = 0.77 (60.1 examples/sec; 0.067 sec/batch; 1h:42m:09s remains)
INFO - root - 2019-11-06 19:17:24.892900: step 57980, total loss = 1.60, predict loss = 0.41 (60.6 examples/sec; 0.066 sec/batch; 1h:41m:13s remains)
INFO - root - 2019-11-06 19:17:25.594727: step 57990, total loss = 1.63, predict loss = 0.38 (65.7 examples/sec; 0.061 sec/batch; 1h:33m:18s remains)
INFO - root - 2019-11-06 19:17:26.364958: step 58000, total loss = 1.43, predict loss = 0.36 (54.0 examples/sec; 0.074 sec/batch; 1h:53m:33s remains)
INFO - root - 2019-11-06 19:17:26.970055: step 58010, total loss = 2.39, predict loss = 0.67 (95.4 examples/sec; 0.042 sec/batch; 1h:04m:18s remains)
INFO - root - 2019-11-06 19:17:27.452118: step 58020, total loss = 2.33, predict loss = 0.73 (96.4 examples/sec; 0.041 sec/batch; 1h:03m:35s remains)
INFO - root - 2019-11-06 19:17:27.923217: step 58030, total loss = 2.12, predict loss = 0.60 (96.3 examples/sec; 0.042 sec/batch; 1h:03m:40s remains)
INFO - root - 2019-11-06 19:17:29.276415: step 58040, total loss = 1.20, predict loss = 0.28 (57.6 examples/sec; 0.070 sec/batch; 1h:46m:31s remains)
INFO - root - 2019-11-06 19:17:30.023004: step 58050, total loss = 0.88, predict loss = 0.26 (55.9 examples/sec; 0.072 sec/batch; 1h:49m:37s remains)
INFO - root - 2019-11-06 19:17:30.855349: step 58060, total loss = 2.65, predict loss = 0.79 (55.7 examples/sec; 0.072 sec/batch; 1h:50m:07s remains)
INFO - root - 2019-11-06 19:17:31.622023: step 58070, total loss = 1.67, predict loss = 0.44 (61.2 examples/sec; 0.065 sec/batch; 1h:40m:11s remains)
INFO - root - 2019-11-06 19:17:32.295973: step 58080, total loss = 1.10, predict loss = 0.27 (82.0 examples/sec; 0.049 sec/batch; 1h:14m:41s remains)
INFO - root - 2019-11-06 19:17:32.761924: step 58090, total loss = 1.13, predict loss = 0.27 (105.7 examples/sec; 0.038 sec/batch; 0h:57m:58s remains)
INFO - root - 2019-11-06 19:17:33.249681: step 58100, total loss = 1.82, predict loss = 0.52 (96.8 examples/sec; 0.041 sec/batch; 1h:03m:16s remains)
INFO - root - 2019-11-06 19:17:34.461522: step 58110, total loss = 2.24, predict loss = 0.59 (66.7 examples/sec; 0.060 sec/batch; 1h:31m:50s remains)
INFO - root - 2019-11-06 19:17:35.226362: step 58120, total loss = 1.54, predict loss = 0.48 (56.5 examples/sec; 0.071 sec/batch; 1h:48m:29s remains)
INFO - root - 2019-11-06 19:17:36.000783: step 58130, total loss = 2.24, predict loss = 0.65 (60.0 examples/sec; 0.067 sec/batch; 1h:42m:08s remains)
INFO - root - 2019-11-06 19:17:36.743346: step 58140, total loss = 2.03, predict loss = 0.54 (64.0 examples/sec; 0.062 sec/batch; 1h:35m:39s remains)
INFO - root - 2019-11-06 19:17:37.466784: step 58150, total loss = 0.99, predict loss = 0.28 (58.1 examples/sec; 0.069 sec/batch; 1h:45m:28s remains)
INFO - root - 2019-11-06 19:17:38.014302: step 58160, total loss = 2.11, predict loss = 0.59 (94.5 examples/sec; 0.042 sec/batch; 1h:04m:45s remains)
INFO - root - 2019-11-06 19:17:38.478965: step 58170, total loss = 1.55, predict loss = 0.42 (98.0 examples/sec; 0.041 sec/batch; 1h:02m:29s remains)
INFO - root - 2019-11-06 19:17:38.953259: step 58180, total loss = 2.88, predict loss = 0.92 (132.6 examples/sec; 0.030 sec/batch; 0h:46m:10s remains)
INFO - root - 2019-11-06 19:17:40.310295: step 58190, total loss = 2.31, predict loss = 0.65 (60.1 examples/sec; 0.067 sec/batch; 1h:41m:45s remains)
INFO - root - 2019-11-06 19:17:40.999493: step 58200, total loss = 1.35, predict loss = 0.33 (72.2 examples/sec; 0.055 sec/batch; 1h:24m:48s remains)
INFO - root - 2019-11-06 19:17:41.678899: step 58210, total loss = 2.40, predict loss = 0.64 (65.1 examples/sec; 0.061 sec/batch; 1h:34m:03s remains)
INFO - root - 2019-11-06 19:17:42.372834: step 58220, total loss = 2.61, predict loss = 0.74 (63.6 examples/sec; 0.063 sec/batch; 1h:36m:11s remains)
INFO - root - 2019-11-06 19:17:43.040857: step 58230, total loss = 1.48, predict loss = 0.38 (66.0 examples/sec; 0.061 sec/batch; 1h:32m:43s remains)
INFO - root - 2019-11-06 19:17:43.503242: step 58240, total loss = 2.01, predict loss = 0.52 (96.8 examples/sec; 0.041 sec/batch; 1h:03m:13s remains)
INFO - root - 2019-11-06 19:17:43.966025: step 58250, total loss = 2.62, predict loss = 0.79 (92.4 examples/sec; 0.043 sec/batch; 1h:06m:11s remains)
INFO - root - 2019-11-06 19:17:45.249945: step 58260, total loss = 2.11, predict loss = 0.61 (61.0 examples/sec; 0.066 sec/batch; 1h:40m:18s remains)
INFO - root - 2019-11-06 19:17:45.984545: step 58270, total loss = 2.71, predict loss = 0.87 (63.5 examples/sec; 0.063 sec/batch; 1h:36m:13s remains)
INFO - root - 2019-11-06 19:17:46.735436: step 58280, total loss = 2.94, predict loss = 0.87 (58.7 examples/sec; 0.068 sec/batch; 1h:44m:10s remains)
INFO - root - 2019-11-06 19:17:47.483755: step 58290, total loss = 2.92, predict loss = 0.88 (62.1 examples/sec; 0.064 sec/batch; 1h:38m:29s remains)
INFO - root - 2019-11-06 19:17:48.272799: step 58300, total loss = 2.40, predict loss = 0.70 (71.1 examples/sec; 0.056 sec/batch; 1h:26m:00s remains)
INFO - root - 2019-11-06 19:17:48.861712: step 58310, total loss = 2.94, predict loss = 0.92 (101.3 examples/sec; 0.039 sec/batch; 1h:00m:20s remains)
INFO - root - 2019-11-06 19:17:49.314989: step 58320, total loss = 2.33, predict loss = 0.63 (101.1 examples/sec; 0.040 sec/batch; 1h:00m:28s remains)
INFO - root - 2019-11-06 19:17:50.457194: step 58330, total loss = 2.48, predict loss = 0.71 (5.3 examples/sec; 0.752 sec/batch; 19h:09m:09s remains)
INFO - root - 2019-11-06 19:17:51.168816: step 58340, total loss = 1.95, predict loss = 0.47 (63.3 examples/sec; 0.063 sec/batch; 1h:36m:33s remains)
INFO - root - 2019-11-06 19:17:51.951898: step 58350, total loss = 2.68, predict loss = 0.80 (52.5 examples/sec; 0.076 sec/batch; 1h:56m:29s remains)
INFO - root - 2019-11-06 19:17:52.761229: step 58360, total loss = 1.61, predict loss = 0.41 (57.2 examples/sec; 0.070 sec/batch; 1h:46m:51s remains)
INFO - root - 2019-11-06 19:17:53.525159: step 58370, total loss = 1.49, predict loss = 0.38 (58.4 examples/sec; 0.068 sec/batch; 1h:44m:36s remains)
INFO - root - 2019-11-06 19:17:54.199053: step 58380, total loss = 2.30, predict loss = 0.69 (87.6 examples/sec; 0.046 sec/batch; 1h:09m:44s remains)
INFO - root - 2019-11-06 19:17:54.653760: step 58390, total loss = 3.25, predict loss = 1.07 (99.4 examples/sec; 0.040 sec/batch; 1h:01m:27s remains)
INFO - root - 2019-11-06 19:17:55.110114: step 58400, total loss = 1.87, predict loss = 0.51 (96.4 examples/sec; 0.042 sec/batch; 1h:03m:21s remains)
INFO - root - 2019-11-06 19:17:56.331302: step 58410, total loss = 1.48, predict loss = 0.37 (61.8 examples/sec; 0.065 sec/batch; 1h:38m:44s remains)
INFO - root - 2019-11-06 19:17:57.091066: step 58420, total loss = 2.17, predict loss = 0.59 (57.5 examples/sec; 0.070 sec/batch; 1h:46m:08s remains)
INFO - root - 2019-11-06 19:17:57.865119: step 58430, total loss = 1.47, predict loss = 0.42 (57.0 examples/sec; 0.070 sec/batch; 1h:47m:07s remains)
INFO - root - 2019-11-06 19:17:58.639382: step 58440, total loss = 1.68, predict loss = 0.45 (59.8 examples/sec; 0.067 sec/batch; 1h:42m:03s remains)
INFO - root - 2019-11-06 19:17:59.432724: step 58450, total loss = 1.79, predict loss = 0.55 (66.4 examples/sec; 0.060 sec/batch; 1h:31m:53s remains)
INFO - root - 2019-11-06 19:17:59.995712: step 58460, total loss = 2.24, predict loss = 0.60 (98.3 examples/sec; 0.041 sec/batch; 1h:02m:05s remains)
INFO - root - 2019-11-06 19:18:00.468958: step 58470, total loss = 1.98, predict loss = 0.52 (94.8 examples/sec; 0.042 sec/batch; 1h:04m:20s remains)
INFO - root - 2019-11-06 19:18:01.621862: step 58480, total loss = 2.99, predict loss = 0.89 (65.0 examples/sec; 0.061 sec/batch; 1h:33m:48s remains)
INFO - root - 2019-11-06 19:18:02.292420: step 58490, total loss = 1.79, predict loss = 0.46 (60.3 examples/sec; 0.066 sec/batch; 1h:41m:12s remains)
INFO - root - 2019-11-06 19:18:03.027970: step 58500, total loss = 1.13, predict loss = 0.31 (62.9 examples/sec; 0.064 sec/batch; 1h:36m:58s remains)
INFO - root - 2019-11-06 19:18:03.805821: step 58510, total loss = 2.58, predict loss = 0.74 (58.6 examples/sec; 0.068 sec/batch; 1h:44m:06s remains)
INFO - root - 2019-11-06 19:18:04.504447: step 58520, total loss = 1.88, predict loss = 0.50 (68.9 examples/sec; 0.058 sec/batch; 1h:28m:27s remains)
INFO - root - 2019-11-06 19:18:05.113537: step 58530, total loss = 2.06, predict loss = 0.60 (97.1 examples/sec; 0.041 sec/batch; 1h:02m:47s remains)
INFO - root - 2019-11-06 19:18:05.598915: step 58540, total loss = 1.66, predict loss = 0.44 (87.6 examples/sec; 0.046 sec/batch; 1h:09m:35s remains)
INFO - root - 2019-11-06 19:18:06.048163: step 58550, total loss = 1.87, predict loss = 0.55 (96.4 examples/sec; 0.042 sec/batch; 1h:03m:15s remains)
INFO - root - 2019-11-06 19:18:07.298481: step 58560, total loss = 1.73, predict loss = 0.50 (60.6 examples/sec; 0.066 sec/batch; 1h:40m:39s remains)
INFO - root - 2019-11-06 19:18:08.048258: step 58570, total loss = 1.60, predict loss = 0.40 (54.3 examples/sec; 0.074 sec/batch; 1h:52m:17s remains)
INFO - root - 2019-11-06 19:18:08.827873: step 58580, total loss = 1.69, predict loss = 0.45 (59.7 examples/sec; 0.067 sec/batch; 1h:42m:03s remains)
INFO - root - 2019-11-06 19:18:09.622750: step 58590, total loss = 2.03, predict loss = 0.60 (60.7 examples/sec; 0.066 sec/batch; 1h:40m:20s remains)
INFO - root - 2019-11-06 19:18:10.361903: step 58600, total loss = 1.31, predict loss = 0.33 (65.7 examples/sec; 0.061 sec/batch; 1h:32m:43s remains)
INFO - root - 2019-11-06 19:18:10.899159: step 58610, total loss = 1.68, predict loss = 0.49 (95.7 examples/sec; 0.042 sec/batch; 1h:03m:41s remains)
INFO - root - 2019-11-06 19:18:11.381638: step 58620, total loss = 2.38, predict loss = 0.67 (99.1 examples/sec; 0.040 sec/batch; 1h:01m:27s remains)
INFO - root - 2019-11-06 19:18:12.548912: step 58630, total loss = 1.66, predict loss = 0.41 (68.9 examples/sec; 0.058 sec/batch; 1h:28m:27s remains)
INFO - root - 2019-11-06 19:18:13.228475: step 58640, total loss = 1.83, predict loss = 0.45 (55.2 examples/sec; 0.073 sec/batch; 1h:50m:24s remains)
INFO - root - 2019-11-06 19:18:14.001796: step 58650, total loss = 0.99, predict loss = 0.25 (59.8 examples/sec; 0.067 sec/batch; 1h:41m:53s remains)
INFO - root - 2019-11-06 19:18:14.760844: step 58660, total loss = 2.59, predict loss = 0.78 (60.2 examples/sec; 0.066 sec/batch; 1h:41m:05s remains)
INFO - root - 2019-11-06 19:18:15.527052: step 58670, total loss = 2.36, predict loss = 0.70 (56.9 examples/sec; 0.070 sec/batch; 1h:46m:56s remains)
INFO - root - 2019-11-06 19:18:16.199532: step 58680, total loss = 2.57, predict loss = 0.85 (101.5 examples/sec; 0.039 sec/batch; 0h:59m:57s remains)
INFO - root - 2019-11-06 19:18:16.637333: step 58690, total loss = 1.59, predict loss = 0.42 (99.2 examples/sec; 0.040 sec/batch; 1h:01m:20s remains)
INFO - root - 2019-11-06 19:18:17.122921: step 58700, total loss = 2.71, predict loss = 0.80 (85.4 examples/sec; 0.047 sec/batch; 1h:11m:14s remains)
INFO - root - 2019-11-06 19:18:18.447816: step 58710, total loss = 1.96, predict loss = 0.51 (54.7 examples/sec; 0.073 sec/batch; 1h:51m:11s remains)
INFO - root - 2019-11-06 19:18:19.239777: step 58720, total loss = 1.43, predict loss = 0.36 (58.4 examples/sec; 0.068 sec/batch; 1h:44m:06s remains)
INFO - root - 2019-11-06 19:18:19.998025: step 58730, total loss = 1.96, predict loss = 0.54 (54.5 examples/sec; 0.073 sec/batch; 1h:51m:37s remains)
INFO - root - 2019-11-06 19:18:20.744633: step 58740, total loss = 1.34, predict loss = 0.32 (56.1 examples/sec; 0.071 sec/batch; 1h:48m:23s remains)
INFO - root - 2019-11-06 19:18:21.463517: step 58750, total loss = 1.99, predict loss = 0.61 (75.6 examples/sec; 0.053 sec/batch; 1h:20m:28s remains)
INFO - root - 2019-11-06 19:18:21.946225: step 58760, total loss = 3.02, predict loss = 0.92 (94.7 examples/sec; 0.042 sec/batch; 1h:04m:14s remains)
INFO - root - 2019-11-06 19:18:22.388300: step 58770, total loss = 0.97, predict loss = 0.26 (96.7 examples/sec; 0.041 sec/batch; 1h:02m:51s remains)
INFO - root - 2019-11-06 19:18:23.612255: step 58780, total loss = 1.72, predict loss = 0.50 (64.0 examples/sec; 0.063 sec/batch; 1h:35m:02s remains)
INFO - root - 2019-11-06 19:18:24.365070: step 58790, total loss = 1.48, predict loss = 0.37 (53.9 examples/sec; 0.074 sec/batch; 1h:52m:43s remains)
INFO - root - 2019-11-06 19:18:25.061194: step 58800, total loss = 2.16, predict loss = 0.64 (59.3 examples/sec; 0.067 sec/batch; 1h:42m:32s remains)
INFO - root - 2019-11-06 19:18:25.874973: step 58810, total loss = 1.61, predict loss = 0.43 (50.6 examples/sec; 0.079 sec/batch; 2h:00m:03s remains)
INFO - root - 2019-11-06 19:18:26.634757: step 58820, total loss = 0.98, predict loss = 0.28 (65.1 examples/sec; 0.061 sec/batch; 1h:33m:21s remains)
INFO - root - 2019-11-06 19:18:27.217136: step 58830, total loss = 1.76, predict loss = 0.44 (93.1 examples/sec; 0.043 sec/batch; 1h:05m:15s remains)
INFO - root - 2019-11-06 19:18:27.687188: step 58840, total loss = 1.83, predict loss = 0.51 (99.1 examples/sec; 0.040 sec/batch; 1h:01m:19s remains)
INFO - root - 2019-11-06 19:18:28.138533: step 58850, total loss = 2.08, predict loss = 0.59 (89.3 examples/sec; 0.045 sec/batch; 1h:08m:01s remains)
INFO - root - 2019-11-06 19:18:29.487003: step 58860, total loss = 1.29, predict loss = 0.33 (58.6 examples/sec; 0.068 sec/batch; 1h:43m:40s remains)
INFO - root - 2019-11-06 19:18:30.218962: step 58870, total loss = 1.86, predict loss = 0.48 (57.8 examples/sec; 0.069 sec/batch; 1h:45m:10s remains)
INFO - root - 2019-11-06 19:18:30.958643: step 58880, total loss = 1.88, predict loss = 0.54 (57.2 examples/sec; 0.070 sec/batch; 1h:46m:13s remains)
INFO - root - 2019-11-06 19:18:31.747107: step 58890, total loss = 2.37, predict loss = 0.72 (63.7 examples/sec; 0.063 sec/batch; 1h:35m:23s remains)
INFO - root - 2019-11-06 19:18:32.428930: step 58900, total loss = 1.80, predict loss = 0.49 (69.2 examples/sec; 0.058 sec/batch; 1h:27m:45s remains)
INFO - root - 2019-11-06 19:18:32.915615: step 58910, total loss = 2.92, predict loss = 0.89 (91.5 examples/sec; 0.044 sec/batch; 1h:06m:21s remains)
INFO - root - 2019-11-06 19:18:33.379344: step 58920, total loss = 1.11, predict loss = 0.31 (101.7 examples/sec; 0.039 sec/batch; 0h:59m:41s remains)
INFO - root - 2019-11-06 19:18:34.618420: step 58930, total loss = 1.16, predict loss = 0.31 (64.8 examples/sec; 0.062 sec/batch; 1h:33m:39s remains)
INFO - root - 2019-11-06 19:18:35.344608: step 58940, total loss = 2.87, predict loss = 0.85 (70.9 examples/sec; 0.056 sec/batch; 1h:25m:39s remains)
INFO - root - 2019-11-06 19:18:36.084890: step 58950, total loss = 1.83, predict loss = 0.50 (56.7 examples/sec; 0.071 sec/batch; 1h:47m:06s remains)
INFO - root - 2019-11-06 19:18:36.808088: step 58960, total loss = 1.76, predict loss = 0.50 (66.3 examples/sec; 0.060 sec/batch; 1h:31m:30s remains)
INFO - root - 2019-11-06 19:18:37.545378: step 58970, total loss = 2.06, predict loss = 0.58 (58.1 examples/sec; 0.069 sec/batch; 1h:44m:29s remains)
INFO - root - 2019-11-06 19:18:38.127360: step 58980, total loss = 1.77, predict loss = 0.51 (84.9 examples/sec; 0.047 sec/batch; 1h:11m:29s remains)
INFO - root - 2019-11-06 19:18:38.593488: step 58990, total loss = 1.80, predict loss = 0.52 (91.7 examples/sec; 0.044 sec/batch; 1h:06m:10s remains)
INFO - root - 2019-11-06 19:18:39.047406: step 59000, total loss = 1.24, predict loss = 0.32 (125.8 examples/sec; 0.032 sec/batch; 0h:48m:12s remains)
INFO - root - 2019-11-06 19:18:40.431867: step 59010, total loss = 1.80, predict loss = 0.45 (50.1 examples/sec; 0.080 sec/batch; 2h:01m:01s remains)
INFO - root - 2019-11-06 19:18:41.206037: step 59020, total loss = 2.68, predict loss = 0.72 (60.8 examples/sec; 0.066 sec/batch; 1h:39m:47s remains)
INFO - root - 2019-11-06 19:18:41.956933: step 59030, total loss = 2.44, predict loss = 0.69 (52.0 examples/sec; 0.077 sec/batch; 1h:56m:42s remains)
INFO - root - 2019-11-06 19:18:42.738579: step 59040, total loss = 1.48, predict loss = 0.38 (61.7 examples/sec; 0.065 sec/batch; 1h:38m:19s remains)
INFO - root - 2019-11-06 19:18:43.433649: step 59050, total loss = 2.13, predict loss = 0.56 (74.4 examples/sec; 0.054 sec/batch; 1h:21m:29s remains)
INFO - root - 2019-11-06 19:18:43.941821: step 59060, total loss = 1.29, predict loss = 0.33 (97.5 examples/sec; 0.041 sec/batch; 1h:02m:10s remains)
INFO - root - 2019-11-06 19:18:44.420048: step 59070, total loss = 2.77, predict loss = 0.81 (89.7 examples/sec; 0.045 sec/batch; 1h:07m:34s remains)
INFO - root - 2019-11-06 19:18:45.649534: step 59080, total loss = 1.61, predict loss = 0.42 (68.9 examples/sec; 0.058 sec/batch; 1h:27m:59s remains)
INFO - root - 2019-11-06 19:18:46.345984: step 59090, total loss = 1.92, predict loss = 0.48 (66.3 examples/sec; 0.060 sec/batch; 1h:31m:23s remains)
INFO - root - 2019-11-06 19:18:47.118461: step 59100, total loss = 1.00, predict loss = 0.27 (55.0 examples/sec; 0.073 sec/batch; 1h:50m:07s remains)
INFO - root - 2019-11-06 19:18:47.812739: step 59110, total loss = 2.03, predict loss = 0.53 (65.3 examples/sec; 0.061 sec/batch; 1h:32m:46s remains)
INFO - root - 2019-11-06 19:18:48.510021: step 59120, total loss = 1.53, predict loss = 0.41 (69.3 examples/sec; 0.058 sec/batch; 1h:27m:29s remains)
INFO - root - 2019-11-06 19:18:49.082590: step 59130, total loss = 1.35, predict loss = 0.35 (93.5 examples/sec; 0.043 sec/batch; 1h:04m:45s remains)
INFO - root - 2019-11-06 19:18:49.551481: step 59140, total loss = 1.51, predict loss = 0.45 (95.3 examples/sec; 0.042 sec/batch; 1h:03m:33s remains)
INFO - root - 2019-11-06 19:18:50.642544: step 59150, total loss = 1.73, predict loss = 0.44 (5.7 examples/sec; 0.705 sec/batch; 17h:48m:07s remains)
INFO - root - 2019-11-06 19:18:51.406188: step 59160, total loss = 1.85, predict loss = 0.59 (53.4 examples/sec; 0.075 sec/batch; 1h:53m:20s remains)
INFO - root - 2019-11-06 19:18:52.128937: step 59170, total loss = 1.42, predict loss = 0.36 (65.8 examples/sec; 0.061 sec/batch; 1h:32m:05s remains)
INFO - root - 2019-11-06 19:18:52.853750: step 59180, total loss = 1.30, predict loss = 0.36 (63.3 examples/sec; 0.063 sec/batch; 1h:35m:38s remains)
INFO - root - 2019-11-06 19:18:53.633276: step 59190, total loss = 2.52, predict loss = 0.71 (56.7 examples/sec; 0.071 sec/batch; 1h:46m:48s remains)
INFO - root - 2019-11-06 19:18:54.354297: step 59200, total loss = 2.38, predict loss = 0.67 (84.9 examples/sec; 0.047 sec/batch; 1h:11m:17s remains)
INFO - root - 2019-11-06 19:18:54.814208: step 59210, total loss = 2.66, predict loss = 0.82 (88.9 examples/sec; 0.045 sec/batch; 1h:08m:07s remains)
INFO - root - 2019-11-06 19:18:55.306704: step 59220, total loss = 1.82, predict loss = 0.51 (99.7 examples/sec; 0.040 sec/batch; 1h:00m:42s remains)
INFO - root - 2019-11-06 19:18:56.633048: step 59230, total loss = 1.06, predict loss = 0.26 (55.8 examples/sec; 0.072 sec/batch; 1h:48m:26s remains)
INFO - root - 2019-11-06 19:18:57.403400: step 59240, total loss = 1.83, predict loss = 0.52 (59.8 examples/sec; 0.067 sec/batch; 1h:41m:09s remains)
INFO - root - 2019-11-06 19:18:58.139102: step 59250, total loss = 1.52, predict loss = 0.42 (62.0 examples/sec; 0.065 sec/batch; 1h:37m:34s remains)
INFO - root - 2019-11-06 19:18:58.898441: step 59260, total loss = 2.56, predict loss = 0.74 (69.8 examples/sec; 0.057 sec/batch; 1h:26m:36s remains)
INFO - root - 2019-11-06 19:18:59.577572: step 59270, total loss = 1.65, predict loss = 0.47 (73.5 examples/sec; 0.054 sec/batch; 1h:22m:20s remains)
INFO - root - 2019-11-06 19:19:00.115970: step 59280, total loss = 2.07, predict loss = 0.58 (96.2 examples/sec; 0.042 sec/batch; 1h:02m:51s remains)
INFO - root - 2019-11-06 19:19:00.565564: step 59290, total loss = 2.41, predict loss = 0.69 (99.2 examples/sec; 0.040 sec/batch; 1h:00m:57s remains)
INFO - root - 2019-11-06 19:19:01.755320: step 59300, total loss = 2.61, predict loss = 0.73 (74.3 examples/sec; 0.054 sec/batch; 1h:21m:24s remains)
INFO - root - 2019-11-06 19:19:02.479587: step 59310, total loss = 2.55, predict loss = 0.79 (53.0 examples/sec; 0.075 sec/batch; 1h:54m:01s remains)
INFO - root - 2019-11-06 19:19:03.291600: step 59320, total loss = 2.03, predict loss = 0.57 (60.5 examples/sec; 0.066 sec/batch; 1h:39m:57s remains)
INFO - root - 2019-11-06 19:19:04.031336: step 59330, total loss = 1.75, predict loss = 0.48 (64.6 examples/sec; 0.062 sec/batch; 1h:33m:34s remains)
INFO - root - 2019-11-06 19:19:04.770510: step 59340, total loss = 2.23, predict loss = 0.59 (55.5 examples/sec; 0.072 sec/batch; 1h:48m:58s remains)
INFO - root - 2019-11-06 19:19:05.444831: step 59350, total loss = 1.95, predict loss = 0.54 (80.5 examples/sec; 0.050 sec/batch; 1h:15m:06s remains)
INFO - root - 2019-11-06 19:19:05.891818: step 59360, total loss = 1.48, predict loss = 0.39 (89.7 examples/sec; 0.045 sec/batch; 1h:07m:23s remains)
INFO - root - 2019-11-06 19:19:06.344437: step 59370, total loss = 1.78, predict loss = 0.47 (94.9 examples/sec; 0.042 sec/batch; 1h:03m:41s remains)
INFO - root - 2019-11-06 19:19:07.691584: step 59380, total loss = 2.52, predict loss = 0.73 (52.1 examples/sec; 0.077 sec/batch; 1h:55m:54s remains)
INFO - root - 2019-11-06 19:19:08.395075: step 59390, total loss = 1.65, predict loss = 0.52 (60.9 examples/sec; 0.066 sec/batch; 1h:39m:08s remains)
INFO - root - 2019-11-06 19:19:09.140434: step 59400, total loss = 1.55, predict loss = 0.39 (55.7 examples/sec; 0.072 sec/batch; 1h:48m:24s remains)
INFO - root - 2019-11-06 19:19:09.886042: step 59410, total loss = 2.32, predict loss = 0.65 (40.7 examples/sec; 0.098 sec/batch; 2h:28m:33s remains)
INFO - root - 2019-11-06 19:19:10.697089: step 59420, total loss = 2.80, predict loss = 0.94 (59.2 examples/sec; 0.068 sec/batch; 1h:41m:56s remains)
INFO - root - 2019-11-06 19:19:11.263668: step 59430, total loss = 1.72, predict loss = 0.44 (79.0 examples/sec; 0.051 sec/batch; 1h:16m:26s remains)
INFO - root - 2019-11-06 19:19:11.769078: step 59440, total loss = 2.02, predict loss = 0.59 (91.6 examples/sec; 0.044 sec/batch; 1h:05m:55s remains)
INFO - root - 2019-11-06 19:19:12.911898: step 59450, total loss = 1.94, predict loss = 0.50 (69.8 examples/sec; 0.057 sec/batch; 1h:26m:28s remains)
INFO - root - 2019-11-06 19:19:13.642906: step 59460, total loss = 1.55, predict loss = 0.45 (58.1 examples/sec; 0.069 sec/batch; 1h:43m:50s remains)
INFO - root - 2019-11-06 19:19:14.452946: step 59470, total loss = 2.33, predict loss = 0.62 (49.9 examples/sec; 0.080 sec/batch; 2h:00m:50s remains)
INFO - root - 2019-11-06 19:19:15.222963: step 59480, total loss = 2.20, predict loss = 0.66 (58.4 examples/sec; 0.068 sec/batch; 1h:43m:18s remains)
INFO - root - 2019-11-06 19:19:16.038570: step 59490, total loss = 2.88, predict loss = 0.81 (55.3 examples/sec; 0.072 sec/batch; 1h:49m:10s remains)
INFO - root - 2019-11-06 19:19:16.706200: step 59500, total loss = 2.75, predict loss = 0.85 (97.3 examples/sec; 0.041 sec/batch; 1h:01m:59s remains)
INFO - root - 2019-11-06 19:19:17.154957: step 59510, total loss = 1.75, predict loss = 0.48 (92.7 examples/sec; 0.043 sec/batch; 1h:05m:03s remains)
INFO - root - 2019-11-06 19:19:17.608996: step 59520, total loss = 0.93, predict loss = 0.24 (96.6 examples/sec; 0.041 sec/batch; 1h:02m:24s remains)
INFO - root - 2019-11-06 19:19:18.918192: step 59530, total loss = 2.38, predict loss = 0.69 (62.9 examples/sec; 0.064 sec/batch; 1h:35m:52s remains)
INFO - root - 2019-11-06 19:19:19.671178: step 59540, total loss = 2.25, predict loss = 0.62 (58.7 examples/sec; 0.068 sec/batch; 1h:42m:49s remains)
INFO - root - 2019-11-06 19:19:20.442488: step 59550, total loss = 2.20, predict loss = 0.60 (58.6 examples/sec; 0.068 sec/batch; 1h:42m:51s remains)
INFO - root - 2019-11-06 19:19:21.165470: step 59560, total loss = 2.23, predict loss = 0.61 (72.3 examples/sec; 0.055 sec/batch; 1h:23m:21s remains)
INFO - root - 2019-11-06 19:19:21.813178: step 59570, total loss = 2.67, predict loss = 0.76 (70.0 examples/sec; 0.057 sec/batch; 1h:26m:09s remains)
INFO - root - 2019-11-06 19:19:22.333547: step 59580, total loss = 2.36, predict loss = 0.64 (96.5 examples/sec; 0.041 sec/batch; 1h:02m:27s remains)
INFO - root - 2019-11-06 19:19:22.783946: step 59590, total loss = 1.75, predict loss = 0.46 (98.8 examples/sec; 0.040 sec/batch; 1h:00m:58s remains)
INFO - root - 2019-11-06 19:19:24.069341: step 59600, total loss = 2.86, predict loss = 0.90 (44.0 examples/sec; 0.091 sec/batch; 2h:16m:51s remains)
INFO - root - 2019-11-06 19:19:24.831460: step 59610, total loss = 2.36, predict loss = 0.65 (66.3 examples/sec; 0.060 sec/batch; 1h:30m:50s remains)
INFO - root - 2019-11-06 19:19:25.563790: step 59620, total loss = 2.03, predict loss = 0.54 (57.8 examples/sec; 0.069 sec/batch; 1h:44m:19s remains)
INFO - root - 2019-11-06 19:19:26.316359: step 59630, total loss = 1.87, predict loss = 0.46 (58.0 examples/sec; 0.069 sec/batch; 1h:43m:48s remains)
INFO - root - 2019-11-06 19:19:27.090298: step 59640, total loss = 2.49, predict loss = 0.68 (52.6 examples/sec; 0.076 sec/batch; 1h:54m:30s remains)
INFO - root - 2019-11-06 19:19:27.703939: step 59650, total loss = 0.90, predict loss = 0.23 (103.6 examples/sec; 0.039 sec/batch; 0h:58m:08s remains)
INFO - root - 2019-11-06 19:19:28.177953: step 59660, total loss = 1.40, predict loss = 0.37 (97.9 examples/sec; 0.041 sec/batch; 1h:01m:31s remains)
INFO - root - 2019-11-06 19:19:28.624866: step 59670, total loss = 1.94, predict loss = 0.53 (87.9 examples/sec; 0.046 sec/batch; 1h:08m:31s remains)
INFO - root - 2019-11-06 19:19:29.965107: step 59680, total loss = 1.38, predict loss = 0.33 (55.8 examples/sec; 0.072 sec/batch; 1h:47m:58s remains)
INFO - root - 2019-11-06 19:19:30.723536: step 59690, total loss = 1.66, predict loss = 0.40 (60.7 examples/sec; 0.066 sec/batch; 1h:39m:15s remains)
INFO - root - 2019-11-06 19:19:31.459801: step 59700, total loss = 1.75, predict loss = 0.47 (59.7 examples/sec; 0.067 sec/batch; 1h:40m:51s remains)
INFO - root - 2019-11-06 19:19:32.156764: step 59710, total loss = 1.34, predict loss = 0.36 (61.9 examples/sec; 0.065 sec/batch; 1h:37m:12s remains)
INFO - root - 2019-11-06 19:19:32.842610: step 59720, total loss = 1.68, predict loss = 0.45 (73.1 examples/sec; 0.055 sec/batch; 1h:22m:17s remains)
INFO - root - 2019-11-06 19:19:33.325039: step 59730, total loss = 2.01, predict loss = 0.55 (102.6 examples/sec; 0.039 sec/batch; 0h:58m:40s remains)
INFO - root - 2019-11-06 19:19:33.798142: step 59740, total loss = 1.99, predict loss = 0.55 (93.6 examples/sec; 0.043 sec/batch; 1h:04m:18s remains)
INFO - root - 2019-11-06 19:19:35.033647: step 59750, total loss = 2.48, predict loss = 0.74 (62.6 examples/sec; 0.064 sec/batch; 1h:36m:05s remains)
INFO - root - 2019-11-06 19:19:35.784334: step 59760, total loss = 1.73, predict loss = 0.46 (58.0 examples/sec; 0.069 sec/batch; 1h:43m:41s remains)
INFO - root - 2019-11-06 19:19:36.579757: step 59770, total loss = 1.48, predict loss = 0.34 (46.9 examples/sec; 0.085 sec/batch; 2h:08m:13s remains)
INFO - root - 2019-11-06 19:19:37.372384: step 59780, total loss = 1.25, predict loss = 0.36 (66.4 examples/sec; 0.060 sec/batch; 1h:30m:34s remains)
INFO - root - 2019-11-06 19:19:38.091393: step 59790, total loss = 1.55, predict loss = 0.42 (62.0 examples/sec; 0.065 sec/batch; 1h:37m:02s remains)
INFO - root - 2019-11-06 19:19:38.635979: step 59800, total loss = 1.81, predict loss = 0.49 (84.5 examples/sec; 0.047 sec/batch; 1h:11m:10s remains)
INFO - root - 2019-11-06 19:19:39.084854: step 59810, total loss = 1.80, predict loss = 0.46 (99.9 examples/sec; 0.040 sec/batch; 1h:00m:11s remains)
INFO - root - 2019-11-06 19:19:39.565572: step 59820, total loss = 3.10, predict loss = 0.99 (125.2 examples/sec; 0.032 sec/batch; 0h:48m:00s remains)
INFO - root - 2019-11-06 19:19:40.998579: step 59830, total loss = 0.94, predict loss = 0.25 (50.8 examples/sec; 0.079 sec/batch; 1h:58m:20s remains)
INFO - root - 2019-11-06 19:19:41.835880: step 59840, total loss = 2.04, predict loss = 0.59 (66.4 examples/sec; 0.060 sec/batch; 1h:30m:30s remains)
INFO - root - 2019-11-06 19:19:42.524201: step 59850, total loss = 1.91, predict loss = 0.49 (65.2 examples/sec; 0.061 sec/batch; 1h:32m:07s remains)
INFO - root - 2019-11-06 19:19:43.289761: step 59860, total loss = 1.02, predict loss = 0.29 (60.6 examples/sec; 0.066 sec/batch; 1h:39m:14s remains)
INFO - root - 2019-11-06 19:19:43.933568: step 59870, total loss = 2.43, predict loss = 0.69 (83.9 examples/sec; 0.048 sec/batch; 1h:11m:38s remains)
INFO - root - 2019-11-06 19:19:44.379238: step 59880, total loss = 1.20, predict loss = 0.30 (98.7 examples/sec; 0.041 sec/batch; 1h:00m:53s remains)
INFO - root - 2019-11-06 19:19:44.828796: step 59890, total loss = 1.70, predict loss = 0.45 (95.8 examples/sec; 0.042 sec/batch; 1h:02m:41s remains)
INFO - root - 2019-11-06 19:19:46.075098: step 59900, total loss = 2.14, predict loss = 0.57 (61.4 examples/sec; 0.065 sec/batch; 1h:37m:50s remains)
INFO - root - 2019-11-06 19:19:46.780208: step 59910, total loss = 2.23, predict loss = 0.61 (55.0 examples/sec; 0.073 sec/batch; 1h:49m:11s remains)
INFO - root - 2019-11-06 19:19:47.499309: step 59920, total loss = 1.76, predict loss = 0.45 (66.3 examples/sec; 0.060 sec/batch; 1h:30m:34s remains)
INFO - root - 2019-11-06 19:19:48.260130: step 59930, total loss = 1.99, predict loss = 0.57 (57.9 examples/sec; 0.069 sec/batch; 1h:43m:45s remains)
INFO - root - 2019-11-06 19:19:49.023075: step 59940, total loss = 2.48, predict loss = 0.70 (65.1 examples/sec; 0.061 sec/batch; 1h:32m:13s remains)
INFO - root - 2019-11-06 19:19:49.580937: step 59950, total loss = 2.74, predict loss = 0.84 (94.3 examples/sec; 0.042 sec/batch; 1h:03m:41s remains)
INFO - root - 2019-11-06 19:19:50.041155: step 59960, total loss = 1.97, predict loss = 0.56 (93.3 examples/sec; 0.043 sec/batch; 1h:04m:18s remains)
INFO - root - 2019-11-06 19:19:51.207673: step 59970, total loss = 2.55, predict loss = 0.73 (5.2 examples/sec; 0.770 sec/batch; 19h:15m:06s remains)
