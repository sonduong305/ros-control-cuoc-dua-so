INFO - bisenet-v2 - Running command 'main'
INFO - bisenet-v2 - Started run with ID "4"
INFO - root - nvidia-ml-py is not installed, automatically select gpu is disabled!
WARNING:tensorflow:From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - tensorflow - From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:53: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:53: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:65: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:65: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
INFO - root - preproces -- augment
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:73: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:73: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:74: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:74: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:131: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/Dataset/dataset.py:131: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:164: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:164: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e1dca588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e1dca588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e1dca588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e1dca588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e24352b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e24352b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e24352b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e24352b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e2435390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e2435390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e2435390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e2435390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2d6b6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2d6b6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2d6b6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2d6b6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e2daf2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e2daf2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e2daf2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e2daf2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e31eea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e31eea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e31eea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e31eea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e31eecf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e31eecf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e31eecf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e31eecf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e248da20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e248da20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e248da20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e248da20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e31eea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e31eea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e31eea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e31eea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2d3c048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2d3c048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2d3c048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2d3c048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f85e248dc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f85e248dc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f85e248dc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f85e248dc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c91c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c91c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c91c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c91c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2435128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2435128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2435128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2435128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c40f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c40f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c40f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c40f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2c90f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2c90f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2c90f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2c90f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2d34860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2d34860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2d34860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2d34860>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2435d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2435d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2435d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2435d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c90470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c90470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c90470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c90470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2435d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2435d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2435d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2435d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c90898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c90898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c90898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c90898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a83f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a83f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a83f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a83f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e24359b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e24359b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e24359b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e24359b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a83e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a83e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a83e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a83e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2a68898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2a68898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2a68898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2a68898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a4cd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a4cd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a4cd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a4cd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2a68908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2a68908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2a68908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2a68908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a1fba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a1fba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a1fba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2a1fba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2b05f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2b05f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2b05f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2b05f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e28db278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e28db278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e28db278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e28db278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2b05668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2b05668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2b05668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2b05668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e28076a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e28076a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e28076a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e28076a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e296cba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e296cba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e296cba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e296cba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e28db978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e28db978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e28db978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e28db978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e28922b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e28922b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e28922b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e28922b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e278fef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e278fef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e278fef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e278fef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e281ea20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e281ea20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e281ea20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e281ea20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e26d0438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e26d0438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e26d0438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e26d0438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e25e8eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e25e8eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e25e8eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e25e8eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e29ebd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e29ebd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e29ebd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e29ebd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e29ebe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e29ebe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e29ebe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e29ebe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e296c940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e296c940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e296c940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e296c940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e272f9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e272f9e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e272f9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e272f9e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2618ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2618ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2618ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2618ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2698e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2698e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2698e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2698e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e278f128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e278f128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e278f128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e278f128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e26b78d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e26b78d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e26b78d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e26b78d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e238c978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e238c978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e238c978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e238c978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e26b78d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e26b78d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e26b78d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e26b78d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e27b6240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e27b6240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e27b6240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e27b6240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e3192898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e3192898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e3192898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e3192898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e231f6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e231f6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e231f6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e231f6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e22f7898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e22f7898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e22f7898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e22f7898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2218e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2218e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2218e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2218e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2680f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2680f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2680f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2680f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e21e6fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e21e6fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e21e6fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e21e6fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2218e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2218e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2218e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2218e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e231f6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e231f6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e231f6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e231f6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2218908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2218908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2218908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2218908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2110fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2110fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2110fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2110fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2d67240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2d67240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2d67240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2d67240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e205f6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e205f6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e205f6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e205f6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e21c22e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e21c22e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e21c22e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e21c22e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2128d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2128d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2128d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2128d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2128588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2128588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2128588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2128588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1f8d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1f8d710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1f8d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1f8d710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2128c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2128c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2128c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2128c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1f89b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1f89b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1f89b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1f89b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2011f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2011f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2011f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2011f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1fa0a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1fa0a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1fa0a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1fa0a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2618780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2618780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2618780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2618780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e210ba20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e210ba20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e210ba20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e210ba20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2110cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2110cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2110cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2110cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1ce4dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1ce4dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1ce4dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1ce4dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2210780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2210780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2210780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2210780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1d7d2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1d7d2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1d7d2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1d7d2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1d7d2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1d7d2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1d7d2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1d7d2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1addba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1addba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1addba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1addba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1d7d080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1d7d080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1d7d080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1d7d080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1f06dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1f06dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1f06dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1f06dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e21129b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e21129b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e21129b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e21129b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1ac9908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1ac9908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1ac9908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1ac9908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1ac92b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1ac92b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1ac92b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1ac92b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1b7b400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1b7b400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1b7b400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1b7b400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2daf668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2daf668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2daf668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2daf668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1e74470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1e74470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1e74470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1e74470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1afc978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1afc978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1afc978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1afc978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1a0d7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1a0d7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1a0d7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1a0d7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1bf8b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1bf8b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1bf8b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1bf8b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e18c8b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e18c8b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e18c8b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e18c8b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e18da8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e18da8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e18da8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e18da8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e187fac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e187fac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e187fac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e187fac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1902be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1902be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1902be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1902be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1740d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1740d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1740d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1740d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1e74550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1e74550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1e74550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1e74550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e17df0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e17df0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e17df0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e17df0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1e74748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1e74748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1e74748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1e74748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e17754e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e17754e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e17754e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e17754e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1ac9908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1ac9908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1ac9908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1ac9908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e159acc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e159acc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e159acc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e159acc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e16dc278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e16dc278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e16dc278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e16dc278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e159ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e159ac50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e159ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e159ac50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e17b7320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e17b7320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e17b7320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e17b7320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e14aa940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e14aa940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e14aa940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e14aa940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1637748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1637748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1637748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1637748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e153aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e153aa90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e153aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e153aa90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e14f6c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e14f6c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e14f6c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e14f6c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e144a9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e144a9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e144a9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e144a9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1c00e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1c00e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1c00e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e1c00e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e17b70f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e17b70f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e17b70f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e17b70f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e13afda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e13afda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e13afda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e13afda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1339e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1339e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1339e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1339e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e16aa588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e16aa588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e16aa588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e16aa588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e130b9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e130b9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e130b9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e130b9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e2c40d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e2c40d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e2c40d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e2c40d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e130bc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e130bc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e130bc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e130bc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:186: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:186: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e128f7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e128f7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e128f7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e128f7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e11afb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e11afb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e11afb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e11afb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e11955c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e11955c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e11955c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e11955c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e11522b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e11522b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e11522b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e11522b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e11af6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e11af6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e11af6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e11af6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e10df908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e10df908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e10df908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e10df908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e14d1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e14d1a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e14d1a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e14d1a58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2d67358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2d67358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2d67358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2d67358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fd1a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fd1a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fd1a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fd1a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0fd17f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0fd17f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0fd17f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0fd17f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0f51d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0f51d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0f51d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0f51d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0fbb7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0fbb7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0fbb7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0fbb7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fbbef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fbbef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fbbef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fbbef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0f519e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0f519e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0f519e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0f519e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fbbeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fbbeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fbbeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fbbeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fd14a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fd14a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fd14a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0fd14a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0f53dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0f53dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0f53dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0f53dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0e8acc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0e8acc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0e8acc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0e8acc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0f46c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0f46c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0f46c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0f46c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0e8a6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0e8a6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0e8a6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0e8a6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0e8afd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0e8afd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0e8afd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0e8afd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1152ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1152ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1152ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e1152ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0d69908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0d69908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0d69908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0d69908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0d69eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0d69eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0d69eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0d69eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0d69908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0d69908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0d69908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0d69908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0cd7fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0cd7fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0cd7fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0cd7fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0cd7d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0cd7d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0cd7d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0cd7d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:224: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:224: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:228: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:228: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:231: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:231: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:236: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:243: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:243: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:247: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING - tensorflow - From /home/nhatdeptrai/Desktop/bisenet-tensorflow/models/bisenet.py:247: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING:tensorflow:From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - tensorflow - From /home/nhatdeptrai/anaconda3/envs/cuocduaso/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - root - img_mean is not explicitly specified, using default value: None
INFO - root - preproces -- None
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e08378d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e08378d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e08378d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e08378d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0837b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0837b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0837b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0837b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0837c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0837c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0837c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0837c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e08ee128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e08ee128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e08ee128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e08ee128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0837a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e08370b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e08370b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e08370b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e08370b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e08eea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e08eea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e08eea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e08eea90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f85e0837a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f85e0837a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f85e0837a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f85e0837a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09ecbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09ecbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09ecbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09ecbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e084fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e084fc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e084fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e084fc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084fb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084fb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084fb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084fb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e084f978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e084f978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e084f978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e084f978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e08eec88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e08eec88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e08eec88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e08eec88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e084f400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e084f400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e084f400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e084f400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084fa58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084fa58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084fa58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084fa58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ec748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ec748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ec748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ec748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084fe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084fe10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09a28d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09a28d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09a28d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09a28d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084f710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084f710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084f710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e084f710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09a25f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09a25f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09a25f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09a25f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09a2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09a2390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09a2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09a2390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09a2a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09a2a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09a2a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09a2a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09a2e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09a2e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09a2e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09a2e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2c84c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2c84c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2c84c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2c84c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0a0dba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0a0dba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0a0dba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0a0dba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2c849b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2c849b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2c849b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e2c849b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0a0de80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0a0de80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0a0de80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0a0de80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ebc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ebc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ebc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ebc50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c842e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c842e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c842e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c842e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09b3b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09b3b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09b3b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09b3b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c84320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c84320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c84320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c84320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ec0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ec0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ec0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ec0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09c38d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09c38d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09c38d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09c38d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0965a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0965a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0965a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0965a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09c3828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09c3828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09c3828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09c3828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09abcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09abcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09abcc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09abcc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09b36d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09b36d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09b36d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09b36d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ab8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ab8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ab8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ab8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0965780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0965780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0965780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0965780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0939ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0939ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0939ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0939ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c88be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c88be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c88be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c88be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0939cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0939cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0939cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0939cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09ce9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09ce9e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09ce9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09ce9e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09390b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09390b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09390b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09390b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09519b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09519b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09519b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09519b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ab978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ab978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ab978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e09ab978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0951c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0951c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0951c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0951c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e092a470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e092a470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e092a470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e092a470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09390f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09390f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09390f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09390f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e092a470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e092a470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e092a470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e092a470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09abfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09abfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09abfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09abfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e092ec88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e092ec88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e092ec88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e092ec88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e092a828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e092a828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e092a828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e092a828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07916d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07916d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07916d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07916d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09510b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09510b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09510b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e09510b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0791198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0791198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0791198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0791198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c882b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c882b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c882b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e2c882b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0810c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0810c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0810c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0810c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07910b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07910b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07910b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07910b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e094cb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e094cb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e094cb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e094cb38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0810c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0810c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0810c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0810c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e079df98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e079df98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e079df98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e079df98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0810550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0810550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0810550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0810550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0803cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0803cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0803cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0803cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e079d630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e079d630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e079d630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e079d630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0803fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0803fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0803fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0803fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07917f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07917f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07917f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07917f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a38d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a38d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a38d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a38d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0803be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0803be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0803be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0803be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a0dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a0dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a0dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a0dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a3978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a3978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a3978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a3978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a3668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a3668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a3668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a3668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a20b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a20b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a20b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a20b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e078aeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e078aeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e078aeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e078aeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a20b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a20b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a20b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a20b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e078a320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e078a320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e078a320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e078a320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0776278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0776278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0776278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0776278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e078ae80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e078ae80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e078ae80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e078ae80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a0048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a0048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a0048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a0048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07cf7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07cf7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07cf7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07cf7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e078a358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e078a358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e078a358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e078a358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a37b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a37b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a37b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e07a37b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e078a320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e078a320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e078a320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e078a320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0810588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0810588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0810588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0810588>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e08103c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e08103c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e08103c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e08103c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0776e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0776e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0776e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0776e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0747198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0747198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0747198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0747198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e071af98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e071af98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e071af98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e071af98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0776390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0776390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0776390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0776390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069e2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069e2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069e2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069e2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0776da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0776da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0776da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e0776da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069e9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069e9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069e9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069e9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a2e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a2e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a2e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e07a2e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069e2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069e2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069e2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069e2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e069e4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e069e4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e069e4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e069e4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069c828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069c828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069c828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069c828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e069e4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e069e4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e069e4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e069e4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e071a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e071a390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e071a390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e071a390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e071a320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e071a320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e071a320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e071a320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069a7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069a7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069a7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069a7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e070c438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e070c438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e070c438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e070c438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e06a8978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e06a8978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e06a8978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e06a8978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e06a86a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e06a86a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e06a86a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e06a86a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069a128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069a128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069a128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e069a128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e06eb1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e06eb1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e06eb1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e06eb1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e06a8240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e06a8240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e06a8240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e06a8240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e069af28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e069af28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e069af28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e069af28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0663128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0663128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0663128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0663128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e06a8240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e06a8240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e06a8240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SeparableConv2D.call of <tensorflow.python.layers.convolutional.SeparableConv2D object at 0x7f85e06a8240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e066f5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e066f5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e066f5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e066f5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0663128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0663128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0663128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0663128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e06df2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e06df2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e06df2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e06df2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e066f0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e066f0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e066f0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e066f0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e062fac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e062fac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e062fac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e062fac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0619898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0619898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0619898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0619898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0676630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0676630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0676630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0676630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0676630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0676630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0676630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0676630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e062ff60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e062ff60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e062ff60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e062ff60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e062f128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e062f128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e062f128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e062f128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0676ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0676ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0676ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0676ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e059cd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e059cd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e059cd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e059cd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e059ce80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e059ce80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e059ce80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e059ce80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e063a240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e063a240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e063a240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e063a240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e062fda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e062fda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e062fda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e062fda0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e05aa400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e05aa400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e05aa400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e05aa400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e059e630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e059e630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e059e630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e059e630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e059ed68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e059ed68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e059ed68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e059ed68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e062fef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e062fef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e062fef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e062fef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e062fd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e062fd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e062fd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e062fd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e05c2ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e05c2ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e05c2ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e05c2ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e063a7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e063a7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e063a7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e063a7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e05c2320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e05c2320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e05c2320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e05c2320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0564f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0564f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0564f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e0564f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0590320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0590320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0590320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0590320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e056cef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e056cef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e056cef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e056cef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0564400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0564400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0564400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f85e0564400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e056c828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e056c828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e056c828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e056c828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e04b6ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e04b6ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e04b6ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e04b6ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e04b6ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e04b6ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING - tensorflow - Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e04b6ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f85e04b6ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From train.py:66: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From train.py:66: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING - tensorflow - From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING:tensorflow:From train.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING - tensorflow - From train.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING:tensorflow:From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING - tensorflow - From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING - tensorflow - From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING - tensorflow - From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

2019-11-03 22:59:09.553400: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-03 22:59:09.559179: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-11-03 22:59:09.676062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-03 22:59:09.676532: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5567ff262f10 executing computations on platform CUDA. Devices:
2019-11-03 22:59:09.676547: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-11-03 22:59:09.705846: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz
2019-11-03 22:59:09.706150: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5567ff1b0090 executing computations on platform Host. Devices:
2019-11-03 22:59:09.706165: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-11-03 22:59:09.706534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-03 22:59:09.706916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-11-03 22:59:09.707788: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-03 22:59:09.722501: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-11-03 22:59:09.730056: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-11-03 22:59:09.732479: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-11-03 22:59:09.747058: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-11-03 22:59:09.759086: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-11-03 22:59:09.790890: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-11-03 22:59:09.791105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-03 22:59:09.791761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-03 22:59:09.792290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-11-03 22:59:09.792580: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-11-03 22:59:09.793941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-03 22:59:09.793972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-11-03 22:59:09.793982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-11-03 22:59:09.794458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-03 22:59:09.795073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-03 22:59:09.795631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7045 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-11-03 22:59:11.108487: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
INFO - root - Train for 6000000 steps
2019-11-03 22:59:16.577698: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
INFO - root - 2019-11-03 22:59:18.323479: step 0, total loss = 9.40, predict loss = 2.89 (0.6 examples/sec; 6.234 sec/batch; 10390h:06m:51s remains)
2019-11-03 22:59:19.855688: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
INFO - root - 2019-11-03 22:59:22.642776: step 10, total loss = 4.77, predict loss = 1.31 (65.0 examples/sec; 0.062 sec/batch; 102h:38m:00s remains)
INFO - root - 2019-11-03 22:59:23.228896: step 20, total loss = 2.94, predict loss = 0.89 (66.7 examples/sec; 0.060 sec/batch; 99h:55m:59s remains)
INFO - root - 2019-11-03 22:59:23.843741: step 30, total loss = 4.54, predict loss = 1.32 (69.2 examples/sec; 0.058 sec/batch; 96h:20m:48s remains)
INFO - root - 2019-11-03 22:59:24.442152: step 40, total loss = 4.05, predict loss = 1.26 (82.3 examples/sec; 0.049 sec/batch; 80h:58m:56s remains)
INFO - root - 2019-11-03 22:59:25.069856: step 50, total loss = 3.26, predict loss = 0.89 (73.6 examples/sec; 0.054 sec/batch; 90h:36m:05s remains)
INFO - root - 2019-11-03 22:59:25.725703: step 60, total loss = 2.71, predict loss = 0.84 (71.9 examples/sec; 0.056 sec/batch; 92h:43m:30s remains)
INFO - root - 2019-11-03 22:59:26.383582: step 70, total loss = 3.23, predict loss = 0.99 (62.5 examples/sec; 0.064 sec/batch; 106h:37m:23s remains)
INFO - root - 2019-11-03 22:59:27.068865: step 80, total loss = 3.62, predict loss = 0.97 (71.7 examples/sec; 0.056 sec/batch; 92h:57m:27s remains)
INFO - root - 2019-11-03 22:59:27.700848: step 90, total loss = 2.73, predict loss = 0.74 (70.6 examples/sec; 0.057 sec/batch; 94h:22m:41s remains)
INFO - root - 2019-11-03 22:59:28.311850: step 100, total loss = 3.30, predict loss = 0.94 (65.1 examples/sec; 0.061 sec/batch; 102h:24m:49s remains)
INFO - root - 2019-11-03 22:59:28.961380: step 110, total loss = 2.38, predict loss = 0.59 (77.9 examples/sec; 0.051 sec/batch; 85h:31m:59s remains)
INFO - root - 2019-11-03 22:59:29.630034: step 120, total loss = 2.36, predict loss = 0.61 (67.3 examples/sec; 0.059 sec/batch; 99h:00m:53s remains)
INFO - root - 2019-11-03 22:59:30.279386: step 130, total loss = 2.41, predict loss = 0.64 (63.2 examples/sec; 0.063 sec/batch; 105h:24m:17s remains)
INFO - root - 2019-11-03 22:59:30.924730: step 140, total loss = 2.24, predict loss = 0.53 (75.6 examples/sec; 0.053 sec/batch; 88h:14m:06s remains)
INFO - root - 2019-11-03 22:59:31.597280: step 150, total loss = 2.17, predict loss = 0.56 (68.7 examples/sec; 0.058 sec/batch; 96h:59m:11s remains)
INFO - root - 2019-11-03 22:59:32.217769: step 160, total loss = 2.18, predict loss = 0.55 (73.0 examples/sec; 0.055 sec/batch; 91h:22m:23s remains)
INFO - root - 2019-11-03 22:59:32.845194: step 170, total loss = 2.03, predict loss = 0.54 (75.0 examples/sec; 0.053 sec/batch; 88h:50m:14s remains)
INFO - root - 2019-11-03 22:59:33.495101: step 180, total loss = 1.96, predict loss = 0.51 (69.5 examples/sec; 0.058 sec/batch; 95h:53m:32s remains)
INFO - root - 2019-11-03 22:59:34.189003: step 190, total loss = 1.82, predict loss = 0.50 (66.5 examples/sec; 0.060 sec/batch; 100h:12m:00s remains)
INFO - root - 2019-11-03 22:59:34.831330: step 200, total loss = 2.81, predict loss = 0.76 (68.5 examples/sec; 0.058 sec/batch; 97h:17m:03s remains)
INFO - root - 2019-11-03 22:59:35.437313: step 210, total loss = 1.56, predict loss = 0.37 (80.4 examples/sec; 0.050 sec/batch; 82h:56m:30s remains)
INFO - root - 2019-11-03 22:59:36.042746: step 220, total loss = 2.00, predict loss = 0.52 (75.1 examples/sec; 0.053 sec/batch; 88h:44m:55s remains)
INFO - root - 2019-11-03 22:59:36.687888: step 230, total loss = 1.55, predict loss = 0.38 (74.1 examples/sec; 0.054 sec/batch; 90h:00m:25s remains)
INFO - root - 2019-11-03 22:59:37.315879: step 240, total loss = 1.76, predict loss = 0.43 (68.7 examples/sec; 0.058 sec/batch; 97h:04m:14s remains)
INFO - root - 2019-11-03 22:59:37.966668: step 250, total loss = 2.24, predict loss = 0.52 (62.2 examples/sec; 0.064 sec/batch; 107h:12m:57s remains)
INFO - root - 2019-11-03 22:59:38.623852: step 260, total loss = 1.99, predict loss = 0.47 (64.9 examples/sec; 0.062 sec/batch; 102h:46m:25s remains)
INFO - root - 2019-11-03 22:59:39.338746: step 270, total loss = 2.02, predict loss = 0.49 (69.3 examples/sec; 0.058 sec/batch; 96h:13m:08s remains)
INFO - root - 2019-11-03 22:59:39.980071: step 280, total loss = 2.13, predict loss = 0.54 (73.2 examples/sec; 0.055 sec/batch; 91h:06m:53s remains)
INFO - root - 2019-11-03 22:59:40.611440: step 290, total loss = 2.15, predict loss = 0.62 (76.0 examples/sec; 0.053 sec/batch; 87h:40m:35s remains)
INFO - root - 2019-11-03 22:59:41.255882: step 300, total loss = 1.75, predict loss = 0.44 (68.2 examples/sec; 0.059 sec/batch; 97h:48m:54s remains)
INFO - root - 2019-11-03 22:59:41.922920: step 310, total loss = 2.39, predict loss = 0.62 (68.7 examples/sec; 0.058 sec/batch; 96h:58m:16s remains)
INFO - root - 2019-11-03 22:59:42.598104: step 320, total loss = 2.15, predict loss = 0.53 (68.6 examples/sec; 0.058 sec/batch; 97h:08m:04s remains)
INFO - root - 2019-11-03 22:59:43.260399: step 330, total loss = 1.85, predict loss = 0.49 (74.9 examples/sec; 0.053 sec/batch; 89h:00m:56s remains)
INFO - root - 2019-11-03 22:59:43.916491: step 340, total loss = 2.65, predict loss = 0.78 (73.2 examples/sec; 0.055 sec/batch; 91h:01m:54s remains)
INFO - root - 2019-11-03 22:59:44.558267: step 350, total loss = 2.76, predict loss = 0.67 (72.1 examples/sec; 0.055 sec/batch; 92h:25m:04s remains)
INFO - root - 2019-11-03 22:59:45.165545: step 360, total loss = 3.52, predict loss = 1.03 (80.3 examples/sec; 0.050 sec/batch; 82h:59m:53s remains)
INFO - root - 2019-11-03 22:59:45.791345: step 370, total loss = 1.86, predict loss = 0.45 (77.3 examples/sec; 0.052 sec/batch; 86h:16m:13s remains)
INFO - root - 2019-11-03 22:59:46.430383: step 380, total loss = 1.80, predict loss = 0.42 (61.0 examples/sec; 0.066 sec/batch; 109h:12m:28s remains)
INFO - root - 2019-11-03 22:59:47.101212: step 390, total loss = 1.82, predict loss = 0.44 (74.2 examples/sec; 0.054 sec/batch; 89h:51m:17s remains)
INFO - root - 2019-11-03 22:59:47.720918: step 400, total loss = 2.12, predict loss = 0.54 (71.1 examples/sec; 0.056 sec/batch; 93h:48m:18s remains)
INFO - root - 2019-11-03 22:59:48.324047: step 410, total loss = 1.98, predict loss = 0.53 (69.0 examples/sec; 0.058 sec/batch; 96h:38m:22s remains)
INFO - root - 2019-11-03 22:59:48.954049: step 420, total loss = 2.06, predict loss = 0.54 (69.4 examples/sec; 0.058 sec/batch; 96h:02m:13s remains)
INFO - root - 2019-11-03 22:59:49.592437: step 430, total loss = 1.86, predict loss = 0.41 (65.3 examples/sec; 0.061 sec/batch; 102h:05m:02s remains)
INFO - root - 2019-11-03 22:59:50.215346: step 440, total loss = 1.80, predict loss = 0.43 (67.5 examples/sec; 0.059 sec/batch; 98h:49m:16s remains)
INFO - root - 2019-11-03 22:59:50.879979: step 450, total loss = 1.87, predict loss = 0.50 (74.0 examples/sec; 0.054 sec/batch; 90h:07m:54s remains)
INFO - root - 2019-11-03 22:59:51.527117: step 460, total loss = 1.77, predict loss = 0.39 (66.3 examples/sec; 0.060 sec/batch; 100h:30m:41s remains)
INFO - root - 2019-11-03 22:59:52.171722: step 470, total loss = 1.63, predict loss = 0.40 (75.0 examples/sec; 0.053 sec/batch; 88h:55m:27s remains)
INFO - root - 2019-11-03 22:59:52.835315: step 480, total loss = 1.26, predict loss = 0.31 (69.3 examples/sec; 0.058 sec/batch; 96h:14m:13s remains)
INFO - root - 2019-11-03 22:59:53.495209: step 490, total loss = 1.69, predict loss = 0.44 (65.7 examples/sec; 0.061 sec/batch; 101h:30m:24s remains)
INFO - root - 2019-11-03 22:59:54.190931: step 500, total loss = 1.90, predict loss = 0.51 (72.0 examples/sec; 0.056 sec/batch; 92h:32m:34s remains)
INFO - root - 2019-11-03 22:59:54.799448: step 510, total loss = 1.64, predict loss = 0.47 (64.9 examples/sec; 0.062 sec/batch; 102h:43m:12s remains)
INFO - root - 2019-11-03 22:59:55.459631: step 520, total loss = 1.99, predict loss = 0.49 (68.2 examples/sec; 0.059 sec/batch; 97h:47m:59s remains)
INFO - root - 2019-11-03 22:59:56.085237: step 530, total loss = 2.55, predict loss = 0.72 (71.2 examples/sec; 0.056 sec/batch; 93h:39m:30s remains)
INFO - root - 2019-11-03 22:59:56.740238: step 540, total loss = 1.50, predict loss = 0.37 (66.2 examples/sec; 0.060 sec/batch; 100h:38m:14s remains)
INFO - root - 2019-11-03 22:59:57.362451: step 550, total loss = 1.52, predict loss = 0.33 (71.8 examples/sec; 0.056 sec/batch; 92h:54m:04s remains)
INFO - root - 2019-11-03 22:59:58.011784: step 560, total loss = 2.11, predict loss = 0.54 (70.4 examples/sec; 0.057 sec/batch; 94h:41m:34s remains)
INFO - root - 2019-11-03 22:59:58.616713: step 570, total loss = 1.58, predict loss = 0.36 (66.5 examples/sec; 0.060 sec/batch; 100h:12m:35s remains)
INFO - root - 2019-11-03 22:59:59.237678: step 580, total loss = 2.56, predict loss = 0.69 (68.5 examples/sec; 0.058 sec/batch; 97h:15m:39s remains)
INFO - root - 2019-11-03 22:59:59.910086: step 590, total loss = 1.79, predict loss = 0.37 (64.6 examples/sec; 0.062 sec/batch; 103h:11m:20s remains)
INFO - root - 2019-11-03 23:00:00.591766: step 600, total loss = 1.41, predict loss = 0.34 (64.5 examples/sec; 0.062 sec/batch; 103h:16m:08s remains)
INFO - root - 2019-11-03 23:00:01.244730: step 610, total loss = 1.38, predict loss = 0.33 (77.9 examples/sec; 0.051 sec/batch; 85h:32m:09s remains)
INFO - root - 2019-11-03 23:00:01.892870: step 620, total loss = 1.93, predict loss = 0.45 (72.1 examples/sec; 0.055 sec/batch; 92h:28m:48s remains)
INFO - root - 2019-11-03 23:00:02.529922: step 630, total loss = 1.49, predict loss = 0.33 (72.2 examples/sec; 0.055 sec/batch; 92h:22m:21s remains)
INFO - root - 2019-11-03 23:00:03.155494: step 640, total loss = 1.23, predict loss = 0.34 (68.3 examples/sec; 0.059 sec/batch; 97h:40m:05s remains)
INFO - root - 2019-11-03 23:00:03.822191: step 650, total loss = 1.53, predict loss = 0.40 (70.3 examples/sec; 0.057 sec/batch; 94h:52m:39s remains)
INFO - root - 2019-11-03 23:00:04.465016: step 660, total loss = 2.18, predict loss = 0.52 (75.0 examples/sec; 0.053 sec/batch; 88h:53m:10s remains)
INFO - root - 2019-11-03 23:00:05.122604: step 670, total loss = 2.06, predict loss = 0.49 (72.8 examples/sec; 0.055 sec/batch; 91h:32m:17s remains)
INFO - root - 2019-11-03 23:00:05.742415: step 680, total loss = 1.57, predict loss = 0.38 (74.8 examples/sec; 0.053 sec/batch; 89h:04m:27s remains)
INFO - root - 2019-11-03 23:00:06.406798: step 690, total loss = 1.58, predict loss = 0.40 (71.6 examples/sec; 0.056 sec/batch; 93h:08m:51s remains)
INFO - root - 2019-11-03 23:00:07.084901: step 700, total loss = 1.93, predict loss = 0.42 (67.5 examples/sec; 0.059 sec/batch; 98h:47m:19s remains)
INFO - root - 2019-11-03 23:00:07.719375: step 710, total loss = 2.15, predict loss = 0.49 (68.4 examples/sec; 0.058 sec/batch; 97h:26m:24s remains)
INFO - root - 2019-11-03 23:00:08.395538: step 720, total loss = 2.72, predict loss = 0.69 (63.4 examples/sec; 0.063 sec/batch; 105h:08m:47s remains)
INFO - root - 2019-11-03 23:00:09.059534: step 730, total loss = 2.65, predict loss = 0.73 (68.6 examples/sec; 0.058 sec/batch; 97h:10m:37s remains)
INFO - root - 2019-11-03 23:00:09.652333: step 740, total loss = 2.07, predict loss = 0.55 (77.4 examples/sec; 0.052 sec/batch; 86h:08m:58s remains)
INFO - root - 2019-11-03 23:00:10.280638: step 750, total loss = 1.51, predict loss = 0.34 (71.2 examples/sec; 0.056 sec/batch; 93h:33m:26s remains)
INFO - root - 2019-11-03 23:00:10.904464: step 760, total loss = 1.32, predict loss = 0.35 (68.9 examples/sec; 0.058 sec/batch; 96h:47m:25s remains)
INFO - root - 2019-11-03 23:00:11.523481: step 770, total loss = 1.45, predict loss = 0.35 (75.9 examples/sec; 0.053 sec/batch; 87h:52m:48s remains)
INFO - root - 2019-11-03 23:00:12.132559: step 780, total loss = 2.11, predict loss = 0.46 (69.5 examples/sec; 0.058 sec/batch; 95h:51m:01s remains)
INFO - root - 2019-11-03 23:00:12.803968: step 790, total loss = 2.25, predict loss = 0.46 (66.2 examples/sec; 0.060 sec/batch; 100h:45m:12s remains)
INFO - root - 2019-11-03 23:00:13.455853: step 800, total loss = 1.25, predict loss = 0.25 (74.0 examples/sec; 0.054 sec/batch; 90h:03m:42s remains)
INFO - root - 2019-11-03 23:00:14.135504: step 810, total loss = 1.74, predict loss = 0.33 (64.1 examples/sec; 0.062 sec/batch; 104h:00m:18s remains)
INFO - root - 2019-11-03 23:00:14.736872: step 820, total loss = 1.21, predict loss = 0.33 (72.4 examples/sec; 0.055 sec/batch; 92h:01m:58s remains)
INFO - root - 2019-11-03 23:00:15.377707: step 830, total loss = 1.92, predict loss = 0.51 (71.8 examples/sec; 0.056 sec/batch; 92h:53m:37s remains)
INFO - root - 2019-11-03 23:00:16.034300: step 840, total loss = 1.62, predict loss = 0.33 (73.4 examples/sec; 0.054 sec/batch; 90h:46m:33s remains)
INFO - root - 2019-11-03 23:00:16.680160: step 850, total loss = 1.26, predict loss = 0.36 (66.2 examples/sec; 0.060 sec/batch; 100h:40m:51s remains)
INFO - root - 2019-11-03 23:00:17.346936: step 860, total loss = 1.64, predict loss = 0.44 (70.2 examples/sec; 0.057 sec/batch; 94h:56m:47s remains)
INFO - root - 2019-11-03 23:00:18.010586: step 870, total loss = 0.91, predict loss = 0.24 (67.2 examples/sec; 0.060 sec/batch; 99h:12m:04s remains)
INFO - root - 2019-11-03 23:00:18.704581: step 880, total loss = 1.65, predict loss = 0.43 (63.9 examples/sec; 0.063 sec/batch; 104h:20m:20s remains)
INFO - root - 2019-11-03 23:00:19.362823: step 890, total loss = 1.25, predict loss = 0.30 (72.1 examples/sec; 0.055 sec/batch; 92h:29m:00s remains)
INFO - root - 2019-11-03 23:00:19.964387: step 900, total loss = 1.63, predict loss = 0.40 (71.7 examples/sec; 0.056 sec/batch; 92h:57m:36s remains)
INFO - root - 2019-11-03 23:00:20.635637: step 910, total loss = 1.16, predict loss = 0.29 (72.6 examples/sec; 0.055 sec/batch; 91h:50m:41s remains)
INFO - root - 2019-11-03 23:00:21.243632: step 920, total loss = 1.96, predict loss = 0.54 (74.6 examples/sec; 0.054 sec/batch; 89h:23m:41s remains)
INFO - root - 2019-11-03 23:00:21.850481: step 930, total loss = 1.56, predict loss = 0.40 (69.6 examples/sec; 0.058 sec/batch; 95h:49m:41s remains)
INFO - root - 2019-11-03 23:00:22.499172: step 940, total loss = 1.14, predict loss = 0.29 (70.2 examples/sec; 0.057 sec/batch; 95h:00m:30s remains)
INFO - root - 2019-11-03 23:00:23.134499: step 950, total loss = 1.77, predict loss = 0.40 (70.2 examples/sec; 0.057 sec/batch; 94h:59m:15s remains)
INFO - root - 2019-11-03 23:00:23.866619: step 960, total loss = 1.50, predict loss = 0.39 (54.2 examples/sec; 0.074 sec/batch; 122h:56m:16s remains)
INFO - root - 2019-11-03 23:00:24.495964: step 970, total loss = 1.29, predict loss = 0.28 (70.2 examples/sec; 0.057 sec/batch; 94h:59m:35s remains)
INFO - root - 2019-11-03 23:00:25.140799: step 980, total loss = 1.90, predict loss = 0.43 (70.5 examples/sec; 0.057 sec/batch; 94h:36m:17s remains)
INFO - root - 2019-11-03 23:00:25.821864: step 990, total loss = 1.65, predict loss = 0.41 (61.2 examples/sec; 0.065 sec/batch; 108h:50m:55s remains)
INFO - root - 2019-11-03 23:00:26.459482: step 1000, total loss = 1.62, predict loss = 0.37 (69.3 examples/sec; 0.058 sec/batch; 96h:14m:36s remains)
INFO - root - 2019-11-03 23:00:27.082952: step 1010, total loss = 1.54, predict loss = 0.38 (69.1 examples/sec; 0.058 sec/batch; 96h:31m:32s remains)
INFO - root - 2019-11-03 23:00:27.691751: step 1020, total loss = 1.51, predict loss = 0.35 (82.3 examples/sec; 0.049 sec/batch; 80h:59m:44s remains)
INFO - root - 2019-11-03 23:00:28.364909: step 1030, total loss = 1.61, predict loss = 0.34 (69.9 examples/sec; 0.057 sec/batch; 95h:21m:29s remains)
INFO - root - 2019-11-03 23:00:29.022272: step 1040, total loss = 1.58, predict loss = 0.29 (63.6 examples/sec; 0.063 sec/batch; 104h:48m:09s remains)
INFO - root - 2019-11-03 23:00:29.655190: step 1050, total loss = 1.76, predict loss = 0.36 (79.8 examples/sec; 0.050 sec/batch; 83h:32m:02s remains)
INFO - root - 2019-11-03 23:00:30.303757: step 1060, total loss = 0.87, predict loss = 0.18 (63.1 examples/sec; 0.063 sec/batch; 105h:41m:18s remains)
INFO - root - 2019-11-03 23:00:30.961182: step 1070, total loss = 1.58, predict loss = 0.32 (79.6 examples/sec; 0.050 sec/batch; 83h:42m:32s remains)
INFO - root - 2019-11-03 23:00:31.675529: step 1080, total loss = 1.13, predict loss = 0.22 (61.1 examples/sec; 0.065 sec/batch; 109h:04m:00s remains)
INFO - root - 2019-11-03 23:00:32.338575: step 1090, total loss = 1.11, predict loss = 0.26 (67.9 examples/sec; 0.059 sec/batch; 98h:12m:15s remains)
INFO - root - 2019-11-03 23:00:32.990798: step 1100, total loss = 1.36, predict loss = 0.30 (68.1 examples/sec; 0.059 sec/batch; 97h:51m:17s remains)
INFO - root - 2019-11-03 23:00:33.605499: step 1110, total loss = 0.93, predict loss = 0.22 (76.3 examples/sec; 0.052 sec/batch; 87h:19m:21s remains)
INFO - root - 2019-11-03 23:00:34.259425: step 1120, total loss = 1.98, predict loss = 0.47 (65.5 examples/sec; 0.061 sec/batch; 101h:46m:57s remains)
INFO - root - 2019-11-03 23:00:34.911415: step 1130, total loss = 1.34, predict loss = 0.41 (69.6 examples/sec; 0.057 sec/batch; 95h:47m:34s remains)
INFO - root - 2019-11-03 23:00:35.537591: step 1140, total loss = 1.69, predict loss = 0.37 (79.0 examples/sec; 0.051 sec/batch; 84h:24m:30s remains)
INFO - root - 2019-11-03 23:00:36.174689: step 1150, total loss = 1.61, predict loss = 0.40 (71.9 examples/sec; 0.056 sec/batch; 92h:38m:48s remains)
INFO - root - 2019-11-03 23:00:36.804473: step 1160, total loss = 1.33, predict loss = 0.30 (75.5 examples/sec; 0.053 sec/batch; 88h:13m:37s remains)
INFO - root - 2019-11-03 23:00:37.431482: step 1170, total loss = 1.63, predict loss = 0.39 (73.3 examples/sec; 0.055 sec/batch; 90h:59m:34s remains)
INFO - root - 2019-11-03 23:00:38.083318: step 1180, total loss = 1.38, predict loss = 0.32 (66.2 examples/sec; 0.060 sec/batch; 100h:38m:25s remains)
INFO - root - 2019-11-03 23:00:38.748493: step 1190, total loss = 1.64, predict loss = 0.41 (70.4 examples/sec; 0.057 sec/batch; 94h:37m:03s remains)
INFO - root - 2019-11-03 23:00:39.380647: step 1200, total loss = 1.51, predict loss = 0.37 (68.6 examples/sec; 0.058 sec/batch; 97h:10m:24s remains)
INFO - root - 2019-11-03 23:00:40.042950: step 1210, total loss = 1.35, predict loss = 0.34 (71.7 examples/sec; 0.056 sec/batch; 93h:00m:12s remains)
INFO - root - 2019-11-03 23:00:40.689371: step 1220, total loss = 1.37, predict loss = 0.30 (68.5 examples/sec; 0.058 sec/batch; 97h:19m:41s remains)
INFO - root - 2019-11-03 23:00:41.351486: step 1230, total loss = 2.25, predict loss = 0.52 (66.0 examples/sec; 0.061 sec/batch; 100h:56m:11s remains)
INFO - root - 2019-11-03 23:00:42.026279: step 1240, total loss = 1.28, predict loss = 0.31 (69.3 examples/sec; 0.058 sec/batch; 96h:10m:15s remains)
INFO - root - 2019-11-03 23:00:42.690284: step 1250, total loss = 2.00, predict loss = 0.52 (67.8 examples/sec; 0.059 sec/batch; 98h:22m:30s remains)
INFO - root - 2019-11-03 23:00:43.319138: step 1260, total loss = 1.54, predict loss = 0.35 (76.9 examples/sec; 0.052 sec/batch; 86h:39m:10s remains)
INFO - root - 2019-11-03 23:00:43.993503: step 1270, total loss = 2.38, predict loss = 0.60 (70.6 examples/sec; 0.057 sec/batch; 94h:26m:24s remains)
INFO - root - 2019-11-03 23:00:44.691422: step 1280, total loss = 1.30, predict loss = 0.29 (65.8 examples/sec; 0.061 sec/batch; 101h:21m:49s remains)
INFO - root - 2019-11-03 23:00:45.381140: step 1290, total loss = 1.28, predict loss = 0.31 (61.4 examples/sec; 0.065 sec/batch; 108h:31m:31s remains)
INFO - root - 2019-11-03 23:00:46.002728: step 1300, total loss = 1.31, predict loss = 0.29 (74.5 examples/sec; 0.054 sec/batch; 89h:26m:09s remains)
INFO - root - 2019-11-03 23:00:46.614605: step 1310, total loss = 1.45, predict loss = 0.31 (65.6 examples/sec; 0.061 sec/batch; 101h:33m:16s remains)
INFO - root - 2019-11-03 23:00:47.237688: step 1320, total loss = 1.94, predict loss = 0.55 (66.1 examples/sec; 0.061 sec/batch; 100h:53m:39s remains)
INFO - root - 2019-11-03 23:00:47.897263: step 1330, total loss = 0.94, predict loss = 0.22 (77.5 examples/sec; 0.052 sec/batch; 85h:59m:28s remains)
INFO - root - 2019-11-03 23:00:48.556238: step 1340, total loss = 0.80, predict loss = 0.19 (66.0 examples/sec; 0.061 sec/batch; 100h:57m:48s remains)
INFO - root - 2019-11-03 23:00:49.208129: step 1350, total loss = 0.73, predict loss = 0.18 (82.1 examples/sec; 0.049 sec/batch; 81h:11m:50s remains)
INFO - root - 2019-11-03 23:00:49.889856: step 1360, total loss = 0.91, predict loss = 0.23 (68.0 examples/sec; 0.059 sec/batch; 97h:59m:29s remains)
INFO - root - 2019-11-03 23:00:50.500084: step 1370, total loss = 2.38, predict loss = 0.72 (77.1 examples/sec; 0.052 sec/batch; 86h:24m:05s remains)
INFO - root - 2019-11-03 23:00:51.108083: step 1380, total loss = 1.89, predict loss = 0.50 (77.4 examples/sec; 0.052 sec/batch; 86h:08m:52s remains)
INFO - root - 2019-11-03 23:00:51.711928: step 1390, total loss = 1.30, predict loss = 0.31 (67.5 examples/sec; 0.059 sec/batch; 98h:41m:06s remains)
INFO - root - 2019-11-03 23:00:52.313192: step 1400, total loss = 1.43, predict loss = 0.36 (73.3 examples/sec; 0.055 sec/batch; 90h:58m:29s remains)
INFO - root - 2019-11-03 23:00:52.949792: step 1410, total loss = 1.45, predict loss = 0.34 (70.1 examples/sec; 0.057 sec/batch; 95h:05m:35s remains)
INFO - root - 2019-11-03 23:00:53.575930: step 1420, total loss = 1.69, predict loss = 0.40 (71.8 examples/sec; 0.056 sec/batch; 92h:51m:11s remains)
INFO - root - 2019-11-03 23:00:54.222650: step 1430, total loss = 2.20, predict loss = 0.51 (65.4 examples/sec; 0.061 sec/batch; 101h:51m:30s remains)
INFO - root - 2019-11-03 23:00:54.878649: step 1440, total loss = 1.70, predict loss = 0.43 (63.3 examples/sec; 0.063 sec/batch; 105h:14m:58s remains)
INFO - root - 2019-11-03 23:00:55.520754: step 1450, total loss = 1.73, predict loss = 0.44 (65.4 examples/sec; 0.061 sec/batch; 101h:53m:00s remains)
INFO - root - 2019-11-03 23:00:56.171724: step 1460, total loss = 1.73, predict loss = 0.41 (70.9 examples/sec; 0.056 sec/batch; 94h:01m:51s remains)
INFO - root - 2019-11-03 23:00:56.790989: step 1470, total loss = 1.64, predict loss = 0.42 (75.0 examples/sec; 0.053 sec/batch; 88h:50m:49s remains)
INFO - root - 2019-11-03 23:00:57.462957: step 1480, total loss = 1.99, predict loss = 0.49 (69.1 examples/sec; 0.058 sec/batch; 96h:26m:15s remains)
INFO - root - 2019-11-03 23:00:58.123014: step 1490, total loss = 1.85, predict loss = 0.50 (74.2 examples/sec; 0.054 sec/batch; 89h:48m:58s remains)
INFO - root - 2019-11-03 23:00:58.779040: step 1500, total loss = 2.03, predict loss = 0.56 (84.5 examples/sec; 0.047 sec/batch; 78h:50m:16s remains)
INFO - root - 2019-11-03 23:00:59.385170: step 1510, total loss = 1.36, predict loss = 0.32 (72.7 examples/sec; 0.055 sec/batch; 91h:42m:46s remains)
INFO - root - 2019-11-03 23:01:00.064270: step 1520, total loss = 1.75, predict loss = 0.44 (62.8 examples/sec; 0.064 sec/batch; 106h:09m:25s remains)
INFO - root - 2019-11-03 23:01:00.712561: step 1530, total loss = 1.40, predict loss = 0.34 (74.4 examples/sec; 0.054 sec/batch; 89h:32m:29s remains)
INFO - root - 2019-11-03 23:01:01.377358: step 1540, total loss = 1.49, predict loss = 0.38 (66.0 examples/sec; 0.061 sec/batch; 100h:57m:12s remains)
INFO - root - 2019-11-03 23:01:02.039683: step 1550, total loss = 1.47, predict loss = 0.34 (78.6 examples/sec; 0.051 sec/batch; 84h:50m:28s remains)
INFO - root - 2019-11-03 23:01:02.704524: step 1560, total loss = 1.81, predict loss = 0.47 (69.1 examples/sec; 0.058 sec/batch; 96h:29m:13s remains)
INFO - root - 2019-11-03 23:01:03.369423: step 1570, total loss = 1.23, predict loss = 0.30 (68.2 examples/sec; 0.059 sec/batch; 97h:46m:36s remains)
INFO - root - 2019-11-03 23:01:04.022494: step 1580, total loss = 1.62, predict loss = 0.49 (73.4 examples/sec; 0.054 sec/batch; 90h:48m:25s remains)
INFO - root - 2019-11-03 23:01:04.636314: step 1590, total loss = 1.37, predict loss = 0.36 (75.6 examples/sec; 0.053 sec/batch; 88h:11m:27s remains)
INFO - root - 2019-11-03 23:01:05.307811: step 1600, total loss = 1.01, predict loss = 0.26 (69.4 examples/sec; 0.058 sec/batch; 96h:00m:56s remains)
INFO - root - 2019-11-03 23:01:05.975282: step 1610, total loss = 1.37, predict loss = 0.31 (61.9 examples/sec; 0.065 sec/batch; 107h:38m:34s remains)
INFO - root - 2019-11-03 23:01:06.650255: step 1620, total loss = 1.25, predict loss = 0.29 (66.4 examples/sec; 0.060 sec/batch; 100h:23m:32s remains)
INFO - root - 2019-11-03 23:01:07.300675: step 1630, total loss = 1.51, predict loss = 0.40 (67.8 examples/sec; 0.059 sec/batch; 98h:15m:23s remains)
INFO - root - 2019-11-03 23:01:07.987783: step 1640, total loss = 1.21, predict loss = 0.27 (61.1 examples/sec; 0.065 sec/batch; 108h:59m:45s remains)
INFO - root - 2019-11-03 23:01:08.706045: step 1650, total loss = 1.61, predict loss = 0.39 (67.1 examples/sec; 0.060 sec/batch; 99h:18m:48s remains)
INFO - root - 2019-11-03 23:01:09.308773: step 1660, total loss = 1.57, predict loss = 0.40 (86.0 examples/sec; 0.047 sec/batch; 77h:28m:55s remains)
INFO - root - 2019-11-03 23:01:09.922342: step 1670, total loss = 1.04, predict loss = 0.26 (72.2 examples/sec; 0.055 sec/batch; 92h:21m:35s remains)
INFO - root - 2019-11-03 23:01:10.542148: step 1680, total loss = 1.28, predict loss = 0.39 (75.2 examples/sec; 0.053 sec/batch; 88h:37m:56s remains)
INFO - root - 2019-11-03 23:01:11.193450: step 1690, total loss = 1.14, predict loss = 0.29 (64.2 examples/sec; 0.062 sec/batch; 103h:46m:44s remains)
INFO - root - 2019-11-03 23:01:11.859533: step 1700, total loss = 1.13, predict loss = 0.29 (71.3 examples/sec; 0.056 sec/batch; 93h:29m:12s remains)
INFO - root - 2019-11-03 23:01:12.514996: step 1710, total loss = 1.15, predict loss = 0.30 (72.3 examples/sec; 0.055 sec/batch; 92h:10m:57s remains)
INFO - root - 2019-11-03 23:01:13.168610: step 1720, total loss = 1.00, predict loss = 0.26 (65.4 examples/sec; 0.061 sec/batch; 101h:51m:29s remains)
INFO - root - 2019-11-03 23:01:13.749934: step 1730, total loss = 1.11, predict loss = 0.24 (81.5 examples/sec; 0.049 sec/batch; 81h:45m:08s remains)
INFO - root - 2019-11-03 23:01:14.396327: step 1740, total loss = 1.18, predict loss = 0.29 (73.2 examples/sec; 0.055 sec/batch; 91h:06m:09s remains)
INFO - root - 2019-11-03 23:01:15.048168: step 1750, total loss = 1.09, predict loss = 0.25 (67.5 examples/sec; 0.059 sec/batch; 98h:45m:24s remains)
INFO - root - 2019-11-03 23:01:15.685048: step 1760, total loss = 1.11, predict loss = 0.27 (70.4 examples/sec; 0.057 sec/batch; 94h:42m:08s remains)
INFO - root - 2019-11-03 23:01:16.351273: step 1770, total loss = 0.90, predict loss = 0.22 (79.8 examples/sec; 0.050 sec/batch; 83h:29m:16s remains)
INFO - root - 2019-11-03 23:01:16.980525: step 1780, total loss = 1.01, predict loss = 0.27 (81.6 examples/sec; 0.049 sec/batch; 81h:41m:58s remains)
INFO - root - 2019-11-03 23:01:17.659051: step 1790, total loss = 0.95, predict loss = 0.24 (63.5 examples/sec; 0.063 sec/batch; 104h:53m:51s remains)
INFO - root - 2019-11-03 23:01:18.311561: step 1800, total loss = 1.21, predict loss = 0.31 (67.6 examples/sec; 0.059 sec/batch; 98h:33m:30s remains)
INFO - root - 2019-11-03 23:01:19.058340: step 1810, total loss = 1.53, predict loss = 0.36 (56.5 examples/sec; 0.071 sec/batch; 117h:58m:39s remains)
INFO - root - 2019-11-03 23:01:19.727501: step 1820, total loss = 1.00, predict loss = 0.24 (65.9 examples/sec; 0.061 sec/batch; 101h:04m:18s remains)
INFO - root - 2019-11-03 23:01:20.375452: step 1830, total loss = 1.55, predict loss = 0.42 (70.2 examples/sec; 0.057 sec/batch; 94h:55m:23s remains)
INFO - root - 2019-11-03 23:01:21.058433: step 1840, total loss = 1.31, predict loss = 0.35 (64.2 examples/sec; 0.062 sec/batch; 103h:49m:49s remains)
INFO - root - 2019-11-03 23:01:21.741732: step 1850, total loss = 1.22, predict loss = 0.33 (74.3 examples/sec; 0.054 sec/batch; 89h:38m:31s remains)
INFO - root - 2019-11-03 23:01:22.526347: step 1860, total loss = 1.64, predict loss = 0.44 (64.3 examples/sec; 0.062 sec/batch; 103h:43m:35s remains)
INFO - root - 2019-11-03 23:01:23.176636: step 1870, total loss = 1.72, predict loss = 0.43 (63.6 examples/sec; 0.063 sec/batch; 104h:50m:04s remains)
INFO - root - 2019-11-03 23:01:23.875058: step 1880, total loss = 1.30, predict loss = 0.34 (70.4 examples/sec; 0.057 sec/batch; 94h:36m:59s remains)
INFO - root - 2019-11-03 23:01:24.480372: step 1890, total loss = 1.50, predict loss = 0.37 (75.1 examples/sec; 0.053 sec/batch; 88h:47m:30s remains)
INFO - root - 2019-11-03 23:01:25.128534: step 1900, total loss = 1.43, predict loss = 0.40 (69.6 examples/sec; 0.057 sec/batch; 95h:45m:44s remains)
INFO - root - 2019-11-03 23:01:25.803917: step 1910, total loss = 1.63, predict loss = 0.42 (66.1 examples/sec; 0.061 sec/batch; 100h:48m:46s remains)
INFO - root - 2019-11-03 23:01:26.490570: step 1920, total loss = 1.57, predict loss = 0.41 (65.3 examples/sec; 0.061 sec/batch; 102h:03m:30s remains)
INFO - root - 2019-11-03 23:01:27.130446: step 1930, total loss = 1.69, predict loss = 0.45 (66.7 examples/sec; 0.060 sec/batch; 99h:54m:07s remains)
INFO - root - 2019-11-03 23:01:27.765044: step 1940, total loss = 1.27, predict loss = 0.30 (65.5 examples/sec; 0.061 sec/batch; 101h:48m:21s remains)
INFO - root - 2019-11-03 23:01:28.388465: step 1950, total loss = 1.30, predict loss = 0.32 (76.3 examples/sec; 0.052 sec/batch; 87h:21m:17s remains)
INFO - root - 2019-11-03 23:01:28.986983: step 1960, total loss = 1.57, predict loss = 0.39 (74.1 examples/sec; 0.054 sec/batch; 89h:58m:26s remains)
INFO - root - 2019-11-03 23:01:29.635443: step 1970, total loss = 1.07, predict loss = 0.28 (56.9 examples/sec; 0.070 sec/batch; 117h:10m:55s remains)
INFO - root - 2019-11-03 23:01:30.352867: step 1980, total loss = 1.31, predict loss = 0.33 (56.3 examples/sec; 0.071 sec/batch; 118h:21m:11s remains)
INFO - root - 2019-11-03 23:01:31.021491: step 1990, total loss = 1.01, predict loss = 0.25 (70.0 examples/sec; 0.057 sec/batch; 95h:08m:42s remains)
INFO - root - 2019-11-03 23:01:31.675063: step 2000, total loss = 1.28, predict loss = 0.37 (65.7 examples/sec; 0.061 sec/batch; 101h:28m:53s remains)
INFO - root - 2019-11-03 23:01:32.319265: step 2010, total loss = 1.19, predict loss = 0.29 (69.8 examples/sec; 0.057 sec/batch; 95h:32m:31s remains)
INFO - root - 2019-11-03 23:01:32.985506: step 2020, total loss = 1.03, predict loss = 0.26 (69.9 examples/sec; 0.057 sec/batch; 95h:19m:58s remains)
INFO - root - 2019-11-03 23:01:33.612700: step 2030, total loss = 1.44, predict loss = 0.37 (75.6 examples/sec; 0.053 sec/batch; 88h:07m:10s remains)
INFO - root - 2019-11-03 23:01:34.273048: step 2040, total loss = 1.38, predict loss = 0.36 (70.6 examples/sec; 0.057 sec/batch; 94h:26m:08s remains)
INFO - root - 2019-11-03 23:01:34.916866: step 2050, total loss = 1.33, predict loss = 0.33 (70.5 examples/sec; 0.057 sec/batch; 94h:31m:13s remains)
INFO - root - 2019-11-03 23:01:35.558284: step 2060, total loss = 1.58, predict loss = 0.39 (71.2 examples/sec; 0.056 sec/batch; 93h:36m:58s remains)
INFO - root - 2019-11-03 23:01:36.193924: step 2070, total loss = 1.33, predict loss = 0.33 (66.2 examples/sec; 0.060 sec/batch; 100h:40m:15s remains)
INFO - root - 2019-11-03 23:01:36.805162: step 2080, total loss = 1.21, predict loss = 0.30 (76.2 examples/sec; 0.052 sec/batch; 87h:27m:39s remains)
INFO - root - 2019-11-03 23:01:37.414190: step 2090, total loss = 1.46, predict loss = 0.37 (74.0 examples/sec; 0.054 sec/batch; 90h:02m:16s remains)
INFO - root - 2019-11-03 23:01:38.077396: step 2100, total loss = 1.23, predict loss = 0.31 (72.2 examples/sec; 0.055 sec/batch; 92h:18m:55s remains)
INFO - root - 2019-11-03 23:01:38.729563: step 2110, total loss = 1.24, predict loss = 0.28 (76.6 examples/sec; 0.052 sec/batch; 87h:00m:40s remains)
INFO - root - 2019-11-03 23:01:39.330834: step 2120, total loss = 1.18, predict loss = 0.31 (74.9 examples/sec; 0.053 sec/batch; 89h:01m:27s remains)
INFO - root - 2019-11-03 23:01:39.942866: step 2130, total loss = 1.48, predict loss = 0.40 (71.7 examples/sec; 0.056 sec/batch; 92h:57m:52s remains)
INFO - root - 2019-11-03 23:01:40.568285: step 2140, total loss = 1.14, predict loss = 0.29 (72.8 examples/sec; 0.055 sec/batch; 91h:30m:12s remains)
INFO - root - 2019-11-03 23:01:41.245272: step 2150, total loss = 1.17, predict loss = 0.28 (72.2 examples/sec; 0.055 sec/batch; 92h:21m:59s remains)
INFO - root - 2019-11-03 23:01:41.882931: step 2160, total loss = 1.16, predict loss = 0.29 (64.8 examples/sec; 0.062 sec/batch; 102h:52m:31s remains)
INFO - root - 2019-11-03 23:01:42.574988: step 2170, total loss = 0.96, predict loss = 0.22 (69.0 examples/sec; 0.058 sec/batch; 96h:37m:27s remains)
INFO - root - 2019-11-03 23:01:43.196995: step 2180, total loss = 1.19, predict loss = 0.30 (72.2 examples/sec; 0.055 sec/batch; 92h:15m:48s remains)
INFO - root - 2019-11-03 23:01:43.815142: step 2190, total loss = 1.10, predict loss = 0.24 (75.3 examples/sec; 0.053 sec/batch; 88h:30m:35s remains)
INFO - root - 2019-11-03 23:01:44.473636: step 2200, total loss = 1.08, predict loss = 0.27 (64.5 examples/sec; 0.062 sec/batch; 103h:15m:45s remains)
INFO - root - 2019-11-03 23:01:45.178880: step 2210, total loss = 0.99, predict loss = 0.25 (65.1 examples/sec; 0.061 sec/batch; 102h:19m:46s remains)
INFO - root - 2019-11-03 23:01:45.828425: step 2220, total loss = 1.17, predict loss = 0.28 (68.7 examples/sec; 0.058 sec/batch; 96h:57m:00s remains)
INFO - root - 2019-11-03 23:01:46.455234: step 2230, total loss = 1.00, predict loss = 0.25 (79.7 examples/sec; 0.050 sec/batch; 83h:40m:00s remains)
INFO - root - 2019-11-03 23:01:47.073787: step 2240, total loss = 1.01, predict loss = 0.25 (75.0 examples/sec; 0.053 sec/batch; 88h:48m:20s remains)
INFO - root - 2019-11-03 23:01:47.701354: step 2250, total loss = 1.08, predict loss = 0.27 (79.7 examples/sec; 0.050 sec/batch; 83h:35m:21s remains)
INFO - root - 2019-11-03 23:01:48.350703: step 2260, total loss = 1.02, predict loss = 0.22 (74.1 examples/sec; 0.054 sec/batch; 89h:53m:09s remains)
INFO - root - 2019-11-03 23:01:48.997706: step 2270, total loss = 0.87, predict loss = 0.20 (72.7 examples/sec; 0.055 sec/batch; 91h:42m:08s remains)
INFO - root - 2019-11-03 23:01:49.621366: step 2280, total loss = 1.11, predict loss = 0.26 (72.1 examples/sec; 0.055 sec/batch; 92h:21m:56s remains)
INFO - root - 2019-11-03 23:01:50.259009: step 2290, total loss = 0.90, predict loss = 0.19 (82.6 examples/sec; 0.048 sec/batch; 80h:41m:01s remains)
INFO - root - 2019-11-03 23:01:50.898827: step 2300, total loss = 1.03, predict loss = 0.24 (68.7 examples/sec; 0.058 sec/batch; 96h:57m:16s remains)
INFO - root - 2019-11-03 23:01:51.552076: step 2310, total loss = 1.16, predict loss = 0.28 (66.0 examples/sec; 0.061 sec/batch; 100h:54m:00s remains)
INFO - root - 2019-11-03 23:01:52.184513: step 2320, total loss = 1.15, predict loss = 0.27 (67.1 examples/sec; 0.060 sec/batch; 99h:16m:30s remains)
INFO - root - 2019-11-03 23:01:52.783062: step 2330, total loss = 1.19, predict loss = 0.27 (74.9 examples/sec; 0.053 sec/batch; 88h:58m:34s remains)
INFO - root - 2019-11-03 23:01:53.414056: step 2340, total loss = 1.11, predict loss = 0.25 (69.6 examples/sec; 0.057 sec/batch; 95h:44m:31s remains)
INFO - root - 2019-11-03 23:01:54.022823: step 2350, total loss = 1.19, predict loss = 0.27 (67.2 examples/sec; 0.059 sec/batch; 99h:05m:57s remains)
INFO - root - 2019-11-03 23:01:54.665091: step 2360, total loss = 1.11, predict loss = 0.24 (73.3 examples/sec; 0.055 sec/batch; 90h:53m:00s remains)
INFO - root - 2019-11-03 23:01:55.321066: step 2370, total loss = 1.12, predict loss = 0.28 (63.1 examples/sec; 0.063 sec/batch; 105h:37m:18s remains)
INFO - root - 2019-11-03 23:01:55.967559: step 2380, total loss = 1.31, predict loss = 0.31 (64.0 examples/sec; 0.062 sec/batch; 104h:03m:15s remains)
INFO - root - 2019-11-03 23:01:56.590894: step 2390, total loss = 1.06, predict loss = 0.25 (71.4 examples/sec; 0.056 sec/batch; 93h:21m:52s remains)
INFO - root - 2019-11-03 23:01:57.233856: step 2400, total loss = 1.26, predict loss = 0.29 (62.5 examples/sec; 0.064 sec/batch; 106h:32m:29s remains)
INFO - root - 2019-11-03 23:01:57.869732: step 2410, total loss = 1.29, predict loss = 0.34 (70.9 examples/sec; 0.056 sec/batch; 94h:02m:33s remains)
INFO - root - 2019-11-03 23:01:58.515714: step 2420, total loss = 1.11, predict loss = 0.29 (69.6 examples/sec; 0.057 sec/batch; 95h:46m:11s remains)
INFO - root - 2019-11-03 23:01:59.153997: step 2430, total loss = 1.81, predict loss = 0.47 (73.2 examples/sec; 0.055 sec/batch; 91h:04m:17s remains)
INFO - root - 2019-11-03 23:01:59.834340: step 2440, total loss = 1.35, predict loss = 0.33 (65.6 examples/sec; 0.061 sec/batch; 101h:31m:15s remains)
INFO - root - 2019-11-03 23:02:00.449041: step 2450, total loss = 0.95, predict loss = 0.23 (77.5 examples/sec; 0.052 sec/batch; 85h:56m:01s remains)
INFO - root - 2019-11-03 23:02:01.082415: step 2460, total loss = 0.88, predict loss = 0.20 (76.7 examples/sec; 0.052 sec/batch; 86h:54m:20s remains)
INFO - root - 2019-11-03 23:02:01.741514: step 2470, total loss = 1.15, predict loss = 0.26 (74.9 examples/sec; 0.053 sec/batch; 88h:55m:16s remains)
INFO - root - 2019-11-03 23:02:02.371349: step 2480, total loss = 1.22, predict loss = 0.28 (64.5 examples/sec; 0.062 sec/batch; 103h:20m:18s remains)
INFO - root - 2019-11-03 23:02:03.055988: step 2490, total loss = 1.31, predict loss = 0.34 (61.7 examples/sec; 0.065 sec/batch; 107h:58m:57s remains)
INFO - root - 2019-11-03 23:02:03.687886: step 2500, total loss = 1.47, predict loss = 0.37 (71.9 examples/sec; 0.056 sec/batch; 92h:40m:06s remains)
INFO - root - 2019-11-03 23:02:04.411604: step 2510, total loss = 1.33, predict loss = 0.33 (57.3 examples/sec; 0.070 sec/batch; 116h:16m:19s remains)
INFO - root - 2019-11-03 23:02:05.053066: step 2520, total loss = 1.38, predict loss = 0.34 (71.1 examples/sec; 0.056 sec/batch; 93h:39m:48s remains)
INFO - root - 2019-11-03 23:02:05.675509: step 2530, total loss = 1.38, predict loss = 0.35 (77.7 examples/sec; 0.051 sec/batch; 85h:42m:49s remains)
INFO - root - 2019-11-03 23:02:06.304885: step 2540, total loss = 1.47, predict loss = 0.38 (67.6 examples/sec; 0.059 sec/batch; 98h:37m:38s remains)
INFO - root - 2019-11-03 23:02:06.923569: step 2550, total loss = 1.64, predict loss = 0.37 (79.1 examples/sec; 0.051 sec/batch; 84h:13m:26s remains)
INFO - root - 2019-11-03 23:02:07.567646: step 2560, total loss = 1.46, predict loss = 0.36 (65.9 examples/sec; 0.061 sec/batch; 101h:08m:12s remains)
INFO - root - 2019-11-03 23:02:08.232684: step 2570, total loss = 0.92, predict loss = 0.24 (65.7 examples/sec; 0.061 sec/batch; 101h:25m:53s remains)
INFO - root - 2019-11-03 23:02:08.924017: step 2580, total loss = 1.16, predict loss = 0.30 (69.5 examples/sec; 0.058 sec/batch; 95h:51m:02s remains)
INFO - root - 2019-11-03 23:02:09.637930: step 2590, total loss = 0.94, predict loss = 0.24 (73.1 examples/sec; 0.055 sec/batch; 91h:06m:09s remains)
INFO - root - 2019-11-03 23:02:10.364963: step 2600, total loss = 1.16, predict loss = 0.31 (64.8 examples/sec; 0.062 sec/batch; 102h:51m:07s remains)
INFO - root - 2019-11-03 23:02:10.999398: step 2610, total loss = 1.27, predict loss = 0.32 (67.9 examples/sec; 0.059 sec/batch; 98h:10m:18s remains)
INFO - root - 2019-11-03 23:02:11.655792: step 2620, total loss = 1.52, predict loss = 0.41 (70.9 examples/sec; 0.056 sec/batch; 94h:00m:29s remains)
INFO - root - 2019-11-03 23:02:12.278864: step 2630, total loss = 1.27, predict loss = 0.31 (65.9 examples/sec; 0.061 sec/batch; 101h:06m:15s remains)
INFO - root - 2019-11-03 23:02:12.915386: step 2640, total loss = 1.19, predict loss = 0.27 (70.6 examples/sec; 0.057 sec/batch; 94h:23m:48s remains)
INFO - root - 2019-11-03 23:02:13.567609: step 2650, total loss = 1.33, predict loss = 0.35 (66.5 examples/sec; 0.060 sec/batch; 100h:09m:15s remains)
INFO - root - 2019-11-03 23:02:14.228955: step 2660, total loss = 1.11, predict loss = 0.29 (63.2 examples/sec; 0.063 sec/batch; 105h:28m:31s remains)
INFO - root - 2019-11-03 23:02:14.884089: step 2670, total loss = 1.25, predict loss = 0.32 (66.7 examples/sec; 0.060 sec/batch; 99h:58m:33s remains)
INFO - root - 2019-11-03 23:02:15.542278: step 2680, total loss = 1.14, predict loss = 0.27 (68.5 examples/sec; 0.058 sec/batch; 97h:16m:28s remains)
INFO - root - 2019-11-03 23:02:16.165154: step 2690, total loss = 1.29, predict loss = 0.33 (70.6 examples/sec; 0.057 sec/batch; 94h:22m:45s remains)
INFO - root - 2019-11-03 23:02:16.784691: step 2700, total loss = 1.09, predict loss = 0.25 (61.5 examples/sec; 0.065 sec/batch; 108h:21m:19s remains)
INFO - root - 2019-11-03 23:02:17.313862: step 2710, total loss = 1.48, predict loss = 0.30 (86.1 examples/sec; 0.046 sec/batch; 77h:26m:07s remains)
INFO - root - 2019-11-03 23:02:17.773262: step 2720, total loss = 1.40, predict loss = 0.29 (101.0 examples/sec; 0.040 sec/batch; 66h:00m:33s remains)
INFO - root - 2019-11-03 23:02:18.819438: step 2730, total loss = 0.99, predict loss = 0.19 (75.6 examples/sec; 0.053 sec/batch; 88h:10m:41s remains)
INFO - root - 2019-11-03 23:02:19.398240: step 2740, total loss = 0.89, predict loss = 0.22 (78.9 examples/sec; 0.051 sec/batch; 84h:30m:06s remains)
INFO - root - 2019-11-03 23:02:20.009374: step 2750, total loss = 2.10, predict loss = 0.50 (75.9 examples/sec; 0.053 sec/batch; 87h:49m:52s remains)
INFO - root - 2019-11-03 23:02:20.624264: step 2760, total loss = 1.30, predict loss = 0.33 (80.6 examples/sec; 0.050 sec/batch; 82h:43m:32s remains)
INFO - root - 2019-11-03 23:02:21.251616: step 2770, total loss = 1.40, predict loss = 0.33 (68.4 examples/sec; 0.059 sec/batch; 97h:28m:35s remains)
INFO - root - 2019-11-03 23:02:21.875197: step 2780, total loss = 1.41, predict loss = 0.47 (78.6 examples/sec; 0.051 sec/batch; 84h:44m:02s remains)
INFO - root - 2019-11-03 23:02:22.518818: step 2790, total loss = 1.52, predict loss = 0.38 (72.7 examples/sec; 0.055 sec/batch; 91h:40m:21s remains)
INFO - root - 2019-11-03 23:02:23.181992: step 2800, total loss = 1.30, predict loss = 0.28 (68.5 examples/sec; 0.058 sec/batch; 97h:12m:54s remains)
INFO - root - 2019-11-03 23:02:23.858444: step 2810, total loss = 1.61, predict loss = 0.46 (69.9 examples/sec; 0.057 sec/batch; 95h:23m:27s remains)
INFO - root - 2019-11-03 23:02:24.490150: step 2820, total loss = 1.45, predict loss = 0.35 (71.1 examples/sec; 0.056 sec/batch; 93h:40m:51s remains)
INFO - root - 2019-11-03 23:02:25.107396: step 2830, total loss = 1.54, predict loss = 0.37 (70.5 examples/sec; 0.057 sec/batch; 94h:34m:41s remains)
INFO - root - 2019-11-03 23:02:25.759585: step 2840, total loss = 1.46, predict loss = 0.35 (68.3 examples/sec; 0.059 sec/batch; 97h:33m:33s remains)
INFO - root - 2019-11-03 23:02:26.370408: step 2850, total loss = 1.66, predict loss = 0.34 (73.9 examples/sec; 0.054 sec/batch; 90h:12m:44s remains)
INFO - root - 2019-11-03 23:02:26.965017: step 2860, total loss = 1.26, predict loss = 0.31 (71.8 examples/sec; 0.056 sec/batch; 92h:46m:43s remains)
INFO - root - 2019-11-03 23:02:27.601998: step 2870, total loss = 1.06, predict loss = 0.27 (63.1 examples/sec; 0.063 sec/batch; 105h:34m:52s remains)
INFO - root - 2019-11-03 23:02:28.233761: step 2880, total loss = 1.19, predict loss = 0.29 (67.9 examples/sec; 0.059 sec/batch; 98h:10m:24s remains)
INFO - root - 2019-11-03 23:02:28.847765: step 2890, total loss = 1.48, predict loss = 0.36 (67.1 examples/sec; 0.060 sec/batch; 99h:14m:18s remains)
INFO - root - 2019-11-03 23:02:29.490436: step 2900, total loss = 1.16, predict loss = 0.27 (66.8 examples/sec; 0.060 sec/batch; 99h:49m:18s remains)
INFO - root - 2019-11-03 23:02:30.106248: step 2910, total loss = 1.71, predict loss = 0.42 (73.8 examples/sec; 0.054 sec/batch; 90h:20m:20s remains)
INFO - root - 2019-11-03 23:02:30.727818: step 2920, total loss = 1.10, predict loss = 0.26 (73.0 examples/sec; 0.055 sec/batch; 91h:19m:16s remains)
INFO - root - 2019-11-03 23:02:31.339590: step 2930, total loss = 1.56, predict loss = 0.44 (69.8 examples/sec; 0.057 sec/batch; 95h:29m:09s remains)
INFO - root - 2019-11-03 23:02:31.952079: step 2940, total loss = 0.99, predict loss = 0.25 (69.5 examples/sec; 0.058 sec/batch; 95h:53m:50s remains)
INFO - root - 2019-11-03 23:02:32.566597: step 2950, total loss = 1.32, predict loss = 0.32 (83.5 examples/sec; 0.048 sec/batch; 79h:50m:31s remains)
INFO - root - 2019-11-03 23:02:33.226952: step 2960, total loss = 1.06, predict loss = 0.24 (63.5 examples/sec; 0.063 sec/batch; 104h:53m:57s remains)
INFO - root - 2019-11-03 23:02:33.882903: step 2970, total loss = 1.24, predict loss = 0.29 (74.6 examples/sec; 0.054 sec/batch; 89h:19m:28s remains)
INFO - root - 2019-11-03 23:02:34.511247: step 2980, total loss = 1.11, predict loss = 0.25 (72.7 examples/sec; 0.055 sec/batch; 91h:41m:25s remains)
INFO - root - 2019-11-03 23:02:35.121966: step 2990, total loss = 1.02, predict loss = 0.24 (72.9 examples/sec; 0.055 sec/batch; 91h:25m:56s remains)
INFO - root - 2019-11-03 23:02:35.756691: step 3000, total loss = 1.09, predict loss = 0.27 (70.6 examples/sec; 0.057 sec/batch; 94h:21m:55s remains)
INFO - root - 2019-11-03 23:02:36.378491: step 3010, total loss = 1.34, predict loss = 0.29 (67.7 examples/sec; 0.059 sec/batch; 98h:22m:09s remains)
INFO - root - 2019-11-03 23:02:37.003058: step 3020, total loss = 1.30, predict loss = 0.34 (69.7 examples/sec; 0.057 sec/batch; 95h:38m:36s remains)
INFO - root - 2019-11-03 23:02:37.604754: step 3030, total loss = 1.42, predict loss = 0.35 (82.3 examples/sec; 0.049 sec/batch; 80h:57m:21s remains)
INFO - root - 2019-11-03 23:02:38.221638: step 3040, total loss = 1.75, predict loss = 0.42 (77.9 examples/sec; 0.051 sec/batch; 85h:31m:25s remains)
INFO - root - 2019-11-03 23:02:38.852755: step 3050, total loss = 1.43, predict loss = 0.35 (73.8 examples/sec; 0.054 sec/batch; 90h:16m:35s remains)
INFO - root - 2019-11-03 23:02:39.509978: step 3060, total loss = 1.33, predict loss = 0.30 (67.4 examples/sec; 0.059 sec/batch; 98h:54m:02s remains)
INFO - root - 2019-11-03 23:02:40.152139: step 3070, total loss = 1.34, predict loss = 0.29 (82.1 examples/sec; 0.049 sec/batch; 81h:09m:32s remains)
INFO - root - 2019-11-03 23:02:40.820756: step 3080, total loss = 1.04, predict loss = 0.25 (64.2 examples/sec; 0.062 sec/batch; 103h:51m:52s remains)
INFO - root - 2019-11-03 23:02:41.493262: step 3090, total loss = 1.21, predict loss = 0.30 (70.1 examples/sec; 0.057 sec/batch; 95h:04m:13s remains)
INFO - root - 2019-11-03 23:02:42.153158: step 3100, total loss = 1.29, predict loss = 0.33 (67.0 examples/sec; 0.060 sec/batch; 99h:23m:00s remains)
INFO - root - 2019-11-03 23:02:42.739306: step 3110, total loss = 1.27, predict loss = 0.28 (73.0 examples/sec; 0.055 sec/batch; 91h:18m:59s remains)
INFO - root - 2019-11-03 23:02:43.349310: step 3120, total loss = 1.04, predict loss = 0.25 (65.1 examples/sec; 0.061 sec/batch; 102h:24m:51s remains)
INFO - root - 2019-11-03 23:02:43.975862: step 3130, total loss = 1.02, predict loss = 0.23 (72.6 examples/sec; 0.055 sec/batch; 91h:43m:38s remains)
INFO - root - 2019-11-03 23:02:44.597601: step 3140, total loss = 1.69, predict loss = 0.48 (70.5 examples/sec; 0.057 sec/batch; 94h:27m:44s remains)
INFO - root - 2019-11-03 23:02:45.219512: step 3150, total loss = 2.74, predict loss = 0.76 (71.9 examples/sec; 0.056 sec/batch; 92h:38m:14s remains)
INFO - root - 2019-11-03 23:02:45.846839: step 3160, total loss = 1.60, predict loss = 0.39 (66.2 examples/sec; 0.060 sec/batch; 100h:36m:19s remains)
INFO - root - 2019-11-03 23:02:46.496294: step 3170, total loss = 1.15, predict loss = 0.28 (70.1 examples/sec; 0.057 sec/batch; 95h:00m:51s remains)
INFO - root - 2019-11-03 23:02:47.182325: step 3180, total loss = 1.29, predict loss = 0.30 (58.8 examples/sec; 0.068 sec/batch; 113h:22m:17s remains)
INFO - root - 2019-11-03 23:02:47.878543: step 3190, total loss = 1.34, predict loss = 0.33 (60.2 examples/sec; 0.066 sec/batch; 110h:45m:17s remains)
INFO - root - 2019-11-03 23:02:48.511018: step 3200, total loss = 1.14, predict loss = 0.27 (78.4 examples/sec; 0.051 sec/batch; 84h:58m:47s remains)
INFO - root - 2019-11-03 23:02:49.182647: step 3210, total loss = 1.25, predict loss = 0.34 (77.2 examples/sec; 0.052 sec/batch; 86h:18m:23s remains)
INFO - root - 2019-11-03 23:02:49.858619: step 3220, total loss = 1.41, predict loss = 0.37 (57.2 examples/sec; 0.070 sec/batch; 116h:24m:20s remains)
INFO - root - 2019-11-03 23:02:50.543800: step 3230, total loss = 1.38, predict loss = 0.34 (72.0 examples/sec; 0.056 sec/batch; 92h:33m:56s remains)
INFO - root - 2019-11-03 23:02:51.171246: step 3240, total loss = 1.05, predict loss = 0.26 (68.9 examples/sec; 0.058 sec/batch; 96h:43m:07s remains)
INFO - root - 2019-11-03 23:02:51.812432: step 3250, total loss = 1.18, predict loss = 0.29 (67.5 examples/sec; 0.059 sec/batch; 98h:45m:25s remains)
INFO - root - 2019-11-03 23:02:52.444650: step 3260, total loss = 1.08, predict loss = 0.25 (72.0 examples/sec; 0.056 sec/batch; 92h:36m:19s remains)
INFO - root - 2019-11-03 23:02:53.093591: step 3270, total loss = 1.02, predict loss = 0.25 (72.4 examples/sec; 0.055 sec/batch; 92h:02m:44s remains)
INFO - root - 2019-11-03 23:02:53.691393: step 3280, total loss = 0.92, predict loss = 0.22 (72.9 examples/sec; 0.055 sec/batch; 91h:20m:34s remains)
INFO - root - 2019-11-03 23:02:54.343430: step 3290, total loss = 1.33, predict loss = 0.35 (69.6 examples/sec; 0.057 sec/batch; 95h:41m:07s remains)
INFO - root - 2019-11-03 23:02:54.978599: step 3300, total loss = 0.88, predict loss = 0.20 (62.1 examples/sec; 0.064 sec/batch; 107h:22m:51s remains)
INFO - root - 2019-11-03 23:02:55.636073: step 3310, total loss = 1.27, predict loss = 0.29 (63.9 examples/sec; 0.063 sec/batch; 104h:11m:27s remains)
INFO - root - 2019-11-03 23:02:56.305470: step 3320, total loss = 1.04, predict loss = 0.23 (65.9 examples/sec; 0.061 sec/batch; 101h:04m:28s remains)
INFO - root - 2019-11-03 23:02:56.910028: step 3330, total loss = 0.97, predict loss = 0.22 (65.4 examples/sec; 0.061 sec/batch; 101h:52m:06s remains)
INFO - root - 2019-11-03 23:02:57.532024: step 3340, total loss = 1.15, predict loss = 0.32 (77.6 examples/sec; 0.052 sec/batch; 85h:52m:18s remains)
INFO - root - 2019-11-03 23:02:58.155851: step 3350, total loss = 0.79, predict loss = 0.16 (69.7 examples/sec; 0.057 sec/batch; 95h:35m:07s remains)
INFO - root - 2019-11-03 23:02:58.790052: step 3360, total loss = 0.85, predict loss = 0.20 (73.5 examples/sec; 0.054 sec/batch; 90h:42m:05s remains)
INFO - root - 2019-11-03 23:02:59.417047: step 3370, total loss = 1.00, predict loss = 0.19 (68.6 examples/sec; 0.058 sec/batch; 97h:09m:02s remains)
INFO - root - 2019-11-03 23:03:00.041705: step 3380, total loss = 1.32, predict loss = 0.29 (72.0 examples/sec; 0.056 sec/batch; 92h:28m:48s remains)
INFO - root - 2019-11-03 23:03:00.671523: step 3390, total loss = 0.96, predict loss = 0.23 (73.5 examples/sec; 0.054 sec/batch; 90h:40m:57s remains)
INFO - root - 2019-11-03 23:03:01.301663: step 3400, total loss = 0.91, predict loss = 0.20 (70.8 examples/sec; 0.056 sec/batch; 94h:03m:30s remains)
INFO - root - 2019-11-03 23:03:01.941992: step 3410, total loss = 1.15, predict loss = 0.27 (67.3 examples/sec; 0.059 sec/batch; 98h:56m:06s remains)
INFO - root - 2019-11-03 23:03:02.575254: step 3420, total loss = 1.39, predict loss = 0.35 (75.9 examples/sec; 0.053 sec/batch; 87h:49m:35s remains)
INFO - root - 2019-11-03 23:03:03.191443: step 3430, total loss = 1.50, predict loss = 0.36 (71.4 examples/sec; 0.056 sec/batch; 93h:20m:59s remains)
INFO - root - 2019-11-03 23:03:03.779602: step 3440, total loss = 1.43, predict loss = 0.33 (73.9 examples/sec; 0.054 sec/batch; 90h:12m:48s remains)
INFO - root - 2019-11-03 23:03:04.384603: step 3450, total loss = 1.53, predict loss = 0.38 (68.5 examples/sec; 0.058 sec/batch; 97h:16m:42s remains)
INFO - root - 2019-11-03 23:03:05.094376: step 3460, total loss = 1.50, predict loss = 0.38 (72.1 examples/sec; 0.055 sec/batch; 92h:22m:53s remains)
INFO - root - 2019-11-03 23:03:05.723096: step 3470, total loss = 1.26, predict loss = 0.30 (79.1 examples/sec; 0.051 sec/batch; 84h:14m:26s remains)
INFO - root - 2019-11-03 23:03:06.334858: step 3480, total loss = 1.44, predict loss = 0.31 (76.0 examples/sec; 0.053 sec/batch; 87h:41m:52s remains)
INFO - root - 2019-11-03 23:03:06.954022: step 3490, total loss = 1.18, predict loss = 0.27 (72.0 examples/sec; 0.056 sec/batch; 92h:33m:15s remains)
INFO - root - 2019-11-03 23:03:07.620494: step 3500, total loss = 1.45, predict loss = 0.40 (64.3 examples/sec; 0.062 sec/batch; 103h:32m:25s remains)
INFO - root - 2019-11-03 23:03:08.269210: step 3510, total loss = 1.16, predict loss = 0.30 (71.1 examples/sec; 0.056 sec/batch; 93h:42m:03s remains)
INFO - root - 2019-11-03 23:03:08.924367: step 3520, total loss = 1.00, predict loss = 0.24 (61.5 examples/sec; 0.065 sec/batch; 108h:19m:25s remains)
INFO - root - 2019-11-03 23:03:09.583694: step 3530, total loss = 1.34, predict loss = 0.31 (66.0 examples/sec; 0.061 sec/batch; 101h:00m:29s remains)
INFO - root - 2019-11-03 23:03:10.229478: step 3540, total loss = 1.17, predict loss = 0.30 (61.9 examples/sec; 0.065 sec/batch; 107h:38m:44s remains)
INFO - root - 2019-11-03 23:03:10.875476: step 3550, total loss = 1.20, predict loss = 0.28 (61.6 examples/sec; 0.065 sec/batch; 108h:14m:55s remains)
INFO - root - 2019-11-03 23:03:11.534545: step 3560, total loss = 1.01, predict loss = 0.26 (72.2 examples/sec; 0.055 sec/batch; 92h:14m:30s remains)
INFO - root - 2019-11-03 23:03:12.159382: step 3570, total loss = 0.85, predict loss = 0.17 (67.0 examples/sec; 0.060 sec/batch; 99h:30m:48s remains)
INFO - root - 2019-11-03 23:03:12.793641: step 3580, total loss = 0.62, predict loss = 0.15 (65.9 examples/sec; 0.061 sec/batch; 101h:02m:35s remains)
INFO - root - 2019-11-03 23:03:13.433385: step 3590, total loss = 0.83, predict loss = 0.20 (65.9 examples/sec; 0.061 sec/batch; 101h:09m:57s remains)
INFO - root - 2019-11-03 23:03:14.077245: step 3600, total loss = 0.68, predict loss = 0.15 (66.2 examples/sec; 0.060 sec/batch; 100h:35m:58s remains)
INFO - root - 2019-11-03 23:03:14.716260: step 3610, total loss = 0.80, predict loss = 0.18 (66.1 examples/sec; 0.061 sec/batch; 100h:50m:36s remains)
INFO - root - 2019-11-03 23:03:15.371706: step 3620, total loss = 0.60, predict loss = 0.13 (66.4 examples/sec; 0.060 sec/batch; 100h:19m:19s remains)
INFO - root - 2019-11-03 23:03:15.981593: step 3630, total loss = 0.97, predict loss = 0.21 (75.3 examples/sec; 0.053 sec/batch; 88h:27m:46s remains)
INFO - root - 2019-11-03 23:03:16.609879: step 3640, total loss = 1.64, predict loss = 0.42 (72.3 examples/sec; 0.055 sec/batch; 92h:08m:43s remains)
INFO - root - 2019-11-03 23:03:17.207196: step 3650, total loss = 1.05, predict loss = 0.25 (71.4 examples/sec; 0.056 sec/batch; 93h:19m:29s remains)
INFO - root - 2019-11-03 23:03:17.843522: step 3660, total loss = 1.07, predict loss = 0.23 (65.3 examples/sec; 0.061 sec/batch; 102h:03m:57s remains)
INFO - root - 2019-11-03 23:03:18.480946: step 3670, total loss = 1.17, predict loss = 0.29 (73.8 examples/sec; 0.054 sec/batch; 90h:15m:20s remains)
INFO - root - 2019-11-03 23:03:19.120295: step 3680, total loss = 0.92, predict loss = 0.22 (68.8 examples/sec; 0.058 sec/batch; 96h:47m:26s remains)
INFO - root - 2019-11-03 23:03:19.863225: step 3690, total loss = 0.96, predict loss = 0.21 (56.9 examples/sec; 0.070 sec/batch; 117h:03m:22s remains)
INFO - root - 2019-11-03 23:03:20.664444: step 3700, total loss = 0.84, predict loss = 0.21 (66.0 examples/sec; 0.061 sec/batch; 100h:56m:53s remains)
INFO - root - 2019-11-03 23:03:21.333624: step 3710, total loss = 0.81, predict loss = 0.20 (70.8 examples/sec; 0.056 sec/batch; 94h:05m:51s remains)
INFO - root - 2019-11-03 23:03:21.928903: step 3720, total loss = 1.04, predict loss = 0.27 (73.1 examples/sec; 0.055 sec/batch; 91h:11m:08s remains)
INFO - root - 2019-11-03 23:03:22.536786: step 3730, total loss = 1.00, predict loss = 0.26 (74.1 examples/sec; 0.054 sec/batch; 89h:52m:32s remains)
INFO - root - 2019-11-03 23:03:23.146568: step 3740, total loss = 1.02, predict loss = 0.26 (70.1 examples/sec; 0.057 sec/batch; 95h:02m:49s remains)
INFO - root - 2019-11-03 23:03:23.753241: step 3750, total loss = 1.29, predict loss = 0.30 (67.3 examples/sec; 0.059 sec/batch; 98h:57m:36s remains)
INFO - root - 2019-11-03 23:03:24.400109: step 3760, total loss = 1.06, predict loss = 0.25 (66.0 examples/sec; 0.061 sec/batch; 100h:54m:32s remains)
INFO - root - 2019-11-03 23:03:25.096023: step 3770, total loss = 1.42, predict loss = 0.37 (77.5 examples/sec; 0.052 sec/batch; 86h:00m:09s remains)
INFO - root - 2019-11-03 23:03:25.828684: step 3780, total loss = 0.81, predict loss = 0.19 (52.8 examples/sec; 0.076 sec/batch; 126h:08m:38s remains)
INFO - root - 2019-11-03 23:03:26.479015: step 3790, total loss = 1.16, predict loss = 0.32 (68.8 examples/sec; 0.058 sec/batch; 96h:51m:45s remains)
INFO - root - 2019-11-03 23:03:27.104540: step 3800, total loss = 0.97, predict loss = 0.22 (68.5 examples/sec; 0.058 sec/batch; 97h:14m:06s remains)
INFO - root - 2019-11-03 23:03:27.794174: step 3810, total loss = 0.69, predict loss = 0.20 (75.5 examples/sec; 0.053 sec/batch; 88h:14m:19s remains)
INFO - root - 2019-11-03 23:03:28.452215: step 3820, total loss = 0.94, predict loss = 0.22 (67.3 examples/sec; 0.059 sec/batch; 98h:59m:09s remains)
INFO - root - 2019-11-03 23:03:29.091980: step 3830, total loss = 0.58, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 92h:45m:31s remains)
INFO - root - 2019-11-03 23:03:29.739724: step 3840, total loss = 0.89, predict loss = 0.21 (67.9 examples/sec; 0.059 sec/batch; 98h:07m:18s remains)
INFO - root - 2019-11-03 23:03:30.438379: step 3850, total loss = 0.91, predict loss = 0.22 (61.8 examples/sec; 0.065 sec/batch; 107h:51m:47s remains)
INFO - root - 2019-11-03 23:03:31.231487: step 3860, total loss = 0.82, predict loss = 0.18 (46.7 examples/sec; 0.086 sec/batch; 142h:31m:08s remains)
INFO - root - 2019-11-03 23:03:32.156454: step 3870, total loss = 0.79, predict loss = 0.17 (49.7 examples/sec; 0.080 sec/batch; 133h:57m:13s remains)
INFO - root - 2019-11-03 23:03:33.036179: step 3880, total loss = 1.19, predict loss = 0.30 (58.6 examples/sec; 0.068 sec/batch; 113h:46m:47s remains)
INFO - root - 2019-11-03 23:03:33.850250: step 3890, total loss = 1.11, predict loss = 0.27 (65.3 examples/sec; 0.061 sec/batch; 102h:05m:20s remains)
INFO - root - 2019-11-03 23:03:34.550909: step 3900, total loss = 1.24, predict loss = 0.30 (74.5 examples/sec; 0.054 sec/batch; 89h:28m:10s remains)
INFO - root - 2019-11-03 23:03:35.199254: step 3910, total loss = 0.91, predict loss = 0.21 (67.9 examples/sec; 0.059 sec/batch; 98h:11m:23s remains)
INFO - root - 2019-11-03 23:03:35.815909: step 3920, total loss = 1.07, predict loss = 0.24 (75.5 examples/sec; 0.053 sec/batch; 88h:15m:14s remains)
INFO - root - 2019-11-03 23:03:36.443197: step 3930, total loss = 0.91, predict loss = 0.19 (69.0 examples/sec; 0.058 sec/batch; 96h:32m:39s remains)
INFO - root - 2019-11-03 23:03:37.113942: step 3940, total loss = 1.02, predict loss = 0.26 (59.0 examples/sec; 0.068 sec/batch; 112h:58m:21s remains)
INFO - root - 2019-11-03 23:03:37.805871: step 3950, total loss = 0.73, predict loss = 0.17 (72.5 examples/sec; 0.055 sec/batch; 91h:55m:12s remains)
INFO - root - 2019-11-03 23:03:38.390924: step 3960, total loss = 1.33, predict loss = 0.40 (71.1 examples/sec; 0.056 sec/batch; 93h:43m:22s remains)
INFO - root - 2019-11-03 23:03:39.064227: step 3970, total loss = 1.30, predict loss = 0.31 (60.7 examples/sec; 0.066 sec/batch; 109h:48m:24s remains)
INFO - root - 2019-11-03 23:03:39.677684: step 3980, total loss = 0.95, predict loss = 0.21 (77.3 examples/sec; 0.052 sec/batch; 86h:13m:48s remains)
INFO - root - 2019-11-03 23:03:40.386754: step 3990, total loss = 1.09, predict loss = 0.24 (55.1 examples/sec; 0.073 sec/batch; 120h:54m:22s remains)
INFO - root - 2019-11-03 23:03:41.013304: step 4000, total loss = 1.18, predict loss = 0.28 (70.4 examples/sec; 0.057 sec/batch; 94h:34m:09s remains)
INFO - root - 2019-11-03 23:03:41.623606: step 4010, total loss = 1.00, predict loss = 0.23 (77.7 examples/sec; 0.051 sec/batch; 85h:41m:38s remains)
INFO - root - 2019-11-03 23:03:42.295622: step 4020, total loss = 0.92, predict loss = 0.21 (78.1 examples/sec; 0.051 sec/batch; 85h:17m:07s remains)
INFO - root - 2019-11-03 23:03:42.898655: step 4030, total loss = 0.95, predict loss = 0.20 (71.5 examples/sec; 0.056 sec/batch; 93h:10m:06s remains)
INFO - root - 2019-11-03 23:03:43.565250: step 4040, total loss = 1.08, predict loss = 0.25 (73.7 examples/sec; 0.054 sec/batch; 90h:23m:40s remains)
INFO - root - 2019-11-03 23:03:44.195046: step 4050, total loss = 1.08, predict loss = 0.24 (66.4 examples/sec; 0.060 sec/batch; 100h:24m:30s remains)
INFO - root - 2019-11-03 23:03:44.819056: step 4060, total loss = 0.88, predict loss = 0.19 (70.3 examples/sec; 0.057 sec/batch; 94h:45m:33s remains)
INFO - root - 2019-11-03 23:03:45.468465: step 4070, total loss = 0.83, predict loss = 0.20 (70.8 examples/sec; 0.057 sec/batch; 94h:09m:44s remains)
INFO - root - 2019-11-03 23:03:46.125379: step 4080, total loss = 0.69, predict loss = 0.14 (66.5 examples/sec; 0.060 sec/batch; 100h:07m:28s remains)
INFO - root - 2019-11-03 23:03:46.739522: step 4090, total loss = 1.08, predict loss = 0.24 (67.3 examples/sec; 0.059 sec/batch; 99h:03m:04s remains)
INFO - root - 2019-11-03 23:03:47.355356: step 4100, total loss = 0.90, predict loss = 0.23 (65.4 examples/sec; 0.061 sec/batch; 101h:48m:09s remains)
INFO - root - 2019-11-03 23:03:48.019631: step 4110, total loss = 1.22, predict loss = 0.30 (66.5 examples/sec; 0.060 sec/batch; 100h:11m:17s remains)
INFO - root - 2019-11-03 23:03:48.729937: step 4120, total loss = 0.93, predict loss = 0.22 (57.3 examples/sec; 0.070 sec/batch; 116h:15m:07s remains)
INFO - root - 2019-11-03 23:03:49.298753: step 4130, total loss = 1.02, predict loss = 0.25 (76.8 examples/sec; 0.052 sec/batch; 86h:41m:52s remains)
INFO - root - 2019-11-03 23:03:49.900093: step 4140, total loss = 1.73, predict loss = 0.45 (72.1 examples/sec; 0.055 sec/batch; 92h:23m:33s remains)
INFO - root - 2019-11-03 23:03:50.555796: step 4150, total loss = 1.26, predict loss = 0.32 (65.7 examples/sec; 0.061 sec/batch; 101h:25m:27s remains)
INFO - root - 2019-11-03 23:03:51.158607: step 4160, total loss = 1.54, predict loss = 0.36 (68.7 examples/sec; 0.058 sec/batch; 96h:54m:27s remains)
INFO - root - 2019-11-03 23:03:51.739242: step 4170, total loss = 2.43, predict loss = 0.62 (75.8 examples/sec; 0.053 sec/batch; 87h:53m:43s remains)
INFO - root - 2019-11-03 23:03:52.356177: step 4180, total loss = 1.60, predict loss = 0.41 (74.7 examples/sec; 0.054 sec/batch; 89h:10m:37s remains)
INFO - root - 2019-11-03 23:03:52.989912: step 4190, total loss = 1.37, predict loss = 0.36 (64.8 examples/sec; 0.062 sec/batch; 102h:48m:25s remains)
INFO - root - 2019-11-03 23:03:53.637423: step 4200, total loss = 1.53, predict loss = 0.37 (73.1 examples/sec; 0.055 sec/batch; 91h:11m:09s remains)
INFO - root - 2019-11-03 23:03:54.287130: step 4210, total loss = 1.71, predict loss = 0.49 (72.1 examples/sec; 0.055 sec/batch; 92h:25m:47s remains)
INFO - root - 2019-11-03 23:03:54.931456: step 4220, total loss = 1.30, predict loss = 0.32 (73.3 examples/sec; 0.055 sec/batch; 90h:52m:55s remains)
INFO - root - 2019-11-03 23:03:55.531605: step 4230, total loss = 1.00, predict loss = 0.23 (68.9 examples/sec; 0.058 sec/batch; 96h:45m:01s remains)
INFO - root - 2019-11-03 23:03:56.128144: step 4240, total loss = 1.07, predict loss = 0.25 (64.8 examples/sec; 0.062 sec/batch; 102h:44m:17s remains)
INFO - root - 2019-11-03 23:03:56.722636: step 4250, total loss = 1.30, predict loss = 0.32 (68.0 examples/sec; 0.059 sec/batch; 97h:56m:21s remains)
INFO - root - 2019-11-03 23:03:57.350820: step 4260, total loss = 1.13, predict loss = 0.26 (66.8 examples/sec; 0.060 sec/batch; 99h:44m:56s remains)
INFO - root - 2019-11-03 23:03:57.951431: step 4270, total loss = 1.27, predict loss = 0.28 (67.3 examples/sec; 0.059 sec/batch; 98h:57m:38s remains)
INFO - root - 2019-11-03 23:03:58.613459: step 4280, total loss = 1.02, predict loss = 0.24 (65.7 examples/sec; 0.061 sec/batch; 101h:26m:33s remains)
INFO - root - 2019-11-03 23:03:59.235535: step 4290, total loss = 1.03, predict loss = 0.24 (71.5 examples/sec; 0.056 sec/batch; 93h:14m:18s remains)
INFO - root - 2019-11-03 23:03:59.893735: step 4300, total loss = 1.13, predict loss = 0.26 (67.5 examples/sec; 0.059 sec/batch; 98h:39m:07s remains)
INFO - root - 2019-11-03 23:04:00.561441: step 4310, total loss = 0.89, predict loss = 0.21 (61.8 examples/sec; 0.065 sec/batch; 107h:52m:53s remains)
INFO - root - 2019-11-03 23:04:01.249074: step 4320, total loss = 1.00, predict loss = 0.22 (67.2 examples/sec; 0.060 sec/batch; 99h:08m:35s remains)
INFO - root - 2019-11-03 23:04:01.894706: step 4330, total loss = 1.15, predict loss = 0.29 (68.4 examples/sec; 0.059 sec/batch; 97h:27m:45s remains)
INFO - root - 2019-11-03 23:04:02.590677: step 4340, total loss = 0.92, predict loss = 0.23 (57.3 examples/sec; 0.070 sec/batch; 116h:11m:14s remains)
INFO - root - 2019-11-03 23:04:03.280154: step 4350, total loss = 1.17, predict loss = 0.27 (48.8 examples/sec; 0.082 sec/batch; 136h:36m:07s remains)
INFO - root - 2019-11-03 23:04:03.950368: step 4360, total loss = 1.02, predict loss = 0.24 (60.5 examples/sec; 0.066 sec/batch; 110h:07m:35s remains)
INFO - root - 2019-11-03 23:04:04.652116: step 4370, total loss = 0.83, predict loss = 0.20 (64.5 examples/sec; 0.062 sec/batch; 103h:14m:22s remains)
INFO - root - 2019-11-03 23:04:05.307863: step 4380, total loss = 1.03, predict loss = 0.24 (67.0 examples/sec; 0.060 sec/batch; 99h:28m:32s remains)
INFO - root - 2019-11-03 23:04:05.898142: step 4390, total loss = 0.96, predict loss = 0.24 (73.7 examples/sec; 0.054 sec/batch; 90h:20m:51s remains)
INFO - root - 2019-11-03 23:04:06.518964: step 4400, total loss = 1.01, predict loss = 0.26 (71.9 examples/sec; 0.056 sec/batch; 92h:36m:24s remains)
INFO - root - 2019-11-03 23:04:07.143604: step 4410, total loss = 0.86, predict loss = 0.21 (75.8 examples/sec; 0.053 sec/batch; 87h:51m:25s remains)
INFO - root - 2019-11-03 23:04:07.770143: step 4420, total loss = 0.87, predict loss = 0.20 (71.9 examples/sec; 0.056 sec/batch; 92h:36m:49s remains)
INFO - root - 2019-11-03 23:04:08.402539: step 4430, total loss = 1.07, predict loss = 0.27 (66.8 examples/sec; 0.060 sec/batch; 99h:47m:43s remains)
INFO - root - 2019-11-03 23:04:09.055992: step 4440, total loss = 0.77, predict loss = 0.19 (74.0 examples/sec; 0.054 sec/batch; 90h:02m:24s remains)
INFO - root - 2019-11-03 23:04:09.708305: step 4450, total loss = 0.92, predict loss = 0.24 (69.1 examples/sec; 0.058 sec/batch; 96h:24m:07s remains)
INFO - root - 2019-11-03 23:04:10.384433: step 4460, total loss = 1.22, predict loss = 0.31 (68.2 examples/sec; 0.059 sec/batch; 97h:44m:08s remains)
INFO - root - 2019-11-03 23:04:10.996876: step 4470, total loss = 0.78, predict loss = 0.18 (76.5 examples/sec; 0.052 sec/batch; 87h:07m:47s remains)
INFO - root - 2019-11-03 23:04:11.608723: step 4480, total loss = 0.85, predict loss = 0.20 (73.0 examples/sec; 0.055 sec/batch; 91h:18m:27s remains)
INFO - root - 2019-11-03 23:04:12.211929: step 4490, total loss = 0.73, predict loss = 0.17 (83.2 examples/sec; 0.048 sec/batch; 80h:04m:46s remains)
INFO - root - 2019-11-03 23:04:12.822045: step 4500, total loss = 0.69, predict loss = 0.16 (71.7 examples/sec; 0.056 sec/batch; 92h:52m:05s remains)
INFO - root - 2019-11-03 23:04:13.436160: step 4510, total loss = 1.19, predict loss = 0.28 (67.7 examples/sec; 0.059 sec/batch; 98h:20m:59s remains)
INFO - root - 2019-11-03 23:04:14.052896: step 4520, total loss = 0.96, predict loss = 0.22 (74.3 examples/sec; 0.054 sec/batch; 89h:42m:41s remains)
INFO - root - 2019-11-03 23:04:14.697669: step 4530, total loss = 0.90, predict loss = 0.23 (71.9 examples/sec; 0.056 sec/batch; 92h:41m:26s remains)
INFO - root - 2019-11-03 23:04:15.430825: step 4540, total loss = 0.98, predict loss = 0.25 (69.0 examples/sec; 0.058 sec/batch; 96h:32m:07s remains)
INFO - root - 2019-11-03 23:04:16.538354: step 4550, total loss = 1.12, predict loss = 0.26 (69.9 examples/sec; 0.057 sec/batch; 95h:19m:13s remains)
INFO - root - 2019-11-03 23:04:17.159788: step 4560, total loss = 0.97, predict loss = 0.24 (67.1 examples/sec; 0.060 sec/batch; 99h:12m:43s remains)
INFO - root - 2019-11-03 23:04:17.818040: step 4570, total loss = 1.05, predict loss = 0.25 (64.3 examples/sec; 0.062 sec/batch; 103h:35m:33s remains)
INFO - root - 2019-11-03 23:04:18.482118: step 4580, total loss = 1.15, predict loss = 0.30 (73.0 examples/sec; 0.055 sec/batch; 91h:13m:00s remains)
INFO - root - 2019-11-03 23:04:19.150875: step 4590, total loss = 0.81, predict loss = 0.19 (57.1 examples/sec; 0.070 sec/batch; 116h:34m:49s remains)
INFO - root - 2019-11-03 23:04:19.882010: step 4600, total loss = 1.42, predict loss = 0.37 (60.0 examples/sec; 0.067 sec/batch; 110h:56m:59s remains)
INFO - root - 2019-11-03 23:04:20.505062: step 4610, total loss = 1.42, predict loss = 0.37 (69.8 examples/sec; 0.057 sec/batch; 95h:28m:16s remains)
INFO - root - 2019-11-03 23:04:21.148170: step 4620, total loss = 1.24, predict loss = 0.30 (65.0 examples/sec; 0.062 sec/batch; 102h:29m:39s remains)
INFO - root - 2019-11-03 23:04:21.828364: step 4630, total loss = 0.97, predict loss = 0.24 (66.1 examples/sec; 0.061 sec/batch; 100h:48m:55s remains)
INFO - root - 2019-11-03 23:04:22.481553: step 4640, total loss = 1.05, predict loss = 0.24 (71.8 examples/sec; 0.056 sec/batch; 92h:42m:54s remains)
INFO - root - 2019-11-03 23:04:23.109772: step 4650, total loss = 1.22, predict loss = 0.28 (67.7 examples/sec; 0.059 sec/batch; 98h:21m:23s remains)
INFO - root - 2019-11-03 23:04:23.712171: step 4660, total loss = 0.89, predict loss = 0.22 (68.7 examples/sec; 0.058 sec/batch; 97h:01m:56s remains)
INFO - root - 2019-11-03 23:04:24.368705: step 4670, total loss = 0.80, predict loss = 0.18 (71.8 examples/sec; 0.056 sec/batch; 92h:45m:58s remains)
INFO - root - 2019-11-03 23:04:25.016834: step 4680, total loss = 1.01, predict loss = 0.26 (70.1 examples/sec; 0.057 sec/batch; 95h:05m:31s remains)
INFO - root - 2019-11-03 23:04:25.639876: step 4690, total loss = 1.01, predict loss = 0.24 (67.3 examples/sec; 0.059 sec/batch; 99h:00m:00s remains)
INFO - root - 2019-11-03 23:04:26.258490: step 4700, total loss = 0.75, predict loss = 0.18 (72.7 examples/sec; 0.055 sec/batch; 91h:39m:53s remains)
INFO - root - 2019-11-03 23:04:26.899995: step 4710, total loss = 1.04, predict loss = 0.28 (72.5 examples/sec; 0.055 sec/batch; 91h:52m:34s remains)
INFO - root - 2019-11-03 23:04:27.576941: step 4720, total loss = 0.75, predict loss = 0.19 (70.8 examples/sec; 0.057 sec/batch; 94h:08m:17s remains)
INFO - root - 2019-11-03 23:04:28.184931: step 4730, total loss = 0.90, predict loss = 0.21 (72.7 examples/sec; 0.055 sec/batch; 91h:34m:00s remains)
INFO - root - 2019-11-03 23:04:28.812820: step 4740, total loss = 0.90, predict loss = 0.23 (69.4 examples/sec; 0.058 sec/batch; 96h:00m:10s remains)
INFO - root - 2019-11-03 23:04:29.461175: step 4750, total loss = 1.05, predict loss = 0.26 (76.6 examples/sec; 0.052 sec/batch; 86h:56m:46s remains)
INFO - root - 2019-11-03 23:04:30.098543: step 4760, total loss = 0.82, predict loss = 0.18 (67.9 examples/sec; 0.059 sec/batch; 98h:05m:16s remains)
INFO - root - 2019-11-03 23:04:30.769569: step 4770, total loss = 0.99, predict loss = 0.25 (74.1 examples/sec; 0.054 sec/batch; 89h:53m:54s remains)
INFO - root - 2019-11-03 23:04:31.388919: step 4780, total loss = 0.78, predict loss = 0.19 (71.8 examples/sec; 0.056 sec/batch; 92h:46m:55s remains)
INFO - root - 2019-11-03 23:04:32.016820: step 4790, total loss = 1.18, predict loss = 0.29 (68.1 examples/sec; 0.059 sec/batch; 97h:51m:35s remains)
INFO - root - 2019-11-03 23:04:32.679339: step 4800, total loss = 0.90, predict loss = 0.21 (68.0 examples/sec; 0.059 sec/batch; 97h:56m:50s remains)
INFO - root - 2019-11-03 23:04:33.368715: step 4810, total loss = 0.83, predict loss = 0.20 (74.8 examples/sec; 0.053 sec/batch; 89h:01m:55s remains)
INFO - root - 2019-11-03 23:04:33.975915: step 4820, total loss = 1.14, predict loss = 0.28 (72.5 examples/sec; 0.055 sec/batch; 91h:50m:55s remains)
INFO - root - 2019-11-03 23:04:34.572269: step 4830, total loss = 0.98, predict loss = 0.24 (74.4 examples/sec; 0.054 sec/batch; 89h:34m:13s remains)
INFO - root - 2019-11-03 23:04:35.237710: step 4840, total loss = 1.05, predict loss = 0.25 (66.7 examples/sec; 0.060 sec/batch; 99h:56m:39s remains)
INFO - root - 2019-11-03 23:04:35.915620: step 4850, total loss = 1.04, predict loss = 0.27 (64.8 examples/sec; 0.062 sec/batch; 102h:51m:23s remains)
INFO - root - 2019-11-03 23:04:36.544710: step 4860, total loss = 0.96, predict loss = 0.25 (81.6 examples/sec; 0.049 sec/batch; 81h:35m:52s remains)
INFO - root - 2019-11-03 23:04:37.158983: step 4870, total loss = 0.91, predict loss = 0.22 (69.9 examples/sec; 0.057 sec/batch; 95h:19m:07s remains)
INFO - root - 2019-11-03 23:04:37.792704: step 4880, total loss = 1.08, predict loss = 0.26 (64.5 examples/sec; 0.062 sec/batch; 103h:12m:39s remains)
INFO - root - 2019-11-03 23:04:38.435100: step 4890, total loss = 0.96, predict loss = 0.22 (74.4 examples/sec; 0.054 sec/batch; 89h:32m:46s remains)
INFO - root - 2019-11-03 23:04:39.056533: step 4900, total loss = 0.78, predict loss = 0.20 (70.2 examples/sec; 0.057 sec/batch; 94h:50m:00s remains)
INFO - root - 2019-11-03 23:04:39.690975: step 4910, total loss = 0.99, predict loss = 0.23 (62.1 examples/sec; 0.064 sec/batch; 107h:13m:04s remains)
INFO - root - 2019-11-03 23:04:40.306956: step 4920, total loss = 0.79, predict loss = 0.19 (77.8 examples/sec; 0.051 sec/batch; 85h:34m:06s remains)
INFO - root - 2019-11-03 23:04:40.982971: step 4930, total loss = 1.12, predict loss = 0.27 (66.4 examples/sec; 0.060 sec/batch; 100h:20m:26s remains)
INFO - root - 2019-11-03 23:04:41.594306: step 4940, total loss = 0.91, predict loss = 0.22 (82.5 examples/sec; 0.048 sec/batch; 80h:44m:52s remains)
INFO - root - 2019-11-03 23:04:42.208240: step 4950, total loss = 0.85, predict loss = 0.20 (68.2 examples/sec; 0.059 sec/batch; 97h:42m:08s remains)
INFO - root - 2019-11-03 23:04:42.840657: step 4960, total loss = 1.14, predict loss = 0.28 (71.3 examples/sec; 0.056 sec/batch; 93h:26m:15s remains)
INFO - root - 2019-11-03 23:04:43.469871: step 4970, total loss = 0.95, predict loss = 0.22 (62.2 examples/sec; 0.064 sec/batch; 107h:08m:16s remains)
INFO - root - 2019-11-03 23:04:44.063658: step 4980, total loss = 0.88, predict loss = 0.20 (76.9 examples/sec; 0.052 sec/batch; 86h:37m:57s remains)
INFO - root - 2019-11-03 23:04:44.682947: step 4990, total loss = 0.98, predict loss = 0.22 (80.2 examples/sec; 0.050 sec/batch; 83h:05m:39s remains)
INFO - root - 2019-11-03 23:04:45.315143: step 5000, total loss = 0.74, predict loss = 0.17 (65.0 examples/sec; 0.062 sec/batch; 102h:28m:23s remains)
INFO - root - 2019-11-03 23:04:45.962585: step 5010, total loss = 0.85, predict loss = 0.20 (68.8 examples/sec; 0.058 sec/batch; 96h:51m:44s remains)
INFO - root - 2019-11-03 23:04:46.623525: step 5020, total loss = 0.89, predict loss = 0.20 (60.1 examples/sec; 0.067 sec/batch; 110h:55m:03s remains)
INFO - root - 2019-11-03 23:04:47.261633: step 5030, total loss = 0.97, predict loss = 0.22 (79.2 examples/sec; 0.050 sec/batch; 84h:04m:46s remains)
INFO - root - 2019-11-03 23:04:47.890065: step 5040, total loss = 1.15, predict loss = 0.30 (76.6 examples/sec; 0.052 sec/batch; 87h:00m:30s remains)
INFO - root - 2019-11-03 23:04:48.506835: step 5050, total loss = 0.94, predict loss = 0.22 (67.2 examples/sec; 0.060 sec/batch; 99h:07m:38s remains)
INFO - root - 2019-11-03 23:04:49.187005: step 5060, total loss = 1.42, predict loss = 0.34 (60.6 examples/sec; 0.066 sec/batch; 109h:58m:54s remains)
INFO - root - 2019-11-03 23:04:49.906552: step 5070, total loss = 0.92, predict loss = 0.21 (62.4 examples/sec; 0.064 sec/batch; 106h:49m:56s remains)
INFO - root - 2019-11-03 23:04:50.592432: step 5080, total loss = 0.95, predict loss = 0.24 (75.6 examples/sec; 0.053 sec/batch; 88h:07m:33s remains)
INFO - root - 2019-11-03 23:04:51.268297: step 5090, total loss = 0.93, predict loss = 0.20 (73.6 examples/sec; 0.054 sec/batch; 90h:33m:09s remains)
INFO - root - 2019-11-03 23:04:51.878306: step 5100, total loss = 0.92, predict loss = 0.22 (65.3 examples/sec; 0.061 sec/batch; 101h:57m:10s remains)
INFO - root - 2019-11-03 23:04:52.544617: step 5110, total loss = 1.09, predict loss = 0.26 (65.3 examples/sec; 0.061 sec/batch; 101h:59m:51s remains)
INFO - root - 2019-11-03 23:04:53.242184: step 5120, total loss = 1.23, predict loss = 0.30 (58.1 examples/sec; 0.069 sec/batch; 114h:42m:39s remains)
INFO - root - 2019-11-03 23:04:53.916633: step 5130, total loss = 0.86, predict loss = 0.23 (75.9 examples/sec; 0.053 sec/batch; 87h:42m:49s remains)
INFO - root - 2019-11-03 23:04:54.532098: step 5140, total loss = 1.00, predict loss = 0.24 (78.3 examples/sec; 0.051 sec/batch; 85h:06m:33s remains)
INFO - root - 2019-11-03 23:04:55.153973: step 5150, total loss = 1.71, predict loss = 0.41 (77.7 examples/sec; 0.051 sec/batch; 85h:45m:05s remains)
INFO - root - 2019-11-03 23:04:55.748901: step 5160, total loss = 1.12, predict loss = 0.25 (72.5 examples/sec; 0.055 sec/batch; 91h:52m:14s remains)
INFO - root - 2019-11-03 23:04:56.373263: step 5170, total loss = 1.09, predict loss = 0.27 (67.5 examples/sec; 0.059 sec/batch; 98h:38m:34s remains)
INFO - root - 2019-11-03 23:04:56.993503: step 5180, total loss = 0.96, predict loss = 0.24 (80.4 examples/sec; 0.050 sec/batch; 82h:53m:22s remains)
INFO - root - 2019-11-03 23:04:57.631862: step 5190, total loss = 0.96, predict loss = 0.23 (64.5 examples/sec; 0.062 sec/batch; 103h:17m:02s remains)
INFO - root - 2019-11-03 23:04:58.277748: step 5200, total loss = 1.01, predict loss = 0.24 (65.9 examples/sec; 0.061 sec/batch; 101h:06m:54s remains)
INFO - root - 2019-11-03 23:04:58.980913: step 5210, total loss = 1.14, predict loss = 0.28 (59.6 examples/sec; 0.067 sec/batch; 111h:51m:10s remains)
INFO - root - 2019-11-03 23:04:59.626698: step 5220, total loss = 1.03, predict loss = 0.25 (66.5 examples/sec; 0.060 sec/batch; 100h:08m:42s remains)
INFO - root - 2019-11-03 23:05:00.322336: step 5230, total loss = 1.06, predict loss = 0.24 (75.7 examples/sec; 0.053 sec/batch; 87h:58m:02s remains)
INFO - root - 2019-11-03 23:05:00.964151: step 5240, total loss = 1.03, predict loss = 0.24 (67.1 examples/sec; 0.060 sec/batch; 99h:17m:44s remains)
INFO - root - 2019-11-03 23:05:01.612170: step 5250, total loss = 0.89, predict loss = 0.21 (63.2 examples/sec; 0.063 sec/batch; 105h:28m:05s remains)
INFO - root - 2019-11-03 23:05:02.302232: step 5260, total loss = 1.12, predict loss = 0.26 (64.2 examples/sec; 0.062 sec/batch; 103h:41m:39s remains)
INFO - root - 2019-11-03 23:05:02.925074: step 5270, total loss = 0.89, predict loss = 0.21 (75.7 examples/sec; 0.053 sec/batch; 87h:59m:47s remains)
INFO - root - 2019-11-03 23:05:03.622715: step 5280, total loss = 1.17, predict loss = 0.28 (74.7 examples/sec; 0.054 sec/batch; 89h:06m:47s remains)
INFO - root - 2019-11-03 23:05:04.265572: step 5290, total loss = 1.07, predict loss = 0.26 (64.0 examples/sec; 0.062 sec/batch; 103h:59m:40s remains)
INFO - root - 2019-11-03 23:05:04.903054: step 5300, total loss = 1.45, predict loss = 0.33 (68.5 examples/sec; 0.058 sec/batch; 97h:17m:50s remains)
INFO - root - 2019-11-03 23:05:05.520007: step 5310, total loss = 1.45, predict loss = 0.38 (78.9 examples/sec; 0.051 sec/batch; 84h:25m:26s remains)
INFO - root - 2019-11-03 23:05:06.159861: step 5320, total loss = 0.98, predict loss = 0.25 (73.8 examples/sec; 0.054 sec/batch; 90h:13m:52s remains)
INFO - root - 2019-11-03 23:05:06.781765: step 5330, total loss = 0.83, predict loss = 0.20 (72.3 examples/sec; 0.055 sec/batch; 92h:04m:19s remains)
INFO - root - 2019-11-03 23:05:07.410057: step 5340, total loss = 0.83, predict loss = 0.20 (68.7 examples/sec; 0.058 sec/batch; 96h:53m:35s remains)
INFO - root - 2019-11-03 23:05:08.006037: step 5350, total loss = 1.04, predict loss = 0.26 (70.6 examples/sec; 0.057 sec/batch; 94h:22m:50s remains)
INFO - root - 2019-11-03 23:05:08.675730: step 5360, total loss = 1.24, predict loss = 0.34 (63.0 examples/sec; 0.063 sec/batch; 105h:44m:00s remains)
INFO - root - 2019-11-03 23:05:09.322283: step 5370, total loss = 1.11, predict loss = 0.27 (66.1 examples/sec; 0.061 sec/batch; 100h:45m:16s remains)
INFO - root - 2019-11-03 23:05:09.968367: step 5380, total loss = 0.95, predict loss = 0.23 (69.4 examples/sec; 0.058 sec/batch; 95h:59m:31s remains)
INFO - root - 2019-11-03 23:05:10.622794: step 5390, total loss = 1.16, predict loss = 0.28 (60.1 examples/sec; 0.067 sec/batch; 110h:51m:02s remains)
INFO - root - 2019-11-03 23:05:11.255308: step 5400, total loss = 0.91, predict loss = 0.21 (74.6 examples/sec; 0.054 sec/batch; 89h:19m:08s remains)
INFO - root - 2019-11-03 23:05:11.864135: step 5410, total loss = 0.90, predict loss = 0.20 (66.9 examples/sec; 0.060 sec/batch; 99h:33m:47s remains)
INFO - root - 2019-11-03 23:05:12.476535: step 5420, total loss = 1.01, predict loss = 0.24 (70.6 examples/sec; 0.057 sec/batch; 94h:21m:46s remains)
INFO - root - 2019-11-03 23:05:13.069895: step 5430, total loss = 1.08, predict loss = 0.27 (92.6 examples/sec; 0.043 sec/batch; 71h:57m:38s remains)
INFO - root - 2019-11-03 23:05:13.568461: step 5440, total loss = 0.75, predict loss = 0.18 (94.1 examples/sec; 0.042 sec/batch; 70h:45m:50s remains)
INFO - root - 2019-11-03 23:05:14.037087: step 5450, total loss = 0.68, predict loss = 0.15 (83.7 examples/sec; 0.048 sec/batch; 79h:33m:11s remains)
INFO - root - 2019-11-03 23:05:15.084191: step 5460, total loss = 0.69, predict loss = 0.17 (70.9 examples/sec; 0.056 sec/batch; 93h:58m:06s remains)
INFO - root - 2019-11-03 23:05:15.755390: step 5470, total loss = 1.02, predict loss = 0.23 (62.7 examples/sec; 0.064 sec/batch; 106h:17m:11s remains)
INFO - root - 2019-11-03 23:05:16.421271: step 5480, total loss = 1.44, predict loss = 0.32 (59.5 examples/sec; 0.067 sec/batch; 111h:58m:34s remains)
INFO - root - 2019-11-03 23:05:17.041343: step 5490, total loss = 0.93, predict loss = 0.21 (73.4 examples/sec; 0.054 sec/batch; 90h:41m:22s remains)
INFO - root - 2019-11-03 23:05:17.669981: step 5500, total loss = 1.33, predict loss = 0.34 (67.9 examples/sec; 0.059 sec/batch; 98h:06m:59s remains)
INFO - root - 2019-11-03 23:05:18.292867: step 5510, total loss = 1.48, predict loss = 0.33 (78.5 examples/sec; 0.051 sec/batch; 84h:50m:36s remains)
INFO - root - 2019-11-03 23:05:18.917377: step 5520, total loss = 1.13, predict loss = 0.28 (68.0 examples/sec; 0.059 sec/batch; 97h:53m:51s remains)
INFO - root - 2019-11-03 23:05:19.569404: step 5530, total loss = 1.09, predict loss = 0.24 (72.2 examples/sec; 0.055 sec/batch; 92h:18m:04s remains)
INFO - root - 2019-11-03 23:05:20.200156: step 5540, total loss = 1.41, predict loss = 0.36 (77.6 examples/sec; 0.052 sec/batch; 85h:50m:28s remains)
INFO - root - 2019-11-03 23:05:20.872178: step 5550, total loss = 1.10, predict loss = 0.27 (68.2 examples/sec; 0.059 sec/batch; 97h:38m:58s remains)
INFO - root - 2019-11-03 23:05:21.503373: step 5560, total loss = 1.38, predict loss = 0.35 (74.3 examples/sec; 0.054 sec/batch; 89h:36m:21s remains)
INFO - root - 2019-11-03 23:05:22.063014: step 5570, total loss = 1.66, predict loss = 0.38 (82.0 examples/sec; 0.049 sec/batch; 81h:14m:55s remains)
INFO - root - 2019-11-03 23:05:22.646265: step 5580, total loss = 1.67, predict loss = 0.37 (68.5 examples/sec; 0.058 sec/batch; 97h:10m:32s remains)
INFO - root - 2019-11-03 23:05:23.262789: step 5590, total loss = 1.40, predict loss = 0.28 (54.4 examples/sec; 0.074 sec/batch; 122h:24m:07s remains)
INFO - root - 2019-11-03 23:05:23.917419: step 5600, total loss = 0.94, predict loss = 0.21 (72.5 examples/sec; 0.055 sec/batch; 91h:52m:31s remains)
INFO - root - 2019-11-03 23:05:24.510451: step 5610, total loss = 1.33, predict loss = 0.33 (72.7 examples/sec; 0.055 sec/batch; 91h:37m:49s remains)
INFO - root - 2019-11-03 23:05:25.098685: step 5620, total loss = 0.97, predict loss = 0.21 (81.1 examples/sec; 0.049 sec/batch; 82h:07m:35s remains)
INFO - root - 2019-11-03 23:05:25.740189: step 5630, total loss = 1.04, predict loss = 0.24 (64.1 examples/sec; 0.062 sec/batch; 103h:50m:19s remains)
INFO - root - 2019-11-03 23:05:26.360417: step 5640, total loss = 1.03, predict loss = 0.22 (77.8 examples/sec; 0.051 sec/batch; 85h:39m:04s remains)
INFO - root - 2019-11-03 23:05:26.993435: step 5650, total loss = 1.09, predict loss = 0.30 (72.0 examples/sec; 0.056 sec/batch; 92h:31m:31s remains)
INFO - root - 2019-11-03 23:05:27.585409: step 5660, total loss = 1.06, predict loss = 0.26 (83.6 examples/sec; 0.048 sec/batch; 79h:40m:08s remains)
INFO - root - 2019-11-03 23:05:28.193645: step 5670, total loss = 0.83, predict loss = 0.22 (70.6 examples/sec; 0.057 sec/batch; 94h:21m:01s remains)
INFO - root - 2019-11-03 23:05:28.837501: step 5680, total loss = 0.79, predict loss = 0.17 (78.3 examples/sec; 0.051 sec/batch; 85h:02m:01s remains)
INFO - root - 2019-11-03 23:05:29.428915: step 5690, total loss = 1.09, predict loss = 0.27 (80.0 examples/sec; 0.050 sec/batch; 83h:17m:32s remains)
INFO - root - 2019-11-03 23:05:30.066438: step 5700, total loss = 1.31, predict loss = 0.32 (68.2 examples/sec; 0.059 sec/batch; 97h:38m:00s remains)
INFO - root - 2019-11-03 23:05:30.704613: step 5710, total loss = 1.20, predict loss = 0.28 (68.6 examples/sec; 0.058 sec/batch; 97h:04m:45s remains)
INFO - root - 2019-11-03 23:05:31.347129: step 5720, total loss = 1.34, predict loss = 0.30 (67.3 examples/sec; 0.059 sec/batch; 98h:59m:00s remains)
INFO - root - 2019-11-03 23:05:32.061814: step 5730, total loss = 1.17, predict loss = 0.28 (56.3 examples/sec; 0.071 sec/batch; 118h:19m:33s remains)
INFO - root - 2019-11-03 23:05:32.815291: step 5740, total loss = 1.12, predict loss = 0.25 (50.9 examples/sec; 0.079 sec/batch; 130h:48m:02s remains)
INFO - root - 2019-11-03 23:05:33.610112: step 5750, total loss = 1.02, predict loss = 0.24 (53.8 examples/sec; 0.074 sec/batch; 123h:51m:36s remains)
INFO - root - 2019-11-03 23:05:34.385436: step 5760, total loss = 1.81, predict loss = 0.43 (69.2 examples/sec; 0.058 sec/batch; 96h:17m:47s remains)
INFO - root - 2019-11-03 23:05:35.022641: step 5770, total loss = 1.39, predict loss = 0.33 (71.7 examples/sec; 0.056 sec/batch; 92h:56m:42s remains)
INFO - root - 2019-11-03 23:05:35.618145: step 5780, total loss = 1.08, predict loss = 0.24 (68.6 examples/sec; 0.058 sec/batch; 97h:02m:14s remains)
INFO - root - 2019-11-03 23:05:36.258979: step 5790, total loss = 1.21, predict loss = 0.30 (73.1 examples/sec; 0.055 sec/batch; 91h:04m:54s remains)
INFO - root - 2019-11-03 23:05:36.883358: step 5800, total loss = 0.78, predict loss = 0.18 (69.1 examples/sec; 0.058 sec/batch; 96h:18m:56s remains)
INFO - root - 2019-11-03 23:05:37.504331: step 5810, total loss = 1.07, predict loss = 0.24 (70.2 examples/sec; 0.057 sec/batch; 94h:52m:45s remains)
INFO - root - 2019-11-03 23:05:38.161669: step 5820, total loss = 0.98, predict loss = 0.22 (70.7 examples/sec; 0.057 sec/batch; 94h:11m:38s remains)
INFO - root - 2019-11-03 23:05:38.771677: step 5830, total loss = 0.91, predict loss = 0.20 (70.7 examples/sec; 0.057 sec/batch; 94h:08m:31s remains)
INFO - root - 2019-11-03 23:05:39.416903: step 5840, total loss = 1.32, predict loss = 0.27 (65.9 examples/sec; 0.061 sec/batch; 101h:03m:59s remains)
INFO - root - 2019-11-03 23:05:40.042234: step 5850, total loss = 0.98, predict loss = 0.22 (79.3 examples/sec; 0.050 sec/batch; 83h:59m:09s remains)
INFO - root - 2019-11-03 23:05:40.665613: step 5860, total loss = 0.94, predict loss = 0.24 (67.6 examples/sec; 0.059 sec/batch; 98h:35m:34s remains)
INFO - root - 2019-11-03 23:05:41.299217: step 5870, total loss = 1.19, predict loss = 0.26 (72.8 examples/sec; 0.055 sec/batch; 91h:27m:20s remains)
INFO - root - 2019-11-03 23:05:41.922073: step 5880, total loss = 1.44, predict loss = 0.35 (75.2 examples/sec; 0.053 sec/batch; 88h:33m:32s remains)
INFO - root - 2019-11-03 23:05:42.567457: step 5890, total loss = 1.12, predict loss = 0.28 (72.8 examples/sec; 0.055 sec/batch; 91h:32m:49s remains)
INFO - root - 2019-11-03 23:05:43.218598: step 5900, total loss = 1.23, predict loss = 0.31 (67.2 examples/sec; 0.060 sec/batch; 99h:06m:30s remains)
INFO - root - 2019-11-03 23:05:43.849522: step 5910, total loss = 0.92, predict loss = 0.23 (65.1 examples/sec; 0.061 sec/batch; 102h:18m:35s remains)
INFO - root - 2019-11-03 23:05:44.506316: step 5920, total loss = 1.10, predict loss = 0.27 (70.2 examples/sec; 0.057 sec/batch; 94h:53m:29s remains)
INFO - root - 2019-11-03 23:05:45.166347: step 5930, total loss = 1.00, predict loss = 0.23 (63.7 examples/sec; 0.063 sec/batch; 104h:30m:11s remains)
INFO - root - 2019-11-03 23:05:45.780767: step 5940, total loss = 0.88, predict loss = 0.22 (74.7 examples/sec; 0.054 sec/batch; 89h:09m:54s remains)
INFO - root - 2019-11-03 23:05:46.393834: step 5950, total loss = 1.00, predict loss = 0.23 (80.2 examples/sec; 0.050 sec/batch; 83h:00m:56s remains)
INFO - root - 2019-11-03 23:05:46.995869: step 5960, total loss = 0.82, predict loss = 0.20 (80.2 examples/sec; 0.050 sec/batch; 83h:02m:14s remains)
INFO - root - 2019-11-03 23:05:47.611704: step 5970, total loss = 1.01, predict loss = 0.23 (71.2 examples/sec; 0.056 sec/batch; 93h:34m:41s remains)
INFO - root - 2019-11-03 23:05:48.266402: step 5980, total loss = 0.97, predict loss = 0.25 (64.7 examples/sec; 0.062 sec/batch; 102h:56m:46s remains)
INFO - root - 2019-11-03 23:05:48.943673: step 5990, total loss = 1.09, predict loss = 0.25 (66.2 examples/sec; 0.060 sec/batch; 100h:39m:05s remains)
INFO - root - 2019-11-03 23:05:49.623208: step 6000, total loss = 1.03, predict loss = 0.26 (65.6 examples/sec; 0.061 sec/batch; 101h:32m:26s remains)
INFO - root - 2019-11-03 23:05:50.298160: step 6010, total loss = 1.06, predict loss = 0.23 (66.6 examples/sec; 0.060 sec/batch; 100h:00m:22s remains)
INFO - root - 2019-11-03 23:05:50.979103: step 6020, total loss = 0.62, predict loss = 0.15 (67.5 examples/sec; 0.059 sec/batch; 98h:43m:04s remains)
INFO - root - 2019-11-03 23:05:51.695766: step 6030, total loss = 0.77, predict loss = 0.20 (61.7 examples/sec; 0.065 sec/batch; 107h:59m:59s remains)
INFO - root - 2019-11-03 23:05:52.301579: step 6040, total loss = 0.69, predict loss = 0.17 (86.8 examples/sec; 0.046 sec/batch; 76h:45m:35s remains)
INFO - root - 2019-11-03 23:05:52.902191: step 6050, total loss = 0.86, predict loss = 0.19 (68.5 examples/sec; 0.058 sec/batch; 97h:12m:33s remains)
INFO - root - 2019-11-03 23:05:53.539582: step 6060, total loss = 0.57, predict loss = 0.12 (70.4 examples/sec; 0.057 sec/batch; 94h:32m:43s remains)
INFO - root - 2019-11-03 23:05:54.155040: step 6070, total loss = 0.73, predict loss = 0.18 (63.8 examples/sec; 0.063 sec/batch; 104h:20m:26s remains)
INFO - root - 2019-11-03 23:05:54.800589: step 6080, total loss = 0.69, predict loss = 0.14 (77.6 examples/sec; 0.052 sec/batch; 85h:49m:41s remains)
INFO - root - 2019-11-03 23:05:55.449843: step 6090, total loss = 0.75, predict loss = 0.16 (62.7 examples/sec; 0.064 sec/batch; 106h:11m:26s remains)
INFO - root - 2019-11-03 23:05:56.042795: step 6100, total loss = 0.87, predict loss = 0.21 (69.0 examples/sec; 0.058 sec/batch; 96h:33m:20s remains)
INFO - root - 2019-11-03 23:05:56.652522: step 6110, total loss = 1.22, predict loss = 0.27 (70.3 examples/sec; 0.057 sec/batch; 94h:42m:51s remains)
INFO - root - 2019-11-03 23:05:57.287704: step 6120, total loss = 1.11, predict loss = 0.24 (60.7 examples/sec; 0.066 sec/batch; 109h:42m:14s remains)
INFO - root - 2019-11-03 23:05:57.932392: step 6130, total loss = 0.96, predict loss = 0.24 (68.9 examples/sec; 0.058 sec/batch; 96h:36m:59s remains)
INFO - root - 2019-11-03 23:05:58.599777: step 6140, total loss = 1.16, predict loss = 0.25 (68.6 examples/sec; 0.058 sec/batch; 97h:06m:35s remains)
INFO - root - 2019-11-03 23:05:59.235582: step 6150, total loss = 1.13, predict loss = 0.26 (71.0 examples/sec; 0.056 sec/batch; 93h:45m:04s remains)
INFO - root - 2019-11-03 23:05:59.888207: step 6160, total loss = 1.05, predict loss = 0.22 (70.9 examples/sec; 0.056 sec/batch; 93h:54m:21s remains)
INFO - root - 2019-11-03 23:06:00.535763: step 6170, total loss = 1.34, predict loss = 0.33 (70.9 examples/sec; 0.056 sec/batch; 93h:57m:36s remains)
INFO - root - 2019-11-03 23:06:01.200717: step 6180, total loss = 1.07, predict loss = 0.27 (73.7 examples/sec; 0.054 sec/batch; 90h:20m:16s remains)
INFO - root - 2019-11-03 23:06:01.847040: step 6190, total loss = 1.19, predict loss = 0.28 (79.5 examples/sec; 0.050 sec/batch; 83h:43m:13s remains)
INFO - root - 2019-11-03 23:06:02.521373: step 6200, total loss = 1.02, predict loss = 0.26 (68.8 examples/sec; 0.058 sec/batch; 96h:46m:04s remains)
INFO - root - 2019-11-03 23:06:03.140492: step 6210, total loss = 1.48, predict loss = 0.35 (65.8 examples/sec; 0.061 sec/batch; 101h:09m:54s remains)
INFO - root - 2019-11-03 23:06:03.795167: step 6220, total loss = 0.99, predict loss = 0.23 (60.9 examples/sec; 0.066 sec/batch; 109h:16m:21s remains)
INFO - root - 2019-11-03 23:06:04.413668: step 6230, total loss = 0.95, predict loss = 0.23 (65.3 examples/sec; 0.061 sec/batch; 102h:02m:19s remains)
INFO - root - 2019-11-03 23:06:05.020969: step 6240, total loss = 0.66, predict loss = 0.16 (67.8 examples/sec; 0.059 sec/batch; 98h:13m:47s remains)
INFO - root - 2019-11-03 23:06:05.690940: step 6250, total loss = 0.87, predict loss = 0.17 (63.1 examples/sec; 0.063 sec/batch; 105h:31m:11s remains)
INFO - root - 2019-11-03 23:06:06.297789: step 6260, total loss = 1.38, predict loss = 0.31 (71.1 examples/sec; 0.056 sec/batch; 93h:43m:25s remains)
INFO - root - 2019-11-03 23:06:06.893631: step 6270, total loss = 0.89, predict loss = 0.20 (74.8 examples/sec; 0.053 sec/batch; 89h:04m:19s remains)
INFO - root - 2019-11-03 23:06:07.484324: step 6280, total loss = 0.82, predict loss = 0.18 (71.9 examples/sec; 0.056 sec/batch; 92h:39m:17s remains)
INFO - root - 2019-11-03 23:06:08.081635: step 6290, total loss = 0.62, predict loss = 0.14 (68.4 examples/sec; 0.058 sec/batch; 97h:23m:40s remains)
INFO - root - 2019-11-03 23:06:08.697402: step 6300, total loss = 0.66, predict loss = 0.14 (67.0 examples/sec; 0.060 sec/batch; 99h:25m:02s remains)
INFO - root - 2019-11-03 23:06:09.295456: step 6310, total loss = 0.81, predict loss = 0.20 (75.1 examples/sec; 0.053 sec/batch; 88h:37m:22s remains)
INFO - root - 2019-11-03 23:06:09.934838: step 6320, total loss = 0.60, predict loss = 0.13 (66.1 examples/sec; 0.061 sec/batch; 100h:47m:04s remains)
INFO - root - 2019-11-03 23:06:10.569430: step 6330, total loss = 0.71, predict loss = 0.18 (67.6 examples/sec; 0.059 sec/batch; 98h:32m:39s remains)
INFO - root - 2019-11-03 23:06:11.171671: step 6340, total loss = 0.77, predict loss = 0.19 (78.2 examples/sec; 0.051 sec/batch; 85h:08m:41s remains)
INFO - root - 2019-11-03 23:06:11.834647: step 6350, total loss = 0.85, predict loss = 0.20 (74.5 examples/sec; 0.054 sec/batch; 89h:21m:54s remains)
INFO - root - 2019-11-03 23:06:12.543209: step 6360, total loss = 0.97, predict loss = 0.22 (56.0 examples/sec; 0.071 sec/batch; 118h:53m:42s remains)
INFO - root - 2019-11-03 23:06:13.224847: step 6370, total loss = 0.69, predict loss = 0.16 (71.1 examples/sec; 0.056 sec/batch; 93h:38m:13s remains)
INFO - root - 2019-11-03 23:06:13.945426: step 6380, total loss = 0.76, predict loss = 0.16 (66.7 examples/sec; 0.060 sec/batch; 99h:51m:45s remains)
INFO - root - 2019-11-03 23:06:14.611526: step 6390, total loss = 0.84, predict loss = 0.18 (66.8 examples/sec; 0.060 sec/batch; 99h:44m:28s remains)
INFO - root - 2019-11-03 23:06:15.251785: step 6400, total loss = 0.69, predict loss = 0.15 (63.1 examples/sec; 0.063 sec/batch; 105h:36m:43s remains)
INFO - root - 2019-11-03 23:06:15.943978: step 6410, total loss = 0.88, predict loss = 0.21 (59.3 examples/sec; 0.067 sec/batch; 112h:19m:09s remains)
INFO - root - 2019-11-03 23:06:16.643250: step 6420, total loss = 0.82, predict loss = 0.19 (83.0 examples/sec; 0.048 sec/batch; 80h:12m:55s remains)
INFO - root - 2019-11-03 23:06:17.245486: step 6430, total loss = 0.85, predict loss = 0.19 (63.1 examples/sec; 0.063 sec/batch; 105h:33m:22s remains)
INFO - root - 2019-11-03 23:06:17.954966: step 6440, total loss = 0.68, predict loss = 0.15 (56.9 examples/sec; 0.070 sec/batch; 116h:57m:39s remains)
INFO - root - 2019-11-03 23:06:18.590507: step 6450, total loss = 1.05, predict loss = 0.25 (70.7 examples/sec; 0.057 sec/batch; 94h:15m:22s remains)
INFO - root - 2019-11-03 23:06:19.249768: step 6460, total loss = 1.09, predict loss = 0.29 (66.2 examples/sec; 0.060 sec/batch; 100h:39m:31s remains)
INFO - root - 2019-11-03 23:06:19.923617: step 6470, total loss = 0.99, predict loss = 0.24 (71.1 examples/sec; 0.056 sec/batch; 93h:39m:39s remains)
INFO - root - 2019-11-03 23:06:20.571144: step 6480, total loss = 0.90, predict loss = 0.20 (70.6 examples/sec; 0.057 sec/batch; 94h:22m:05s remains)
INFO - root - 2019-11-03 23:06:21.200002: step 6490, total loss = 0.78, predict loss = 0.19 (68.1 examples/sec; 0.059 sec/batch; 97h:46m:25s remains)
INFO - root - 2019-11-03 23:06:21.844745: step 6500, total loss = 1.04, predict loss = 0.24 (76.5 examples/sec; 0.052 sec/batch; 87h:03m:18s remains)
INFO - root - 2019-11-03 23:06:22.519051: step 6510, total loss = 0.88, predict loss = 0.22 (65.0 examples/sec; 0.062 sec/batch; 102h:23m:24s remains)
INFO - root - 2019-11-03 23:06:23.233812: step 6520, total loss = 0.72, predict loss = 0.15 (62.3 examples/sec; 0.064 sec/batch; 106h:55m:23s remains)
INFO - root - 2019-11-03 23:06:23.857629: step 6530, total loss = 0.53, predict loss = 0.11 (74.9 examples/sec; 0.053 sec/batch; 88h:56m:08s remains)
INFO - root - 2019-11-03 23:06:24.490816: step 6540, total loss = 0.61, predict loss = 0.14 (77.5 examples/sec; 0.052 sec/batch; 85h:55m:09s remains)
INFO - root - 2019-11-03 23:06:25.160294: step 6550, total loss = 0.60, predict loss = 0.12 (64.6 examples/sec; 0.062 sec/batch; 103h:09m:16s remains)
INFO - root - 2019-11-03 23:06:25.808076: step 6560, total loss = 0.59, predict loss = 0.12 (69.6 examples/sec; 0.057 sec/batch; 95h:42m:26s remains)
INFO - root - 2019-11-03 23:06:26.449171: step 6570, total loss = 0.34, predict loss = 0.07 (53.9 examples/sec; 0.074 sec/batch; 123h:31m:00s remains)
INFO - root - 2019-11-03 23:06:27.110026: step 6580, total loss = 0.58, predict loss = 0.14 (71.3 examples/sec; 0.056 sec/batch; 93h:21m:45s remains)
INFO - root - 2019-11-03 23:06:27.735708: step 6590, total loss = 0.92, predict loss = 0.21 (72.7 examples/sec; 0.055 sec/batch; 91h:33m:46s remains)
INFO - root - 2019-11-03 23:06:28.365305: step 6600, total loss = 0.83, predict loss = 0.21 (64.2 examples/sec; 0.062 sec/batch; 103h:39m:04s remains)
INFO - root - 2019-11-03 23:06:29.032863: step 6610, total loss = 0.89, predict loss = 0.22 (66.0 examples/sec; 0.061 sec/batch; 100h:51m:02s remains)
INFO - root - 2019-11-03 23:06:29.680053: step 6620, total loss = 1.15, predict loss = 0.26 (70.0 examples/sec; 0.057 sec/batch; 95h:07m:32s remains)
INFO - root - 2019-11-03 23:06:30.350715: step 6630, total loss = 0.77, predict loss = 0.17 (73.1 examples/sec; 0.055 sec/batch; 91h:05m:21s remains)
INFO - root - 2019-11-03 23:06:30.993478: step 6640, total loss = 0.64, predict loss = 0.15 (70.2 examples/sec; 0.057 sec/batch; 94h:47m:56s remains)
INFO - root - 2019-11-03 23:06:31.681989: step 6650, total loss = 0.73, predict loss = 0.17 (61.5 examples/sec; 0.065 sec/batch; 108h:12m:59s remains)
INFO - root - 2019-11-03 23:06:32.370174: step 6660, total loss = 0.79, predict loss = 0.18 (63.3 examples/sec; 0.063 sec/batch; 105h:09m:20s remains)
INFO - root - 2019-11-03 23:06:33.036992: step 6670, total loss = 1.19, predict loss = 0.30 (65.4 examples/sec; 0.061 sec/batch; 101h:45m:22s remains)
INFO - root - 2019-11-03 23:06:33.729430: step 6680, total loss = 0.86, predict loss = 0.20 (56.0 examples/sec; 0.071 sec/batch; 118h:53m:54s remains)
INFO - root - 2019-11-03 23:06:34.336871: step 6690, total loss = 0.87, predict loss = 0.18 (70.7 examples/sec; 0.057 sec/batch; 94h:07m:46s remains)
INFO - root - 2019-11-03 23:06:34.929029: step 6700, total loss = 0.95, predict loss = 0.22 (79.3 examples/sec; 0.050 sec/batch; 84h:00m:33s remains)
INFO - root - 2019-11-03 23:06:35.549263: step 6710, total loss = 0.86, predict loss = 0.20 (65.2 examples/sec; 0.061 sec/batch; 102h:05m:18s remains)
INFO - root - 2019-11-03 23:06:36.179948: step 6720, total loss = 1.00, predict loss = 0.22 (75.5 examples/sec; 0.053 sec/batch; 88h:15m:29s remains)
INFO - root - 2019-11-03 23:06:36.854890: step 6730, total loss = 0.75, predict loss = 0.16 (58.6 examples/sec; 0.068 sec/batch; 113h:33m:44s remains)
INFO - root - 2019-11-03 23:06:37.569766: step 6740, total loss = 0.91, predict loss = 0.22 (73.7 examples/sec; 0.054 sec/batch; 90h:24m:35s remains)
INFO - root - 2019-11-03 23:06:38.215955: step 6750, total loss = 0.99, predict loss = 0.24 (74.8 examples/sec; 0.054 sec/batch; 89h:04m:27s remains)
INFO - root - 2019-11-03 23:06:38.881362: step 6760, total loss = 0.73, predict loss = 0.16 (66.6 examples/sec; 0.060 sec/batch; 100h:02m:55s remains)
INFO - root - 2019-11-03 23:06:39.586473: step 6770, total loss = 0.99, predict loss = 0.23 (59.2 examples/sec; 0.068 sec/batch; 112h:29m:03s remains)
INFO - root - 2019-11-03 23:06:40.297685: step 6780, total loss = 0.93, predict loss = 0.25 (64.6 examples/sec; 0.062 sec/batch; 103h:03m:44s remains)
INFO - root - 2019-11-03 23:06:40.905699: step 6790, total loss = 0.81, predict loss = 0.19 (70.3 examples/sec; 0.057 sec/batch; 94h:46m:19s remains)
INFO - root - 2019-11-03 23:06:41.536619: step 6800, total loss = 0.85, predict loss = 0.20 (62.4 examples/sec; 0.064 sec/batch; 106h:43m:14s remains)
INFO - root - 2019-11-03 23:06:42.197130: step 6810, total loss = 0.87, predict loss = 0.20 (66.6 examples/sec; 0.060 sec/batch; 99h:57m:09s remains)
INFO - root - 2019-11-03 23:06:42.811966: step 6820, total loss = 0.98, predict loss = 0.21 (78.7 examples/sec; 0.051 sec/batch; 84h:35m:15s remains)
INFO - root - 2019-11-03 23:06:43.445384: step 6830, total loss = 0.47, predict loss = 0.12 (70.4 examples/sec; 0.057 sec/batch; 94h:34m:48s remains)
INFO - root - 2019-11-03 23:06:44.092627: step 6840, total loss = 0.95, predict loss = 0.21 (51.1 examples/sec; 0.078 sec/batch; 130h:14m:25s remains)
INFO - root - 2019-11-03 23:06:44.753067: step 6850, total loss = 1.07, predict loss = 0.28 (61.3 examples/sec; 0.065 sec/batch; 108h:37m:29s remains)
INFO - root - 2019-11-03 23:06:45.396992: step 6860, total loss = 0.98, predict loss = 0.23 (64.6 examples/sec; 0.062 sec/batch; 103h:07m:54s remains)
INFO - root - 2019-11-03 23:06:46.075926: step 6870, total loss = 1.26, predict loss = 0.32 (74.5 examples/sec; 0.054 sec/batch; 89h:19m:42s remains)
INFO - root - 2019-11-03 23:06:46.800206: step 6880, total loss = 1.29, predict loss = 0.33 (57.3 examples/sec; 0.070 sec/batch; 116h:12m:10s remains)
INFO - root - 2019-11-03 23:06:47.501183: step 6890, total loss = 1.52, predict loss = 0.37 (73.0 examples/sec; 0.055 sec/batch; 91h:12m:26s remains)
INFO - root - 2019-11-03 23:06:48.136857: step 6900, total loss = 1.21, predict loss = 0.28 (73.2 examples/sec; 0.055 sec/batch; 91h:01m:23s remains)
INFO - root - 2019-11-03 23:06:48.744919: step 6910, total loss = 1.30, predict loss = 0.32 (66.2 examples/sec; 0.060 sec/batch; 100h:35m:59s remains)
INFO - root - 2019-11-03 23:06:49.525256: step 6920, total loss = 1.20, predict loss = 0.29 (67.9 examples/sec; 0.059 sec/batch; 98h:01m:23s remains)
INFO - root - 2019-11-03 23:06:50.151368: step 6930, total loss = 1.32, predict loss = 0.36 (86.3 examples/sec; 0.046 sec/batch; 77h:07m:45s remains)
INFO - root - 2019-11-03 23:06:50.728693: step 6940, total loss = 1.82, predict loss = 0.45 (81.8 examples/sec; 0.049 sec/batch; 81h:23m:37s remains)
INFO - root - 2019-11-03 23:06:51.347522: step 6950, total loss = 1.02, predict loss = 0.27 (65.9 examples/sec; 0.061 sec/batch; 101h:05m:02s remains)
INFO - root - 2019-11-03 23:06:51.980637: step 6960, total loss = 1.18, predict loss = 0.30 (67.1 examples/sec; 0.060 sec/batch; 99h:10m:21s remains)
INFO - root - 2019-11-03 23:06:52.741556: step 6970, total loss = 1.11, predict loss = 0.26 (57.3 examples/sec; 0.070 sec/batch; 116h:09m:48s remains)
INFO - root - 2019-11-03 23:06:53.484589: step 6980, total loss = 1.23, predict loss = 0.29 (56.0 examples/sec; 0.071 sec/batch; 119h:00m:37s remains)
INFO - root - 2019-11-03 23:06:54.342981: step 6990, total loss = 1.09, predict loss = 0.25 (51.8 examples/sec; 0.077 sec/batch; 128h:37m:47s remains)
INFO - root - 2019-11-03 23:06:55.010642: step 7000, total loss = 1.10, predict loss = 0.25 (81.3 examples/sec; 0.049 sec/batch; 81h:52m:15s remains)
INFO - root - 2019-11-03 23:06:55.602067: step 7010, total loss = 0.88, predict loss = 0.21 (78.7 examples/sec; 0.051 sec/batch; 84h:37m:51s remains)
INFO - root - 2019-11-03 23:06:56.225334: step 7020, total loss = 0.93, predict loss = 0.22 (83.4 examples/sec; 0.048 sec/batch; 79h:50m:17s remains)
INFO - root - 2019-11-03 23:06:56.863084: step 7030, total loss = 0.75, predict loss = 0.17 (77.4 examples/sec; 0.052 sec/batch; 86h:05m:14s remains)
INFO - root - 2019-11-03 23:06:57.492369: step 7040, total loss = 0.93, predict loss = 0.23 (73.0 examples/sec; 0.055 sec/batch; 91h:15m:58s remains)
INFO - root - 2019-11-03 23:06:58.116010: step 7050, total loss = 0.91, predict loss = 0.21 (74.4 examples/sec; 0.054 sec/batch; 89h:28m:34s remains)
INFO - root - 2019-11-03 23:06:58.774048: step 7060, total loss = 0.82, predict loss = 0.21 (68.0 examples/sec; 0.059 sec/batch; 97h:56m:51s remains)
INFO - root - 2019-11-03 23:06:59.487550: step 7070, total loss = 0.90, predict loss = 0.22 (61.6 examples/sec; 0.065 sec/batch; 108h:07m:35s remains)
INFO - root - 2019-11-03 23:07:00.185642: step 7080, total loss = 1.08, predict loss = 0.24 (74.6 examples/sec; 0.054 sec/batch; 89h:15m:52s remains)
INFO - root - 2019-11-03 23:07:00.778065: step 7090, total loss = 0.88, predict loss = 0.21 (67.6 examples/sec; 0.059 sec/batch; 98h:30m:34s remains)
INFO - root - 2019-11-03 23:07:01.407761: step 7100, total loss = 0.88, predict loss = 0.20 (69.3 examples/sec; 0.058 sec/batch; 96h:07m:37s remains)
INFO - root - 2019-11-03 23:07:02.027159: step 7110, total loss = 0.81, predict loss = 0.18 (69.3 examples/sec; 0.058 sec/batch; 96h:08m:59s remains)
INFO - root - 2019-11-03 23:07:02.663859: step 7120, total loss = 0.86, predict loss = 0.19 (81.5 examples/sec; 0.049 sec/batch; 81h:42m:54s remains)
INFO - root - 2019-11-03 23:07:03.264453: step 7130, total loss = 0.74, predict loss = 0.16 (76.8 examples/sec; 0.052 sec/batch; 86h:45m:31s remains)
INFO - root - 2019-11-03 23:07:03.903294: step 7140, total loss = 0.67, predict loss = 0.15 (74.3 examples/sec; 0.054 sec/batch; 89h:39m:46s remains)
INFO - root - 2019-11-03 23:07:04.543296: step 7150, total loss = 0.67, predict loss = 0.17 (66.4 examples/sec; 0.060 sec/batch; 100h:18m:26s remains)
INFO - root - 2019-11-03 23:07:05.193935: step 7160, total loss = 0.64, predict loss = 0.16 (65.3 examples/sec; 0.061 sec/batch; 101h:56m:38s remains)
INFO - root - 2019-11-03 23:07:05.838539: step 7170, total loss = 0.61, predict loss = 0.14 (67.3 examples/sec; 0.059 sec/batch; 98h:53m:07s remains)
INFO - root - 2019-11-03 23:07:06.467201: step 7180, total loss = 0.77, predict loss = 0.19 (69.2 examples/sec; 0.058 sec/batch; 96h:12m:33s remains)
INFO - root - 2019-11-03 23:07:07.136704: step 7190, total loss = 1.03, predict loss = 0.29 (64.9 examples/sec; 0.062 sec/batch; 102h:38m:49s remains)
INFO - root - 2019-11-03 23:07:07.773389: step 7200, total loss = 0.80, predict loss = 0.18 (75.0 examples/sec; 0.053 sec/batch; 88h:46m:37s remains)
INFO - root - 2019-11-03 23:07:08.415792: step 7210, total loss = 0.74, predict loss = 0.18 (61.3 examples/sec; 0.065 sec/batch; 108h:34m:26s remains)
INFO - root - 2019-11-03 23:07:09.039428: step 7220, total loss = 0.94, predict loss = 0.22 (70.0 examples/sec; 0.057 sec/batch; 95h:11m:11s remains)
INFO - root - 2019-11-03 23:07:09.716600: step 7230, total loss = 0.69, predict loss = 0.17 (70.5 examples/sec; 0.057 sec/batch; 94h:27m:32s remains)
INFO - root - 2019-11-03 23:07:10.419091: step 7240, total loss = 0.79, predict loss = 0.17 (65.4 examples/sec; 0.061 sec/batch; 101h:49m:39s remains)
INFO - root - 2019-11-03 23:07:11.086150: step 7250, total loss = 0.79, predict loss = 0.20 (74.0 examples/sec; 0.054 sec/batch; 89h:55m:33s remains)
INFO - root - 2019-11-03 23:07:11.702297: step 7260, total loss = 0.92, predict loss = 0.21 (63.4 examples/sec; 0.063 sec/batch; 104h:57m:32s remains)
INFO - root - 2019-11-03 23:07:12.321646: step 7270, total loss = 1.12, predict loss = 0.29 (72.6 examples/sec; 0.055 sec/batch; 91h:41m:23s remains)
INFO - root - 2019-11-03 23:07:12.974189: step 7280, total loss = 0.88, predict loss = 0.22 (74.9 examples/sec; 0.053 sec/batch; 88h:53m:24s remains)
INFO - root - 2019-11-03 23:07:13.607857: step 7290, total loss = 0.65, predict loss = 0.15 (65.0 examples/sec; 0.062 sec/batch; 102h:24m:52s remains)
INFO - root - 2019-11-03 23:07:14.257014: step 7300, total loss = 1.07, predict loss = 0.26 (68.4 examples/sec; 0.058 sec/batch; 97h:18m:46s remains)
INFO - root - 2019-11-03 23:07:14.855110: step 7310, total loss = 0.87, predict loss = 0.19 (73.0 examples/sec; 0.055 sec/batch; 91h:15m:33s remains)
INFO - root - 2019-11-03 23:07:15.468125: step 7320, total loss = 0.87, predict loss = 0.21 (64.6 examples/sec; 0.062 sec/batch; 103h:05m:34s remains)
INFO - root - 2019-11-03 23:07:16.157445: step 7330, total loss = 1.12, predict loss = 0.29 (68.6 examples/sec; 0.058 sec/batch; 97h:07m:35s remains)
INFO - root - 2019-11-03 23:07:16.794606: step 7340, total loss = 1.07, predict loss = 0.26 (72.3 examples/sec; 0.055 sec/batch; 92h:02m:29s remains)
INFO - root - 2019-11-03 23:07:17.459487: step 7350, total loss = 1.23, predict loss = 0.33 (65.7 examples/sec; 0.061 sec/batch; 101h:23m:05s remains)
INFO - root - 2019-11-03 23:07:18.086463: step 7360, total loss = 1.16, predict loss = 0.26 (76.1 examples/sec; 0.053 sec/batch; 87h:26m:33s remains)
INFO - root - 2019-11-03 23:07:18.725395: step 7370, total loss = 0.91, predict loss = 0.23 (68.0 examples/sec; 0.059 sec/batch; 97h:53m:36s remains)
INFO - root - 2019-11-03 23:07:19.372747: step 7380, total loss = 0.80, predict loss = 0.20 (68.5 examples/sec; 0.058 sec/batch; 97h:15m:24s remains)
INFO - root - 2019-11-03 23:07:20.039194: step 7390, total loss = 0.92, predict loss = 0.21 (63.7 examples/sec; 0.063 sec/batch; 104h:32m:42s remains)
INFO - root - 2019-11-03 23:07:20.711239: step 7400, total loss = 0.86, predict loss = 0.20 (69.0 examples/sec; 0.058 sec/batch; 96h:27m:11s remains)
INFO - root - 2019-11-03 23:07:21.380563: step 7410, total loss = 0.86, predict loss = 0.19 (60.5 examples/sec; 0.066 sec/batch; 110h:01m:15s remains)
INFO - root - 2019-11-03 23:07:22.027653: step 7420, total loss = 0.86, predict loss = 0.19 (76.3 examples/sec; 0.052 sec/batch; 87h:17m:17s remains)
INFO - root - 2019-11-03 23:07:22.634547: step 7430, total loss = 0.78, predict loss = 0.16 (83.8 examples/sec; 0.048 sec/batch; 79h:25m:46s remains)
INFO - root - 2019-11-03 23:07:23.277448: step 7440, total loss = 1.11, predict loss = 0.27 (70.3 examples/sec; 0.057 sec/batch; 94h:46m:12s remains)
INFO - root - 2019-11-03 23:07:23.930760: step 7450, total loss = 0.75, predict loss = 0.18 (64.0 examples/sec; 0.062 sec/batch; 104h:01m:35s remains)
INFO - root - 2019-11-03 23:07:24.629872: step 7460, total loss = 0.73, predict loss = 0.19 (69.1 examples/sec; 0.058 sec/batch; 96h:18m:50s remains)
INFO - root - 2019-11-03 23:07:25.231103: step 7470, total loss = 0.89, predict loss = 0.23 (77.1 examples/sec; 0.052 sec/batch; 86h:23m:08s remains)
INFO - root - 2019-11-03 23:07:25.908972: step 7480, total loss = 0.71, predict loss = 0.17 (67.6 examples/sec; 0.059 sec/batch; 98h:27m:06s remains)
INFO - root - 2019-11-03 23:07:26.539647: step 7490, total loss = 0.75, predict loss = 0.19 (66.1 examples/sec; 0.061 sec/batch; 100h:47m:50s remains)
INFO - root - 2019-11-03 23:07:27.197455: step 7500, total loss = 0.74, predict loss = 0.17 (70.9 examples/sec; 0.056 sec/batch; 93h:51m:55s remains)
INFO - root - 2019-11-03 23:07:27.898918: step 7510, total loss = 0.87, predict loss = 0.21 (68.1 examples/sec; 0.059 sec/batch; 97h:49m:48s remains)
INFO - root - 2019-11-03 23:07:28.541677: step 7520, total loss = 0.95, predict loss = 0.22 (63.2 examples/sec; 0.063 sec/batch; 105h:20m:24s remains)
INFO - root - 2019-11-03 23:07:29.184931: step 7530, total loss = 1.29, predict loss = 0.31 (65.7 examples/sec; 0.061 sec/batch; 101h:21m:31s remains)
INFO - root - 2019-11-03 23:07:29.828601: step 7540, total loss = 1.57, predict loss = 0.44 (73.7 examples/sec; 0.054 sec/batch; 90h:17m:46s remains)
INFO - root - 2019-11-03 23:07:30.465799: step 7550, total loss = 0.89, predict loss = 0.21 (70.8 examples/sec; 0.056 sec/batch; 94h:02m:43s remains)
INFO - root - 2019-11-03 23:07:31.126510: step 7560, total loss = 0.90, predict loss = 0.20 (64.3 examples/sec; 0.062 sec/batch; 103h:28m:43s remains)
INFO - root - 2019-11-03 23:07:31.784831: step 7570, total loss = 1.11, predict loss = 0.28 (66.3 examples/sec; 0.060 sec/batch; 100h:22m:42s remains)
INFO - root - 2019-11-03 23:07:32.411245: step 7580, total loss = 0.90, predict loss = 0.23 (71.3 examples/sec; 0.056 sec/batch; 93h:24m:35s remains)
INFO - root - 2019-11-03 23:07:33.021260: step 7590, total loss = 0.89, predict loss = 0.22 (67.7 examples/sec; 0.059 sec/batch; 98h:17m:50s remains)
INFO - root - 2019-11-03 23:07:33.627657: step 7600, total loss = 0.83, predict loss = 0.20 (79.4 examples/sec; 0.050 sec/batch; 83h:54m:01s remains)
INFO - root - 2019-11-03 23:07:34.217508: step 7610, total loss = 0.90, predict loss = 0.22 (71.9 examples/sec; 0.056 sec/batch; 92h:34m:20s remains)
INFO - root - 2019-11-03 23:07:34.821191: step 7620, total loss = 1.03, predict loss = 0.25 (63.3 examples/sec; 0.063 sec/batch; 105h:14m:28s remains)
INFO - root - 2019-11-03 23:07:35.483569: step 7630, total loss = 0.87, predict loss = 0.21 (58.8 examples/sec; 0.068 sec/batch; 113h:13m:34s remains)
INFO - root - 2019-11-03 23:07:36.155680: step 7640, total loss = 0.66, predict loss = 0.16 (63.6 examples/sec; 0.063 sec/batch; 104h:42m:55s remains)
INFO - root - 2019-11-03 23:07:36.832053: step 7650, total loss = 0.99, predict loss = 0.24 (68.1 examples/sec; 0.059 sec/batch; 97h:42m:30s remains)
INFO - root - 2019-11-03 23:07:37.475376: step 7660, total loss = 0.79, predict loss = 0.18 (68.9 examples/sec; 0.058 sec/batch; 96h:36m:53s remains)
INFO - root - 2019-11-03 23:07:38.126471: step 7670, total loss = 0.66, predict loss = 0.16 (65.4 examples/sec; 0.061 sec/batch; 101h:46m:14s remains)
INFO - root - 2019-11-03 23:07:38.735334: step 7680, total loss = 0.85, predict loss = 0.18 (80.9 examples/sec; 0.049 sec/batch; 82h:18m:28s remains)
INFO - root - 2019-11-03 23:07:39.360772: step 7690, total loss = 0.80, predict loss = 0.19 (66.5 examples/sec; 0.060 sec/batch; 100h:07m:58s remains)
INFO - root - 2019-11-03 23:07:39.992125: step 7700, total loss = 0.62, predict loss = 0.14 (70.3 examples/sec; 0.057 sec/batch; 94h:44m:16s remains)
INFO - root - 2019-11-03 23:07:40.624897: step 7710, total loss = 0.83, predict loss = 0.18 (70.1 examples/sec; 0.057 sec/batch; 95h:02m:20s remains)
INFO - root - 2019-11-03 23:07:41.261182: step 7720, total loss = 0.99, predict loss = 0.23 (75.0 examples/sec; 0.053 sec/batch; 88h:49m:50s remains)
INFO - root - 2019-11-03 23:07:41.885109: step 7730, total loss = 0.89, predict loss = 0.21 (64.3 examples/sec; 0.062 sec/batch; 103h:36m:33s remains)
INFO - root - 2019-11-03 23:07:42.524021: step 7740, total loss = 0.64, predict loss = 0.15 (74.2 examples/sec; 0.054 sec/batch; 89h:43m:55s remains)
INFO - root - 2019-11-03 23:07:43.139112: step 7750, total loss = 0.83, predict loss = 0.19 (69.5 examples/sec; 0.058 sec/batch; 95h:46m:30s remains)
INFO - root - 2019-11-03 23:07:43.735459: step 7760, total loss = 0.74, predict loss = 0.17 (78.2 examples/sec; 0.051 sec/batch; 85h:11m:20s remains)
INFO - root - 2019-11-03 23:07:44.382624: step 7770, total loss = 0.76, predict loss = 0.17 (64.3 examples/sec; 0.062 sec/batch; 103h:31m:55s remains)
INFO - root - 2019-11-03 23:07:45.044329: step 7780, total loss = 0.66, predict loss = 0.17 (75.2 examples/sec; 0.053 sec/batch; 88h:34m:49s remains)
INFO - root - 2019-11-03 23:07:45.686538: step 7790, total loss = 0.91, predict loss = 0.21 (64.8 examples/sec; 0.062 sec/batch; 102h:49m:17s remains)
INFO - root - 2019-11-03 23:07:46.335560: step 7800, total loss = 0.97, predict loss = 0.22 (67.1 examples/sec; 0.060 sec/batch; 99h:12m:24s remains)
INFO - root - 2019-11-03 23:07:46.979297: step 7810, total loss = 0.88, predict loss = 0.22 (71.7 examples/sec; 0.056 sec/batch; 92h:48m:46s remains)
INFO - root - 2019-11-03 23:07:47.580637: step 7820, total loss = 0.83, predict loss = 0.20 (73.8 examples/sec; 0.054 sec/batch; 90h:10m:21s remains)
INFO - root - 2019-11-03 23:07:48.186971: step 7830, total loss = 0.88, predict loss = 0.20 (73.6 examples/sec; 0.054 sec/batch; 90h:28m:19s remains)
INFO - root - 2019-11-03 23:07:48.788393: step 7840, total loss = 0.75, predict loss = 0.17 (71.3 examples/sec; 0.056 sec/batch; 93h:20m:29s remains)
INFO - root - 2019-11-03 23:07:49.394207: step 7850, total loss = 1.00, predict loss = 0.25 (66.9 examples/sec; 0.060 sec/batch; 99h:29m:07s remains)
INFO - root - 2019-11-03 23:07:50.025769: step 7860, total loss = 0.78, predict loss = 0.17 (75.1 examples/sec; 0.053 sec/batch; 88h:38m:01s remains)
INFO - root - 2019-11-03 23:07:50.667462: step 7870, total loss = 1.08, predict loss = 0.27 (67.3 examples/sec; 0.059 sec/batch; 98h:53m:38s remains)
INFO - root - 2019-11-03 23:07:51.282826: step 7880, total loss = 0.88, predict loss = 0.21 (65.5 examples/sec; 0.061 sec/batch; 101h:42m:38s remains)
INFO - root - 2019-11-03 23:07:51.923422: step 7890, total loss = 0.88, predict loss = 0.22 (67.3 examples/sec; 0.059 sec/batch; 98h:59m:57s remains)
INFO - root - 2019-11-03 23:07:52.568641: step 7900, total loss = 0.81, predict loss = 0.20 (70.0 examples/sec; 0.057 sec/batch; 95h:03m:42s remains)
INFO - root - 2019-11-03 23:07:53.187671: step 7910, total loss = 1.03, predict loss = 0.25 (70.2 examples/sec; 0.057 sec/batch; 94h:52m:58s remains)
INFO - root - 2019-11-03 23:07:53.824303: step 7920, total loss = 1.05, predict loss = 0.25 (76.5 examples/sec; 0.052 sec/batch; 87h:03m:42s remains)
INFO - root - 2019-11-03 23:07:54.528006: step 7930, total loss = 1.07, predict loss = 0.26 (69.2 examples/sec; 0.058 sec/batch; 96h:11m:30s remains)
INFO - root - 2019-11-03 23:07:55.130341: step 7940, total loss = 1.01, predict loss = 0.24 (67.0 examples/sec; 0.060 sec/batch; 99h:18m:30s remains)
INFO - root - 2019-11-03 23:07:55.798469: step 7950, total loss = 1.02, predict loss = 0.23 (62.7 examples/sec; 0.064 sec/batch; 106h:13m:09s remains)
INFO - root - 2019-11-03 23:07:56.490798: step 7960, total loss = 0.90, predict loss = 0.21 (72.4 examples/sec; 0.055 sec/batch; 92h:00m:19s remains)
INFO - root - 2019-11-03 23:07:57.157411: step 7970, total loss = 1.23, predict loss = 0.30 (76.3 examples/sec; 0.052 sec/batch; 87h:12m:58s remains)
INFO - root - 2019-11-03 23:07:57.841111: step 7980, total loss = 1.10, predict loss = 0.28 (68.0 examples/sec; 0.059 sec/batch; 97h:50m:49s remains)
INFO - root - 2019-11-03 23:07:58.514061: step 7990, total loss = 1.22, predict loss = 0.29 (63.1 examples/sec; 0.063 sec/batch; 105h:33m:35s remains)
INFO - root - 2019-11-03 23:07:59.181154: step 8000, total loss = 1.18, predict loss = 0.30 (68.3 examples/sec; 0.059 sec/batch; 97h:26m:18s remains)
INFO - root - 2019-11-03 23:07:59.840169: step 8010, total loss = 0.99, predict loss = 0.23 (70.5 examples/sec; 0.057 sec/batch; 94h:24m:07s remains)
INFO - root - 2019-11-03 23:08:00.460225: step 8020, total loss = 0.94, predict loss = 0.23 (78.7 examples/sec; 0.051 sec/batch; 84h:32m:53s remains)
INFO - root - 2019-11-03 23:08:01.049565: step 8030, total loss = 0.60, predict loss = 0.14 (70.5 examples/sec; 0.057 sec/batch; 94h:23m:58s remains)
INFO - root - 2019-11-03 23:08:01.694891: step 8040, total loss = 0.90, predict loss = 0.21 (78.8 examples/sec; 0.051 sec/batch; 84h:31m:04s remains)
INFO - root - 2019-11-03 23:08:02.371743: step 8050, total loss = 0.89, predict loss = 0.20 (72.7 examples/sec; 0.055 sec/batch; 91h:37m:09s remains)
INFO - root - 2019-11-03 23:08:03.012331: step 8060, total loss = 0.76, predict loss = 0.19 (71.4 examples/sec; 0.056 sec/batch; 93h:14m:51s remains)
INFO - root - 2019-11-03 23:08:03.691543: step 8070, total loss = 0.80, predict loss = 0.18 (61.6 examples/sec; 0.065 sec/batch; 108h:08m:38s remains)
INFO - root - 2019-11-03 23:08:04.350718: step 8080, total loss = 0.88, predict loss = 0.20 (72.5 examples/sec; 0.055 sec/batch; 91h:47m:57s remains)
INFO - root - 2019-11-03 23:08:04.980256: step 8090, total loss = 1.07, predict loss = 0.24 (73.9 examples/sec; 0.054 sec/batch; 90h:07m:01s remains)
INFO - root - 2019-11-03 23:08:05.630760: step 8100, total loss = 0.91, predict loss = 0.21 (70.6 examples/sec; 0.057 sec/batch; 94h:18m:10s remains)
INFO - root - 2019-11-03 23:08:06.288508: step 8110, total loss = 1.24, predict loss = 0.29 (73.4 examples/sec; 0.055 sec/batch; 90h:42m:45s remains)
INFO - root - 2019-11-03 23:08:06.932557: step 8120, total loss = 0.99, predict loss = 0.24 (72.1 examples/sec; 0.056 sec/batch; 92h:23m:12s remains)
INFO - root - 2019-11-03 23:08:07.590967: step 8130, total loss = 0.92, predict loss = 0.22 (71.3 examples/sec; 0.056 sec/batch; 93h:22m:27s remains)
INFO - root - 2019-11-03 23:08:08.218988: step 8140, total loss = 0.99, predict loss = 0.22 (68.7 examples/sec; 0.058 sec/batch; 96h:57m:42s remains)
INFO - root - 2019-11-03 23:08:08.884171: step 8150, total loss = 0.78, predict loss = 0.18 (72.6 examples/sec; 0.055 sec/batch; 91h:39m:45s remains)
INFO - root - 2019-11-03 23:08:09.508849: step 8160, total loss = 0.65, predict loss = 0.16 (74.0 examples/sec; 0.054 sec/batch; 89h:57m:57s remains)
INFO - root - 2019-11-03 23:08:10.028125: step 8170, total loss = 0.77, predict loss = 0.18 (96.5 examples/sec; 0.041 sec/batch; 69h:00m:52s remains)
INFO - root - 2019-11-03 23:08:10.492906: step 8180, total loss = 1.03, predict loss = 0.25 (88.4 examples/sec; 0.045 sec/batch; 75h:20m:00s remains)
INFO - root - 2019-11-03 23:08:11.602536: step 8190, total loss = 1.12, predict loss = 0.25 (71.1 examples/sec; 0.056 sec/batch; 93h:39m:41s remains)
INFO - root - 2019-11-03 23:08:12.275962: step 8200, total loss = 0.78, predict loss = 0.20 (62.3 examples/sec; 0.064 sec/batch; 106h:55m:17s remains)
INFO - root - 2019-11-03 23:08:12.932869: step 8210, total loss = 0.84, predict loss = 0.20 (67.8 examples/sec; 0.059 sec/batch; 98h:08m:59s remains)
INFO - root - 2019-11-03 23:08:13.584184: step 8220, total loss = 1.04, predict loss = 0.23 (76.9 examples/sec; 0.052 sec/batch; 86h:31m:24s remains)
INFO - root - 2019-11-03 23:08:14.187618: step 8230, total loss = 0.93, predict loss = 0.23 (73.5 examples/sec; 0.054 sec/batch; 90h:37m:00s remains)
INFO - root - 2019-11-03 23:08:14.810824: step 8240, total loss = 0.98, predict loss = 0.23 (71.6 examples/sec; 0.056 sec/batch; 93h:02m:14s remains)
INFO - root - 2019-11-03 23:08:15.425561: step 8250, total loss = 0.83, predict loss = 0.19 (79.1 examples/sec; 0.051 sec/batch; 84h:07m:12s remains)
INFO - root - 2019-11-03 23:08:16.054786: step 8260, total loss = 0.82, predict loss = 0.18 (63.8 examples/sec; 0.063 sec/batch; 104h:22m:03s remains)
INFO - root - 2019-11-03 23:08:16.670526: step 8270, total loss = 0.94, predict loss = 0.23 (68.6 examples/sec; 0.058 sec/batch; 97h:04m:47s remains)
INFO - root - 2019-11-03 23:08:17.286936: step 8280, total loss = 1.60, predict loss = 0.41 (68.5 examples/sec; 0.058 sec/batch; 97h:08m:14s remains)
INFO - root - 2019-11-03 23:08:17.923076: step 8290, total loss = 1.23, predict loss = 0.29 (63.3 examples/sec; 0.063 sec/batch; 105h:13m:58s remains)
INFO - root - 2019-11-03 23:08:18.578136: step 8300, total loss = 1.36, predict loss = 0.32 (61.4 examples/sec; 0.065 sec/batch; 108h:24m:16s remains)
INFO - root - 2019-11-03 23:08:19.213453: step 8310, total loss = 1.39, predict loss = 0.34 (70.6 examples/sec; 0.057 sec/batch; 94h:15m:24s remains)
INFO - root - 2019-11-03 23:08:19.842308: step 8320, total loss = 1.06, predict loss = 0.25 (66.0 examples/sec; 0.061 sec/batch; 100h:56m:46s remains)
INFO - root - 2019-11-03 23:08:20.462889: step 8330, total loss = 0.94, predict loss = 0.22 (69.9 examples/sec; 0.057 sec/batch; 95h:10m:40s remains)
INFO - root - 2019-11-03 23:08:21.120203: step 8340, total loss = 0.91, predict loss = 0.21 (63.8 examples/sec; 0.063 sec/batch; 104h:16m:34s remains)
INFO - root - 2019-11-03 23:08:21.801820: step 8350, total loss = 0.75, predict loss = 0.17 (66.1 examples/sec; 0.061 sec/batch; 100h:44m:41s remains)
INFO - root - 2019-11-03 23:08:22.436205: step 8360, total loss = 0.91, predict loss = 0.21 (66.0 examples/sec; 0.061 sec/batch; 100h:48m:00s remains)
INFO - root - 2019-11-03 23:08:23.057140: step 8370, total loss = 0.75, predict loss = 0.18 (72.1 examples/sec; 0.055 sec/batch; 92h:18m:51s remains)
INFO - root - 2019-11-03 23:08:23.648132: step 8380, total loss = 0.88, predict loss = 0.21 (78.4 examples/sec; 0.051 sec/batch; 84h:53m:16s remains)
INFO - root - 2019-11-03 23:08:24.348514: step 8390, total loss = 1.90, predict loss = 0.48 (80.9 examples/sec; 0.049 sec/batch; 82h:14m:58s remains)
INFO - root - 2019-11-03 23:08:24.952517: step 8400, total loss = 0.82, predict loss = 0.19 (73.1 examples/sec; 0.055 sec/batch; 91h:02m:29s remains)
INFO - root - 2019-11-03 23:08:25.555192: step 8410, total loss = 0.85, predict loss = 0.18 (73.9 examples/sec; 0.054 sec/batch; 90h:03m:38s remains)
INFO - root - 2019-11-03 23:08:26.193594: step 8420, total loss = 0.64, predict loss = 0.16 (66.6 examples/sec; 0.060 sec/batch; 99h:56m:41s remains)
INFO - root - 2019-11-03 23:08:26.863016: step 8430, total loss = 0.93, predict loss = 0.22 (69.5 examples/sec; 0.058 sec/batch; 95h:45m:16s remains)
INFO - root - 2019-11-03 23:08:27.529900: step 8440, total loss = 1.17, predict loss = 0.26 (63.8 examples/sec; 0.063 sec/batch; 104h:19m:01s remains)
INFO - root - 2019-11-03 23:08:28.190786: step 8450, total loss = 1.02, predict loss = 0.24 (68.6 examples/sec; 0.058 sec/batch; 97h:04m:20s remains)
INFO - root - 2019-11-03 23:08:28.815899: step 8460, total loss = 0.91, predict loss = 0.21 (65.3 examples/sec; 0.061 sec/batch; 101h:53m:35s remains)
INFO - root - 2019-11-03 23:08:29.424010: step 8470, total loss = 0.93, predict loss = 0.20 (74.2 examples/sec; 0.054 sec/batch; 89h:46m:17s remains)
INFO - root - 2019-11-03 23:08:30.034193: step 8480, total loss = 0.81, predict loss = 0.18 (64.2 examples/sec; 0.062 sec/batch; 103h:37m:20s remains)
INFO - root - 2019-11-03 23:08:30.672046: step 8490, total loss = 1.01, predict loss = 0.23 (63.8 examples/sec; 0.063 sec/batch; 104h:17m:46s remains)
INFO - root - 2019-11-03 23:08:31.322667: step 8500, total loss = 0.94, predict loss = 0.22 (68.7 examples/sec; 0.058 sec/batch; 96h:52m:06s remains)
INFO - root - 2019-11-03 23:08:31.990235: step 8510, total loss = 0.99, predict loss = 0.20 (61.9 examples/sec; 0.065 sec/batch; 107h:33m:45s remains)
INFO - root - 2019-11-03 23:08:32.696885: step 8520, total loss = 1.01, predict loss = 0.23 (66.0 examples/sec; 0.061 sec/batch; 100h:53m:48s remains)
INFO - root - 2019-11-03 23:08:33.406022: step 8530, total loss = 1.09, predict loss = 0.26 (66.3 examples/sec; 0.060 sec/batch; 100h:21m:37s remains)
INFO - root - 2019-11-03 23:08:34.114496: step 8540, total loss = 1.20, predict loss = 0.24 (62.3 examples/sec; 0.064 sec/batch; 106h:47m:17s remains)
INFO - root - 2019-11-03 23:08:34.785162: step 8550, total loss = 0.96, predict loss = 0.22 (61.6 examples/sec; 0.065 sec/batch; 108h:04m:54s remains)
INFO - root - 2019-11-03 23:08:35.412530: step 8560, total loss = 0.81, predict loss = 0.20 (67.1 examples/sec; 0.060 sec/batch; 99h:11m:51s remains)
INFO - root - 2019-11-03 23:08:36.022155: step 8570, total loss = 0.92, predict loss = 0.21 (71.9 examples/sec; 0.056 sec/batch; 92h:31m:31s remains)
INFO - root - 2019-11-03 23:08:36.654631: step 8580, total loss = 0.87, predict loss = 0.19 (66.1 examples/sec; 0.061 sec/batch; 100h:41m:34s remains)
INFO - root - 2019-11-03 23:08:37.257799: step 8590, total loss = 1.22, predict loss = 0.30 (65.5 examples/sec; 0.061 sec/batch; 101h:42m:38s remains)
INFO - root - 2019-11-03 23:08:37.874327: step 8600, total loss = 1.12, predict loss = 0.26 (73.3 examples/sec; 0.055 sec/batch; 90h:52m:10s remains)
INFO - root - 2019-11-03 23:08:38.500326: step 8610, total loss = 1.07, predict loss = 0.23 (69.9 examples/sec; 0.057 sec/batch; 95h:11m:51s remains)
INFO - root - 2019-11-03 23:08:39.124033: step 8620, total loss = 1.65, predict loss = 0.39 (80.6 examples/sec; 0.050 sec/batch; 82h:32m:57s remains)
INFO - root - 2019-11-03 23:08:39.759018: step 8630, total loss = 0.91, predict loss = 0.23 (79.5 examples/sec; 0.050 sec/batch; 83h:45m:02s remains)
INFO - root - 2019-11-03 23:08:40.372154: step 8640, total loss = 1.55, predict loss = 0.46 (67.6 examples/sec; 0.059 sec/batch; 98h:31m:55s remains)
INFO - root - 2019-11-03 23:08:40.980678: step 8650, total loss = 0.80, predict loss = 0.23 (67.3 examples/sec; 0.059 sec/batch; 98h:57m:06s remains)
INFO - root - 2019-11-03 23:08:41.619367: step 8660, total loss = 0.85, predict loss = 0.20 (70.5 examples/sec; 0.057 sec/batch; 94h:24m:40s remains)
INFO - root - 2019-11-03 23:08:42.267620: step 8670, total loss = 0.86, predict loss = 0.18 (65.5 examples/sec; 0.061 sec/batch; 101h:39m:40s remains)
INFO - root - 2019-11-03 23:08:42.933269: step 8680, total loss = 1.59, predict loss = 0.40 (71.1 examples/sec; 0.056 sec/batch; 93h:41m:12s remains)
INFO - root - 2019-11-03 23:08:43.564719: step 8690, total loss = 0.98, predict loss = 0.24 (74.4 examples/sec; 0.054 sec/batch; 89h:32m:01s remains)
INFO - root - 2019-11-03 23:08:44.206364: step 8700, total loss = 0.97, predict loss = 0.23 (72.0 examples/sec; 0.056 sec/batch; 92h:28m:05s remains)
INFO - root - 2019-11-03 23:08:44.816459: step 8710, total loss = 0.95, predict loss = 0.24 (70.9 examples/sec; 0.056 sec/batch; 93h:49m:47s remains)
INFO - root - 2019-11-03 23:08:45.453711: step 8720, total loss = 0.65, predict loss = 0.15 (67.6 examples/sec; 0.059 sec/batch; 98h:26m:32s remains)
INFO - root - 2019-11-03 23:08:46.073489: step 8730, total loss = 0.76, predict loss = 0.19 (64.7 examples/sec; 0.062 sec/batch; 102h:51m:11s remains)
INFO - root - 2019-11-03 23:08:46.693592: step 8740, total loss = 0.65, predict loss = 0.14 (64.0 examples/sec; 0.063 sec/batch; 104h:02m:09s remains)
INFO - root - 2019-11-03 23:08:47.313032: step 8750, total loss = 0.73, predict loss = 0.19 (73.9 examples/sec; 0.054 sec/batch; 90h:03m:09s remains)
INFO - root - 2019-11-03 23:08:47.895476: step 8760, total loss = 0.67, predict loss = 0.16 (73.7 examples/sec; 0.054 sec/batch; 90h:21m:57s remains)
INFO - root - 2019-11-03 23:08:48.511949: step 8770, total loss = 0.89, predict loss = 0.21 (61.1 examples/sec; 0.065 sec/batch; 108h:57m:21s remains)
INFO - root - 2019-11-03 23:08:49.149498: step 8780, total loss = 1.16, predict loss = 0.35 (65.2 examples/sec; 0.061 sec/batch; 102h:10m:31s remains)
INFO - root - 2019-11-03 23:08:49.821637: step 8790, total loss = 1.13, predict loss = 0.33 (65.5 examples/sec; 0.061 sec/batch; 101h:40m:54s remains)
INFO - root - 2019-11-03 23:08:50.435663: step 8800, total loss = 0.64, predict loss = 0.11 (71.7 examples/sec; 0.056 sec/batch; 92h:49m:38s remains)
INFO - root - 2019-11-03 23:08:51.069289: step 8810, total loss = 1.10, predict loss = 0.31 (69.1 examples/sec; 0.058 sec/batch; 96h:22m:51s remains)
INFO - root - 2019-11-03 23:08:51.759413: step 8820, total loss = 0.61, predict loss = 0.16 (81.7 examples/sec; 0.049 sec/batch; 81h:29m:16s remains)
INFO - root - 2019-11-03 23:08:52.366149: step 8830, total loss = 1.27, predict loss = 0.34 (70.6 examples/sec; 0.057 sec/batch; 94h:18m:49s remains)
INFO - root - 2019-11-03 23:08:53.003888: step 8840, total loss = 0.94, predict loss = 0.23 (66.4 examples/sec; 0.060 sec/batch; 100h:10m:51s remains)
INFO - root - 2019-11-03 23:08:53.623762: step 8850, total loss = 0.98, predict loss = 0.22 (67.2 examples/sec; 0.060 sec/batch; 99h:02m:31s remains)
INFO - root - 2019-11-03 23:08:54.314333: step 8860, total loss = 0.93, predict loss = 0.21 (74.5 examples/sec; 0.054 sec/batch; 89h:19m:14s remains)
INFO - root - 2019-11-03 23:08:54.881822: step 8870, total loss = 0.99, predict loss = 0.24 (73.6 examples/sec; 0.054 sec/batch; 90h:27m:51s remains)
INFO - root - 2019-11-03 23:08:55.484367: step 8880, total loss = 0.98, predict loss = 0.20 (68.5 examples/sec; 0.058 sec/batch; 97h:14m:26s remains)
INFO - root - 2019-11-03 23:08:56.101510: step 8890, total loss = 0.97, predict loss = 0.25 (62.3 examples/sec; 0.064 sec/batch; 106h:47m:37s remains)
INFO - root - 2019-11-03 23:08:56.759448: step 8900, total loss = 0.77, predict loss = 0.19 (66.8 examples/sec; 0.060 sec/batch; 99h:42m:50s remains)
INFO - root - 2019-11-03 23:08:57.409588: step 8910, total loss = 1.12, predict loss = 0.25 (79.1 examples/sec; 0.051 sec/batch; 84h:11m:16s remains)
INFO - root - 2019-11-03 23:08:58.023600: step 8920, total loss = 1.06, predict loss = 0.24 (70.0 examples/sec; 0.057 sec/batch; 95h:04m:45s remains)
INFO - root - 2019-11-03 23:08:58.651987: step 8930, total loss = 0.90, predict loss = 0.21 (62.2 examples/sec; 0.064 sec/batch; 107h:03m:23s remains)
INFO - root - 2019-11-03 23:08:59.300804: step 8940, total loss = 0.86, predict loss = 0.21 (61.8 examples/sec; 0.065 sec/batch; 107h:38m:16s remains)
INFO - root - 2019-11-03 23:08:59.960539: step 8950, total loss = 1.17, predict loss = 0.30 (66.7 examples/sec; 0.060 sec/batch; 99h:51m:21s remains)
INFO - root - 2019-11-03 23:09:00.557062: step 8960, total loss = 0.96, predict loss = 0.23 (77.0 examples/sec; 0.052 sec/batch; 86h:30m:02s remains)
INFO - root - 2019-11-03 23:09:01.143042: step 8970, total loss = 0.94, predict loss = 0.20 (70.0 examples/sec; 0.057 sec/batch; 95h:05m:58s remains)
INFO - root - 2019-11-03 23:09:01.769547: step 8980, total loss = 0.88, predict loss = 0.20 (60.4 examples/sec; 0.066 sec/batch; 110h:09m:22s remains)
INFO - root - 2019-11-03 23:09:02.419147: step 8990, total loss = 0.86, predict loss = 0.22 (74.6 examples/sec; 0.054 sec/batch; 89h:10m:31s remains)
INFO - root - 2019-11-03 23:09:03.092882: step 9000, total loss = 0.62, predict loss = 0.13 (65.6 examples/sec; 0.061 sec/batch; 101h:32m:35s remains)
INFO - root - 2019-11-03 23:09:03.782189: step 9010, total loss = 0.83, predict loss = 0.17 (66.0 examples/sec; 0.061 sec/batch; 100h:53m:59s remains)
INFO - root - 2019-11-03 23:09:04.449769: step 9020, total loss = 0.68, predict loss = 0.16 (63.1 examples/sec; 0.063 sec/batch; 105h:29m:42s remains)
INFO - root - 2019-11-03 23:09:05.136716: step 9030, total loss = 0.51, predict loss = 0.11 (67.0 examples/sec; 0.060 sec/batch; 99h:18m:21s remains)
INFO - root - 2019-11-03 23:09:05.733911: step 9040, total loss = 0.57, predict loss = 0.14 (71.4 examples/sec; 0.056 sec/batch; 93h:17m:10s remains)
INFO - root - 2019-11-03 23:09:06.339194: step 9050, total loss = 0.51, predict loss = 0.14 (72.1 examples/sec; 0.055 sec/batch; 92h:19m:55s remains)
INFO - root - 2019-11-03 23:09:06.989843: step 9060, total loss = 0.69, predict loss = 0.15 (71.3 examples/sec; 0.056 sec/batch; 93h:22m:06s remains)
INFO - root - 2019-11-03 23:09:07.641569: step 9070, total loss = 0.52, predict loss = 0.13 (59.2 examples/sec; 0.068 sec/batch; 112h:30m:14s remains)
INFO - root - 2019-11-03 23:09:08.295243: step 9080, total loss = 0.62, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 92h:11m:13s remains)
INFO - root - 2019-11-03 23:09:09.395123: step 9090, total loss = 0.66, predict loss = 0.15 (70.8 examples/sec; 0.056 sec/batch; 93h:59m:36s remains)
INFO - root - 2019-11-03 23:09:10.006899: step 9100, total loss = 0.95, predict loss = 0.22 (71.0 examples/sec; 0.056 sec/batch; 93h:44m:21s remains)
INFO - root - 2019-11-03 23:09:10.625137: step 9110, total loss = 0.76, predict loss = 0.16 (75.0 examples/sec; 0.053 sec/batch; 88h:46m:39s remains)
INFO - root - 2019-11-03 23:09:11.231434: step 9120, total loss = 0.64, predict loss = 0.16 (75.3 examples/sec; 0.053 sec/batch; 88h:26m:56s remains)
INFO - root - 2019-11-03 23:09:11.842906: step 9130, total loss = 0.99, predict loss = 0.24 (75.1 examples/sec; 0.053 sec/batch; 88h:37m:35s remains)
INFO - root - 2019-11-03 23:09:12.465907: step 9140, total loss = 0.72, predict loss = 0.15 (73.0 examples/sec; 0.055 sec/batch; 91h:14m:50s remains)
INFO - root - 2019-11-03 23:09:13.058030: step 9150, total loss = 0.72, predict loss = 0.19 (75.8 examples/sec; 0.053 sec/batch; 87h:46m:16s remains)
INFO - root - 2019-11-03 23:09:13.683211: step 9160, total loss = 0.79, predict loss = 0.20 (63.2 examples/sec; 0.063 sec/batch; 105h:20m:43s remains)
INFO - root - 2019-11-03 23:09:14.316527: step 9170, total loss = 0.76, predict loss = 0.16 (65.7 examples/sec; 0.061 sec/batch; 101h:21m:12s remains)
INFO - root - 2019-11-03 23:09:14.992338: step 9180, total loss = 0.85, predict loss = 0.21 (78.3 examples/sec; 0.051 sec/batch; 85h:03m:03s remains)
INFO - root - 2019-11-03 23:09:15.641047: step 9190, total loss = 1.00, predict loss = 0.24 (71.7 examples/sec; 0.056 sec/batch; 92h:49m:38s remains)
INFO - root - 2019-11-03 23:09:16.243333: step 9200, total loss = 0.92, predict loss = 0.23 (68.7 examples/sec; 0.058 sec/batch; 96h:49m:31s remains)
INFO - root - 2019-11-03 23:09:16.893032: step 9210, total loss = 0.81, predict loss = 0.19 (61.0 examples/sec; 0.066 sec/batch; 109h:09m:18s remains)
INFO - root - 2019-11-03 23:09:17.569967: step 9220, total loss = 0.85, predict loss = 0.20 (69.5 examples/sec; 0.058 sec/batch; 95h:42m:43s remains)
INFO - root - 2019-11-03 23:09:18.204834: step 9230, total loss = 0.83, predict loss = 0.21 (71.3 examples/sec; 0.056 sec/batch; 93h:21m:38s remains)
INFO - root - 2019-11-03 23:09:18.861483: step 9240, total loss = 0.62, predict loss = 0.14 (61.4 examples/sec; 0.065 sec/batch; 108h:29m:40s remains)
INFO - root - 2019-11-03 23:09:19.552947: step 9250, total loss = 0.53, predict loss = 0.11 (61.9 examples/sec; 0.065 sec/batch; 107h:32m:16s remains)
INFO - root - 2019-11-03 23:09:20.247383: step 9260, total loss = 0.59, predict loss = 0.13 (64.2 examples/sec; 0.062 sec/batch; 103h:44m:21s remains)
INFO - root - 2019-11-03 23:09:20.897787: step 9270, total loss = 0.43, predict loss = 0.09 (62.1 examples/sec; 0.064 sec/batch; 107h:10m:27s remains)
INFO - root - 2019-11-03 23:09:21.570704: step 9280, total loss = 0.51, predict loss = 0.10 (67.2 examples/sec; 0.059 sec/batch; 98h:59m:50s remains)
INFO - root - 2019-11-03 23:09:22.180722: step 9290, total loss = 0.91, predict loss = 0.22 (69.2 examples/sec; 0.058 sec/batch; 96h:07m:14s remains)
INFO - root - 2019-11-03 23:09:22.815651: step 9300, total loss = 0.85, predict loss = 0.19 (63.0 examples/sec; 0.063 sec/batch; 105h:37m:31s remains)
INFO - root - 2019-11-03 23:09:23.464978: step 9310, total loss = 0.83, predict loss = 0.18 (69.5 examples/sec; 0.058 sec/batch; 95h:45m:33s remains)
INFO - root - 2019-11-03 23:09:24.133007: step 9320, total loss = 0.75, predict loss = 0.17 (52.9 examples/sec; 0.076 sec/batch; 125h:44m:16s remains)
INFO - root - 2019-11-03 23:09:24.785413: step 9330, total loss = 0.60, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 98h:04m:19s remains)
INFO - root - 2019-11-03 23:09:25.464046: step 9340, total loss = 0.85, predict loss = 0.20 (72.4 examples/sec; 0.055 sec/batch; 91h:53m:13s remains)
INFO - root - 2019-11-03 23:09:26.105421: step 9350, total loss = 0.91, predict loss = 0.22 (66.2 examples/sec; 0.060 sec/batch; 100h:31m:46s remains)
INFO - root - 2019-11-03 23:09:26.745599: step 9360, total loss = 1.06, predict loss = 0.25 (71.4 examples/sec; 0.056 sec/batch; 93h:14m:07s remains)
INFO - root - 2019-11-03 23:09:27.404104: step 9370, total loss = 0.82, predict loss = 0.18 (72.9 examples/sec; 0.055 sec/batch; 91h:16m:36s remains)
INFO - root - 2019-11-03 23:09:28.060929: step 9380, total loss = 0.66, predict loss = 0.15 (62.6 examples/sec; 0.064 sec/batch; 106h:20m:04s remains)
INFO - root - 2019-11-03 23:09:28.738264: step 9390, total loss = 0.60, predict loss = 0.13 (63.6 examples/sec; 0.063 sec/batch; 104h:42m:29s remains)
INFO - root - 2019-11-03 23:09:29.393859: step 9400, total loss = 0.58, predict loss = 0.14 (65.2 examples/sec; 0.061 sec/batch; 102h:07m:36s remains)
INFO - root - 2019-11-03 23:09:30.053742: step 9410, total loss = 0.60, predict loss = 0.14 (66.3 examples/sec; 0.060 sec/batch; 100h:20m:26s remains)
INFO - root - 2019-11-03 23:09:30.784944: step 9420, total loss = 0.71, predict loss = 0.16 (60.3 examples/sec; 0.066 sec/batch; 110h:22m:03s remains)
INFO - root - 2019-11-03 23:09:31.415594: step 9430, total loss = 0.76, predict loss = 0.17 (72.4 examples/sec; 0.055 sec/batch; 91h:54m:10s remains)
INFO - root - 2019-11-03 23:09:32.040501: step 9440, total loss = 0.87, predict loss = 0.20 (71.9 examples/sec; 0.056 sec/batch; 92h:35m:03s remains)
INFO - root - 2019-11-03 23:09:32.676432: step 9450, total loss = 0.89, predict loss = 0.20 (66.1 examples/sec; 0.061 sec/batch; 100h:45m:24s remains)
INFO - root - 2019-11-03 23:09:33.346210: step 9460, total loss = 0.91, predict loss = 0.21 (64.0 examples/sec; 0.063 sec/batch; 104h:01m:40s remains)
INFO - root - 2019-11-03 23:09:34.013958: step 9470, total loss = 0.75, predict loss = 0.17 (63.9 examples/sec; 0.063 sec/batch; 104h:08m:43s remains)
INFO - root - 2019-11-03 23:09:34.656330: step 9480, total loss = 0.78, predict loss = 0.17 (77.6 examples/sec; 0.052 sec/batch; 85h:44m:37s remains)
INFO - root - 2019-11-03 23:09:35.257621: step 9490, total loss = 0.97, predict loss = 0.22 (75.1 examples/sec; 0.053 sec/batch; 88h:39m:05s remains)
INFO - root - 2019-11-03 23:09:35.889686: step 9500, total loss = 0.98, predict loss = 0.23 (77.2 examples/sec; 0.052 sec/batch; 86h:15m:30s remains)
INFO - root - 2019-11-03 23:09:36.553852: step 9510, total loss = 1.08, predict loss = 0.24 (67.1 examples/sec; 0.060 sec/batch; 99h:07m:51s remains)
INFO - root - 2019-11-03 23:09:37.154384: step 9520, total loss = 0.56, predict loss = 0.13 (70.7 examples/sec; 0.057 sec/batch; 94h:05m:10s remains)
INFO - root - 2019-11-03 23:09:37.805468: step 9530, total loss = 0.59, predict loss = 0.15 (80.2 examples/sec; 0.050 sec/batch; 83h:00m:59s remains)
INFO - root - 2019-11-03 23:09:38.433419: step 9540, total loss = 0.77, predict loss = 0.18 (72.3 examples/sec; 0.055 sec/batch; 92h:07m:20s remains)
INFO - root - 2019-11-03 23:09:39.066440: step 9550, total loss = 0.67, predict loss = 0.17 (68.0 examples/sec; 0.059 sec/batch; 97h:49m:48s remains)
INFO - root - 2019-11-03 23:09:39.707000: step 9560, total loss = 0.82, predict loss = 0.20 (64.7 examples/sec; 0.062 sec/batch; 102h:56m:24s remains)
INFO - root - 2019-11-03 23:09:40.396993: step 9570, total loss = 0.99, predict loss = 0.22 (68.5 examples/sec; 0.058 sec/batch; 97h:10m:54s remains)
INFO - root - 2019-11-03 23:09:41.058291: step 9580, total loss = 0.78, predict loss = 0.17 (68.5 examples/sec; 0.058 sec/batch; 97h:14m:07s remains)
INFO - root - 2019-11-03 23:09:41.686582: step 9590, total loss = 0.94, predict loss = 0.23 (73.9 examples/sec; 0.054 sec/batch; 90h:07m:02s remains)
INFO - root - 2019-11-03 23:09:42.305687: step 9600, total loss = 1.16, predict loss = 0.28 (75.1 examples/sec; 0.053 sec/batch; 88h:35m:02s remains)
INFO - root - 2019-11-03 23:09:42.935249: step 9610, total loss = 1.14, predict loss = 0.29 (73.2 examples/sec; 0.055 sec/batch; 90h:53m:42s remains)
INFO - root - 2019-11-03 23:09:43.573361: step 9620, total loss = 1.21, predict loss = 0.30 (68.1 examples/sec; 0.059 sec/batch; 97h:43m:37s remains)
INFO - root - 2019-11-03 23:09:44.194935: step 9630, total loss = 1.29, predict loss = 0.30 (74.1 examples/sec; 0.054 sec/batch; 89h:47m:23s remains)
INFO - root - 2019-11-03 23:09:44.859660: step 9640, total loss = 1.04, predict loss = 0.26 (66.1 examples/sec; 0.061 sec/batch; 100h:43m:29s remains)
INFO - root - 2019-11-03 23:09:45.450293: step 9650, total loss = 1.20, predict loss = 0.29 (69.2 examples/sec; 0.058 sec/batch; 96h:12m:33s remains)
INFO - root - 2019-11-03 23:09:46.067534: step 9660, total loss = 1.43, predict loss = 0.35 (75.7 examples/sec; 0.053 sec/batch; 87h:53m:03s remains)
INFO - root - 2019-11-03 23:09:46.697525: step 9670, total loss = 1.12, predict loss = 0.27 (79.9 examples/sec; 0.050 sec/batch; 83h:20m:34s remains)
INFO - root - 2019-11-03 23:09:47.344932: step 9680, total loss = 0.97, predict loss = 0.23 (75.4 examples/sec; 0.053 sec/batch; 88h:18m:49s remains)
INFO - root - 2019-11-03 23:09:47.999980: step 9690, total loss = 1.05, predict loss = 0.25 (65.7 examples/sec; 0.061 sec/batch; 101h:20m:53s remains)
INFO - root - 2019-11-03 23:09:48.629688: step 9700, total loss = 0.82, predict loss = 0.20 (73.9 examples/sec; 0.054 sec/batch; 90h:01m:04s remains)
INFO - root - 2019-11-03 23:09:49.271036: step 9710, total loss = 0.94, predict loss = 0.23 (65.5 examples/sec; 0.061 sec/batch; 101h:37m:03s remains)
INFO - root - 2019-11-03 23:09:49.912950: step 9720, total loss = 0.86, predict loss = 0.19 (66.1 examples/sec; 0.061 sec/batch; 100h:41m:57s remains)
INFO - root - 2019-11-03 23:09:50.591497: step 9730, total loss = 0.87, predict loss = 0.20 (57.9 examples/sec; 0.069 sec/batch; 114h:52m:42s remains)
INFO - root - 2019-11-03 23:09:51.210257: step 9740, total loss = 0.93, predict loss = 0.21 (76.0 examples/sec; 0.053 sec/batch; 87h:31m:49s remains)
INFO - root - 2019-11-03 23:09:51.837677: step 9750, total loss = 1.11, predict loss = 0.25 (69.9 examples/sec; 0.057 sec/batch; 95h:09m:22s remains)
INFO - root - 2019-11-03 23:09:52.490441: step 9760, total loss = 0.99, predict loss = 0.24 (65.0 examples/sec; 0.062 sec/batch; 102h:26m:11s remains)
INFO - root - 2019-11-03 23:09:53.139975: step 9770, total loss = 1.32, predict loss = 0.32 (64.4 examples/sec; 0.062 sec/batch; 103h:18m:48s remains)
INFO - root - 2019-11-03 23:09:53.799749: step 9780, total loss = 0.69, predict loss = 0.16 (73.3 examples/sec; 0.055 sec/batch; 90h:49m:47s remains)
INFO - root - 2019-11-03 23:09:54.506438: step 9790, total loss = 0.78, predict loss = 0.19 (74.6 examples/sec; 0.054 sec/batch; 89h:15m:29s remains)
INFO - root - 2019-11-03 23:09:55.116711: step 9800, total loss = 1.01, predict loss = 0.23 (77.2 examples/sec; 0.052 sec/batch; 86h:15m:49s remains)
INFO - root - 2019-11-03 23:09:55.746335: step 9810, total loss = 0.82, predict loss = 0.19 (66.4 examples/sec; 0.060 sec/batch; 100h:10m:26s remains)
INFO - root - 2019-11-03 23:09:56.373650: step 9820, total loss = 0.88, predict loss = 0.20 (73.2 examples/sec; 0.055 sec/batch; 90h:55m:18s remains)
INFO - root - 2019-11-03 23:09:57.044461: step 9830, total loss = 0.65, predict loss = 0.15 (61.5 examples/sec; 0.065 sec/batch; 108h:14m:37s remains)
INFO - root - 2019-11-03 23:09:57.737224: step 9840, total loss = 0.80, predict loss = 0.18 (64.7 examples/sec; 0.062 sec/batch; 102h:48m:21s remains)
INFO - root - 2019-11-03 23:09:58.367646: step 9850, total loss = 0.91, predict loss = 0.22 (74.0 examples/sec; 0.054 sec/batch; 89h:58m:14s remains)
INFO - root - 2019-11-03 23:09:59.032998: step 9860, total loss = 0.51, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 96h:56m:46s remains)
INFO - root - 2019-11-03 23:09:59.664951: step 9870, total loss = 0.52, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 92h:49m:56s remains)
INFO - root - 2019-11-03 23:10:00.290069: step 9880, total loss = 0.71, predict loss = 0.17 (68.9 examples/sec; 0.058 sec/batch; 96h:36m:10s remains)
INFO - root - 2019-11-03 23:10:00.935923: step 9890, total loss = 0.66, predict loss = 0.16 (71.9 examples/sec; 0.056 sec/batch; 92h:36m:03s remains)
INFO - root - 2019-11-03 23:10:01.586054: step 9900, total loss = 0.59, predict loss = 0.16 (77.3 examples/sec; 0.052 sec/batch; 86h:07m:17s remains)
INFO - root - 2019-11-03 23:10:02.212988: step 9910, total loss = 0.82, predict loss = 0.19 (68.9 examples/sec; 0.058 sec/batch; 96h:37m:52s remains)
INFO - root - 2019-11-03 23:10:02.816379: step 9920, total loss = 0.63, predict loss = 0.13 (78.7 examples/sec; 0.051 sec/batch; 84h:35m:20s remains)
INFO - root - 2019-11-03 23:10:03.435703: step 9930, total loss = 0.67, predict loss = 0.16 (65.7 examples/sec; 0.061 sec/batch; 101h:18m:49s remains)
INFO - root - 2019-11-03 23:10:04.159168: step 9940, total loss = 0.67, predict loss = 0.15 (63.4 examples/sec; 0.063 sec/batch; 104h:57m:30s remains)
INFO - root - 2019-11-03 23:10:04.815579: step 9950, total loss = 0.66, predict loss = 0.17 (63.2 examples/sec; 0.063 sec/batch; 105h:23m:09s remains)
INFO - root - 2019-11-03 23:10:05.469127: step 9960, total loss = 0.56, predict loss = 0.13 (71.5 examples/sec; 0.056 sec/batch; 93h:04m:09s remains)
INFO - root - 2019-11-03 23:10:06.079823: step 9970, total loss = 0.78, predict loss = 0.18 (80.3 examples/sec; 0.050 sec/batch; 82h:54m:54s remains)
INFO - root - 2019-11-03 23:10:06.731700: step 9980, total loss = 0.72, predict loss = 0.17 (64.9 examples/sec; 0.062 sec/batch; 102h:31m:40s remains)
INFO - root - 2019-11-03 23:10:07.358273: step 9990, total loss = 0.84, predict loss = 0.19 (70.9 examples/sec; 0.056 sec/batch; 93h:49m:49s remains)
INFO - root - 2019-11-03 23:10:07.980036: step 10000, total loss = 0.95, predict loss = 0.23 (71.1 examples/sec; 0.056 sec/batch; 93h:37m:40s remains)
INFO - root - 2019-11-03 23:10:08.584086: step 10010, total loss = 0.82, predict loss = 0.20 (79.1 examples/sec; 0.051 sec/batch; 84h:07m:58s remains)
INFO - root - 2019-11-03 23:10:09.191656: step 10020, total loss = 0.77, predict loss = 0.19 (65.9 examples/sec; 0.061 sec/batch; 101h:03m:44s remains)
INFO - root - 2019-11-03 23:10:09.846146: step 10030, total loss = 0.83, predict loss = 0.21 (67.4 examples/sec; 0.059 sec/batch; 98h:41m:56s remains)
INFO - root - 2019-11-03 23:10:10.483380: step 10040, total loss = 1.21, predict loss = 0.29 (70.7 examples/sec; 0.057 sec/batch; 94h:11m:49s remains)
INFO - root - 2019-11-03 23:10:11.146720: step 10050, total loss = 1.15, predict loss = 0.27 (66.3 examples/sec; 0.060 sec/batch; 100h:20m:41s remains)
INFO - root - 2019-11-03 23:10:11.794402: step 10060, total loss = 0.99, predict loss = 0.24 (64.5 examples/sec; 0.062 sec/batch; 103h:08m:53s remains)
INFO - root - 2019-11-03 23:10:12.424788: step 10070, total loss = 0.90, predict loss = 0.21 (73.0 examples/sec; 0.055 sec/batch; 91h:12m:16s remains)
INFO - root - 2019-11-03 23:10:13.061686: step 10080, total loss = 1.13, predict loss = 0.29 (70.7 examples/sec; 0.057 sec/batch; 94h:04m:15s remains)
INFO - root - 2019-11-03 23:10:13.722666: step 10090, total loss = 1.11, predict loss = 0.28 (72.8 examples/sec; 0.055 sec/batch; 91h:24m:05s remains)
INFO - root - 2019-11-03 23:10:14.421694: step 10100, total loss = 0.94, predict loss = 0.24 (66.4 examples/sec; 0.060 sec/batch; 100h:18m:14s remains)
INFO - root - 2019-11-03 23:10:15.031585: step 10110, total loss = 0.98, predict loss = 0.22 (77.7 examples/sec; 0.051 sec/batch; 85h:36m:11s remains)
INFO - root - 2019-11-03 23:10:15.669767: step 10120, total loss = 0.86, predict loss = 0.21 (73.8 examples/sec; 0.054 sec/batch; 90h:07m:34s remains)
INFO - root - 2019-11-03 23:10:16.303196: step 10130, total loss = 0.72, predict loss = 0.16 (73.7 examples/sec; 0.054 sec/batch; 90h:18m:30s remains)
INFO - root - 2019-11-03 23:10:16.947566: step 10140, total loss = 0.71, predict loss = 0.16 (77.0 examples/sec; 0.052 sec/batch; 86h:28m:51s remains)
INFO - root - 2019-11-03 23:10:17.632200: step 10150, total loss = 0.70, predict loss = 0.17 (57.1 examples/sec; 0.070 sec/batch; 116h:33m:20s remains)
INFO - root - 2019-11-03 23:10:18.306586: step 10160, total loss = 0.67, predict loss = 0.16 (61.9 examples/sec; 0.065 sec/batch; 107h:31m:30s remains)
INFO - root - 2019-11-03 23:10:18.941867: step 10170, total loss = 0.97, predict loss = 0.24 (85.8 examples/sec; 0.047 sec/batch; 77h:35m:00s remains)
INFO - root - 2019-11-03 23:10:19.565349: step 10180, total loss = 0.79, predict loss = 0.19 (71.8 examples/sec; 0.056 sec/batch; 92h:38m:01s remains)
INFO - root - 2019-11-03 23:10:20.257085: step 10190, total loss = 0.65, predict loss = 0.15 (55.5 examples/sec; 0.072 sec/batch; 119h:53m:33s remains)
INFO - root - 2019-11-03 23:10:20.914882: step 10200, total loss = 0.77, predict loss = 0.19 (70.6 examples/sec; 0.057 sec/batch; 94h:17m:34s remains)
INFO - root - 2019-11-03 23:10:21.542058: step 10210, total loss = 0.87, predict loss = 0.21 (75.6 examples/sec; 0.053 sec/batch; 88h:03m:40s remains)
INFO - root - 2019-11-03 23:10:22.194200: step 10220, total loss = 0.76, predict loss = 0.17 (69.3 examples/sec; 0.058 sec/batch; 96h:04m:58s remains)
INFO - root - 2019-11-03 23:10:22.829786: step 10230, total loss = 0.75, predict loss = 0.18 (75.1 examples/sec; 0.053 sec/batch; 88h:40m:02s remains)
INFO - root - 2019-11-03 23:10:23.465651: step 10240, total loss = 0.91, predict loss = 0.21 (69.8 examples/sec; 0.057 sec/batch; 95h:19m:19s remains)
INFO - root - 2019-11-03 23:10:24.156426: step 10250, total loss = 0.83, predict loss = 0.21 (57.3 examples/sec; 0.070 sec/batch; 116h:12m:16s remains)
INFO - root - 2019-11-03 23:10:24.750083: step 10260, total loss = 1.02, predict loss = 0.25 (73.2 examples/sec; 0.055 sec/batch; 90h:52m:16s remains)
INFO - root - 2019-11-03 23:10:25.346146: step 10270, total loss = 0.92, predict loss = 0.22 (70.0 examples/sec; 0.057 sec/batch; 95h:01m:32s remains)
INFO - root - 2019-11-03 23:10:25.946682: step 10280, total loss = 0.76, predict loss = 0.18 (73.2 examples/sec; 0.055 sec/batch; 90h:57m:47s remains)
INFO - root - 2019-11-03 23:10:26.594132: step 10290, total loss = 0.96, predict loss = 0.23 (61.5 examples/sec; 0.065 sec/batch; 108h:09m:51s remains)
INFO - root - 2019-11-03 23:10:27.244554: step 10300, total loss = 0.74, predict loss = 0.18 (62.3 examples/sec; 0.064 sec/batch; 106h:48m:04s remains)
INFO - root - 2019-11-03 23:10:27.927797: step 10310, total loss = 0.85, predict loss = 0.21 (62.5 examples/sec; 0.064 sec/batch; 106h:25m:42s remains)
INFO - root - 2019-11-03 23:10:28.519090: step 10320, total loss = 0.95, predict loss = 0.21 (71.9 examples/sec; 0.056 sec/batch; 92h:31m:35s remains)
INFO - root - 2019-11-03 23:10:29.137234: step 10330, total loss = 0.87, predict loss = 0.21 (67.8 examples/sec; 0.059 sec/batch; 98h:12m:37s remains)
INFO - root - 2019-11-03 23:10:29.758261: step 10340, total loss = 0.67, predict loss = 0.17 (68.8 examples/sec; 0.058 sec/batch; 96h:40m:57s remains)
INFO - root - 2019-11-03 23:10:30.410084: step 10350, total loss = 0.81, predict loss = 0.21 (67.5 examples/sec; 0.059 sec/batch; 98h:37m:34s remains)
INFO - root - 2019-11-03 23:10:31.034832: step 10360, total loss = 0.88, predict loss = 0.21 (66.5 examples/sec; 0.060 sec/batch; 100h:06m:59s remains)
INFO - root - 2019-11-03 23:10:31.671045: step 10370, total loss = 0.96, predict loss = 0.22 (76.3 examples/sec; 0.052 sec/batch; 87h:12m:00s remains)
INFO - root - 2019-11-03 23:10:32.331469: step 10380, total loss = 0.64, predict loss = 0.14 (72.0 examples/sec; 0.056 sec/batch; 92h:28m:31s remains)
INFO - root - 2019-11-03 23:10:32.975246: step 10390, total loss = 0.89, predict loss = 0.22 (74.5 examples/sec; 0.054 sec/batch; 89h:16m:37s remains)
INFO - root - 2019-11-03 23:10:33.638298: step 10400, total loss = 1.00, predict loss = 0.23 (69.0 examples/sec; 0.058 sec/batch; 96h:24m:08s remains)
INFO - root - 2019-11-03 23:10:34.299370: step 10410, total loss = 0.91, predict loss = 0.22 (62.9 examples/sec; 0.064 sec/batch; 105h:44m:42s remains)
INFO - root - 2019-11-03 23:10:34.940592: step 10420, total loss = 0.70, predict loss = 0.17 (69.3 examples/sec; 0.058 sec/batch; 96h:03m:16s remains)
INFO - root - 2019-11-03 23:10:35.561803: step 10430, total loss = 0.67, predict loss = 0.15 (74.9 examples/sec; 0.053 sec/batch; 88h:50m:26s remains)
INFO - root - 2019-11-03 23:10:36.194475: step 10440, total loss = 0.75, predict loss = 0.18 (81.1 examples/sec; 0.049 sec/batch; 82h:02m:10s remains)
INFO - root - 2019-11-03 23:10:36.812023: step 10450, total loss = 0.67, predict loss = 0.15 (77.4 examples/sec; 0.052 sec/batch; 86h:02m:08s remains)
INFO - root - 2019-11-03 23:10:37.413204: step 10460, total loss = 0.66, predict loss = 0.15 (71.2 examples/sec; 0.056 sec/batch; 93h:29m:23s remains)
INFO - root - 2019-11-03 23:10:37.997939: step 10470, total loss = 1.01, predict loss = 0.26 (75.4 examples/sec; 0.053 sec/batch; 88h:18m:11s remains)
INFO - root - 2019-11-03 23:10:38.667314: step 10480, total loss = 0.85, predict loss = 0.20 (58.5 examples/sec; 0.068 sec/batch; 113h:46m:44s remains)
INFO - root - 2019-11-03 23:10:39.343098: step 10490, total loss = 0.95, predict loss = 0.23 (70.2 examples/sec; 0.057 sec/batch; 94h:47m:17s remains)
INFO - root - 2019-11-03 23:10:39.972554: step 10500, total loss = 0.79, predict loss = 0.19 (69.7 examples/sec; 0.057 sec/batch; 95h:28m:07s remains)
INFO - root - 2019-11-03 23:10:40.577664: step 10510, total loss = 0.67, predict loss = 0.16 (76.3 examples/sec; 0.052 sec/batch; 87h:11m:09s remains)
INFO - root - 2019-11-03 23:10:41.239570: step 10520, total loss = 0.65, predict loss = 0.15 (60.7 examples/sec; 0.066 sec/batch; 109h:36m:57s remains)
INFO - root - 2019-11-03 23:10:41.921253: step 10530, total loss = 0.87, predict loss = 0.22 (64.3 examples/sec; 0.062 sec/batch; 103h:29m:54s remains)
INFO - root - 2019-11-03 23:10:42.567130: step 10540, total loss = 0.83, predict loss = 0.20 (69.9 examples/sec; 0.057 sec/batch; 95h:10m:16s remains)
INFO - root - 2019-11-03 23:10:43.243826: step 10550, total loss = 0.89, predict loss = 0.20 (68.7 examples/sec; 0.058 sec/batch; 96h:56m:09s remains)
INFO - root - 2019-11-03 23:10:43.947805: step 10560, total loss = 0.86, predict loss = 0.20 (61.1 examples/sec; 0.065 sec/batch; 108h:58m:22s remains)
INFO - root - 2019-11-03 23:10:44.580386: step 10570, total loss = 0.87, predict loss = 0.21 (67.2 examples/sec; 0.059 sec/batch; 98h:58m:35s remains)
INFO - root - 2019-11-03 23:10:45.197344: step 10580, total loss = 0.77, predict loss = 0.17 (70.0 examples/sec; 0.057 sec/batch; 95h:06m:56s remains)
INFO - root - 2019-11-03 23:10:45.855203: step 10590, total loss = 0.78, predict loss = 0.17 (77.1 examples/sec; 0.052 sec/batch; 86h:21m:26s remains)
INFO - root - 2019-11-03 23:10:46.532656: step 10600, total loss = 0.76, predict loss = 0.18 (61.9 examples/sec; 0.065 sec/batch; 107h:25m:26s remains)
INFO - root - 2019-11-03 23:10:47.137261: step 10610, total loss = 0.81, predict loss = 0.20 (77.4 examples/sec; 0.052 sec/batch; 85h:59m:17s remains)
INFO - root - 2019-11-03 23:10:47.762990: step 10620, total loss = 0.68, predict loss = 0.15 (72.8 examples/sec; 0.055 sec/batch; 91h:26m:34s remains)
INFO - root - 2019-11-03 23:10:48.426496: step 10630, total loss = 0.82, predict loss = 0.20 (63.8 examples/sec; 0.063 sec/batch; 104h:18m:35s remains)
INFO - root - 2019-11-03 23:10:49.142351: step 10640, total loss = 0.89, predict loss = 0.22 (61.8 examples/sec; 0.065 sec/batch; 107h:36m:03s remains)
INFO - root - 2019-11-03 23:10:49.774558: step 10650, total loss = 1.20, predict loss = 0.29 (65.0 examples/sec; 0.062 sec/batch; 102h:19m:25s remains)
INFO - root - 2019-11-03 23:10:50.424949: step 10660, total loss = 1.07, predict loss = 0.27 (66.4 examples/sec; 0.060 sec/batch; 100h:15m:37s remains)
INFO - root - 2019-11-03 23:10:51.048302: step 10670, total loss = 0.80, predict loss = 0.19 (67.0 examples/sec; 0.060 sec/batch; 99h:19m:37s remains)
INFO - root - 2019-11-03 23:10:51.659937: step 10680, total loss = 0.84, predict loss = 0.20 (74.1 examples/sec; 0.054 sec/batch; 89h:50m:41s remains)
INFO - root - 2019-11-03 23:10:52.265445: step 10690, total loss = 0.87, predict loss = 0.21 (70.0 examples/sec; 0.057 sec/batch; 95h:07m:45s remains)
INFO - root - 2019-11-03 23:10:52.885926: step 10700, total loss = 0.89, predict loss = 0.22 (74.5 examples/sec; 0.054 sec/batch; 89h:16m:53s remains)
INFO - root - 2019-11-03 23:10:53.506620: step 10710, total loss = 0.86, predict loss = 0.21 (81.0 examples/sec; 0.049 sec/batch; 82h:09m:44s remains)
INFO - root - 2019-11-03 23:10:54.175406: step 10720, total loss = 1.11, predict loss = 0.26 (57.8 examples/sec; 0.069 sec/batch; 115h:13m:03s remains)
INFO - root - 2019-11-03 23:10:54.765622: step 10730, total loss = 1.19, predict loss = 0.28 (73.5 examples/sec; 0.054 sec/batch; 90h:30m:31s remains)
INFO - root - 2019-11-03 23:10:55.353443: step 10740, total loss = 1.17, predict loss = 0.29 (74.7 examples/sec; 0.054 sec/batch; 89h:07m:46s remains)
INFO - root - 2019-11-03 23:10:56.006500: step 10750, total loss = 0.90, predict loss = 0.21 (77.3 examples/sec; 0.052 sec/batch; 86h:03m:50s remains)
INFO - root - 2019-11-03 23:10:56.698740: step 10760, total loss = 0.94, predict loss = 0.22 (57.4 examples/sec; 0.070 sec/batch; 116h:01m:51s remains)
INFO - root - 2019-11-03 23:10:57.303048: step 10770, total loss = 0.76, predict loss = 0.19 (76.7 examples/sec; 0.052 sec/batch; 86h:42m:51s remains)
INFO - root - 2019-11-03 23:10:57.936366: step 10780, total loss = 0.61, predict loss = 0.13 (68.0 examples/sec; 0.059 sec/batch; 97h:49m:57s remains)
INFO - root - 2019-11-03 23:10:58.577648: step 10790, total loss = 0.54, predict loss = 0.12 (66.4 examples/sec; 0.060 sec/batch; 100h:16m:01s remains)
INFO - root - 2019-11-03 23:10:59.197113: step 10800, total loss = 0.70, predict loss = 0.16 (71.6 examples/sec; 0.056 sec/batch; 92h:56m:44s remains)
INFO - root - 2019-11-03 23:10:59.829055: step 10810, total loss = 0.86, predict loss = 0.19 (70.5 examples/sec; 0.057 sec/batch; 94h:20m:17s remains)
INFO - root - 2019-11-03 23:11:00.478852: step 10820, total loss = 0.85, predict loss = 0.19 (62.8 examples/sec; 0.064 sec/batch; 105h:56m:22s remains)
INFO - root - 2019-11-03 23:11:01.085033: step 10830, total loss = 0.82, predict loss = 0.21 (76.3 examples/sec; 0.052 sec/batch; 87h:14m:28s remains)
INFO - root - 2019-11-03 23:11:01.729886: step 10840, total loss = 0.79, predict loss = 0.18 (69.7 examples/sec; 0.057 sec/batch; 95h:28m:02s remains)
INFO - root - 2019-11-03 23:11:02.330805: step 10850, total loss = 0.84, predict loss = 0.21 (83.8 examples/sec; 0.048 sec/batch; 79h:26m:04s remains)
INFO - root - 2019-11-03 23:11:02.968794: step 10860, total loss = 0.68, predict loss = 0.16 (67.6 examples/sec; 0.059 sec/batch; 98h:24m:26s remains)
INFO - root - 2019-11-03 23:11:03.646273: step 10870, total loss = 0.94, predict loss = 0.22 (66.2 examples/sec; 0.060 sec/batch; 100h:30m:37s remains)
INFO - root - 2019-11-03 23:11:04.329424: step 10880, total loss = 0.65, predict loss = 0.15 (66.0 examples/sec; 0.061 sec/batch; 100h:48m:40s remains)
INFO - root - 2019-11-03 23:11:04.939085: step 10890, total loss = 1.00, predict loss = 0.24 (95.0 examples/sec; 0.042 sec/batch; 70h:03m:41s remains)
INFO - root - 2019-11-03 23:11:05.391554: step 10900, total loss = 0.52, predict loss = 0.11 (91.9 examples/sec; 0.044 sec/batch; 72h:25m:58s remains)
INFO - root - 2019-11-03 23:11:06.398772: step 10910, total loss = 1.12, predict loss = 0.22 (81.8 examples/sec; 0.049 sec/batch; 81h:22m:45s remains)
INFO - root - 2019-11-03 23:11:07.023942: step 10920, total loss = 0.70, predict loss = 0.17 (69.6 examples/sec; 0.057 sec/batch; 95h:39m:14s remains)
INFO - root - 2019-11-03 23:11:07.650184: step 10930, total loss = 1.26, predict loss = 0.30 (80.2 examples/sec; 0.050 sec/batch; 82h:58m:06s remains)
INFO - root - 2019-11-03 23:11:08.249858: step 10940, total loss = 0.73, predict loss = 0.14 (69.3 examples/sec; 0.058 sec/batch; 96h:01m:12s remains)
INFO - root - 2019-11-03 23:11:08.854347: step 10950, total loss = 0.79, predict loss = 0.18 (73.0 examples/sec; 0.055 sec/batch; 91h:07m:25s remains)
INFO - root - 2019-11-03 23:11:09.461218: step 10960, total loss = 0.96, predict loss = 0.23 (73.4 examples/sec; 0.054 sec/batch; 90h:36m:30s remains)
INFO - root - 2019-11-03 23:11:10.122712: step 10970, total loss = 1.34, predict loss = 0.32 (76.8 examples/sec; 0.052 sec/batch; 86h:36m:33s remains)
INFO - root - 2019-11-03 23:11:10.740708: step 10980, total loss = 1.11, predict loss = 0.25 (71.1 examples/sec; 0.056 sec/batch; 93h:35m:48s remains)
INFO - root - 2019-11-03 23:11:11.386176: step 10990, total loss = 0.97, predict loss = 0.24 (62.7 examples/sec; 0.064 sec/batch; 106h:10m:23s remains)
INFO - root - 2019-11-03 23:11:12.070059: step 11000, total loss = 1.20, predict loss = 0.27 (60.7 examples/sec; 0.066 sec/batch; 109h:39m:18s remains)
INFO - root - 2019-11-03 23:11:12.702609: step 11010, total loss = 0.97, predict loss = 0.25 (68.2 examples/sec; 0.059 sec/batch; 97h:32m:27s remains)
INFO - root - 2019-11-03 23:11:13.355057: step 11020, total loss = 1.36, predict loss = 0.31 (71.5 examples/sec; 0.056 sec/batch; 93h:04m:15s remains)
INFO - root - 2019-11-03 23:11:13.976046: step 11030, total loss = 0.97, predict loss = 0.23 (71.8 examples/sec; 0.056 sec/batch; 92h:43m:48s remains)
INFO - root - 2019-11-03 23:11:14.600820: step 11040, total loss = 1.16, predict loss = 0.27 (74.2 examples/sec; 0.054 sec/batch; 89h:43m:42s remains)
INFO - root - 2019-11-03 23:11:15.230345: step 11050, total loss = 0.91, predict loss = 0.22 (73.6 examples/sec; 0.054 sec/batch; 90h:25m:08s remains)
INFO - root - 2019-11-03 23:11:15.839526: step 11060, total loss = 0.94, predict loss = 0.22 (83.0 examples/sec; 0.048 sec/batch; 80h:09m:39s remains)
INFO - root - 2019-11-03 23:11:16.460635: step 11070, total loss = 0.89, predict loss = 0.19 (75.4 examples/sec; 0.053 sec/batch; 88h:13m:37s remains)
INFO - root - 2019-11-03 23:11:17.080611: step 11080, total loss = 1.04, predict loss = 0.25 (72.1 examples/sec; 0.055 sec/batch; 92h:18m:13s remains)
INFO - root - 2019-11-03 23:11:17.737498: step 11090, total loss = 0.92, predict loss = 0.22 (66.0 examples/sec; 0.061 sec/batch; 100h:50m:22s remains)
INFO - root - 2019-11-03 23:11:18.375417: step 11100, total loss = 0.88, predict loss = 0.20 (68.1 examples/sec; 0.059 sec/batch; 97h:40m:17s remains)
INFO - root - 2019-11-03 23:11:19.016202: step 11110, total loss = 0.99, predict loss = 0.21 (67.0 examples/sec; 0.060 sec/batch; 99h:20m:02s remains)
INFO - root - 2019-11-03 23:11:19.640243: step 11120, total loss = 0.64, predict loss = 0.14 (71.1 examples/sec; 0.056 sec/batch; 93h:32m:37s remains)
INFO - root - 2019-11-03 23:11:20.274236: step 11130, total loss = 1.06, predict loss = 0.26 (79.7 examples/sec; 0.050 sec/batch; 83h:32m:32s remains)
INFO - root - 2019-11-03 23:11:20.896155: step 11140, total loss = 0.87, predict loss = 0.21 (64.2 examples/sec; 0.062 sec/batch; 103h:37m:18s remains)
INFO - root - 2019-11-03 23:11:21.598220: step 11150, total loss = 1.08, predict loss = 0.24 (61.6 examples/sec; 0.065 sec/batch; 108h:03m:40s remains)
INFO - root - 2019-11-03 23:11:22.233643: step 11160, total loss = 1.03, predict loss = 0.24 (68.5 examples/sec; 0.058 sec/batch; 97h:07m:07s remains)
INFO - root - 2019-11-03 23:11:22.847450: step 11170, total loss = 0.86, predict loss = 0.21 (77.8 examples/sec; 0.051 sec/batch; 85h:30m:01s remains)
INFO - root - 2019-11-03 23:11:23.463907: step 11180, total loss = 1.16, predict loss = 0.28 (71.1 examples/sec; 0.056 sec/batch; 93h:35m:01s remains)
INFO - root - 2019-11-03 23:11:24.185094: step 11190, total loss = 0.89, predict loss = 0.20 (57.0 examples/sec; 0.070 sec/batch; 116h:38m:47s remains)
INFO - root - 2019-11-03 23:11:24.778796: step 11200, total loss = 0.81, predict loss = 0.18 (80.1 examples/sec; 0.050 sec/batch; 83h:03m:37s remains)
INFO - root - 2019-11-03 23:11:25.420027: step 11210, total loss = 1.04, predict loss = 0.23 (75.2 examples/sec; 0.053 sec/batch; 88h:26m:09s remains)
INFO - root - 2019-11-03 23:11:26.037547: step 11220, total loss = 1.10, predict loss = 0.25 (72.6 examples/sec; 0.055 sec/batch; 91h:38m:45s remains)
INFO - root - 2019-11-03 23:11:26.672921: step 11230, total loss = 1.03, predict loss = 0.23 (68.7 examples/sec; 0.058 sec/batch; 96h:53m:45s remains)
INFO - root - 2019-11-03 23:11:27.318831: step 11240, total loss = 0.94, predict loss = 0.22 (79.1 examples/sec; 0.051 sec/batch; 84h:08m:11s remains)
INFO - root - 2019-11-03 23:11:27.954562: step 11250, total loss = 1.09, predict loss = 0.24 (73.6 examples/sec; 0.054 sec/batch; 90h:25m:56s remains)
INFO - root - 2019-11-03 23:11:28.540806: step 11260, total loss = 0.94, predict loss = 0.23 (69.1 examples/sec; 0.058 sec/batch; 96h:14m:53s remains)
INFO - root - 2019-11-03 23:11:29.140584: step 11270, total loss = 1.00, predict loss = 0.23 (73.0 examples/sec; 0.055 sec/batch; 91h:09m:06s remains)
INFO - root - 2019-11-03 23:11:29.736823: step 11280, total loss = 0.78, predict loss = 0.17 (72.1 examples/sec; 0.056 sec/batch; 92h:21m:09s remains)
INFO - root - 2019-11-03 23:11:30.320553: step 11290, total loss = 0.76, predict loss = 0.16 (79.6 examples/sec; 0.050 sec/batch; 83h:34m:23s remains)
INFO - root - 2019-11-03 23:11:30.936059: step 11300, total loss = 0.71, predict loss = 0.16 (75.9 examples/sec; 0.053 sec/batch; 87h:42m:23s remains)
INFO - root - 2019-11-03 23:11:31.534594: step 11310, total loss = 0.88, predict loss = 0.18 (73.6 examples/sec; 0.054 sec/batch; 90h:26m:50s remains)
INFO - root - 2019-11-03 23:11:32.145870: step 11320, total loss = 0.87, predict loss = 0.20 (74.5 examples/sec; 0.054 sec/batch; 89h:22m:01s remains)
INFO - root - 2019-11-03 23:11:32.809030: step 11330, total loss = 0.91, predict loss = 0.21 (63.6 examples/sec; 0.063 sec/batch; 104h:41m:36s remains)
INFO - root - 2019-11-03 23:11:33.429201: step 11340, total loss = 1.08, predict loss = 0.26 (65.8 examples/sec; 0.061 sec/batch; 101h:07m:03s remains)
INFO - root - 2019-11-03 23:11:34.051223: step 11350, total loss = 0.73, predict loss = 0.18 (83.2 examples/sec; 0.048 sec/batch; 79h:55m:45s remains)
INFO - root - 2019-11-03 23:11:34.652218: step 11360, total loss = 1.40, predict loss = 0.40 (90.1 examples/sec; 0.044 sec/batch; 73h:50m:54s remains)
INFO - root - 2019-11-03 23:11:35.222726: step 11370, total loss = 0.94, predict loss = 0.23 (91.0 examples/sec; 0.044 sec/batch; 73h:09m:36s remains)
INFO - root - 2019-11-03 23:11:35.813973: step 11380, total loss = 0.89, predict loss = 0.22 (77.8 examples/sec; 0.051 sec/batch; 85h:32m:58s remains)
INFO - root - 2019-11-03 23:11:36.434595: step 11390, total loss = 0.66, predict loss = 0.15 (63.7 examples/sec; 0.063 sec/batch; 104h:29m:24s remains)
INFO - root - 2019-11-03 23:11:37.059779: step 11400, total loss = 0.67, predict loss = 0.17 (65.9 examples/sec; 0.061 sec/batch; 100h:58m:06s remains)
INFO - root - 2019-11-03 23:11:37.691829: step 11410, total loss = 1.29, predict loss = 0.33 (76.6 examples/sec; 0.052 sec/batch; 86h:49m:44s remains)
INFO - root - 2019-11-03 23:11:38.337483: step 11420, total loss = 0.76, predict loss = 0.18 (64.5 examples/sec; 0.062 sec/batch; 103h:10m:43s remains)
INFO - root - 2019-11-03 23:11:38.969899: step 11430, total loss = 1.00, predict loss = 0.24 (70.1 examples/sec; 0.057 sec/batch; 94h:57m:23s remains)
INFO - root - 2019-11-03 23:11:39.616235: step 11440, total loss = 0.77, predict loss = 0.16 (73.6 examples/sec; 0.054 sec/batch; 90h:25m:46s remains)
INFO - root - 2019-11-03 23:11:40.224027: step 11450, total loss = 0.70, predict loss = 0.18 (69.6 examples/sec; 0.057 sec/batch; 95h:38m:25s remains)
INFO - root - 2019-11-03 23:11:40.840799: step 11460, total loss = 0.75, predict loss = 0.18 (72.2 examples/sec; 0.055 sec/batch; 92h:09m:01s remains)
INFO - root - 2019-11-03 23:11:41.463416: step 11470, total loss = 0.64, predict loss = 0.15 (68.4 examples/sec; 0.059 sec/batch; 97h:19m:54s remains)
INFO - root - 2019-11-03 23:11:42.108293: step 11480, total loss = 0.71, predict loss = 0.15 (69.9 examples/sec; 0.057 sec/batch; 95h:10m:21s remains)
INFO - root - 2019-11-03 23:11:42.723965: step 11490, total loss = 0.63, predict loss = 0.17 (80.0 examples/sec; 0.050 sec/batch; 83h:09m:06s remains)
INFO - root - 2019-11-03 23:11:43.353335: step 11500, total loss = 0.56, predict loss = 0.13 (66.8 examples/sec; 0.060 sec/batch; 99h:35m:20s remains)
INFO - root - 2019-11-03 23:11:43.988556: step 11510, total loss = 0.53, predict loss = 0.13 (63.8 examples/sec; 0.063 sec/batch; 104h:15m:17s remains)
INFO - root - 2019-11-03 23:11:44.650183: step 11520, total loss = 0.79, predict loss = 0.19 (75.9 examples/sec; 0.053 sec/batch; 87h:38m:56s remains)
INFO - root - 2019-11-03 23:11:45.264358: step 11530, total loss = 0.81, predict loss = 0.18 (65.5 examples/sec; 0.061 sec/batch; 101h:32m:14s remains)
INFO - root - 2019-11-03 23:11:45.907547: step 11540, total loss = 0.68, predict loss = 0.15 (63.1 examples/sec; 0.063 sec/batch; 105h:29m:05s remains)
INFO - root - 2019-11-03 23:11:46.517228: step 11550, total loss = 0.69, predict loss = 0.16 (69.7 examples/sec; 0.057 sec/batch; 95h:29m:19s remains)
INFO - root - 2019-11-03 23:11:47.137476: step 11560, total loss = 0.79, predict loss = 0.18 (72.7 examples/sec; 0.055 sec/batch; 91h:28m:43s remains)
INFO - root - 2019-11-03 23:11:47.739914: step 11570, total loss = 0.87, predict loss = 0.19 (72.1 examples/sec; 0.055 sec/batch; 92h:15m:02s remains)
INFO - root - 2019-11-03 23:11:48.373620: step 11580, total loss = 1.07, predict loss = 0.24 (61.8 examples/sec; 0.065 sec/batch; 107h:41m:35s remains)
INFO - root - 2019-11-03 23:11:49.069438: step 11590, total loss = 0.76, predict loss = 0.17 (65.2 examples/sec; 0.061 sec/batch; 102h:01m:21s remains)
INFO - root - 2019-11-03 23:11:49.774460: step 11600, total loss = 1.01, predict loss = 0.25 (65.6 examples/sec; 0.061 sec/batch; 101h:29m:45s remains)
INFO - root - 2019-11-03 23:11:50.438055: step 11610, total loss = 1.09, predict loss = 0.26 (70.3 examples/sec; 0.057 sec/batch; 94h:36m:48s remains)
INFO - root - 2019-11-03 23:11:51.097626: step 11620, total loss = 1.68, predict loss = 0.43 (68.4 examples/sec; 0.058 sec/batch; 97h:13m:26s remains)
INFO - root - 2019-11-03 23:11:51.731015: step 11630, total loss = 1.05, predict loss = 0.28 (71.3 examples/sec; 0.056 sec/batch; 93h:16m:39s remains)
INFO - root - 2019-11-03 23:11:52.372086: step 11640, total loss = 0.95, predict loss = 0.21 (76.9 examples/sec; 0.052 sec/batch; 86h:32m:02s remains)
INFO - root - 2019-11-03 23:11:53.053570: step 11650, total loss = 0.95, predict loss = 0.22 (61.5 examples/sec; 0.065 sec/batch; 108h:13m:10s remains)
INFO - root - 2019-11-03 23:11:53.679457: step 11660, total loss = 0.95, predict loss = 0.21 (69.6 examples/sec; 0.057 sec/batch; 95h:35m:03s remains)
INFO - root - 2019-11-03 23:11:54.417369: step 11670, total loss = 0.86, predict loss = 0.21 (76.4 examples/sec; 0.052 sec/batch; 87h:02m:29s remains)
INFO - root - 2019-11-03 23:11:55.022685: step 11680, total loss = 0.80, predict loss = 0.19 (74.8 examples/sec; 0.053 sec/batch; 88h:57m:44s remains)
INFO - root - 2019-11-03 23:11:55.645068: step 11690, total loss = 0.79, predict loss = 0.20 (64.0 examples/sec; 0.062 sec/batch; 103h:56m:55s remains)
INFO - root - 2019-11-03 23:11:56.268357: step 11700, total loss = 0.86, predict loss = 0.19 (72.7 examples/sec; 0.055 sec/batch; 91h:34m:55s remains)
INFO - root - 2019-11-03 23:11:56.898784: step 11710, total loss = 1.09, predict loss = 0.26 (65.6 examples/sec; 0.061 sec/batch; 101h:24m:23s remains)
INFO - root - 2019-11-03 23:11:57.501493: step 11720, total loss = 0.93, predict loss = 0.21 (74.8 examples/sec; 0.053 sec/batch; 88h:58m:52s remains)
INFO - root - 2019-11-03 23:11:58.117937: step 11730, total loss = 0.63, predict loss = 0.13 (63.9 examples/sec; 0.063 sec/batch; 104h:03m:59s remains)
INFO - root - 2019-11-03 23:11:58.742196: step 11740, total loss = 0.74, predict loss = 0.15 (64.3 examples/sec; 0.062 sec/batch; 103h:28m:13s remains)
INFO - root - 2019-11-03 23:11:59.384011: step 11750, total loss = 0.71, predict loss = 0.16 (74.6 examples/sec; 0.054 sec/batch; 89h:10m:54s remains)
INFO - root - 2019-11-03 23:12:00.005512: step 11760, total loss = 0.42, predict loss = 0.09 (64.3 examples/sec; 0.062 sec/batch; 103h:25m:31s remains)
INFO - root - 2019-11-03 23:12:00.664026: step 11770, total loss = 0.68, predict loss = 0.17 (59.0 examples/sec; 0.068 sec/batch; 112h:43m:00s remains)
INFO - root - 2019-11-03 23:12:01.299779: step 11780, total loss = 0.71, predict loss = 0.16 (65.5 examples/sec; 0.061 sec/batch; 101h:36m:23s remains)
INFO - root - 2019-11-03 23:12:01.959339: step 11790, total loss = 0.77, predict loss = 0.21 (73.3 examples/sec; 0.055 sec/batch; 90h:45m:36s remains)
INFO - root - 2019-11-03 23:12:02.621751: step 11800, total loss = 0.98, predict loss = 0.25 (72.6 examples/sec; 0.055 sec/batch; 91h:39m:23s remains)
INFO - root - 2019-11-03 23:12:03.236191: step 11810, total loss = 0.73, predict loss = 0.18 (65.9 examples/sec; 0.061 sec/batch; 100h:57m:39s remains)
INFO - root - 2019-11-03 23:12:03.851738: step 11820, total loss = 0.95, predict loss = 0.22 (67.2 examples/sec; 0.059 sec/batch; 98h:57m:49s remains)
INFO - root - 2019-11-03 23:12:04.482554: step 11830, total loss = 0.85, predict loss = 0.22 (68.0 examples/sec; 0.059 sec/batch; 97h:53m:08s remains)
INFO - root - 2019-11-03 23:12:05.120444: step 11840, total loss = 0.60, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 95h:26m:31s remains)
INFO - root - 2019-11-03 23:12:05.748574: step 11850, total loss = 0.66, predict loss = 0.15 (73.1 examples/sec; 0.055 sec/batch; 91h:03m:12s remains)
INFO - root - 2019-11-03 23:12:06.378825: step 11860, total loss = 0.83, predict loss = 0.19 (65.4 examples/sec; 0.061 sec/batch; 101h:40m:08s remains)
INFO - root - 2019-11-03 23:12:06.974500: step 11870, total loss = 0.52, predict loss = 0.11 (79.8 examples/sec; 0.050 sec/batch; 83h:25m:12s remains)
INFO - root - 2019-11-03 23:12:07.610708: step 11880, total loss = 0.59, predict loss = 0.13 (68.5 examples/sec; 0.058 sec/batch; 97h:10m:35s remains)
INFO - root - 2019-11-03 23:12:08.280158: step 11890, total loss = 0.88, predict loss = 0.21 (67.4 examples/sec; 0.059 sec/batch; 98h:45m:05s remains)
INFO - root - 2019-11-03 23:12:08.950262: step 11900, total loss = 0.72, predict loss = 0.18 (64.6 examples/sec; 0.062 sec/batch; 102h:57m:14s remains)
INFO - root - 2019-11-03 23:12:09.613193: step 11910, total loss = 0.90, predict loss = 0.21 (69.3 examples/sec; 0.058 sec/batch; 96h:04m:14s remains)
INFO - root - 2019-11-03 23:12:10.309743: step 11920, total loss = 0.84, predict loss = 0.20 (61.6 examples/sec; 0.065 sec/batch; 108h:03m:17s remains)
INFO - root - 2019-11-03 23:12:10.986238: step 11930, total loss = 0.86, predict loss = 0.20 (60.5 examples/sec; 0.066 sec/batch; 109h:53m:43s remains)
INFO - root - 2019-11-03 23:12:11.635837: step 11940, total loss = 0.86, predict loss = 0.17 (61.2 examples/sec; 0.065 sec/batch; 108h:42m:37s remains)
INFO - root - 2019-11-03 23:12:12.353121: step 11950, total loss = 0.85, predict loss = 0.21 (61.9 examples/sec; 0.065 sec/batch; 107h:34m:14s remains)
INFO - root - 2019-11-03 23:12:13.033856: step 11960, total loss = 0.71, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 102h:10m:42s remains)
INFO - root - 2019-11-03 23:12:13.641521: step 11970, total loss = 0.50, predict loss = 0.11 (81.6 examples/sec; 0.049 sec/batch; 81h:30m:29s remains)
INFO - root - 2019-11-03 23:12:14.276067: step 11980, total loss = 0.55, predict loss = 0.12 (65.3 examples/sec; 0.061 sec/batch; 101h:54m:40s remains)
INFO - root - 2019-11-03 23:12:14.887695: step 11990, total loss = 0.36, predict loss = 0.07 (70.2 examples/sec; 0.057 sec/batch; 94h:50m:21s remains)
INFO - root - 2019-11-03 23:12:15.502517: step 12000, total loss = 0.35, predict loss = 0.07 (79.9 examples/sec; 0.050 sec/batch; 83h:16m:26s remains)
INFO - root - 2019-11-03 23:12:16.109332: step 12010, total loss = 0.50, predict loss = 0.10 (78.8 examples/sec; 0.051 sec/batch; 84h:24m:52s remains)
INFO - root - 2019-11-03 23:12:16.719426: step 12020, total loss = 0.91, predict loss = 0.22 (70.9 examples/sec; 0.056 sec/batch; 93h:47m:23s remains)
INFO - root - 2019-11-03 23:12:17.314330: step 12030, total loss = 0.73, predict loss = 0.17 (73.1 examples/sec; 0.055 sec/batch; 91h:00m:45s remains)
INFO - root - 2019-11-03 23:12:17.969622: step 12040, total loss = 0.83, predict loss = 0.17 (64.1 examples/sec; 0.062 sec/batch; 103h:46m:52s remains)
INFO - root - 2019-11-03 23:12:18.591454: step 12050, total loss = 0.61, predict loss = 0.14 (75.3 examples/sec; 0.053 sec/batch; 88h:18m:50s remains)
INFO - root - 2019-11-03 23:12:19.218172: step 12060, total loss = 0.87, predict loss = 0.20 (73.9 examples/sec; 0.054 sec/batch; 90h:04m:50s remains)
INFO - root - 2019-11-03 23:12:19.844538: step 12070, total loss = 0.81, predict loss = 0.18 (69.8 examples/sec; 0.057 sec/batch; 95h:20m:06s remains)
INFO - root - 2019-11-03 23:12:20.479709: step 12080, total loss = 0.60, predict loss = 0.14 (70.1 examples/sec; 0.057 sec/batch; 94h:54m:03s remains)
INFO - root - 2019-11-03 23:12:21.084906: step 12090, total loss = 0.72, predict loss = 0.17 (84.5 examples/sec; 0.047 sec/batch; 78h:44m:17s remains)
INFO - root - 2019-11-03 23:12:21.701433: step 12100, total loss = 0.65, predict loss = 0.15 (75.1 examples/sec; 0.053 sec/batch; 88h:38m:51s remains)
INFO - root - 2019-11-03 23:12:22.327041: step 12110, total loss = 0.50, predict loss = 0.11 (73.7 examples/sec; 0.054 sec/batch; 90h:20m:07s remains)
INFO - root - 2019-11-03 23:12:22.945406: step 12120, total loss = 0.56, predict loss = 0.12 (80.3 examples/sec; 0.050 sec/batch; 82h:53m:28s remains)
INFO - root - 2019-11-03 23:12:23.597498: step 12130, total loss = 0.82, predict loss = 0.19 (68.6 examples/sec; 0.058 sec/batch; 97h:01m:29s remains)
INFO - root - 2019-11-03 23:12:24.302580: step 12140, total loss = 0.76, predict loss = 0.17 (76.5 examples/sec; 0.052 sec/batch; 86h:59m:19s remains)
INFO - root - 2019-11-03 23:12:24.888954: step 12150, total loss = 0.71, predict loss = 0.17 (75.5 examples/sec; 0.053 sec/batch; 88h:07m:37s remains)
INFO - root - 2019-11-03 23:12:25.475507: step 12160, total loss = 0.67, predict loss = 0.16 (70.5 examples/sec; 0.057 sec/batch; 94h:21m:26s remains)
INFO - root - 2019-11-03 23:12:26.095119: step 12170, total loss = 0.93, predict loss = 0.21 (67.5 examples/sec; 0.059 sec/batch; 98h:32m:26s remains)
INFO - root - 2019-11-03 23:12:26.729286: step 12180, total loss = 0.78, predict loss = 0.17 (66.0 examples/sec; 0.061 sec/batch; 100h:45m:09s remains)
INFO - root - 2019-11-03 23:12:27.349945: step 12190, total loss = 0.73, predict loss = 0.15 (71.3 examples/sec; 0.056 sec/batch; 93h:17m:05s remains)
INFO - root - 2019-11-03 23:12:27.982431: step 12200, total loss = 0.87, predict loss = 0.21 (72.5 examples/sec; 0.055 sec/batch; 91h:46m:14s remains)
INFO - root - 2019-11-03 23:12:28.605216: step 12210, total loss = 0.98, predict loss = 0.24 (91.6 examples/sec; 0.044 sec/batch; 72h:40m:17s remains)
INFO - root - 2019-11-03 23:12:29.212355: step 12220, total loss = 0.73, predict loss = 0.16 (80.8 examples/sec; 0.050 sec/batch; 82h:20m:15s remains)
INFO - root - 2019-11-03 23:12:29.852078: step 12230, total loss = 0.89, predict loss = 0.20 (66.3 examples/sec; 0.060 sec/batch; 100h:18m:41s remains)
INFO - root - 2019-11-03 23:12:30.509801: step 12240, total loss = 0.52, predict loss = 0.11 (69.3 examples/sec; 0.058 sec/batch; 96h:03m:03s remains)
INFO - root - 2019-11-03 23:12:31.171247: step 12250, total loss = 0.71, predict loss = 0.17 (71.1 examples/sec; 0.056 sec/batch; 93h:35m:41s remains)
INFO - root - 2019-11-03 23:12:31.861984: step 12260, total loss = 0.70, predict loss = 0.17 (62.1 examples/sec; 0.064 sec/batch; 107h:11m:40s remains)
INFO - root - 2019-11-03 23:12:32.487024: step 12270, total loss = 0.50, predict loss = 0.11 (65.1 examples/sec; 0.061 sec/batch; 102h:15m:04s remains)
INFO - root - 2019-11-03 23:12:33.107496: step 12280, total loss = 0.63, predict loss = 0.15 (75.9 examples/sec; 0.053 sec/batch; 87h:37m:10s remains)
INFO - root - 2019-11-03 23:12:33.687267: step 12290, total loss = 0.52, predict loss = 0.12 (75.9 examples/sec; 0.053 sec/batch; 87h:37m:31s remains)
INFO - root - 2019-11-03 23:12:34.328075: step 12300, total loss = 0.60, predict loss = 0.14 (58.8 examples/sec; 0.068 sec/batch; 113h:08m:12s remains)
INFO - root - 2019-11-03 23:12:34.962229: step 12310, total loss = 0.70, predict loss = 0.17 (70.7 examples/sec; 0.057 sec/batch; 94h:03m:59s remains)
INFO - root - 2019-11-03 23:12:35.613835: step 12320, total loss = 0.88, predict loss = 0.19 (65.5 examples/sec; 0.061 sec/batch; 101h:33m:23s remains)
INFO - root - 2019-11-03 23:12:36.216537: step 12330, total loss = 1.32, predict loss = 0.31 (78.3 examples/sec; 0.051 sec/batch; 84h:56m:24s remains)
INFO - root - 2019-11-03 23:12:36.876279: step 12340, total loss = 1.32, predict loss = 0.32 (70.1 examples/sec; 0.057 sec/batch; 94h:55m:38s remains)
INFO - root - 2019-11-03 23:12:37.512846: step 12350, total loss = 1.41, predict loss = 0.36 (61.3 examples/sec; 0.065 sec/batch; 108h:29m:48s remains)
INFO - root - 2019-11-03 23:12:38.207491: step 12360, total loss = 1.10, predict loss = 0.26 (72.0 examples/sec; 0.056 sec/batch; 92h:26m:41s remains)
INFO - root - 2019-11-03 23:12:38.818787: step 12370, total loss = 0.99, predict loss = 0.23 (66.2 examples/sec; 0.060 sec/batch; 100h:27m:32s remains)
INFO - root - 2019-11-03 23:12:39.492657: step 12380, total loss = 1.33, predict loss = 0.32 (54.6 examples/sec; 0.073 sec/batch; 121h:57m:01s remains)
INFO - root - 2019-11-03 23:12:40.116506: step 12390, total loss = 0.94, predict loss = 0.22 (76.4 examples/sec; 0.052 sec/batch; 87h:07m:45s remains)
INFO - root - 2019-11-03 23:12:40.725282: step 12400, total loss = 1.06, predict loss = 0.26 (74.4 examples/sec; 0.054 sec/batch; 89h:26m:17s remains)
INFO - root - 2019-11-03 23:12:41.349720: step 12410, total loss = 1.17, predict loss = 0.29 (71.5 examples/sec; 0.056 sec/batch; 93h:01m:33s remains)
INFO - root - 2019-11-03 23:12:41.953281: step 12420, total loss = 0.85, predict loss = 0.19 (73.7 examples/sec; 0.054 sec/batch; 90h:17m:18s remains)
INFO - root - 2019-11-03 23:12:42.590627: step 12430, total loss = 0.99, predict loss = 0.22 (67.2 examples/sec; 0.060 sec/batch; 99h:04m:21s remains)
INFO - root - 2019-11-03 23:12:43.198627: step 12440, total loss = 1.06, predict loss = 0.25 (72.6 examples/sec; 0.055 sec/batch; 91h:38m:35s remains)
INFO - root - 2019-11-03 23:12:43.812770: step 12450, total loss = 0.69, predict loss = 0.16 (68.4 examples/sec; 0.058 sec/batch; 97h:16m:36s remains)
INFO - root - 2019-11-03 23:12:44.422110: step 12460, total loss = 1.01, predict loss = 0.23 (69.5 examples/sec; 0.058 sec/batch; 95h:40m:22s remains)
INFO - root - 2019-11-03 23:12:45.061457: step 12470, total loss = 0.99, predict loss = 0.23 (71.9 examples/sec; 0.056 sec/batch; 92h:28m:46s remains)
INFO - root - 2019-11-03 23:12:45.681977: step 12480, total loss = 0.72, predict loss = 0.18 (63.0 examples/sec; 0.064 sec/batch; 105h:39m:24s remains)
INFO - root - 2019-11-03 23:12:46.365253: step 12490, total loss = 0.91, predict loss = 0.23 (68.3 examples/sec; 0.059 sec/batch; 97h:22m:49s remains)
INFO - root - 2019-11-03 23:12:46.998142: step 12500, total loss = 0.98, predict loss = 0.23 (69.3 examples/sec; 0.058 sec/batch; 95h:58m:56s remains)
INFO - root - 2019-11-03 23:12:47.642842: step 12510, total loss = 0.97, predict loss = 0.26 (59.9 examples/sec; 0.067 sec/batch; 111h:05m:33s remains)
INFO - root - 2019-11-03 23:12:48.367443: step 12520, total loss = 0.86, predict loss = 0.19 (64.0 examples/sec; 0.062 sec/batch; 103h:56m:44s remains)
INFO - root - 2019-11-03 23:12:48.967371: step 12530, total loss = 0.77, predict loss = 0.19 (70.0 examples/sec; 0.057 sec/batch; 95h:05m:54s remains)
INFO - root - 2019-11-03 23:12:49.607874: step 12540, total loss = 0.68, predict loss = 0.17 (67.4 examples/sec; 0.059 sec/batch; 98h:44m:35s remains)
INFO - root - 2019-11-03 23:12:50.302222: step 12550, total loss = 0.89, predict loss = 0.19 (55.4 examples/sec; 0.072 sec/batch; 120h:02m:54s remains)
INFO - root - 2019-11-03 23:12:51.061020: step 12560, total loss = 0.96, predict loss = 0.23 (71.0 examples/sec; 0.056 sec/batch; 93h:44m:06s remains)
INFO - root - 2019-11-03 23:12:51.872545: step 12570, total loss = 0.79, predict loss = 0.19 (53.2 examples/sec; 0.075 sec/batch; 124h:56m:45s remains)
INFO - root - 2019-11-03 23:12:52.609180: step 12580, total loss = 0.75, predict loss = 0.18 (77.1 examples/sec; 0.052 sec/batch; 86h:16m:05s remains)
INFO - root - 2019-11-03 23:12:53.189814: step 12590, total loss = 0.61, predict loss = 0.15 (67.0 examples/sec; 0.060 sec/batch; 99h:19m:25s remains)
INFO - root - 2019-11-03 23:12:53.826703: step 12600, total loss = 0.59, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 95h:34m:03s remains)
INFO - root - 2019-11-03 23:12:54.472421: step 12610, total loss = 0.54, predict loss = 0.12 (64.7 examples/sec; 0.062 sec/batch; 102h:53m:20s remains)
INFO - root - 2019-11-03 23:12:55.091794: step 12620, total loss = 0.79, predict loss = 0.19 (74.9 examples/sec; 0.053 sec/batch; 88h:50m:36s remains)
INFO - root - 2019-11-03 23:12:55.763847: step 12630, total loss = 0.58, predict loss = 0.12 (61.4 examples/sec; 0.065 sec/batch; 108h:19m:03s remains)
INFO - root - 2019-11-03 23:12:56.516571: step 12640, total loss = 0.57, predict loss = 0.14 (64.1 examples/sec; 0.062 sec/batch; 103h:50m:58s remains)
INFO - root - 2019-11-03 23:12:57.095543: step 12650, total loss = 0.68, predict loss = 0.14 (83.2 examples/sec; 0.048 sec/batch; 79h:58m:57s remains)
INFO - root - 2019-11-03 23:12:57.748985: step 12660, total loss = 0.67, predict loss = 0.16 (61.2 examples/sec; 0.065 sec/batch; 108h:44m:12s remains)
INFO - root - 2019-11-03 23:12:58.352514: step 12670, total loss = 0.61, predict loss = 0.14 (70.9 examples/sec; 0.056 sec/batch; 93h:51m:22s remains)
INFO - root - 2019-11-03 23:12:58.947844: step 12680, total loss = 0.70, predict loss = 0.16 (65.9 examples/sec; 0.061 sec/batch; 100h:59m:35s remains)
INFO - root - 2019-11-03 23:12:59.579410: step 12690, total loss = 0.69, predict loss = 0.16 (66.0 examples/sec; 0.061 sec/batch; 100h:51m:31s remains)
INFO - root - 2019-11-03 23:13:00.199118: step 12700, total loss = 0.80, predict loss = 0.19 (68.0 examples/sec; 0.059 sec/batch; 97h:47m:41s remains)
INFO - root - 2019-11-03 23:13:00.836404: step 12710, total loss = 0.73, predict loss = 0.17 (67.7 examples/sec; 0.059 sec/batch; 98h:14m:48s remains)
INFO - root - 2019-11-03 23:13:01.451394: step 12720, total loss = 0.78, predict loss = 0.18 (63.6 examples/sec; 0.063 sec/batch; 104h:33m:41s remains)
INFO - root - 2019-11-03 23:13:02.114256: step 12730, total loss = 0.96, predict loss = 0.21 (71.8 examples/sec; 0.056 sec/batch; 92h:37m:39s remains)
INFO - root - 2019-11-03 23:13:02.726991: step 12740, total loss = 0.73, predict loss = 0.18 (75.1 examples/sec; 0.053 sec/batch; 88h:35m:34s remains)
INFO - root - 2019-11-03 23:13:03.350003: step 12750, total loss = 0.79, predict loss = 0.17 (73.7 examples/sec; 0.054 sec/batch; 90h:15m:46s remains)
INFO - root - 2019-11-03 23:13:03.943466: step 12760, total loss = 0.93, predict loss = 0.23 (76.1 examples/sec; 0.053 sec/batch; 87h:24m:55s remains)
INFO - root - 2019-11-03 23:13:04.585303: step 12770, total loss = 0.84, predict loss = 0.19 (71.9 examples/sec; 0.056 sec/batch; 92h:29m:43s remains)
INFO - root - 2019-11-03 23:13:05.277527: step 12780, total loss = 0.95, predict loss = 0.24 (72.0 examples/sec; 0.056 sec/batch; 92h:20m:55s remains)
INFO - root - 2019-11-03 23:13:05.918194: step 12790, total loss = 1.03, predict loss = 0.23 (62.3 examples/sec; 0.064 sec/batch; 106h:51m:31s remains)
INFO - root - 2019-11-03 23:13:06.569971: step 12800, total loss = 1.28, predict loss = 0.32 (78.6 examples/sec; 0.051 sec/batch; 84h:38m:05s remains)
INFO - root - 2019-11-03 23:13:07.157612: step 12810, total loss = 1.21, predict loss = 0.27 (77.5 examples/sec; 0.052 sec/batch; 85h:51m:21s remains)
INFO - root - 2019-11-03 23:13:07.881415: step 12820, total loss = 0.98, predict loss = 0.24 (55.6 examples/sec; 0.072 sec/batch; 119h:37m:53s remains)
INFO - root - 2019-11-03 23:13:08.509664: step 12830, total loss = 0.93, predict loss = 0.24 (84.3 examples/sec; 0.047 sec/batch; 78h:52m:33s remains)
INFO - root - 2019-11-03 23:13:09.133197: step 12840, total loss = 0.86, predict loss = 0.19 (67.3 examples/sec; 0.059 sec/batch; 98h:47m:19s remains)
INFO - root - 2019-11-03 23:13:09.758916: step 12850, total loss = 0.77, predict loss = 0.17 (65.9 examples/sec; 0.061 sec/batch; 100h:59m:10s remains)
INFO - root - 2019-11-03 23:13:10.438882: step 12860, total loss = 0.85, predict loss = 0.21 (67.6 examples/sec; 0.059 sec/batch; 98h:26m:54s remains)
INFO - root - 2019-11-03 23:13:11.083896: step 12870, total loss = 0.57, predict loss = 0.12 (64.9 examples/sec; 0.062 sec/batch; 102h:31m:59s remains)
INFO - root - 2019-11-03 23:13:11.850676: step 12880, total loss = 0.53, predict loss = 0.12 (43.8 examples/sec; 0.091 sec/batch; 152h:03m:09s remains)
INFO - root - 2019-11-03 23:13:12.526994: step 12890, total loss = 0.59, predict loss = 0.13 (72.5 examples/sec; 0.055 sec/batch; 91h:48m:04s remains)
INFO - root - 2019-11-03 23:13:13.188301: step 12900, total loss = 0.67, predict loss = 0.16 (65.5 examples/sec; 0.061 sec/batch; 101h:36m:21s remains)
INFO - root - 2019-11-03 23:13:13.935474: step 12910, total loss = 0.69, predict loss = 0.16 (61.5 examples/sec; 0.065 sec/batch; 108h:13m:19s remains)
INFO - root - 2019-11-03 23:13:14.579760: step 12920, total loss = 0.65, predict loss = 0.16 (61.9 examples/sec; 0.065 sec/batch; 107h:33m:10s remains)
INFO - root - 2019-11-03 23:13:15.216520: step 12930, total loss = 0.67, predict loss = 0.16 (60.6 examples/sec; 0.066 sec/batch; 109h:51m:04s remains)
INFO - root - 2019-11-03 23:13:15.833836: step 12940, total loss = 0.90, predict loss = 0.23 (68.7 examples/sec; 0.058 sec/batch; 96h:46m:29s remains)
INFO - root - 2019-11-03 23:13:16.441983: step 12950, total loss = 0.91, predict loss = 0.22 (76.0 examples/sec; 0.053 sec/batch; 87h:34m:32s remains)
INFO - root - 2019-11-03 23:13:17.056329: step 12960, total loss = 0.97, predict loss = 0.24 (67.7 examples/sec; 0.059 sec/batch; 98h:15m:13s remains)
INFO - root - 2019-11-03 23:13:17.672430: step 12970, total loss = 0.74, predict loss = 0.17 (73.2 examples/sec; 0.055 sec/batch; 90h:51m:50s remains)
INFO - root - 2019-11-03 23:13:18.306222: step 12980, total loss = 1.01, predict loss = 0.25 (71.9 examples/sec; 0.056 sec/batch; 92h:33m:50s remains)
INFO - root - 2019-11-03 23:13:18.958880: step 12990, total loss = 0.79, predict loss = 0.20 (72.0 examples/sec; 0.056 sec/batch; 92h:26m:55s remains)
INFO - root - 2019-11-03 23:13:19.576006: step 13000, total loss = 0.76, predict loss = 0.19 (74.5 examples/sec; 0.054 sec/batch; 89h:16m:28s remains)
INFO - root - 2019-11-03 23:13:20.192336: step 13010, total loss = 0.86, predict loss = 0.21 (73.3 examples/sec; 0.055 sec/batch; 90h:47m:48s remains)
INFO - root - 2019-11-03 23:13:20.827460: step 13020, total loss = 0.91, predict loss = 0.21 (63.7 examples/sec; 0.063 sec/batch; 104h:22m:03s remains)
INFO - root - 2019-11-03 23:13:21.522346: step 13030, total loss = 1.01, predict loss = 0.24 (65.3 examples/sec; 0.061 sec/batch; 101h:48m:44s remains)
INFO - root - 2019-11-03 23:13:22.211092: step 13040, total loss = 1.02, predict loss = 0.27 (73.2 examples/sec; 0.055 sec/batch; 90h:50m:07s remains)
INFO - root - 2019-11-03 23:13:22.872852: step 13050, total loss = 0.90, predict loss = 0.21 (71.0 examples/sec; 0.056 sec/batch; 93h:44m:25s remains)
INFO - root - 2019-11-03 23:13:23.556375: step 13060, total loss = 0.79, predict loss = 0.18 (53.3 examples/sec; 0.075 sec/batch; 124h:53m:43s remains)
INFO - root - 2019-11-03 23:13:24.271490: step 13070, total loss = 0.75, predict loss = 0.19 (75.1 examples/sec; 0.053 sec/batch; 88h:31m:45s remains)
INFO - root - 2019-11-03 23:13:24.984509: step 13080, total loss = 0.81, predict loss = 0.19 (72.1 examples/sec; 0.056 sec/batch; 92h:18m:59s remains)
INFO - root - 2019-11-03 23:13:25.630551: step 13090, total loss = 0.83, predict loss = 0.17 (75.6 examples/sec; 0.053 sec/batch; 88h:00m:02s remains)
INFO - root - 2019-11-03 23:13:26.217230: step 13100, total loss = 0.80, predict loss = 0.18 (73.1 examples/sec; 0.055 sec/batch; 91h:01m:09s remains)
INFO - root - 2019-11-03 23:13:26.846811: step 13110, total loss = 0.83, predict loss = 0.18 (65.6 examples/sec; 0.061 sec/batch; 101h:21m:04s remains)
INFO - root - 2019-11-03 23:13:27.518009: step 13120, total loss = 0.77, predict loss = 0.15 (68.0 examples/sec; 0.059 sec/batch; 97h:46m:32s remains)
INFO - root - 2019-11-03 23:13:28.159552: step 13130, total loss = 0.70, predict loss = 0.17 (68.3 examples/sec; 0.059 sec/batch; 97h:19m:36s remains)
INFO - root - 2019-11-03 23:13:28.804118: step 13140, total loss = 0.84, predict loss = 0.20 (70.7 examples/sec; 0.057 sec/batch; 94h:05m:45s remains)
INFO - root - 2019-11-03 23:13:29.450367: step 13150, total loss = 0.82, predict loss = 0.21 (76.2 examples/sec; 0.053 sec/batch; 87h:19m:36s remains)
INFO - root - 2019-11-03 23:13:30.082424: step 13160, total loss = 0.61, predict loss = 0.14 (74.6 examples/sec; 0.054 sec/batch; 89h:06m:47s remains)
INFO - root - 2019-11-03 23:13:30.670074: step 13170, total loss = 0.78, predict loss = 0.18 (70.3 examples/sec; 0.057 sec/batch; 94h:40m:02s remains)
INFO - root - 2019-11-03 23:13:31.277471: step 13180, total loss = 0.80, predict loss = 0.18 (68.5 examples/sec; 0.058 sec/batch; 97h:04m:39s remains)
INFO - root - 2019-11-03 23:13:31.937573: step 13190, total loss = 0.72, predict loss = 0.18 (79.4 examples/sec; 0.050 sec/batch; 83h:49m:18s remains)
INFO - root - 2019-11-03 23:13:32.595005: step 13200, total loss = 0.76, predict loss = 0.17 (67.6 examples/sec; 0.059 sec/batch; 98h:21m:31s remains)
INFO - root - 2019-11-03 23:13:33.233273: step 13210, total loss = 0.73, predict loss = 0.18 (69.5 examples/sec; 0.058 sec/batch; 95h:40m:35s remains)
INFO - root - 2019-11-03 23:13:33.870298: step 13220, total loss = 0.88, predict loss = 0.21 (78.1 examples/sec; 0.051 sec/batch; 85h:11m:53s remains)
INFO - root - 2019-11-03 23:13:34.552381: step 13230, total loss = 0.81, predict loss = 0.18 (57.9 examples/sec; 0.069 sec/batch; 114h:50m:05s remains)
INFO - root - 2019-11-03 23:13:35.193792: step 13240, total loss = 0.62, predict loss = 0.15 (70.8 examples/sec; 0.056 sec/batch; 93h:54m:57s remains)
INFO - root - 2019-11-03 23:13:35.850844: step 13250, total loss = 0.90, predict loss = 0.21 (68.9 examples/sec; 0.058 sec/batch; 96h:35m:45s remains)
INFO - root - 2019-11-03 23:13:36.511270: step 13260, total loss = 0.80, predict loss = 0.19 (64.1 examples/sec; 0.062 sec/batch; 103h:44m:11s remains)
INFO - root - 2019-11-03 23:13:37.157708: step 13270, total loss = 0.87, predict loss = 0.21 (73.7 examples/sec; 0.054 sec/batch; 90h:17m:01s remains)
INFO - root - 2019-11-03 23:13:37.760833: step 13280, total loss = 0.83, predict loss = 0.20 (71.4 examples/sec; 0.056 sec/batch; 93h:10m:38s remains)
INFO - root - 2019-11-03 23:13:38.399376: step 13290, total loss = 0.74, predict loss = 0.16 (61.6 examples/sec; 0.065 sec/batch; 107h:55m:02s remains)
INFO - root - 2019-11-03 23:13:39.072014: step 13300, total loss = 0.83, predict loss = 0.21 (76.3 examples/sec; 0.052 sec/batch; 87h:07m:28s remains)
INFO - root - 2019-11-03 23:13:39.667714: step 13310, total loss = 0.68, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 87h:41m:51s remains)
INFO - root - 2019-11-03 23:13:40.273403: step 13320, total loss = 0.78, predict loss = 0.20 (65.4 examples/sec; 0.061 sec/batch; 101h:39m:28s remains)
INFO - root - 2019-11-03 23:13:40.910436: step 13330, total loss = 0.63, predict loss = 0.14 (77.6 examples/sec; 0.052 sec/batch; 85h:45m:46s remains)
INFO - root - 2019-11-03 23:13:41.544639: step 13340, total loss = 0.74, predict loss = 0.18 (73.8 examples/sec; 0.054 sec/batch; 90h:07m:16s remains)
INFO - root - 2019-11-03 23:13:42.201502: step 13350, total loss = 1.01, predict loss = 0.24 (60.1 examples/sec; 0.067 sec/batch; 110h:43m:41s remains)
INFO - root - 2019-11-03 23:13:42.894140: step 13360, total loss = 0.64, predict loss = 0.16 (62.9 examples/sec; 0.064 sec/batch; 105h:45m:36s remains)
INFO - root - 2019-11-03 23:13:43.592757: step 13370, total loss = 1.00, predict loss = 0.24 (60.2 examples/sec; 0.066 sec/batch; 110h:26m:11s remains)
INFO - root - 2019-11-03 23:13:44.192032: step 13380, total loss = 1.03, predict loss = 0.25 (71.8 examples/sec; 0.056 sec/batch; 92h:35m:28s remains)
INFO - root - 2019-11-03 23:13:44.799104: step 13390, total loss = 0.89, predict loss = 0.22 (70.1 examples/sec; 0.057 sec/batch; 94h:52m:30s remains)
INFO - root - 2019-11-03 23:13:45.428290: step 13400, total loss = 0.91, predict loss = 0.22 (67.2 examples/sec; 0.059 sec/batch; 98h:55m:48s remains)
INFO - root - 2019-11-03 23:13:46.068394: step 13410, total loss = 0.81, predict loss = 0.19 (56.5 examples/sec; 0.071 sec/batch; 117h:39m:48s remains)
INFO - root - 2019-11-03 23:13:46.766461: step 13420, total loss = 0.82, predict loss = 0.20 (60.3 examples/sec; 0.066 sec/batch; 110h:24m:04s remains)
INFO - root - 2019-11-03 23:13:47.430766: step 13430, total loss = 0.79, predict loss = 0.18 (65.3 examples/sec; 0.061 sec/batch; 101h:47m:58s remains)
INFO - root - 2019-11-03 23:13:48.091725: step 13440, total loss = 0.96, predict loss = 0.22 (68.9 examples/sec; 0.058 sec/batch; 96h:33m:27s remains)
INFO - root - 2019-11-03 23:13:48.740665: step 13450, total loss = 1.07, predict loss = 0.26 (67.1 examples/sec; 0.060 sec/batch; 99h:10m:00s remains)
INFO - root - 2019-11-03 23:13:49.381560: step 13460, total loss = 1.19, predict loss = 0.29 (74.0 examples/sec; 0.054 sec/batch; 89h:56m:40s remains)
INFO - root - 2019-11-03 23:13:50.006787: step 13470, total loss = 0.90, predict loss = 0.20 (73.8 examples/sec; 0.054 sec/batch; 90h:07m:06s remains)
INFO - root - 2019-11-03 23:13:50.617677: step 13480, total loss = 0.87, predict loss = 0.21 (72.2 examples/sec; 0.055 sec/batch; 92h:04m:02s remains)
INFO - root - 2019-11-03 23:13:51.259981: step 13490, total loss = 0.91, predict loss = 0.21 (67.1 examples/sec; 0.060 sec/batch; 99h:03m:38s remains)
INFO - root - 2019-11-03 23:13:51.935049: step 13500, total loss = 0.76, predict loss = 0.16 (74.4 examples/sec; 0.054 sec/batch; 89h:23m:40s remains)
INFO - root - 2019-11-03 23:13:52.569064: step 13510, total loss = 0.77, predict loss = 0.20 (63.3 examples/sec; 0.063 sec/batch; 105h:09m:08s remains)
INFO - root - 2019-11-03 23:13:53.168693: step 13520, total loss = 0.75, predict loss = 0.17 (67.2 examples/sec; 0.060 sec/batch; 99h:01m:43s remains)
INFO - root - 2019-11-03 23:13:53.808278: step 13530, total loss = 0.89, predict loss = 0.22 (61.8 examples/sec; 0.065 sec/batch; 107h:37m:33s remains)
INFO - root - 2019-11-03 23:13:54.438701: step 13540, total loss = 0.63, predict loss = 0.14 (84.4 examples/sec; 0.047 sec/batch; 78h:50m:55s remains)
INFO - root - 2019-11-03 23:13:55.100519: step 13550, total loss = 0.81, predict loss = 0.19 (62.2 examples/sec; 0.064 sec/batch; 106h:57m:44s remains)
INFO - root - 2019-11-03 23:13:55.763068: step 13560, total loss = 0.82, predict loss = 0.21 (70.7 examples/sec; 0.057 sec/batch; 94h:07m:58s remains)
INFO - root - 2019-11-03 23:13:56.362199: step 13570, total loss = 0.96, predict loss = 0.22 (67.8 examples/sec; 0.059 sec/batch; 98h:06m:00s remains)
INFO - root - 2019-11-03 23:13:56.966542: step 13580, total loss = 0.85, predict loss = 0.22 (68.0 examples/sec; 0.059 sec/batch; 97h:50m:52s remains)
INFO - root - 2019-11-03 23:13:57.578759: step 13590, total loss = 0.93, predict loss = 0.22 (75.6 examples/sec; 0.053 sec/batch; 88h:00m:45s remains)
INFO - root - 2019-11-03 23:13:58.204363: step 13600, total loss = 0.83, predict loss = 0.18 (68.1 examples/sec; 0.059 sec/batch; 97h:38m:43s remains)
INFO - root - 2019-11-03 23:13:58.865134: step 13610, total loss = 0.85, predict loss = 0.19 (62.6 examples/sec; 0.064 sec/batch; 106h:19m:12s remains)
INFO - root - 2019-11-03 23:13:59.401990: step 13620, total loss = 0.78, predict loss = 0.17 (93.4 examples/sec; 0.043 sec/batch; 71h:13m:22s remains)
INFO - root - 2019-11-03 23:14:00.435551: step 13630, total loss = 0.61, predict loss = 0.14 (86.9 examples/sec; 0.046 sec/batch; 76h:34m:28s remains)
INFO - root - 2019-11-03 23:14:01.486027: step 13640, total loss = 0.48, predict loss = 0.10 (71.0 examples/sec; 0.056 sec/batch; 93h:40m:57s remains)
INFO - root - 2019-11-03 23:14:02.107569: step 13650, total loss = 0.67, predict loss = 0.16 (65.7 examples/sec; 0.061 sec/batch; 101h:11m:33s remains)
INFO - root - 2019-11-03 23:14:02.733744: step 13660, total loss = 0.94, predict loss = 0.22 (69.9 examples/sec; 0.057 sec/batch; 95h:12m:19s remains)
INFO - root - 2019-11-03 23:14:03.367655: step 13670, total loss = 0.76, predict loss = 0.17 (72.5 examples/sec; 0.055 sec/batch; 91h:47m:40s remains)
INFO - root - 2019-11-03 23:14:03.976809: step 13680, total loss = 0.99, predict loss = 0.23 (66.7 examples/sec; 0.060 sec/batch; 99h:39m:19s remains)
INFO - root - 2019-11-03 23:14:04.587183: step 13690, total loss = 0.80, predict loss = 0.18 (73.9 examples/sec; 0.054 sec/batch; 90h:01m:37s remains)
INFO - root - 2019-11-03 23:14:05.232200: step 13700, total loss = 0.84, predict loss = 0.19 (65.4 examples/sec; 0.061 sec/batch; 101h:37m:41s remains)
INFO - root - 2019-11-03 23:14:05.851938: step 13710, total loss = 0.87, predict loss = 0.20 (83.5 examples/sec; 0.048 sec/batch; 79h:39m:13s remains)
INFO - root - 2019-11-03 23:14:06.451350: step 13720, total loss = 0.95, predict loss = 0.23 (79.3 examples/sec; 0.050 sec/batch; 83h:50m:04s remains)
INFO - root - 2019-11-03 23:14:07.107956: step 13730, total loss = 1.13, predict loss = 0.29 (67.0 examples/sec; 0.060 sec/batch; 99h:19m:37s remains)
INFO - root - 2019-11-03 23:14:07.797599: step 13740, total loss = 1.27, predict loss = 0.32 (62.5 examples/sec; 0.064 sec/batch; 106h:20m:53s remains)
INFO - root - 2019-11-03 23:14:08.449774: step 13750, total loss = 0.84, predict loss = 0.21 (71.0 examples/sec; 0.056 sec/batch; 93h:41m:46s remains)
INFO - root - 2019-11-03 23:14:09.101172: step 13760, total loss = 1.07, predict loss = 0.23 (66.9 examples/sec; 0.060 sec/batch; 99h:22m:11s remains)
INFO - root - 2019-11-03 23:14:09.783096: step 13770, total loss = 0.86, predict loss = 0.19 (60.2 examples/sec; 0.066 sec/batch; 110h:31m:49s remains)
INFO - root - 2019-11-03 23:14:10.393504: step 13780, total loss = 0.72, predict loss = 0.16 (78.0 examples/sec; 0.051 sec/batch; 85h:15m:17s remains)
INFO - root - 2019-11-03 23:14:10.996968: step 13790, total loss = 0.85, predict loss = 0.18 (78.1 examples/sec; 0.051 sec/batch; 85h:07m:49s remains)
INFO - root - 2019-11-03 23:14:11.637021: step 13800, total loss = 0.94, predict loss = 0.24 (75.0 examples/sec; 0.053 sec/batch; 88h:41m:06s remains)
INFO - root - 2019-11-03 23:14:12.319636: step 13810, total loss = 0.80, predict loss = 0.18 (65.9 examples/sec; 0.061 sec/batch; 100h:52m:27s remains)
INFO - root - 2019-11-03 23:14:12.942663: step 13820, total loss = 1.15, predict loss = 0.25 (76.9 examples/sec; 0.052 sec/batch; 86h:28m:28s remains)
INFO - root - 2019-11-03 23:14:13.574684: step 13830, total loss = 0.67, predict loss = 0.15 (65.4 examples/sec; 0.061 sec/batch; 101h:38m:13s remains)
INFO - root - 2019-11-03 23:14:14.237326: step 13840, total loss = 0.80, predict loss = 0.17 (58.7 examples/sec; 0.068 sec/batch; 113h:24m:12s remains)
INFO - root - 2019-11-03 23:14:14.886126: step 13850, total loss = 0.48, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 98h:52m:38s remains)
INFO - root - 2019-11-03 23:14:15.504454: step 13860, total loss = 0.78, predict loss = 0.18 (66.7 examples/sec; 0.060 sec/batch; 99h:43m:58s remains)
INFO - root - 2019-11-03 23:14:16.141571: step 13870, total loss = 0.81, predict loss = 0.19 (71.9 examples/sec; 0.056 sec/batch; 92h:33m:25s remains)
INFO - root - 2019-11-03 23:14:16.755388: step 13880, total loss = 0.84, predict loss = 0.19 (70.3 examples/sec; 0.057 sec/batch; 94h:36m:33s remains)
INFO - root - 2019-11-03 23:14:17.343213: step 13890, total loss = 1.14, predict loss = 0.26 (79.9 examples/sec; 0.050 sec/batch; 83h:15m:14s remains)
INFO - root - 2019-11-03 23:14:17.975281: step 13900, total loss = 0.67, predict loss = 0.16 (71.0 examples/sec; 0.056 sec/batch; 93h:44m:12s remains)
INFO - root - 2019-11-03 23:14:18.633389: step 13910, total loss = 0.74, predict loss = 0.17 (69.3 examples/sec; 0.058 sec/batch; 95h:57m:28s remains)
INFO - root - 2019-11-03 23:14:19.333072: step 13920, total loss = 0.77, predict loss = 0.18 (62.7 examples/sec; 0.064 sec/batch; 106h:05m:38s remains)
INFO - root - 2019-11-03 23:14:19.967200: step 13930, total loss = 1.03, predict loss = 0.23 (65.8 examples/sec; 0.061 sec/batch; 101h:07m:27s remains)
INFO - root - 2019-11-03 23:14:20.601157: step 13940, total loss = 0.92, predict loss = 0.21 (77.7 examples/sec; 0.051 sec/batch; 85h:37m:34s remains)
INFO - root - 2019-11-03 23:14:21.227971: step 13950, total loss = 0.80, predict loss = 0.18 (74.5 examples/sec; 0.054 sec/batch; 89h:13m:13s remains)
INFO - root - 2019-11-03 23:14:21.897248: step 13960, total loss = 0.97, predict loss = 0.21 (63.5 examples/sec; 0.063 sec/batch; 104h:42m:27s remains)
INFO - root - 2019-11-03 23:14:22.532013: step 13970, total loss = 0.86, predict loss = 0.19 (73.0 examples/sec; 0.055 sec/batch; 91h:02m:57s remains)
INFO - root - 2019-11-03 23:14:23.192933: step 13980, total loss = 1.00, predict loss = 0.21 (59.4 examples/sec; 0.067 sec/batch; 111h:54m:29s remains)
INFO - root - 2019-11-03 23:14:23.852319: step 13990, total loss = 0.89, predict loss = 0.17 (63.9 examples/sec; 0.063 sec/batch; 104h:05m:13s remains)
INFO - root - 2019-11-03 23:14:24.484204: step 14000, total loss = 0.82, predict loss = 0.18 (74.1 examples/sec; 0.054 sec/batch; 89h:43m:57s remains)
INFO - root - 2019-11-03 23:14:25.098960: step 14010, total loss = 0.75, predict loss = 0.16 (75.0 examples/sec; 0.053 sec/batch; 88h:40m:42s remains)
INFO - root - 2019-11-03 23:14:25.683101: step 14020, total loss = 1.04, predict loss = 0.23 (85.7 examples/sec; 0.047 sec/batch; 77h:36m:29s remains)
INFO - root - 2019-11-03 23:14:26.310972: step 14030, total loss = 0.75, predict loss = 0.18 (66.2 examples/sec; 0.060 sec/batch; 100h:32m:07s remains)
INFO - root - 2019-11-03 23:14:26.963478: step 14040, total loss = 0.83, predict loss = 0.19 (66.1 examples/sec; 0.061 sec/batch; 100h:37m:05s remains)
INFO - root - 2019-11-03 23:14:27.598147: step 14050, total loss = 0.86, predict loss = 0.20 (74.2 examples/sec; 0.054 sec/batch; 89h:36m:07s remains)
INFO - root - 2019-11-03 23:14:28.235799: step 14060, total loss = 0.81, predict loss = 0.18 (61.5 examples/sec; 0.065 sec/batch; 108h:12m:30s remains)
INFO - root - 2019-11-03 23:14:28.866811: step 14070, total loss = 0.94, predict loss = 0.24 (74.1 examples/sec; 0.054 sec/batch; 89h:47m:06s remains)
INFO - root - 2019-11-03 23:14:29.511568: step 14080, total loss = 0.76, predict loss = 0.16 (68.0 examples/sec; 0.059 sec/batch; 97h:51m:50s remains)
INFO - root - 2019-11-03 23:14:30.157206: step 14090, total loss = 0.79, predict loss = 0.17 (66.7 examples/sec; 0.060 sec/batch; 99h:43m:04s remains)
INFO - root - 2019-11-03 23:14:30.802975: step 14100, total loss = 0.93, predict loss = 0.24 (67.5 examples/sec; 0.059 sec/batch; 98h:29m:42s remains)
INFO - root - 2019-11-03 23:14:31.462801: step 14110, total loss = 0.86, predict loss = 0.20 (71.1 examples/sec; 0.056 sec/batch; 93h:30m:32s remains)
INFO - root - 2019-11-03 23:14:32.098253: step 14120, total loss = 1.09, predict loss = 0.30 (66.8 examples/sec; 0.060 sec/batch; 99h:34m:43s remains)
INFO - root - 2019-11-03 23:14:32.749411: step 14130, total loss = 0.70, predict loss = 0.15 (65.2 examples/sec; 0.061 sec/batch; 102h:03m:47s remains)
INFO - root - 2019-11-03 23:14:33.386937: step 14140, total loss = 0.75, predict loss = 0.20 (75.3 examples/sec; 0.053 sec/batch; 88h:22m:48s remains)
INFO - root - 2019-11-03 23:14:34.035170: step 14150, total loss = 1.05, predict loss = 0.26 (70.0 examples/sec; 0.057 sec/batch; 95h:00m:12s remains)
INFO - root - 2019-11-03 23:14:34.689145: step 14160, total loss = 0.85, predict loss = 0.21 (72.4 examples/sec; 0.055 sec/batch; 91h:55m:22s remains)
INFO - root - 2019-11-03 23:14:35.322360: step 14170, total loss = 0.85, predict loss = 0.20 (67.4 examples/sec; 0.059 sec/batch; 98h:44m:26s remains)
INFO - root - 2019-11-03 23:14:36.006794: step 14180, total loss = 1.04, predict loss = 0.27 (60.8 examples/sec; 0.066 sec/batch; 109h:26m:46s remains)
INFO - root - 2019-11-03 23:14:36.663296: step 14190, total loss = 0.64, predict loss = 0.16 (67.8 examples/sec; 0.059 sec/batch; 98h:05m:32s remains)
INFO - root - 2019-11-03 23:14:37.379201: step 14200, total loss = 0.86, predict loss = 0.21 (64.4 examples/sec; 0.062 sec/batch; 103h:13m:59s remains)
INFO - root - 2019-11-03 23:14:37.987855: step 14210, total loss = 0.63, predict loss = 0.15 (75.3 examples/sec; 0.053 sec/batch; 88h:17m:14s remains)
INFO - root - 2019-11-03 23:14:38.600943: step 14220, total loss = 0.59, predict loss = 0.16 (80.8 examples/sec; 0.050 sec/batch; 82h:20m:20s remains)
INFO - root - 2019-11-03 23:14:39.230036: step 14230, total loss = 0.52, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 94h:12m:41s remains)
INFO - root - 2019-11-03 23:14:39.849682: step 14240, total loss = 0.63, predict loss = 0.15 (74.2 examples/sec; 0.054 sec/batch; 89h:34m:25s remains)
INFO - root - 2019-11-03 23:14:40.473066: step 14250, total loss = 0.93, predict loss = 0.24 (69.4 examples/sec; 0.058 sec/batch; 95h:48m:40s remains)
INFO - root - 2019-11-03 23:14:41.123679: step 14260, total loss = 0.65, predict loss = 0.16 (69.3 examples/sec; 0.058 sec/batch; 95h:56m:52s remains)
INFO - root - 2019-11-03 23:14:41.822027: step 14270, total loss = 0.74, predict loss = 0.18 (67.6 examples/sec; 0.059 sec/batch; 98h:23m:34s remains)
INFO - root - 2019-11-03 23:14:42.453853: step 14280, total loss = 0.43, predict loss = 0.08 (74.7 examples/sec; 0.054 sec/batch; 89h:02m:52s remains)
INFO - root - 2019-11-03 23:14:43.096502: step 14290, total loss = 0.63, predict loss = 0.14 (65.7 examples/sec; 0.061 sec/batch; 101h:14m:26s remains)
INFO - root - 2019-11-03 23:14:43.753649: step 14300, total loss = 1.15, predict loss = 0.27 (61.5 examples/sec; 0.065 sec/batch; 108h:04m:13s remains)
INFO - root - 2019-11-03 23:14:44.416614: step 14310, total loss = 0.74, predict loss = 0.17 (75.3 examples/sec; 0.053 sec/batch; 88h:18m:23s remains)
INFO - root - 2019-11-03 23:14:45.084391: step 14320, total loss = 0.85, predict loss = 0.21 (64.3 examples/sec; 0.062 sec/batch; 103h:24m:05s remains)
INFO - root - 2019-11-03 23:14:45.717603: step 14330, total loss = 0.92, predict loss = 0.22 (73.2 examples/sec; 0.055 sec/batch; 90h:48m:16s remains)
INFO - root - 2019-11-03 23:14:46.407304: step 14340, total loss = 0.68, predict loss = 0.15 (66.6 examples/sec; 0.060 sec/batch; 99h:49m:37s remains)
INFO - root - 2019-11-03 23:14:47.100068: step 14350, total loss = 0.91, predict loss = 0.21 (58.3 examples/sec; 0.069 sec/batch; 114h:06m:16s remains)
INFO - root - 2019-11-03 23:14:47.722581: step 14360, total loss = 0.64, predict loss = 0.14 (84.5 examples/sec; 0.047 sec/batch; 78h:40m:56s remains)
INFO - root - 2019-11-03 23:14:48.371434: step 14370, total loss = 0.85, predict loss = 0.22 (79.7 examples/sec; 0.050 sec/batch; 83h:27m:51s remains)
INFO - root - 2019-11-03 23:14:49.005182: step 14380, total loss = 0.79, predict loss = 0.20 (69.9 examples/sec; 0.057 sec/batch; 95h:08m:14s remains)
INFO - root - 2019-11-03 23:14:49.636706: step 14390, total loss = 0.98, predict loss = 0.23 (66.6 examples/sec; 0.060 sec/batch; 99h:47m:16s remains)
INFO - root - 2019-11-03 23:14:50.305313: step 14400, total loss = 1.29, predict loss = 0.34 (68.2 examples/sec; 0.059 sec/batch; 97h:35m:10s remains)
INFO - root - 2019-11-03 23:14:50.907390: step 14410, total loss = 0.74, predict loss = 0.19 (67.2 examples/sec; 0.060 sec/batch; 98h:59m:22s remains)
INFO - root - 2019-11-03 23:14:51.537428: step 14420, total loss = 0.90, predict loss = 0.21 (64.0 examples/sec; 0.063 sec/batch; 103h:58m:05s remains)
INFO - root - 2019-11-03 23:14:52.201623: step 14430, total loss = 0.72, predict loss = 0.17 (79.6 examples/sec; 0.050 sec/batch; 83h:30m:19s remains)
INFO - root - 2019-11-03 23:14:52.836306: step 14440, total loss = 0.71, predict loss = 0.16 (66.5 examples/sec; 0.060 sec/batch; 100h:01m:43s remains)
INFO - root - 2019-11-03 23:14:53.473252: step 14450, total loss = 0.57, predict loss = 0.13 (67.9 examples/sec; 0.059 sec/batch; 97h:59m:52s remains)
INFO - root - 2019-11-03 23:14:54.069355: step 14460, total loss = 0.65, predict loss = 0.15 (76.4 examples/sec; 0.052 sec/batch; 87h:03m:17s remains)
INFO - root - 2019-11-03 23:14:54.691427: step 14470, total loss = 0.76, predict loss = 0.17 (68.8 examples/sec; 0.058 sec/batch; 96h:38m:19s remains)
INFO - root - 2019-11-03 23:14:55.297882: step 14480, total loss = 0.44, predict loss = 0.08 (69.0 examples/sec; 0.058 sec/batch; 96h:23m:49s remains)
INFO - root - 2019-11-03 23:14:55.901037: step 14490, total loss = 0.48, predict loss = 0.11 (76.2 examples/sec; 0.053 sec/batch; 87h:18m:39s remains)
INFO - root - 2019-11-03 23:14:56.514814: step 14500, total loss = 0.41, predict loss = 0.09 (74.5 examples/sec; 0.054 sec/batch; 89h:14m:31s remains)
INFO - root - 2019-11-03 23:14:57.127627: step 14510, total loss = 0.72, predict loss = 0.15 (72.4 examples/sec; 0.055 sec/batch; 91h:55m:00s remains)
INFO - root - 2019-11-03 23:14:57.761234: step 14520, total loss = 0.59, predict loss = 0.14 (68.2 examples/sec; 0.059 sec/batch; 97h:27m:41s remains)
INFO - root - 2019-11-03 23:14:58.382151: step 14530, total loss = 0.63, predict loss = 0.14 (72.3 examples/sec; 0.055 sec/batch; 91h:57m:46s remains)
INFO - root - 2019-11-03 23:14:59.020918: step 14540, total loss = 0.48, predict loss = 0.11 (58.7 examples/sec; 0.068 sec/batch; 113h:22m:39s remains)
INFO - root - 2019-11-03 23:14:59.738511: step 14550, total loss = 1.22, predict loss = 0.33 (62.9 examples/sec; 0.064 sec/batch; 105h:47m:29s remains)
INFO - root - 2019-11-03 23:15:00.431145: step 14560, total loss = 0.78, predict loss = 0.17 (69.5 examples/sec; 0.058 sec/batch; 95h:40m:11s remains)
INFO - root - 2019-11-03 23:15:01.032091: step 14570, total loss = 0.66, predict loss = 0.14 (71.7 examples/sec; 0.056 sec/batch; 92h:45m:09s remains)
INFO - root - 2019-11-03 23:15:01.654227: step 14580, total loss = 0.86, predict loss = 0.21 (76.0 examples/sec; 0.053 sec/batch; 87h:31m:25s remains)
INFO - root - 2019-11-03 23:15:02.316719: step 14590, total loss = 0.62, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 90h:57m:31s remains)
INFO - root - 2019-11-03 23:15:02.960256: step 14600, total loss = 0.69, predict loss = 0.16 (68.0 examples/sec; 0.059 sec/batch; 97h:48m:35s remains)
INFO - root - 2019-11-03 23:15:03.562092: step 14610, total loss = 0.65, predict loss = 0.14 (67.8 examples/sec; 0.059 sec/batch; 98h:08m:35s remains)
INFO - root - 2019-11-03 23:15:04.160556: step 14620, total loss = 0.59, predict loss = 0.14 (75.9 examples/sec; 0.053 sec/batch; 87h:39m:49s remains)
INFO - root - 2019-11-03 23:15:04.764446: step 14630, total loss = 0.78, predict loss = 0.18 (70.5 examples/sec; 0.057 sec/batch; 94h:21m:54s remains)
INFO - root - 2019-11-03 23:15:05.401723: step 14640, total loss = 0.92, predict loss = 0.22 (75.5 examples/sec; 0.053 sec/batch; 88h:05m:35s remains)
INFO - root - 2019-11-03 23:15:06.047296: step 14650, total loss = 0.99, predict loss = 0.24 (78.4 examples/sec; 0.051 sec/batch; 84h:46m:53s remains)
INFO - root - 2019-11-03 23:15:06.663986: step 14660, total loss = 1.01, predict loss = 0.24 (84.1 examples/sec; 0.048 sec/batch; 79h:04m:52s remains)
INFO - root - 2019-11-03 23:15:07.311574: step 14670, total loss = 0.86, predict loss = 0.19 (68.8 examples/sec; 0.058 sec/batch; 96h:41m:50s remains)
INFO - root - 2019-11-03 23:15:07.946848: step 14680, total loss = 0.47, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 100h:53m:00s remains)
INFO - root - 2019-11-03 23:15:08.611523: step 14690, total loss = 0.52, predict loss = 0.12 (61.0 examples/sec; 0.066 sec/batch; 108h:56m:23s remains)
INFO - root - 2019-11-03 23:15:09.337333: step 14700, total loss = 0.60, predict loss = 0.14 (63.1 examples/sec; 0.063 sec/batch; 105h:28m:33s remains)
INFO - root - 2019-11-03 23:15:09.989790: step 14710, total loss = 0.48, predict loss = 0.10 (67.1 examples/sec; 0.060 sec/batch; 99h:08m:02s remains)
INFO - root - 2019-11-03 23:15:10.624351: step 14720, total loss = 0.60, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 94h:35m:01s remains)
INFO - root - 2019-11-03 23:15:11.253980: step 14730, total loss = 0.46, predict loss = 0.10 (66.5 examples/sec; 0.060 sec/batch; 99h:57m:43s remains)
INFO - root - 2019-11-03 23:15:11.884770: step 14740, total loss = 0.59, predict loss = 0.13 (70.6 examples/sec; 0.057 sec/batch; 94h:14m:14s remains)
INFO - root - 2019-11-03 23:15:12.501934: step 14750, total loss = 0.55, predict loss = 0.13 (77.0 examples/sec; 0.052 sec/batch; 86h:22m:23s remains)
INFO - root - 2019-11-03 23:15:13.141016: step 14760, total loss = 0.60, predict loss = 0.14 (66.5 examples/sec; 0.060 sec/batch; 99h:56m:23s remains)
INFO - root - 2019-11-03 23:15:13.805738: step 14770, total loss = 0.75, predict loss = 0.17 (75.1 examples/sec; 0.053 sec/batch; 88h:33m:45s remains)
INFO - root - 2019-11-03 23:15:14.451478: step 14780, total loss = 0.65, predict loss = 0.14 (66.6 examples/sec; 0.060 sec/batch; 99h:46m:54s remains)
INFO - root - 2019-11-03 23:15:15.111874: step 14790, total loss = 0.74, predict loss = 0.18 (63.1 examples/sec; 0.063 sec/batch; 105h:27m:03s remains)
INFO - root - 2019-11-03 23:15:15.767201: step 14800, total loss = 0.73, predict loss = 0.17 (68.6 examples/sec; 0.058 sec/batch; 96h:58m:15s remains)
INFO - root - 2019-11-03 23:15:16.389815: step 14810, total loss = 0.77, predict loss = 0.18 (68.7 examples/sec; 0.058 sec/batch; 96h:48m:04s remains)
INFO - root - 2019-11-03 23:15:17.044099: step 14820, total loss = 0.58, predict loss = 0.12 (65.4 examples/sec; 0.061 sec/batch; 101h:44m:11s remains)
INFO - root - 2019-11-03 23:15:17.755949: step 14830, total loss = 0.92, predict loss = 0.18 (66.7 examples/sec; 0.060 sec/batch; 99h:38m:44s remains)
INFO - root - 2019-11-03 23:15:18.434497: step 14840, total loss = 0.73, predict loss = 0.16 (62.9 examples/sec; 0.064 sec/batch; 105h:44m:56s remains)
INFO - root - 2019-11-03 23:15:19.129452: step 14850, total loss = 0.69, predict loss = 0.17 (64.4 examples/sec; 0.062 sec/batch; 103h:14m:27s remains)
INFO - root - 2019-11-03 23:15:19.792082: step 14860, total loss = 0.54, predict loss = 0.10 (78.8 examples/sec; 0.051 sec/batch; 84h:23m:11s remains)
INFO - root - 2019-11-03 23:15:20.462006: step 14870, total loss = 0.79, predict loss = 0.18 (61.3 examples/sec; 0.065 sec/batch; 108h:28m:47s remains)
INFO - root - 2019-11-03 23:15:21.162206: step 14880, total loss = 0.73, predict loss = 0.17 (70.6 examples/sec; 0.057 sec/batch; 94h:08m:11s remains)
INFO - root - 2019-11-03 23:15:21.805965: step 14890, total loss = 0.76, predict loss = 0.19 (66.5 examples/sec; 0.060 sec/batch; 100h:03m:25s remains)
INFO - root - 2019-11-03 23:15:22.427249: step 14900, total loss = 0.68, predict loss = 0.13 (85.4 examples/sec; 0.047 sec/batch; 77h:53m:01s remains)
INFO - root - 2019-11-03 23:15:23.048640: step 14910, total loss = 0.72, predict loss = 0.15 (80.9 examples/sec; 0.049 sec/batch; 82h:11m:39s remains)
INFO - root - 2019-11-03 23:15:23.663187: step 14920, total loss = 0.88, predict loss = 0.20 (81.4 examples/sec; 0.049 sec/batch; 81h:43m:30s remains)
INFO - root - 2019-11-03 23:15:24.277271: step 14930, total loss = 1.00, predict loss = 0.25 (72.5 examples/sec; 0.055 sec/batch; 91h:42m:19s remains)
INFO - root - 2019-11-03 23:15:24.884287: step 14940, total loss = 0.96, predict loss = 0.21 (70.8 examples/sec; 0.056 sec/batch; 93h:53m:22s remains)
INFO - root - 2019-11-03 23:15:25.542825: step 14950, total loss = 0.61, predict loss = 0.13 (69.3 examples/sec; 0.058 sec/batch; 95h:58m:16s remains)
INFO - root - 2019-11-03 23:15:26.190170: step 14960, total loss = 0.81, predict loss = 0.19 (69.7 examples/sec; 0.057 sec/batch; 95h:24m:11s remains)
INFO - root - 2019-11-03 23:15:26.847308: step 14970, total loss = 0.67, predict loss = 0.15 (65.4 examples/sec; 0.061 sec/batch; 101h:37m:18s remains)
INFO - root - 2019-11-03 23:15:27.477352: step 14980, total loss = 0.71, predict loss = 0.15 (77.5 examples/sec; 0.052 sec/batch; 85h:48m:43s remains)
INFO - root - 2019-11-03 23:15:28.120147: step 14990, total loss = 0.60, predict loss = 0.13 (70.1 examples/sec; 0.057 sec/batch; 94h:50m:07s remains)
INFO - root - 2019-11-03 23:15:28.754721: step 15000, total loss = 0.36, predict loss = 0.07 (75.8 examples/sec; 0.053 sec/batch; 87h:45m:50s remains)
INFO - root - 2019-11-03 23:15:30.021677: step 15010, total loss = 0.64, predict loss = 0.15 (74.2 examples/sec; 0.054 sec/batch; 89h:35m:08s remains)
INFO - root - 2019-11-03 23:15:30.623426: step 15020, total loss = 0.66, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 92h:04m:33s remains)
INFO - root - 2019-11-03 23:15:31.258014: step 15030, total loss = 0.71, predict loss = 0.16 (76.9 examples/sec; 0.052 sec/batch; 86h:29m:29s remains)
INFO - root - 2019-11-03 23:15:31.907072: step 15040, total loss = 1.01, predict loss = 0.22 (70.9 examples/sec; 0.056 sec/batch; 93h:48m:24s remains)
INFO - root - 2019-11-03 23:15:32.561439: step 15050, total loss = 0.82, predict loss = 0.19 (73.0 examples/sec; 0.055 sec/batch; 91h:02m:07s remains)
INFO - root - 2019-11-03 23:15:33.189383: step 15060, total loss = 1.04, predict loss = 0.22 (69.2 examples/sec; 0.058 sec/batch; 96h:08m:28s remains)
INFO - root - 2019-11-03 23:15:33.872667: step 15070, total loss = 1.01, predict loss = 0.24 (68.0 examples/sec; 0.059 sec/batch; 97h:45m:46s remains)
INFO - root - 2019-11-03 23:15:34.516931: step 15080, total loss = 1.37, predict loss = 0.35 (75.7 examples/sec; 0.053 sec/batch; 87h:53m:10s remains)
INFO - root - 2019-11-03 23:15:35.121921: step 15090, total loss = 1.12, predict loss = 0.28 (76.4 examples/sec; 0.052 sec/batch; 87h:02m:25s remains)
INFO - root - 2019-11-03 23:15:35.750543: step 15100, total loss = 1.01, predict loss = 0.25 (71.9 examples/sec; 0.056 sec/batch; 92h:26m:15s remains)
INFO - root - 2019-11-03 23:15:36.371122: step 15110, total loss = 0.99, predict loss = 0.24 (68.9 examples/sec; 0.058 sec/batch; 96h:31m:08s remains)
INFO - root - 2019-11-03 23:15:36.996225: step 15120, total loss = 0.98, predict loss = 0.23 (70.7 examples/sec; 0.057 sec/batch; 94h:04m:37s remains)
INFO - root - 2019-11-03 23:15:37.648429: step 15130, total loss = 0.90, predict loss = 0.21 (72.6 examples/sec; 0.055 sec/batch; 91h:36m:53s remains)
INFO - root - 2019-11-03 23:15:38.317721: step 15140, total loss = 0.84, predict loss = 0.18 (63.1 examples/sec; 0.063 sec/batch; 105h:25m:47s remains)
INFO - root - 2019-11-03 23:15:38.958644: step 15150, total loss = 0.75, predict loss = 0.17 (64.9 examples/sec; 0.062 sec/batch; 102h:30m:03s remains)
INFO - root - 2019-11-03 23:15:39.640575: step 15160, total loss = 0.81, predict loss = 0.18 (69.5 examples/sec; 0.058 sec/batch; 95h:38m:07s remains)
INFO - root - 2019-11-03 23:15:40.260829: step 15170, total loss = 0.87, predict loss = 0.21 (78.0 examples/sec; 0.051 sec/batch; 85h:18m:04s remains)
INFO - root - 2019-11-03 23:15:40.927845: step 15180, total loss = 0.80, predict loss = 0.18 (68.6 examples/sec; 0.058 sec/batch; 96h:55m:33s remains)
INFO - root - 2019-11-03 23:15:41.550096: step 15190, total loss = 0.84, predict loss = 0.19 (75.6 examples/sec; 0.053 sec/batch; 87h:55m:29s remains)
INFO - root - 2019-11-03 23:15:42.164154: step 15200, total loss = 0.63, predict loss = 0.14 (65.9 examples/sec; 0.061 sec/batch; 100h:56m:58s remains)
INFO - root - 2019-11-03 23:15:42.791505: step 15210, total loss = 0.82, predict loss = 0.19 (68.6 examples/sec; 0.058 sec/batch; 96h:53m:39s remains)
INFO - root - 2019-11-03 23:15:43.387875: step 15220, total loss = 0.77, predict loss = 0.19 (77.8 examples/sec; 0.051 sec/batch; 85h:25m:08s remains)
INFO - root - 2019-11-03 23:15:44.007404: step 15230, total loss = 0.73, predict loss = 0.18 (74.4 examples/sec; 0.054 sec/batch; 89h:25m:07s remains)
INFO - root - 2019-11-03 23:15:44.617371: step 15240, total loss = 0.79, predict loss = 0.17 (73.9 examples/sec; 0.054 sec/batch; 90h:00m:18s remains)
INFO - root - 2019-11-03 23:15:45.263173: step 15250, total loss = 0.73, predict loss = 0.19 (70.1 examples/sec; 0.057 sec/batch; 94h:50m:44s remains)
INFO - root - 2019-11-03 23:15:45.876238: step 15260, total loss = 0.84, predict loss = 0.20 (66.9 examples/sec; 0.060 sec/batch; 99h:23m:58s remains)
INFO - root - 2019-11-03 23:15:46.492025: step 15270, total loss = 0.76, predict loss = 0.17 (72.6 examples/sec; 0.055 sec/batch; 91h:36m:46s remains)
INFO - root - 2019-11-03 23:15:47.112154: step 15280, total loss = 0.93, predict loss = 0.21 (83.5 examples/sec; 0.048 sec/batch; 79h:36m:27s remains)
INFO - root - 2019-11-03 23:15:47.740658: step 15290, total loss = 0.74, predict loss = 0.18 (67.1 examples/sec; 0.060 sec/batch; 99h:05m:19s remains)
INFO - root - 2019-11-03 23:15:48.405815: step 15300, total loss = 0.53, predict loss = 0.12 (59.5 examples/sec; 0.067 sec/batch; 111h:43m:57s remains)
INFO - root - 2019-11-03 23:15:49.026487: step 15310, total loss = 0.45, predict loss = 0.10 (78.1 examples/sec; 0.051 sec/batch; 85h:09m:02s remains)
INFO - root - 2019-11-03 23:15:49.645767: step 15320, total loss = 0.66, predict loss = 0.17 (66.0 examples/sec; 0.061 sec/batch; 100h:46m:04s remains)
INFO - root - 2019-11-03 23:15:50.247880: step 15330, total loss = 0.52, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 96h:45m:29s remains)
INFO - root - 2019-11-03 23:15:50.874984: step 15340, total loss = 0.53, predict loss = 0.13 (75.6 examples/sec; 0.053 sec/batch; 87h:57m:31s remains)
INFO - root - 2019-11-03 23:15:51.523830: step 15350, total loss = 0.57, predict loss = 0.14 (67.1 examples/sec; 0.060 sec/batch; 99h:03m:40s remains)
INFO - root - 2019-11-03 23:15:52.161562: step 15360, total loss = 0.58, predict loss = 0.14 (68.1 examples/sec; 0.059 sec/batch; 97h:39m:11s remains)
INFO - root - 2019-11-03 23:15:52.827369: step 15370, total loss = 0.61, predict loss = 0.14 (62.3 examples/sec; 0.064 sec/batch; 106h:41m:40s remains)
INFO - root - 2019-11-03 23:15:53.484183: step 15380, total loss = 0.77, predict loss = 0.18 (67.0 examples/sec; 0.060 sec/batch; 99h:15m:43s remains)
INFO - root - 2019-11-03 23:15:54.122002: step 15390, total loss = 0.83, predict loss = 0.20 (70.1 examples/sec; 0.057 sec/batch; 94h:49m:23s remains)
INFO - root - 2019-11-03 23:15:54.724407: step 15400, total loss = 0.73, predict loss = 0.18 (78.6 examples/sec; 0.051 sec/batch; 84h:38m:21s remains)
INFO - root - 2019-11-03 23:15:55.359435: step 15410, total loss = 0.65, predict loss = 0.15 (74.0 examples/sec; 0.054 sec/batch; 89h:52m:03s remains)
INFO - root - 2019-11-03 23:15:55.997790: step 15420, total loss = 0.95, predict loss = 0.25 (72.8 examples/sec; 0.055 sec/batch; 91h:17m:55s remains)
INFO - root - 2019-11-03 23:15:56.679887: step 15430, total loss = 0.76, predict loss = 0.18 (63.0 examples/sec; 0.063 sec/batch; 105h:29m:38s remains)
INFO - root - 2019-11-03 23:15:57.355702: step 15440, total loss = 0.80, predict loss = 0.17 (66.2 examples/sec; 0.060 sec/batch; 100h:28m:38s remains)
INFO - root - 2019-11-03 23:15:57.965084: step 15450, total loss = 0.84, predict loss = 0.21 (74.2 examples/sec; 0.054 sec/batch; 89h:38m:49s remains)
INFO - root - 2019-11-03 23:15:58.572763: step 15460, total loss = 0.79, predict loss = 0.18 (75.7 examples/sec; 0.053 sec/batch; 87h:47m:04s remains)
INFO - root - 2019-11-03 23:15:59.220677: step 15470, total loss = 0.82, predict loss = 0.19 (72.3 examples/sec; 0.055 sec/batch; 92h:00m:59s remains)
INFO - root - 2019-11-03 23:15:59.877286: step 15480, total loss = 0.89, predict loss = 0.22 (69.0 examples/sec; 0.058 sec/batch; 96h:23m:01s remains)
INFO - root - 2019-11-03 23:16:00.501077: step 15490, total loss = 1.03, predict loss = 0.25 (73.0 examples/sec; 0.055 sec/batch; 91h:06m:45s remains)
INFO - root - 2019-11-03 23:16:01.136826: step 15500, total loss = 0.65, predict loss = 0.16 (75.6 examples/sec; 0.053 sec/batch; 87h:57m:24s remains)
INFO - root - 2019-11-03 23:16:01.779035: step 15510, total loss = 0.92, predict loss = 0.23 (62.9 examples/sec; 0.064 sec/batch; 105h:39m:48s remains)
INFO - root - 2019-11-03 23:16:02.415158: step 15520, total loss = 0.99, predict loss = 0.24 (79.1 examples/sec; 0.051 sec/batch; 84h:06m:32s remains)
INFO - root - 2019-11-03 23:16:03.067109: step 15530, total loss = 0.88, predict loss = 0.22 (71.9 examples/sec; 0.056 sec/batch; 92h:30m:55s remains)
INFO - root - 2019-11-03 23:16:03.700874: step 15540, total loss = 0.84, predict loss = 0.21 (73.5 examples/sec; 0.054 sec/batch; 90h:30m:06s remains)
INFO - root - 2019-11-03 23:16:04.342622: step 15550, total loss = 0.73, predict loss = 0.17 (75.1 examples/sec; 0.053 sec/batch; 88h:30m:56s remains)
INFO - root - 2019-11-03 23:16:04.991371: step 15560, total loss = 0.84, predict loss = 0.20 (67.6 examples/sec; 0.059 sec/batch; 98h:22m:36s remains)
INFO - root - 2019-11-03 23:16:05.663132: step 15570, total loss = 0.70, predict loss = 0.14 (62.6 examples/sec; 0.064 sec/batch; 106h:08m:58s remains)
INFO - root - 2019-11-03 23:16:06.293269: step 15580, total loss = 0.94, predict loss = 0.24 (72.8 examples/sec; 0.055 sec/batch; 91h:21m:28s remains)
INFO - root - 2019-11-03 23:16:06.924228: step 15590, total loss = 0.88, predict loss = 0.22 (69.0 examples/sec; 0.058 sec/batch; 96h:24m:33s remains)
INFO - root - 2019-11-03 23:16:07.560086: step 15600, total loss = 0.50, predict loss = 0.10 (73.0 examples/sec; 0.055 sec/batch; 91h:05m:53s remains)
INFO - root - 2019-11-03 23:16:08.175137: step 15610, total loss = 0.77, predict loss = 0.17 (72.7 examples/sec; 0.055 sec/batch; 91h:27m:17s remains)
INFO - root - 2019-11-03 23:16:08.858375: step 15620, total loss = 0.62, predict loss = 0.12 (61.3 examples/sec; 0.065 sec/batch; 108h:32m:12s remains)
INFO - root - 2019-11-03 23:16:09.548470: step 15630, total loss = 0.66, predict loss = 0.16 (63.7 examples/sec; 0.063 sec/batch; 104h:20m:12s remains)
INFO - root - 2019-11-03 23:16:10.172862: step 15640, total loss = 0.60, predict loss = 0.13 (70.7 examples/sec; 0.057 sec/batch; 94h:01m:35s remains)
INFO - root - 2019-11-03 23:16:10.811955: step 15650, total loss = 0.63, predict loss = 0.15 (68.3 examples/sec; 0.059 sec/batch; 97h:23m:44s remains)
INFO - root - 2019-11-03 23:16:11.483025: step 15660, total loss = 0.70, predict loss = 0.16 (66.9 examples/sec; 0.060 sec/batch; 99h:25m:22s remains)
INFO - root - 2019-11-03 23:16:12.158989: step 15670, total loss = 0.82, predict loss = 0.19 (63.4 examples/sec; 0.063 sec/batch; 104h:51m:46s remains)
INFO - root - 2019-11-03 23:16:12.814819: step 15680, total loss = 0.84, predict loss = 0.19 (70.9 examples/sec; 0.056 sec/batch; 93h:44m:16s remains)
INFO - root - 2019-11-03 23:16:13.426872: step 15690, total loss = 0.66, predict loss = 0.15 (76.3 examples/sec; 0.052 sec/batch; 87h:11m:53s remains)
INFO - root - 2019-11-03 23:16:14.026775: step 15700, total loss = 0.82, predict loss = 0.19 (88.9 examples/sec; 0.045 sec/batch; 74h:45m:49s remains)
INFO - root - 2019-11-03 23:16:14.617454: step 15710, total loss = 0.81, predict loss = 0.19 (72.7 examples/sec; 0.055 sec/batch; 91h:24m:26s remains)
INFO - root - 2019-11-03 23:16:15.239997: step 15720, total loss = 0.74, predict loss = 0.17 (84.8 examples/sec; 0.047 sec/batch; 78h:26m:59s remains)
INFO - root - 2019-11-03 23:16:15.882820: step 15730, total loss = 0.83, predict loss = 0.19 (64.0 examples/sec; 0.063 sec/batch; 103h:57m:08s remains)
INFO - root - 2019-11-03 23:16:16.515825: step 15740, total loss = 0.93, predict loss = 0.21 (75.2 examples/sec; 0.053 sec/batch; 88h:25m:29s remains)
INFO - root - 2019-11-03 23:16:17.159741: step 15750, total loss = 0.92, predict loss = 0.22 (70.9 examples/sec; 0.056 sec/batch; 93h:47m:20s remains)
INFO - root - 2019-11-03 23:16:17.762704: step 15760, total loss = 0.74, predict loss = 0.18 (72.7 examples/sec; 0.055 sec/batch; 91h:24m:29s remains)
INFO - root - 2019-11-03 23:16:18.374770: step 15770, total loss = 0.63, predict loss = 0.14 (77.7 examples/sec; 0.051 sec/batch; 85h:34m:19s remains)
INFO - root - 2019-11-03 23:16:19.034956: step 15780, total loss = 0.70, predict loss = 0.16 (69.7 examples/sec; 0.057 sec/batch; 95h:27m:29s remains)
INFO - root - 2019-11-03 23:16:19.691558: step 15790, total loss = 0.65, predict loss = 0.16 (65.9 examples/sec; 0.061 sec/batch; 100h:49m:20s remains)
INFO - root - 2019-11-03 23:16:20.356361: step 15800, total loss = 0.71, predict loss = 0.16 (68.3 examples/sec; 0.059 sec/batch; 97h:24m:08s remains)
INFO - root - 2019-11-03 23:16:20.981963: step 15810, total loss = 0.67, predict loss = 0.16 (69.0 examples/sec; 0.058 sec/batch; 96h:17m:55s remains)
INFO - root - 2019-11-03 23:16:21.611728: step 15820, total loss = 0.86, predict loss = 0.21 (71.6 examples/sec; 0.056 sec/batch; 92h:52m:52s remains)
INFO - root - 2019-11-03 23:16:22.237556: step 15830, total loss = 0.69, predict loss = 0.16 (75.3 examples/sec; 0.053 sec/batch; 88h:14m:37s remains)
INFO - root - 2019-11-03 23:16:22.845475: step 15840, total loss = 0.90, predict loss = 0.20 (76.7 examples/sec; 0.052 sec/batch; 86h:43m:42s remains)
INFO - root - 2019-11-03 23:16:23.488380: step 15850, total loss = 0.73, predict loss = 0.18 (77.2 examples/sec; 0.052 sec/batch; 86h:05m:29s remains)
INFO - root - 2019-11-03 23:16:24.158333: step 15860, total loss = 0.68, predict loss = 0.15 (60.7 examples/sec; 0.066 sec/batch; 109h:28m:21s remains)
INFO - root - 2019-11-03 23:16:24.819271: step 15870, total loss = 0.73, predict loss = 0.18 (74.0 examples/sec; 0.054 sec/batch; 89h:51m:19s remains)
INFO - root - 2019-11-03 23:16:25.468513: step 15880, total loss = 0.71, predict loss = 0.17 (68.7 examples/sec; 0.058 sec/batch; 96h:47m:43s remains)
INFO - root - 2019-11-03 23:16:26.089453: step 15890, total loss = 0.71, predict loss = 0.16 (71.1 examples/sec; 0.056 sec/batch; 93h:32m:50s remains)
INFO - root - 2019-11-03 23:16:26.707376: step 15900, total loss = 0.62, predict loss = 0.15 (75.4 examples/sec; 0.053 sec/batch; 88h:13m:06s remains)
INFO - root - 2019-11-03 23:16:27.351560: step 15910, total loss = 0.73, predict loss = 0.17 (70.8 examples/sec; 0.056 sec/batch; 93h:52m:12s remains)
INFO - root - 2019-11-03 23:16:27.993533: step 15920, total loss = 0.56, predict loss = 0.11 (67.9 examples/sec; 0.059 sec/batch; 97h:52m:23s remains)
INFO - root - 2019-11-03 23:16:28.684980: step 15930, total loss = 0.59, predict loss = 0.12 (69.2 examples/sec; 0.058 sec/batch; 96h:08m:23s remains)
INFO - root - 2019-11-03 23:16:29.329337: step 15940, total loss = 0.82, predict loss = 0.19 (72.7 examples/sec; 0.055 sec/batch; 91h:29m:57s remains)
INFO - root - 2019-11-03 23:16:29.993165: step 15950, total loss = 0.84, predict loss = 0.19 (62.6 examples/sec; 0.064 sec/batch; 106h:17m:13s remains)
INFO - root - 2019-11-03 23:16:30.663309: step 15960, total loss = 0.69, predict loss = 0.16 (59.6 examples/sec; 0.067 sec/batch; 111h:29m:13s remains)
INFO - root - 2019-11-03 23:16:31.325502: step 15970, total loss = 0.76, predict loss = 0.18 (70.0 examples/sec; 0.057 sec/batch; 94h:59m:53s remains)
INFO - root - 2019-11-03 23:16:31.986340: step 15980, total loss = 0.77, predict loss = 0.17 (74.4 examples/sec; 0.054 sec/batch; 89h:24m:55s remains)
INFO - root - 2019-11-03 23:16:32.591338: step 15990, total loss = 0.72, predict loss = 0.15 (71.4 examples/sec; 0.056 sec/batch; 93h:09m:17s remains)
INFO - root - 2019-11-03 23:16:33.210230: step 16000, total loss = 0.77, predict loss = 0.17 (77.7 examples/sec; 0.051 sec/batch; 85h:34m:56s remains)
INFO - root - 2019-11-03 23:16:33.858575: step 16010, total loss = 0.69, predict loss = 0.17 (66.6 examples/sec; 0.060 sec/batch; 99h:49m:08s remains)
INFO - root - 2019-11-03 23:16:34.475648: step 16020, total loss = 0.74, predict loss = 0.18 (76.8 examples/sec; 0.052 sec/batch; 86h:33m:25s remains)
INFO - root - 2019-11-03 23:16:35.095900: step 16030, total loss = 0.65, predict loss = 0.15 (70.4 examples/sec; 0.057 sec/batch; 94h:24m:20s remains)
INFO - root - 2019-11-03 23:16:35.742271: step 16040, total loss = 0.78, predict loss = 0.18 (69.6 examples/sec; 0.057 sec/batch; 95h:31m:34s remains)
INFO - root - 2019-11-03 23:16:36.445408: step 16050, total loss = 0.83, predict loss = 0.21 (64.0 examples/sec; 0.062 sec/batch; 103h:48m:40s remains)
INFO - root - 2019-11-03 23:16:37.084489: step 16060, total loss = 0.65, predict loss = 0.15 (79.1 examples/sec; 0.051 sec/batch; 84h:04m:47s remains)
INFO - root - 2019-11-03 23:16:37.689517: step 16070, total loss = 0.74, predict loss = 0.17 (80.4 examples/sec; 0.050 sec/batch; 82h:41m:05s remains)
INFO - root - 2019-11-03 23:16:38.339034: step 16080, total loss = 0.83, predict loss = 0.21 (69.4 examples/sec; 0.058 sec/batch; 95h:45m:36s remains)
INFO - root - 2019-11-03 23:16:38.979709: step 16090, total loss = 0.88, predict loss = 0.21 (69.2 examples/sec; 0.058 sec/batch; 96h:02m:46s remains)
INFO - root - 2019-11-03 23:16:39.640188: step 16100, total loss = 0.86, predict loss = 0.19 (65.8 examples/sec; 0.061 sec/batch; 101h:03m:42s remains)
INFO - root - 2019-11-03 23:16:40.283353: step 16110, total loss = 0.82, predict loss = 0.19 (73.0 examples/sec; 0.055 sec/batch; 91h:07m:37s remains)
INFO - root - 2019-11-03 23:16:40.904869: step 16120, total loss = 0.78, predict loss = 0.19 (74.8 examples/sec; 0.053 sec/batch; 88h:54m:44s remains)
INFO - root - 2019-11-03 23:16:41.518834: step 16130, total loss = 0.82, predict loss = 0.21 (74.6 examples/sec; 0.054 sec/batch; 89h:07m:57s remains)
INFO - root - 2019-11-03 23:16:42.175872: step 16140, total loss = 1.00, predict loss = 0.24 (62.9 examples/sec; 0.064 sec/batch; 105h:44m:35s remains)
INFO - root - 2019-11-03 23:16:42.813884: step 16150, total loss = 1.12, predict loss = 0.30 (68.6 examples/sec; 0.058 sec/batch; 96h:52m:03s remains)
INFO - root - 2019-11-03 23:16:43.442542: step 16160, total loss = 1.22, predict loss = 0.31 (78.1 examples/sec; 0.051 sec/batch; 85h:09m:18s remains)
INFO - root - 2019-11-03 23:16:44.097370: step 16170, total loss = 0.89, predict loss = 0.22 (82.1 examples/sec; 0.049 sec/batch; 81h:00m:15s remains)
INFO - root - 2019-11-03 23:16:44.730277: step 16180, total loss = 0.89, predict loss = 0.20 (69.6 examples/sec; 0.058 sec/batch; 95h:34m:33s remains)
INFO - root - 2019-11-03 23:16:45.359541: step 16190, total loss = 0.99, predict loss = 0.25 (74.5 examples/sec; 0.054 sec/batch; 89h:12m:40s remains)
INFO - root - 2019-11-03 23:16:46.008605: step 16200, total loss = 0.59, predict loss = 0.14 (79.4 examples/sec; 0.050 sec/batch; 83h:45m:32s remains)
INFO - root - 2019-11-03 23:16:46.685458: step 16210, total loss = 0.71, predict loss = 0.18 (72.7 examples/sec; 0.055 sec/batch; 91h:24m:38s remains)
INFO - root - 2019-11-03 23:16:47.307810: step 16220, total loss = 0.90, predict loss = 0.21 (71.4 examples/sec; 0.056 sec/batch; 93h:09m:54s remains)
INFO - root - 2019-11-03 23:16:47.922810: step 16230, total loss = 0.82, predict loss = 0.19 (74.5 examples/sec; 0.054 sec/batch; 89h:15m:19s remains)
INFO - root - 2019-11-03 23:16:48.555591: step 16240, total loss = 1.19, predict loss = 0.31 (72.3 examples/sec; 0.055 sec/batch; 91h:57m:45s remains)
INFO - root - 2019-11-03 23:16:49.175174: step 16250, total loss = 0.98, predict loss = 0.23 (77.8 examples/sec; 0.051 sec/batch; 85h:30m:11s remains)
INFO - root - 2019-11-03 23:16:49.805671: step 16260, total loss = 0.86, predict loss = 0.21 (69.6 examples/sec; 0.058 sec/batch; 95h:35m:37s remains)
INFO - root - 2019-11-03 23:16:50.442953: step 16270, total loss = 0.85, predict loss = 0.20 (75.5 examples/sec; 0.053 sec/batch; 88h:05m:28s remains)
INFO - root - 2019-11-03 23:16:51.100747: step 16280, total loss = 0.95, predict loss = 0.23 (64.8 examples/sec; 0.062 sec/batch; 102h:32m:49s remains)
INFO - root - 2019-11-03 23:16:51.751936: step 16290, total loss = 0.68, predict loss = 0.16 (71.3 examples/sec; 0.056 sec/batch; 93h:17m:38s remains)
INFO - root - 2019-11-03 23:16:52.450568: step 16300, total loss = 0.78, predict loss = 0.18 (68.5 examples/sec; 0.058 sec/batch; 97h:00m:59s remains)
INFO - root - 2019-11-03 23:16:53.132568: step 16310, total loss = 0.89, predict loss = 0.21 (64.1 examples/sec; 0.062 sec/batch; 103h:39m:03s remains)
INFO - root - 2019-11-03 23:16:53.779647: step 16320, total loss = 0.76, predict loss = 0.17 (66.0 examples/sec; 0.061 sec/batch; 100h:40m:42s remains)
INFO - root - 2019-11-03 23:16:54.402253: step 16330, total loss = 0.54, predict loss = 0.12 (72.8 examples/sec; 0.055 sec/batch; 91h:21m:06s remains)
INFO - root - 2019-11-03 23:16:55.012230: step 16340, total loss = 0.81, predict loss = 0.18 (85.8 examples/sec; 0.047 sec/batch; 77h:31m:48s remains)
INFO - root - 2019-11-03 23:16:55.539754: step 16350, total loss = 0.73, predict loss = 0.15 (95.0 examples/sec; 0.042 sec/batch; 70h:00m:48s remains)
INFO - root - 2019-11-03 23:16:56.003932: step 16360, total loss = 0.80, predict loss = 0.19 (93.6 examples/sec; 0.043 sec/batch; 71h:03m:02s remains)
INFO - root - 2019-11-03 23:16:57.061333: step 16370, total loss = 0.72, predict loss = 0.17 (74.5 examples/sec; 0.054 sec/batch; 89h:15m:47s remains)
INFO - root - 2019-11-03 23:16:57.678526: step 16380, total loss = 0.74, predict loss = 0.18 (75.9 examples/sec; 0.053 sec/batch; 87h:35m:59s remains)
INFO - root - 2019-11-03 23:16:58.286550: step 16390, total loss = 0.59, predict loss = 0.13 (80.6 examples/sec; 0.050 sec/batch; 82h:32m:17s remains)
INFO - root - 2019-11-03 23:16:58.921379: step 16400, total loss = 0.93, predict loss = 0.23 (66.0 examples/sec; 0.061 sec/batch; 100h:46m:57s remains)
INFO - root - 2019-11-03 23:16:59.617745: step 16410, total loss = 0.66, predict loss = 0.15 (71.4 examples/sec; 0.056 sec/batch; 93h:10m:01s remains)
INFO - root - 2019-11-03 23:17:00.300824: step 16420, total loss = 0.93, predict loss = 0.24 (74.3 examples/sec; 0.054 sec/batch; 89h:26m:12s remains)
INFO - root - 2019-11-03 23:17:00.956588: step 16430, total loss = 0.83, predict loss = 0.19 (72.3 examples/sec; 0.055 sec/batch; 91h:58m:53s remains)
INFO - root - 2019-11-03 23:17:01.589930: step 16440, total loss = 0.72, predict loss = 0.16 (80.0 examples/sec; 0.050 sec/batch; 83h:04m:00s remains)
INFO - root - 2019-11-03 23:17:02.257847: step 16450, total loss = 0.94, predict loss = 0.21 (65.2 examples/sec; 0.061 sec/batch; 101h:59m:44s remains)
INFO - root - 2019-11-03 23:17:02.892626: step 16460, total loss = 1.07, predict loss = 0.27 (69.6 examples/sec; 0.057 sec/batch; 95h:27m:22s remains)
INFO - root - 2019-11-03 23:17:03.540427: step 16470, total loss = 1.06, predict loss = 0.25 (71.4 examples/sec; 0.056 sec/batch; 93h:07m:31s remains)
INFO - root - 2019-11-03 23:17:04.206882: step 16480, total loss = 1.17, predict loss = 0.28 (70.1 examples/sec; 0.057 sec/batch; 94h:50m:16s remains)
INFO - root - 2019-11-03 23:17:04.875605: step 16490, total loss = 1.01, predict loss = 0.23 (67.8 examples/sec; 0.059 sec/batch; 98h:05m:18s remains)
INFO - root - 2019-11-03 23:17:05.538374: step 16500, total loss = 0.82, predict loss = 0.19 (61.1 examples/sec; 0.065 sec/batch; 108h:44m:12s remains)
INFO - root - 2019-11-03 23:17:06.259591: step 16510, total loss = 0.81, predict loss = 0.19 (70.6 examples/sec; 0.057 sec/batch; 94h:11m:36s remains)
INFO - root - 2019-11-03 23:17:06.914443: step 16520, total loss = 0.88, predict loss = 0.21 (64.1 examples/sec; 0.062 sec/batch; 103h:47m:13s remains)
INFO - root - 2019-11-03 23:17:07.555110: step 16530, total loss = 0.80, predict loss = 0.18 (79.1 examples/sec; 0.051 sec/batch; 84h:04m:22s remains)
INFO - root - 2019-11-03 23:17:08.168345: step 16540, total loss = 0.74, predict loss = 0.17 (68.3 examples/sec; 0.059 sec/batch; 97h:23m:01s remains)
INFO - root - 2019-11-03 23:17:08.797377: step 16550, total loss = 0.74, predict loss = 0.18 (67.2 examples/sec; 0.060 sec/batch; 98h:54m:37s remains)
INFO - root - 2019-11-03 23:17:09.409161: step 16560, total loss = 0.82, predict loss = 0.22 (64.2 examples/sec; 0.062 sec/batch; 103h:35m:43s remains)
INFO - root - 2019-11-03 23:17:10.032179: step 16570, total loss = 0.82, predict loss = 0.18 (65.4 examples/sec; 0.061 sec/batch; 101h:39m:52s remains)
INFO - root - 2019-11-03 23:17:10.681054: step 16580, total loss = 0.72, predict loss = 0.14 (66.7 examples/sec; 0.060 sec/batch; 99h:42m:50s remains)
INFO - root - 2019-11-03 23:17:11.308777: step 16590, total loss = 0.66, predict loss = 0.16 (70.2 examples/sec; 0.057 sec/batch; 94h:41m:36s remains)
INFO - root - 2019-11-03 23:17:11.957437: step 16600, total loss = 0.69, predict loss = 0.15 (67.0 examples/sec; 0.060 sec/batch; 99h:12m:19s remains)
INFO - root - 2019-11-03 23:17:12.559084: step 16610, total loss = 0.81, predict loss = 0.17 (77.9 examples/sec; 0.051 sec/batch; 85h:18m:27s remains)
INFO - root - 2019-11-03 23:17:13.160469: step 16620, total loss = 1.07, predict loss = 0.24 (73.0 examples/sec; 0.055 sec/batch; 91h:05m:46s remains)
INFO - root - 2019-11-03 23:17:13.773374: step 16630, total loss = 0.61, predict loss = 0.13 (68.2 examples/sec; 0.059 sec/batch; 97h:30m:53s remains)
INFO - root - 2019-11-03 23:17:14.384254: step 16640, total loss = 0.85, predict loss = 0.19 (75.8 examples/sec; 0.053 sec/batch; 87h:40m:58s remains)
INFO - root - 2019-11-03 23:17:14.993145: step 16650, total loss = 0.87, predict loss = 0.20 (74.0 examples/sec; 0.054 sec/batch; 89h:49m:57s remains)
INFO - root - 2019-11-03 23:17:15.637852: step 16660, total loss = 0.81, predict loss = 0.17 (76.1 examples/sec; 0.053 sec/batch; 87h:18m:47s remains)
INFO - root - 2019-11-03 23:17:16.290166: step 16670, total loss = 0.80, predict loss = 0.19 (63.4 examples/sec; 0.063 sec/batch; 104h:56m:01s remains)
INFO - root - 2019-11-03 23:17:16.935556: step 16680, total loss = 0.87, predict loss = 0.20 (67.3 examples/sec; 0.059 sec/batch; 98h:51m:13s remains)
INFO - root - 2019-11-03 23:17:17.557567: step 16690, total loss = 0.84, predict loss = 0.19 (65.1 examples/sec; 0.061 sec/batch; 102h:09m:07s remains)
INFO - root - 2019-11-03 23:17:18.258529: step 16700, total loss = 0.85, predict loss = 0.17 (67.5 examples/sec; 0.059 sec/batch; 98h:31m:20s remains)
INFO - root - 2019-11-03 23:17:18.920774: step 16710, total loss = 0.97, predict loss = 0.22 (67.8 examples/sec; 0.059 sec/batch; 97h:59m:48s remains)
INFO - root - 2019-11-03 23:17:19.596863: step 16720, total loss = 0.97, predict loss = 0.21 (66.4 examples/sec; 0.060 sec/batch; 100h:07m:41s remains)
INFO - root - 2019-11-03 23:17:20.288933: step 16730, total loss = 0.80, predict loss = 0.18 (68.3 examples/sec; 0.059 sec/batch; 97h:20m:03s remains)
INFO - root - 2019-11-03 23:17:20.933525: step 16740, total loss = 0.79, predict loss = 0.17 (66.9 examples/sec; 0.060 sec/batch; 99h:25m:49s remains)
INFO - root - 2019-11-03 23:17:21.629753: step 16750, total loss = 0.87, predict loss = 0.20 (64.6 examples/sec; 0.062 sec/batch; 102h:56m:52s remains)
INFO - root - 2019-11-03 23:17:22.261937: step 16760, total loss = 0.69, predict loss = 0.16 (68.2 examples/sec; 0.059 sec/batch; 97h:32m:12s remains)
INFO - root - 2019-11-03 23:17:22.899046: step 16770, total loss = 0.85, predict loss = 0.18 (70.7 examples/sec; 0.057 sec/batch; 93h:58m:40s remains)
INFO - root - 2019-11-03 23:17:23.518959: step 16780, total loss = 0.77, predict loss = 0.17 (80.4 examples/sec; 0.050 sec/batch; 82h:38m:38s remains)
INFO - root - 2019-11-03 23:17:24.142894: step 16790, total loss = 0.96, predict loss = 0.19 (69.0 examples/sec; 0.058 sec/batch; 96h:17m:00s remains)
INFO - root - 2019-11-03 23:17:24.791052: step 16800, total loss = 0.80, predict loss = 0.17 (66.7 examples/sec; 0.060 sec/batch; 99h:36m:42s remains)
INFO - root - 2019-11-03 23:17:25.413367: step 16810, total loss = 1.26, predict loss = 0.31 (71.0 examples/sec; 0.056 sec/batch; 93h:41m:36s remains)
INFO - root - 2019-11-03 23:17:26.065923: step 16820, total loss = 0.68, predict loss = 0.15 (64.2 examples/sec; 0.062 sec/batch; 103h:37m:24s remains)
INFO - root - 2019-11-03 23:17:26.690439: step 16830, total loss = 0.75, predict loss = 0.18 (71.2 examples/sec; 0.056 sec/batch; 93h:22m:31s remains)
INFO - root - 2019-11-03 23:17:27.320305: step 16840, total loss = 0.73, predict loss = 0.17 (73.3 examples/sec; 0.055 sec/batch; 90h:39m:26s remains)
INFO - root - 2019-11-03 23:17:28.016944: step 16850, total loss = 0.64, predict loss = 0.14 (60.9 examples/sec; 0.066 sec/batch; 109h:06m:26s remains)
INFO - root - 2019-11-03 23:17:28.688648: step 16860, total loss = 0.84, predict loss = 0.20 (62.2 examples/sec; 0.064 sec/batch; 106h:55m:17s remains)
INFO - root - 2019-11-03 23:17:29.384288: step 16870, total loss = 0.74, predict loss = 0.17 (67.8 examples/sec; 0.059 sec/batch; 97h:59m:56s remains)
INFO - root - 2019-11-03 23:17:30.070288: step 16880, total loss = 0.76, predict loss = 0.19 (58.2 examples/sec; 0.069 sec/batch; 114h:11m:54s remains)
INFO - root - 2019-11-03 23:17:30.705524: step 16890, total loss = 0.71, predict loss = 0.16 (72.0 examples/sec; 0.056 sec/batch; 92h:20m:16s remains)
INFO - root - 2019-11-03 23:17:31.383773: step 16900, total loss = 0.80, predict loss = 0.19 (62.6 examples/sec; 0.064 sec/batch; 106h:08m:23s remains)
INFO - root - 2019-11-03 23:17:32.101013: step 16910, total loss = 0.65, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 96h:43m:30s remains)
INFO - root - 2019-11-03 23:17:32.750371: step 16920, total loss = 0.66, predict loss = 0.17 (68.5 examples/sec; 0.058 sec/batch; 97h:02m:48s remains)
INFO - root - 2019-11-03 23:17:33.375287: step 16930, total loss = 0.56, predict loss = 0.13 (73.0 examples/sec; 0.055 sec/batch; 91h:06m:53s remains)
INFO - root - 2019-11-03 23:17:34.031923: step 16940, total loss = 0.57, predict loss = 0.14 (66.6 examples/sec; 0.060 sec/batch; 99h:46m:31s remains)
INFO - root - 2019-11-03 23:17:34.714554: step 16950, total loss = 0.51, predict loss = 0.10 (64.4 examples/sec; 0.062 sec/batch; 103h:15m:02s remains)
INFO - root - 2019-11-03 23:17:35.363123: step 16960, total loss = 1.17, predict loss = 0.27 (85.1 examples/sec; 0.047 sec/batch; 78h:07m:13s remains)
INFO - root - 2019-11-03 23:17:36.019958: step 16970, total loss = 0.52, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 94h:09m:28s remains)
INFO - root - 2019-11-03 23:17:36.706700: step 16980, total loss = 0.62, predict loss = 0.15 (64.6 examples/sec; 0.062 sec/batch; 102h:50m:14s remains)
INFO - root - 2019-11-03 23:17:37.379067: step 16990, total loss = 0.59, predict loss = 0.13 (74.8 examples/sec; 0.054 sec/batch; 88h:55m:42s remains)
INFO - root - 2019-11-03 23:17:37.976796: step 17000, total loss = 0.54, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 93h:08m:06s remains)
INFO - root - 2019-11-03 23:17:38.589874: step 17010, total loss = 0.77, predict loss = 0.20 (64.0 examples/sec; 0.062 sec/batch; 103h:50m:24s remains)
INFO - root - 2019-11-03 23:17:39.260371: step 17020, total loss = 0.71, predict loss = 0.15 (65.8 examples/sec; 0.061 sec/batch; 101h:05m:36s remains)
INFO - root - 2019-11-03 23:17:39.868999: step 17030, total loss = 1.02, predict loss = 0.22 (75.7 examples/sec; 0.053 sec/batch; 87h:48m:00s remains)
INFO - root - 2019-11-03 23:17:40.464597: step 17040, total loss = 0.81, predict loss = 0.20 (76.8 examples/sec; 0.052 sec/batch; 86h:33m:03s remains)
INFO - root - 2019-11-03 23:17:41.104263: step 17050, total loss = 1.01, predict loss = 0.25 (69.9 examples/sec; 0.057 sec/batch; 95h:05m:51s remains)
INFO - root - 2019-11-03 23:17:41.748337: step 17060, total loss = 0.96, predict loss = 0.24 (71.9 examples/sec; 0.056 sec/batch; 92h:26m:36s remains)
INFO - root - 2019-11-03 23:17:42.369508: step 17070, total loss = 0.93, predict loss = 0.22 (69.4 examples/sec; 0.058 sec/batch; 95h:49m:32s remains)
INFO - root - 2019-11-03 23:17:43.007871: step 17080, total loss = 0.68, predict loss = 0.17 (63.7 examples/sec; 0.063 sec/batch; 104h:23m:21s remains)
INFO - root - 2019-11-03 23:17:43.639291: step 17090, total loss = 0.83, predict loss = 0.19 (69.6 examples/sec; 0.057 sec/batch; 95h:31m:11s remains)
INFO - root - 2019-11-03 23:17:44.281378: step 17100, total loss = 0.81, predict loss = 0.20 (70.5 examples/sec; 0.057 sec/batch; 94h:18m:41s remains)
INFO - root - 2019-11-03 23:17:44.911434: step 17110, total loss = 0.65, predict loss = 0.15 (73.7 examples/sec; 0.054 sec/batch; 90h:14m:49s remains)
INFO - root - 2019-11-03 23:17:45.509423: step 17120, total loss = 0.59, predict loss = 0.13 (82.5 examples/sec; 0.048 sec/batch; 80h:33m:51s remains)
INFO - root - 2019-11-03 23:17:46.132780: step 17130, total loss = 0.54, predict loss = 0.12 (67.2 examples/sec; 0.059 sec/batch; 98h:52m:34s remains)
INFO - root - 2019-11-03 23:17:46.750910: step 17140, total loss = 0.84, predict loss = 0.19 (80.1 examples/sec; 0.050 sec/batch; 83h:02m:34s remains)
INFO - root - 2019-11-03 23:17:47.376240: step 17150, total loss = 0.69, predict loss = 0.15 (66.7 examples/sec; 0.060 sec/batch; 99h:42m:22s remains)
INFO - root - 2019-11-03 23:17:48.038885: step 17160, total loss = 1.04, predict loss = 0.24 (66.2 examples/sec; 0.060 sec/batch; 100h:23m:52s remains)
INFO - root - 2019-11-03 23:17:48.652290: step 17170, total loss = 0.78, predict loss = 0.19 (71.0 examples/sec; 0.056 sec/batch; 93h:35m:12s remains)
INFO - root - 2019-11-03 23:17:49.315272: step 17180, total loss = 0.55, predict loss = 0.12 (71.6 examples/sec; 0.056 sec/batch; 92h:52m:50s remains)
INFO - root - 2019-11-03 23:17:49.986825: step 17190, total loss = 0.54, predict loss = 0.12 (71.6 examples/sec; 0.056 sec/batch; 92h:47m:28s remains)
INFO - root - 2019-11-03 23:17:50.661391: step 17200, total loss = 0.40, predict loss = 0.09 (64.5 examples/sec; 0.062 sec/batch; 103h:00m:43s remains)
INFO - root - 2019-11-03 23:17:51.315921: step 17210, total loss = 0.44, predict loss = 0.09 (67.1 examples/sec; 0.060 sec/batch; 99h:07m:55s remains)
INFO - root - 2019-11-03 23:17:51.979466: step 17220, total loss = 0.50, predict loss = 0.11 (69.5 examples/sec; 0.058 sec/batch; 95h:35m:17s remains)
INFO - root - 2019-11-03 23:17:52.642476: step 17230, total loss = 0.38, predict loss = 0.08 (68.3 examples/sec; 0.059 sec/batch; 97h:22m:25s remains)
INFO - root - 2019-11-03 23:17:53.321595: step 17240, total loss = 0.40, predict loss = 0.07 (73.6 examples/sec; 0.054 sec/batch; 90h:17m:26s remains)
INFO - root - 2019-11-03 23:17:54.026606: step 17250, total loss = 0.46, predict loss = 0.10 (70.0 examples/sec; 0.057 sec/batch; 95h:01m:41s remains)
INFO - root - 2019-11-03 23:17:54.639428: step 17260, total loss = 0.67, predict loss = 0.14 (70.0 examples/sec; 0.057 sec/batch; 95h:00m:38s remains)
INFO - root - 2019-11-03 23:17:55.270271: step 17270, total loss = 0.43, predict loss = 0.10 (67.3 examples/sec; 0.059 sec/batch; 98h:50m:16s remains)
INFO - root - 2019-11-03 23:17:55.926664: step 17280, total loss = 0.89, predict loss = 0.21 (72.8 examples/sec; 0.055 sec/batch; 91h:15m:36s remains)
INFO - root - 2019-11-03 23:17:56.588920: step 17290, total loss = 0.62, predict loss = 0.14 (74.8 examples/sec; 0.053 sec/batch; 88h:53m:51s remains)
INFO - root - 2019-11-03 23:17:57.233512: step 17300, total loss = 0.87, predict loss = 0.20 (65.1 examples/sec; 0.061 sec/batch; 102h:03m:08s remains)
INFO - root - 2019-11-03 23:17:57.824514: step 17310, total loss = 0.51, predict loss = 0.11 (74.9 examples/sec; 0.053 sec/batch; 88h:46m:44s remains)
INFO - root - 2019-11-03 23:17:58.420377: step 17320, total loss = 0.79, predict loss = 0.20 (79.6 examples/sec; 0.050 sec/batch; 83h:30m:27s remains)
INFO - root - 2019-11-03 23:17:59.046597: step 17330, total loss = 0.43, predict loss = 0.09 (69.9 examples/sec; 0.057 sec/batch; 95h:07m:40s remains)
INFO - root - 2019-11-03 23:17:59.687567: step 17340, total loss = 0.71, predict loss = 0.18 (69.3 examples/sec; 0.058 sec/batch; 95h:55m:34s remains)
INFO - root - 2019-11-03 23:18:00.341499: step 17350, total loss = 0.52, predict loss = 0.11 (78.5 examples/sec; 0.051 sec/batch; 84h:42m:23s remains)
INFO - root - 2019-11-03 23:18:00.960833: step 17360, total loss = 0.60, predict loss = 0.13 (75.1 examples/sec; 0.053 sec/batch; 88h:29m:16s remains)
INFO - root - 2019-11-03 23:18:01.612053: step 17370, total loss = 0.58, predict loss = 0.13 (76.4 examples/sec; 0.052 sec/batch; 86h:59m:30s remains)
INFO - root - 2019-11-03 23:18:02.257167: step 17380, total loss = 0.61, predict loss = 0.15 (65.9 examples/sec; 0.061 sec/batch; 100h:55m:34s remains)
INFO - root - 2019-11-03 23:18:02.905451: step 17390, total loss = 0.87, predict loss = 0.19 (73.5 examples/sec; 0.054 sec/batch; 90h:25m:49s remains)
INFO - root - 2019-11-03 23:18:03.531904: step 17400, total loss = 0.84, predict loss = 0.21 (67.5 examples/sec; 0.059 sec/batch; 98h:29m:29s remains)
INFO - root - 2019-11-03 23:18:04.174186: step 17410, total loss = 0.62, predict loss = 0.15 (65.8 examples/sec; 0.061 sec/batch; 101h:05m:07s remains)
INFO - root - 2019-11-03 23:18:04.799671: step 17420, total loss = 0.57, predict loss = 0.13 (78.7 examples/sec; 0.051 sec/batch; 84h:29m:43s remains)
INFO - root - 2019-11-03 23:18:05.436233: step 17430, total loss = 0.63, predict loss = 0.14 (66.3 examples/sec; 0.060 sec/batch; 100h:17m:40s remains)
INFO - root - 2019-11-03 23:18:06.059104: step 17440, total loss = 0.34, predict loss = 0.07 (76.1 examples/sec; 0.053 sec/batch; 87h:24m:07s remains)
INFO - root - 2019-11-03 23:18:06.675956: step 17450, total loss = 0.45, predict loss = 0.09 (68.9 examples/sec; 0.058 sec/batch; 96h:28m:09s remains)
INFO - root - 2019-11-03 23:18:07.287355: step 17460, total loss = 0.75, predict loss = 0.16 (72.0 examples/sec; 0.056 sec/batch; 92h:20m:47s remains)
INFO - root - 2019-11-03 23:18:07.889345: step 17470, total loss = 0.38, predict loss = 0.08 (74.0 examples/sec; 0.054 sec/batch; 89h:47m:52s remains)
INFO - root - 2019-11-03 23:18:08.519019: step 17480, total loss = 0.49, predict loss = 0.11 (76.4 examples/sec; 0.052 sec/batch; 86h:58m:06s remains)
INFO - root - 2019-11-03 23:18:09.161224: step 17490, total loss = 0.73, predict loss = 0.17 (75.9 examples/sec; 0.053 sec/batch; 87h:31m:38s remains)
INFO - root - 2019-11-03 23:18:09.803963: step 17500, total loss = 0.69, predict loss = 0.17 (67.8 examples/sec; 0.059 sec/batch; 98h:01m:37s remains)
INFO - root - 2019-11-03 23:18:10.437701: step 17510, total loss = 0.71, predict loss = 0.16 (69.4 examples/sec; 0.058 sec/batch; 95h:43m:00s remains)
INFO - root - 2019-11-03 23:18:11.044091: step 17520, total loss = 0.68, predict loss = 0.16 (78.1 examples/sec; 0.051 sec/batch; 85h:03m:56s remains)
INFO - root - 2019-11-03 23:18:11.657650: step 17530, total loss = 0.66, predict loss = 0.15 (71.3 examples/sec; 0.056 sec/batch; 93h:17m:21s remains)
INFO - root - 2019-11-03 23:18:12.297360: step 17540, total loss = 0.79, predict loss = 0.18 (67.3 examples/sec; 0.059 sec/batch; 98h:49m:57s remains)
INFO - root - 2019-11-03 23:18:12.921538: step 17550, total loss = 0.59, predict loss = 0.14 (70.1 examples/sec; 0.057 sec/batch; 94h:48m:38s remains)
INFO - root - 2019-11-03 23:18:13.548113: step 17560, total loss = 0.58, predict loss = 0.12 (61.2 examples/sec; 0.065 sec/batch; 108h:36m:31s remains)
INFO - root - 2019-11-03 23:18:14.187049: step 17570, total loss = 0.62, predict loss = 0.14 (70.9 examples/sec; 0.056 sec/batch; 93h:47m:26s remains)
INFO - root - 2019-11-03 23:18:14.799616: step 17580, total loss = 0.62, predict loss = 0.14 (80.5 examples/sec; 0.050 sec/batch; 82h:34m:06s remains)
INFO - root - 2019-11-03 23:18:15.408009: step 17590, total loss = 0.65, predict loss = 0.14 (62.2 examples/sec; 0.064 sec/batch; 106h:51m:00s remains)
INFO - root - 2019-11-03 23:18:16.063056: step 17600, total loss = 0.63, predict loss = 0.14 (69.9 examples/sec; 0.057 sec/batch; 95h:09m:35s remains)
INFO - root - 2019-11-03 23:18:16.665248: step 17610, total loss = 0.68, predict loss = 0.15 (60.0 examples/sec; 0.067 sec/batch; 110h:46m:37s remains)
INFO - root - 2019-11-03 23:18:17.309310: step 17620, total loss = 0.49, predict loss = 0.10 (69.3 examples/sec; 0.058 sec/batch; 95h:57m:08s remains)
INFO - root - 2019-11-03 23:18:17.960597: step 17630, total loss = 0.60, predict loss = 0.13 (67.7 examples/sec; 0.059 sec/batch; 98h:07m:56s remains)
INFO - root - 2019-11-03 23:18:18.620105: step 17640, total loss = 0.72, predict loss = 0.16 (65.9 examples/sec; 0.061 sec/batch; 100h:53m:34s remains)
INFO - root - 2019-11-03 23:18:19.245112: step 17650, total loss = 0.62, predict loss = 0.15 (72.3 examples/sec; 0.055 sec/batch; 91h:56m:53s remains)
INFO - root - 2019-11-03 23:18:19.912577: step 17660, total loss = 0.79, predict loss = 0.18 (67.9 examples/sec; 0.059 sec/batch; 97h:55m:44s remains)
INFO - root - 2019-11-03 23:18:20.585331: step 17670, total loss = 0.89, predict loss = 0.22 (71.4 examples/sec; 0.056 sec/batch; 93h:03m:19s remains)
INFO - root - 2019-11-03 23:18:21.206133: step 17680, total loss = 0.64, predict loss = 0.14 (71.6 examples/sec; 0.056 sec/batch; 92h:49m:55s remains)
INFO - root - 2019-11-03 23:18:21.839245: step 17690, total loss = 0.79, predict loss = 0.17 (56.9 examples/sec; 0.070 sec/batch; 116h:50m:42s remains)
INFO - root - 2019-11-03 23:18:22.461947: step 17700, total loss = 0.67, predict loss = 0.16 (72.2 examples/sec; 0.055 sec/batch; 92h:03m:13s remains)
INFO - root - 2019-11-03 23:18:23.093414: step 17710, total loss = 0.72, predict loss = 0.18 (65.2 examples/sec; 0.061 sec/batch; 101h:54m:51s remains)
INFO - root - 2019-11-03 23:18:23.689263: step 17720, total loss = 0.56, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 94h:38m:22s remains)
INFO - root - 2019-11-03 23:18:24.333901: step 17730, total loss = 0.65, predict loss = 0.13 (60.5 examples/sec; 0.066 sec/batch; 109h:50m:07s remains)
INFO - root - 2019-11-03 23:18:25.040741: step 17740, total loss = 0.51, predict loss = 0.12 (62.5 examples/sec; 0.064 sec/batch; 106h:23m:22s remains)
INFO - root - 2019-11-03 23:18:25.669043: step 17750, total loss = 0.93, predict loss = 0.22 (73.5 examples/sec; 0.054 sec/batch; 90h:27m:46s remains)
INFO - root - 2019-11-03 23:18:26.316482: step 17760, total loss = 0.71, predict loss = 0.17 (66.8 examples/sec; 0.060 sec/batch; 99h:26m:58s remains)
INFO - root - 2019-11-03 23:18:26.969313: step 17770, total loss = 0.73, predict loss = 0.17 (63.5 examples/sec; 0.063 sec/batch; 104h:41m:11s remains)
INFO - root - 2019-11-03 23:18:27.604347: step 17780, total loss = 1.18, predict loss = 0.28 (75.0 examples/sec; 0.053 sec/batch; 88h:36m:20s remains)
INFO - root - 2019-11-03 23:18:28.280190: step 17790, total loss = 1.23, predict loss = 0.28 (61.3 examples/sec; 0.065 sec/batch; 108h:21m:56s remains)
INFO - root - 2019-11-03 23:18:28.997393: step 17800, total loss = 0.95, predict loss = 0.24 (63.1 examples/sec; 0.063 sec/batch; 105h:20m:20s remains)
INFO - root - 2019-11-03 23:18:29.755933: step 17810, total loss = 1.17, predict loss = 0.29 (52.2 examples/sec; 0.077 sec/batch; 127h:21m:14s remains)
INFO - root - 2019-11-03 23:18:30.525191: step 17820, total loss = 1.10, predict loss = 0.27 (65.0 examples/sec; 0.062 sec/batch; 102h:14m:55s remains)
INFO - root - 2019-11-03 23:18:31.257051: step 17830, total loss = 1.27, predict loss = 0.32 (76.5 examples/sec; 0.052 sec/batch; 86h:53m:06s remains)
INFO - root - 2019-11-03 23:18:31.874950: step 17840, total loss = 0.89, predict loss = 0.20 (67.5 examples/sec; 0.059 sec/batch; 98h:29m:26s remains)
INFO - root - 2019-11-03 23:18:32.503806: step 17850, total loss = 0.92, predict loss = 0.21 (67.5 examples/sec; 0.059 sec/batch; 98h:27m:40s remains)
INFO - root - 2019-11-03 23:18:33.115980: step 17860, total loss = 0.74, predict loss = 0.17 (72.0 examples/sec; 0.056 sec/batch; 92h:17m:14s remains)
INFO - root - 2019-11-03 23:18:33.754781: step 17870, total loss = 1.02, predict loss = 0.25 (76.6 examples/sec; 0.052 sec/batch; 86h:44m:31s remains)
INFO - root - 2019-11-03 23:18:34.389991: step 17880, total loss = 0.87, predict loss = 0.19 (62.1 examples/sec; 0.064 sec/batch; 107h:05m:03s remains)
INFO - root - 2019-11-03 23:18:35.025746: step 17890, total loss = 0.83, predict loss = 0.20 (72.2 examples/sec; 0.055 sec/batch; 92h:02m:01s remains)
INFO - root - 2019-11-03 23:18:35.665358: step 17900, total loss = 0.84, predict loss = 0.20 (59.5 examples/sec; 0.067 sec/batch; 111h:46m:03s remains)
INFO - root - 2019-11-03 23:18:36.305056: step 17910, total loss = 0.79, predict loss = 0.18 (68.2 examples/sec; 0.059 sec/batch; 97h:26m:22s remains)
INFO - root - 2019-11-03 23:18:36.924807: step 17920, total loss = 1.07, predict loss = 0.27 (82.2 examples/sec; 0.049 sec/batch; 80h:51m:29s remains)
INFO - root - 2019-11-03 23:18:37.554299: step 17930, total loss = 0.86, predict loss = 0.20 (68.8 examples/sec; 0.058 sec/batch; 96h:32m:23s remains)
INFO - root - 2019-11-03 23:18:38.205221: step 17940, total loss = 0.71, predict loss = 0.16 (65.2 examples/sec; 0.061 sec/batch; 101h:54m:45s remains)
INFO - root - 2019-11-03 23:18:38.834392: step 17950, total loss = 0.78, predict loss = 0.18 (81.1 examples/sec; 0.049 sec/batch; 81h:55m:34s remains)
INFO - root - 2019-11-03 23:18:39.474828: step 17960, total loss = 0.71, predict loss = 0.16 (67.0 examples/sec; 0.060 sec/batch; 99h:10m:49s remains)
INFO - root - 2019-11-03 23:18:40.126161: step 17970, total loss = 0.68, predict loss = 0.17 (68.3 examples/sec; 0.059 sec/batch; 97h:15m:34s remains)
INFO - root - 2019-11-03 23:18:40.752182: step 17980, total loss = 0.72, predict loss = 0.16 (66.9 examples/sec; 0.060 sec/batch; 99h:24m:49s remains)
INFO - root - 2019-11-03 23:18:41.396007: step 17990, total loss = 0.75, predict loss = 0.17 (66.5 examples/sec; 0.060 sec/batch; 99h:54m:07s remains)
INFO - root - 2019-11-03 23:18:42.032431: step 18000, total loss = 0.79, predict loss = 0.19 (65.5 examples/sec; 0.061 sec/batch; 101h:25m:57s remains)
INFO - root - 2019-11-03 23:18:42.676327: step 18010, total loss = 0.70, predict loss = 0.17 (67.4 examples/sec; 0.059 sec/batch; 98h:34m:31s remains)
INFO - root - 2019-11-03 23:18:43.365114: step 18020, total loss = 0.80, predict loss = 0.19 (59.0 examples/sec; 0.068 sec/batch; 112h:42m:26s remains)
INFO - root - 2019-11-03 23:18:44.008717: step 18030, total loss = 0.77, predict loss = 0.18 (67.8 examples/sec; 0.059 sec/batch; 98h:02m:27s remains)
INFO - root - 2019-11-03 23:18:44.643512: step 18040, total loss = 0.72, predict loss = 0.15 (63.8 examples/sec; 0.063 sec/batch; 104h:13m:44s remains)
INFO - root - 2019-11-03 23:18:45.271807: step 18050, total loss = 0.61, predict loss = 0.15 (74.0 examples/sec; 0.054 sec/batch; 89h:51m:43s remains)
INFO - root - 2019-11-03 23:18:45.910171: step 18060, total loss = 0.57, predict loss = 0.13 (61.3 examples/sec; 0.065 sec/batch; 108h:28m:39s remains)
INFO - root - 2019-11-03 23:18:46.545685: step 18070, total loss = 0.62, predict loss = 0.14 (65.5 examples/sec; 0.061 sec/batch; 101h:31m:25s remains)
INFO - root - 2019-11-03 23:18:47.161817: step 18080, total loss = 0.41, predict loss = 0.09 (78.8 examples/sec; 0.051 sec/batch; 84h:21m:15s remains)
INFO - root - 2019-11-03 23:18:47.765982: step 18090, total loss = 0.59, predict loss = 0.14 (73.7 examples/sec; 0.054 sec/batch; 90h:11m:48s remains)
INFO - root - 2019-11-03 23:18:48.389424: step 18100, total loss = 0.75, predict loss = 0.17 (81.3 examples/sec; 0.049 sec/batch; 81h:47m:50s remains)
INFO - root - 2019-11-03 23:18:49.015915: step 18110, total loss = 0.67, predict loss = 0.16 (70.7 examples/sec; 0.057 sec/batch; 94h:00m:30s remains)
INFO - root - 2019-11-03 23:18:49.604599: step 18120, total loss = 0.64, predict loss = 0.15 (74.0 examples/sec; 0.054 sec/batch; 89h:51m:52s remains)
INFO - root - 2019-11-03 23:18:50.192318: step 18130, total loss = 0.67, predict loss = 0.16 (68.3 examples/sec; 0.059 sec/batch; 97h:19m:31s remains)
INFO - root - 2019-11-03 23:18:50.785559: step 18140, total loss = 0.62, predict loss = 0.13 (72.0 examples/sec; 0.056 sec/batch; 92h:20m:15s remains)
INFO - root - 2019-11-03 23:18:51.407322: step 18150, total loss = 0.66, predict loss = 0.16 (67.4 examples/sec; 0.059 sec/batch; 98h:33m:38s remains)
INFO - root - 2019-11-03 23:18:51.999748: step 18160, total loss = 0.56, predict loss = 0.13 (74.8 examples/sec; 0.053 sec/batch; 88h:49m:07s remains)
INFO - root - 2019-11-03 23:18:53.095552: step 18170, total loss = 0.89, predict loss = 0.22 (79.7 examples/sec; 0.050 sec/batch; 83h:22m:06s remains)
INFO - root - 2019-11-03 23:18:53.700885: step 18180, total loss = 0.71, predict loss = 0.18 (68.8 examples/sec; 0.058 sec/batch; 96h:35m:26s remains)
INFO - root - 2019-11-03 23:18:54.320841: step 18190, total loss = 0.71, predict loss = 0.16 (65.8 examples/sec; 0.061 sec/batch; 100h:59m:14s remains)
INFO - root - 2019-11-03 23:18:54.958298: step 18200, total loss = 0.78, predict loss = 0.18 (66.4 examples/sec; 0.060 sec/batch; 100h:02m:02s remains)
INFO - root - 2019-11-03 23:18:55.635548: step 18210, total loss = 0.65, predict loss = 0.16 (65.3 examples/sec; 0.061 sec/batch; 101h:43m:51s remains)
INFO - root - 2019-11-03 23:18:56.306648: step 18220, total loss = 0.94, predict loss = 0.21 (72.3 examples/sec; 0.055 sec/batch; 91h:55m:23s remains)
INFO - root - 2019-11-03 23:18:56.893493: step 18230, total loss = 0.88, predict loss = 0.21 (75.4 examples/sec; 0.053 sec/batch; 88h:06m:34s remains)
INFO - root - 2019-11-03 23:18:57.532269: step 18240, total loss = 0.87, predict loss = 0.20 (69.3 examples/sec; 0.058 sec/batch; 95h:54m:41s remains)
INFO - root - 2019-11-03 23:18:58.149551: step 18250, total loss = 1.07, predict loss = 0.26 (74.5 examples/sec; 0.054 sec/batch; 89h:12m:31s remains)
INFO - root - 2019-11-03 23:18:58.786029: step 18260, total loss = 0.85, predict loss = 0.21 (80.9 examples/sec; 0.049 sec/batch; 82h:11m:52s remains)
INFO - root - 2019-11-03 23:18:59.412241: step 18270, total loss = 1.01, predict loss = 0.25 (73.2 examples/sec; 0.055 sec/batch; 90h:49m:53s remains)
INFO - root - 2019-11-03 23:19:00.047756: step 18280, total loss = 0.80, predict loss = 0.19 (72.4 examples/sec; 0.055 sec/batch; 91h:44m:34s remains)
INFO - root - 2019-11-03 23:19:00.752321: step 18290, total loss = 0.63, predict loss = 0.14 (75.8 examples/sec; 0.053 sec/batch; 87h:43m:19s remains)
INFO - root - 2019-11-03 23:19:01.342845: step 18300, total loss = 1.06, predict loss = 0.27 (76.7 examples/sec; 0.052 sec/batch; 86h:42m:21s remains)
INFO - root - 2019-11-03 23:19:01.940192: step 18310, total loss = 0.68, predict loss = 0.14 (69.2 examples/sec; 0.058 sec/batch; 96h:01m:36s remains)
INFO - root - 2019-11-03 23:19:02.593493: step 18320, total loss = 0.62, predict loss = 0.14 (68.3 examples/sec; 0.059 sec/batch; 97h:19m:13s remains)
INFO - root - 2019-11-03 23:19:03.206440: step 18330, total loss = 0.61, predict loss = 0.14 (70.4 examples/sec; 0.057 sec/batch; 94h:21m:28s remains)
INFO - root - 2019-11-03 23:19:03.816258: step 18340, total loss = 0.49, predict loss = 0.11 (63.1 examples/sec; 0.063 sec/batch; 105h:18m:56s remains)
INFO - root - 2019-11-03 23:19:04.459601: step 18350, total loss = 0.66, predict loss = 0.16 (67.8 examples/sec; 0.059 sec/batch; 97h:57m:22s remains)
INFO - root - 2019-11-03 23:19:05.142959: step 18360, total loss = 0.59, predict loss = 0.14 (66.7 examples/sec; 0.060 sec/batch; 99h:41m:55s remains)
INFO - root - 2019-11-03 23:19:05.822703: step 18370, total loss = 0.56, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 95h:44m:24s remains)
INFO - root - 2019-11-03 23:19:06.496640: step 18380, total loss = 0.52, predict loss = 0.11 (63.8 examples/sec; 0.063 sec/batch; 104h:11m:00s remains)
INFO - root - 2019-11-03 23:19:07.131463: step 18390, total loss = 0.82, predict loss = 0.19 (64.0 examples/sec; 0.062 sec/batch; 103h:46m:55s remains)
INFO - root - 2019-11-03 23:19:07.801567: step 18400, total loss = 0.62, predict loss = 0.15 (71.0 examples/sec; 0.056 sec/batch; 93h:37m:35s remains)
INFO - root - 2019-11-03 23:19:08.507875: step 18410, total loss = 0.63, predict loss = 0.14 (58.3 examples/sec; 0.069 sec/batch; 113h:55m:27s remains)
INFO - root - 2019-11-03 23:19:09.169457: step 18420, total loss = 0.64, predict loss = 0.14 (63.1 examples/sec; 0.063 sec/batch; 105h:15m:07s remains)
INFO - root - 2019-11-03 23:19:09.915796: step 18430, total loss = 0.92, predict loss = 0.23 (60.4 examples/sec; 0.066 sec/batch; 109h:57m:18s remains)
INFO - root - 2019-11-03 23:19:10.549296: step 18440, total loss = 0.62, predict loss = 0.15 (76.6 examples/sec; 0.052 sec/batch; 86h:46m:20s remains)
INFO - root - 2019-11-03 23:19:11.161451: step 18450, total loss = 0.77, predict loss = 0.18 (77.6 examples/sec; 0.052 sec/batch; 85h:39m:42s remains)
INFO - root - 2019-11-03 23:19:11.805909: step 18460, total loss = 0.92, predict loss = 0.22 (71.6 examples/sec; 0.056 sec/batch; 92h:49m:09s remains)
INFO - root - 2019-11-03 23:19:12.425724: step 18470, total loss = 0.88, predict loss = 0.20 (67.7 examples/sec; 0.059 sec/batch; 98h:13m:09s remains)
INFO - root - 2019-11-03 23:19:13.054529: step 18480, total loss = 0.81, predict loss = 0.21 (81.7 examples/sec; 0.049 sec/batch; 81h:22m:32s remains)
INFO - root - 2019-11-03 23:19:13.714083: step 18490, total loss = 0.82, predict loss = 0.20 (74.3 examples/sec; 0.054 sec/batch; 89h:27m:32s remains)
INFO - root - 2019-11-03 23:19:14.443759: step 18500, total loss = 0.77, predict loss = 0.19 (57.8 examples/sec; 0.069 sec/batch; 114h:57m:37s remains)
INFO - root - 2019-11-03 23:19:15.184979: step 18510, total loss = 0.68, predict loss = 0.17 (67.1 examples/sec; 0.060 sec/batch; 99h:00m:53s remains)
INFO - root - 2019-11-03 23:19:15.814065: step 18520, total loss = 0.70, predict loss = 0.17 (78.1 examples/sec; 0.051 sec/batch; 85h:03m:34s remains)
INFO - root - 2019-11-03 23:19:16.432318: step 18530, total loss = 0.63, predict loss = 0.15 (79.3 examples/sec; 0.050 sec/batch; 83h:51m:05s remains)
INFO - root - 2019-11-03 23:19:17.070466: step 18540, total loss = 0.77, predict loss = 0.18 (69.1 examples/sec; 0.058 sec/batch; 96h:09m:47s remains)
INFO - root - 2019-11-03 23:19:17.721100: step 18550, total loss = 0.82, predict loss = 0.19 (69.7 examples/sec; 0.057 sec/batch; 95h:24m:37s remains)
INFO - root - 2019-11-03 23:19:18.352395: step 18560, total loss = 0.83, predict loss = 0.21 (80.3 examples/sec; 0.050 sec/batch; 82h:43m:03s remains)
INFO - root - 2019-11-03 23:19:18.994851: step 18570, total loss = 0.68, predict loss = 0.16 (70.3 examples/sec; 0.057 sec/batch; 94h:29m:53s remains)
INFO - root - 2019-11-03 23:19:19.618423: step 18580, total loss = 0.89, predict loss = 0.21 (69.8 examples/sec; 0.057 sec/batch; 95h:16m:04s remains)
INFO - root - 2019-11-03 23:19:20.244004: step 18590, total loss = 0.67, predict loss = 0.16 (70.2 examples/sec; 0.057 sec/batch; 94h:43m:42s remains)
INFO - root - 2019-11-03 23:19:20.845442: step 18600, total loss = 0.79, predict loss = 0.18 (72.0 examples/sec; 0.056 sec/batch; 92h:17m:25s remains)
INFO - root - 2019-11-03 23:19:21.464801: step 18610, total loss = 0.49, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 88h:04m:20s remains)
INFO - root - 2019-11-03 23:19:22.088985: step 18620, total loss = 0.56, predict loss = 0.12 (68.8 examples/sec; 0.058 sec/batch; 96h:39m:25s remains)
INFO - root - 2019-11-03 23:19:22.726101: step 18630, total loss = 0.56, predict loss = 0.13 (81.5 examples/sec; 0.049 sec/batch; 81h:31m:59s remains)
INFO - root - 2019-11-03 23:19:23.377869: step 18640, total loss = 0.68, predict loss = 0.16 (79.2 examples/sec; 0.051 sec/batch; 83h:55m:43s remains)
INFO - root - 2019-11-03 23:19:24.013211: step 18650, total loss = 0.64, predict loss = 0.14 (72.9 examples/sec; 0.055 sec/batch; 91h:08m:01s remains)
INFO - root - 2019-11-03 23:19:24.646600: step 18660, total loss = 0.78, predict loss = 0.20 (72.4 examples/sec; 0.055 sec/batch; 91h:47m:17s remains)
INFO - root - 2019-11-03 23:19:25.289141: step 18670, total loss = 0.69, predict loss = 0.16 (73.0 examples/sec; 0.055 sec/batch; 91h:00m:24s remains)
INFO - root - 2019-11-03 23:19:25.900234: step 18680, total loss = 0.63, predict loss = 0.14 (69.7 examples/sec; 0.057 sec/batch; 95h:17m:38s remains)
INFO - root - 2019-11-03 23:19:26.538206: step 18690, total loss = 0.62, predict loss = 0.15 (73.0 examples/sec; 0.055 sec/batch; 91h:05m:01s remains)
INFO - root - 2019-11-03 23:19:27.183513: step 18700, total loss = 0.73, predict loss = 0.16 (68.1 examples/sec; 0.059 sec/batch; 97h:33m:35s remains)
INFO - root - 2019-11-03 23:19:27.865295: step 18710, total loss = 0.67, predict loss = 0.15 (62.5 examples/sec; 0.064 sec/batch; 106h:22m:00s remains)
INFO - root - 2019-11-03 23:19:28.540357: step 18720, total loss = 0.70, predict loss = 0.16 (64.6 examples/sec; 0.062 sec/batch; 102h:54m:37s remains)
INFO - root - 2019-11-03 23:19:29.250602: step 18730, total loss = 0.67, predict loss = 0.16 (63.3 examples/sec; 0.063 sec/batch; 104h:59m:07s remains)
INFO - root - 2019-11-03 23:19:29.879294: step 18740, total loss = 0.82, predict loss = 0.18 (71.3 examples/sec; 0.056 sec/batch; 93h:09m:58s remains)
INFO - root - 2019-11-03 23:19:30.532725: step 18750, total loss = 0.84, predict loss = 0.20 (66.0 examples/sec; 0.061 sec/batch; 100h:44m:25s remains)
INFO - root - 2019-11-03 23:19:31.207343: step 18760, total loss = 0.63, predict loss = 0.15 (71.4 examples/sec; 0.056 sec/batch; 93h:03m:25s remains)
INFO - root - 2019-11-03 23:19:31.834216: step 18770, total loss = 0.66, predict loss = 0.15 (76.0 examples/sec; 0.053 sec/batch; 87h:27m:27s remains)
INFO - root - 2019-11-03 23:19:32.482341: step 18780, total loss = 0.78, predict loss = 0.19 (67.7 examples/sec; 0.059 sec/batch; 98h:10m:45s remains)
INFO - root - 2019-11-03 23:19:33.114985: step 18790, total loss = 0.80, predict loss = 0.18 (68.9 examples/sec; 0.058 sec/batch; 96h:23m:24s remains)
INFO - root - 2019-11-03 23:19:33.747950: step 18800, total loss = 1.16, predict loss = 0.29 (76.6 examples/sec; 0.052 sec/batch; 86h:45m:50s remains)
INFO - root - 2019-11-03 23:19:34.409700: step 18810, total loss = 0.78, predict loss = 0.19 (70.4 examples/sec; 0.057 sec/batch; 94h:23m:26s remains)
INFO - root - 2019-11-03 23:19:35.129242: step 18820, total loss = 0.71, predict loss = 0.16 (61.2 examples/sec; 0.065 sec/batch; 108h:30m:32s remains)
INFO - root - 2019-11-03 23:19:35.856507: step 18830, total loss = 1.03, predict loss = 0.23 (65.8 examples/sec; 0.061 sec/batch; 100h:59m:54s remains)
INFO - root - 2019-11-03 23:19:36.486977: step 18840, total loss = 0.75, predict loss = 0.19 (70.5 examples/sec; 0.057 sec/batch; 94h:15m:42s remains)
INFO - root - 2019-11-03 23:19:37.114558: step 18850, total loss = 0.95, predict loss = 0.23 (77.2 examples/sec; 0.052 sec/batch; 86h:05m:51s remains)
INFO - root - 2019-11-03 23:19:37.735364: step 18860, total loss = 0.82, predict loss = 0.21 (64.7 examples/sec; 0.062 sec/batch; 102h:41m:47s remains)
INFO - root - 2019-11-03 23:19:38.381843: step 18870, total loss = 0.80, predict loss = 0.18 (70.9 examples/sec; 0.056 sec/batch; 93h:42m:36s remains)
INFO - root - 2019-11-03 23:19:39.038419: step 18880, total loss = 0.91, predict loss = 0.22 (63.8 examples/sec; 0.063 sec/batch; 104h:11m:44s remains)
INFO - root - 2019-11-03 23:19:39.688530: step 18890, total loss = 1.16, predict loss = 0.27 (68.8 examples/sec; 0.058 sec/batch; 96h:35m:22s remains)
INFO - root - 2019-11-03 23:19:40.330357: step 18900, total loss = 0.91, predict loss = 0.22 (74.8 examples/sec; 0.053 sec/batch; 88h:50m:54s remains)
INFO - root - 2019-11-03 23:19:40.976946: step 18910, total loss = 0.92, predict loss = 0.21 (71.6 examples/sec; 0.056 sec/batch; 92h:50m:19s remains)
INFO - root - 2019-11-03 23:19:41.642475: step 18920, total loss = 1.08, predict loss = 0.27 (80.0 examples/sec; 0.050 sec/batch; 83h:05m:59s remains)
INFO - root - 2019-11-03 23:19:42.295716: step 18930, total loss = 0.73, predict loss = 0.17 (74.5 examples/sec; 0.054 sec/batch; 89h:08m:53s remains)
INFO - root - 2019-11-03 23:19:42.917847: step 18940, total loss = 0.90, predict loss = 0.22 (69.3 examples/sec; 0.058 sec/batch; 95h:50m:58s remains)
INFO - root - 2019-11-03 23:19:43.527503: step 18950, total loss = 0.78, predict loss = 0.19 (70.9 examples/sec; 0.056 sec/batch; 93h:44m:22s remains)
INFO - root - 2019-11-03 23:19:44.150803: step 18960, total loss = 0.64, predict loss = 0.15 (66.9 examples/sec; 0.060 sec/batch; 99h:20m:32s remains)
INFO - root - 2019-11-03 23:19:44.796725: step 18970, total loss = 0.93, predict loss = 0.22 (70.7 examples/sec; 0.057 sec/batch; 94h:02m:51s remains)
INFO - root - 2019-11-03 23:19:45.428940: step 18980, total loss = 0.70, predict loss = 0.19 (66.2 examples/sec; 0.060 sec/batch; 100h:25m:38s remains)
INFO - root - 2019-11-03 23:19:46.102657: step 18990, total loss = 0.65, predict loss = 0.15 (67.1 examples/sec; 0.060 sec/batch; 99h:02m:07s remains)
INFO - root - 2019-11-03 23:19:46.769998: step 19000, total loss = 0.86, predict loss = 0.25 (60.4 examples/sec; 0.066 sec/batch; 110h:05m:52s remains)
INFO - root - 2019-11-03 23:19:47.441930: step 19010, total loss = 0.60, predict loss = 0.14 (58.9 examples/sec; 0.068 sec/batch; 112h:45m:33s remains)
INFO - root - 2019-11-03 23:19:48.122669: step 19020, total loss = 0.91, predict loss = 0.21 (74.1 examples/sec; 0.054 sec/batch; 89h:43m:47s remains)
INFO - root - 2019-11-03 23:19:48.740811: step 19030, total loss = 0.71, predict loss = 0.18 (70.1 examples/sec; 0.057 sec/batch; 94h:45m:48s remains)
INFO - root - 2019-11-03 23:19:49.388835: step 19040, total loss = 0.79, predict loss = 0.17 (63.9 examples/sec; 0.063 sec/batch; 104h:04m:19s remains)
INFO - root - 2019-11-03 23:19:50.063885: step 19050, total loss = 0.65, predict loss = 0.16 (76.9 examples/sec; 0.052 sec/batch; 86h:26m:53s remains)
INFO - root - 2019-11-03 23:19:50.734719: step 19060, total loss = 0.70, predict loss = 0.16 (68.5 examples/sec; 0.058 sec/batch; 96h:58m:28s remains)
INFO - root - 2019-11-03 23:19:51.330667: step 19070, total loss = 0.62, predict loss = 0.13 (96.4 examples/sec; 0.041 sec/batch; 68h:55m:05s remains)
INFO - root - 2019-11-03 23:19:51.799697: step 19080, total loss = 0.85, predict loss = 0.20 (89.4 examples/sec; 0.045 sec/batch; 74h:21m:48s remains)
INFO - root - 2019-11-03 23:19:52.277708: step 19090, total loss = 0.66, predict loss = 0.15 (98.3 examples/sec; 0.041 sec/batch; 67h:34m:35s remains)
INFO - root - 2019-11-03 23:19:53.399066: step 19100, total loss = 0.81, predict loss = 0.17 (70.5 examples/sec; 0.057 sec/batch; 94h:11m:58s remains)
INFO - root - 2019-11-03 23:19:53.994262: step 19110, total loss = 0.57, predict loss = 0.12 (80.5 examples/sec; 0.050 sec/batch; 82h:33m:33s remains)
INFO - root - 2019-11-03 23:19:54.661633: step 19120, total loss = 0.83, predict loss = 0.20 (67.5 examples/sec; 0.059 sec/batch; 98h:24m:47s remains)
INFO - root - 2019-11-03 23:19:55.282272: step 19130, total loss = 0.73, predict loss = 0.17 (70.2 examples/sec; 0.057 sec/batch; 94h:43m:06s remains)
INFO - root - 2019-11-03 23:19:55.914733: step 19140, total loss = 0.52, predict loss = 0.12 (72.2 examples/sec; 0.055 sec/batch; 92h:01m:49s remains)
INFO - root - 2019-11-03 23:19:56.529870: step 19150, total loss = 0.72, predict loss = 0.16 (75.1 examples/sec; 0.053 sec/batch; 88h:28m:13s remains)
INFO - root - 2019-11-03 23:19:57.141561: step 19160, total loss = 0.87, predict loss = 0.20 (79.1 examples/sec; 0.051 sec/batch; 84h:01m:13s remains)
INFO - root - 2019-11-03 23:19:57.773175: step 19170, total loss = 0.91, predict loss = 0.22 (71.8 examples/sec; 0.056 sec/batch; 92h:30m:02s remains)
INFO - root - 2019-11-03 23:19:58.438565: step 19180, total loss = 0.75, predict loss = 0.18 (59.7 examples/sec; 0.067 sec/batch; 111h:14m:20s remains)
INFO - root - 2019-11-03 23:19:59.131664: step 19190, total loss = 1.08, predict loss = 0.24 (66.5 examples/sec; 0.060 sec/batch; 99h:54m:55s remains)
INFO - root - 2019-11-03 23:19:59.749484: step 19200, total loss = 0.97, predict loss = 0.23 (65.5 examples/sec; 0.061 sec/batch; 101h:30m:25s remains)
INFO - root - 2019-11-03 23:20:00.394604: step 19210, total loss = 0.98, predict loss = 0.23 (67.9 examples/sec; 0.059 sec/batch; 97h:53m:41s remains)
INFO - root - 2019-11-03 23:20:01.075842: step 19220, total loss = 0.82, predict loss = 0.19 (72.0 examples/sec; 0.056 sec/batch; 92h:21m:35s remains)
INFO - root - 2019-11-03 23:20:01.683016: step 19230, total loss = 0.61, predict loss = 0.14 (72.8 examples/sec; 0.055 sec/batch; 91h:14m:59s remains)
INFO - root - 2019-11-03 23:20:02.277005: step 19240, total loss = 0.92, predict loss = 0.21 (74.9 examples/sec; 0.053 sec/batch; 88h:40m:51s remains)
INFO - root - 2019-11-03 23:20:02.890555: step 19250, total loss = 0.69, predict loss = 0.15 (74.2 examples/sec; 0.054 sec/batch; 89h:31m:59s remains)
INFO - root - 2019-11-03 23:20:03.511190: step 19260, total loss = 0.61, predict loss = 0.12 (69.0 examples/sec; 0.058 sec/batch; 96h:20m:30s remains)
INFO - root - 2019-11-03 23:20:04.143710: step 19270, total loss = 0.66, predict loss = 0.14 (75.1 examples/sec; 0.053 sec/batch; 88h:27m:45s remains)
INFO - root - 2019-11-03 23:20:04.787579: step 19280, total loss = 0.74, predict loss = 0.15 (78.8 examples/sec; 0.051 sec/batch; 84h:19m:01s remains)
INFO - root - 2019-11-03 23:20:05.392787: step 19290, total loss = 1.23, predict loss = 0.30 (78.9 examples/sec; 0.051 sec/batch; 84h:12m:16s remains)
INFO - root - 2019-11-03 23:20:05.996260: step 19300, total loss = 0.86, predict loss = 0.20 (72.1 examples/sec; 0.055 sec/batch; 92h:08m:06s remains)
INFO - root - 2019-11-03 23:20:06.622504: step 19310, total loss = 0.90, predict loss = 0.22 (66.2 examples/sec; 0.060 sec/batch; 100h:23m:33s remains)
INFO - root - 2019-11-03 23:20:07.268132: step 19320, total loss = 0.86, predict loss = 0.20 (67.8 examples/sec; 0.059 sec/batch; 98h:04m:26s remains)
INFO - root - 2019-11-03 23:20:07.909516: step 19330, total loss = 0.65, predict loss = 0.15 (63.7 examples/sec; 0.063 sec/batch; 104h:18m:21s remains)
INFO - root - 2019-11-03 23:20:08.538899: step 19340, total loss = 0.92, predict loss = 0.20 (72.1 examples/sec; 0.056 sec/batch; 92h:12m:43s remains)
INFO - root - 2019-11-03 23:20:09.173296: step 19350, total loss = 0.75, predict loss = 0.18 (60.9 examples/sec; 0.066 sec/batch; 109h:02m:58s remains)
INFO - root - 2019-11-03 23:20:09.783709: step 19360, total loss = 0.90, predict loss = 0.21 (78.1 examples/sec; 0.051 sec/batch; 85h:07m:49s remains)
INFO - root - 2019-11-03 23:20:10.383709: step 19370, total loss = 0.85, predict loss = 0.19 (87.9 examples/sec; 0.046 sec/batch; 75h:36m:40s remains)
INFO - root - 2019-11-03 23:20:11.011122: step 19380, total loss = 0.73, predict loss = 0.16 (74.8 examples/sec; 0.053 sec/batch; 88h:49m:38s remains)
INFO - root - 2019-11-03 23:20:11.647849: step 19390, total loss = 0.89, predict loss = 0.19 (69.1 examples/sec; 0.058 sec/batch; 96h:13m:03s remains)
INFO - root - 2019-11-03 23:20:12.303182: step 19400, total loss = 0.77, predict loss = 0.17 (82.4 examples/sec; 0.049 sec/batch; 80h:38m:07s remains)
INFO - root - 2019-11-03 23:20:12.934855: step 19410, total loss = 1.03, predict loss = 0.23 (77.0 examples/sec; 0.052 sec/batch; 86h:19m:19s remains)
INFO - root - 2019-11-03 23:20:13.569317: step 19420, total loss = 1.10, predict loss = 0.25 (77.9 examples/sec; 0.051 sec/batch; 85h:21m:18s remains)
INFO - root - 2019-11-03 23:20:14.215618: step 19430, total loss = 0.87, predict loss = 0.20 (62.4 examples/sec; 0.064 sec/batch; 106h:34m:15s remains)
INFO - root - 2019-11-03 23:20:14.885634: step 19440, total loss = 0.87, predict loss = 0.18 (66.5 examples/sec; 0.060 sec/batch; 99h:57m:48s remains)
INFO - root - 2019-11-03 23:20:15.524060: step 19450, total loss = 0.83, predict loss = 0.19 (65.1 examples/sec; 0.061 sec/batch; 102h:00m:56s remains)
INFO - root - 2019-11-03 23:20:16.146835: step 19460, total loss = 0.90, predict loss = 0.21 (74.9 examples/sec; 0.053 sec/batch; 88h:45m:39s remains)
INFO - root - 2019-11-03 23:20:16.818728: step 19470, total loss = 0.80, predict loss = 0.18 (70.8 examples/sec; 0.057 sec/batch; 93h:55m:10s remains)
INFO - root - 2019-11-03 23:20:17.457450: step 19480, total loss = 0.78, predict loss = 0.16 (69.7 examples/sec; 0.057 sec/batch; 95h:24m:02s remains)
INFO - root - 2019-11-03 23:20:18.102605: step 19490, total loss = 0.65, predict loss = 0.13 (65.6 examples/sec; 0.061 sec/batch; 101h:16m:22s remains)
INFO - root - 2019-11-03 23:20:18.751415: step 19500, total loss = 0.89, predict loss = 0.19 (77.9 examples/sec; 0.051 sec/batch; 85h:20m:53s remains)
INFO - root - 2019-11-03 23:20:19.408293: step 19510, total loss = 0.76, predict loss = 0.17 (68.5 examples/sec; 0.058 sec/batch; 96h:57m:06s remains)
INFO - root - 2019-11-03 23:20:20.026883: step 19520, total loss = 0.92, predict loss = 0.20 (74.7 examples/sec; 0.054 sec/batch; 88h:55m:34s remains)
INFO - root - 2019-11-03 23:20:20.632296: step 19530, total loss = 0.81, predict loss = 0.20 (72.0 examples/sec; 0.056 sec/batch; 92h:19m:39s remains)
INFO - root - 2019-11-03 23:20:21.272773: step 19540, total loss = 0.86, predict loss = 0.21 (62.9 examples/sec; 0.064 sec/batch; 105h:39m:43s remains)
INFO - root - 2019-11-03 23:20:21.909342: step 19550, total loss = 0.71, predict loss = 0.17 (65.5 examples/sec; 0.061 sec/batch; 101h:29m:29s remains)
INFO - root - 2019-11-03 23:20:22.529523: step 19560, total loss = 0.82, predict loss = 0.19 (77.4 examples/sec; 0.052 sec/batch; 85h:53m:06s remains)
INFO - root - 2019-11-03 23:20:23.169333: step 19570, total loss = 1.14, predict loss = 0.26 (71.7 examples/sec; 0.056 sec/batch; 92h:40m:45s remains)
INFO - root - 2019-11-03 23:20:23.807217: step 19580, total loss = 0.79, predict loss = 0.17 (68.2 examples/sec; 0.059 sec/batch; 97h:25m:04s remains)
INFO - root - 2019-11-03 23:20:24.455549: step 19590, total loss = 0.93, predict loss = 0.22 (69.9 examples/sec; 0.057 sec/batch; 95h:02m:21s remains)
INFO - root - 2019-11-03 23:20:25.068481: step 19600, total loss = 0.67, predict loss = 0.15 (85.9 examples/sec; 0.047 sec/batch; 77h:21m:14s remains)
INFO - root - 2019-11-03 23:20:25.681020: step 19610, total loss = 0.85, predict loss = 0.19 (68.6 examples/sec; 0.058 sec/batch; 96h:53m:10s remains)
INFO - root - 2019-11-03 23:20:26.329981: step 19620, total loss = 0.78, predict loss = 0.19 (68.6 examples/sec; 0.058 sec/batch; 96h:52m:38s remains)
INFO - root - 2019-11-03 23:20:26.964134: step 19630, total loss = 0.80, predict loss = 0.19 (69.0 examples/sec; 0.058 sec/batch; 96h:18m:46s remains)
INFO - root - 2019-11-03 23:20:27.574863: step 19640, total loss = 0.84, predict loss = 0.20 (70.0 examples/sec; 0.057 sec/batch; 94h:54m:44s remains)
INFO - root - 2019-11-03 23:20:28.240728: step 19650, total loss = 0.47, predict loss = 0.10 (73.2 examples/sec; 0.055 sec/batch; 90h:43m:18s remains)
INFO - root - 2019-11-03 23:20:28.888036: step 19660, total loss = 0.59, predict loss = 0.14 (68.7 examples/sec; 0.058 sec/batch; 96h:44m:08s remains)
INFO - root - 2019-11-03 23:20:29.489814: step 19670, total loss = 0.53, predict loss = 0.12 (74.0 examples/sec; 0.054 sec/batch; 89h:44m:29s remains)
INFO - root - 2019-11-03 23:20:30.197154: step 19680, total loss = 0.54, predict loss = 0.13 (71.2 examples/sec; 0.056 sec/batch; 93h:20m:35s remains)
INFO - root - 2019-11-03 23:20:30.971890: step 19690, total loss = 0.58, predict loss = 0.13 (78.5 examples/sec; 0.051 sec/batch; 84h:41m:53s remains)
INFO - root - 2019-11-03 23:20:31.726427: step 19700, total loss = 0.51, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 97h:13m:16s remains)
INFO - root - 2019-11-03 23:20:32.415512: step 19710, total loss = 0.48, predict loss = 0.09 (75.0 examples/sec; 0.053 sec/batch; 88h:38m:25s remains)
INFO - root - 2019-11-03 23:20:33.040469: step 19720, total loss = 0.61, predict loss = 0.13 (76.8 examples/sec; 0.052 sec/batch; 86h:30m:11s remains)
INFO - root - 2019-11-03 23:20:33.648708: step 19730, total loss = 0.63, predict loss = 0.16 (77.3 examples/sec; 0.052 sec/batch; 85h:54m:36s remains)
INFO - root - 2019-11-03 23:20:34.246764: step 19740, total loss = 0.72, predict loss = 0.18 (84.1 examples/sec; 0.048 sec/batch; 78h:59m:06s remains)
INFO - root - 2019-11-03 23:20:34.856547: step 19750, total loss = 0.67, predict loss = 0.14 (75.4 examples/sec; 0.053 sec/batch; 88h:07m:16s remains)
INFO - root - 2019-11-03 23:20:35.464218: step 19760, total loss = 0.74, predict loss = 0.16 (81.6 examples/sec; 0.049 sec/batch; 81h:26m:50s remains)
INFO - root - 2019-11-03 23:20:36.092239: step 19770, total loss = 0.55, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 91h:28m:52s remains)
INFO - root - 2019-11-03 23:20:36.721070: step 19780, total loss = 0.77, predict loss = 0.16 (78.5 examples/sec; 0.051 sec/batch; 84h:41m:15s remains)
INFO - root - 2019-11-03 23:20:37.320104: step 19790, total loss = 0.87, predict loss = 0.20 (84.3 examples/sec; 0.047 sec/batch; 78h:51m:29s remains)
INFO - root - 2019-11-03 23:20:37.935318: step 19800, total loss = 1.07, predict loss = 0.25 (73.6 examples/sec; 0.054 sec/batch; 90h:14m:50s remains)
INFO - root - 2019-11-03 23:20:38.591999: step 19810, total loss = 1.00, predict loss = 0.25 (70.7 examples/sec; 0.057 sec/batch; 93h:56m:54s remains)
INFO - root - 2019-11-03 23:20:39.228785: step 19820, total loss = 0.75, predict loss = 0.16 (77.2 examples/sec; 0.052 sec/batch; 86h:05m:21s remains)
INFO - root - 2019-11-03 23:20:39.893114: step 19830, total loss = 0.94, predict loss = 0.22 (63.9 examples/sec; 0.063 sec/batch; 103h:57m:51s remains)
INFO - root - 2019-11-03 23:20:40.553791: step 19840, total loss = 0.86, predict loss = 0.20 (72.2 examples/sec; 0.055 sec/batch; 91h:58m:12s remains)
INFO - root - 2019-11-03 23:20:41.212409: step 19850, total loss = 0.74, predict loss = 0.17 (64.2 examples/sec; 0.062 sec/batch; 103h:32m:02s remains)
INFO - root - 2019-11-03 23:20:41.857882: step 19860, total loss = 0.58, predict loss = 0.12 (60.3 examples/sec; 0.066 sec/batch; 110h:10m:06s remains)
INFO - root - 2019-11-03 23:20:42.600999: step 19870, total loss = 0.70, predict loss = 0.18 (70.5 examples/sec; 0.057 sec/batch; 94h:11m:00s remains)
INFO - root - 2019-11-03 23:20:43.238502: step 19880, total loss = 0.89, predict loss = 0.21 (64.4 examples/sec; 0.062 sec/batch; 103h:06m:28s remains)
INFO - root - 2019-11-03 23:20:43.842098: step 19890, total loss = 0.67, predict loss = 0.16 (72.1 examples/sec; 0.056 sec/batch; 92h:13m:17s remains)
INFO - root - 2019-11-03 23:20:44.590758: step 19900, total loss = 0.70, predict loss = 0.16 (70.1 examples/sec; 0.057 sec/batch; 94h:47m:37s remains)
INFO - root - 2019-11-03 23:20:45.281618: step 19910, total loss = 0.52, predict loss = 0.12 (61.7 examples/sec; 0.065 sec/batch; 107h:44m:26s remains)
INFO - root - 2019-11-03 23:20:45.971157: step 19920, total loss = 0.45, predict loss = 0.09 (64.3 examples/sec; 0.062 sec/batch; 103h:20m:46s remains)
INFO - root - 2019-11-03 23:20:46.643159: step 19930, total loss = 0.51, predict loss = 0.11 (63.3 examples/sec; 0.063 sec/batch; 104h:55m:27s remains)
INFO - root - 2019-11-03 23:20:47.322431: step 19940, total loss = 0.51, predict loss = 0.10 (62.6 examples/sec; 0.064 sec/batch; 106h:06m:20s remains)
INFO - root - 2019-11-03 23:20:48.011650: step 19950, total loss = 0.43, predict loss = 0.09 (63.9 examples/sec; 0.063 sec/batch; 103h:56m:37s remains)
INFO - root - 2019-11-03 23:20:48.626513: step 19960, total loss = 0.35, predict loss = 0.06 (70.6 examples/sec; 0.057 sec/batch; 94h:04m:44s remains)
INFO - root - 2019-11-03 23:20:49.261343: step 19970, total loss = 0.42, predict loss = 0.09 (68.6 examples/sec; 0.058 sec/batch; 96h:53m:14s remains)
INFO - root - 2019-11-03 23:20:49.904474: step 19980, total loss = 0.47, predict loss = 0.10 (74.5 examples/sec; 0.054 sec/batch; 89h:11m:54s remains)
INFO - root - 2019-11-03 23:20:50.555535: step 19990, total loss = 0.62, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 96h:44m:21s remains)
INFO - root - 2019-11-03 23:20:51.145870: step 20000, total loss = 0.79, predict loss = 0.17 (75.6 examples/sec; 0.053 sec/batch; 87h:55m:33s remains)
INFO - root - 2019-11-03 23:20:51.763037: step 20010, total loss = 0.86, predict loss = 0.20 (74.7 examples/sec; 0.054 sec/batch; 88h:54m:30s remains)
INFO - root - 2019-11-03 23:20:52.375543: step 20020, total loss = 0.77, predict loss = 0.17 (72.3 examples/sec; 0.055 sec/batch; 91h:52m:17s remains)
INFO - root - 2019-11-03 23:20:52.997907: step 20030, total loss = 0.67, predict loss = 0.16 (81.0 examples/sec; 0.049 sec/batch; 81h:58m:53s remains)
INFO - root - 2019-11-03 23:20:53.624678: step 20040, total loss = 0.89, predict loss = 0.21 (63.8 examples/sec; 0.063 sec/batch; 104h:09m:10s remains)
INFO - root - 2019-11-03 23:20:54.303921: step 20050, total loss = 0.57, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 96h:40m:49s remains)
INFO - root - 2019-11-03 23:20:54.924774: step 20060, total loss = 0.79, predict loss = 0.19 (75.3 examples/sec; 0.053 sec/batch; 88h:12m:48s remains)
INFO - root - 2019-11-03 23:20:55.594810: step 20070, total loss = 0.48, predict loss = 0.10 (62.9 examples/sec; 0.064 sec/batch; 105h:37m:02s remains)
INFO - root - 2019-11-03 23:20:56.213480: step 20080, total loss = 0.59, predict loss = 0.13 (75.0 examples/sec; 0.053 sec/batch; 88h:32m:04s remains)
INFO - root - 2019-11-03 23:20:56.848767: step 20090, total loss = 0.89, predict loss = 0.21 (70.7 examples/sec; 0.057 sec/batch; 93h:58m:55s remains)
INFO - root - 2019-11-03 23:20:57.529381: step 20100, total loss = 0.80, predict loss = 0.19 (66.1 examples/sec; 0.061 sec/batch; 100h:32m:05s remains)
INFO - root - 2019-11-03 23:20:58.127579: step 20110, total loss = 0.82, predict loss = 0.20 (79.5 examples/sec; 0.050 sec/batch; 83h:37m:10s remains)
INFO - root - 2019-11-03 23:20:58.749129: step 20120, total loss = 0.88, predict loss = 0.21 (74.5 examples/sec; 0.054 sec/batch; 89h:12m:27s remains)
INFO - root - 2019-11-03 23:20:59.421642: step 20130, total loss = 0.51, predict loss = 0.11 (63.1 examples/sec; 0.063 sec/batch; 105h:14m:18s remains)
INFO - root - 2019-11-03 23:21:00.091451: step 20140, total loss = 0.74, predict loss = 0.17 (64.4 examples/sec; 0.062 sec/batch; 103h:11m:34s remains)
INFO - root - 2019-11-03 23:21:00.838264: step 20150, total loss = 0.41, predict loss = 0.08 (72.6 examples/sec; 0.055 sec/batch; 91h:33m:29s remains)
INFO - root - 2019-11-03 23:21:01.421518: step 20160, total loss = 0.39, predict loss = 0.09 (73.1 examples/sec; 0.055 sec/batch; 90h:52m:35s remains)
INFO - root - 2019-11-03 23:21:02.037255: step 20170, total loss = 0.66, predict loss = 0.16 (70.5 examples/sec; 0.057 sec/batch; 94h:15m:11s remains)
INFO - root - 2019-11-03 23:21:02.700302: step 20180, total loss = 0.41, predict loss = 0.08 (67.5 examples/sec; 0.059 sec/batch; 98h:27m:50s remains)
INFO - root - 2019-11-03 23:21:03.308381: step 20190, total loss = 0.38, predict loss = 0.08 (72.0 examples/sec; 0.056 sec/batch; 92h:17m:00s remains)
INFO - root - 2019-11-03 23:21:03.937122: step 20200, total loss = 0.50, predict loss = 0.10 (66.4 examples/sec; 0.060 sec/batch; 100h:02m:44s remains)
INFO - root - 2019-11-03 23:21:04.544168: step 20210, total loss = 0.51, predict loss = 0.12 (70.3 examples/sec; 0.057 sec/batch; 94h:27m:39s remains)
INFO - root - 2019-11-03 23:21:05.241864: step 20220, total loss = 0.88, predict loss = 0.22 (67.9 examples/sec; 0.059 sec/batch; 97h:50m:55s remains)
INFO - root - 2019-11-03 23:21:05.902831: step 20230, total loss = 0.69, predict loss = 0.16 (73.0 examples/sec; 0.055 sec/batch; 91h:01m:00s remains)
INFO - root - 2019-11-03 23:21:06.520882: step 20240, total loss = 0.62, predict loss = 0.14 (72.1 examples/sec; 0.055 sec/batch; 92h:06m:11s remains)
INFO - root - 2019-11-03 23:21:07.156726: step 20250, total loss = 0.70, predict loss = 0.15 (82.4 examples/sec; 0.049 sec/batch; 80h:37m:49s remains)
INFO - root - 2019-11-03 23:21:07.799515: step 20260, total loss = 0.77, predict loss = 0.19 (65.8 examples/sec; 0.061 sec/batch; 101h:00m:29s remains)
INFO - root - 2019-11-03 23:21:08.438488: step 20270, total loss = 0.61, predict loss = 0.14 (74.7 examples/sec; 0.054 sec/batch; 88h:59m:08s remains)
INFO - root - 2019-11-03 23:21:09.086565: step 20280, total loss = 0.69, predict loss = 0.16 (69.2 examples/sec; 0.058 sec/batch; 95h:58m:14s remains)
INFO - root - 2019-11-03 23:21:09.721917: step 20290, total loss = 0.71, predict loss = 0.19 (70.6 examples/sec; 0.057 sec/batch; 94h:09m:18s remains)
INFO - root - 2019-11-03 23:21:10.337948: step 20300, total loss = 0.58, predict loss = 0.12 (72.4 examples/sec; 0.055 sec/batch; 91h:48m:47s remains)
INFO - root - 2019-11-03 23:21:10.932124: step 20310, total loss = 0.70, predict loss = 0.16 (74.9 examples/sec; 0.053 sec/batch; 88h:39m:45s remains)
INFO - root - 2019-11-03 23:21:11.560166: step 20320, total loss = 0.60, predict loss = 0.13 (63.9 examples/sec; 0.063 sec/batch; 104h:01m:07s remains)
INFO - root - 2019-11-03 23:21:12.165910: step 20330, total loss = 0.84, predict loss = 0.19 (72.0 examples/sec; 0.056 sec/batch; 92h:20m:09s remains)
INFO - root - 2019-11-03 23:21:12.815819: step 20340, total loss = 0.75, predict loss = 0.18 (72.4 examples/sec; 0.055 sec/batch; 91h:49m:23s remains)
INFO - root - 2019-11-03 23:21:13.450390: step 20350, total loss = 0.61, predict loss = 0.14 (71.4 examples/sec; 0.056 sec/batch; 93h:00m:54s remains)
INFO - root - 2019-11-03 23:21:14.081125: step 20360, total loss = 0.77, predict loss = 0.18 (64.2 examples/sec; 0.062 sec/batch; 103h:28m:41s remains)
INFO - root - 2019-11-03 23:21:14.715716: step 20370, total loss = 0.54, predict loss = 0.12 (77.4 examples/sec; 0.052 sec/batch; 85h:47m:36s remains)
INFO - root - 2019-11-03 23:21:15.345408: step 20380, total loss = 0.88, predict loss = 0.22 (69.7 examples/sec; 0.057 sec/batch; 95h:20m:18s remains)
INFO - root - 2019-11-03 23:21:15.965945: step 20390, total loss = 0.53, predict loss = 0.12 (81.4 examples/sec; 0.049 sec/batch; 81h:38m:09s remains)
INFO - root - 2019-11-03 23:21:16.615209: step 20400, total loss = 0.67, predict loss = 0.15 (68.3 examples/sec; 0.059 sec/batch; 97h:13m:00s remains)
INFO - root - 2019-11-03 23:21:17.223040: step 20410, total loss = 0.73, predict loss = 0.15 (80.0 examples/sec; 0.050 sec/batch; 83h:02m:56s remains)
INFO - root - 2019-11-03 23:21:17.861806: step 20420, total loss = 0.80, predict loss = 0.19 (64.7 examples/sec; 0.062 sec/batch; 102h:39m:23s remains)
INFO - root - 2019-11-03 23:21:18.497930: step 20430, total loss = 0.48, predict loss = 0.11 (65.6 examples/sec; 0.061 sec/batch; 101h:16m:19s remains)
INFO - root - 2019-11-03 23:21:19.143990: step 20440, total loss = 0.62, predict loss = 0.14 (78.0 examples/sec; 0.051 sec/batch; 85h:13m:14s remains)
INFO - root - 2019-11-03 23:21:19.769266: step 20450, total loss = 0.42, predict loss = 0.10 (69.3 examples/sec; 0.058 sec/batch; 95h:52m:02s remains)
INFO - root - 2019-11-03 23:21:20.420774: step 20460, total loss = 0.77, predict loss = 0.21 (69.4 examples/sec; 0.058 sec/batch; 95h:47m:23s remains)
INFO - root - 2019-11-03 23:21:21.057383: step 20470, total loss = 0.63, predict loss = 0.15 (67.8 examples/sec; 0.059 sec/batch; 97h:57m:59s remains)
INFO - root - 2019-11-03 23:21:21.697554: step 20480, total loss = 0.73, predict loss = 0.16 (70.5 examples/sec; 0.057 sec/batch; 94h:13m:54s remains)
INFO - root - 2019-11-03 23:21:22.320017: step 20490, total loss = 0.90, predict loss = 0.23 (75.8 examples/sec; 0.053 sec/batch; 87h:38m:19s remains)
INFO - root - 2019-11-03 23:21:22.965555: step 20500, total loss = 0.70, predict loss = 0.16 (67.3 examples/sec; 0.059 sec/batch; 98h:43m:36s remains)
INFO - root - 2019-11-03 23:21:23.596429: step 20510, total loss = 0.98, predict loss = 0.23 (76.5 examples/sec; 0.052 sec/batch; 86h:49m:04s remains)
INFO - root - 2019-11-03 23:21:24.210810: step 20520, total loss = 0.67, predict loss = 0.15 (70.0 examples/sec; 0.057 sec/batch; 94h:51m:19s remains)
INFO - root - 2019-11-03 23:21:24.840202: step 20530, total loss = 1.02, predict loss = 0.25 (70.1 examples/sec; 0.057 sec/batch; 94h:49m:03s remains)
INFO - root - 2019-11-03 23:21:25.475272: step 20540, total loss = 1.04, predict loss = 0.26 (69.5 examples/sec; 0.058 sec/batch; 95h:35m:50s remains)
INFO - root - 2019-11-03 23:21:26.083571: step 20550, total loss = 1.31, predict loss = 0.33 (72.7 examples/sec; 0.055 sec/batch; 91h:24m:42s remains)
INFO - root - 2019-11-03 23:21:26.703839: step 20560, total loss = 1.02, predict loss = 0.24 (68.2 examples/sec; 0.059 sec/batch; 97h:25m:49s remains)
INFO - root - 2019-11-03 23:21:27.355936: step 20570, total loss = 0.91, predict loss = 0.22 (71.7 examples/sec; 0.056 sec/batch; 92h:36m:56s remains)
INFO - root - 2019-11-03 23:21:27.990675: step 20580, total loss = 1.01, predict loss = 0.24 (66.7 examples/sec; 0.060 sec/batch; 99h:40m:48s remains)
INFO - root - 2019-11-03 23:21:28.635466: step 20590, total loss = 1.01, predict loss = 0.22 (72.4 examples/sec; 0.055 sec/batch; 91h:48m:28s remains)
INFO - root - 2019-11-03 23:21:29.276664: step 20600, total loss = 0.78, predict loss = 0.18 (73.1 examples/sec; 0.055 sec/batch; 90h:54m:10s remains)
INFO - root - 2019-11-03 23:21:29.952433: step 20610, total loss = 0.71, predict loss = 0.15 (58.7 examples/sec; 0.068 sec/batch; 113h:14m:43s remains)
INFO - root - 2019-11-03 23:21:30.685505: step 20620, total loss = 0.74, predict loss = 0.18 (58.8 examples/sec; 0.068 sec/batch; 113h:01m:02s remains)
INFO - root - 2019-11-03 23:21:31.319119: step 20630, total loss = 0.79, predict loss = 0.18 (69.3 examples/sec; 0.058 sec/batch; 95h:50m:26s remains)
INFO - root - 2019-11-03 23:21:31.967435: step 20640, total loss = 0.68, predict loss = 0.16 (78.6 examples/sec; 0.051 sec/batch; 84h:33m:06s remains)
INFO - root - 2019-11-03 23:21:32.582675: step 20650, total loss = 0.68, predict loss = 0.15 (79.0 examples/sec; 0.051 sec/batch; 84h:03m:53s remains)
INFO - root - 2019-11-03 23:21:33.219948: step 20660, total loss = 0.62, predict loss = 0.14 (73.5 examples/sec; 0.054 sec/batch; 90h:21m:05s remains)
INFO - root - 2019-11-03 23:21:33.827320: step 20670, total loss = 0.67, predict loss = 0.16 (68.4 examples/sec; 0.058 sec/batch; 97h:03m:39s remains)
INFO - root - 2019-11-03 23:21:34.465532: step 20680, total loss = 0.70, predict loss = 0.16 (68.9 examples/sec; 0.058 sec/batch; 96h:25m:50s remains)
INFO - root - 2019-11-03 23:21:35.140036: step 20690, total loss = 0.71, predict loss = 0.16 (61.8 examples/sec; 0.065 sec/batch; 107h:26m:42s remains)
INFO - root - 2019-11-03 23:21:35.799886: step 20700, total loss = 0.71, predict loss = 0.17 (75.0 examples/sec; 0.053 sec/batch; 88h:34m:51s remains)
INFO - root - 2019-11-03 23:21:36.425727: step 20710, total loss = 0.70, predict loss = 0.18 (85.6 examples/sec; 0.047 sec/batch; 77h:35m:15s remains)
INFO - root - 2019-11-03 23:21:37.055233: step 20720, total loss = 0.83, predict loss = 0.18 (66.6 examples/sec; 0.060 sec/batch; 99h:47m:09s remains)
INFO - root - 2019-11-03 23:21:37.678572: step 20730, total loss = 0.69, predict loss = 0.15 (73.3 examples/sec; 0.055 sec/batch; 90h:35m:23s remains)
INFO - root - 2019-11-03 23:21:38.340592: step 20740, total loss = 0.72, predict loss = 0.17 (69.5 examples/sec; 0.058 sec/batch; 95h:31m:25s remains)
INFO - root - 2019-11-03 23:21:38.955454: step 20750, total loss = 0.65, predict loss = 0.15 (80.9 examples/sec; 0.049 sec/batch; 82h:04m:31s remains)
INFO - root - 2019-11-03 23:21:39.601537: step 20760, total loss = 0.50, predict loss = 0.12 (86.0 examples/sec; 0.047 sec/batch; 77h:14m:35s remains)
INFO - root - 2019-11-03 23:21:40.234308: step 20770, total loss = 0.46, predict loss = 0.10 (69.4 examples/sec; 0.058 sec/batch; 95h:41m:40s remains)
INFO - root - 2019-11-03 23:21:40.882137: step 20780, total loss = 0.58, predict loss = 0.14 (61.1 examples/sec; 0.065 sec/batch; 108h:39m:15s remains)
INFO - root - 2019-11-03 23:21:41.552005: step 20790, total loss = 0.68, predict loss = 0.17 (74.1 examples/sec; 0.054 sec/batch; 89h:37m:42s remains)
INFO - root - 2019-11-03 23:21:42.195097: step 20800, total loss = 0.62, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 102h:07m:35s remains)
INFO - root - 2019-11-03 23:21:42.828391: step 20810, total loss = 0.55, predict loss = 0.13 (74.2 examples/sec; 0.054 sec/batch; 89h:28m:48s remains)
INFO - root - 2019-11-03 23:21:43.450420: step 20820, total loss = 0.64, predict loss = 0.15 (70.3 examples/sec; 0.057 sec/batch; 94h:29m:17s remains)
INFO - root - 2019-11-03 23:21:44.090742: step 20830, total loss = 0.61, predict loss = 0.14 (68.6 examples/sec; 0.058 sec/batch; 96h:48m:34s remains)
INFO - root - 2019-11-03 23:21:44.751454: step 20840, total loss = 0.73, predict loss = 0.16 (67.9 examples/sec; 0.059 sec/batch; 97h:51m:42s remains)
INFO - root - 2019-11-03 23:21:45.392645: step 20850, total loss = 0.53, predict loss = 0.12 (62.7 examples/sec; 0.064 sec/batch; 106h:01m:24s remains)
INFO - root - 2019-11-03 23:21:46.053408: step 20860, total loss = 0.55, predict loss = 0.12 (65.7 examples/sec; 0.061 sec/batch; 101h:09m:19s remains)
INFO - root - 2019-11-03 23:21:46.755527: step 20870, total loss = 0.55, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 96h:38m:12s remains)
INFO - root - 2019-11-03 23:21:47.437308: step 20880, total loss = 0.57, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 103h:15m:24s remains)
INFO - root - 2019-11-03 23:21:48.094608: step 20890, total loss = 0.62, predict loss = 0.14 (66.4 examples/sec; 0.060 sec/batch; 100h:07m:04s remains)
INFO - root - 2019-11-03 23:21:48.739036: step 20900, total loss = 0.73, predict loss = 0.18 (69.1 examples/sec; 0.058 sec/batch; 96h:09m:21s remains)
INFO - root - 2019-11-03 23:21:49.367749: step 20910, total loss = 0.99, predict loss = 0.22 (79.0 examples/sec; 0.051 sec/batch; 84h:06m:38s remains)
INFO - root - 2019-11-03 23:21:49.987231: step 20920, total loss = 0.74, predict loss = 0.17 (67.3 examples/sec; 0.059 sec/batch; 98h:45m:55s remains)
INFO - root - 2019-11-03 23:21:50.661632: step 20930, total loss = 0.90, predict loss = 0.21 (65.9 examples/sec; 0.061 sec/batch; 100h:52m:41s remains)
INFO - root - 2019-11-03 23:21:51.268070: step 20940, total loss = 0.78, predict loss = 0.18 (73.7 examples/sec; 0.054 sec/batch; 90h:08m:53s remains)
INFO - root - 2019-11-03 23:21:51.885489: step 20950, total loss = 0.83, predict loss = 0.20 (84.6 examples/sec; 0.047 sec/batch; 78h:32m:26s remains)
INFO - root - 2019-11-03 23:21:52.514717: step 20960, total loss = 0.82, predict loss = 0.20 (60.0 examples/sec; 0.067 sec/batch; 110h:42m:40s remains)
INFO - root - 2019-11-03 23:21:53.140691: step 20970, total loss = 0.83, predict loss = 0.20 (86.6 examples/sec; 0.046 sec/batch; 76h:44m:59s remains)
INFO - root - 2019-11-03 23:21:53.755601: step 20980, total loss = 1.00, predict loss = 0.24 (68.5 examples/sec; 0.058 sec/batch; 97h:02m:38s remains)
INFO - root - 2019-11-03 23:21:54.367728: step 20990, total loss = 1.14, predict loss = 0.27 (66.9 examples/sec; 0.060 sec/batch; 99h:19m:14s remains)
INFO - root - 2019-11-03 23:21:55.006818: step 21000, total loss = 0.74, predict loss = 0.18 (66.0 examples/sec; 0.061 sec/batch; 100h:42m:45s remains)
INFO - root - 2019-11-03 23:21:55.614409: step 21010, total loss = 0.85, predict loss = 0.19 (72.3 examples/sec; 0.055 sec/batch; 91h:49m:29s remains)
INFO - root - 2019-11-03 23:21:56.246912: step 21020, total loss = 0.79, predict loss = 0.19 (76.3 examples/sec; 0.052 sec/batch; 87h:06m:35s remains)
INFO - root - 2019-11-03 23:21:56.875033: step 21030, total loss = 0.90, predict loss = 0.22 (63.6 examples/sec; 0.063 sec/batch; 104h:25m:10s remains)
INFO - root - 2019-11-03 23:21:57.593175: step 21040, total loss = 0.82, predict loss = 0.19 (67.8 examples/sec; 0.059 sec/batch; 97h:58m:51s remains)
INFO - root - 2019-11-03 23:21:58.251390: step 21050, total loss = 0.62, predict loss = 0.14 (67.4 examples/sec; 0.059 sec/batch; 98h:33m:32s remains)
INFO - root - 2019-11-03 23:21:58.888139: step 21060, total loss = 0.58, predict loss = 0.13 (72.4 examples/sec; 0.055 sec/batch; 91h:48m:37s remains)
INFO - root - 2019-11-03 23:21:59.493343: step 21070, total loss = 0.57, predict loss = 0.13 (68.4 examples/sec; 0.058 sec/batch; 97h:05m:49s remains)
INFO - root - 2019-11-03 23:22:00.140920: step 21080, total loss = 0.58, predict loss = 0.14 (64.1 examples/sec; 0.062 sec/batch; 103h:34m:11s remains)
INFO - root - 2019-11-03 23:22:00.862804: step 21090, total loss = 0.57, predict loss = 0.12 (81.7 examples/sec; 0.049 sec/batch; 81h:15m:57s remains)
INFO - root - 2019-11-03 23:22:01.469439: step 21100, total loss = 0.51, predict loss = 0.12 (81.7 examples/sec; 0.049 sec/batch; 81h:19m:20s remains)
INFO - root - 2019-11-03 23:22:02.142719: step 21110, total loss = 0.57, predict loss = 0.12 (62.8 examples/sec; 0.064 sec/batch; 105h:50m:44s remains)
INFO - root - 2019-11-03 23:22:02.805250: step 21120, total loss = 0.79, predict loss = 0.18 (64.2 examples/sec; 0.062 sec/batch; 103h:24m:57s remains)
INFO - root - 2019-11-03 23:22:03.474764: step 21130, total loss = 0.60, predict loss = 0.14 (69.9 examples/sec; 0.057 sec/batch; 95h:04m:09s remains)
INFO - root - 2019-11-03 23:22:04.163873: step 21140, total loss = 0.67, predict loss = 0.16 (66.9 examples/sec; 0.060 sec/batch; 99h:14m:37s remains)
INFO - root - 2019-11-03 23:22:04.828553: step 21150, total loss = 0.59, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 102h:02m:19s remains)
INFO - root - 2019-11-03 23:22:05.492361: step 21160, total loss = 0.69, predict loss = 0.17 (68.2 examples/sec; 0.059 sec/batch; 97h:22m:20s remains)
INFO - root - 2019-11-03 23:22:06.129529: step 21170, total loss = 0.72, predict loss = 0.18 (68.0 examples/sec; 0.059 sec/batch; 97h:38m:03s remains)
INFO - root - 2019-11-03 23:22:06.732298: step 21180, total loss = 0.73, predict loss = 0.18 (75.4 examples/sec; 0.053 sec/batch; 88h:04m:58s remains)
INFO - root - 2019-11-03 23:22:07.397302: step 21190, total loss = 0.65, predict loss = 0.15 (66.4 examples/sec; 0.060 sec/batch; 100h:02m:52s remains)
INFO - root - 2019-11-03 23:22:08.059355: step 21200, total loss = 0.80, predict loss = 0.19 (73.6 examples/sec; 0.054 sec/batch; 90h:19m:01s remains)
INFO - root - 2019-11-03 23:22:08.690764: step 21210, total loss = 0.79, predict loss = 0.20 (69.1 examples/sec; 0.058 sec/batch; 96h:12m:09s remains)
INFO - root - 2019-11-03 23:22:09.336084: step 21220, total loss = 0.77, predict loss = 0.20 (65.2 examples/sec; 0.061 sec/batch; 101h:49m:02s remains)
INFO - root - 2019-11-03 23:22:10.025227: step 21230, total loss = 0.72, predict loss = 0.18 (61.9 examples/sec; 0.065 sec/batch; 107h:23m:33s remains)
INFO - root - 2019-11-03 23:22:10.716486: step 21240, total loss = 0.70, predict loss = 0.16 (64.9 examples/sec; 0.062 sec/batch; 102h:23m:10s remains)
INFO - root - 2019-11-03 23:22:11.361423: step 21250, total loss = 0.69, predict loss = 0.16 (70.4 examples/sec; 0.057 sec/batch; 94h:20m:55s remains)
INFO - root - 2019-11-03 23:22:12.009925: step 21260, total loss = 0.60, predict loss = 0.15 (72.1 examples/sec; 0.056 sec/batch; 92h:11m:34s remains)
INFO - root - 2019-11-03 23:22:12.650732: step 21270, total loss = 0.62, predict loss = 0.13 (74.5 examples/sec; 0.054 sec/batch; 89h:10m:10s remains)
INFO - root - 2019-11-03 23:22:13.309616: step 21280, total loss = 0.91, predict loss = 0.20 (72.0 examples/sec; 0.056 sec/batch; 92h:16m:49s remains)
INFO - root - 2019-11-03 23:22:13.911326: step 21290, total loss = 0.60, predict loss = 0.14 (72.5 examples/sec; 0.055 sec/batch; 91h:35m:28s remains)
INFO - root - 2019-11-03 23:22:14.546136: step 21300, total loss = 0.60, predict loss = 0.14 (66.6 examples/sec; 0.060 sec/batch; 99h:41m:32s remains)
INFO - root - 2019-11-03 23:22:15.161278: step 21310, total loss = 0.62, predict loss = 0.14 (70.8 examples/sec; 0.056 sec/batch; 93h:46m:46s remains)
INFO - root - 2019-11-03 23:22:15.781924: step 21320, total loss = 0.64, predict loss = 0.15 (77.7 examples/sec; 0.051 sec/batch; 85h:29m:00s remains)
INFO - root - 2019-11-03 23:22:16.437198: step 21330, total loss = 0.65, predict loss = 0.14 (63.4 examples/sec; 0.063 sec/batch; 104h:49m:08s remains)
INFO - root - 2019-11-03 23:22:17.055201: step 21340, total loss = 0.64, predict loss = 0.14 (68.4 examples/sec; 0.058 sec/batch; 97h:08m:41s remains)
INFO - root - 2019-11-03 23:22:17.707004: step 21350, total loss = 0.64, predict loss = 0.14 (68.2 examples/sec; 0.059 sec/batch; 97h:24m:06s remains)
INFO - root - 2019-11-03 23:22:18.365395: step 21360, total loss = 0.47, predict loss = 0.10 (67.4 examples/sec; 0.059 sec/batch; 98h:36m:07s remains)
INFO - root - 2019-11-03 23:22:19.045311: step 21370, total loss = 0.59, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 97h:34m:15s remains)
INFO - root - 2019-11-03 23:22:19.725851: step 21380, total loss = 0.58, predict loss = 0.14 (74.6 examples/sec; 0.054 sec/batch; 89h:00m:23s remains)
INFO - root - 2019-11-03 23:22:20.383251: step 21390, total loss = 0.58, predict loss = 0.12 (65.8 examples/sec; 0.061 sec/batch; 100h:54m:57s remains)
INFO - root - 2019-11-03 23:22:21.045709: step 21400, total loss = 0.65, predict loss = 0.15 (59.6 examples/sec; 0.067 sec/batch; 111h:31m:40s remains)
INFO - root - 2019-11-03 23:22:21.669096: step 21410, total loss = 0.69, predict loss = 0.16 (76.1 examples/sec; 0.053 sec/batch; 87h:20m:27s remains)
INFO - root - 2019-11-03 23:22:22.320440: step 21420, total loss = 0.53, predict loss = 0.12 (73.5 examples/sec; 0.054 sec/batch; 90h:19m:17s remains)
INFO - root - 2019-11-03 23:22:22.944724: step 21430, total loss = 0.67, predict loss = 0.15 (72.6 examples/sec; 0.055 sec/batch; 91h:27m:43s remains)
INFO - root - 2019-11-03 23:22:23.568859: step 21440, total loss = 0.64, predict loss = 0.15 (73.3 examples/sec; 0.055 sec/batch; 90h:39m:29s remains)
INFO - root - 2019-11-03 23:22:24.229510: step 21450, total loss = 0.67, predict loss = 0.16 (75.4 examples/sec; 0.053 sec/batch; 88h:05m:09s remains)
INFO - root - 2019-11-03 23:22:24.875889: step 21460, total loss = 0.95, predict loss = 0.22 (66.0 examples/sec; 0.061 sec/batch; 100h:38m:38s remains)
INFO - root - 2019-11-03 23:22:25.490087: step 21470, total loss = 0.70, predict loss = 0.17 (67.0 examples/sec; 0.060 sec/batch; 99h:04m:34s remains)
INFO - root - 2019-11-03 23:22:26.117839: step 21480, total loss = 0.81, predict loss = 0.19 (68.9 examples/sec; 0.058 sec/batch; 96h:25m:08s remains)
INFO - root - 2019-11-03 23:22:26.762765: step 21490, total loss = 0.56, predict loss = 0.13 (67.4 examples/sec; 0.059 sec/batch; 98h:32m:51s remains)
INFO - root - 2019-11-03 23:22:27.396466: step 21500, total loss = 0.70, predict loss = 0.16 (74.5 examples/sec; 0.054 sec/batch; 89h:12m:47s remains)
INFO - root - 2019-11-03 23:22:28.029180: step 21510, total loss = 0.77, predict loss = 0.17 (78.2 examples/sec; 0.051 sec/batch; 84h:54m:24s remains)
INFO - root - 2019-11-03 23:22:28.668130: step 21520, total loss = 0.60, predict loss = 0.14 (78.9 examples/sec; 0.051 sec/batch; 84h:11m:27s remains)
INFO - root - 2019-11-03 23:22:29.289428: step 21530, total loss = 0.69, predict loss = 0.17 (70.2 examples/sec; 0.057 sec/batch; 94h:35m:55s remains)
INFO - root - 2019-11-03 23:22:29.937494: step 21540, total loss = 0.75, predict loss = 0.19 (66.6 examples/sec; 0.060 sec/batch; 99h:41m:28s remains)
INFO - root - 2019-11-03 23:22:30.642637: step 21550, total loss = 0.75, predict loss = 0.19 (50.8 examples/sec; 0.079 sec/batch; 130h:48m:43s remains)
INFO - root - 2019-11-03 23:22:31.278003: step 21560, total loss = 0.79, predict loss = 0.18 (64.7 examples/sec; 0.062 sec/batch; 102h:42m:22s remains)
INFO - root - 2019-11-03 23:22:31.904930: step 21570, total loss = 0.87, predict loss = 0.20 (62.3 examples/sec; 0.064 sec/batch; 106h:35m:23s remains)
INFO - root - 2019-11-03 23:22:32.588134: step 21580, total loss = 0.85, predict loss = 0.20 (65.2 examples/sec; 0.061 sec/batch; 101h:49m:11s remains)
INFO - root - 2019-11-03 23:22:33.251947: step 21590, total loss = 0.79, predict loss = 0.19 (71.2 examples/sec; 0.056 sec/batch; 93h:20m:33s remains)
INFO - root - 2019-11-03 23:22:33.918373: step 21600, total loss = 0.96, predict loss = 0.23 (77.8 examples/sec; 0.051 sec/batch; 85h:21m:59s remains)
INFO - root - 2019-11-03 23:22:34.546820: step 21610, total loss = 1.09, predict loss = 0.26 (60.8 examples/sec; 0.066 sec/batch; 109h:19m:53s remains)
INFO - root - 2019-11-03 23:22:35.216893: step 21620, total loss = 1.13, predict loss = 0.25 (68.7 examples/sec; 0.058 sec/batch; 96h:40m:49s remains)
INFO - root - 2019-11-03 23:22:35.829284: step 21630, total loss = 0.96, predict loss = 0.22 (73.3 examples/sec; 0.055 sec/batch; 90h:40m:31s remains)
INFO - root - 2019-11-03 23:22:36.457484: step 21640, total loss = 0.87, predict loss = 0.22 (59.0 examples/sec; 0.068 sec/batch; 112h:39m:08s remains)
INFO - root - 2019-11-03 23:22:37.148441: step 21650, total loss = 1.02, predict loss = 0.24 (66.7 examples/sec; 0.060 sec/batch; 99h:31m:50s remains)
INFO - root - 2019-11-03 23:22:37.806678: step 21660, total loss = 0.87, predict loss = 0.21 (78.8 examples/sec; 0.051 sec/batch; 84h:14m:59s remains)
INFO - root - 2019-11-03 23:22:38.464105: step 21670, total loss = 0.71, predict loss = 0.16 (67.3 examples/sec; 0.059 sec/batch; 98h:40m:42s remains)
INFO - root - 2019-11-03 23:22:39.092178: step 21680, total loss = 0.52, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 97h:05m:43s remains)
INFO - root - 2019-11-03 23:22:39.725563: step 21690, total loss = 0.88, predict loss = 0.21 (74.5 examples/sec; 0.054 sec/batch; 89h:08m:41s remains)
INFO - root - 2019-11-03 23:22:40.359104: step 21700, total loss = 0.51, predict loss = 0.11 (65.5 examples/sec; 0.061 sec/batch; 101h:23m:29s remains)
INFO - root - 2019-11-03 23:22:41.003994: step 21710, total loss = 0.76, predict loss = 0.17 (71.5 examples/sec; 0.056 sec/batch; 92h:56m:56s remains)
INFO - root - 2019-11-03 23:22:41.616821: step 21720, total loss = 0.69, predict loss = 0.16 (73.1 examples/sec; 0.055 sec/batch; 90h:48m:27s remains)
INFO - root - 2019-11-03 23:22:42.227521: step 21730, total loss = 0.66, predict loss = 0.16 (79.1 examples/sec; 0.051 sec/batch; 83h:59m:01s remains)
INFO - root - 2019-11-03 23:22:42.867500: step 21740, total loss = 0.77, predict loss = 0.18 (67.7 examples/sec; 0.059 sec/batch; 98h:04m:38s remains)
INFO - root - 2019-11-03 23:22:43.532064: step 21750, total loss = 0.74, predict loss = 0.18 (70.4 examples/sec; 0.057 sec/batch; 94h:17m:38s remains)
INFO - root - 2019-11-03 23:22:44.154871: step 21760, total loss = 0.71, predict loss = 0.17 (75.6 examples/sec; 0.053 sec/batch; 87h:54m:35s remains)
INFO - root - 2019-11-03 23:22:44.770223: step 21770, total loss = 0.80, predict loss = 0.19 (68.7 examples/sec; 0.058 sec/batch; 96h:45m:24s remains)
INFO - root - 2019-11-03 23:22:45.385222: step 21780, total loss = 0.46, predict loss = 0.12 (66.7 examples/sec; 0.060 sec/batch; 99h:38m:54s remains)
INFO - root - 2019-11-03 23:22:46.031990: step 21790, total loss = 0.65, predict loss = 0.16 (69.7 examples/sec; 0.057 sec/batch; 95h:18m:42s remains)
INFO - root - 2019-11-03 23:22:46.599134: step 21800, total loss = 0.53, predict loss = 0.12 (94.9 examples/sec; 0.042 sec/batch; 69h:59m:31s remains)
INFO - root - 2019-11-03 23:22:47.053561: step 21810, total loss = 0.86, predict loss = 0.20 (96.9 examples/sec; 0.041 sec/batch; 68h:32m:12s remains)
INFO - root - 2019-11-03 23:22:48.080094: step 21820, total loss = 0.96, predict loss = 0.21 (65.5 examples/sec; 0.061 sec/batch; 101h:23m:24s remains)
INFO - root - 2019-11-03 23:22:48.706715: step 21830, total loss = 0.53, predict loss = 0.12 (69.3 examples/sec; 0.058 sec/batch; 95h:53m:40s remains)
INFO - root - 2019-11-03 23:22:49.345994: step 21840, total loss = 0.55, predict loss = 0.13 (80.3 examples/sec; 0.050 sec/batch; 82h:40m:19s remains)
INFO - root - 2019-11-03 23:22:50.001655: step 21850, total loss = 0.86, predict loss = 0.19 (65.7 examples/sec; 0.061 sec/batch; 101h:05m:22s remains)
INFO - root - 2019-11-03 23:22:50.637226: step 21860, total loss = 0.70, predict loss = 0.16 (68.3 examples/sec; 0.059 sec/batch; 97h:15m:14s remains)
INFO - root - 2019-11-03 23:22:51.277805: step 21870, total loss = 0.90, predict loss = 0.22 (83.6 examples/sec; 0.048 sec/batch; 79h:27m:48s remains)
INFO - root - 2019-11-03 23:22:51.933472: step 21880, total loss = 0.64, predict loss = 0.15 (66.2 examples/sec; 0.060 sec/batch; 100h:21m:13s remains)
INFO - root - 2019-11-03 23:22:52.584042: step 21890, total loss = 0.47, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 99h:54m:42s remains)
INFO - root - 2019-11-03 23:22:53.230631: step 21900, total loss = 0.86, predict loss = 0.18 (67.1 examples/sec; 0.060 sec/batch; 98h:56m:57s remains)
INFO - root - 2019-11-03 23:22:53.917097: step 21910, total loss = 1.03, predict loss = 0.21 (62.3 examples/sec; 0.064 sec/batch; 106h:34m:47s remains)
INFO - root - 2019-11-03 23:22:54.560683: step 21920, total loss = 0.84, predict loss = 0.22 (65.5 examples/sec; 0.061 sec/batch; 101h:21m:15s remains)
INFO - root - 2019-11-03 23:22:55.245554: step 21930, total loss = 0.80, predict loss = 0.19 (71.2 examples/sec; 0.056 sec/batch; 93h:20m:38s remains)
INFO - root - 2019-11-03 23:22:55.922737: step 21940, total loss = 0.63, predict loss = 0.13 (61.3 examples/sec; 0.065 sec/batch; 108h:25m:10s remains)
INFO - root - 2019-11-03 23:22:56.598891: step 21950, total loss = 0.70, predict loss = 0.15 (65.0 examples/sec; 0.062 sec/batch; 102h:09m:30s remains)
INFO - root - 2019-11-03 23:22:57.255858: step 21960, total loss = 0.60, predict loss = 0.14 (64.2 examples/sec; 0.062 sec/batch; 103h:27m:13s remains)
INFO - root - 2019-11-03 23:22:57.909322: step 21970, total loss = 0.64, predict loss = 0.14 (67.8 examples/sec; 0.059 sec/batch; 98h:00m:29s remains)
INFO - root - 2019-11-03 23:22:58.529591: step 21980, total loss = 0.67, predict loss = 0.16 (77.0 examples/sec; 0.052 sec/batch; 86h:14m:26s remains)
INFO - root - 2019-11-03 23:22:59.184007: step 21990, total loss = 0.72, predict loss = 0.17 (66.3 examples/sec; 0.060 sec/batch; 100h:12m:32s remains)
INFO - root - 2019-11-03 23:22:59.855158: step 22000, total loss = 0.67, predict loss = 0.14 (64.9 examples/sec; 0.062 sec/batch; 102h:16m:11s remains)
INFO - root - 2019-11-03 23:23:00.616972: step 22010, total loss = 0.71, predict loss = 0.17 (55.6 examples/sec; 0.072 sec/batch; 119h:32m:37s remains)
INFO - root - 2019-11-03 23:23:01.284485: step 22020, total loss = 0.50, predict loss = 0.10 (68.5 examples/sec; 0.058 sec/batch; 97h:00m:57s remains)
INFO - root - 2019-11-03 23:23:01.932572: step 22030, total loss = 0.89, predict loss = 0.21 (61.4 examples/sec; 0.065 sec/batch; 108h:09m:59s remains)
INFO - root - 2019-11-03 23:23:02.645432: step 22040, total loss = 0.69, predict loss = 0.15 (63.8 examples/sec; 0.063 sec/batch; 104h:03m:48s remains)
INFO - root - 2019-11-03 23:23:03.312023: step 22050, total loss = 0.52, predict loss = 0.09 (66.7 examples/sec; 0.060 sec/batch; 99h:33m:20s remains)
INFO - root - 2019-11-03 23:23:03.983889: step 22060, total loss = 0.73, predict loss = 0.17 (73.8 examples/sec; 0.054 sec/batch; 90h:03m:37s remains)
INFO - root - 2019-11-03 23:23:04.604957: step 22070, total loss = 0.77, predict loss = 0.16 (71.6 examples/sec; 0.056 sec/batch; 92h:48m:56s remains)
INFO - root - 2019-11-03 23:23:05.262010: step 22080, total loss = 0.77, predict loss = 0.18 (67.0 examples/sec; 0.060 sec/batch; 99h:08m:58s remains)
INFO - root - 2019-11-03 23:23:05.893932: step 22090, total loss = 0.92, predict loss = 0.21 (56.5 examples/sec; 0.071 sec/batch; 117h:35m:30s remains)
INFO - root - 2019-11-03 23:23:06.500981: step 22100, total loss = 0.94, predict loss = 0.21 (67.8 examples/sec; 0.059 sec/batch; 97h:58m:43s remains)
INFO - root - 2019-11-03 23:23:07.113788: step 22110, total loss = 1.00, predict loss = 0.24 (76.0 examples/sec; 0.053 sec/batch; 87h:27m:05s remains)
INFO - root - 2019-11-03 23:23:07.728696: step 22120, total loss = 0.70, predict loss = 0.16 (74.9 examples/sec; 0.053 sec/batch; 88h:40m:58s remains)
INFO - root - 2019-11-03 23:23:08.356979: step 22130, total loss = 0.72, predict loss = 0.16 (78.5 examples/sec; 0.051 sec/batch; 84h:35m:52s remains)
INFO - root - 2019-11-03 23:23:09.002512: step 22140, total loss = 0.92, predict loss = 0.21 (65.0 examples/sec; 0.062 sec/batch; 102h:08m:04s remains)
INFO - root - 2019-11-03 23:23:09.644418: step 22150, total loss = 0.76, predict loss = 0.17 (73.6 examples/sec; 0.054 sec/batch; 90h:11m:48s remains)
INFO - root - 2019-11-03 23:23:10.293459: step 22160, total loss = 0.81, predict loss = 0.18 (64.9 examples/sec; 0.062 sec/batch; 102h:21m:51s remains)
INFO - root - 2019-11-03 23:23:10.942871: step 22170, total loss = 0.92, predict loss = 0.21 (74.6 examples/sec; 0.054 sec/batch; 89h:05m:24s remains)
INFO - root - 2019-11-03 23:23:11.562447: step 22180, total loss = 0.75, predict loss = 0.16 (69.8 examples/sec; 0.057 sec/batch; 95h:06m:51s remains)
INFO - root - 2019-11-03 23:23:12.182686: step 22190, total loss = 1.10, predict loss = 0.25 (69.5 examples/sec; 0.058 sec/batch; 95h:35m:29s remains)
INFO - root - 2019-11-03 23:23:12.863963: step 22200, total loss = 0.66, predict loss = 0.13 (61.0 examples/sec; 0.066 sec/batch; 108h:51m:30s remains)
INFO - root - 2019-11-03 23:23:13.484028: step 22210, total loss = 0.71, predict loss = 0.15 (74.9 examples/sec; 0.053 sec/batch; 88h:42m:19s remains)
INFO - root - 2019-11-03 23:23:14.090616: step 22220, total loss = 0.89, predict loss = 0.18 (67.6 examples/sec; 0.059 sec/batch; 98h:12m:42s remains)
INFO - root - 2019-11-03 23:23:14.716455: step 22230, total loss = 0.77, predict loss = 0.17 (71.0 examples/sec; 0.056 sec/batch; 93h:35m:29s remains)
INFO - root - 2019-11-03 23:23:15.373859: step 22240, total loss = 0.74, predict loss = 0.15 (64.1 examples/sec; 0.062 sec/batch; 103h:35m:24s remains)
INFO - root - 2019-11-03 23:23:16.037799: step 22250, total loss = 0.60, predict loss = 0.13 (69.9 examples/sec; 0.057 sec/batch; 94h:58m:02s remains)
INFO - root - 2019-11-03 23:23:16.669002: step 22260, total loss = 0.70, predict loss = 0.15 (66.5 examples/sec; 0.060 sec/batch; 99h:57m:04s remains)
INFO - root - 2019-11-03 23:23:17.352695: step 22270, total loss = 0.53, predict loss = 0.12 (66.8 examples/sec; 0.060 sec/batch; 99h:29m:53s remains)
INFO - root - 2019-11-03 23:23:17.998728: step 22280, total loss = 0.83, predict loss = 0.22 (71.4 examples/sec; 0.056 sec/batch; 93h:02m:07s remains)
INFO - root - 2019-11-03 23:23:18.632249: step 22290, total loss = 0.89, predict loss = 0.23 (70.4 examples/sec; 0.057 sec/batch; 94h:18m:37s remains)
INFO - root - 2019-11-03 23:23:19.271523: step 22300, total loss = 0.63, predict loss = 0.15 (67.7 examples/sec; 0.059 sec/batch; 98h:06m:51s remains)
INFO - root - 2019-11-03 23:23:19.913465: step 22310, total loss = 0.92, predict loss = 0.22 (75.4 examples/sec; 0.053 sec/batch; 88h:06m:55s remains)
INFO - root - 2019-11-03 23:23:20.585858: step 22320, total loss = 0.75, predict loss = 0.18 (60.8 examples/sec; 0.066 sec/batch; 109h:09m:58s remains)
INFO - root - 2019-11-03 23:23:21.218710: step 22330, total loss = 0.75, predict loss = 0.16 (71.1 examples/sec; 0.056 sec/batch; 93h:28m:35s remains)
INFO - root - 2019-11-03 23:23:21.884660: step 22340, total loss = 0.80, predict loss = 0.19 (74.4 examples/sec; 0.054 sec/batch; 89h:18m:47s remains)
INFO - root - 2019-11-03 23:23:22.640421: step 22350, total loss = 0.78, predict loss = 0.19 (68.7 examples/sec; 0.058 sec/batch; 96h:41m:59s remains)
INFO - root - 2019-11-03 23:23:23.269412: step 22360, total loss = 0.75, predict loss = 0.17 (73.0 examples/sec; 0.055 sec/batch; 90h:58m:02s remains)
INFO - root - 2019-11-03 23:23:23.869890: step 22370, total loss = 0.77, predict loss = 0.19 (68.5 examples/sec; 0.058 sec/batch; 97h:01m:32s remains)
INFO - root - 2019-11-03 23:23:24.523867: step 22380, total loss = 0.61, predict loss = 0.15 (67.3 examples/sec; 0.059 sec/batch; 98h:37m:39s remains)
INFO - root - 2019-11-03 23:23:25.152829: step 22390, total loss = 0.61, predict loss = 0.14 (73.7 examples/sec; 0.054 sec/batch; 90h:06m:05s remains)
INFO - root - 2019-11-03 23:23:25.740246: step 22400, total loss = 0.49, predict loss = 0.11 (84.3 examples/sec; 0.047 sec/batch; 78h:45m:19s remains)
INFO - root - 2019-11-03 23:23:26.399779: step 22410, total loss = 0.53, predict loss = 0.13 (62.7 examples/sec; 0.064 sec/batch; 105h:59m:23s remains)
INFO - root - 2019-11-03 23:23:27.072597: step 22420, total loss = 0.45, predict loss = 0.09 (71.4 examples/sec; 0.056 sec/batch; 92h:59m:45s remains)
INFO - root - 2019-11-03 23:23:27.682063: step 22430, total loss = 0.79, predict loss = 0.18 (68.7 examples/sec; 0.058 sec/batch; 96h:44m:03s remains)
INFO - root - 2019-11-03 23:23:28.333410: step 22440, total loss = 0.53, predict loss = 0.11 (63.1 examples/sec; 0.063 sec/batch; 105h:18m:21s remains)
INFO - root - 2019-11-03 23:23:28.992369: step 22450, total loss = 0.68, predict loss = 0.16 (83.5 examples/sec; 0.048 sec/batch; 79h:34m:45s remains)
INFO - root - 2019-11-03 23:23:29.618782: step 22460, total loss = 1.03, predict loss = 0.26 (68.5 examples/sec; 0.058 sec/batch; 96h:54m:25s remains)
INFO - root - 2019-11-03 23:23:30.247092: step 22470, total loss = 0.54, predict loss = 0.13 (84.1 examples/sec; 0.048 sec/batch; 78h:59m:29s remains)
INFO - root - 2019-11-03 23:23:30.930951: step 22480, total loss = 0.57, predict loss = 0.13 (74.4 examples/sec; 0.054 sec/batch; 89h:13m:54s remains)
INFO - root - 2019-11-03 23:23:31.574101: step 22490, total loss = 0.90, predict loss = 0.20 (66.5 examples/sec; 0.060 sec/batch; 99h:50m:19s remains)
INFO - root - 2019-11-03 23:23:32.226114: step 22500, total loss = 0.90, predict loss = 0.23 (63.1 examples/sec; 0.063 sec/batch; 105h:10m:55s remains)
INFO - root - 2019-11-03 23:23:32.943694: step 22510, total loss = 0.87, predict loss = 0.21 (66.9 examples/sec; 0.060 sec/batch; 99h:16m:39s remains)
INFO - root - 2019-11-03 23:23:33.581492: step 22520, total loss = 0.84, predict loss = 0.19 (67.1 examples/sec; 0.060 sec/batch; 98h:57m:58s remains)
INFO - root - 2019-11-03 23:23:34.199566: step 22530, total loss = 0.76, predict loss = 0.18 (63.0 examples/sec; 0.063 sec/batch; 105h:21m:25s remains)
INFO - root - 2019-11-03 23:23:34.891027: step 22540, total loss = 0.75, predict loss = 0.18 (59.4 examples/sec; 0.067 sec/batch; 111h:43m:45s remains)
INFO - root - 2019-11-03 23:23:35.558505: step 22550, total loss = 0.73, predict loss = 0.16 (73.4 examples/sec; 0.055 sec/batch; 90h:32m:05s remains)
INFO - root - 2019-11-03 23:23:36.152284: step 22560, total loss = 0.68, predict loss = 0.15 (73.4 examples/sec; 0.055 sec/batch; 90h:30m:01s remains)
INFO - root - 2019-11-03 23:23:36.786796: step 22570, total loss = 0.67, predict loss = 0.16 (67.4 examples/sec; 0.059 sec/batch; 98h:35m:10s remains)
INFO - root - 2019-11-03 23:23:37.415985: step 22580, total loss = 0.60, predict loss = 0.14 (62.6 examples/sec; 0.064 sec/batch; 106h:03m:45s remains)
INFO - root - 2019-11-03 23:23:38.057085: step 22590, total loss = 0.65, predict loss = 0.16 (69.8 examples/sec; 0.057 sec/batch; 95h:12m:07s remains)
INFO - root - 2019-11-03 23:23:38.683132: step 22600, total loss = 0.75, predict loss = 0.16 (74.1 examples/sec; 0.054 sec/batch; 89h:36m:13s remains)
INFO - root - 2019-11-03 23:23:39.308275: step 22610, total loss = 1.05, predict loss = 0.28 (67.4 examples/sec; 0.059 sec/batch; 98h:35m:02s remains)
INFO - root - 2019-11-03 23:23:39.968682: step 22620, total loss = 0.71, predict loss = 0.17 (67.1 examples/sec; 0.060 sec/batch; 98h:56m:26s remains)
INFO - root - 2019-11-03 23:23:40.636272: step 22630, total loss = 0.58, predict loss = 0.14 (64.7 examples/sec; 0.062 sec/batch; 102h:37m:35s remains)
INFO - root - 2019-11-03 23:23:41.273367: step 22640, total loss = 0.75, predict loss = 0.17 (65.4 examples/sec; 0.061 sec/batch; 101h:33m:30s remains)
INFO - root - 2019-11-03 23:23:41.921722: step 22650, total loss = 0.53, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 99h:02m:24s remains)
INFO - root - 2019-11-03 23:23:42.579983: step 22660, total loss = 0.50, predict loss = 0.10 (68.0 examples/sec; 0.059 sec/batch; 97h:41m:01s remains)
INFO - root - 2019-11-03 23:23:43.209968: step 22670, total loss = 0.68, predict loss = 0.16 (69.6 examples/sec; 0.057 sec/batch; 95h:27m:05s remains)
INFO - root - 2019-11-03 23:23:43.852182: step 22680, total loss = 0.41, predict loss = 0.08 (67.1 examples/sec; 0.060 sec/batch; 98h:55m:22s remains)
INFO - root - 2019-11-03 23:23:44.513979: step 22690, total loss = 0.36, predict loss = 0.07 (79.7 examples/sec; 0.050 sec/batch; 83h:18m:49s remains)
INFO - root - 2019-11-03 23:23:45.130359: step 22700, total loss = 0.40, predict loss = 0.09 (70.0 examples/sec; 0.057 sec/batch; 94h:55m:01s remains)
INFO - root - 2019-11-03 23:23:46.301407: step 22710, total loss = 0.55, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 97h:46m:26s remains)
INFO - root - 2019-11-03 23:23:47.014661: step 22720, total loss = 0.40, predict loss = 0.07 (75.5 examples/sec; 0.053 sec/batch; 87h:59m:24s remains)
INFO - root - 2019-11-03 23:23:47.629360: step 22730, total loss = 0.64, predict loss = 0.16 (65.9 examples/sec; 0.061 sec/batch; 100h:47m:19s remains)
INFO - root - 2019-11-03 23:23:48.286050: step 22740, total loss = 0.55, predict loss = 0.11 (65.5 examples/sec; 0.061 sec/batch; 101h:21m:34s remains)
INFO - root - 2019-11-03 23:23:48.935068: step 22750, total loss = 0.52, predict loss = 0.12 (84.1 examples/sec; 0.048 sec/batch; 78h:58m:42s remains)
INFO - root - 2019-11-03 23:23:49.545425: step 22760, total loss = 0.46, predict loss = 0.09 (74.0 examples/sec; 0.054 sec/batch; 89h:47m:34s remains)
INFO - root - 2019-11-03 23:23:50.179348: step 22770, total loss = 0.68, predict loss = 0.17 (76.8 examples/sec; 0.052 sec/batch; 86h:29m:10s remains)
INFO - root - 2019-11-03 23:23:50.846567: step 22780, total loss = 0.66, predict loss = 0.15 (60.2 examples/sec; 0.066 sec/batch; 110h:20m:31s remains)
INFO - root - 2019-11-03 23:23:51.517267: step 22790, total loss = 0.68, predict loss = 0.17 (74.7 examples/sec; 0.054 sec/batch; 88h:51m:28s remains)
INFO - root - 2019-11-03 23:23:52.143678: step 22800, total loss = 0.71, predict loss = 0.15 (76.7 examples/sec; 0.052 sec/batch; 86h:33m:38s remains)
INFO - root - 2019-11-03 23:23:52.747628: step 22810, total loss = 0.77, predict loss = 0.17 (71.2 examples/sec; 0.056 sec/batch; 93h:20m:22s remains)
INFO - root - 2019-11-03 23:23:53.398917: step 22820, total loss = 0.73, predict loss = 0.17 (60.5 examples/sec; 0.066 sec/batch; 109h:45m:32s remains)
INFO - root - 2019-11-03 23:23:54.013870: step 22830, total loss = 0.82, predict loss = 0.20 (71.2 examples/sec; 0.056 sec/batch; 93h:18m:27s remains)
INFO - root - 2019-11-03 23:23:54.614375: step 22840, total loss = 0.82, predict loss = 0.19 (73.3 examples/sec; 0.055 sec/batch; 90h:38m:28s remains)
INFO - root - 2019-11-03 23:23:55.241979: step 22850, total loss = 0.81, predict loss = 0.19 (66.1 examples/sec; 0.060 sec/batch; 100h:25m:44s remains)
INFO - root - 2019-11-03 23:23:55.903332: step 22860, total loss = 0.78, predict loss = 0.20 (66.7 examples/sec; 0.060 sec/batch; 99h:34m:07s remains)
INFO - root - 2019-11-03 23:23:56.557378: step 22870, total loss = 0.50, predict loss = 0.11 (63.4 examples/sec; 0.063 sec/batch; 104h:43m:29s remains)
INFO - root - 2019-11-03 23:23:57.206084: step 22880, total loss = 0.53, predict loss = 0.12 (74.6 examples/sec; 0.054 sec/batch; 88h:59m:25s remains)
INFO - root - 2019-11-03 23:23:57.818055: step 22890, total loss = 0.66, predict loss = 0.16 (69.0 examples/sec; 0.058 sec/batch; 96h:15m:24s remains)
INFO - root - 2019-11-03 23:23:58.465833: step 22900, total loss = 0.50, predict loss = 0.11 (70.6 examples/sec; 0.057 sec/batch; 94h:05m:26s remains)
INFO - root - 2019-11-03 23:23:59.113521: step 22910, total loss = 0.55, predict loss = 0.12 (71.6 examples/sec; 0.056 sec/batch; 92h:45m:36s remains)
INFO - root - 2019-11-03 23:23:59.801888: step 22920, total loss = 0.44, predict loss = 0.09 (67.6 examples/sec; 0.059 sec/batch; 98h:14m:15s remains)
INFO - root - 2019-11-03 23:24:00.512443: step 22930, total loss = 0.34, predict loss = 0.06 (55.7 examples/sec; 0.072 sec/batch; 119h:11m:46s remains)
INFO - root - 2019-11-03 23:24:01.196420: step 22940, total loss = 0.73, predict loss = 0.18 (71.6 examples/sec; 0.056 sec/batch; 92h:47m:04s remains)
INFO - root - 2019-11-03 23:24:01.830879: step 22950, total loss = 0.72, predict loss = 0.17 (73.5 examples/sec; 0.054 sec/batch; 90h:22m:46s remains)
INFO - root - 2019-11-03 23:24:02.459890: step 22960, total loss = 0.41, predict loss = 0.08 (64.2 examples/sec; 0.062 sec/batch; 103h:22m:08s remains)
INFO - root - 2019-11-03 23:24:03.093023: step 22970, total loss = 0.74, predict loss = 0.17 (74.8 examples/sec; 0.053 sec/batch; 88h:44m:10s remains)
INFO - root - 2019-11-03 23:24:03.717204: step 22980, total loss = 0.62, predict loss = 0.16 (68.3 examples/sec; 0.059 sec/batch; 97h:11m:12s remains)
INFO - root - 2019-11-03 23:24:04.350133: step 22990, total loss = 0.62, predict loss = 0.14 (60.9 examples/sec; 0.066 sec/batch; 109h:07m:08s remains)
INFO - root - 2019-11-03 23:24:05.054694: step 23000, total loss = 0.69, predict loss = 0.15 (64.5 examples/sec; 0.062 sec/batch; 102h:54m:23s remains)
INFO - root - 2019-11-03 23:24:05.682335: step 23010, total loss = 0.73, predict loss = 0.16 (70.4 examples/sec; 0.057 sec/batch; 94h:17m:41s remains)
INFO - root - 2019-11-03 23:24:06.303685: step 23020, total loss = 0.51, predict loss = 0.12 (77.3 examples/sec; 0.052 sec/batch; 85h:53m:32s remains)
INFO - root - 2019-11-03 23:24:06.934766: step 23030, total loss = 0.60, predict loss = 0.15 (72.9 examples/sec; 0.055 sec/batch; 91h:08m:28s remains)
INFO - root - 2019-11-03 23:24:07.552548: step 23040, total loss = 0.59, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 87h:31m:44s remains)
INFO - root - 2019-11-03 23:24:08.204112: step 23050, total loss = 0.63, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 96h:02m:05s remains)
INFO - root - 2019-11-03 23:24:08.840635: step 23060, total loss = 0.62, predict loss = 0.14 (72.9 examples/sec; 0.055 sec/batch; 91h:02m:58s remains)
INFO - root - 2019-11-03 23:24:09.454528: step 23070, total loss = 0.68, predict loss = 0.15 (78.0 examples/sec; 0.051 sec/batch; 85h:07m:47s remains)
INFO - root - 2019-11-03 23:24:10.065654: step 23080, total loss = 0.60, predict loss = 0.14 (72.8 examples/sec; 0.055 sec/batch; 91h:14m:44s remains)
INFO - root - 2019-11-03 23:24:10.694402: step 23090, total loss = 0.88, predict loss = 0.19 (78.1 examples/sec; 0.051 sec/batch; 85h:01m:12s remains)
INFO - root - 2019-11-03 23:24:11.359437: step 23100, total loss = 0.83, predict loss = 0.19 (71.1 examples/sec; 0.056 sec/batch; 93h:27m:07s remains)
INFO - root - 2019-11-03 23:24:12.016625: step 23110, total loss = 0.63, predict loss = 0.14 (80.7 examples/sec; 0.050 sec/batch; 82h:20m:27s remains)
INFO - root - 2019-11-03 23:24:12.686981: step 23120, total loss = 0.77, predict loss = 0.18 (74.5 examples/sec; 0.054 sec/batch; 89h:08m:32s remains)
INFO - root - 2019-11-03 23:24:13.385564: step 23130, total loss = 0.82, predict loss = 0.19 (67.3 examples/sec; 0.059 sec/batch; 98h:42m:40s remains)
INFO - root - 2019-11-03 23:24:14.013825: step 23140, total loss = 0.81, predict loss = 0.18 (69.1 examples/sec; 0.058 sec/batch; 96h:06m:42s remains)
INFO - root - 2019-11-03 23:24:14.658704: step 23150, total loss = 0.64, predict loss = 0.14 (73.1 examples/sec; 0.055 sec/batch; 90h:52m:01s remains)
INFO - root - 2019-11-03 23:24:15.313543: step 23160, total loss = 0.55, predict loss = 0.13 (63.1 examples/sec; 0.063 sec/batch; 105h:12m:30s remains)
INFO - root - 2019-11-03 23:24:15.980418: step 23170, total loss = 0.57, predict loss = 0.13 (66.5 examples/sec; 0.060 sec/batch; 99h:47m:47s remains)
INFO - root - 2019-11-03 23:24:16.620616: step 23180, total loss = 0.81, predict loss = 0.20 (63.9 examples/sec; 0.063 sec/batch; 103h:56m:31s remains)
INFO - root - 2019-11-03 23:24:17.243894: step 23190, total loss = 0.52, predict loss = 0.12 (67.2 examples/sec; 0.060 sec/batch; 98h:48m:35s remains)
INFO - root - 2019-11-03 23:24:17.887056: step 23200, total loss = 0.65, predict loss = 0.16 (73.1 examples/sec; 0.055 sec/batch; 90h:52m:29s remains)
INFO - root - 2019-11-03 23:24:18.537643: step 23210, total loss = 0.73, predict loss = 0.17 (62.8 examples/sec; 0.064 sec/batch; 105h:42m:11s remains)
INFO - root - 2019-11-03 23:24:19.188495: step 23220, total loss = 0.57, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 91h:31m:54s remains)
INFO - root - 2019-11-03 23:24:19.807028: step 23230, total loss = 0.84, predict loss = 0.19 (74.1 examples/sec; 0.054 sec/batch; 89h:37m:42s remains)
INFO - root - 2019-11-03 23:24:20.435914: step 23240, total loss = 0.97, predict loss = 0.22 (69.0 examples/sec; 0.058 sec/batch; 96h:11m:09s remains)
INFO - root - 2019-11-03 23:24:21.077825: step 23250, total loss = 0.87, predict loss = 0.21 (65.9 examples/sec; 0.061 sec/batch; 100h:50m:26s remains)
INFO - root - 2019-11-03 23:24:21.756234: step 23260, total loss = 1.23, predict loss = 0.27 (64.7 examples/sec; 0.062 sec/batch; 102h:39m:09s remains)
INFO - root - 2019-11-03 23:24:22.433333: step 23270, total loss = 1.11, predict loss = 0.26 (67.6 examples/sec; 0.059 sec/batch; 98h:11m:19s remains)
INFO - root - 2019-11-03 23:24:23.034269: step 23280, total loss = 1.37, predict loss = 0.31 (74.5 examples/sec; 0.054 sec/batch; 89h:09m:58s remains)
INFO - root - 2019-11-03 23:24:23.628424: step 23290, total loss = 1.01, predict loss = 0.23 (70.5 examples/sec; 0.057 sec/batch; 94h:15m:23s remains)
INFO - root - 2019-11-03 23:24:24.228242: step 23300, total loss = 1.16, predict loss = 0.31 (81.4 examples/sec; 0.049 sec/batch; 81h:33m:59s remains)
INFO - root - 2019-11-03 23:24:24.831057: step 23310, total loss = 0.82, predict loss = 0.18 (77.6 examples/sec; 0.052 sec/batch; 85h:31m:42s remains)
INFO - root - 2019-11-03 23:24:25.484980: step 23320, total loss = 0.71, predict loss = 0.16 (65.3 examples/sec; 0.061 sec/batch; 101h:43m:28s remains)
INFO - root - 2019-11-03 23:24:26.084357: step 23330, total loss = 0.96, predict loss = 0.23 (72.6 examples/sec; 0.055 sec/batch; 91h:31m:30s remains)
INFO - root - 2019-11-03 23:24:26.702508: step 23340, total loss = 0.73, predict loss = 0.17 (68.6 examples/sec; 0.058 sec/batch; 96h:48m:04s remains)
INFO - root - 2019-11-03 23:24:27.347005: step 23350, total loss = 0.69, predict loss = 0.14 (70.8 examples/sec; 0.056 sec/batch; 93h:47m:33s remains)
INFO - root - 2019-11-03 23:24:27.979333: step 23360, total loss = 0.70, predict loss = 0.15 (68.9 examples/sec; 0.058 sec/batch; 96h:19m:59s remains)
INFO - root - 2019-11-03 23:24:28.628278: step 23370, total loss = 0.73, predict loss = 0.17 (67.0 examples/sec; 0.060 sec/batch; 99h:05m:30s remains)
INFO - root - 2019-11-03 23:24:29.280709: step 23380, total loss = 0.94, predict loss = 0.22 (62.8 examples/sec; 0.064 sec/batch; 105h:47m:00s remains)
INFO - root - 2019-11-03 23:24:29.927281: step 23390, total loss = 0.61, predict loss = 0.15 (77.9 examples/sec; 0.051 sec/batch; 85h:17m:39s remains)
INFO - root - 2019-11-03 23:24:30.685656: step 23400, total loss = 0.83, predict loss = 0.19 (59.1 examples/sec; 0.068 sec/batch; 112h:21m:14s remains)
INFO - root - 2019-11-03 23:24:31.327982: step 23410, total loss = 0.60, predict loss = 0.14 (70.2 examples/sec; 0.057 sec/batch; 94h:36m:22s remains)
INFO - root - 2019-11-03 23:24:31.984445: step 23420, total loss = 0.68, predict loss = 0.17 (70.4 examples/sec; 0.057 sec/batch; 94h:17m:43s remains)
INFO - root - 2019-11-03 23:24:32.606873: step 23430, total loss = 0.65, predict loss = 0.16 (74.2 examples/sec; 0.054 sec/batch; 89h:28m:15s remains)
INFO - root - 2019-11-03 23:24:33.228680: step 23440, total loss = 0.80, predict loss = 0.20 (69.2 examples/sec; 0.058 sec/batch; 96h:01m:39s remains)
INFO - root - 2019-11-03 23:24:33.861278: step 23450, total loss = 0.69, predict loss = 0.15 (73.2 examples/sec; 0.055 sec/batch; 90h:42m:37s remains)
INFO - root - 2019-11-03 23:24:34.493942: step 23460, total loss = 0.63, predict loss = 0.14 (63.1 examples/sec; 0.063 sec/batch; 105h:18m:28s remains)
INFO - root - 2019-11-03 23:24:35.126403: step 23470, total loss = 0.55, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 100h:17m:16s remains)
INFO - root - 2019-11-03 23:24:35.723760: step 23480, total loss = 0.69, predict loss = 0.14 (76.3 examples/sec; 0.052 sec/batch; 86h:59m:50s remains)
INFO - root - 2019-11-03 23:24:36.393168: step 23490, total loss = 0.62, predict loss = 0.14 (69.8 examples/sec; 0.057 sec/batch; 95h:10m:25s remains)
INFO - root - 2019-11-03 23:24:37.014305: step 23500, total loss = 0.62, predict loss = 0.15 (73.4 examples/sec; 0.054 sec/batch; 90h:27m:54s remains)
INFO - root - 2019-11-03 23:24:37.687260: step 23510, total loss = 0.42, predict loss = 0.10 (60.6 examples/sec; 0.066 sec/batch; 109h:32m:14s remains)
INFO - root - 2019-11-03 23:24:38.378328: step 23520, total loss = 0.57, predict loss = 0.13 (56.4 examples/sec; 0.071 sec/batch; 117h:50m:28s remains)
INFO - root - 2019-11-03 23:24:39.012027: step 23530, total loss = 0.53, predict loss = 0.12 (71.2 examples/sec; 0.056 sec/batch; 93h:19m:09s remains)
INFO - root - 2019-11-03 23:24:39.625469: step 23540, total loss = 0.60, predict loss = 0.14 (75.9 examples/sec; 0.053 sec/batch; 87h:27m:49s remains)
INFO - root - 2019-11-03 23:24:40.240843: step 23550, total loss = 0.51, predict loss = 0.10 (68.0 examples/sec; 0.059 sec/batch; 97h:35m:32s remains)
INFO - root - 2019-11-03 23:24:40.866122: step 23560, total loss = 0.50, predict loss = 0.11 (71.8 examples/sec; 0.056 sec/batch; 92h:26m:52s remains)
INFO - root - 2019-11-03 23:24:41.479150: step 23570, total loss = 0.65, predict loss = 0.14 (74.7 examples/sec; 0.054 sec/batch; 88h:54m:02s remains)
INFO - root - 2019-11-03 23:24:42.142658: step 23580, total loss = 0.52, predict loss = 0.12 (69.2 examples/sec; 0.058 sec/batch; 95h:59m:59s remains)
INFO - root - 2019-11-03 23:24:42.816300: step 23590, total loss = 0.73, predict loss = 0.17 (64.3 examples/sec; 0.062 sec/batch; 103h:20m:26s remains)
INFO - root - 2019-11-03 23:24:43.467900: step 23600, total loss = 0.64, predict loss = 0.15 (65.8 examples/sec; 0.061 sec/batch; 100h:54m:18s remains)
INFO - root - 2019-11-03 23:24:44.119320: step 23610, total loss = 0.72, predict loss = 0.17 (77.2 examples/sec; 0.052 sec/batch; 86h:02m:05s remains)
INFO - root - 2019-11-03 23:24:44.805739: step 23620, total loss = 0.72, predict loss = 0.17 (61.8 examples/sec; 0.065 sec/batch; 107h:23m:05s remains)
INFO - root - 2019-11-03 23:24:45.524064: step 23630, total loss = 0.74, predict loss = 0.17 (58.7 examples/sec; 0.068 sec/batch; 113h:04m:13s remains)
INFO - root - 2019-11-03 23:24:46.151670: step 23640, total loss = 0.60, predict loss = 0.13 (70.0 examples/sec; 0.057 sec/batch; 94h:54m:30s remains)
INFO - root - 2019-11-03 23:24:46.770405: step 23650, total loss = 0.68, predict loss = 0.15 (61.3 examples/sec; 0.065 sec/batch; 108h:20m:11s remains)
INFO - root - 2019-11-03 23:24:47.422750: step 23660, total loss = 0.95, predict loss = 0.23 (67.2 examples/sec; 0.060 sec/batch; 98h:47m:09s remains)
INFO - root - 2019-11-03 23:24:48.084000: step 23670, total loss = 0.89, predict loss = 0.22 (70.6 examples/sec; 0.057 sec/batch; 94h:03m:18s remains)
INFO - root - 2019-11-03 23:24:48.696468: step 23680, total loss = 0.76, predict loss = 0.18 (69.2 examples/sec; 0.058 sec/batch; 95h:56m:50s remains)
INFO - root - 2019-11-03 23:24:49.336311: step 23690, total loss = 0.91, predict loss = 0.22 (70.2 examples/sec; 0.057 sec/batch; 94h:37m:39s remains)
INFO - root - 2019-11-03 23:24:49.992543: step 23700, total loss = 0.87, predict loss = 0.20 (72.0 examples/sec; 0.056 sec/batch; 92h:10m:45s remains)
INFO - root - 2019-11-03 23:24:50.637728: step 23710, total loss = 0.88, predict loss = 0.22 (69.1 examples/sec; 0.058 sec/batch; 96h:02m:01s remains)
INFO - root - 2019-11-03 23:24:51.279360: step 23720, total loss = 0.74, predict loss = 0.17 (74.1 examples/sec; 0.054 sec/batch; 89h:40m:16s remains)
INFO - root - 2019-11-03 23:24:51.945815: step 23730, total loss = 0.77, predict loss = 0.18 (72.1 examples/sec; 0.056 sec/batch; 92h:09m:37s remains)
INFO - root - 2019-11-03 23:24:52.545822: step 23740, total loss = 0.84, predict loss = 0.21 (75.6 examples/sec; 0.053 sec/batch; 87h:47m:24s remains)
INFO - root - 2019-11-03 23:24:53.174270: step 23750, total loss = 0.69, predict loss = 0.15 (67.2 examples/sec; 0.060 sec/batch; 98h:50m:50s remains)
INFO - root - 2019-11-03 23:24:53.812227: step 23760, total loss = 0.64, predict loss = 0.14 (66.4 examples/sec; 0.060 sec/batch; 99h:59m:46s remains)
INFO - root - 2019-11-03 23:24:54.473999: step 23770, total loss = 0.76, predict loss = 0.18 (63.4 examples/sec; 0.063 sec/batch; 104h:43m:49s remains)
INFO - root - 2019-11-03 23:24:55.110223: step 23780, total loss = 0.60, predict loss = 0.14 (83.3 examples/sec; 0.048 sec/batch; 79h:43m:33s remains)
INFO - root - 2019-11-03 23:24:55.758653: step 23790, total loss = 0.56, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 99h:48m:16s remains)
INFO - root - 2019-11-03 23:24:56.384675: step 23800, total loss = 0.65, predict loss = 0.14 (80.1 examples/sec; 0.050 sec/batch; 82h:56m:54s remains)
INFO - root - 2019-11-03 23:24:57.035237: step 23810, total loss = 0.66, predict loss = 0.14 (65.8 examples/sec; 0.061 sec/batch; 100h:59m:12s remains)
INFO - root - 2019-11-03 23:24:57.660502: step 23820, total loss = 0.59, predict loss = 0.13 (72.3 examples/sec; 0.055 sec/batch; 91h:50m:08s remains)
INFO - root - 2019-11-03 23:24:58.294355: step 23830, total loss = 0.56, predict loss = 0.13 (70.9 examples/sec; 0.056 sec/batch; 93h:38m:03s remains)
INFO - root - 2019-11-03 23:24:58.931308: step 23840, total loss = 0.62, predict loss = 0.15 (64.9 examples/sec; 0.062 sec/batch; 102h:19m:16s remains)
INFO - root - 2019-11-03 23:24:59.550568: step 23850, total loss = 0.64, predict loss = 0.15 (79.0 examples/sec; 0.051 sec/batch; 84h:02m:47s remains)
INFO - root - 2019-11-03 23:25:00.187511: step 23860, total loss = 0.58, predict loss = 0.12 (74.0 examples/sec; 0.054 sec/batch; 89h:42m:46s remains)
INFO - root - 2019-11-03 23:25:00.924380: step 23870, total loss = 0.84, predict loss = 0.20 (73.8 examples/sec; 0.054 sec/batch; 90h:01m:42s remains)
INFO - root - 2019-11-03 23:25:01.535126: step 23880, total loss = 0.68, predict loss = 0.15 (67.4 examples/sec; 0.059 sec/batch; 98h:34m:43s remains)
INFO - root - 2019-11-03 23:25:02.156061: step 23890, total loss = 0.71, predict loss = 0.17 (66.5 examples/sec; 0.060 sec/batch; 99h:48m:35s remains)
INFO - root - 2019-11-03 23:25:02.794977: step 23900, total loss = 0.70, predict loss = 0.17 (75.0 examples/sec; 0.053 sec/batch; 88h:31m:18s remains)
INFO - root - 2019-11-03 23:25:03.425374: step 23910, total loss = 0.81, predict loss = 0.19 (70.5 examples/sec; 0.057 sec/batch; 94h:13m:31s remains)
INFO - root - 2019-11-03 23:25:04.022597: step 23920, total loss = 0.80, predict loss = 0.19 (79.8 examples/sec; 0.050 sec/batch; 83h:09m:26s remains)
INFO - root - 2019-11-03 23:25:04.652416: step 23930, total loss = 0.81, predict loss = 0.19 (62.3 examples/sec; 0.064 sec/batch; 106h:33m:04s remains)
INFO - root - 2019-11-03 23:25:05.297176: step 23940, total loss = 0.82, predict loss = 0.22 (66.9 examples/sec; 0.060 sec/batch; 99h:17m:36s remains)
INFO - root - 2019-11-03 23:25:05.914449: step 23950, total loss = 0.74, predict loss = 0.18 (63.2 examples/sec; 0.063 sec/batch; 105h:05m:27s remains)
INFO - root - 2019-11-03 23:25:06.529588: step 23960, total loss = 0.63, predict loss = 0.15 (71.9 examples/sec; 0.056 sec/batch; 92h:21m:51s remains)
INFO - root - 2019-11-03 23:25:07.140794: step 23970, total loss = 0.83, predict loss = 0.21 (73.9 examples/sec; 0.054 sec/batch; 89h:51m:28s remains)
INFO - root - 2019-11-03 23:25:07.725044: step 23980, total loss = 0.64, predict loss = 0.16 (77.3 examples/sec; 0.052 sec/batch; 85h:54m:55s remains)
INFO - root - 2019-11-03 23:25:08.333571: step 23990, total loss = 0.62, predict loss = 0.14 (81.4 examples/sec; 0.049 sec/batch; 81h:34m:49s remains)
INFO - root - 2019-11-03 23:25:08.930475: step 24000, total loss = 0.66, predict loss = 0.16 (72.5 examples/sec; 0.055 sec/batch; 91h:34m:53s remains)
INFO - root - 2019-11-03 23:25:09.629026: step 24010, total loss = 0.65, predict loss = 0.14 (59.6 examples/sec; 0.067 sec/batch; 111h:25m:50s remains)
INFO - root - 2019-11-03 23:25:10.283146: step 24020, total loss = 0.64, predict loss = 0.14 (74.0 examples/sec; 0.054 sec/batch; 89h:45m:19s remains)
INFO - root - 2019-11-03 23:25:10.939334: step 24030, total loss = 0.75, predict loss = 0.17 (75.5 examples/sec; 0.053 sec/batch; 87h:58m:16s remains)
INFO - root - 2019-11-03 23:25:11.588179: step 24040, total loss = 0.57, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 92h:58m:09s remains)
INFO - root - 2019-11-03 23:25:12.203309: step 24050, total loss = 0.60, predict loss = 0.14 (69.9 examples/sec; 0.057 sec/batch; 95h:00m:52s remains)
INFO - root - 2019-11-03 23:25:12.827078: step 24060, total loss = 0.61, predict loss = 0.14 (72.2 examples/sec; 0.055 sec/batch; 91h:59m:13s remains)
INFO - root - 2019-11-03 23:25:13.445502: step 24070, total loss = 0.64, predict loss = 0.14 (73.1 examples/sec; 0.055 sec/batch; 90h:48m:48s remains)
INFO - root - 2019-11-03 23:25:14.055254: step 24080, total loss = 0.54, predict loss = 0.12 (76.8 examples/sec; 0.052 sec/batch; 86h:28m:06s remains)
INFO - root - 2019-11-03 23:25:14.674064: step 24090, total loss = 0.67, predict loss = 0.16 (67.9 examples/sec; 0.059 sec/batch; 97h:47m:57s remains)
INFO - root - 2019-11-03 23:25:15.310612: step 24100, total loss = 0.58, predict loss = 0.14 (69.2 examples/sec; 0.058 sec/batch; 95h:59m:51s remains)
INFO - root - 2019-11-03 23:25:15.905784: step 24110, total loss = 0.65, predict loss = 0.15 (79.9 examples/sec; 0.050 sec/batch; 83h:04m:02s remains)
INFO - root - 2019-11-03 23:25:16.526276: step 24120, total loss = 0.61, predict loss = 0.15 (77.9 examples/sec; 0.051 sec/batch; 85h:13m:18s remains)
INFO - root - 2019-11-03 23:25:17.167817: step 24130, total loss = 0.62, predict loss = 0.14 (65.6 examples/sec; 0.061 sec/batch; 101h:13m:53s remains)
INFO - root - 2019-11-03 23:25:17.779972: step 24140, total loss = 0.63, predict loss = 0.15 (75.5 examples/sec; 0.053 sec/batch; 87h:58m:54s remains)
INFO - root - 2019-11-03 23:25:18.378336: step 24150, total loss = 0.60, predict loss = 0.14 (67.3 examples/sec; 0.059 sec/batch; 98h:40m:55s remains)
INFO - root - 2019-11-03 23:25:19.006826: step 24160, total loss = 0.75, predict loss = 0.16 (65.3 examples/sec; 0.061 sec/batch; 101h:44m:22s remains)
INFO - root - 2019-11-03 23:25:19.634786: step 24170, total loss = 0.69, predict loss = 0.16 (72.1 examples/sec; 0.055 sec/batch; 92h:04m:43s remains)
INFO - root - 2019-11-03 23:25:20.275086: step 24180, total loss = 0.57, predict loss = 0.14 (66.5 examples/sec; 0.060 sec/batch; 99h:46m:34s remains)
INFO - root - 2019-11-03 23:25:20.899134: step 24190, total loss = 0.65, predict loss = 0.15 (68.3 examples/sec; 0.059 sec/batch; 97h:10m:57s remains)
INFO - root - 2019-11-03 23:25:21.564157: step 24200, total loss = 0.85, predict loss = 0.20 (69.0 examples/sec; 0.058 sec/batch; 96h:15m:51s remains)
INFO - root - 2019-11-03 23:25:22.189614: step 24210, total loss = 0.73, predict loss = 0.17 (63.2 examples/sec; 0.063 sec/batch; 104h:59m:26s remains)
INFO - root - 2019-11-03 23:25:22.825334: step 24220, total loss = 0.78, predict loss = 0.17 (71.5 examples/sec; 0.056 sec/batch; 92h:50m:33s remains)
INFO - root - 2019-11-03 23:25:23.467590: step 24230, total loss = 0.64, predict loss = 0.15 (64.7 examples/sec; 0.062 sec/batch; 102h:37m:33s remains)
INFO - root - 2019-11-03 23:25:24.139870: step 24240, total loss = 0.68, predict loss = 0.15 (68.9 examples/sec; 0.058 sec/batch; 96h:26m:03s remains)
INFO - root - 2019-11-03 23:25:24.783637: step 24250, total loss = 0.86, predict loss = 0.21 (65.6 examples/sec; 0.061 sec/batch; 101h:14m:21s remains)
INFO - root - 2019-11-03 23:25:25.434669: step 24260, total loss = 0.54, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 100h:49m:07s remains)
INFO - root - 2019-11-03 23:25:26.187081: step 24270, total loss = 0.65, predict loss = 0.15 (56.3 examples/sec; 0.071 sec/batch; 118h:01m:10s remains)
INFO - root - 2019-11-03 23:25:26.822847: step 24280, total loss = 0.73, predict loss = 0.17 (68.6 examples/sec; 0.058 sec/batch; 96h:44m:10s remains)
INFO - root - 2019-11-03 23:25:27.447882: step 24290, total loss = 0.68, predict loss = 0.17 (65.0 examples/sec; 0.062 sec/batch; 102h:06m:26s remains)
INFO - root - 2019-11-03 23:25:28.081563: step 24300, total loss = 0.78, predict loss = 0.18 (66.4 examples/sec; 0.060 sec/batch; 99h:57m:02s remains)
INFO - root - 2019-11-03 23:25:28.696450: step 24310, total loss = 0.84, predict loss = 0.20 (73.5 examples/sec; 0.054 sec/batch; 90h:22m:43s remains)
INFO - root - 2019-11-03 23:25:29.287940: step 24320, total loss = 0.73, predict loss = 0.18 (83.3 examples/sec; 0.048 sec/batch; 79h:42m:49s remains)
INFO - root - 2019-11-03 23:25:29.931610: step 24330, total loss = 0.89, predict loss = 0.21 (75.1 examples/sec; 0.053 sec/batch; 88h:22m:00s remains)
INFO - root - 2019-11-03 23:25:30.614264: step 24340, total loss = 0.87, predict loss = 0.21 (59.6 examples/sec; 0.067 sec/batch; 111h:25m:39s remains)
INFO - root - 2019-11-03 23:25:31.227999: step 24350, total loss = 0.84, predict loss = 0.19 (83.7 examples/sec; 0.048 sec/batch; 79h:20m:59s remains)
INFO - root - 2019-11-03 23:25:31.828337: step 24360, total loss = 0.93, predict loss = 0.22 (76.8 examples/sec; 0.052 sec/batch; 86h:29m:50s remains)
INFO - root - 2019-11-03 23:25:32.480646: step 24370, total loss = 0.84, predict loss = 0.21 (73.4 examples/sec; 0.055 sec/batch; 90h:29m:16s remains)
INFO - root - 2019-11-03 23:25:33.167093: step 24380, total loss = 0.74, predict loss = 0.17 (62.6 examples/sec; 0.064 sec/batch; 106h:06m:20s remains)
INFO - root - 2019-11-03 23:25:33.860136: step 24390, total loss = 0.76, predict loss = 0.18 (65.7 examples/sec; 0.061 sec/batch; 101h:03m:24s remains)
INFO - root - 2019-11-03 23:25:34.554120: step 24400, total loss = 0.61, predict loss = 0.14 (61.3 examples/sec; 0.065 sec/batch; 108h:16m:44s remains)
INFO - root - 2019-11-03 23:25:35.246286: step 24410, total loss = 0.55, predict loss = 0.13 (65.8 examples/sec; 0.061 sec/batch; 100h:49m:52s remains)
INFO - root - 2019-11-03 23:25:35.972089: step 24420, total loss = 0.54, predict loss = 0.12 (58.1 examples/sec; 0.069 sec/batch; 114h:15m:46s remains)
INFO - root - 2019-11-03 23:25:36.639616: step 24430, total loss = 0.92, predict loss = 0.21 (70.4 examples/sec; 0.057 sec/batch; 94h:18m:25s remains)
INFO - root - 2019-11-03 23:25:37.276922: step 24440, total loss = 0.69, predict loss = 0.17 (68.5 examples/sec; 0.058 sec/batch; 96h:52m:27s remains)
INFO - root - 2019-11-03 23:25:37.962594: step 24450, total loss = 0.83, predict loss = 0.20 (61.6 examples/sec; 0.065 sec/batch; 107h:47m:27s remains)
INFO - root - 2019-11-03 23:25:38.604554: step 24460, total loss = 0.90, predict loss = 0.22 (69.2 examples/sec; 0.058 sec/batch; 95h:52m:58s remains)
INFO - root - 2019-11-03 23:25:39.274035: step 24470, total loss = 0.73, predict loss = 0.17 (66.9 examples/sec; 0.060 sec/batch; 99h:18m:00s remains)
INFO - root - 2019-11-03 23:25:39.934377: step 24480, total loss = 0.72, predict loss = 0.16 (68.0 examples/sec; 0.059 sec/batch; 97h:34m:54s remains)
INFO - root - 2019-11-03 23:25:40.580306: step 24490, total loss = 0.74, predict loss = 0.17 (69.6 examples/sec; 0.057 sec/batch; 95h:22m:18s remains)
INFO - root - 2019-11-03 23:25:41.218383: step 24500, total loss = 0.81, predict loss = 0.18 (74.6 examples/sec; 0.054 sec/batch; 89h:02m:16s remains)
INFO - root - 2019-11-03 23:25:41.848558: step 24510, total loss = 0.61, predict loss = 0.14 (75.8 examples/sec; 0.053 sec/batch; 87h:36m:54s remains)
INFO - root - 2019-11-03 23:25:42.489910: step 24520, total loss = 0.72, predict loss = 0.17 (67.6 examples/sec; 0.059 sec/batch; 98h:17m:17s remains)
INFO - root - 2019-11-03 23:25:43.001785: step 24530, total loss = 0.74, predict loss = 0.16 (96.8 examples/sec; 0.041 sec/batch; 68h:36m:48s remains)
INFO - root - 2019-11-03 23:25:43.470596: step 24540, total loss = 0.75, predict loss = 0.17 (100.0 examples/sec; 0.040 sec/batch; 66h:24m:10s remains)
INFO - root - 2019-11-03 23:25:44.526489: step 24550, total loss = 0.47, predict loss = 0.10 (71.3 examples/sec; 0.056 sec/batch; 93h:08m:03s remains)
INFO - root - 2019-11-03 23:25:45.180274: step 24560, total loss = 0.69, predict loss = 0.15 (79.4 examples/sec; 0.050 sec/batch; 83h:37m:51s remains)
INFO - root - 2019-11-03 23:25:45.849414: step 24570, total loss = 0.60, predict loss = 0.14 (65.9 examples/sec; 0.061 sec/batch; 100h:44m:06s remains)
INFO - root - 2019-11-03 23:25:46.497919: step 24580, total loss = 0.54, predict loss = 0.12 (68.5 examples/sec; 0.058 sec/batch; 96h:57m:49s remains)
INFO - root - 2019-11-03 23:25:47.140446: step 24590, total loss = 0.72, predict loss = 0.17 (76.1 examples/sec; 0.053 sec/batch; 87h:12m:09s remains)
INFO - root - 2019-11-03 23:25:47.761747: step 24600, total loss = 0.74, predict loss = 0.16 (73.7 examples/sec; 0.054 sec/batch; 90h:05m:13s remains)
INFO - root - 2019-11-03 23:25:48.384049: step 24610, total loss = 0.76, predict loss = 0.18 (76.8 examples/sec; 0.052 sec/batch; 86h:28m:16s remains)
INFO - root - 2019-11-03 23:25:49.047568: step 24620, total loss = 0.77, predict loss = 0.16 (65.1 examples/sec; 0.061 sec/batch; 101h:56m:57s remains)
INFO - root - 2019-11-03 23:25:49.723484: step 24630, total loss = 1.07, predict loss = 0.24 (66.5 examples/sec; 0.060 sec/batch; 99h:46m:49s remains)
INFO - root - 2019-11-03 23:25:50.365862: step 24640, total loss = 0.89, predict loss = 0.20 (73.9 examples/sec; 0.054 sec/batch; 89h:53m:20s remains)
INFO - root - 2019-11-03 23:25:50.995217: step 24650, total loss = 0.86, predict loss = 0.20 (78.8 examples/sec; 0.051 sec/batch; 84h:16m:13s remains)
INFO - root - 2019-11-03 23:25:51.601616: step 24660, total loss = 0.82, predict loss = 0.18 (68.3 examples/sec; 0.059 sec/batch; 97h:08m:15s remains)
INFO - root - 2019-11-03 23:25:52.220656: step 24670, total loss = 0.87, predict loss = 0.18 (71.9 examples/sec; 0.056 sec/batch; 92h:22m:29s remains)
INFO - root - 2019-11-03 23:25:52.827664: step 24680, total loss = 0.62, predict loss = 0.15 (71.8 examples/sec; 0.056 sec/batch; 92h:29m:09s remains)
INFO - root - 2019-11-03 23:25:53.457917: step 24690, total loss = 0.83, predict loss = 0.17 (70.7 examples/sec; 0.057 sec/batch; 93h:57m:52s remains)
INFO - root - 2019-11-03 23:25:54.116050: step 24700, total loss = 0.73, predict loss = 0.17 (63.6 examples/sec; 0.063 sec/batch; 104h:22m:42s remains)
INFO - root - 2019-11-03 23:25:54.797393: step 24710, total loss = 0.89, predict loss = 0.21 (60.1 examples/sec; 0.067 sec/batch; 110h:24m:53s remains)
INFO - root - 2019-11-03 23:25:55.447020: step 24720, total loss = 0.82, predict loss = 0.17 (70.0 examples/sec; 0.057 sec/batch; 94h:50m:33s remains)
INFO - root - 2019-11-03 23:25:56.108237: step 24730, total loss = 0.54, predict loss = 0.12 (71.1 examples/sec; 0.056 sec/batch; 93h:19m:56s remains)
INFO - root - 2019-11-03 23:25:56.748705: step 24740, total loss = 0.59, predict loss = 0.14 (71.9 examples/sec; 0.056 sec/batch; 92h:18m:33s remains)
INFO - root - 2019-11-03 23:25:57.395065: step 24750, total loss = 0.66, predict loss = 0.15 (62.1 examples/sec; 0.064 sec/batch; 106h:58m:10s remains)
INFO - root - 2019-11-03 23:25:58.033481: step 24760, total loss = 0.81, predict loss = 0.19 (71.2 examples/sec; 0.056 sec/batch; 93h:10m:59s remains)
INFO - root - 2019-11-03 23:25:58.660572: step 24770, total loss = 0.72, predict loss = 0.17 (66.8 examples/sec; 0.060 sec/batch; 99h:26m:19s remains)
INFO - root - 2019-11-03 23:25:59.288012: step 24780, total loss = 0.64, predict loss = 0.15 (72.7 examples/sec; 0.055 sec/batch; 91h:16m:24s remains)
INFO - root - 2019-11-03 23:25:59.906971: step 24790, total loss = 0.47, predict loss = 0.10 (67.6 examples/sec; 0.059 sec/batch; 98h:11m:07s remains)
INFO - root - 2019-11-03 23:26:00.614350: step 24800, total loss = 0.64, predict loss = 0.13 (55.6 examples/sec; 0.072 sec/batch; 119h:30m:53s remains)
INFO - root - 2019-11-03 23:26:01.245362: step 24810, total loss = 0.94, predict loss = 0.22 (72.7 examples/sec; 0.055 sec/batch; 91h:21m:45s remains)
INFO - root - 2019-11-03 23:26:01.839345: step 24820, total loss = 0.90, predict loss = 0.21 (84.3 examples/sec; 0.047 sec/batch; 78h:44m:20s remains)
INFO - root - 2019-11-03 23:26:02.455700: step 24830, total loss = 1.06, predict loss = 0.24 (73.3 examples/sec; 0.055 sec/batch; 90h:33m:23s remains)
INFO - root - 2019-11-03 23:26:03.093839: step 24840, total loss = 0.86, predict loss = 0.19 (67.3 examples/sec; 0.059 sec/batch; 98h:42m:49s remains)
INFO - root - 2019-11-03 23:26:03.717248: step 24850, total loss = 1.00, predict loss = 0.23 (69.5 examples/sec; 0.058 sec/batch; 95h:34m:57s remains)
INFO - root - 2019-11-03 23:26:04.369346: step 24860, total loss = 0.85, predict loss = 0.19 (69.3 examples/sec; 0.058 sec/batch; 95h:49m:54s remains)
INFO - root - 2019-11-03 23:26:04.957275: step 24870, total loss = 0.81, predict loss = 0.17 (69.3 examples/sec; 0.058 sec/batch; 95h:49m:46s remains)
INFO - root - 2019-11-03 23:26:05.583885: step 24880, total loss = 0.60, predict loss = 0.13 (71.6 examples/sec; 0.056 sec/batch; 92h:43m:17s remains)
INFO - root - 2019-11-03 23:26:06.250106: step 24890, total loss = 0.89, predict loss = 0.21 (65.7 examples/sec; 0.061 sec/batch; 101h:00m:46s remains)
INFO - root - 2019-11-03 23:26:06.878693: step 24900, total loss = 0.71, predict loss = 0.17 (75.1 examples/sec; 0.053 sec/batch; 88h:24m:45s remains)
INFO - root - 2019-11-03 23:26:07.541043: step 24910, total loss = 0.74, predict loss = 0.17 (74.0 examples/sec; 0.054 sec/batch; 89h:44m:37s remains)
INFO - root - 2019-11-03 23:26:08.179138: step 24920, total loss = 0.58, predict loss = 0.12 (72.6 examples/sec; 0.055 sec/batch; 91h:27m:12s remains)
INFO - root - 2019-11-03 23:26:08.807145: step 24930, total loss = 0.71, predict loss = 0.16 (76.9 examples/sec; 0.052 sec/batch; 86h:19m:03s remains)
INFO - root - 2019-11-03 23:26:09.484649: step 24940, total loss = 0.54, predict loss = 0.10 (66.6 examples/sec; 0.060 sec/batch; 99h:45m:15s remains)
INFO - root - 2019-11-03 23:26:10.061281: step 24950, total loss = 0.67, predict loss = 0.16 (73.2 examples/sec; 0.055 sec/batch; 90h:44m:17s remains)
INFO - root - 2019-11-03 23:26:10.681901: step 24960, total loss = 0.91, predict loss = 0.21 (70.9 examples/sec; 0.056 sec/batch; 93h:37m:02s remains)
INFO - root - 2019-11-03 23:26:11.295486: step 24970, total loss = 0.67, predict loss = 0.15 (70.2 examples/sec; 0.057 sec/batch; 94h:32m:12s remains)
INFO - root - 2019-11-03 23:26:11.917235: step 24980, total loss = 0.77, predict loss = 0.18 (68.8 examples/sec; 0.058 sec/batch; 96h:31m:43s remains)
INFO - root - 2019-11-03 23:26:12.555537: step 24990, total loss = 0.87, predict loss = 0.20 (65.8 examples/sec; 0.061 sec/batch; 100h:51m:20s remains)
INFO - root - 2019-11-03 23:26:13.232249: step 25000, total loss = 0.81, predict loss = 0.20 (68.5 examples/sec; 0.058 sec/batch; 96h:52m:35s remains)
INFO - root - 2019-11-03 23:26:13.867621: step 25010, total loss = 0.67, predict loss = 0.16 (75.6 examples/sec; 0.053 sec/batch; 87h:47m:27s remains)
INFO - root - 2019-11-03 23:26:14.493873: step 25020, total loss = 0.65, predict loss = 0.16 (67.5 examples/sec; 0.059 sec/batch; 98h:21m:28s remains)
INFO - root - 2019-11-03 23:26:15.123960: step 25030, total loss = 0.97, predict loss = 0.22 (67.2 examples/sec; 0.060 sec/batch; 98h:51m:01s remains)
INFO - root - 2019-11-03 23:26:15.752556: step 25040, total loss = 0.77, predict loss = 0.19 (66.2 examples/sec; 0.060 sec/batch; 100h:14m:46s remains)
INFO - root - 2019-11-03 23:26:16.347470: step 25050, total loss = 0.60, predict loss = 0.14 (67.0 examples/sec; 0.060 sec/batch; 99h:07m:02s remains)
INFO - root - 2019-11-03 23:26:16.978817: step 25060, total loss = 0.72, predict loss = 0.17 (68.9 examples/sec; 0.058 sec/batch; 96h:23m:41s remains)
INFO - root - 2019-11-03 23:26:17.609815: step 25070, total loss = 0.67, predict loss = 0.15 (70.3 examples/sec; 0.057 sec/batch; 94h:28m:56s remains)
INFO - root - 2019-11-03 23:26:18.199241: step 25080, total loss = 0.64, predict loss = 0.14 (72.3 examples/sec; 0.055 sec/batch; 91h:50m:22s remains)
INFO - root - 2019-11-03 23:26:18.852532: step 25090, total loss = 0.57, predict loss = 0.14 (64.4 examples/sec; 0.062 sec/batch; 103h:01m:49s remains)
INFO - root - 2019-11-03 23:26:19.479649: step 25100, total loss = 0.72, predict loss = 0.17 (64.8 examples/sec; 0.062 sec/batch; 102h:28m:45s remains)
INFO - root - 2019-11-03 23:26:20.079345: step 25110, total loss = 0.48, predict loss = 0.11 (86.1 examples/sec; 0.046 sec/batch; 77h:05m:54s remains)
INFO - root - 2019-11-03 23:26:20.696352: step 25120, total loss = 0.43, predict loss = 0.10 (79.5 examples/sec; 0.050 sec/batch; 83h:32m:30s remains)
INFO - root - 2019-11-03 23:26:21.374209: step 25130, total loss = 0.56, predict loss = 0.14 (61.1 examples/sec; 0.065 sec/batch; 108h:41m:15s remains)
INFO - root - 2019-11-03 23:26:22.002390: step 25140, total loss = 0.65, predict loss = 0.18 (70.4 examples/sec; 0.057 sec/batch; 94h:14m:47s remains)
INFO - root - 2019-11-03 23:26:22.641075: step 25150, total loss = 0.50, predict loss = 0.13 (74.5 examples/sec; 0.054 sec/batch; 89h:07m:05s remains)
INFO - root - 2019-11-03 23:26:23.247277: step 25160, total loss = 0.70, predict loss = 0.16 (72.9 examples/sec; 0.055 sec/batch; 91h:02m:02s remains)
INFO - root - 2019-11-03 23:26:23.867808: step 25170, total loss = 0.45, predict loss = 0.10 (69.5 examples/sec; 0.058 sec/batch; 95h:28m:33s remains)
INFO - root - 2019-11-03 23:26:24.484289: step 25180, total loss = 0.55, predict loss = 0.12 (68.9 examples/sec; 0.058 sec/batch; 96h:17m:49s remains)
INFO - root - 2019-11-03 23:26:25.078731: step 25190, total loss = 0.46, predict loss = 0.10 (82.8 examples/sec; 0.048 sec/batch; 80h:09m:43s remains)
INFO - root - 2019-11-03 23:26:25.684385: step 25200, total loss = 0.81, predict loss = 0.20 (82.4 examples/sec; 0.049 sec/batch; 80h:31m:32s remains)
INFO - root - 2019-11-03 23:26:26.323365: step 25210, total loss = 0.74, predict loss = 0.17 (71.3 examples/sec; 0.056 sec/batch; 93h:04m:53s remains)
INFO - root - 2019-11-03 23:26:26.973947: step 25220, total loss = 0.79, predict loss = 0.18 (72.4 examples/sec; 0.055 sec/batch; 91h:42m:00s remains)
INFO - root - 2019-11-03 23:26:27.622554: step 25230, total loss = 0.76, predict loss = 0.17 (63.9 examples/sec; 0.063 sec/batch; 103h:54m:46s remains)
INFO - root - 2019-11-03 23:26:28.270486: step 25240, total loss = 0.85, predict loss = 0.18 (75.9 examples/sec; 0.053 sec/batch; 87h:29m:48s remains)
INFO - root - 2019-11-03 23:26:28.917819: step 25250, total loss = 0.61, predict loss = 0.14 (68.1 examples/sec; 0.059 sec/batch; 97h:29m:58s remains)
INFO - root - 2019-11-03 23:26:29.580528: step 25260, total loss = 0.94, predict loss = 0.24 (70.5 examples/sec; 0.057 sec/batch; 94h:10m:21s remains)
INFO - root - 2019-11-03 23:26:30.249081: step 25270, total loss = 0.55, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 98h:53m:29s remains)
INFO - root - 2019-11-03 23:26:30.988003: step 25280, total loss = 0.62, predict loss = 0.13 (64.8 examples/sec; 0.062 sec/batch; 102h:30m:43s remains)
INFO - root - 2019-11-03 23:26:31.620728: step 25290, total loss = 0.65, predict loss = 0.16 (64.5 examples/sec; 0.062 sec/batch; 102h:57m:58s remains)
INFO - root - 2019-11-03 23:26:32.242872: step 25300, total loss = 0.63, predict loss = 0.15 (65.5 examples/sec; 0.061 sec/batch; 101h:17m:53s remains)
INFO - root - 2019-11-03 23:26:32.883011: step 25310, total loss = 0.80, predict loss = 0.19 (79.1 examples/sec; 0.051 sec/batch; 83h:55m:31s remains)
INFO - root - 2019-11-03 23:26:33.543857: step 25320, total loss = 0.63, predict loss = 0.16 (60.1 examples/sec; 0.067 sec/batch; 110h:26m:56s remains)
INFO - root - 2019-11-03 23:26:34.199779: step 25330, total loss = 0.67, predict loss = 0.15 (66.0 examples/sec; 0.061 sec/batch; 100h:32m:21s remains)
INFO - root - 2019-11-03 23:26:34.837240: step 25340, total loss = 0.66, predict loss = 0.17 (69.5 examples/sec; 0.058 sec/batch; 95h:33m:16s remains)
INFO - root - 2019-11-03 23:26:35.443183: step 25350, total loss = 0.49, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 99h:46m:05s remains)
INFO - root - 2019-11-03 23:26:36.186199: step 25360, total loss = 0.64, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:53m:33s remains)
INFO - root - 2019-11-03 23:26:36.830960: step 25370, total loss = 0.45, predict loss = 0.10 (73.5 examples/sec; 0.054 sec/batch; 90h:18m:53s remains)
INFO - root - 2019-11-03 23:26:37.469349: step 25380, total loss = 0.55, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 97h:53m:31s remains)
INFO - root - 2019-11-03 23:26:38.127855: step 25390, total loss = 0.47, predict loss = 0.11 (73.5 examples/sec; 0.054 sec/batch; 90h:19m:15s remains)
INFO - root - 2019-11-03 23:26:38.751112: step 25400, total loss = 0.47, predict loss = 0.10 (73.3 examples/sec; 0.055 sec/batch; 90h:34m:26s remains)
INFO - root - 2019-11-03 23:26:39.413533: step 25410, total loss = 0.36, predict loss = 0.08 (77.9 examples/sec; 0.051 sec/batch; 85h:15m:49s remains)
INFO - root - 2019-11-03 23:26:40.105620: step 25420, total loss = 0.42, predict loss = 0.10 (59.7 examples/sec; 0.067 sec/batch; 111h:13m:00s remains)
INFO - root - 2019-11-03 23:26:40.751686: step 25430, total loss = 0.54, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 95h:09m:08s remains)
INFO - root - 2019-11-03 23:26:41.434005: step 25440, total loss = 0.53, predict loss = 0.11 (65.3 examples/sec; 0.061 sec/batch; 101h:38m:44s remains)
INFO - root - 2019-11-03 23:26:42.090575: step 25450, total loss = 0.43, predict loss = 0.09 (63.8 examples/sec; 0.063 sec/batch; 104h:07m:05s remains)
INFO - root - 2019-11-03 23:26:42.733959: step 25460, total loss = 0.56, predict loss = 0.12 (66.6 examples/sec; 0.060 sec/batch; 99h:40m:11s remains)
INFO - root - 2019-11-03 23:26:43.357557: step 25470, total loss = 0.66, predict loss = 0.16 (61.7 examples/sec; 0.065 sec/batch; 107h:35m:37s remains)
INFO - root - 2019-11-03 23:26:43.997368: step 25480, total loss = 0.48, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 95h:15m:07s remains)
INFO - root - 2019-11-03 23:26:44.647951: step 25490, total loss = 0.68, predict loss = 0.15 (77.6 examples/sec; 0.052 sec/batch; 85h:32m:05s remains)
INFO - root - 2019-11-03 23:26:45.290155: step 25500, total loss = 0.77, predict loss = 0.18 (68.4 examples/sec; 0.058 sec/batch; 97h:04m:17s remains)
INFO - root - 2019-11-03 23:26:46.000217: step 25510, total loss = 0.55, predict loss = 0.13 (49.6 examples/sec; 0.081 sec/batch; 133h:48m:49s remains)
INFO - root - 2019-11-03 23:26:46.639919: step 25520, total loss = 0.60, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 95h:25m:10s remains)
INFO - root - 2019-11-03 23:26:47.290746: step 25530, total loss = 0.83, predict loss = 0.19 (67.1 examples/sec; 0.060 sec/batch; 98h:58m:02s remains)
INFO - root - 2019-11-03 23:26:47.978066: step 25540, total loss = 0.63, predict loss = 0.15 (65.8 examples/sec; 0.061 sec/batch; 100h:57m:09s remains)
INFO - root - 2019-11-03 23:26:48.593169: step 25550, total loss = 0.62, predict loss = 0.15 (74.6 examples/sec; 0.054 sec/batch; 88h:57m:10s remains)
INFO - root - 2019-11-03 23:26:49.261987: step 25560, total loss = 0.64, predict loss = 0.15 (63.9 examples/sec; 0.063 sec/batch; 103h:57m:20s remains)
INFO - root - 2019-11-03 23:26:49.914205: step 25570, total loss = 0.77, predict loss = 0.17 (67.0 examples/sec; 0.060 sec/batch; 99h:00m:21s remains)
INFO - root - 2019-11-03 23:26:50.581806: step 25580, total loss = 0.41, predict loss = 0.10 (74.4 examples/sec; 0.054 sec/batch; 89h:11m:25s remains)
INFO - root - 2019-11-03 23:26:51.227677: step 25590, total loss = 0.58, predict loss = 0.14 (65.0 examples/sec; 0.062 sec/batch; 102h:05m:56s remains)
INFO - root - 2019-11-03 23:26:51.879728: step 25600, total loss = 0.70, predict loss = 0.15 (74.0 examples/sec; 0.054 sec/batch; 89h:40m:39s remains)
INFO - root - 2019-11-03 23:26:52.529124: step 25610, total loss = 0.47, predict loss = 0.11 (70.8 examples/sec; 0.057 sec/batch; 93h:46m:15s remains)
INFO - root - 2019-11-03 23:26:53.183557: step 25620, total loss = 0.38, predict loss = 0.08 (77.3 examples/sec; 0.052 sec/batch; 85h:54m:02s remains)
INFO - root - 2019-11-03 23:26:53.789113: step 25630, total loss = 0.46, predict loss = 0.11 (71.6 examples/sec; 0.056 sec/batch; 92h:43m:08s remains)
INFO - root - 2019-11-03 23:26:54.400834: step 25640, total loss = 0.45, predict loss = 0.10 (81.6 examples/sec; 0.049 sec/batch; 81h:23m:37s remains)
INFO - root - 2019-11-03 23:26:55.030705: step 25650, total loss = 0.47, predict loss = 0.11 (66.0 examples/sec; 0.061 sec/batch; 100h:35m:32s remains)
INFO - root - 2019-11-03 23:26:55.678549: step 25660, total loss = 0.51, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 99h:47m:26s remains)
INFO - root - 2019-11-03 23:26:56.327406: step 25670, total loss = 0.76, predict loss = 0.18 (70.5 examples/sec; 0.057 sec/batch; 94h:13m:27s remains)
INFO - root - 2019-11-03 23:26:56.972157: step 25680, total loss = 0.71, predict loss = 0.19 (69.9 examples/sec; 0.057 sec/batch; 94h:56m:13s remains)
INFO - root - 2019-11-03 23:26:57.612528: step 25690, total loss = 0.81, predict loss = 0.19 (75.3 examples/sec; 0.053 sec/batch; 88h:11m:55s remains)
INFO - root - 2019-11-03 23:26:58.258664: step 25700, total loss = 0.60, predict loss = 0.14 (67.4 examples/sec; 0.059 sec/batch; 98h:28m:27s remains)
INFO - root - 2019-11-03 23:26:58.891173: step 25710, total loss = 0.59, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 93h:32m:09s remains)
INFO - root - 2019-11-03 23:26:59.548262: step 25720, total loss = 0.69, predict loss = 0.16 (70.8 examples/sec; 0.057 sec/batch; 93h:48m:53s remains)
INFO - root - 2019-11-03 23:27:00.196014: step 25730, total loss = 0.60, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 95h:20m:44s remains)
INFO - root - 2019-11-03 23:27:00.893391: step 25740, total loss = 0.51, predict loss = 0.12 (74.3 examples/sec; 0.054 sec/batch; 89h:19m:52s remains)
INFO - root - 2019-11-03 23:27:01.544955: step 25750, total loss = 0.55, predict loss = 0.14 (70.8 examples/sec; 0.056 sec/batch; 93h:43m:01s remains)
INFO - root - 2019-11-03 23:27:02.156925: step 25760, total loss = 0.60, predict loss = 0.14 (76.0 examples/sec; 0.053 sec/batch; 87h:17m:57s remains)
INFO - root - 2019-11-03 23:27:02.793872: step 25770, total loss = 0.40, predict loss = 0.09 (69.5 examples/sec; 0.058 sec/batch; 95h:33m:41s remains)
INFO - root - 2019-11-03 23:27:03.450646: step 25780, total loss = 0.47, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 95h:15m:28s remains)
INFO - root - 2019-11-03 23:27:04.057389: step 25790, total loss = 0.69, predict loss = 0.16 (66.0 examples/sec; 0.061 sec/batch; 100h:31m:29s remains)
INFO - root - 2019-11-03 23:27:04.686262: step 25800, total loss = 0.52, predict loss = 0.11 (68.4 examples/sec; 0.058 sec/batch; 96h:59m:21s remains)
INFO - root - 2019-11-03 23:27:05.326838: step 25810, total loss = 0.67, predict loss = 0.15 (72.1 examples/sec; 0.056 sec/batch; 92h:07m:01s remains)
INFO - root - 2019-11-03 23:27:05.948497: step 25820, total loss = 0.61, predict loss = 0.13 (64.4 examples/sec; 0.062 sec/batch; 103h:04m:51s remains)
INFO - root - 2019-11-03 23:27:06.567442: step 25830, total loss = 0.85, predict loss = 0.19 (77.1 examples/sec; 0.052 sec/batch; 86h:03m:13s remains)
INFO - root - 2019-11-03 23:27:07.220348: step 25840, total loss = 0.79, predict loss = 0.17 (71.2 examples/sec; 0.056 sec/batch; 93h:11m:07s remains)
INFO - root - 2019-11-03 23:27:07.884101: step 25850, total loss = 0.75, predict loss = 0.18 (72.3 examples/sec; 0.055 sec/batch; 91h:49m:34s remains)
INFO - root - 2019-11-03 23:27:08.528812: step 25860, total loss = 0.93, predict loss = 0.22 (72.2 examples/sec; 0.055 sec/batch; 91h:54m:46s remains)
INFO - root - 2019-11-03 23:27:09.164484: step 25870, total loss = 0.70, predict loss = 0.17 (73.5 examples/sec; 0.054 sec/batch; 90h:17m:30s remains)
INFO - root - 2019-11-03 23:27:09.774480: step 25880, total loss = 0.47, predict loss = 0.10 (77.2 examples/sec; 0.052 sec/batch; 85h:57m:13s remains)
INFO - root - 2019-11-03 23:27:10.397369: step 25890, total loss = 0.54, predict loss = 0.12 (67.8 examples/sec; 0.059 sec/batch; 97h:50m:41s remains)
INFO - root - 2019-11-03 23:27:11.004354: step 25900, total loss = 0.54, predict loss = 0.12 (70.8 examples/sec; 0.056 sec/batch; 93h:42m:45s remains)
INFO - root - 2019-11-03 23:27:11.679486: step 25910, total loss = 0.62, predict loss = 0.14 (61.5 examples/sec; 0.065 sec/batch; 107h:59m:13s remains)
INFO - root - 2019-11-03 23:27:12.312791: step 25920, total loss = 0.55, predict loss = 0.13 (68.9 examples/sec; 0.058 sec/batch; 96h:19m:53s remains)
INFO - root - 2019-11-03 23:27:12.951780: step 25930, total loss = 0.63, predict loss = 0.14 (69.9 examples/sec; 0.057 sec/batch; 94h:58m:00s remains)
INFO - root - 2019-11-03 23:27:13.588158: step 25940, total loss = 0.64, predict loss = 0.14 (71.2 examples/sec; 0.056 sec/batch; 93h:16m:19s remains)
INFO - root - 2019-11-03 23:27:14.199626: step 25950, total loss = 0.63, predict loss = 0.14 (74.7 examples/sec; 0.054 sec/batch; 88h:49m:15s remains)
INFO - root - 2019-11-03 23:27:14.827072: step 25960, total loss = 0.59, predict loss = 0.14 (65.7 examples/sec; 0.061 sec/batch; 101h:05m:18s remains)
INFO - root - 2019-11-03 23:27:15.461293: step 25970, total loss = 1.16, predict loss = 0.26 (62.5 examples/sec; 0.064 sec/batch; 106h:11m:24s remains)
INFO - root - 2019-11-03 23:27:16.111637: step 25980, total loss = 0.87, predict loss = 0.20 (62.1 examples/sec; 0.064 sec/batch; 106h:52m:36s remains)
INFO - root - 2019-11-03 23:27:16.763991: step 25990, total loss = 1.19, predict loss = 0.29 (66.7 examples/sec; 0.060 sec/batch; 99h:30m:49s remains)
INFO - root - 2019-11-03 23:27:17.407177: step 26000, total loss = 0.95, predict loss = 0.22 (73.9 examples/sec; 0.054 sec/batch; 89h:51m:09s remains)
INFO - root - 2019-11-03 23:27:18.074778: step 26010, total loss = 1.00, predict loss = 0.23 (69.1 examples/sec; 0.058 sec/batch; 96h:07m:37s remains)
INFO - root - 2019-11-03 23:27:18.730508: step 26020, total loss = 1.06, predict loss = 0.23 (69.7 examples/sec; 0.057 sec/batch; 95h:10m:38s remains)
INFO - root - 2019-11-03 23:27:19.371127: step 26030, total loss = 1.09, predict loss = 0.26 (76.3 examples/sec; 0.052 sec/batch; 86h:59m:50s remains)
INFO - root - 2019-11-03 23:27:20.004444: step 26040, total loss = 1.00, predict loss = 0.24 (74.1 examples/sec; 0.054 sec/batch; 89h:33m:38s remains)
INFO - root - 2019-11-03 23:27:20.632689: step 26050, total loss = 0.90, predict loss = 0.21 (83.5 examples/sec; 0.048 sec/batch; 79h:31m:03s remains)
INFO - root - 2019-11-03 23:27:21.292681: step 26060, total loss = 0.70, predict loss = 0.16 (65.0 examples/sec; 0.062 sec/batch; 102h:11m:09s remains)
INFO - root - 2019-11-03 23:27:21.965130: step 26070, total loss = 0.61, predict loss = 0.14 (76.8 examples/sec; 0.052 sec/batch; 86h:25m:57s remains)
INFO - root - 2019-11-03 23:27:22.679687: step 26080, total loss = 0.83, predict loss = 0.19 (65.1 examples/sec; 0.061 sec/batch; 101h:56m:29s remains)
INFO - root - 2019-11-03 23:27:23.314325: step 26090, total loss = 0.63, predict loss = 0.14 (74.5 examples/sec; 0.054 sec/batch; 89h:03m:50s remains)
INFO - root - 2019-11-03 23:27:23.960112: step 26100, total loss = 0.76, predict loss = 0.17 (76.2 examples/sec; 0.052 sec/batch; 87h:04m:08s remains)
INFO - root - 2019-11-03 23:27:24.572346: step 26110, total loss = 0.70, predict loss = 0.17 (63.7 examples/sec; 0.063 sec/batch; 104h:16m:33s remains)
INFO - root - 2019-11-03 23:27:25.218509: step 26120, total loss = 0.64, predict loss = 0.14 (79.3 examples/sec; 0.050 sec/batch; 83h:40m:38s remains)
INFO - root - 2019-11-03 23:27:25.836840: step 26130, total loss = 0.58, predict loss = 0.14 (65.1 examples/sec; 0.061 sec/batch; 101h:55m:54s remains)
INFO - root - 2019-11-03 23:27:26.523984: step 26140, total loss = 0.80, predict loss = 0.20 (64.7 examples/sec; 0.062 sec/batch; 102h:39m:22s remains)
INFO - root - 2019-11-03 23:27:27.201837: step 26150, total loss = 0.60, predict loss = 0.15 (53.9 examples/sec; 0.074 sec/batch; 123h:13m:58s remains)
INFO - root - 2019-11-03 23:27:27.992816: step 26160, total loss = 0.63, predict loss = 0.14 (62.1 examples/sec; 0.064 sec/batch; 106h:48m:17s remains)
INFO - root - 2019-11-03 23:27:28.755857: step 26170, total loss = 0.70, predict loss = 0.16 (61.6 examples/sec; 0.065 sec/batch; 107h:41m:43s remains)
INFO - root - 2019-11-03 23:27:29.529720: step 26180, total loss = 1.02, predict loss = 0.26 (60.8 examples/sec; 0.066 sec/batch; 109h:14m:00s remains)
INFO - root - 2019-11-03 23:27:30.254706: step 26190, total loss = 0.73, predict loss = 0.16 (74.2 examples/sec; 0.054 sec/batch; 89h:24m:15s remains)
INFO - root - 2019-11-03 23:27:30.888194: step 26200, total loss = 0.70, predict loss = 0.16 (72.6 examples/sec; 0.055 sec/batch; 91h:28m:15s remains)
INFO - root - 2019-11-03 23:27:31.513945: step 26210, total loss = 0.78, predict loss = 0.18 (68.1 examples/sec; 0.059 sec/batch; 97h:29m:31s remains)
INFO - root - 2019-11-03 23:27:32.140500: step 26220, total loss = 0.55, predict loss = 0.12 (70.6 examples/sec; 0.057 sec/batch; 94h:02m:13s remains)
INFO - root - 2019-11-03 23:27:32.769227: step 26230, total loss = 0.72, predict loss = 0.16 (73.1 examples/sec; 0.055 sec/batch; 90h:51m:23s remains)
INFO - root - 2019-11-03 23:27:33.414337: step 26240, total loss = 0.65, predict loss = 0.15 (64.5 examples/sec; 0.062 sec/batch; 102h:54m:38s remains)
INFO - root - 2019-11-03 23:27:34.129258: step 26250, total loss = 0.58, predict loss = 0.13 (63.5 examples/sec; 0.063 sec/batch; 104h:32m:18s remains)
INFO - root - 2019-11-03 23:27:34.741710: step 26260, total loss = 0.48, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 90h:53m:44s remains)
INFO - root - 2019-11-03 23:27:35.340918: step 26270, total loss = 0.50, predict loss = 0.11 (65.3 examples/sec; 0.061 sec/batch; 101h:42m:55s remains)
INFO - root - 2019-11-03 23:27:35.971789: step 26280, total loss = 0.53, predict loss = 0.12 (74.8 examples/sec; 0.053 sec/batch; 88h:42m:52s remains)
INFO - root - 2019-11-03 23:27:36.627487: step 26290, total loss = 0.71, predict loss = 0.15 (78.1 examples/sec; 0.051 sec/batch; 84h:56m:46s remains)
INFO - root - 2019-11-03 23:27:37.246055: step 26300, total loss = 0.59, predict loss = 0.13 (75.9 examples/sec; 0.053 sec/batch; 87h:28m:53s remains)
INFO - root - 2019-11-03 23:27:37.912927: step 26310, total loss = 0.46, predict loss = 0.11 (79.1 examples/sec; 0.051 sec/batch; 83h:55m:22s remains)
INFO - root - 2019-11-03 23:27:38.566741: step 26320, total loss = 0.65, predict loss = 0.15 (69.6 examples/sec; 0.058 sec/batch; 95h:25m:38s remains)
INFO - root - 2019-11-03 23:27:39.234473: step 26330, total loss = 0.54, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 95h:57m:16s remains)
INFO - root - 2019-11-03 23:27:39.873992: step 26340, total loss = 0.72, predict loss = 0.17 (62.6 examples/sec; 0.064 sec/batch; 106h:06m:26s remains)
INFO - root - 2019-11-03 23:27:40.522074: step 26350, total loss = 0.54, predict loss = 0.13 (69.5 examples/sec; 0.058 sec/batch; 95h:32m:04s remains)
INFO - root - 2019-11-03 23:27:41.135831: step 26360, total loss = 0.66, predict loss = 0.15 (79.9 examples/sec; 0.050 sec/batch; 83h:01m:13s remains)
INFO - root - 2019-11-03 23:27:41.757430: step 26370, total loss = 0.68, predict loss = 0.17 (68.9 examples/sec; 0.058 sec/batch; 96h:17m:47s remains)
INFO - root - 2019-11-03 23:27:42.416056: step 26380, total loss = 0.75, predict loss = 0.17 (62.1 examples/sec; 0.064 sec/batch; 106h:51m:49s remains)
INFO - root - 2019-11-03 23:27:43.059272: step 26390, total loss = 0.87, predict loss = 0.20 (74.7 examples/sec; 0.054 sec/batch; 88h:49m:58s remains)
INFO - root - 2019-11-03 23:27:43.694773: step 26400, total loss = 0.68, predict loss = 0.15 (66.2 examples/sec; 0.060 sec/batch; 100h:11m:48s remains)
INFO - root - 2019-11-03 23:27:44.359038: step 26410, total loss = 0.78, predict loss = 0.17 (69.2 examples/sec; 0.058 sec/batch; 95h:58m:03s remains)
INFO - root - 2019-11-03 23:27:44.997871: step 26420, total loss = 0.94, predict loss = 0.23 (65.2 examples/sec; 0.061 sec/batch; 101h:47m:49s remains)
INFO - root - 2019-11-03 23:27:45.644185: step 26430, total loss = 0.77, predict loss = 0.19 (71.1 examples/sec; 0.056 sec/batch; 93h:23m:30s remains)
INFO - root - 2019-11-03 23:27:46.273008: step 26440, total loss = 0.82, predict loss = 0.20 (73.6 examples/sec; 0.054 sec/batch; 90h:09m:19s remains)
INFO - root - 2019-11-03 23:27:46.960299: step 26450, total loss = 0.83, predict loss = 0.19 (67.8 examples/sec; 0.059 sec/batch; 97h:56m:51s remains)
INFO - root - 2019-11-03 23:27:47.577543: step 26460, total loss = 0.79, predict loss = 0.19 (72.3 examples/sec; 0.055 sec/batch; 91h:51m:27s remains)
INFO - root - 2019-11-03 23:27:48.213913: step 26470, total loss = 0.68, predict loss = 0.16 (70.2 examples/sec; 0.057 sec/batch; 94h:32m:43s remains)
INFO - root - 2019-11-03 23:27:48.853158: step 26480, total loss = 0.56, predict loss = 0.12 (71.6 examples/sec; 0.056 sec/batch; 92h:44m:31s remains)
INFO - root - 2019-11-03 23:27:49.491147: step 26490, total loss = 0.69, predict loss = 0.16 (70.5 examples/sec; 0.057 sec/batch; 94h:11m:26s remains)
INFO - root - 2019-11-03 23:27:50.136154: step 26500, total loss = 0.68, predict loss = 0.14 (72.0 examples/sec; 0.056 sec/batch; 92h:10m:22s remains)
INFO - root - 2019-11-03 23:27:50.771982: step 26510, total loss = 0.44, predict loss = 0.10 (76.8 examples/sec; 0.052 sec/batch; 86h:22m:19s remains)
INFO - root - 2019-11-03 23:27:51.420896: step 26520, total loss = 0.67, predict loss = 0.14 (78.5 examples/sec; 0.051 sec/batch; 84h:34m:18s remains)
INFO - root - 2019-11-03 23:27:52.028604: step 26530, total loss = 0.52, predict loss = 0.11 (78.9 examples/sec; 0.051 sec/batch; 84h:05m:44s remains)
INFO - root - 2019-11-03 23:27:52.663811: step 26540, total loss = 0.54, predict loss = 0.12 (73.1 examples/sec; 0.055 sec/batch; 90h:45m:00s remains)
INFO - root - 2019-11-03 23:27:53.276510: step 26550, total loss = 0.50, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 87h:56m:59s remains)
INFO - root - 2019-11-03 23:27:53.890649: step 26560, total loss = 0.53, predict loss = 0.11 (71.9 examples/sec; 0.056 sec/batch; 92h:21m:08s remains)
INFO - root - 2019-11-03 23:27:54.531334: step 26570, total loss = 0.56, predict loss = 0.14 (67.0 examples/sec; 0.060 sec/batch; 99h:00m:11s remains)
INFO - root - 2019-11-03 23:27:55.212179: step 26580, total loss = 0.50, predict loss = 0.11 (63.7 examples/sec; 0.063 sec/batch; 104h:14m:51s remains)
INFO - root - 2019-11-03 23:27:55.870365: step 26590, total loss = 0.59, predict loss = 0.15 (63.8 examples/sec; 0.063 sec/batch; 104h:06m:39s remains)
INFO - root - 2019-11-03 23:27:56.525530: step 26600, total loss = 0.67, predict loss = 0.15 (61.7 examples/sec; 0.065 sec/batch; 107h:32m:57s remains)
INFO - root - 2019-11-03 23:27:57.160754: step 26610, total loss = 1.07, predict loss = 0.25 (63.5 examples/sec; 0.063 sec/batch; 104h:35m:34s remains)
INFO - root - 2019-11-03 23:27:57.835648: step 26620, total loss = 0.73, predict loss = 0.16 (67.2 examples/sec; 0.059 sec/batch; 98h:42m:43s remains)
INFO - root - 2019-11-03 23:27:58.514178: step 26630, total loss = 0.80, predict loss = 0.18 (68.4 examples/sec; 0.058 sec/batch; 97h:03m:05s remains)
INFO - root - 2019-11-03 23:27:59.179253: step 26640, total loss = 0.64, predict loss = 0.14 (68.5 examples/sec; 0.058 sec/batch; 96h:52m:53s remains)
INFO - root - 2019-11-03 23:27:59.890130: step 26650, total loss = 0.69, predict loss = 0.17 (71.6 examples/sec; 0.056 sec/batch; 92h:41m:54s remains)
INFO - root - 2019-11-03 23:28:00.506493: step 26660, total loss = 0.75, predict loss = 0.17 (71.1 examples/sec; 0.056 sec/batch; 93h:23m:53s remains)
INFO - root - 2019-11-03 23:28:01.149433: step 26670, total loss = 0.65, predict loss = 0.15 (71.2 examples/sec; 0.056 sec/batch; 93h:10m:43s remains)
INFO - root - 2019-11-03 23:28:01.792083: step 26680, total loss = 0.72, predict loss = 0.16 (75.6 examples/sec; 0.053 sec/batch; 87h:45m:16s remains)
INFO - root - 2019-11-03 23:28:02.405628: step 26690, total loss = 0.72, predict loss = 0.18 (67.4 examples/sec; 0.059 sec/batch; 98h:24m:35s remains)
INFO - root - 2019-11-03 23:28:03.043078: step 26700, total loss = 0.69, predict loss = 0.16 (72.0 examples/sec; 0.056 sec/batch; 92h:13m:41s remains)
INFO - root - 2019-11-03 23:28:03.647737: step 26710, total loss = 0.55, predict loss = 0.14 (77.5 examples/sec; 0.052 sec/batch; 85h:38m:58s remains)
INFO - root - 2019-11-03 23:28:04.312263: step 26720, total loss = 0.67, predict loss = 0.15 (60.5 examples/sec; 0.066 sec/batch; 109h:44m:20s remains)
INFO - root - 2019-11-03 23:28:04.982363: step 26730, total loss = 0.66, predict loss = 0.15 (65.2 examples/sec; 0.061 sec/batch; 101h:48m:57s remains)
INFO - root - 2019-11-03 23:28:05.650941: step 26740, total loss = 0.66, predict loss = 0.16 (72.3 examples/sec; 0.055 sec/batch; 91h:47m:52s remains)
INFO - root - 2019-11-03 23:28:06.287875: step 26750, total loss = 0.76, predict loss = 0.18 (69.9 examples/sec; 0.057 sec/batch; 94h:57m:29s remains)
INFO - root - 2019-11-03 23:28:06.929934: step 26760, total loss = 0.59, predict loss = 0.13 (64.0 examples/sec; 0.063 sec/batch; 103h:43m:10s remains)
INFO - root - 2019-11-03 23:28:07.558546: step 26770, total loss = 0.81, predict loss = 0.19 (72.8 examples/sec; 0.055 sec/batch; 91h:07m:22s remains)
INFO - root - 2019-11-03 23:28:08.184889: step 26780, total loss = 0.54, predict loss = 0.12 (68.3 examples/sec; 0.059 sec/batch; 97h:06m:28s remains)
INFO - root - 2019-11-03 23:28:08.854722: step 26790, total loss = 0.56, predict loss = 0.12 (66.6 examples/sec; 0.060 sec/batch; 99h:39m:12s remains)
INFO - root - 2019-11-03 23:28:09.530689: step 26800, total loss = 0.51, predict loss = 0.11 (59.9 examples/sec; 0.067 sec/batch; 110h:45m:42s remains)
INFO - root - 2019-11-03 23:28:10.195610: step 26810, total loss = 0.64, predict loss = 0.15 (64.7 examples/sec; 0.062 sec/batch; 102h:30m:54s remains)
INFO - root - 2019-11-03 23:28:10.838848: step 26820, total loss = 0.54, predict loss = 0.12 (71.2 examples/sec; 0.056 sec/batch; 93h:14m:54s remains)
INFO - root - 2019-11-03 23:28:11.506995: step 26830, total loss = 0.52, predict loss = 0.11 (58.1 examples/sec; 0.069 sec/batch; 114h:16m:05s remains)
INFO - root - 2019-11-03 23:28:12.180286: step 26840, total loss = 0.55, predict loss = 0.13 (61.1 examples/sec; 0.065 sec/batch; 108h:38m:12s remains)
INFO - root - 2019-11-03 23:28:12.868438: step 26850, total loss = 0.67, predict loss = 0.15 (64.2 examples/sec; 0.062 sec/batch; 103h:21m:45s remains)
INFO - root - 2019-11-03 23:28:13.466327: step 26860, total loss = 0.54, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 96h:02m:12s remains)
INFO - root - 2019-11-03 23:28:14.085465: step 26870, total loss = 0.59, predict loss = 0.14 (68.8 examples/sec; 0.058 sec/batch; 96h:29m:09s remains)
INFO - root - 2019-11-03 23:28:14.710960: step 26880, total loss = 0.68, predict loss = 0.15 (73.6 examples/sec; 0.054 sec/batch; 90h:12m:18s remains)
INFO - root - 2019-11-03 23:28:15.323914: step 26890, total loss = 0.58, predict loss = 0.13 (81.7 examples/sec; 0.049 sec/batch; 81h:12m:36s remains)
INFO - root - 2019-11-03 23:28:15.926549: step 26900, total loss = 0.58, predict loss = 0.15 (80.9 examples/sec; 0.049 sec/batch; 82h:03m:37s remains)
INFO - root - 2019-11-03 23:28:16.561453: step 26910, total loss = 0.58, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 95h:08m:14s remains)
INFO - root - 2019-11-03 23:28:17.203948: step 26920, total loss = 0.62, predict loss = 0.15 (67.0 examples/sec; 0.060 sec/batch; 99h:02m:20s remains)
INFO - root - 2019-11-03 23:28:17.835576: step 26930, total loss = 0.49, predict loss = 0.11 (76.7 examples/sec; 0.052 sec/batch; 86h:33m:40s remains)
INFO - root - 2019-11-03 23:28:18.499928: step 26940, total loss = 0.58, predict loss = 0.14 (67.6 examples/sec; 0.059 sec/batch; 98h:12m:11s remains)
INFO - root - 2019-11-03 23:28:19.129500: step 26950, total loss = 0.60, predict loss = 0.15 (73.2 examples/sec; 0.055 sec/batch; 90h:42m:30s remains)
INFO - root - 2019-11-03 23:28:19.762264: step 26960, total loss = 0.58, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 98h:38m:03s remains)
INFO - root - 2019-11-03 23:28:20.436007: step 26970, total loss = 0.75, predict loss = 0.18 (67.3 examples/sec; 0.059 sec/batch; 98h:33m:11s remains)
INFO - root - 2019-11-03 23:28:21.082613: step 26980, total loss = 0.66, predict loss = 0.15 (64.1 examples/sec; 0.062 sec/batch; 103h:33m:42s remains)
INFO - root - 2019-11-03 23:28:21.765728: step 26990, total loss = 0.78, predict loss = 0.20 (64.5 examples/sec; 0.062 sec/batch; 102h:54m:07s remains)
INFO - root - 2019-11-03 23:28:22.424388: step 27000, total loss = 0.89, predict loss = 0.21 (64.0 examples/sec; 0.063 sec/batch; 103h:43m:13s remains)
INFO - root - 2019-11-03 23:28:23.062764: step 27010, total loss = 0.60, predict loss = 0.14 (67.4 examples/sec; 0.059 sec/batch; 98h:28m:44s remains)
INFO - root - 2019-11-03 23:28:23.678269: step 27020, total loss = 0.98, predict loss = 0.22 (64.9 examples/sec; 0.062 sec/batch; 102h:15m:43s remains)
INFO - root - 2019-11-03 23:28:24.317479: step 27030, total loss = 0.83, predict loss = 0.20 (65.6 examples/sec; 0.061 sec/batch; 101h:11m:12s remains)
INFO - root - 2019-11-03 23:28:24.990088: step 27040, total loss = 0.68, predict loss = 0.16 (70.1 examples/sec; 0.057 sec/batch; 94h:43m:39s remains)
INFO - root - 2019-11-03 23:28:25.665837: step 27050, total loss = 0.77, predict loss = 0.18 (70.2 examples/sec; 0.057 sec/batch; 94h:35m:41s remains)
INFO - root - 2019-11-03 23:28:26.324175: step 27060, total loss = 0.84, predict loss = 0.20 (66.1 examples/sec; 0.060 sec/batch; 100h:20m:09s remains)
INFO - root - 2019-11-03 23:28:26.972286: step 27070, total loss = 0.73, predict loss = 0.18 (65.6 examples/sec; 0.061 sec/batch; 101h:05m:57s remains)
INFO - root - 2019-11-03 23:28:27.550914: step 27080, total loss = 0.87, predict loss = 0.22 (81.9 examples/sec; 0.049 sec/batch; 81h:01m:51s remains)
INFO - root - 2019-11-03 23:28:28.183036: step 27090, total loss = 0.56, predict loss = 0.13 (64.7 examples/sec; 0.062 sec/batch; 102h:38m:54s remains)
INFO - root - 2019-11-03 23:28:28.827718: step 27100, total loss = 0.99, predict loss = 0.24 (65.1 examples/sec; 0.061 sec/batch; 102h:00m:04s remains)
INFO - root - 2019-11-03 23:28:29.509021: step 27110, total loss = 0.60, predict loss = 0.14 (58.4 examples/sec; 0.069 sec/batch; 113h:40m:57s remains)
INFO - root - 2019-11-03 23:28:30.186212: step 27120, total loss = 0.86, predict loss = 0.22 (71.4 examples/sec; 0.056 sec/batch; 92h:59m:13s remains)
INFO - root - 2019-11-03 23:28:30.790596: step 27130, total loss = 0.49, predict loss = 0.12 (67.0 examples/sec; 0.060 sec/batch; 99h:02m:54s remains)
INFO - root - 2019-11-03 23:28:31.407135: step 27140, total loss = 0.58, predict loss = 0.13 (71.3 examples/sec; 0.056 sec/batch; 93h:08m:27s remains)
INFO - root - 2019-11-03 23:28:32.042864: step 27150, total loss = 0.68, predict loss = 0.16 (65.6 examples/sec; 0.061 sec/batch; 101h:07m:56s remains)
INFO - root - 2019-11-03 23:28:32.677176: step 27160, total loss = 0.53, predict loss = 0.12 (74.9 examples/sec; 0.053 sec/batch; 88h:37m:26s remains)
INFO - root - 2019-11-03 23:28:33.306657: step 27170, total loss = 0.80, predict loss = 0.19 (68.6 examples/sec; 0.058 sec/batch; 96h:41m:29s remains)
INFO - root - 2019-11-03 23:28:33.932510: step 27180, total loss = 0.64, predict loss = 0.15 (72.0 examples/sec; 0.056 sec/batch; 92h:08m:43s remains)
INFO - root - 2019-11-03 23:28:34.549312: step 27190, total loss = 0.83, predict loss = 0.20 (67.5 examples/sec; 0.059 sec/batch; 98h:19m:22s remains)
INFO - root - 2019-11-03 23:28:35.146301: step 27200, total loss = 0.89, predict loss = 0.21 (78.2 examples/sec; 0.051 sec/batch; 84h:54m:00s remains)
INFO - root - 2019-11-03 23:28:35.764806: step 27210, total loss = 0.60, predict loss = 0.15 (73.8 examples/sec; 0.054 sec/batch; 89h:57m:40s remains)
INFO - root - 2019-11-03 23:28:36.456409: step 27220, total loss = 0.81, predict loss = 0.20 (63.9 examples/sec; 0.063 sec/batch; 103h:50m:02s remains)
INFO - root - 2019-11-03 23:28:37.142972: step 27230, total loss = 0.78, predict loss = 0.18 (61.8 examples/sec; 0.065 sec/batch; 107h:18m:55s remains)
INFO - root - 2019-11-03 23:28:37.809688: step 27240, total loss = 0.78, predict loss = 0.18 (60.4 examples/sec; 0.066 sec/batch; 109h:51m:26s remains)
INFO - root - 2019-11-03 23:28:38.910244: step 27250, total loss = 0.65, predict loss = 0.15 (95.2 examples/sec; 0.042 sec/batch; 69h:40m:58s remains)
INFO - root - 2019-11-03 23:28:39.523432: step 27260, total loss = 0.84, predict loss = 0.19 (96.2 examples/sec; 0.042 sec/batch; 68h:58m:13s remains)
INFO - root - 2019-11-03 23:28:40.000265: step 27270, total loss = 0.49, predict loss = 0.11 (99.4 examples/sec; 0.040 sec/batch; 66h:44m:57s remains)
INFO - root - 2019-11-03 23:28:41.100606: step 27280, total loss = 0.46, predict loss = 0.09 (64.4 examples/sec; 0.062 sec/batch; 103h:00m:10s remains)
INFO - root - 2019-11-03 23:28:41.723541: step 27290, total loss = 0.61, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 91h:21m:50s remains)
INFO - root - 2019-11-03 23:28:42.376789: step 27300, total loss = 0.54, predict loss = 0.11 (73.3 examples/sec; 0.055 sec/batch; 90h:30m:45s remains)
INFO - root - 2019-11-03 23:28:42.995536: step 27310, total loss = 0.77, predict loss = 0.18 (67.1 examples/sec; 0.060 sec/batch; 98h:54m:58s remains)
INFO - root - 2019-11-03 23:28:43.653164: step 27320, total loss = 0.77, predict loss = 0.19 (73.6 examples/sec; 0.054 sec/batch; 90h:10m:37s remains)
INFO - root - 2019-11-03 23:28:44.351026: step 27330, total loss = 0.63, predict loss = 0.16 (52.5 examples/sec; 0.076 sec/batch; 126h:23m:47s remains)
INFO - root - 2019-11-03 23:28:45.117109: step 27340, total loss = 0.63, predict loss = 0.15 (61.2 examples/sec; 0.065 sec/batch; 108h:28m:33s remains)
INFO - root - 2019-11-03 23:28:45.798371: step 27350, total loss = 0.92, predict loss = 0.20 (68.2 examples/sec; 0.059 sec/batch; 97h:17m:07s remains)
INFO - root - 2019-11-03 23:28:46.464211: step 27360, total loss = 0.76, predict loss = 0.17 (65.1 examples/sec; 0.061 sec/batch; 101h:58m:18s remains)
INFO - root - 2019-11-03 23:28:47.115275: step 27370, total loss = 1.18, predict loss = 0.28 (53.8 examples/sec; 0.074 sec/batch; 123h:24m:59s remains)
INFO - root - 2019-11-03 23:28:47.765184: step 27380, total loss = 0.95, predict loss = 0.22 (69.0 examples/sec; 0.058 sec/batch; 96h:13m:18s remains)
INFO - root - 2019-11-03 23:28:48.402716: step 27390, total loss = 0.79, predict loss = 0.19 (69.2 examples/sec; 0.058 sec/batch; 95h:56m:23s remains)
INFO - root - 2019-11-03 23:28:49.022411: step 27400, total loss = 0.62, predict loss = 0.13 (75.4 examples/sec; 0.053 sec/batch; 88h:02m:32s remains)
INFO - root - 2019-11-03 23:28:49.640633: step 27410, total loss = 0.69, predict loss = 0.17 (84.5 examples/sec; 0.047 sec/batch; 78h:30m:09s remains)
INFO - root - 2019-11-03 23:28:50.257291: step 27420, total loss = 0.61, predict loss = 0.13 (72.1 examples/sec; 0.056 sec/batch; 92h:05m:39s remains)
INFO - root - 2019-11-03 23:28:50.984277: step 27430, total loss = 0.86, predict loss = 0.21 (58.2 examples/sec; 0.069 sec/batch; 114h:07m:11s remains)
INFO - root - 2019-11-03 23:28:51.768870: step 27440, total loss = 0.63, predict loss = 0.15 (56.7 examples/sec; 0.071 sec/batch; 117h:06m:10s remains)
INFO - root - 2019-11-03 23:28:52.600529: step 27450, total loss = 0.60, predict loss = 0.14 (67.3 examples/sec; 0.059 sec/batch; 98h:33m:14s remains)
INFO - root - 2019-11-03 23:28:53.180927: step 27460, total loss = 0.57, predict loss = 0.12 (76.0 examples/sec; 0.053 sec/batch; 87h:15m:53s remains)
INFO - root - 2019-11-03 23:28:53.774202: step 27470, total loss = 0.60, predict loss = 0.13 (71.7 examples/sec; 0.056 sec/batch; 92h:30m:29s remains)
INFO - root - 2019-11-03 23:28:54.468899: step 27480, total loss = 0.77, predict loss = 0.16 (70.8 examples/sec; 0.056 sec/batch; 93h:41m:36s remains)
INFO - root - 2019-11-03 23:28:55.076786: step 27490, total loss = 0.46, predict loss = 0.10 (75.1 examples/sec; 0.053 sec/batch; 88h:20m:59s remains)
INFO - root - 2019-11-03 23:28:55.714761: step 27500, total loss = 0.50, predict loss = 0.12 (72.1 examples/sec; 0.055 sec/batch; 92h:03m:46s remains)
INFO - root - 2019-11-03 23:28:56.348893: step 27510, total loss = 0.65, predict loss = 0.14 (72.0 examples/sec; 0.056 sec/batch; 92h:13m:38s remains)
INFO - root - 2019-11-03 23:28:56.956417: step 27520, total loss = 0.64, predict loss = 0.14 (69.9 examples/sec; 0.057 sec/batch; 94h:53m:27s remains)
INFO - root - 2019-11-03 23:28:57.653413: step 27530, total loss = 1.19, predict loss = 0.29 (75.8 examples/sec; 0.053 sec/batch; 87h:33m:37s remains)
INFO - root - 2019-11-03 23:28:58.299193: step 27540, total loss = 0.68, predict loss = 0.14 (60.5 examples/sec; 0.066 sec/batch; 109h:46m:21s remains)
INFO - root - 2019-11-03 23:28:59.002789: step 27550, total loss = 0.64, predict loss = 0.14 (67.9 examples/sec; 0.059 sec/batch; 97h:41m:15s remains)
INFO - root - 2019-11-03 23:28:59.653108: step 27560, total loss = 0.80, predict loss = 0.17 (72.0 examples/sec; 0.056 sec/batch; 92h:07m:58s remains)
INFO - root - 2019-11-03 23:29:00.329075: step 27570, total loss = 0.80, predict loss = 0.18 (71.5 examples/sec; 0.056 sec/batch; 92h:47m:21s remains)
INFO - root - 2019-11-03 23:29:00.961250: step 27580, total loss = 0.86, predict loss = 0.20 (78.1 examples/sec; 0.051 sec/batch; 84h:59m:05s remains)
INFO - root - 2019-11-03 23:29:01.570552: step 27590, total loss = 0.59, predict loss = 0.13 (68.9 examples/sec; 0.058 sec/batch; 96h:15m:25s remains)
INFO - root - 2019-11-03 23:29:02.192142: step 27600, total loss = 0.74, predict loss = 0.17 (64.6 examples/sec; 0.062 sec/batch; 102h:46m:07s remains)
INFO - root - 2019-11-03 23:29:02.815795: step 27610, total loss = 0.63, predict loss = 0.13 (68.5 examples/sec; 0.058 sec/batch; 96h:53m:52s remains)
INFO - root - 2019-11-03 23:29:03.505274: step 27620, total loss = 0.81, predict loss = 0.18 (62.2 examples/sec; 0.064 sec/batch; 106h:45m:29s remains)
INFO - root - 2019-11-03 23:29:04.152546: step 27630, total loss = 0.73, predict loss = 0.16 (77.5 examples/sec; 0.052 sec/batch; 85h:37m:33s remains)
INFO - root - 2019-11-03 23:29:04.759885: step 27640, total loss = 0.69, predict loss = 0.16 (70.2 examples/sec; 0.057 sec/batch; 94h:34m:25s remains)
INFO - root - 2019-11-03 23:29:05.376078: step 27650, total loss = 0.76, predict loss = 0.19 (69.1 examples/sec; 0.058 sec/batch; 95h:58m:28s remains)
INFO - root - 2019-11-03 23:29:06.008232: step 27660, total loss = 0.57, predict loss = 0.13 (67.3 examples/sec; 0.059 sec/batch; 98h:33m:04s remains)
INFO - root - 2019-11-03 23:29:06.636522: step 27670, total loss = 0.60, predict loss = 0.13 (75.8 examples/sec; 0.053 sec/batch; 87h:32m:05s remains)
INFO - root - 2019-11-03 23:29:07.318075: step 27680, total loss = 0.59, predict loss = 0.13 (62.9 examples/sec; 0.064 sec/batch; 105h:25m:23s remains)
INFO - root - 2019-11-03 23:29:07.996757: step 27690, total loss = 0.75, predict loss = 0.15 (72.6 examples/sec; 0.055 sec/batch; 91h:25m:36s remains)
INFO - root - 2019-11-03 23:29:08.669902: step 27700, total loss = 0.77, predict loss = 0.16 (72.3 examples/sec; 0.055 sec/batch; 91h:50m:35s remains)
INFO - root - 2019-11-03 23:29:09.329881: step 27710, total loss = 0.72, predict loss = 0.18 (69.8 examples/sec; 0.057 sec/batch; 95h:02m:47s remains)
INFO - root - 2019-11-03 23:29:09.960682: step 27720, total loss = 0.62, predict loss = 0.15 (69.3 examples/sec; 0.058 sec/batch; 95h:42m:27s remains)
INFO - root - 2019-11-03 23:29:10.611386: step 27730, total loss = 0.61, predict loss = 0.14 (67.1 examples/sec; 0.060 sec/batch; 98h:51m:56s remains)
INFO - root - 2019-11-03 23:29:11.231413: step 27740, total loss = 0.49, predict loss = 0.12 (70.9 examples/sec; 0.056 sec/batch; 93h:35m:39s remains)
INFO - root - 2019-11-03 23:29:11.829797: step 27750, total loss = 0.68, predict loss = 0.16 (66.8 examples/sec; 0.060 sec/batch; 99h:21m:18s remains)
INFO - root - 2019-11-03 23:29:12.475560: step 27760, total loss = 0.87, predict loss = 0.20 (72.2 examples/sec; 0.055 sec/batch; 91h:55m:32s remains)
INFO - root - 2019-11-03 23:29:13.147098: step 27770, total loss = 0.69, predict loss = 0.16 (63.7 examples/sec; 0.063 sec/batch; 104h:05m:46s remains)
INFO - root - 2019-11-03 23:29:13.838758: step 27780, total loss = 0.83, predict loss = 0.22 (63.9 examples/sec; 0.063 sec/batch; 103h:54m:52s remains)
INFO - root - 2019-11-03 23:29:14.513294: step 27790, total loss = 0.70, predict loss = 0.17 (68.1 examples/sec; 0.059 sec/batch; 97h:29m:03s remains)
INFO - root - 2019-11-03 23:29:15.189036: step 27800, total loss = 0.69, predict loss = 0.17 (71.8 examples/sec; 0.056 sec/batch; 92h:26m:38s remains)
INFO - root - 2019-11-03 23:29:15.807890: step 27810, total loss = 0.74, predict loss = 0.18 (76.0 examples/sec; 0.053 sec/batch; 87h:15m:50s remains)
INFO - root - 2019-11-03 23:29:16.409202: step 27820, total loss = 0.60, predict loss = 0.14 (76.5 examples/sec; 0.052 sec/batch; 86h:46m:00s remains)
INFO - root - 2019-11-03 23:29:17.077810: step 27830, total loss = 0.41, predict loss = 0.09 (72.3 examples/sec; 0.055 sec/batch; 91h:49m:25s remains)
INFO - root - 2019-11-03 23:29:17.725888: step 27840, total loss = 0.48, predict loss = 0.11 (62.7 examples/sec; 0.064 sec/batch; 105h:53m:38s remains)
INFO - root - 2019-11-03 23:29:18.418848: step 27850, total loss = 0.70, predict loss = 0.16 (63.5 examples/sec; 0.063 sec/batch; 104h:27m:08s remains)
INFO - root - 2019-11-03 23:29:19.124118: step 27860, total loss = 0.58, predict loss = 0.13 (60.2 examples/sec; 0.066 sec/batch; 110h:08m:51s remains)
INFO - root - 2019-11-03 23:29:19.794453: step 27870, total loss = 1.00, predict loss = 0.29 (57.8 examples/sec; 0.069 sec/batch; 114h:42m:25s remains)
INFO - root - 2019-11-03 23:29:20.427774: step 27880, total loss = 0.57, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 96h:37m:37s remains)
INFO - root - 2019-11-03 23:29:21.059123: step 27890, total loss = 0.35, predict loss = 0.08 (68.6 examples/sec; 0.058 sec/batch; 96h:41m:05s remains)
INFO - root - 2019-11-03 23:29:21.682517: step 27900, total loss = 0.62, predict loss = 0.15 (74.2 examples/sec; 0.054 sec/batch; 89h:28m:07s remains)
INFO - root - 2019-11-03 23:29:22.346235: step 27910, total loss = 0.49, predict loss = 0.10 (77.6 examples/sec; 0.052 sec/batch; 85h:30m:55s remains)
INFO - root - 2019-11-03 23:29:22.951316: step 27920, total loss = 0.36, predict loss = 0.07 (81.1 examples/sec; 0.049 sec/batch; 81h:52m:03s remains)
INFO - root - 2019-11-03 23:29:23.580177: step 27930, total loss = 0.50, predict loss = 0.13 (63.5 examples/sec; 0.063 sec/batch; 104h:25m:42s remains)
INFO - root - 2019-11-03 23:29:24.219362: step 27940, total loss = 0.54, predict loss = 0.12 (68.5 examples/sec; 0.058 sec/batch; 96h:53m:07s remains)
INFO - root - 2019-11-03 23:29:24.892213: step 27950, total loss = 1.22, predict loss = 0.31 (66.6 examples/sec; 0.060 sec/batch; 99h:41m:59s remains)
INFO - root - 2019-11-03 23:29:25.519249: step 27960, total loss = 0.94, predict loss = 0.22 (65.6 examples/sec; 0.061 sec/batch; 101h:13m:33s remains)
INFO - root - 2019-11-03 23:29:26.188935: step 27970, total loss = 0.79, predict loss = 0.20 (73.2 examples/sec; 0.055 sec/batch; 90h:40m:08s remains)
INFO - root - 2019-11-03 23:29:26.863970: step 27980, total loss = 0.88, predict loss = 0.21 (65.9 examples/sec; 0.061 sec/batch; 100h:41m:02s remains)
INFO - root - 2019-11-03 23:29:27.582776: step 27990, total loss = 0.68, predict loss = 0.15 (64.2 examples/sec; 0.062 sec/batch; 103h:21m:25s remains)
INFO - root - 2019-11-03 23:29:28.269632: step 28000, total loss = 0.93, predict loss = 0.23 (66.6 examples/sec; 0.060 sec/batch; 99h:38m:41s remains)
INFO - root - 2019-11-03 23:29:28.965485: step 28010, total loss = 0.79, predict loss = 0.18 (64.8 examples/sec; 0.062 sec/batch; 102h:28m:11s remains)
INFO - root - 2019-11-03 23:29:29.598091: step 28020, total loss = 0.62, predict loss = 0.14 (82.2 examples/sec; 0.049 sec/batch; 80h:41m:58s remains)
INFO - root - 2019-11-03 23:29:30.234747: step 28030, total loss = 0.69, predict loss = 0.16 (67.7 examples/sec; 0.059 sec/batch; 97h:57m:12s remains)
INFO - root - 2019-11-03 23:29:30.856613: step 28040, total loss = 0.86, predict loss = 0.21 (72.0 examples/sec; 0.056 sec/batch; 92h:11m:19s remains)
INFO - root - 2019-11-03 23:29:31.454983: step 28050, total loss = 0.81, predict loss = 0.19 (72.0 examples/sec; 0.056 sec/batch; 92h:11m:10s remains)
INFO - root - 2019-11-03 23:29:32.079576: step 28060, total loss = 0.79, predict loss = 0.19 (67.9 examples/sec; 0.059 sec/batch; 97h:40m:49s remains)
INFO - root - 2019-11-03 23:29:32.684929: step 28070, total loss = 0.68, predict loss = 0.15 (73.3 examples/sec; 0.055 sec/batch; 90h:32m:42s remains)
INFO - root - 2019-11-03 23:29:33.276115: step 28080, total loss = 0.69, predict loss = 0.18 (71.1 examples/sec; 0.056 sec/batch; 93h:21m:49s remains)
INFO - root - 2019-11-03 23:29:33.895083: step 28090, total loss = 0.56, predict loss = 0.13 (73.3 examples/sec; 0.055 sec/batch; 90h:31m:58s remains)
INFO - root - 2019-11-03 23:29:34.516688: step 28100, total loss = 0.75, predict loss = 0.17 (74.8 examples/sec; 0.054 sec/batch; 88h:45m:10s remains)
INFO - root - 2019-11-03 23:29:35.176082: step 28110, total loss = 0.47, predict loss = 0.10 (67.2 examples/sec; 0.059 sec/batch; 98h:41m:20s remains)
INFO - root - 2019-11-03 23:29:35.822560: step 28120, total loss = 0.39, predict loss = 0.08 (64.0 examples/sec; 0.063 sec/batch; 103h:43m:33s remains)
INFO - root - 2019-11-03 23:29:36.468203: step 28130, total loss = 0.62, predict loss = 0.14 (69.1 examples/sec; 0.058 sec/batch; 96h:02m:43s remains)
INFO - root - 2019-11-03 23:29:37.145406: step 28140, total loss = 0.63, predict loss = 0.15 (69.8 examples/sec; 0.057 sec/batch; 95h:06m:37s remains)
INFO - root - 2019-11-03 23:29:37.801353: step 28150, total loss = 0.30, predict loss = 0.06 (60.4 examples/sec; 0.066 sec/batch; 109h:53m:18s remains)
INFO - root - 2019-11-03 23:29:38.401628: step 28160, total loss = 0.36, predict loss = 0.07 (71.9 examples/sec; 0.056 sec/batch; 92h:14m:33s remains)
INFO - root - 2019-11-03 23:29:39.029422: step 28170, total loss = 0.53, predict loss = 0.12 (67.1 examples/sec; 0.060 sec/batch; 98h:52m:47s remains)
INFO - root - 2019-11-03 23:29:39.664662: step 28180, total loss = 0.57, predict loss = 0.13 (63.1 examples/sec; 0.063 sec/batch; 105h:13m:21s remains)
INFO - root - 2019-11-03 23:29:40.290083: step 28190, total loss = 0.62, predict loss = 0.13 (68.0 examples/sec; 0.059 sec/batch; 97h:30m:48s remains)
INFO - root - 2019-11-03 23:29:40.928547: step 28200, total loss = 0.54, predict loss = 0.12 (71.1 examples/sec; 0.056 sec/batch; 93h:17m:55s remains)
INFO - root - 2019-11-03 23:29:41.583119: step 28210, total loss = 0.53, predict loss = 0.11 (72.8 examples/sec; 0.055 sec/batch; 91h:09m:02s remains)
INFO - root - 2019-11-03 23:29:42.241170: step 28220, total loss = 0.77, predict loss = 0.18 (71.2 examples/sec; 0.056 sec/batch; 93h:09m:35s remains)
INFO - root - 2019-11-03 23:29:42.862798: step 28230, total loss = 0.56, predict loss = 0.13 (61.5 examples/sec; 0.065 sec/batch; 107h:52m:24s remains)
INFO - root - 2019-11-03 23:29:43.533971: step 28240, total loss = 0.48, predict loss = 0.11 (75.5 examples/sec; 0.053 sec/batch; 87h:50m:18s remains)
INFO - root - 2019-11-03 23:29:44.115346: step 28250, total loss = 0.49, predict loss = 0.11 (75.7 examples/sec; 0.053 sec/batch; 87h:38m:40s remains)
INFO - root - 2019-11-03 23:29:44.705475: step 28260, total loss = 0.54, predict loss = 0.12 (80.2 examples/sec; 0.050 sec/batch; 82h:45m:41s remains)
INFO - root - 2019-11-03 23:29:45.324646: step 28270, total loss = 0.46, predict loss = 0.10 (68.4 examples/sec; 0.059 sec/batch; 97h:03m:34s remains)
INFO - root - 2019-11-03 23:29:45.972310: step 28280, total loss = 0.73, predict loss = 0.17 (70.1 examples/sec; 0.057 sec/batch; 94h:39m:36s remains)
INFO - root - 2019-11-03 23:29:46.604458: step 28290, total loss = 0.72, predict loss = 0.18 (77.3 examples/sec; 0.052 sec/batch; 85h:51m:01s remains)
INFO - root - 2019-11-03 23:29:47.245814: step 28300, total loss = 0.66, predict loss = 0.18 (72.4 examples/sec; 0.055 sec/batch; 91h:40m:44s remains)
INFO - root - 2019-11-03 23:29:47.867780: step 28310, total loss = 0.43, predict loss = 0.09 (73.4 examples/sec; 0.055 sec/batch; 90h:27m:22s remains)
INFO - root - 2019-11-03 23:29:48.518202: step 28320, total loss = 0.57, predict loss = 0.13 (63.9 examples/sec; 0.063 sec/batch; 103h:51m:37s remains)
INFO - root - 2019-11-03 23:29:49.165515: step 28330, total loss = 0.70, predict loss = 0.16 (73.9 examples/sec; 0.054 sec/batch; 89h:47m:02s remains)
INFO - root - 2019-11-03 23:29:49.792204: step 28340, total loss = 0.47, predict loss = 0.10 (68.8 examples/sec; 0.058 sec/batch; 96h:28m:49s remains)
INFO - root - 2019-11-03 23:29:50.434976: step 28350, total loss = 0.51, predict loss = 0.12 (70.0 examples/sec; 0.057 sec/batch; 94h:48m:37s remains)
INFO - root - 2019-11-03 23:29:51.052573: step 28360, total loss = 0.43, predict loss = 0.09 (78.9 examples/sec; 0.051 sec/batch; 84h:03m:52s remains)
INFO - root - 2019-11-03 23:29:51.663945: step 28370, total loss = 0.59, predict loss = 0.13 (70.6 examples/sec; 0.057 sec/batch; 94h:02m:22s remains)
INFO - root - 2019-11-03 23:29:52.328808: step 28380, total loss = 0.61, predict loss = 0.14 (75.8 examples/sec; 0.053 sec/batch; 87h:30m:59s remains)
INFO - root - 2019-11-03 23:29:52.949831: step 28390, total loss = 0.64, predict loss = 0.16 (74.2 examples/sec; 0.054 sec/batch; 89h:24m:29s remains)
INFO - root - 2019-11-03 23:29:53.604314: step 28400, total loss = 0.64, predict loss = 0.15 (66.5 examples/sec; 0.060 sec/batch; 99h:43m:38s remains)
INFO - root - 2019-11-03 23:29:54.252204: step 28410, total loss = 0.53, predict loss = 0.12 (63.9 examples/sec; 0.063 sec/batch; 103h:49m:26s remains)
INFO - root - 2019-11-03 23:29:54.904658: step 28420, total loss = 0.55, predict loss = 0.13 (74.9 examples/sec; 0.053 sec/batch; 88h:37m:41s remains)
INFO - root - 2019-11-03 23:29:55.572317: step 28430, total loss = 0.52, predict loss = 0.11 (62.0 examples/sec; 0.065 sec/batch; 107h:01m:52s remains)
INFO - root - 2019-11-03 23:29:56.249895: step 28440, total loss = 0.68, predict loss = 0.15 (66.4 examples/sec; 0.060 sec/batch; 99h:54m:11s remains)
INFO - root - 2019-11-03 23:29:56.907084: step 28450, total loss = 0.72, predict loss = 0.18 (71.9 examples/sec; 0.056 sec/batch; 92h:18m:41s remains)
INFO - root - 2019-11-03 23:29:57.500158: step 28460, total loss = 0.50, predict loss = 0.12 (85.5 examples/sec; 0.047 sec/batch; 77h:38m:50s remains)
INFO - root - 2019-11-03 23:29:58.114951: step 28470, total loss = 0.49, predict loss = 0.10 (69.1 examples/sec; 0.058 sec/batch; 96h:04m:35s remains)
INFO - root - 2019-11-03 23:29:58.749252: step 28480, total loss = 0.47, predict loss = 0.10 (75.1 examples/sec; 0.053 sec/batch; 88h:19m:16s remains)
INFO - root - 2019-11-03 23:29:59.353593: step 28490, total loss = 0.49, predict loss = 0.11 (82.3 examples/sec; 0.049 sec/batch; 80h:37m:05s remains)
INFO - root - 2019-11-03 23:29:59.987557: step 28500, total loss = 0.44, predict loss = 0.10 (75.3 examples/sec; 0.053 sec/batch; 88h:06m:02s remains)
INFO - root - 2019-11-03 23:30:00.625146: step 28510, total loss = 0.42, predict loss = 0.10 (70.8 examples/sec; 0.056 sec/batch; 93h:41m:42s remains)
INFO - root - 2019-11-03 23:30:01.232935: step 28520, total loss = 0.56, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 94h:59m:59s remains)
INFO - root - 2019-11-03 23:30:01.845138: step 28530, total loss = 0.76, predict loss = 0.17 (68.2 examples/sec; 0.059 sec/batch; 97h:13m:38s remains)
INFO - root - 2019-11-03 23:30:02.495299: step 28540, total loss = 0.82, predict loss = 0.20 (67.4 examples/sec; 0.059 sec/batch; 98h:27m:00s remains)
INFO - root - 2019-11-03 23:30:03.185867: step 28550, total loss = 0.55, predict loss = 0.13 (55.9 examples/sec; 0.072 sec/batch; 118h:41m:35s remains)
INFO - root - 2019-11-03 23:30:03.830571: step 28560, total loss = 0.46, predict loss = 0.11 (68.5 examples/sec; 0.058 sec/batch; 96h:48m:56s remains)
INFO - root - 2019-11-03 23:30:04.440735: step 28570, total loss = 0.78, predict loss = 0.17 (83.4 examples/sec; 0.048 sec/batch; 79h:35m:56s remains)
INFO - root - 2019-11-03 23:30:05.064253: step 28580, total loss = 0.65, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 91h:20m:38s remains)
INFO - root - 2019-11-03 23:30:05.697460: step 28590, total loss = 0.73, predict loss = 0.17 (71.2 examples/sec; 0.056 sec/batch; 93h:09m:48s remains)
INFO - root - 2019-11-03 23:30:06.313425: step 28600, total loss = 0.48, predict loss = 0.11 (66.4 examples/sec; 0.060 sec/batch; 99h:59m:25s remains)
INFO - root - 2019-11-03 23:30:06.936078: step 28610, total loss = 0.54, predict loss = 0.12 (70.2 examples/sec; 0.057 sec/batch; 94h:32m:18s remains)
INFO - root - 2019-11-03 23:30:07.538606: step 28620, total loss = 0.51, predict loss = 0.11 (73.1 examples/sec; 0.055 sec/batch; 90h:45m:40s remains)
INFO - root - 2019-11-03 23:30:08.186427: step 28630, total loss = 0.40, predict loss = 0.09 (70.8 examples/sec; 0.056 sec/batch; 93h:39m:10s remains)
INFO - root - 2019-11-03 23:30:08.816936: step 28640, total loss = 0.82, predict loss = 0.20 (72.6 examples/sec; 0.055 sec/batch; 91h:26m:38s remains)
INFO - root - 2019-11-03 23:30:09.468774: step 28650, total loss = 0.44, predict loss = 0.10 (74.2 examples/sec; 0.054 sec/batch; 89h:23m:00s remains)
INFO - root - 2019-11-03 23:30:10.130163: step 28660, total loss = 0.19, predict loss = 0.04 (67.0 examples/sec; 0.060 sec/batch; 98h:57m:32s remains)
INFO - root - 2019-11-03 23:30:10.789435: step 28670, total loss = 0.64, predict loss = 0.15 (64.0 examples/sec; 0.062 sec/batch; 103h:39m:28s remains)
INFO - root - 2019-11-03 23:30:11.428869: step 28680, total loss = 0.66, predict loss = 0.15 (71.1 examples/sec; 0.056 sec/batch; 93h:18m:05s remains)
INFO - root - 2019-11-03 23:30:12.085923: step 28690, total loss = 0.71, predict loss = 0.17 (66.8 examples/sec; 0.060 sec/batch; 99h:22m:50s remains)
INFO - root - 2019-11-03 23:30:12.718910: step 28700, total loss = 0.79, predict loss = 0.20 (67.0 examples/sec; 0.060 sec/batch; 99h:00m:21s remains)
INFO - root - 2019-11-03 23:30:13.331422: step 28710, total loss = 0.99, predict loss = 0.24 (76.2 examples/sec; 0.052 sec/batch; 87h:01m:12s remains)
INFO - root - 2019-11-03 23:30:13.968475: step 28720, total loss = 1.32, predict loss = 0.30 (61.0 examples/sec; 0.066 sec/batch; 108h:43m:40s remains)
INFO - root - 2019-11-03 23:30:14.623474: step 28730, total loss = 0.96, predict loss = 0.23 (61.8 examples/sec; 0.065 sec/batch; 107h:16m:49s remains)
INFO - root - 2019-11-03 23:30:15.230019: step 28740, total loss = 1.16, predict loss = 0.26 (77.8 examples/sec; 0.051 sec/batch; 85h:14m:35s remains)
INFO - root - 2019-11-03 23:30:15.863653: step 28750, total loss = 0.95, predict loss = 0.24 (71.4 examples/sec; 0.056 sec/batch; 92h:52m:18s remains)
INFO - root - 2019-11-03 23:30:16.508942: step 28760, total loss = 1.09, predict loss = 0.27 (68.6 examples/sec; 0.058 sec/batch; 96h:42m:20s remains)
INFO - root - 2019-11-03 23:30:17.141480: step 28770, total loss = 0.92, predict loss = 0.22 (73.5 examples/sec; 0.054 sec/batch; 90h:16m:35s remains)
INFO - root - 2019-11-03 23:30:17.753443: step 28780, total loss = 0.78, predict loss = 0.18 (70.5 examples/sec; 0.057 sec/batch; 94h:08m:11s remains)
INFO - root - 2019-11-03 23:30:18.404955: step 28790, total loss = 0.71, predict loss = 0.17 (66.8 examples/sec; 0.060 sec/batch; 99h:19m:17s remains)
INFO - root - 2019-11-03 23:30:19.016823: step 28800, total loss = 0.58, predict loss = 0.13 (76.5 examples/sec; 0.052 sec/batch; 86h:45m:39s remains)
INFO - root - 2019-11-03 23:30:19.621672: step 28810, total loss = 0.81, predict loss = 0.19 (67.1 examples/sec; 0.060 sec/batch; 98h:48m:22s remains)
INFO - root - 2019-11-03 23:30:20.258920: step 28820, total loss = 0.58, predict loss = 0.14 (73.6 examples/sec; 0.054 sec/batch; 90h:06m:23s remains)
INFO - root - 2019-11-03 23:30:20.888150: step 28830, total loss = 0.56, predict loss = 0.12 (80.0 examples/sec; 0.050 sec/batch; 82h:56m:06s remains)
INFO - root - 2019-11-03 23:30:21.518982: step 28840, total loss = 0.55, predict loss = 0.12 (64.3 examples/sec; 0.062 sec/batch; 103h:15m:02s remains)
INFO - root - 2019-11-03 23:30:22.200359: step 28850, total loss = 0.66, predict loss = 0.16 (60.6 examples/sec; 0.066 sec/batch; 109h:29m:21s remains)
INFO - root - 2019-11-03 23:30:22.796378: step 28860, total loss = 0.69, predict loss = 0.17 (82.1 examples/sec; 0.049 sec/batch; 80h:47m:22s remains)
INFO - root - 2019-11-03 23:30:23.435286: step 28870, total loss = 0.69, predict loss = 0.17 (73.5 examples/sec; 0.054 sec/batch; 90h:15m:23s remains)
INFO - root - 2019-11-03 23:30:24.117464: step 28880, total loss = 0.75, predict loss = 0.18 (68.1 examples/sec; 0.059 sec/batch; 97h:28m:33s remains)
INFO - root - 2019-11-03 23:30:24.811866: step 28890, total loss = 0.78, predict loss = 0.18 (67.6 examples/sec; 0.059 sec/batch; 98h:04m:54s remains)
INFO - root - 2019-11-03 23:30:25.391119: step 28900, total loss = 0.63, predict loss = 0.14 (73.7 examples/sec; 0.054 sec/batch; 90h:03m:35s remains)
INFO - root - 2019-11-03 23:30:25.994692: step 28910, total loss = 0.78, predict loss = 0.17 (85.9 examples/sec; 0.047 sec/batch; 77h:12m:20s remains)
INFO - root - 2019-11-03 23:30:26.608819: step 28920, total loss = 0.59, predict loss = 0.13 (65.0 examples/sec; 0.062 sec/batch; 102h:02m:44s remains)
INFO - root - 2019-11-03 23:30:27.266477: step 28930, total loss = 0.74, predict loss = 0.19 (66.4 examples/sec; 0.060 sec/batch; 99h:56m:21s remains)
INFO - root - 2019-11-03 23:30:27.910489: step 28940, total loss = 0.41, predict loss = 0.10 (64.2 examples/sec; 0.062 sec/batch; 103h:15m:47s remains)
INFO - root - 2019-11-03 23:30:28.579048: step 28950, total loss = 0.57, predict loss = 0.11 (58.8 examples/sec; 0.068 sec/batch; 112h:51m:58s remains)
INFO - root - 2019-11-03 23:30:29.256396: step 28960, total loss = 0.60, predict loss = 0.14 (63.3 examples/sec; 0.063 sec/batch; 104h:46m:21s remains)
INFO - root - 2019-11-03 23:30:29.951188: step 28970, total loss = 0.50, predict loss = 0.12 (58.1 examples/sec; 0.069 sec/batch; 114h:08m:12s remains)
INFO - root - 2019-11-03 23:30:30.619102: step 28980, total loss = 0.52, predict loss = 0.12 (66.4 examples/sec; 0.060 sec/batch; 99h:53m:03s remains)
INFO - root - 2019-11-03 23:30:31.256389: step 28990, total loss = 0.48, predict loss = 0.10 (69.7 examples/sec; 0.057 sec/batch; 95h:14m:01s remains)
INFO - root - 2019-11-03 23:30:31.912047: step 29000, total loss = 0.60, predict loss = 0.15 (71.6 examples/sec; 0.056 sec/batch; 92h:37m:16s remains)
INFO - root - 2019-11-03 23:30:32.573790: step 29010, total loss = 0.56, predict loss = 0.12 (71.0 examples/sec; 0.056 sec/batch; 93h:29m:51s remains)
INFO - root - 2019-11-03 23:30:33.222713: step 29020, total loss = 0.52, predict loss = 0.12 (73.3 examples/sec; 0.055 sec/batch; 90h:32m:30s remains)
INFO - root - 2019-11-03 23:30:33.882896: step 29030, total loss = 0.43, predict loss = 0.09 (74.6 examples/sec; 0.054 sec/batch; 88h:56m:56s remains)
INFO - root - 2019-11-03 23:30:34.519237: step 29040, total loss = 0.57, predict loss = 0.13 (67.5 examples/sec; 0.059 sec/batch; 98h:21m:27s remains)
INFO - root - 2019-11-03 23:30:35.170397: step 29050, total loss = 0.46, predict loss = 0.10 (65.0 examples/sec; 0.062 sec/batch; 102h:07m:01s remains)
INFO - root - 2019-11-03 23:30:35.813355: step 29060, total loss = 0.61, predict loss = 0.15 (71.7 examples/sec; 0.056 sec/batch; 92h:28m:59s remains)
INFO - root - 2019-11-03 23:30:36.439883: step 29070, total loss = 0.59, predict loss = 0.14 (67.9 examples/sec; 0.059 sec/batch; 97h:43m:50s remains)
INFO - root - 2019-11-03 23:30:37.072201: step 29080, total loss = 0.64, predict loss = 0.16 (67.8 examples/sec; 0.059 sec/batch; 97h:48m:44s remains)
INFO - root - 2019-11-03 23:30:37.727798: step 29090, total loss = 0.48, predict loss = 0.10 (67.0 examples/sec; 0.060 sec/batch; 98h:59m:29s remains)
INFO - root - 2019-11-03 23:30:38.357812: step 29100, total loss = 0.63, predict loss = 0.15 (65.1 examples/sec; 0.061 sec/batch; 101h:50m:28s remains)
INFO - root - 2019-11-03 23:30:39.010189: step 29110, total loss = 0.90, predict loss = 0.21 (68.6 examples/sec; 0.058 sec/batch; 96h:43m:42s remains)
INFO - root - 2019-11-03 23:30:39.633142: step 29120, total loss = 0.76, predict loss = 0.19 (66.9 examples/sec; 0.060 sec/batch; 99h:09m:18s remains)
INFO - root - 2019-11-03 23:30:40.276296: step 29130, total loss = 0.68, predict loss = 0.16 (67.5 examples/sec; 0.059 sec/batch; 98h:15m:03s remains)
INFO - root - 2019-11-03 23:30:40.950293: step 29140, total loss = 0.52, predict loss = 0.12 (66.5 examples/sec; 0.060 sec/batch; 99h:48m:58s remains)
INFO - root - 2019-11-03 23:30:41.624006: step 29150, total loss = 0.84, predict loss = 0.19 (65.6 examples/sec; 0.061 sec/batch; 101h:03m:42s remains)
INFO - root - 2019-11-03 23:30:42.276972: step 29160, total loss = 0.94, predict loss = 0.23 (72.7 examples/sec; 0.055 sec/batch; 91h:17m:10s remains)
INFO - root - 2019-11-03 23:30:42.922476: step 29170, total loss = 1.00, predict loss = 0.26 (73.0 examples/sec; 0.055 sec/batch; 90h:56m:06s remains)
INFO - root - 2019-11-03 23:30:43.587738: step 29180, total loss = 0.75, predict loss = 0.17 (69.7 examples/sec; 0.057 sec/batch; 95h:14m:50s remains)
INFO - root - 2019-11-03 23:30:44.240116: step 29190, total loss = 0.70, predict loss = 0.16 (63.7 examples/sec; 0.063 sec/batch; 104h:13m:41s remains)
INFO - root - 2019-11-03 23:30:44.850526: step 29200, total loss = 0.78, predict loss = 0.19 (71.8 examples/sec; 0.056 sec/batch; 92h:23m:22s remains)
INFO - root - 2019-11-03 23:30:45.477535: step 29210, total loss = 0.65, predict loss = 0.15 (70.1 examples/sec; 0.057 sec/batch; 94h:40m:57s remains)
INFO - root - 2019-11-03 23:30:46.101095: step 29220, total loss = 0.60, predict loss = 0.13 (68.9 examples/sec; 0.058 sec/batch; 96h:20m:43s remains)
INFO - root - 2019-11-03 23:30:46.738964: step 29230, total loss = 0.54, predict loss = 0.12 (76.8 examples/sec; 0.052 sec/batch; 86h:23m:52s remains)
INFO - root - 2019-11-03 23:30:47.364035: step 29240, total loss = 0.56, predict loss = 0.12 (75.7 examples/sec; 0.053 sec/batch; 87h:35m:16s remains)
INFO - root - 2019-11-03 23:30:47.978334: step 29250, total loss = 0.61, predict loss = 0.15 (74.8 examples/sec; 0.053 sec/batch; 88h:43m:07s remains)
INFO - root - 2019-11-03 23:30:48.605925: step 29260, total loss = 0.57, predict loss = 0.13 (83.5 examples/sec; 0.048 sec/batch; 79h:25m:35s remains)
INFO - root - 2019-11-03 23:30:49.238707: step 29270, total loss = 0.54, predict loss = 0.12 (74.1 examples/sec; 0.054 sec/batch; 89h:29m:41s remains)
INFO - root - 2019-11-03 23:30:49.859307: step 29280, total loss = 0.59, predict loss = 0.14 (70.8 examples/sec; 0.056 sec/batch; 93h:41m:07s remains)
INFO - root - 2019-11-03 23:30:50.475257: step 29290, total loss = 0.53, predict loss = 0.12 (74.3 examples/sec; 0.054 sec/batch; 89h:19m:33s remains)
INFO - root - 2019-11-03 23:30:51.083362: step 29300, total loss = 0.72, predict loss = 0.17 (68.8 examples/sec; 0.058 sec/batch; 96h:29m:41s remains)
INFO - root - 2019-11-03 23:30:51.739194: step 29310, total loss = 0.61, predict loss = 0.15 (61.2 examples/sec; 0.065 sec/batch; 108h:21m:01s remains)
INFO - root - 2019-11-03 23:30:52.428498: step 29320, total loss = 0.76, predict loss = 0.17 (74.4 examples/sec; 0.054 sec/batch; 89h:07m:18s remains)
INFO - root - 2019-11-03 23:30:53.044409: step 29330, total loss = 0.70, predict loss = 0.16 (71.2 examples/sec; 0.056 sec/batch; 93h:12m:14s remains)
INFO - root - 2019-11-03 23:30:53.675014: step 29340, total loss = 0.87, predict loss = 0.20 (67.6 examples/sec; 0.059 sec/batch; 98h:11m:13s remains)
INFO - root - 2019-11-03 23:30:54.320638: step 29350, total loss = 0.89, predict loss = 0.22 (70.0 examples/sec; 0.057 sec/batch; 94h:50m:12s remains)
INFO - root - 2019-11-03 23:30:54.986313: step 29360, total loss = 0.79, predict loss = 0.18 (60.4 examples/sec; 0.066 sec/batch; 109h:46m:38s remains)
INFO - root - 2019-11-03 23:30:55.659624: step 29370, total loss = 0.69, predict loss = 0.17 (72.3 examples/sec; 0.055 sec/batch; 91h:43m:31s remains)
INFO - root - 2019-11-03 23:30:56.289817: step 29380, total loss = 0.99, predict loss = 0.24 (68.2 examples/sec; 0.059 sec/batch; 97h:19m:17s remains)
INFO - root - 2019-11-03 23:30:56.948084: step 29390, total loss = 0.70, predict loss = 0.17 (78.5 examples/sec; 0.051 sec/batch; 84h:30m:25s remains)
INFO - root - 2019-11-03 23:30:57.551831: step 29400, total loss = 0.73, predict loss = 0.18 (76.9 examples/sec; 0.052 sec/batch; 86h:16m:18s remains)
INFO - root - 2019-11-03 23:30:58.169031: step 29410, total loss = 0.76, predict loss = 0.19 (67.4 examples/sec; 0.059 sec/batch; 98h:25m:46s remains)
INFO - root - 2019-11-03 23:30:58.883933: step 29420, total loss = 0.74, predict loss = 0.17 (54.2 examples/sec; 0.074 sec/batch; 122h:20m:35s remains)
INFO - root - 2019-11-03 23:30:59.516322: step 29430, total loss = 0.61, predict loss = 0.15 (69.5 examples/sec; 0.058 sec/batch; 95h:29m:31s remains)
INFO - root - 2019-11-03 23:31:00.097705: step 29440, total loss = 0.67, predict loss = 0.16 (77.7 examples/sec; 0.051 sec/batch; 85h:19m:46s remains)
INFO - root - 2019-11-03 23:31:00.725494: step 29450, total loss = 0.61, predict loss = 0.15 (74.8 examples/sec; 0.053 sec/batch; 88h:42m:20s remains)
INFO - root - 2019-11-03 23:31:01.362411: step 29460, total loss = 0.65, predict loss = 0.15 (63.4 examples/sec; 0.063 sec/batch; 104h:33m:54s remains)
INFO - root - 2019-11-03 23:31:01.985947: step 29470, total loss = 0.63, predict loss = 0.14 (74.4 examples/sec; 0.054 sec/batch; 89h:07m:35s remains)
INFO - root - 2019-11-03 23:31:02.632535: step 29480, total loss = 0.60, predict loss = 0.14 (71.3 examples/sec; 0.056 sec/batch; 93h:03m:45s remains)
INFO - root - 2019-11-03 23:31:03.274455: step 29490, total loss = 0.59, predict loss = 0.14 (67.5 examples/sec; 0.059 sec/batch; 98h:14m:22s remains)
INFO - root - 2019-11-03 23:31:03.908978: step 29500, total loss = 0.52, predict loss = 0.12 (71.0 examples/sec; 0.056 sec/batch; 93h:23m:58s remains)
INFO - root - 2019-11-03 23:31:04.536537: step 29510, total loss = 0.71, predict loss = 0.17 (77.1 examples/sec; 0.052 sec/batch; 86h:05m:23s remains)
INFO - root - 2019-11-03 23:31:05.166692: step 29520, total loss = 0.66, predict loss = 0.16 (71.2 examples/sec; 0.056 sec/batch; 93h:07m:09s remains)
INFO - root - 2019-11-03 23:31:05.838156: step 29530, total loss = 0.52, predict loss = 0.12 (66.0 examples/sec; 0.061 sec/batch; 100h:28m:08s remains)
INFO - root - 2019-11-03 23:31:06.531598: step 29540, total loss = 0.45, predict loss = 0.10 (78.2 examples/sec; 0.051 sec/batch; 84h:49m:13s remains)
INFO - root - 2019-11-03 23:31:07.179677: step 29550, total loss = 0.44, predict loss = 0.10 (64.1 examples/sec; 0.062 sec/batch; 103h:31m:36s remains)
INFO - root - 2019-11-03 23:31:07.805761: step 29560, total loss = 0.69, predict loss = 0.16 (72.7 examples/sec; 0.055 sec/batch; 91h:12m:23s remains)
INFO - root - 2019-11-03 23:31:08.424569: step 29570, total loss = 0.55, predict loss = 0.13 (63.8 examples/sec; 0.063 sec/batch; 103h:54m:02s remains)
INFO - root - 2019-11-03 23:31:09.095017: step 29580, total loss = 0.48, predict loss = 0.10 (73.6 examples/sec; 0.054 sec/batch; 90h:11m:01s remains)
INFO - root - 2019-11-03 23:31:09.710576: step 29590, total loss = 0.57, predict loss = 0.13 (75.5 examples/sec; 0.053 sec/batch; 87h:53m:31s remains)
INFO - root - 2019-11-03 23:31:10.353636: step 29600, total loss = 0.53, predict loss = 0.11 (65.1 examples/sec; 0.061 sec/batch; 101h:56m:06s remains)
INFO - root - 2019-11-03 23:31:10.963337: step 29610, total loss = 0.50, predict loss = 0.10 (71.0 examples/sec; 0.056 sec/batch; 93h:26m:40s remains)
INFO - root - 2019-11-03 23:31:11.580890: step 29620, total loss = 0.63, predict loss = 0.15 (67.6 examples/sec; 0.059 sec/batch; 98h:06m:19s remains)
INFO - root - 2019-11-03 23:31:12.203221: step 29630, total loss = 0.81, predict loss = 0.20 (79.9 examples/sec; 0.050 sec/batch; 83h:02m:13s remains)
INFO - root - 2019-11-03 23:31:12.850452: step 29640, total loss = 0.71, predict loss = 0.17 (66.2 examples/sec; 0.060 sec/batch; 100h:08m:58s remains)
INFO - root - 2019-11-03 23:31:13.494617: step 29650, total loss = 0.63, predict loss = 0.15 (72.7 examples/sec; 0.055 sec/batch; 91h:11m:34s remains)
INFO - root - 2019-11-03 23:31:14.141495: step 29660, total loss = 0.62, predict loss = 0.15 (71.2 examples/sec; 0.056 sec/batch; 93h:10m:15s remains)
INFO - root - 2019-11-03 23:31:14.783437: step 29670, total loss = 0.64, predict loss = 0.15 (61.6 examples/sec; 0.065 sec/batch; 107h:41m:00s remains)
INFO - root - 2019-11-03 23:31:15.420235: step 29680, total loss = 0.76, predict loss = 0.19 (71.1 examples/sec; 0.056 sec/batch; 93h:20m:20s remains)
INFO - root - 2019-11-03 23:31:16.060052: step 29690, total loss = 0.59, predict loss = 0.14 (80.9 examples/sec; 0.049 sec/batch; 82h:01m:59s remains)
INFO - root - 2019-11-03 23:31:16.684615: step 29700, total loss = 0.55, predict loss = 0.11 (69.1 examples/sec; 0.058 sec/batch; 95h:58m:20s remains)
INFO - root - 2019-11-03 23:31:17.342339: step 29710, total loss = 0.60, predict loss = 0.15 (67.5 examples/sec; 0.059 sec/batch; 98h:15m:43s remains)
INFO - root - 2019-11-03 23:31:18.001969: step 29720, total loss = 0.62, predict loss = 0.16 (72.7 examples/sec; 0.055 sec/batch; 91h:18m:11s remains)
INFO - root - 2019-11-03 23:31:18.648330: step 29730, total loss = 0.70, predict loss = 0.16 (78.9 examples/sec; 0.051 sec/batch; 84h:04m:56s remains)
INFO - root - 2019-11-03 23:31:19.295309: step 29740, total loss = 0.79, predict loss = 0.19 (67.3 examples/sec; 0.059 sec/batch; 98h:31m:45s remains)
INFO - root - 2019-11-03 23:31:19.977711: step 29750, total loss = 0.76, predict loss = 0.17 (60.4 examples/sec; 0.066 sec/batch; 109h:46m:22s remains)
INFO - root - 2019-11-03 23:31:20.632429: step 29760, total loss = 0.69, predict loss = 0.17 (68.4 examples/sec; 0.059 sec/batch; 97h:02m:48s remains)
INFO - root - 2019-11-03 23:31:21.267071: step 29770, total loss = 0.70, predict loss = 0.16 (66.5 examples/sec; 0.060 sec/batch; 99h:42m:30s remains)
INFO - root - 2019-11-03 23:31:21.887188: step 29780, total loss = 0.77, predict loss = 0.19 (69.6 examples/sec; 0.058 sec/batch; 95h:21m:39s remains)
INFO - root - 2019-11-03 23:31:22.563408: step 29790, total loss = 0.75, predict loss = 0.18 (73.8 examples/sec; 0.054 sec/batch; 89h:56m:08s remains)
INFO - root - 2019-11-03 23:31:23.181926: step 29800, total loss = 0.79, predict loss = 0.19 (63.7 examples/sec; 0.063 sec/batch; 104h:09m:50s remains)
INFO - root - 2019-11-03 23:31:23.799073: step 29810, total loss = 0.64, predict loss = 0.14 (87.9 examples/sec; 0.046 sec/batch; 75h:27m:42s remains)
INFO - root - 2019-11-03 23:31:24.446324: step 29820, total loss = 1.03, predict loss = 0.24 (65.6 examples/sec; 0.061 sec/batch; 101h:10m:49s remains)
INFO - root - 2019-11-03 23:31:25.081155: step 29830, total loss = 0.81, predict loss = 0.19 (78.1 examples/sec; 0.051 sec/batch; 84h:56m:54s remains)
INFO - root - 2019-11-03 23:31:25.713893: step 29840, total loss = 0.69, predict loss = 0.16 (81.5 examples/sec; 0.049 sec/batch; 81h:26m:24s remains)
INFO - root - 2019-11-03 23:31:26.363046: step 29850, total loss = 0.90, predict loss = 0.21 (72.3 examples/sec; 0.055 sec/batch; 91h:41m:20s remains)
INFO - root - 2019-11-03 23:31:26.996914: step 29860, total loss = 0.68, predict loss = 0.16 (76.6 examples/sec; 0.052 sec/batch; 86h:36m:08s remains)
INFO - root - 2019-11-03 23:31:27.723359: step 29870, total loss = 0.61, predict loss = 0.15 (62.4 examples/sec; 0.064 sec/batch; 106h:13m:47s remains)
INFO - root - 2019-11-03 23:31:28.406209: step 29880, total loss = 0.83, predict loss = 0.20 (78.5 examples/sec; 0.051 sec/batch; 84h:31m:57s remains)
INFO - root - 2019-11-03 23:31:29.093975: step 29890, total loss = 0.83, predict loss = 0.19 (61.4 examples/sec; 0.065 sec/batch; 108h:03m:24s remains)
INFO - root - 2019-11-03 23:31:29.704467: step 29900, total loss = 0.57, predict loss = 0.14 (67.8 examples/sec; 0.059 sec/batch; 97h:49m:53s remains)
INFO - root - 2019-11-03 23:31:30.347536: step 29910, total loss = 0.83, predict loss = 0.21 (63.3 examples/sec; 0.063 sec/batch; 104h:45m:51s remains)
INFO - root - 2019-11-03 23:31:31.004738: step 29920, total loss = 0.77, predict loss = 0.19 (62.6 examples/sec; 0.064 sec/batch; 106h:01m:11s remains)
INFO - root - 2019-11-03 23:31:31.672275: step 29930, total loss = 0.76, predict loss = 0.19 (70.9 examples/sec; 0.056 sec/batch; 93h:34m:28s remains)
INFO - root - 2019-11-03 23:31:32.374835: step 29940, total loss = 0.76, predict loss = 0.18 (65.8 examples/sec; 0.061 sec/batch; 100h:49m:20s remains)
INFO - root - 2019-11-03 23:31:33.052776: step 29950, total loss = 0.83, predict loss = 0.20 (73.7 examples/sec; 0.054 sec/batch; 89h:56m:56s remains)
INFO - root - 2019-11-03 23:31:33.666680: step 29960, total loss = 0.69, predict loss = 0.16 (76.5 examples/sec; 0.052 sec/batch; 86h:39m:48s remains)
INFO - root - 2019-11-03 23:31:34.306655: step 29970, total loss = 0.38, predict loss = 0.07 (67.3 examples/sec; 0.059 sec/batch; 98h:37m:34s remains)
INFO - root - 2019-11-03 23:31:34.894640: step 29980, total loss = 0.60, predict loss = 0.14 (89.1 examples/sec; 0.045 sec/batch; 74h:27m:59s remains)
INFO - root - 2019-11-03 23:31:35.443770: step 29990, total loss = 0.66, predict loss = 0.14 (95.6 examples/sec; 0.042 sec/batch; 69h:24m:49s remains)
INFO - root - 2019-11-03 23:31:36.471029: step 30000, total loss = 0.55, predict loss = 0.13 (6.9 examples/sec; 0.582 sec/batch; 964h:33m:22s remains)
INFO - root - 2019-11-03 23:31:37.785515: step 30010, total loss = 0.62, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 94h:05m:34s remains)
INFO - root - 2019-11-03 23:31:38.423659: step 30020, total loss = 0.70, predict loss = 0.18 (65.8 examples/sec; 0.061 sec/batch; 100h:44m:53s remains)
INFO - root - 2019-11-03 23:31:39.055163: step 30030, total loss = 0.46, predict loss = 0.10 (78.4 examples/sec; 0.051 sec/batch; 84h:34m:49s remains)
INFO - root - 2019-11-03 23:31:39.666751: step 30040, total loss = 0.61, predict loss = 0.14 (76.7 examples/sec; 0.052 sec/batch; 86h:26m:35s remains)
INFO - root - 2019-11-03 23:31:40.320732: step 30050, total loss = 0.73, predict loss = 0.16 (69.3 examples/sec; 0.058 sec/batch; 95h:42m:13s remains)
INFO - root - 2019-11-03 23:31:40.971813: step 30060, total loss = 0.65, predict loss = 0.15 (71.0 examples/sec; 0.056 sec/batch; 93h:25m:05s remains)
INFO - root - 2019-11-03 23:31:41.591325: step 30070, total loss = 0.78, predict loss = 0.18 (90.2 examples/sec; 0.044 sec/batch; 73h:34m:25s remains)
INFO - root - 2019-11-03 23:31:42.250298: step 30080, total loss = 0.77, predict loss = 0.18 (70.0 examples/sec; 0.057 sec/batch; 94h:44m:59s remains)
INFO - root - 2019-11-03 23:31:42.882722: step 30090, total loss = 0.94, predict loss = 0.23 (79.3 examples/sec; 0.050 sec/batch; 83h:38m:22s remains)
INFO - root - 2019-11-03 23:31:43.546069: step 30100, total loss = 1.09, predict loss = 0.26 (65.4 examples/sec; 0.061 sec/batch; 101h:24m:58s remains)
INFO - root - 2019-11-03 23:31:44.182192: step 30110, total loss = 0.60, predict loss = 0.13 (71.7 examples/sec; 0.056 sec/batch; 92h:30m:54s remains)
INFO - root - 2019-11-03 23:31:44.799234: step 30120, total loss = 0.83, predict loss = 0.20 (80.2 examples/sec; 0.050 sec/batch; 82h:41m:59s remains)
INFO - root - 2019-11-03 23:31:45.428584: step 30130, total loss = 0.67, predict loss = 0.15 (74.2 examples/sec; 0.054 sec/batch; 89h:27m:05s remains)
INFO - root - 2019-11-03 23:31:46.053222: step 30140, total loss = 0.67, predict loss = 0.16 (68.5 examples/sec; 0.058 sec/batch; 96h:48m:29s remains)
INFO - root - 2019-11-03 23:31:46.707245: step 30150, total loss = 0.64, predict loss = 0.15 (68.9 examples/sec; 0.058 sec/batch; 96h:20m:02s remains)
INFO - root - 2019-11-03 23:31:47.357274: step 30160, total loss = 0.64, predict loss = 0.15 (68.3 examples/sec; 0.059 sec/batch; 97h:04m:03s remains)
INFO - root - 2019-11-03 23:31:47.990562: step 30170, total loss = 0.68, predict loss = 0.15 (73.0 examples/sec; 0.055 sec/batch; 90h:53m:56s remains)
INFO - root - 2019-11-03 23:31:48.694448: step 30180, total loss = 0.45, predict loss = 0.09 (61.6 examples/sec; 0.065 sec/batch; 107h:42m:18s remains)
INFO - root - 2019-11-03 23:31:49.308497: step 30190, total loss = 0.75, predict loss = 0.17 (82.8 examples/sec; 0.048 sec/batch; 80h:04m:52s remains)
INFO - root - 2019-11-03 23:31:49.927726: step 30200, total loss = 0.58, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 91h:53m:17s remains)
INFO - root - 2019-11-03 23:31:50.559303: step 30210, total loss = 0.41, predict loss = 0.09 (65.6 examples/sec; 0.061 sec/batch; 101h:09m:27s remains)
INFO - root - 2019-11-03 23:31:51.171631: step 30220, total loss = 0.60, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 101h:31m:32s remains)
INFO - root - 2019-11-03 23:31:51.823487: step 30230, total loss = 0.72, predict loss = 0.17 (71.1 examples/sec; 0.056 sec/batch; 93h:17m:45s remains)
INFO - root - 2019-11-03 23:31:52.502099: step 30240, total loss = 0.63, predict loss = 0.15 (77.6 examples/sec; 0.052 sec/batch; 85h:26m:25s remains)
INFO - root - 2019-11-03 23:31:53.185513: step 30250, total loss = 0.75, predict loss = 0.17 (69.1 examples/sec; 0.058 sec/batch; 96h:01m:12s remains)
INFO - root - 2019-11-03 23:31:53.828153: step 30260, total loss = 1.01, predict loss = 0.23 (62.9 examples/sec; 0.064 sec/batch; 105h:25m:13s remains)
INFO - root - 2019-11-03 23:31:54.442135: step 30270, total loss = 0.70, predict loss = 0.16 (65.8 examples/sec; 0.061 sec/batch; 100h:43m:54s remains)
INFO - root - 2019-11-03 23:31:55.093020: step 30280, total loss = 0.66, predict loss = 0.15 (64.8 examples/sec; 0.062 sec/batch; 102h:26m:20s remains)
INFO - root - 2019-11-03 23:31:55.724188: step 30290, total loss = 0.70, predict loss = 0.16 (58.9 examples/sec; 0.068 sec/batch; 112h:35m:03s remains)
INFO - root - 2019-11-03 23:31:56.375732: step 30300, total loss = 0.64, predict loss = 0.15 (72.0 examples/sec; 0.056 sec/batch; 92h:08m:25s remains)
INFO - root - 2019-11-03 23:31:57.038554: step 30310, total loss = 0.83, predict loss = 0.18 (58.4 examples/sec; 0.069 sec/batch; 113h:39m:23s remains)
INFO - root - 2019-11-03 23:31:57.727034: step 30320, total loss = 0.66, predict loss = 0.15 (67.1 examples/sec; 0.060 sec/batch; 98h:50m:56s remains)
INFO - root - 2019-11-03 23:31:58.323474: step 30330, total loss = 0.80, predict loss = 0.19 (75.1 examples/sec; 0.053 sec/batch; 88h:19m:37s remains)
INFO - root - 2019-11-03 23:31:58.987647: step 30340, total loss = 0.76, predict loss = 0.17 (64.6 examples/sec; 0.062 sec/batch; 102h:43m:13s remains)
INFO - root - 2019-11-03 23:31:59.636435: step 30350, total loss = 0.74, predict loss = 0.17 (66.3 examples/sec; 0.060 sec/batch; 100h:01m:25s remains)
INFO - root - 2019-11-03 23:32:00.281909: step 30360, total loss = 0.64, predict loss = 0.14 (73.7 examples/sec; 0.054 sec/batch; 89h:57m:35s remains)
INFO - root - 2019-11-03 23:32:00.909339: step 30370, total loss = 0.67, predict loss = 0.15 (67.4 examples/sec; 0.059 sec/batch; 98h:27m:34s remains)
INFO - root - 2019-11-03 23:32:01.515844: step 30380, total loss = 0.62, predict loss = 0.14 (70.8 examples/sec; 0.056 sec/batch; 93h:41m:06s remains)
INFO - root - 2019-11-03 23:32:02.214555: step 30390, total loss = 0.72, predict loss = 0.16 (64.7 examples/sec; 0.062 sec/batch; 102h:27m:10s remains)
INFO - root - 2019-11-03 23:32:02.902233: step 30400, total loss = 0.70, predict loss = 0.16 (69.0 examples/sec; 0.058 sec/batch; 96h:07m:00s remains)
INFO - root - 2019-11-03 23:32:03.531607: step 30410, total loss = 0.75, predict loss = 0.16 (63.0 examples/sec; 0.064 sec/batch; 105h:20m:46s remains)
INFO - root - 2019-11-03 23:32:04.203243: step 30420, total loss = 0.75, predict loss = 0.16 (58.3 examples/sec; 0.069 sec/batch; 113h:50m:58s remains)
INFO - root - 2019-11-03 23:32:04.838415: step 30430, total loss = 0.80, predict loss = 0.19 (74.2 examples/sec; 0.054 sec/batch; 89h:24m:03s remains)
INFO - root - 2019-11-03 23:32:05.494391: step 30440, total loss = 0.64, predict loss = 0.15 (79.2 examples/sec; 0.050 sec/batch; 83h:42m:17s remains)
INFO - root - 2019-11-03 23:32:06.177427: step 30450, total loss = 0.53, predict loss = 0.12 (69.3 examples/sec; 0.058 sec/batch; 95h:46m:36s remains)
INFO - root - 2019-11-03 23:32:06.884888: step 30460, total loss = 0.59, predict loss = 0.12 (58.0 examples/sec; 0.069 sec/batch; 114h:19m:49s remains)
INFO - root - 2019-11-03 23:32:07.500912: step 30470, total loss = 0.86, predict loss = 0.20 (74.3 examples/sec; 0.054 sec/batch; 89h:14m:42s remains)
INFO - root - 2019-11-03 23:32:08.111240: step 30480, total loss = 0.57, predict loss = 0.13 (68.8 examples/sec; 0.058 sec/batch; 96h:21m:37s remains)
INFO - root - 2019-11-03 23:32:08.733336: step 30490, total loss = 0.56, predict loss = 0.14 (85.4 examples/sec; 0.047 sec/batch; 77h:40m:42s remains)
INFO - root - 2019-11-03 23:32:09.385700: step 30500, total loss = 0.64, predict loss = 0.15 (65.9 examples/sec; 0.061 sec/batch; 100h:41m:26s remains)
INFO - root - 2019-11-03 23:32:10.021069: step 30510, total loss = 0.76, predict loss = 0.18 (70.4 examples/sec; 0.057 sec/batch; 94h:12m:20s remains)
INFO - root - 2019-11-03 23:32:10.654018: step 30520, total loss = 0.85, predict loss = 0.20 (72.0 examples/sec; 0.056 sec/batch; 92h:10m:05s remains)
INFO - root - 2019-11-03 23:32:11.290443: step 30530, total loss = 0.81, predict loss = 0.20 (70.0 examples/sec; 0.057 sec/batch; 94h:45m:36s remains)
INFO - root - 2019-11-03 23:32:11.982858: step 30540, total loss = 0.68, predict loss = 0.17 (57.4 examples/sec; 0.070 sec/batch; 115h:37m:47s remains)
INFO - root - 2019-11-03 23:32:12.614266: step 30550, total loss = 0.50, predict loss = 0.12 (79.0 examples/sec; 0.051 sec/batch; 83h:55m:27s remains)
INFO - root - 2019-11-03 23:32:13.223946: step 30560, total loss = 0.56, predict loss = 0.13 (73.6 examples/sec; 0.054 sec/batch; 90h:05m:30s remains)
INFO - root - 2019-11-03 23:32:13.876948: step 30570, total loss = 0.58, predict loss = 0.14 (76.0 examples/sec; 0.053 sec/batch; 87h:14m:51s remains)
INFO - root - 2019-11-03 23:32:14.552088: step 30580, total loss = 0.73, predict loss = 0.21 (63.2 examples/sec; 0.063 sec/batch; 105h:00m:08s remains)
INFO - root - 2019-11-03 23:32:15.187818: step 30590, total loss = 0.48, predict loss = 0.11 (74.2 examples/sec; 0.054 sec/batch; 89h:26m:57s remains)
INFO - root - 2019-11-03 23:32:15.795995: step 30600, total loss = 0.57, predict loss = 0.14 (78.6 examples/sec; 0.051 sec/batch; 84h:21m:31s remains)
INFO - root - 2019-11-03 23:32:16.393274: step 30610, total loss = 0.59, predict loss = 0.14 (76.5 examples/sec; 0.052 sec/batch; 86h:44m:15s remains)
INFO - root - 2019-11-03 23:32:17.001436: step 30620, total loss = 0.67, predict loss = 0.15 (67.5 examples/sec; 0.059 sec/batch; 98h:17m:03s remains)
INFO - root - 2019-11-03 23:32:17.681246: step 30630, total loss = 0.62, predict loss = 0.14 (63.1 examples/sec; 0.063 sec/batch; 105h:04m:55s remains)
INFO - root - 2019-11-03 23:32:18.309188: step 30640, total loss = 0.56, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 91h:52m:23s remains)
INFO - root - 2019-11-03 23:32:18.945849: step 30650, total loss = 0.76, predict loss = 0.18 (74.0 examples/sec; 0.054 sec/batch; 89h:38m:10s remains)
INFO - root - 2019-11-03 23:32:19.548976: step 30660, total loss = 0.59, predict loss = 0.13 (74.8 examples/sec; 0.053 sec/batch; 88h:39m:27s remains)
INFO - root - 2019-11-03 23:32:20.161549: step 30670, total loss = 0.79, predict loss = 0.18 (72.7 examples/sec; 0.055 sec/batch; 91h:16m:02s remains)
INFO - root - 2019-11-03 23:32:20.808671: step 30680, total loss = 0.59, predict loss = 0.13 (63.7 examples/sec; 0.063 sec/batch; 104h:09m:32s remains)
INFO - root - 2019-11-03 23:32:21.467156: step 30690, total loss = 0.67, predict loss = 0.15 (80.1 examples/sec; 0.050 sec/batch; 82h:50m:23s remains)
INFO - root - 2019-11-03 23:32:22.081820: step 30700, total loss = 0.68, predict loss = 0.16 (64.9 examples/sec; 0.062 sec/batch; 102h:14m:57s remains)
INFO - root - 2019-11-03 23:32:22.728403: step 30710, total loss = 0.73, predict loss = 0.18 (82.4 examples/sec; 0.049 sec/batch; 80h:31m:23s remains)
INFO - root - 2019-11-03 23:32:23.371182: step 30720, total loss = 0.81, predict loss = 0.20 (66.3 examples/sec; 0.060 sec/batch; 100h:01m:16s remains)
INFO - root - 2019-11-03 23:32:24.058489: step 30730, total loss = 0.74, predict loss = 0.18 (61.1 examples/sec; 0.065 sec/batch; 108h:34m:37s remains)
INFO - root - 2019-11-03 23:32:24.677095: step 30740, total loss = 0.81, predict loss = 0.18 (76.0 examples/sec; 0.053 sec/batch; 87h:18m:44s remains)
INFO - root - 2019-11-03 23:32:25.293770: step 30750, total loss = 0.58, predict loss = 0.13 (77.5 examples/sec; 0.052 sec/batch; 85h:36m:28s remains)
INFO - root - 2019-11-03 23:32:25.929811: step 30760, total loss = 0.58, predict loss = 0.12 (67.2 examples/sec; 0.059 sec/batch; 98h:38m:59s remains)
INFO - root - 2019-11-03 23:32:26.555225: step 30770, total loss = 0.54, predict loss = 0.13 (74.4 examples/sec; 0.054 sec/batch; 89h:09m:04s remains)
INFO - root - 2019-11-03 23:32:27.207510: step 30780, total loss = 0.47, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 99h:40m:21s remains)
INFO - root - 2019-11-03 23:32:27.883916: step 30790, total loss = 0.72, predict loss = 0.17 (72.9 examples/sec; 0.055 sec/batch; 91h:01m:49s remains)
INFO - root - 2019-11-03 23:32:28.535788: step 30800, total loss = 0.73, predict loss = 0.16 (71.1 examples/sec; 0.056 sec/batch; 93h:14m:31s remains)
INFO - root - 2019-11-03 23:32:29.206500: step 30810, total loss = 0.44, predict loss = 0.10 (71.3 examples/sec; 0.056 sec/batch; 93h:02m:44s remains)
INFO - root - 2019-11-03 23:32:29.849489: step 30820, total loss = 0.38, predict loss = 0.08 (80.7 examples/sec; 0.050 sec/batch; 82h:12m:56s remains)
INFO - root - 2019-11-03 23:32:30.494211: step 30830, total loss = 0.54, predict loss = 0.13 (64.2 examples/sec; 0.062 sec/batch; 103h:21m:24s remains)
INFO - root - 2019-11-03 23:32:31.191121: step 30840, total loss = 0.45, predict loss = 0.10 (64.1 examples/sec; 0.062 sec/batch; 103h:31m:23s remains)
INFO - root - 2019-11-03 23:32:31.840069: step 30850, total loss = 0.43, predict loss = 0.09 (72.8 examples/sec; 0.055 sec/batch; 91h:06m:16s remains)
INFO - root - 2019-11-03 23:32:32.451151: step 30860, total loss = 0.47, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 94h:02m:38s remains)
INFO - root - 2019-11-03 23:32:33.079827: step 30870, total loss = 0.44, predict loss = 0.10 (63.3 examples/sec; 0.063 sec/batch; 104h:47m:31s remains)
INFO - root - 2019-11-03 23:32:33.781400: step 30880, total loss = 0.55, predict loss = 0.11 (64.2 examples/sec; 0.062 sec/batch; 103h:14m:29s remains)
INFO - root - 2019-11-03 23:32:34.389280: step 30890, total loss = 0.39, predict loss = 0.08 (74.0 examples/sec; 0.054 sec/batch; 89h:40m:00s remains)
INFO - root - 2019-11-03 23:32:35.039298: step 30900, total loss = 0.46, predict loss = 0.10 (68.4 examples/sec; 0.058 sec/batch; 96h:54m:55s remains)
INFO - root - 2019-11-03 23:32:35.746215: step 30910, total loss = 0.44, predict loss = 0.10 (65.9 examples/sec; 0.061 sec/batch; 100h:38m:07s remains)
INFO - root - 2019-11-03 23:32:36.376597: step 30920, total loss = 0.53, predict loss = 0.12 (78.6 examples/sec; 0.051 sec/batch; 84h:24m:33s remains)
INFO - root - 2019-11-03 23:32:36.974300: step 30930, total loss = 0.55, predict loss = 0.13 (72.4 examples/sec; 0.055 sec/batch; 91h:35m:48s remains)
INFO - root - 2019-11-03 23:32:37.581411: step 30940, total loss = 0.70, predict loss = 0.16 (66.9 examples/sec; 0.060 sec/batch; 99h:09m:37s remains)
INFO - root - 2019-11-03 23:32:38.217241: step 30950, total loss = 0.45, predict loss = 0.10 (70.3 examples/sec; 0.057 sec/batch; 94h:24m:28s remains)
INFO - root - 2019-11-03 23:32:38.822115: step 30960, total loss = 0.61, predict loss = 0.13 (77.7 examples/sec; 0.052 sec/batch; 85h:24m:27s remains)
INFO - root - 2019-11-03 23:32:39.437182: step 30970, total loss = 0.54, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 94h:01m:23s remains)
INFO - root - 2019-11-03 23:32:40.104908: step 30980, total loss = 0.54, predict loss = 0.13 (67.2 examples/sec; 0.059 sec/batch; 98h:39m:09s remains)
INFO - root - 2019-11-03 23:32:40.751792: step 30990, total loss = 0.68, predict loss = 0.15 (73.9 examples/sec; 0.054 sec/batch; 89h:44m:47s remains)
INFO - root - 2019-11-03 23:32:41.386641: step 31000, total loss = 0.57, predict loss = 0.13 (77.4 examples/sec; 0.052 sec/batch; 85h:41m:35s remains)
INFO - root - 2019-11-03 23:32:42.024609: step 31010, total loss = 0.58, predict loss = 0.14 (67.5 examples/sec; 0.059 sec/batch; 98h:15m:00s remains)
INFO - root - 2019-11-03 23:32:42.661831: step 31020, total loss = 0.61, predict loss = 0.13 (71.5 examples/sec; 0.056 sec/batch; 92h:42m:53s remains)
INFO - root - 2019-11-03 23:32:43.354540: step 31030, total loss = 0.58, predict loss = 0.13 (69.5 examples/sec; 0.058 sec/batch; 95h:29m:31s remains)
INFO - root - 2019-11-03 23:32:43.952675: step 31040, total loss = 0.39, predict loss = 0.08 (68.4 examples/sec; 0.058 sec/batch; 96h:56m:22s remains)
INFO - root - 2019-11-03 23:32:44.549227: step 31050, total loss = 0.87, predict loss = 0.20 (71.6 examples/sec; 0.056 sec/batch; 92h:35m:50s remains)
INFO - root - 2019-11-03 23:32:45.162155: step 31060, total loss = 0.25, predict loss = 0.05 (69.5 examples/sec; 0.058 sec/batch; 95h:24m:28s remains)
INFO - root - 2019-11-03 23:32:45.825742: step 31070, total loss = 0.41, predict loss = 0.10 (63.8 examples/sec; 0.063 sec/batch; 103h:56m:09s remains)
INFO - root - 2019-11-03 23:32:46.499537: step 31080, total loss = 0.33, predict loss = 0.07 (71.3 examples/sec; 0.056 sec/batch; 92h:59m:13s remains)
INFO - root - 2019-11-03 23:32:47.114973: step 31090, total loss = 0.37, predict loss = 0.07 (81.8 examples/sec; 0.049 sec/batch; 81h:03m:30s remains)
INFO - root - 2019-11-03 23:32:47.749919: step 31100, total loss = 0.44, predict loss = 0.10 (71.0 examples/sec; 0.056 sec/batch; 93h:24m:40s remains)
INFO - root - 2019-11-03 23:32:48.381392: step 31110, total loss = 0.40, predict loss = 0.09 (64.2 examples/sec; 0.062 sec/batch; 103h:18m:47s remains)
INFO - root - 2019-11-03 23:32:49.009387: step 31120, total loss = 0.46, predict loss = 0.10 (72.8 examples/sec; 0.055 sec/batch; 91h:04m:23s remains)
INFO - root - 2019-11-03 23:32:49.625683: step 31130, total loss = 0.69, predict loss = 0.16 (74.7 examples/sec; 0.054 sec/batch; 88h:45m:39s remains)
INFO - root - 2019-11-03 23:32:50.251760: step 31140, total loss = 0.40, predict loss = 0.09 (67.1 examples/sec; 0.060 sec/batch; 98h:46m:43s remains)
INFO - root - 2019-11-03 23:32:50.896653: step 31150, total loss = 0.51, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 97h:59m:56s remains)
INFO - root - 2019-11-03 23:32:51.573093: step 31160, total loss = 0.67, predict loss = 0.15 (69.1 examples/sec; 0.058 sec/batch; 95h:59m:47s remains)
INFO - root - 2019-11-03 23:32:52.261694: step 31170, total loss = 0.75, predict loss = 0.18 (63.3 examples/sec; 0.063 sec/batch; 104h:48m:02s remains)
INFO - root - 2019-11-03 23:32:52.892084: step 31180, total loss = 0.60, predict loss = 0.14 (76.4 examples/sec; 0.052 sec/batch; 86h:50m:33s remains)
INFO - root - 2019-11-03 23:32:53.517440: step 31190, total loss = 0.50, predict loss = 0.10 (59.6 examples/sec; 0.067 sec/batch; 111h:15m:27s remains)
INFO - root - 2019-11-03 23:32:54.130746: step 31200, total loss = 0.50, predict loss = 0.11 (68.8 examples/sec; 0.058 sec/batch; 96h:20m:52s remains)
INFO - root - 2019-11-03 23:32:54.748236: step 31210, total loss = 0.63, predict loss = 0.15 (69.0 examples/sec; 0.058 sec/batch; 96h:10m:01s remains)
INFO - root - 2019-11-03 23:32:55.358085: step 31220, total loss = 0.56, predict loss = 0.15 (82.8 examples/sec; 0.048 sec/batch; 80h:04m:47s remains)
INFO - root - 2019-11-03 23:32:55.991023: step 31230, total loss = 0.48, predict loss = 0.10 (74.5 examples/sec; 0.054 sec/batch; 88h:57m:38s remains)
INFO - root - 2019-11-03 23:32:56.634392: step 31240, total loss = 0.41, predict loss = 0.09 (68.5 examples/sec; 0.058 sec/batch; 96h:45m:54s remains)
INFO - root - 2019-11-03 23:32:57.263881: step 31250, total loss = 0.68, predict loss = 0.16 (71.8 examples/sec; 0.056 sec/batch; 92h:23m:26s remains)
INFO - root - 2019-11-03 23:32:57.902119: step 31260, total loss = 0.72, predict loss = 0.17 (72.2 examples/sec; 0.055 sec/batch; 91h:54m:55s remains)
INFO - root - 2019-11-03 23:32:58.502204: step 31270, total loss = 0.63, predict loss = 0.14 (75.5 examples/sec; 0.053 sec/batch; 87h:52m:38s remains)
INFO - root - 2019-11-03 23:32:59.144157: step 31280, total loss = 0.75, predict loss = 0.17 (66.8 examples/sec; 0.060 sec/batch; 99h:14m:34s remains)
INFO - root - 2019-11-03 23:32:59.784734: step 31290, total loss = 0.68, predict loss = 0.16 (68.2 examples/sec; 0.059 sec/batch; 97h:11m:32s remains)
INFO - root - 2019-11-03 23:33:00.414277: step 31300, total loss = 0.56, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 93h:27m:03s remains)
INFO - root - 2019-11-03 23:33:01.061371: step 31310, total loss = 0.59, predict loss = 0.12 (75.2 examples/sec; 0.053 sec/batch; 88h:10m:30s remains)
INFO - root - 2019-11-03 23:33:01.701436: step 31320, total loss = 0.67, predict loss = 0.16 (63.6 examples/sec; 0.063 sec/batch; 104h:13m:07s remains)
INFO - root - 2019-11-03 23:33:02.356605: step 31330, total loss = 0.52, predict loss = 0.11 (74.5 examples/sec; 0.054 sec/batch; 89h:00m:09s remains)
INFO - root - 2019-11-03 23:33:03.070414: step 31340, total loss = 0.53, predict loss = 0.12 (60.1 examples/sec; 0.067 sec/batch; 110h:23m:45s remains)
INFO - root - 2019-11-03 23:33:03.783863: step 31350, total loss = 0.39, predict loss = 0.08 (61.8 examples/sec; 0.065 sec/batch; 107h:20m:44s remains)
INFO - root - 2019-11-03 23:33:04.465806: step 31360, total loss = 0.35, predict loss = 0.07 (63.8 examples/sec; 0.063 sec/batch; 103h:52m:20s remains)
INFO - root - 2019-11-03 23:33:05.101915: step 31370, total loss = 0.65, predict loss = 0.16 (68.1 examples/sec; 0.059 sec/batch; 97h:19m:13s remains)
INFO - root - 2019-11-03 23:33:05.750418: step 31380, total loss = 0.73, predict loss = 0.17 (58.4 examples/sec; 0.068 sec/batch; 113h:28m:32s remains)
INFO - root - 2019-11-03 23:33:06.344412: step 31390, total loss = 0.68, predict loss = 0.16 (67.5 examples/sec; 0.059 sec/batch; 98h:11m:47s remains)
INFO - root - 2019-11-03 23:33:06.954931: step 31400, total loss = 0.61, predict loss = 0.14 (76.5 examples/sec; 0.052 sec/batch; 86h:42m:10s remains)
INFO - root - 2019-11-03 23:33:07.559722: step 31410, total loss = 0.70, predict loss = 0.16 (68.2 examples/sec; 0.059 sec/batch; 97h:15m:14s remains)
INFO - root - 2019-11-03 23:33:08.183603: step 31420, total loss = 0.64, predict loss = 0.14 (76.7 examples/sec; 0.052 sec/batch; 86h:30m:55s remains)
INFO - root - 2019-11-03 23:33:08.820381: step 31430, total loss = 0.99, predict loss = 0.23 (70.3 examples/sec; 0.057 sec/batch; 94h:16m:24s remains)
INFO - root - 2019-11-03 23:33:09.469918: step 31440, total loss = 0.82, predict loss = 0.20 (74.1 examples/sec; 0.054 sec/batch; 89h:31m:27s remains)
INFO - root - 2019-11-03 23:33:10.111945: step 31450, total loss = 1.06, predict loss = 0.22 (68.3 examples/sec; 0.059 sec/batch; 97h:02m:09s remains)
INFO - root - 2019-11-03 23:33:10.774359: step 31460, total loss = 1.00, predict loss = 0.23 (70.6 examples/sec; 0.057 sec/batch; 93h:59m:50s remains)
INFO - root - 2019-11-03 23:33:11.444451: step 31470, total loss = 0.77, predict loss = 0.18 (70.6 examples/sec; 0.057 sec/batch; 93h:59m:55s remains)
INFO - root - 2019-11-03 23:33:12.113818: step 31480, total loss = 0.78, predict loss = 0.19 (59.7 examples/sec; 0.067 sec/batch; 111h:04m:54s remains)
INFO - root - 2019-11-03 23:33:12.807559: step 31490, total loss = 0.61, predict loss = 0.14 (64.3 examples/sec; 0.062 sec/batch; 103h:12m:46s remains)
INFO - root - 2019-11-03 23:33:13.424510: step 31500, total loss = 0.72, predict loss = 0.17 (76.7 examples/sec; 0.052 sec/batch; 86h:26m:04s remains)
INFO - root - 2019-11-03 23:33:14.032476: step 31510, total loss = 0.62, predict loss = 0.14 (71.9 examples/sec; 0.056 sec/batch; 92h:15m:40s remains)
INFO - root - 2019-11-03 23:33:14.699436: step 31520, total loss = 0.75, predict loss = 0.17 (61.7 examples/sec; 0.065 sec/batch; 107h:30m:09s remains)
INFO - root - 2019-11-03 23:33:15.293069: step 31530, total loss = 0.81, predict loss = 0.18 (70.8 examples/sec; 0.057 sec/batch; 93h:43m:04s remains)
INFO - root - 2019-11-03 23:33:15.893300: step 31540, total loss = 0.61, predict loss = 0.12 (73.7 examples/sec; 0.054 sec/batch; 89h:57m:41s remains)
INFO - root - 2019-11-03 23:33:16.528223: step 31550, total loss = 0.69, predict loss = 0.17 (70.6 examples/sec; 0.057 sec/batch; 93h:57m:30s remains)
INFO - root - 2019-11-03 23:33:17.134433: step 31560, total loss = 0.75, predict loss = 0.16 (72.2 examples/sec; 0.055 sec/batch; 91h:49m:29s remains)
INFO - root - 2019-11-03 23:33:17.707329: step 31570, total loss = 0.52, predict loss = 0.11 (79.0 examples/sec; 0.051 sec/batch; 83h:55m:39s remains)
INFO - root - 2019-11-03 23:33:18.345418: step 31580, total loss = 0.70, predict loss = 0.16 (69.4 examples/sec; 0.058 sec/batch; 95h:33m:17s remains)
INFO - root - 2019-11-03 23:33:18.977219: step 31590, total loss = 0.51, predict loss = 0.12 (73.8 examples/sec; 0.054 sec/batch; 89h:48m:11s remains)
INFO - root - 2019-11-03 23:33:19.575649: step 31600, total loss = 0.62, predict loss = 0.14 (71.1 examples/sec; 0.056 sec/batch; 93h:18m:29s remains)
INFO - root - 2019-11-03 23:33:20.178492: step 31610, total loss = 0.52, predict loss = 0.11 (70.4 examples/sec; 0.057 sec/batch; 94h:09m:29s remains)
INFO - root - 2019-11-03 23:33:20.789335: step 31620, total loss = 0.57, predict loss = 0.13 (75.6 examples/sec; 0.053 sec/batch; 87h:45m:24s remains)
INFO - root - 2019-11-03 23:33:21.419796: step 31630, total loss = 0.67, predict loss = 0.16 (65.0 examples/sec; 0.062 sec/batch; 102h:04m:29s remains)
INFO - root - 2019-11-03 23:33:22.053917: step 31640, total loss = 0.63, predict loss = 0.14 (53.6 examples/sec; 0.075 sec/batch; 123h:41m:06s remains)
INFO - root - 2019-11-03 23:33:22.693907: step 31650, total loss = 0.51, predict loss = 0.12 (68.7 examples/sec; 0.058 sec/batch; 96h:29m:48s remains)
INFO - root - 2019-11-03 23:33:23.268868: step 31660, total loss = 0.67, predict loss = 0.16 (74.0 examples/sec; 0.054 sec/batch; 89h:36m:44s remains)
INFO - root - 2019-11-03 23:33:23.872638: step 31670, total loss = 0.50, predict loss = 0.11 (69.7 examples/sec; 0.057 sec/batch; 95h:07m:51s remains)
INFO - root - 2019-11-03 23:33:24.486124: step 31680, total loss = 0.53, predict loss = 0.12 (78.4 examples/sec; 0.051 sec/batch; 84h:35m:01s remains)
INFO - root - 2019-11-03 23:33:25.112248: step 31690, total loss = 0.55, predict loss = 0.13 (74.9 examples/sec; 0.053 sec/batch; 88h:29m:21s remains)
INFO - root - 2019-11-03 23:33:25.728691: step 31700, total loss = 0.46, predict loss = 0.10 (78.4 examples/sec; 0.051 sec/batch; 84h:37m:59s remains)
INFO - root - 2019-11-03 23:33:26.335457: step 31710, total loss = 0.69, predict loss = 0.17 (69.1 examples/sec; 0.058 sec/batch; 96h:00m:49s remains)
INFO - root - 2019-11-03 23:33:26.961405: step 31720, total loss = 0.59, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 91h:19m:19s remains)
INFO - root - 2019-11-03 23:33:27.598141: step 31730, total loss = 0.48, predict loss = 0.11 (74.3 examples/sec; 0.054 sec/batch; 89h:14m:28s remains)
INFO - root - 2019-11-03 23:33:28.218393: step 31740, total loss = 0.45, predict loss = 0.10 (77.4 examples/sec; 0.052 sec/batch; 85h:43m:12s remains)
INFO - root - 2019-11-03 23:33:28.816949: step 31750, total loss = 0.62, predict loss = 0.14 (78.7 examples/sec; 0.051 sec/batch; 84h:14m:35s remains)
INFO - root - 2019-11-03 23:33:29.427115: step 31760, total loss = 0.69, predict loss = 0.17 (79.8 examples/sec; 0.050 sec/batch; 83h:08m:11s remains)
INFO - root - 2019-11-03 23:33:30.035003: step 31770, total loss = 0.58, predict loss = 0.15 (71.1 examples/sec; 0.056 sec/batch; 93h:18m:59s remains)
INFO - root - 2019-11-03 23:33:30.644210: step 31780, total loss = 0.52, predict loss = 0.12 (67.0 examples/sec; 0.060 sec/batch; 99h:02m:48s remains)
INFO - root - 2019-11-03 23:33:31.782723: step 31790, total loss = 0.65, predict loss = 0.14 (66.6 examples/sec; 0.060 sec/batch; 99h:29m:47s remains)
INFO - root - 2019-11-03 23:33:32.425620: step 31800, total loss = 0.63, predict loss = 0.15 (75.0 examples/sec; 0.053 sec/batch; 88h:25m:26s remains)
INFO - root - 2019-11-03 23:33:33.056484: step 31810, total loss = 0.63, predict loss = 0.15 (79.4 examples/sec; 0.050 sec/batch; 83h:29m:19s remains)
INFO - root - 2019-11-03 23:33:33.702906: step 31820, total loss = 0.80, predict loss = 0.19 (65.1 examples/sec; 0.061 sec/batch; 101h:49m:36s remains)
INFO - root - 2019-11-03 23:33:34.338109: step 31830, total loss = 0.77, predict loss = 0.18 (78.5 examples/sec; 0.051 sec/batch; 84h:25m:51s remains)
INFO - root - 2019-11-03 23:33:34.964709: step 31840, total loss = 0.76, predict loss = 0.18 (72.1 examples/sec; 0.055 sec/batch; 91h:57m:01s remains)
INFO - root - 2019-11-03 23:33:35.611045: step 31850, total loss = 0.61, predict loss = 0.13 (61.2 examples/sec; 0.065 sec/batch; 108h:19m:24s remains)
INFO - root - 2019-11-03 23:33:36.285562: step 31860, total loss = 0.93, predict loss = 0.22 (58.7 examples/sec; 0.068 sec/batch; 113h:00m:17s remains)
INFO - root - 2019-11-03 23:33:36.883676: step 31870, total loss = 0.70, predict loss = 0.16 (74.1 examples/sec; 0.054 sec/batch; 89h:29m:10s remains)
INFO - root - 2019-11-03 23:33:37.493367: step 31880, total loss = 0.61, predict loss = 0.15 (80.9 examples/sec; 0.049 sec/batch; 82h:00m:24s remains)
INFO - root - 2019-11-03 23:33:38.086568: step 31890, total loss = 0.82, predict loss = 0.19 (64.7 examples/sec; 0.062 sec/batch; 102h:26m:09s remains)
INFO - root - 2019-11-03 23:33:38.724828: step 31900, total loss = 0.83, predict loss = 0.20 (65.9 examples/sec; 0.061 sec/batch; 100h:40m:14s remains)
INFO - root - 2019-11-03 23:33:39.381569: step 31910, total loss = 0.74, predict loss = 0.17 (66.3 examples/sec; 0.060 sec/batch; 99h:58m:53s remains)
INFO - root - 2019-11-03 23:33:40.009499: step 31920, total loss = 0.70, predict loss = 0.17 (73.3 examples/sec; 0.055 sec/batch; 90h:28m:34s remains)
INFO - root - 2019-11-03 23:33:40.670647: step 31930, total loss = 0.66, predict loss = 0.15 (64.7 examples/sec; 0.062 sec/batch; 102h:33m:29s remains)
INFO - root - 2019-11-03 23:33:41.332897: step 31940, total loss = 0.50, predict loss = 0.11 (62.8 examples/sec; 0.064 sec/batch; 105h:31m:04s remains)
INFO - root - 2019-11-03 23:33:41.974320: step 31950, total loss = 0.59, predict loss = 0.13 (78.3 examples/sec; 0.051 sec/batch; 84h:41m:37s remains)
INFO - root - 2019-11-03 23:33:42.604886: step 31960, total loss = 0.65, predict loss = 0.14 (66.5 examples/sec; 0.060 sec/batch; 99h:38m:39s remains)
INFO - root - 2019-11-03 23:33:43.237012: step 31970, total loss = 0.57, predict loss = 0.14 (69.0 examples/sec; 0.058 sec/batch; 96h:09m:44s remains)
INFO - root - 2019-11-03 23:33:43.876882: step 31980, total loss = 0.67, predict loss = 0.15 (71.2 examples/sec; 0.056 sec/batch; 93h:07m:13s remains)
INFO - root - 2019-11-03 23:33:44.547912: step 31990, total loss = 0.57, predict loss = 0.12 (69.7 examples/sec; 0.057 sec/batch; 95h:06m:22s remains)
INFO - root - 2019-11-03 23:33:45.178251: step 32000, total loss = 0.51, predict loss = 0.11 (75.3 examples/sec; 0.053 sec/batch; 88h:06m:35s remains)
INFO - root - 2019-11-03 23:33:45.780449: step 32010, total loss = 0.43, predict loss = 0.10 (71.5 examples/sec; 0.056 sec/batch; 92h:46m:42s remains)
INFO - root - 2019-11-03 23:33:46.386185: step 32020, total loss = 0.62, predict loss = 0.14 (76.9 examples/sec; 0.052 sec/batch; 86h:12m:55s remains)
INFO - root - 2019-11-03 23:33:46.999766: step 32030, total loss = 0.47, predict loss = 0.10 (70.0 examples/sec; 0.057 sec/batch; 94h:40m:49s remains)
INFO - root - 2019-11-03 23:33:47.598342: step 32040, total loss = 0.71, predict loss = 0.16 (76.9 examples/sec; 0.052 sec/batch; 86h:10m:26s remains)
INFO - root - 2019-11-03 23:33:48.187314: step 32050, total loss = 0.66, predict loss = 0.15 (63.2 examples/sec; 0.063 sec/batch; 104h:52m:51s remains)
INFO - root - 2019-11-03 23:33:48.850474: step 32060, total loss = 0.76, predict loss = 0.18 (63.9 examples/sec; 0.063 sec/batch; 103h:50m:21s remains)
INFO - root - 2019-11-03 23:33:49.486206: step 32070, total loss = 0.64, predict loss = 0.15 (67.1 examples/sec; 0.060 sec/batch; 98h:52m:53s remains)
INFO - root - 2019-11-03 23:33:50.147280: step 32080, total loss = 0.75, predict loss = 0.18 (80.7 examples/sec; 0.050 sec/batch; 82h:12m:28s remains)
INFO - root - 2019-11-03 23:33:50.757281: step 32090, total loss = 0.68, predict loss = 0.16 (81.6 examples/sec; 0.049 sec/batch; 81h:15m:14s remains)
INFO - root - 2019-11-03 23:33:51.388660: step 32100, total loss = 0.69, predict loss = 0.16 (61.4 examples/sec; 0.065 sec/batch; 107h:56m:02s remains)
INFO - root - 2019-11-03 23:33:52.025324: step 32110, total loss = 0.70, predict loss = 0.16 (64.9 examples/sec; 0.062 sec/batch; 102h:06m:54s remains)
INFO - root - 2019-11-03 23:33:52.678507: step 32120, total loss = 0.76, predict loss = 0.19 (78.7 examples/sec; 0.051 sec/batch; 84h:18m:31s remains)
INFO - root - 2019-11-03 23:33:53.273741: step 32130, total loss = 0.69, predict loss = 0.17 (88.3 examples/sec; 0.045 sec/batch; 75h:08m:06s remains)
INFO - root - 2019-11-03 23:33:53.885213: step 32140, total loss = 0.62, predict loss = 0.16 (77.0 examples/sec; 0.052 sec/batch; 86h:09m:42s remains)
INFO - root - 2019-11-03 23:33:54.505572: step 32150, total loss = 0.66, predict loss = 0.16 (73.7 examples/sec; 0.054 sec/batch; 90h:01m:03s remains)
INFO - root - 2019-11-03 23:33:55.115883: step 32160, total loss = 0.57, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 94h:59m:13s remains)
INFO - root - 2019-11-03 23:33:55.748845: step 32170, total loss = 0.63, predict loss = 0.15 (78.4 examples/sec; 0.051 sec/batch; 84h:34m:42s remains)
INFO - root - 2019-11-03 23:33:56.363249: step 32180, total loss = 0.82, predict loss = 0.20 (70.7 examples/sec; 0.057 sec/batch; 93h:44m:36s remains)
INFO - root - 2019-11-03 23:33:56.987091: step 32190, total loss = 0.70, predict loss = 0.15 (75.5 examples/sec; 0.053 sec/batch; 87h:47m:50s remains)
INFO - root - 2019-11-03 23:33:57.627395: step 32200, total loss = 0.74, predict loss = 0.17 (64.8 examples/sec; 0.062 sec/batch; 102h:16m:41s remains)
INFO - root - 2019-11-03 23:33:58.269333: step 32210, total loss = 0.60, predict loss = 0.13 (71.3 examples/sec; 0.056 sec/batch; 93h:01m:57s remains)
INFO - root - 2019-11-03 23:33:58.915867: step 32220, total loss = 0.55, predict loss = 0.12 (68.2 examples/sec; 0.059 sec/batch; 97h:12m:20s remains)
INFO - root - 2019-11-03 23:33:59.546376: step 32230, total loss = 0.67, predict loss = 0.15 (73.4 examples/sec; 0.054 sec/batch; 90h:20m:12s remains)
INFO - root - 2019-11-03 23:34:00.211997: step 32240, total loss = 0.65, predict loss = 0.16 (65.9 examples/sec; 0.061 sec/batch; 100h:36m:20s remains)
INFO - root - 2019-11-03 23:34:00.865257: step 32250, total loss = 0.64, predict loss = 0.15 (81.0 examples/sec; 0.049 sec/batch; 81h:52m:09s remains)
INFO - root - 2019-11-03 23:34:01.488353: step 32260, total loss = 0.58, predict loss = 0.14 (83.7 examples/sec; 0.048 sec/batch; 79h:13m:18s remains)
INFO - root - 2019-11-03 23:34:02.117667: step 32270, total loss = 0.59, predict loss = 0.13 (72.1 examples/sec; 0.055 sec/batch; 91h:55m:38s remains)
INFO - root - 2019-11-03 23:34:02.783505: step 32280, total loss = 0.49, predict loss = 0.11 (61.4 examples/sec; 0.065 sec/batch; 108h:00m:42s remains)
INFO - root - 2019-11-03 23:34:03.417995: step 32290, total loss = 0.61, predict loss = 0.14 (70.0 examples/sec; 0.057 sec/batch; 94h:46m:54s remains)
INFO - root - 2019-11-03 23:34:04.056374: step 32300, total loss = 0.84, predict loss = 0.21 (63.3 examples/sec; 0.063 sec/batch; 104h:46m:22s remains)
INFO - root - 2019-11-03 23:34:04.674287: step 32310, total loss = 0.64, predict loss = 0.15 (69.6 examples/sec; 0.057 sec/batch; 95h:16m:02s remains)
INFO - root - 2019-11-03 23:34:05.288449: step 32320, total loss = 0.61, predict loss = 0.13 (67.6 examples/sec; 0.059 sec/batch; 98h:01m:52s remains)
INFO - root - 2019-11-03 23:34:05.927936: step 32330, total loss = 0.57, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 99h:40m:15s remains)
INFO - root - 2019-11-03 23:34:06.584138: step 32340, total loss = 0.68, predict loss = 0.15 (69.3 examples/sec; 0.058 sec/batch; 95h:39m:31s remains)
INFO - root - 2019-11-03 23:34:07.193682: step 32350, total loss = 0.71, predict loss = 0.16 (69.4 examples/sec; 0.058 sec/batch; 95h:31m:06s remains)
INFO - root - 2019-11-03 23:34:07.808506: step 32360, total loss = 0.70, predict loss = 0.16 (70.0 examples/sec; 0.057 sec/batch; 94h:40m:16s remains)
INFO - root - 2019-11-03 23:34:08.433338: step 32370, total loss = 0.67, predict loss = 0.16 (68.9 examples/sec; 0.058 sec/batch; 96h:15m:38s remains)
INFO - root - 2019-11-03 23:34:09.071838: step 32380, total loss = 0.63, predict loss = 0.15 (70.1 examples/sec; 0.057 sec/batch; 94h:34m:45s remains)
INFO - root - 2019-11-03 23:34:09.705444: step 32390, total loss = 0.64, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 97h:18m:23s remains)
INFO - root - 2019-11-03 23:34:10.348926: step 32400, total loss = 0.68, predict loss = 0.16 (73.9 examples/sec; 0.054 sec/batch; 89h:41m:55s remains)
INFO - root - 2019-11-03 23:34:11.039656: step 32410, total loss = 0.55, predict loss = 0.13 (65.1 examples/sec; 0.061 sec/batch; 101h:46m:35s remains)
INFO - root - 2019-11-03 23:34:11.690178: step 32420, total loss = 0.62, predict loss = 0.15 (66.0 examples/sec; 0.061 sec/batch; 100h:26m:55s remains)
INFO - root - 2019-11-03 23:34:12.336597: step 32430, total loss = 0.87, predict loss = 0.21 (68.1 examples/sec; 0.059 sec/batch; 97h:21m:06s remains)
INFO - root - 2019-11-03 23:34:12.981322: step 32440, total loss = 0.56, predict loss = 0.14 (65.8 examples/sec; 0.061 sec/batch; 100h:47m:10s remains)
INFO - root - 2019-11-03 23:34:13.676402: step 32450, total loss = 0.68, predict loss = 0.16 (63.5 examples/sec; 0.063 sec/batch; 104h:22m:51s remains)
INFO - root - 2019-11-03 23:34:14.323841: step 32460, total loss = 0.74, predict loss = 0.17 (66.6 examples/sec; 0.060 sec/batch; 99h:35m:10s remains)
INFO - root - 2019-11-03 23:34:14.917672: step 32470, total loss = 0.89, predict loss = 0.21 (73.0 examples/sec; 0.055 sec/batch; 90h:51m:17s remains)
INFO - root - 2019-11-03 23:34:15.522346: step 32480, total loss = 0.94, predict loss = 0.23 (78.1 examples/sec; 0.051 sec/batch; 84h:53m:20s remains)
INFO - root - 2019-11-03 23:34:16.134659: step 32490, total loss = 0.84, predict loss = 0.20 (72.0 examples/sec; 0.056 sec/batch; 92h:07m:43s remains)
INFO - root - 2019-11-03 23:34:16.753893: step 32500, total loss = 0.80, predict loss = 0.18 (79.8 examples/sec; 0.050 sec/batch; 83h:02m:40s remains)
INFO - root - 2019-11-03 23:34:17.355548: step 32510, total loss = 0.76, predict loss = 0.18 (80.5 examples/sec; 0.050 sec/batch; 82h:20m:49s remains)
INFO - root - 2019-11-03 23:34:17.948632: step 32520, total loss = 0.95, predict loss = 0.22 (68.8 examples/sec; 0.058 sec/batch; 96h:20m:01s remains)
INFO - root - 2019-11-03 23:34:18.579916: step 32530, total loss = 0.78, predict loss = 0.19 (73.1 examples/sec; 0.055 sec/batch; 90h:42m:12s remains)
INFO - root - 2019-11-03 23:34:19.235639: step 32540, total loss = 0.83, predict loss = 0.18 (67.3 examples/sec; 0.059 sec/batch; 98h:32m:02s remains)
INFO - root - 2019-11-03 23:34:19.868979: step 32550, total loss = 0.63, predict loss = 0.14 (80.0 examples/sec; 0.050 sec/batch; 82h:52m:50s remains)
INFO - root - 2019-11-03 23:34:20.555939: step 32560, total loss = 0.78, predict loss = 0.19 (63.9 examples/sec; 0.063 sec/batch; 103h:47m:09s remains)
INFO - root - 2019-11-03 23:34:21.166277: step 32570, total loss = 0.73, predict loss = 0.17 (73.6 examples/sec; 0.054 sec/batch; 90h:08m:15s remains)
INFO - root - 2019-11-03 23:34:21.763974: step 32580, total loss = 0.86, predict loss = 0.21 (72.2 examples/sec; 0.055 sec/batch; 91h:46m:55s remains)
INFO - root - 2019-11-03 23:34:22.469506: step 32590, total loss = 0.61, predict loss = 0.14 (76.6 examples/sec; 0.052 sec/batch; 86h:33m:31s remains)
INFO - root - 2019-11-03 23:34:23.079994: step 32600, total loss = 0.56, predict loss = 0.12 (66.2 examples/sec; 0.060 sec/batch; 100h:10m:45s remains)
INFO - root - 2019-11-03 23:34:23.748002: step 32610, total loss = 0.55, predict loss = 0.12 (58.5 examples/sec; 0.068 sec/batch; 113h:25m:53s remains)
INFO - root - 2019-11-03 23:34:24.401203: step 32620, total loss = 0.53, predict loss = 0.13 (65.7 examples/sec; 0.061 sec/batch; 100h:57m:01s remains)
INFO - root - 2019-11-03 23:34:25.076556: step 32630, total loss = 0.64, predict loss = 0.15 (69.7 examples/sec; 0.057 sec/batch; 95h:05m:57s remains)
INFO - root - 2019-11-03 23:34:25.748191: step 32640, total loss = 0.49, predict loss = 0.12 (63.6 examples/sec; 0.063 sec/batch; 104h:14m:26s remains)
INFO - root - 2019-11-03 23:34:26.406676: step 32650, total loss = 0.71, predict loss = 0.17 (76.7 examples/sec; 0.052 sec/batch; 86h:24m:16s remains)
INFO - root - 2019-11-03 23:34:27.032172: step 32660, total loss = 0.72, predict loss = 0.18 (69.9 examples/sec; 0.057 sec/batch; 94h:54m:07s remains)
INFO - root - 2019-11-03 23:34:27.656498: step 32670, total loss = 0.70, predict loss = 0.17 (80.6 examples/sec; 0.050 sec/batch; 82h:17m:09s remains)
INFO - root - 2019-11-03 23:34:28.283245: step 32680, total loss = 0.65, predict loss = 0.14 (85.1 examples/sec; 0.047 sec/batch; 77h:52m:29s remains)
INFO - root - 2019-11-03 23:34:28.929972: step 32690, total loss = 0.56, predict loss = 0.13 (68.3 examples/sec; 0.059 sec/batch; 97h:07m:09s remains)
INFO - root - 2019-11-03 23:34:29.591771: step 32700, total loss = 0.97, predict loss = 0.23 (67.0 examples/sec; 0.060 sec/batch; 98h:54m:11s remains)
INFO - root - 2019-11-03 23:34:30.156907: step 32710, total loss = 0.61, predict loss = 0.14 (100.8 examples/sec; 0.040 sec/batch; 65h:48m:21s remains)
INFO - root - 2019-11-03 23:34:30.640333: step 32720, total loss = 0.43, predict loss = 0.10 (89.0 examples/sec; 0.045 sec/batch; 74h:31m:53s remains)
INFO - root - 2019-11-03 23:34:31.710180: step 32730, total loss = 0.71, predict loss = 0.16 (73.1 examples/sec; 0.055 sec/batch; 90h:45m:46s remains)
INFO - root - 2019-11-03 23:34:32.327411: step 32740, total loss = 0.60, predict loss = 0.12 (76.2 examples/sec; 0.053 sec/batch; 87h:02m:09s remains)
INFO - root - 2019-11-03 23:34:32.972495: step 32750, total loss = 0.42, predict loss = 0.08 (68.8 examples/sec; 0.058 sec/batch; 96h:23m:30s remains)
INFO - root - 2019-11-03 23:34:33.622820: step 32760, total loss = 0.59, predict loss = 0.13 (68.2 examples/sec; 0.059 sec/batch; 97h:11m:21s remains)
INFO - root - 2019-11-03 23:34:34.296595: step 32770, total loss = 0.70, predict loss = 0.17 (57.8 examples/sec; 0.069 sec/batch; 114h:41m:46s remains)
INFO - root - 2019-11-03 23:34:34.891440: step 32780, total loss = 0.52, predict loss = 0.12 (74.7 examples/sec; 0.054 sec/batch; 88h:43m:56s remains)
INFO - root - 2019-11-03 23:34:35.500110: step 32790, total loss = 0.80, predict loss = 0.18 (76.7 examples/sec; 0.052 sec/batch; 86h:23m:15s remains)
INFO - root - 2019-11-03 23:34:36.139620: step 32800, total loss = 0.58, predict loss = 0.13 (73.8 examples/sec; 0.054 sec/batch; 89h:53m:23s remains)
INFO - root - 2019-11-03 23:34:36.778342: step 32810, total loss = 0.63, predict loss = 0.15 (67.6 examples/sec; 0.059 sec/batch; 98h:05m:19s remains)
INFO - root - 2019-11-03 23:34:37.455602: step 32820, total loss = 0.72, predict loss = 0.17 (59.1 examples/sec; 0.068 sec/batch; 112h:08m:32s remains)
INFO - root - 2019-11-03 23:34:38.112744: step 32830, total loss = 0.80, predict loss = 0.19 (68.6 examples/sec; 0.058 sec/batch; 96h:39m:00s remains)
INFO - root - 2019-11-03 23:34:38.754658: step 32840, total loss = 0.59, predict loss = 0.12 (73.1 examples/sec; 0.055 sec/batch; 90h:38m:22s remains)
INFO - root - 2019-11-03 23:34:39.406491: step 32850, total loss = 0.75, predict loss = 0.17 (58.9 examples/sec; 0.068 sec/batch; 112h:33m:26s remains)
INFO - root - 2019-11-03 23:34:40.115145: step 32860, total loss = 0.58, predict loss = 0.13 (62.6 examples/sec; 0.064 sec/batch; 105h:55m:04s remains)
INFO - root - 2019-11-03 23:34:40.749107: step 32870, total loss = 0.61, predict loss = 0.14 (55.4 examples/sec; 0.072 sec/batch; 119h:35m:15s remains)
INFO - root - 2019-11-03 23:34:41.469306: step 32880, total loss = 0.75, predict loss = 0.17 (56.6 examples/sec; 0.071 sec/batch; 117h:11m:44s remains)
INFO - root - 2019-11-03 23:34:42.194686: step 32890, total loss = 0.61, predict loss = 0.14 (56.6 examples/sec; 0.071 sec/batch; 117h:08m:53s remains)
INFO - root - 2019-11-03 23:34:42.857868: step 32900, total loss = 0.61, predict loss = 0.14 (74.1 examples/sec; 0.054 sec/batch; 89h:26m:50s remains)
INFO - root - 2019-11-03 23:34:43.526014: step 32910, total loss = 0.68, predict loss = 0.15 (73.1 examples/sec; 0.055 sec/batch; 90h:39m:06s remains)
INFO - root - 2019-11-03 23:34:44.175805: step 32920, total loss = 0.60, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 99h:06m:35s remains)
INFO - root - 2019-11-03 23:34:44.824819: step 32930, total loss = 0.54, predict loss = 0.12 (66.9 examples/sec; 0.060 sec/batch; 99h:04m:22s remains)
INFO - root - 2019-11-03 23:34:45.486817: step 32940, total loss = 0.45, predict loss = 0.10 (67.7 examples/sec; 0.059 sec/batch; 97h:55m:26s remains)
INFO - root - 2019-11-03 23:34:46.098859: step 32950, total loss = 0.66, predict loss = 0.17 (76.9 examples/sec; 0.052 sec/batch; 86h:12m:47s remains)
INFO - root - 2019-11-03 23:34:46.756355: step 32960, total loss = 0.53, predict loss = 0.11 (66.5 examples/sec; 0.060 sec/batch; 99h:41m:05s remains)
INFO - root - 2019-11-03 23:34:47.410200: step 32970, total loss = 0.82, predict loss = 0.18 (79.7 examples/sec; 0.050 sec/batch; 83h:14m:10s remains)
INFO - root - 2019-11-03 23:34:48.065058: step 32980, total loss = 0.83, predict loss = 0.18 (71.0 examples/sec; 0.056 sec/batch; 93h:23m:27s remains)
INFO - root - 2019-11-03 23:34:48.722540: step 32990, total loss = 0.90, predict loss = 0.21 (63.7 examples/sec; 0.063 sec/batch; 104h:07m:23s remains)
INFO - root - 2019-11-03 23:34:49.353390: step 33000, total loss = 0.89, predict loss = 0.21 (76.6 examples/sec; 0.052 sec/batch; 86h:32m:41s remains)
INFO - root - 2019-11-03 23:34:49.954224: step 33010, total loss = 0.61, predict loss = 0.13 (75.5 examples/sec; 0.053 sec/batch; 87h:49m:49s remains)
INFO - root - 2019-11-03 23:34:50.633125: step 33020, total loss = 0.70, predict loss = 0.14 (59.5 examples/sec; 0.067 sec/batch; 111h:20m:11s remains)
INFO - root - 2019-11-03 23:34:51.315621: step 33030, total loss = 0.73, predict loss = 0.16 (78.0 examples/sec; 0.051 sec/batch; 84h:59m:42s remains)
INFO - root - 2019-11-03 23:34:51.954821: step 33040, total loss = 0.86, predict loss = 0.20 (73.3 examples/sec; 0.055 sec/batch; 90h:28m:32s remains)
INFO - root - 2019-11-03 23:34:52.623197: step 33050, total loss = 0.82, predict loss = 0.19 (73.5 examples/sec; 0.054 sec/batch; 90h:10m:58s remains)
INFO - root - 2019-11-03 23:34:53.256948: step 33060, total loss = 0.67, predict loss = 0.15 (60.4 examples/sec; 0.066 sec/batch; 109h:46m:06s remains)
INFO - root - 2019-11-03 23:34:53.888319: step 33070, total loss = 0.64, predict loss = 0.15 (73.8 examples/sec; 0.054 sec/batch; 89h:53m:00s remains)
INFO - root - 2019-11-03 23:34:54.490331: step 33080, total loss = 0.76, predict loss = 0.17 (67.1 examples/sec; 0.060 sec/batch; 98h:44m:42s remains)
INFO - root - 2019-11-03 23:34:55.078666: step 33090, total loss = 0.74, predict loss = 0.17 (80.9 examples/sec; 0.049 sec/batch; 81h:55m:06s remains)
INFO - root - 2019-11-03 23:34:55.672244: step 33100, total loss = 0.64, predict loss = 0.14 (74.2 examples/sec; 0.054 sec/batch; 89h:21m:24s remains)
INFO - root - 2019-11-03 23:34:56.319743: step 33110, total loss = 0.74, predict loss = 0.15 (69.1 examples/sec; 0.058 sec/batch; 95h:53m:25s remains)
INFO - root - 2019-11-03 23:34:56.970312: step 33120, total loss = 0.64, predict loss = 0.14 (68.6 examples/sec; 0.058 sec/batch; 96h:38m:21s remains)
INFO - root - 2019-11-03 23:34:57.633122: step 33130, total loss = 0.74, predict loss = 0.18 (68.3 examples/sec; 0.059 sec/batch; 97h:01m:51s remains)
INFO - root - 2019-11-03 23:34:58.314783: step 33140, total loss = 0.63, predict loss = 0.14 (71.4 examples/sec; 0.056 sec/batch; 92h:48m:18s remains)
INFO - root - 2019-11-03 23:34:58.994490: step 33150, total loss = 0.62, predict loss = 0.15 (68.7 examples/sec; 0.058 sec/batch; 96h:31m:57s remains)
INFO - root - 2019-11-03 23:34:59.619735: step 33160, total loss = 0.55, predict loss = 0.12 (69.6 examples/sec; 0.057 sec/batch; 95h:16m:04s remains)
INFO - root - 2019-11-03 23:35:00.252736: step 33170, total loss = 0.47, predict loss = 0.11 (77.6 examples/sec; 0.052 sec/batch; 85h:27m:09s remains)
INFO - root - 2019-11-03 23:35:00.876536: step 33180, total loss = 0.74, predict loss = 0.17 (69.4 examples/sec; 0.058 sec/batch; 95h:34m:10s remains)
INFO - root - 2019-11-03 23:35:01.501622: step 33190, total loss = 0.79, predict loss = 0.21 (78.4 examples/sec; 0.051 sec/batch; 84h:32m:59s remains)
INFO - root - 2019-11-03 23:35:02.171671: step 33200, total loss = 0.36, predict loss = 0.09 (63.1 examples/sec; 0.063 sec/batch; 105h:07m:09s remains)
INFO - root - 2019-11-03 23:35:02.775046: step 33210, total loss = 0.84, predict loss = 0.20 (72.8 examples/sec; 0.055 sec/batch; 91h:02m:21s remains)
INFO - root - 2019-11-03 23:35:03.391652: step 33220, total loss = 0.68, predict loss = 0.16 (70.5 examples/sec; 0.057 sec/batch; 94h:00m:07s remains)
INFO - root - 2019-11-03 23:35:04.010615: step 33230, total loss = 0.70, predict loss = 0.18 (72.0 examples/sec; 0.056 sec/batch; 92h:07m:40s remains)
INFO - root - 2019-11-03 23:35:04.625482: step 33240, total loss = 0.49, predict loss = 0.12 (72.5 examples/sec; 0.055 sec/batch; 91h:27m:57s remains)
INFO - root - 2019-11-03 23:35:05.285126: step 33250, total loss = 0.73, predict loss = 0.17 (67.7 examples/sec; 0.059 sec/batch; 97h:55m:24s remains)
INFO - root - 2019-11-03 23:35:05.920850: step 33260, total loss = 0.89, predict loss = 0.21 (69.4 examples/sec; 0.058 sec/batch; 95h:33m:34s remains)
INFO - root - 2019-11-03 23:35:06.585594: step 33270, total loss = 0.59, predict loss = 0.14 (63.2 examples/sec; 0.063 sec/batch; 104h:54m:27s remains)
INFO - root - 2019-11-03 23:35:07.240176: step 33280, total loss = 0.45, predict loss = 0.10 (73.6 examples/sec; 0.054 sec/batch; 90h:04m:13s remains)
INFO - root - 2019-11-03 23:35:07.884229: step 33290, total loss = 0.57, predict loss = 0.14 (67.9 examples/sec; 0.059 sec/batch; 97h:40m:10s remains)
INFO - root - 2019-11-03 23:35:08.515702: step 33300, total loss = 0.49, predict loss = 0.12 (68.4 examples/sec; 0.058 sec/batch; 96h:55m:35s remains)
INFO - root - 2019-11-03 23:35:09.172384: step 33310, total loss = 0.41, predict loss = 0.09 (64.5 examples/sec; 0.062 sec/batch; 102h:45m:49s remains)
INFO - root - 2019-11-03 23:35:09.815150: step 33320, total loss = 0.48, predict loss = 0.11 (63.8 examples/sec; 0.063 sec/batch; 103h:57m:13s remains)
INFO - root - 2019-11-03 23:35:10.451445: step 33330, total loss = 0.63, predict loss = 0.15 (69.4 examples/sec; 0.058 sec/batch; 95h:29m:54s remains)
INFO - root - 2019-11-03 23:35:11.113358: step 33340, total loss = 0.60, predict loss = 0.15 (62.5 examples/sec; 0.064 sec/batch; 106h:07m:30s remains)
INFO - root - 2019-11-03 23:35:11.862753: step 33350, total loss = 0.46, predict loss = 0.10 (60.2 examples/sec; 0.066 sec/batch; 110h:04m:29s remains)
INFO - root - 2019-11-03 23:35:12.485099: step 33360, total loss = 0.48, predict loss = 0.11 (78.4 examples/sec; 0.051 sec/batch; 84h:36m:16s remains)
INFO - root - 2019-11-03 23:35:13.070265: step 33370, total loss = 0.51, predict loss = 0.12 (75.0 examples/sec; 0.053 sec/batch; 88h:24m:51s remains)
INFO - root - 2019-11-03 23:35:13.660529: step 33380, total loss = 0.56, predict loss = 0.13 (73.7 examples/sec; 0.054 sec/batch; 89h:54m:50s remains)
INFO - root - 2019-11-03 23:35:14.302413: step 33390, total loss = 0.39, predict loss = 0.09 (72.3 examples/sec; 0.055 sec/batch; 91h:44m:15s remains)
INFO - root - 2019-11-03 23:35:14.970856: step 33400, total loss = 0.57, predict loss = 0.12 (62.8 examples/sec; 0.064 sec/batch; 105h:30m:09s remains)
INFO - root - 2019-11-03 23:35:15.658876: step 33410, total loss = 0.73, predict loss = 0.16 (68.1 examples/sec; 0.059 sec/batch; 97h:23m:02s remains)
INFO - root - 2019-11-03 23:35:16.289398: step 33420, total loss = 0.64, predict loss = 0.15 (72.0 examples/sec; 0.056 sec/batch; 92h:06m:19s remains)
INFO - root - 2019-11-03 23:35:16.928034: step 33430, total loss = 0.64, predict loss = 0.15 (66.6 examples/sec; 0.060 sec/batch; 99h:32m:11s remains)
INFO - root - 2019-11-03 23:35:17.552250: step 33440, total loss = 0.79, predict loss = 0.18 (75.0 examples/sec; 0.053 sec/batch; 88h:20m:28s remains)
INFO - root - 2019-11-03 23:35:18.175375: step 33450, total loss = 0.54, predict loss = 0.13 (71.7 examples/sec; 0.056 sec/batch; 92h:29m:33s remains)
INFO - root - 2019-11-03 23:35:18.782095: step 33460, total loss = 0.58, predict loss = 0.12 (70.5 examples/sec; 0.057 sec/batch; 94h:04m:24s remains)
INFO - root - 2019-11-03 23:35:19.416735: step 33470, total loss = 0.60, predict loss = 0.13 (81.9 examples/sec; 0.049 sec/batch; 80h:55m:20s remains)
INFO - root - 2019-11-03 23:35:20.051075: step 33480, total loss = 0.64, predict loss = 0.14 (64.1 examples/sec; 0.062 sec/batch; 103h:29m:57s remains)
INFO - root - 2019-11-03 23:35:20.715132: step 33490, total loss = 0.65, predict loss = 0.16 (73.2 examples/sec; 0.055 sec/batch; 90h:31m:11s remains)
INFO - root - 2019-11-03 23:35:21.375642: step 33500, total loss = 0.61, predict loss = 0.13 (66.7 examples/sec; 0.060 sec/batch; 99h:19m:10s remains)
INFO - root - 2019-11-03 23:35:22.024007: step 33510, total loss = 0.81, predict loss = 0.20 (62.0 examples/sec; 0.064 sec/batch; 106h:52m:35s remains)
INFO - root - 2019-11-03 23:35:22.705913: step 33520, total loss = 0.72, predict loss = 0.17 (60.6 examples/sec; 0.066 sec/batch; 109h:24m:48s remains)
INFO - root - 2019-11-03 23:35:23.374750: step 33530, total loss = 0.65, predict loss = 0.16 (83.5 examples/sec; 0.048 sec/batch; 79h:26m:05s remains)
INFO - root - 2019-11-03 23:35:23.987990: step 33540, total loss = 0.42, predict loss = 0.09 (81.9 examples/sec; 0.049 sec/batch; 80h:57m:10s remains)
INFO - root - 2019-11-03 23:35:24.579962: step 33550, total loss = 0.40, predict loss = 0.08 (77.3 examples/sec; 0.052 sec/batch; 85h:44m:45s remains)
INFO - root - 2019-11-03 23:35:25.196773: step 33560, total loss = 0.44, predict loss = 0.09 (64.6 examples/sec; 0.062 sec/batch; 102h:35m:00s remains)
INFO - root - 2019-11-03 23:35:25.848374: step 33570, total loss = 0.39, predict loss = 0.09 (73.2 examples/sec; 0.055 sec/batch; 90h:36m:43s remains)
INFO - root - 2019-11-03 23:35:26.520772: step 33580, total loss = 0.67, predict loss = 0.17 (62.5 examples/sec; 0.064 sec/batch; 106h:05m:59s remains)
INFO - root - 2019-11-03 23:35:27.132634: step 33590, total loss = 0.28, predict loss = 0.06 (82.5 examples/sec; 0.048 sec/batch; 80h:21m:21s remains)
INFO - root - 2019-11-03 23:35:27.767653: step 33600, total loss = 0.33, predict loss = 0.07 (67.0 examples/sec; 0.060 sec/batch; 98h:57m:01s remains)
INFO - root - 2019-11-03 23:35:28.420193: step 33610, total loss = 0.54, predict loss = 0.11 (68.7 examples/sec; 0.058 sec/batch; 96h:26m:51s remains)
INFO - root - 2019-11-03 23:35:29.092849: step 33620, total loss = 0.55, predict loss = 0.13 (69.8 examples/sec; 0.057 sec/batch; 94h:56m:35s remains)
INFO - root - 2019-11-03 23:35:29.773041: step 33630, total loss = 0.54, predict loss = 0.12 (63.3 examples/sec; 0.063 sec/batch; 104h:41m:45s remains)
INFO - root - 2019-11-03 23:35:30.431660: step 33640, total loss = 0.50, predict loss = 0.11 (76.9 examples/sec; 0.052 sec/batch; 86h:12m:57s remains)
INFO - root - 2019-11-03 23:35:31.107906: step 33650, total loss = 0.53, predict loss = 0.12 (71.3 examples/sec; 0.056 sec/batch; 92h:54m:51s remains)
INFO - root - 2019-11-03 23:35:31.708831: step 33660, total loss = 0.57, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 97h:20m:43s remains)
INFO - root - 2019-11-03 23:35:32.319795: step 33670, total loss = 0.82, predict loss = 0.19 (74.7 examples/sec; 0.054 sec/batch; 88h:41m:59s remains)
INFO - root - 2019-11-03 23:35:32.915821: step 33680, total loss = 0.64, predict loss = 0.15 (76.6 examples/sec; 0.052 sec/batch; 86h:34m:08s remains)
INFO - root - 2019-11-03 23:35:33.527642: step 33690, total loss = 0.70, predict loss = 0.17 (69.5 examples/sec; 0.058 sec/batch; 95h:20m:01s remains)
INFO - root - 2019-11-03 23:35:34.158658: step 33700, total loss = 0.59, predict loss = 0.13 (70.7 examples/sec; 0.057 sec/batch; 93h:43m:04s remains)
INFO - root - 2019-11-03 23:35:34.797083: step 33710, total loss = 0.51, predict loss = 0.12 (66.7 examples/sec; 0.060 sec/batch; 99h:26m:02s remains)
INFO - root - 2019-11-03 23:35:35.403760: step 33720, total loss = 0.81, predict loss = 0.18 (70.1 examples/sec; 0.057 sec/batch; 94h:32m:33s remains)
INFO - root - 2019-11-03 23:35:36.029792: step 33730, total loss = 0.67, predict loss = 0.15 (68.2 examples/sec; 0.059 sec/batch; 97h:08m:59s remains)
INFO - root - 2019-11-03 23:35:36.698202: step 33740, total loss = 0.70, predict loss = 0.17 (66.4 examples/sec; 0.060 sec/batch; 99h:50m:27s remains)
INFO - root - 2019-11-03 23:35:37.335708: step 33750, total loss = 0.58, predict loss = 0.14 (69.1 examples/sec; 0.058 sec/batch; 95h:55m:44s remains)
INFO - root - 2019-11-03 23:35:37.993474: step 33760, total loss = 0.53, predict loss = 0.12 (60.2 examples/sec; 0.066 sec/batch; 110h:06m:50s remains)
INFO - root - 2019-11-03 23:35:38.619695: step 33770, total loss = 0.69, predict loss = 0.16 (77.4 examples/sec; 0.052 sec/batch; 85h:40m:19s remains)
INFO - root - 2019-11-03 23:35:39.233515: step 33780, total loss = 0.50, predict loss = 0.12 (70.0 examples/sec; 0.057 sec/batch; 94h:38m:52s remains)
INFO - root - 2019-11-03 23:35:39.859226: step 33790, total loss = 0.32, predict loss = 0.06 (64.6 examples/sec; 0.062 sec/batch; 102h:39m:38s remains)
INFO - root - 2019-11-03 23:35:40.505934: step 33800, total loss = 0.90, predict loss = 0.22 (63.5 examples/sec; 0.063 sec/batch; 104h:23m:30s remains)
INFO - root - 2019-11-03 23:35:41.130378: step 33810, total loss = 0.35, predict loss = 0.07 (76.6 examples/sec; 0.052 sec/batch; 86h:31m:12s remains)
INFO - root - 2019-11-03 23:35:41.748069: step 33820, total loss = 0.32, predict loss = 0.06 (66.7 examples/sec; 0.060 sec/batch; 99h:23m:06s remains)
INFO - root - 2019-11-03 23:35:42.394735: step 33830, total loss = 0.74, predict loss = 0.19 (67.1 examples/sec; 0.060 sec/batch; 98h:43m:26s remains)
INFO - root - 2019-11-03 23:35:43.025202: step 33840, total loss = 0.54, predict loss = 0.13 (75.4 examples/sec; 0.053 sec/batch; 87h:54m:33s remains)
INFO - root - 2019-11-03 23:35:43.646858: step 33850, total loss = 0.69, predict loss = 0.16 (61.9 examples/sec; 0.065 sec/batch; 107h:06m:42s remains)
INFO - root - 2019-11-03 23:35:44.271545: step 33860, total loss = 0.67, predict loss = 0.15 (67.6 examples/sec; 0.059 sec/batch; 98h:02m:19s remains)
INFO - root - 2019-11-03 23:35:44.903614: step 33870, total loss = 0.70, predict loss = 0.17 (68.8 examples/sec; 0.058 sec/batch; 96h:23m:07s remains)
INFO - root - 2019-11-03 23:35:45.515389: step 33880, total loss = 0.55, predict loss = 0.13 (74.8 examples/sec; 0.054 sec/batch; 88h:40m:18s remains)
INFO - root - 2019-11-03 23:35:46.123728: step 33890, total loss = 0.84, predict loss = 0.20 (71.0 examples/sec; 0.056 sec/batch; 93h:20m:25s remains)
INFO - root - 2019-11-03 23:35:46.733659: step 33900, total loss = 0.68, predict loss = 0.17 (80.9 examples/sec; 0.049 sec/batch; 81h:55m:20s remains)
INFO - root - 2019-11-03 23:35:47.336441: step 33910, total loss = 0.73, predict loss = 0.17 (68.2 examples/sec; 0.059 sec/batch; 97h:11m:35s remains)
INFO - root - 2019-11-03 23:35:47.982314: step 33920, total loss = 0.56, predict loss = 0.12 (71.5 examples/sec; 0.056 sec/batch; 92h:44m:34s remains)
INFO - root - 2019-11-03 23:35:48.611752: step 33930, total loss = 0.61, predict loss = 0.15 (69.3 examples/sec; 0.058 sec/batch; 95h:40m:55s remains)
INFO - root - 2019-11-03 23:35:49.273682: step 33940, total loss = 0.59, predict loss = 0.13 (67.4 examples/sec; 0.059 sec/batch; 98h:24m:23s remains)
INFO - root - 2019-11-03 23:35:49.979845: step 33950, total loss = 0.55, predict loss = 0.11 (73.0 examples/sec; 0.055 sec/batch; 90h:50m:50s remains)
INFO - root - 2019-11-03 23:35:50.632607: step 33960, total loss = 0.59, predict loss = 0.13 (65.3 examples/sec; 0.061 sec/batch; 101h:32m:02s remains)
INFO - root - 2019-11-03 23:35:51.307487: step 33970, total loss = 0.62, predict loss = 0.15 (67.5 examples/sec; 0.059 sec/batch; 98h:09m:02s remains)
INFO - root - 2019-11-03 23:35:51.982448: step 33980, total loss = 0.67, predict loss = 0.16 (65.8 examples/sec; 0.061 sec/batch; 100h:46m:05s remains)
INFO - root - 2019-11-03 23:35:52.666594: step 33990, total loss = 0.67, predict loss = 0.15 (64.4 examples/sec; 0.062 sec/batch; 102h:57m:49s remains)
INFO - root - 2019-11-03 23:35:53.307896: step 34000, total loss = 0.79, predict loss = 0.18 (64.6 examples/sec; 0.062 sec/batch; 102h:36m:20s remains)
INFO - root - 2019-11-03 23:35:54.019634: step 34010, total loss = 0.68, predict loss = 0.16 (66.8 examples/sec; 0.060 sec/batch; 99h:14m:27s remains)
INFO - root - 2019-11-03 23:35:54.619294: step 34020, total loss = 0.58, predict loss = 0.14 (79.4 examples/sec; 0.050 sec/batch; 83h:28m:10s remains)
INFO - root - 2019-11-03 23:35:55.263289: step 34030, total loss = 0.67, predict loss = 0.15 (71.3 examples/sec; 0.056 sec/batch; 92h:57m:31s remains)
INFO - root - 2019-11-03 23:35:55.892236: step 34040, total loss = 0.74, predict loss = 0.18 (79.9 examples/sec; 0.050 sec/batch; 82h:57m:51s remains)
INFO - root - 2019-11-03 23:35:56.514662: step 34050, total loss = 0.56, predict loss = 0.12 (75.1 examples/sec; 0.053 sec/batch; 88h:13m:39s remains)
INFO - root - 2019-11-03 23:35:57.151861: step 34060, total loss = 0.52, predict loss = 0.11 (72.6 examples/sec; 0.055 sec/batch; 91h:18m:48s remains)
INFO - root - 2019-11-03 23:35:57.774567: step 34070, total loss = 0.34, predict loss = 0.07 (66.7 examples/sec; 0.060 sec/batch; 99h:25m:08s remains)
INFO - root - 2019-11-03 23:35:58.397299: step 34080, total loss = 0.48, predict loss = 0.11 (68.9 examples/sec; 0.058 sec/batch; 96h:10m:58s remains)
INFO - root - 2019-11-03 23:35:59.001935: step 34090, total loss = 0.44, predict loss = 0.10 (76.0 examples/sec; 0.053 sec/batch; 87h:13m:31s remains)
INFO - root - 2019-11-03 23:35:59.647795: step 34100, total loss = 0.77, predict loss = 0.20 (63.7 examples/sec; 0.063 sec/batch; 104h:05m:32s remains)
INFO - root - 2019-11-03 23:36:00.290365: step 34110, total loss = 0.53, predict loss = 0.13 (73.7 examples/sec; 0.054 sec/batch; 90h:00m:06s remains)
INFO - root - 2019-11-03 23:36:00.962744: step 34120, total loss = 0.53, predict loss = 0.12 (63.4 examples/sec; 0.063 sec/batch; 104h:29m:48s remains)
INFO - root - 2019-11-03 23:36:01.616741: step 34130, total loss = 0.74, predict loss = 0.17 (73.7 examples/sec; 0.054 sec/batch; 89h:58m:42s remains)
INFO - root - 2019-11-03 23:36:02.299973: step 34140, total loss = 0.86, predict loss = 0.21 (75.2 examples/sec; 0.053 sec/batch; 88h:09m:13s remains)
INFO - root - 2019-11-03 23:36:02.955250: step 34150, total loss = 0.75, predict loss = 0.17 (64.7 examples/sec; 0.062 sec/batch; 102h:26m:17s remains)
INFO - root - 2019-11-03 23:36:03.568387: step 34160, total loss = 0.85, predict loss = 0.21 (74.4 examples/sec; 0.054 sec/batch; 89h:04m:36s remains)
INFO - root - 2019-11-03 23:36:04.238219: step 34170, total loss = 0.85, predict loss = 0.22 (63.6 examples/sec; 0.063 sec/batch; 104h:15m:52s remains)
INFO - root - 2019-11-03 23:36:04.904126: step 34180, total loss = 0.91, predict loss = 0.22 (62.3 examples/sec; 0.064 sec/batch; 106h:21m:31s remains)
INFO - root - 2019-11-03 23:36:05.548687: step 34190, total loss = 0.93, predict loss = 0.22 (73.6 examples/sec; 0.054 sec/batch; 90h:03m:11s remains)
INFO - root - 2019-11-03 23:36:06.216539: step 34200, total loss = 0.89, predict loss = 0.22 (69.6 examples/sec; 0.058 sec/batch; 95h:18m:02s remains)
INFO - root - 2019-11-03 23:36:06.897844: step 34210, total loss = 0.94, predict loss = 0.23 (69.6 examples/sec; 0.057 sec/batch; 95h:16m:16s remains)
INFO - root - 2019-11-03 23:36:07.570339: step 34220, total loss = 0.87, predict loss = 0.21 (71.7 examples/sec; 0.056 sec/batch; 92h:25m:21s remains)
INFO - root - 2019-11-03 23:36:08.249993: step 34230, total loss = 0.61, predict loss = 0.14 (68.7 examples/sec; 0.058 sec/batch; 96h:28m:06s remains)
INFO - root - 2019-11-03 23:36:08.933456: step 34240, total loss = 0.70, predict loss = 0.16 (67.1 examples/sec; 0.060 sec/batch; 98h:44m:57s remains)
INFO - root - 2019-11-03 23:36:09.539900: step 34250, total loss = 0.65, predict loss = 0.15 (81.7 examples/sec; 0.049 sec/batch; 81h:07m:35s remains)
INFO - root - 2019-11-03 23:36:10.130461: step 34260, total loss = 0.69, predict loss = 0.17 (73.0 examples/sec; 0.055 sec/batch; 90h:48m:22s remains)
INFO - root - 2019-11-03 23:36:10.758175: step 34270, total loss = 0.71, predict loss = 0.17 (71.1 examples/sec; 0.056 sec/batch; 93h:14m:59s remains)
INFO - root - 2019-11-03 23:36:11.377095: step 34280, total loss = 0.75, predict loss = 0.17 (73.2 examples/sec; 0.055 sec/batch; 90h:34m:05s remains)
INFO - root - 2019-11-03 23:36:11.995807: step 34290, total loss = 0.68, predict loss = 0.16 (71.5 examples/sec; 0.056 sec/batch; 92h:40m:58s remains)
INFO - root - 2019-11-03 23:36:12.654319: step 34300, total loss = 0.62, predict loss = 0.15 (61.5 examples/sec; 0.065 sec/batch; 107h:47m:09s remains)
INFO - root - 2019-11-03 23:36:13.285986: step 34310, total loss = 0.67, predict loss = 0.16 (73.4 examples/sec; 0.055 sec/batch; 90h:21m:44s remains)
INFO - root - 2019-11-03 23:36:13.933658: step 34320, total loss = 0.60, predict loss = 0.15 (78.2 examples/sec; 0.051 sec/batch; 84h:45m:16s remains)
INFO - root - 2019-11-03 23:36:14.559892: step 34330, total loss = 0.55, predict loss = 0.13 (67.8 examples/sec; 0.059 sec/batch; 97h:48m:41s remains)
INFO - root - 2019-11-03 23:36:15.175089: step 34340, total loss = 0.64, predict loss = 0.14 (73.4 examples/sec; 0.054 sec/batch; 90h:16m:12s remains)
INFO - root - 2019-11-03 23:36:15.818171: step 34350, total loss = 0.62, predict loss = 0.14 (70.0 examples/sec; 0.057 sec/batch; 94h:41m:22s remains)
INFO - root - 2019-11-03 23:36:16.444024: step 34360, total loss = 0.65, predict loss = 0.16 (76.7 examples/sec; 0.052 sec/batch; 86h:27m:50s remains)
INFO - root - 2019-11-03 23:36:17.067773: step 34370, total loss = 0.55, predict loss = 0.12 (76.8 examples/sec; 0.052 sec/batch; 86h:18m:05s remains)
INFO - root - 2019-11-03 23:36:17.708776: step 34380, total loss = 0.55, predict loss = 0.12 (68.2 examples/sec; 0.059 sec/batch; 97h:13m:01s remains)
INFO - root - 2019-11-03 23:36:18.356908: step 34390, total loss = 0.71, predict loss = 0.16 (65.3 examples/sec; 0.061 sec/batch; 101h:27m:07s remains)
INFO - root - 2019-11-03 23:36:18.978074: step 34400, total loss = 0.47, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 93h:55m:21s remains)
INFO - root - 2019-11-03 23:36:19.646317: step 34410, total loss = 0.47, predict loss = 0.11 (67.8 examples/sec; 0.059 sec/batch; 97h:47m:43s remains)
INFO - root - 2019-11-03 23:36:20.282197: step 34420, total loss = 0.63, predict loss = 0.15 (72.1 examples/sec; 0.056 sec/batch; 91h:58m:25s remains)
INFO - root - 2019-11-03 23:36:20.911615: step 34430, total loss = 0.53, predict loss = 0.11 (68.8 examples/sec; 0.058 sec/batch; 96h:24m:21s remains)
INFO - root - 2019-11-03 23:36:21.531267: step 34440, total loss = 0.39, predict loss = 0.09 (79.7 examples/sec; 0.050 sec/batch; 83h:07m:36s remains)
INFO - root - 2019-11-03 23:36:22.167919: step 34450, total loss = 0.59, predict loss = 0.13 (61.0 examples/sec; 0.066 sec/batch; 108h:36m:39s remains)
INFO - root - 2019-11-03 23:36:22.786861: step 34460, total loss = 0.61, predict loss = 0.13 (82.9 examples/sec; 0.048 sec/batch; 79h:55m:54s remains)
INFO - root - 2019-11-03 23:36:23.422107: step 34470, total loss = 0.56, predict loss = 0.12 (67.2 examples/sec; 0.060 sec/batch; 98h:41m:18s remains)
INFO - root - 2019-11-03 23:36:24.096336: step 34480, total loss = 0.51, predict loss = 0.12 (67.9 examples/sec; 0.059 sec/batch; 97h:41m:15s remains)
INFO - root - 2019-11-03 23:36:24.705130: step 34490, total loss = 0.58, predict loss = 0.13 (77.7 examples/sec; 0.051 sec/batch; 85h:19m:38s remains)
INFO - root - 2019-11-03 23:36:25.326276: step 34500, total loss = 0.43, predict loss = 0.10 (65.4 examples/sec; 0.061 sec/batch; 101h:17m:18s remains)
INFO - root - 2019-11-03 23:36:25.962721: step 34510, total loss = 0.59, predict loss = 0.13 (74.8 examples/sec; 0.053 sec/batch; 88h:33m:39s remains)
INFO - root - 2019-11-03 23:36:26.615516: step 34520, total loss = 0.58, predict loss = 0.13 (75.5 examples/sec; 0.053 sec/batch; 87h:45m:35s remains)
INFO - root - 2019-11-03 23:36:27.255721: step 34530, total loss = 0.65, predict loss = 0.15 (64.0 examples/sec; 0.063 sec/batch; 103h:35m:45s remains)
INFO - root - 2019-11-03 23:36:27.903794: step 34540, total loss = 0.54, predict loss = 0.13 (66.6 examples/sec; 0.060 sec/batch; 99h:30m:44s remains)
INFO - root - 2019-11-03 23:36:28.557269: step 34550, total loss = 0.67, predict loss = 0.16 (68.1 examples/sec; 0.059 sec/batch; 97h:23m:47s remains)
INFO - root - 2019-11-03 23:36:29.244268: step 34560, total loss = 0.71, predict loss = 0.17 (71.1 examples/sec; 0.056 sec/batch; 93h:11m:33s remains)
INFO - root - 2019-11-03 23:36:29.875458: step 34570, total loss = 0.71, predict loss = 0.19 (64.8 examples/sec; 0.062 sec/batch; 102h:13m:16s remains)
INFO - root - 2019-11-03 23:36:30.541609: step 34580, total loss = 0.73, predict loss = 0.17 (67.9 examples/sec; 0.059 sec/batch; 97h:41m:05s remains)
INFO - root - 2019-11-03 23:36:31.192141: step 34590, total loss = 0.83, predict loss = 0.19 (66.2 examples/sec; 0.060 sec/batch; 100h:08m:41s remains)
INFO - root - 2019-11-03 23:36:31.817860: step 34600, total loss = 0.72, predict loss = 0.17 (75.4 examples/sec; 0.053 sec/batch; 87h:57m:04s remains)
INFO - root - 2019-11-03 23:36:32.463213: step 34610, total loss = 0.80, predict loss = 0.19 (75.8 examples/sec; 0.053 sec/batch; 87h:28m:33s remains)
INFO - root - 2019-11-03 23:36:33.113194: step 34620, total loss = 0.88, predict loss = 0.21 (63.1 examples/sec; 0.063 sec/batch; 105h:06m:37s remains)
INFO - root - 2019-11-03 23:36:33.712944: step 34630, total loss = 0.86, predict loss = 0.21 (79.0 examples/sec; 0.051 sec/batch; 83h:56m:47s remains)
INFO - root - 2019-11-03 23:36:34.338940: step 34640, total loss = 0.70, predict loss = 0.16 (65.7 examples/sec; 0.061 sec/batch; 100h:51m:49s remains)
INFO - root - 2019-11-03 23:36:34.951939: step 34650, total loss = 0.69, predict loss = 0.16 (73.0 examples/sec; 0.055 sec/batch; 90h:44m:33s remains)
INFO - root - 2019-11-03 23:36:35.629291: step 34660, total loss = 0.63, predict loss = 0.14 (70.8 examples/sec; 0.057 sec/batch; 93h:38m:29s remains)
INFO - root - 2019-11-03 23:36:36.223740: step 34670, total loss = 0.59, predict loss = 0.13 (68.2 examples/sec; 0.059 sec/batch; 97h:12m:10s remains)
INFO - root - 2019-11-03 23:36:36.834241: step 34680, total loss = 0.58, predict loss = 0.13 (71.0 examples/sec; 0.056 sec/batch; 93h:18m:06s remains)
INFO - root - 2019-11-03 23:36:37.454295: step 34690, total loss = 0.48, predict loss = 0.12 (65.0 examples/sec; 0.062 sec/batch; 101h:55m:15s remains)
INFO - root - 2019-11-03 23:36:38.088909: step 34700, total loss = 0.62, predict loss = 0.14 (74.2 examples/sec; 0.054 sec/batch; 89h:20m:17s remains)
INFO - root - 2019-11-03 23:36:38.753178: step 34710, total loss = 0.54, predict loss = 0.12 (71.0 examples/sec; 0.056 sec/batch; 93h:18m:56s remains)
INFO - root - 2019-11-03 23:36:39.416424: step 34720, total loss = 0.40, predict loss = 0.09 (75.3 examples/sec; 0.053 sec/batch; 88h:02m:09s remains)
INFO - root - 2019-11-03 23:36:40.114113: step 34730, total loss = 0.62, predict loss = 0.14 (66.0 examples/sec; 0.061 sec/batch; 100h:29m:14s remains)
INFO - root - 2019-11-03 23:36:40.775259: step 34740, total loss = 0.55, predict loss = 0.13 (71.9 examples/sec; 0.056 sec/batch; 92h:14m:06s remains)
INFO - root - 2019-11-03 23:36:41.438055: step 34750, total loss = 0.51, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 100h:30m:08s remains)
INFO - root - 2019-11-03 23:36:42.115610: step 34760, total loss = 0.50, predict loss = 0.12 (76.9 examples/sec; 0.052 sec/batch; 86h:13m:40s remains)
INFO - root - 2019-11-03 23:36:42.716382: step 34770, total loss = 0.48, predict loss = 0.11 (73.1 examples/sec; 0.055 sec/batch; 90h:41m:56s remains)
INFO - root - 2019-11-03 23:36:43.353395: step 34780, total loss = 0.62, predict loss = 0.14 (81.0 examples/sec; 0.049 sec/batch; 81h:52m:16s remains)
INFO - root - 2019-11-03 23:36:44.015772: step 34790, total loss = 0.60, predict loss = 0.14 (69.0 examples/sec; 0.058 sec/batch; 96h:06m:25s remains)
INFO - root - 2019-11-03 23:36:44.666589: step 34800, total loss = 0.78, predict loss = 0.19 (71.9 examples/sec; 0.056 sec/batch; 92h:08m:06s remains)
INFO - root - 2019-11-03 23:36:45.349176: step 34810, total loss = 0.63, predict loss = 0.14 (62.7 examples/sec; 0.064 sec/batch; 105h:42m:41s remains)
INFO - root - 2019-11-03 23:36:45.991537: step 34820, total loss = 0.73, predict loss = 0.18 (72.0 examples/sec; 0.056 sec/batch; 92h:06m:23s remains)
INFO - root - 2019-11-03 23:36:46.624793: step 34830, total loss = 0.63, predict loss = 0.16 (80.6 examples/sec; 0.050 sec/batch; 82h:11m:08s remains)
INFO - root - 2019-11-03 23:36:47.264568: step 34840, total loss = 0.67, predict loss = 0.16 (72.2 examples/sec; 0.055 sec/batch; 91h:50m:21s remains)
INFO - root - 2019-11-03 23:36:47.919988: step 34850, total loss = 0.71, predict loss = 0.18 (67.1 examples/sec; 0.060 sec/batch; 98h:46m:26s remains)
INFO - root - 2019-11-03 23:36:48.584154: step 34860, total loss = 0.72, predict loss = 0.16 (66.7 examples/sec; 0.060 sec/batch; 99h:21m:38s remains)
INFO - root - 2019-11-03 23:36:49.210285: step 34870, total loss = 0.70, predict loss = 0.18 (79.8 examples/sec; 0.050 sec/batch; 83h:00m:39s remains)
INFO - root - 2019-11-03 23:36:49.837020: step 34880, total loss = 0.71, predict loss = 0.16 (68.3 examples/sec; 0.059 sec/batch; 97h:03m:24s remains)
INFO - root - 2019-11-03 23:36:50.467856: step 34890, total loss = 0.53, predict loss = 0.12 (64.0 examples/sec; 0.062 sec/batch; 103h:29m:26s remains)
INFO - root - 2019-11-03 23:36:51.115556: step 34900, total loss = 0.67, predict loss = 0.17 (65.7 examples/sec; 0.061 sec/batch; 100h:56m:08s remains)
INFO - root - 2019-11-03 23:36:51.734741: step 34910, total loss = 0.53, predict loss = 0.12 (67.6 examples/sec; 0.059 sec/batch; 98h:00m:47s remains)
INFO - root - 2019-11-03 23:36:52.398387: step 34920, total loss = 0.52, predict loss = 0.12 (70.8 examples/sec; 0.056 sec/batch; 93h:36m:34s remains)
INFO - root - 2019-11-03 23:36:53.001590: step 34930, total loss = 0.74, predict loss = 0.17 (74.5 examples/sec; 0.054 sec/batch; 88h:59m:26s remains)
INFO - root - 2019-11-03 23:36:53.589990: step 34940, total loss = 0.55, predict loss = 0.12 (76.5 examples/sec; 0.052 sec/batch; 86h:41m:02s remains)
INFO - root - 2019-11-03 23:36:54.218225: step 34950, total loss = 0.61, predict loss = 0.13 (77.8 examples/sec; 0.051 sec/batch; 85h:13m:08s remains)
INFO - root - 2019-11-03 23:36:54.838634: step 34960, total loss = 0.54, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 94h:03m:45s remains)
INFO - root - 2019-11-03 23:36:55.454654: step 34970, total loss = 0.54, predict loss = 0.13 (81.8 examples/sec; 0.049 sec/batch; 80h:58m:45s remains)
INFO - root - 2019-11-03 23:36:56.076262: step 34980, total loss = 0.57, predict loss = 0.13 (71.4 examples/sec; 0.056 sec/batch; 92h:45m:50s remains)
INFO - root - 2019-11-03 23:36:56.702381: step 34990, total loss = 0.51, predict loss = 0.11 (69.9 examples/sec; 0.057 sec/batch; 94h:51m:58s remains)
INFO - root - 2019-11-03 23:36:57.321410: step 35000, total loss = 0.69, predict loss = 0.16 (71.5 examples/sec; 0.056 sec/batch; 92h:45m:14s remains)
INFO - root - 2019-11-03 23:36:57.918971: step 35010, total loss = 0.60, predict loss = 0.13 (82.4 examples/sec; 0.049 sec/batch; 80h:26m:50s remains)
INFO - root - 2019-11-03 23:36:58.511875: step 35020, total loss = 0.43, predict loss = 0.09 (71.1 examples/sec; 0.056 sec/batch; 93h:10m:16s remains)
INFO - root - 2019-11-03 23:36:59.140657: step 35030, total loss = 0.48, predict loss = 0.10 (73.1 examples/sec; 0.055 sec/batch; 90h:41m:20s remains)
INFO - root - 2019-11-03 23:36:59.716705: step 35040, total loss = 0.56, predict loss = 0.13 (79.2 examples/sec; 0.050 sec/batch; 83h:40m:00s remains)
INFO - root - 2019-11-03 23:37:00.348318: step 35050, total loss = 0.51, predict loss = 0.11 (71.0 examples/sec; 0.056 sec/batch; 93h:20m:31s remains)
INFO - root - 2019-11-03 23:37:00.956479: step 35060, total loss = 0.63, predict loss = 0.15 (63.6 examples/sec; 0.063 sec/batch; 104h:10m:55s remains)
INFO - root - 2019-11-03 23:37:01.576629: step 35070, total loss = 0.55, predict loss = 0.13 (63.0 examples/sec; 0.064 sec/batch; 105h:14m:09s remains)
INFO - root - 2019-11-03 23:37:02.224934: step 35080, total loss = 0.73, predict loss = 0.16 (72.8 examples/sec; 0.055 sec/batch; 90h:59m:46s remains)
INFO - root - 2019-11-03 23:37:02.830304: step 35090, total loss = 0.67, predict loss = 0.17 (68.6 examples/sec; 0.058 sec/batch; 96h:37m:05s remains)
INFO - root - 2019-11-03 23:37:03.444745: step 35100, total loss = 0.47, predict loss = 0.10 (79.5 examples/sec; 0.050 sec/batch; 83h:19m:43s remains)
INFO - root - 2019-11-03 23:37:04.048298: step 35110, total loss = 0.61, predict loss = 0.14 (84.9 examples/sec; 0.047 sec/batch; 78h:05m:11s remains)
INFO - root - 2019-11-03 23:37:04.667962: step 35120, total loss = 0.42, predict loss = 0.08 (76.3 examples/sec; 0.052 sec/batch; 86h:53m:44s remains)
INFO - root - 2019-11-03 23:37:05.273600: step 35130, total loss = 0.65, predict loss = 0.15 (73.0 examples/sec; 0.055 sec/batch; 90h:46m:43s remains)
INFO - root - 2019-11-03 23:37:05.891253: step 35140, total loss = 0.70, predict loss = 0.16 (81.0 examples/sec; 0.049 sec/batch; 81h:50m:53s remains)
INFO - root - 2019-11-03 23:37:06.515946: step 35150, total loss = 0.66, predict loss = 0.16 (66.1 examples/sec; 0.061 sec/batch; 100h:19m:13s remains)
INFO - root - 2019-11-03 23:37:07.162124: step 35160, total loss = 0.76, predict loss = 0.17 (66.5 examples/sec; 0.060 sec/batch; 99h:37m:06s remains)
INFO - root - 2019-11-03 23:37:07.818816: step 35170, total loss = 0.77, predict loss = 0.18 (64.3 examples/sec; 0.062 sec/batch; 103h:01m:43s remains)
INFO - root - 2019-11-03 23:37:08.510723: step 35180, total loss = 0.62, predict loss = 0.14 (74.3 examples/sec; 0.054 sec/batch; 89h:11m:02s remains)
INFO - root - 2019-11-03 23:37:09.155875: step 35190, total loss = 0.89, predict loss = 0.21 (63.7 examples/sec; 0.063 sec/batch; 104h:04m:40s remains)
INFO - root - 2019-11-03 23:37:09.774684: step 35200, total loss = 0.92, predict loss = 0.21 (69.4 examples/sec; 0.058 sec/batch; 95h:28m:02s remains)
INFO - root - 2019-11-03 23:37:10.428902: step 35210, total loss = 0.65, predict loss = 0.16 (65.0 examples/sec; 0.062 sec/batch; 101h:55m:35s remains)
INFO - root - 2019-11-03 23:37:11.022376: step 35220, total loss = 0.88, predict loss = 0.19 (74.0 examples/sec; 0.054 sec/batch; 89h:35m:18s remains)
INFO - root - 2019-11-03 23:37:11.634740: step 35230, total loss = 0.59, predict loss = 0.14 (74.8 examples/sec; 0.053 sec/batch; 88h:33m:31s remains)
INFO - root - 2019-11-03 23:37:12.257611: step 35240, total loss = 0.81, predict loss = 0.19 (67.9 examples/sec; 0.059 sec/batch; 97h:32m:34s remains)
INFO - root - 2019-11-03 23:37:12.913787: step 35250, total loss = 0.86, predict loss = 0.20 (69.6 examples/sec; 0.057 sec/batch; 95h:10m:16s remains)
INFO - root - 2019-11-03 23:37:13.546418: step 35260, total loss = 0.60, predict loss = 0.13 (69.1 examples/sec; 0.058 sec/batch; 95h:55m:45s remains)
INFO - root - 2019-11-03 23:37:14.196290: step 35270, total loss = 0.65, predict loss = 0.15 (63.2 examples/sec; 0.063 sec/batch; 104h:50m:21s remains)
INFO - root - 2019-11-03 23:37:14.853132: step 35280, total loss = 0.66, predict loss = 0.16 (69.5 examples/sec; 0.058 sec/batch; 95h:22m:57s remains)
INFO - root - 2019-11-03 23:37:15.515643: step 35290, total loss = 0.62, predict loss = 0.15 (69.8 examples/sec; 0.057 sec/batch; 94h:58m:00s remains)
INFO - root - 2019-11-03 23:37:16.178384: step 35300, total loss = 0.70, predict loss = 0.16 (63.6 examples/sec; 0.063 sec/batch; 104h:09m:35s remains)
INFO - root - 2019-11-03 23:37:16.773194: step 35310, total loss = 0.65, predict loss = 0.14 (72.9 examples/sec; 0.055 sec/batch; 90h:53m:16s remains)
INFO - root - 2019-11-03 23:37:17.392468: step 35320, total loss = 0.74, predict loss = 0.17 (77.1 examples/sec; 0.052 sec/batch; 85h:58m:59s remains)
INFO - root - 2019-11-03 23:37:17.997431: step 35330, total loss = 0.79, predict loss = 0.18 (67.8 examples/sec; 0.059 sec/batch; 97h:47m:20s remains)
INFO - root - 2019-11-03 23:37:18.663890: step 35340, total loss = 0.87, predict loss = 0.19 (67.1 examples/sec; 0.060 sec/batch; 98h:48m:41s remains)
INFO - root - 2019-11-03 23:37:19.302969: step 35350, total loss = 0.56, predict loss = 0.12 (77.6 examples/sec; 0.052 sec/batch; 85h:21m:26s remains)
INFO - root - 2019-11-03 23:37:19.946038: step 35360, total loss = 0.60, predict loss = 0.13 (68.3 examples/sec; 0.059 sec/batch; 96h:57m:59s remains)
INFO - root - 2019-11-03 23:37:20.605101: step 35370, total loss = 0.60, predict loss = 0.14 (77.4 examples/sec; 0.052 sec/batch; 85h:34m:34s remains)
INFO - root - 2019-11-03 23:37:21.215195: step 35380, total loss = 0.69, predict loss = 0.16 (72.5 examples/sec; 0.055 sec/batch; 91h:24m:55s remains)
INFO - root - 2019-11-03 23:37:21.829792: step 35390, total loss = 0.56, predict loss = 0.13 (70.7 examples/sec; 0.057 sec/batch; 93h:42m:05s remains)
INFO - root - 2019-11-03 23:37:22.485311: step 35400, total loss = 0.80, predict loss = 0.19 (78.0 examples/sec; 0.051 sec/batch; 84h:58m:34s remains)
INFO - root - 2019-11-03 23:37:23.076369: step 35410, total loss = 0.68, predict loss = 0.15 (75.2 examples/sec; 0.053 sec/batch; 88h:08m:37s remains)
INFO - root - 2019-11-03 23:37:23.688209: step 35420, total loss = 0.63, predict loss = 0.15 (76.1 examples/sec; 0.053 sec/batch; 87h:03m:17s remains)
INFO - root - 2019-11-03 23:37:24.291220: step 35430, total loss = 0.58, predict loss = 0.14 (92.0 examples/sec; 0.043 sec/batch; 72h:01m:56s remains)
INFO - root - 2019-11-03 23:37:24.807374: step 35440, total loss = 0.56, predict loss = 0.12 (86.4 examples/sec; 0.046 sec/batch; 76h:41m:51s remains)
INFO - root - 2019-11-03 23:37:25.282339: step 35450, total loss = 0.85, predict loss = 0.18 (96.6 examples/sec; 0.041 sec/batch; 68h:37m:38s remains)
INFO - root - 2019-11-03 23:37:26.328837: step 35460, total loss = 0.47, predict loss = 0.10 (76.2 examples/sec; 0.052 sec/batch; 86h:58m:37s remains)
INFO - root - 2019-11-03 23:37:26.948069: step 35470, total loss = 0.56, predict loss = 0.12 (68.2 examples/sec; 0.059 sec/batch; 97h:10m:41s remains)
INFO - root - 2019-11-03 23:37:27.577172: step 35480, total loss = 0.57, predict loss = 0.12 (71.7 examples/sec; 0.056 sec/batch; 92h:26m:55s remains)
INFO - root - 2019-11-03 23:37:28.204797: step 35490, total loss = 0.62, predict loss = 0.14 (73.2 examples/sec; 0.055 sec/batch; 90h:29m:27s remains)
INFO - root - 2019-11-03 23:37:28.855337: step 35500, total loss = 0.68, predict loss = 0.16 (69.7 examples/sec; 0.057 sec/batch; 95h:04m:12s remains)
INFO - root - 2019-11-03 23:37:29.492350: step 35510, total loss = 0.64, predict loss = 0.15 (76.8 examples/sec; 0.052 sec/batch; 86h:14m:26s remains)
INFO - root - 2019-11-03 23:37:30.123421: step 35520, total loss = 0.78, predict loss = 0.19 (70.3 examples/sec; 0.057 sec/batch; 94h:14m:26s remains)
INFO - root - 2019-11-03 23:37:30.770354: step 35530, total loss = 0.59, predict loss = 0.13 (69.2 examples/sec; 0.058 sec/batch; 95h:48m:16s remains)
INFO - root - 2019-11-03 23:37:31.418795: step 35540, total loss = 0.63, predict loss = 0.14 (75.8 examples/sec; 0.053 sec/batch; 87h:25m:35s remains)
INFO - root - 2019-11-03 23:37:32.021624: step 35550, total loss = 0.75, predict loss = 0.18 (82.0 examples/sec; 0.049 sec/batch; 80h:50m:09s remains)
INFO - root - 2019-11-03 23:37:32.717958: step 35560, total loss = 0.82, predict loss = 0.18 (56.7 examples/sec; 0.071 sec/batch; 116h:56m:14s remains)
INFO - root - 2019-11-03 23:37:33.356794: step 35570, total loss = 0.68, predict loss = 0.15 (84.6 examples/sec; 0.047 sec/batch; 78h:20m:55s remains)
INFO - root - 2019-11-03 23:37:33.986343: step 35580, total loss = 0.78, predict loss = 0.18 (77.4 examples/sec; 0.052 sec/batch; 85h:39m:16s remains)
INFO - root - 2019-11-03 23:37:34.621443: step 35590, total loss = 0.76, predict loss = 0.18 (72.7 examples/sec; 0.055 sec/batch; 91h:10m:23s remains)
INFO - root - 2019-11-03 23:37:35.255929: step 35600, total loss = 0.61, predict loss = 0.13 (73.1 examples/sec; 0.055 sec/batch; 90h:42m:36s remains)
INFO - root - 2019-11-03 23:37:35.919873: step 35610, total loss = 0.81, predict loss = 0.20 (81.3 examples/sec; 0.049 sec/batch; 81h:29m:47s remains)
INFO - root - 2019-11-03 23:37:36.497516: step 35620, total loss = 0.77, predict loss = 0.19 (90.2 examples/sec; 0.044 sec/batch; 73h:29m:23s remains)
INFO - root - 2019-11-03 23:37:37.117248: step 35630, total loss = 0.72, predict loss = 0.16 (76.8 examples/sec; 0.052 sec/batch; 86h:15m:04s remains)
INFO - root - 2019-11-03 23:37:37.749472: step 35640, total loss = 0.60, predict loss = 0.13 (79.1 examples/sec; 0.051 sec/batch; 83h:47m:19s remains)
INFO - root - 2019-11-03 23:37:38.393879: step 35650, total loss = 0.67, predict loss = 0.15 (65.2 examples/sec; 0.061 sec/batch; 101h:36m:46s remains)
INFO - root - 2019-11-03 23:37:39.075758: step 35660, total loss = 0.77, predict loss = 0.17 (64.3 examples/sec; 0.062 sec/batch; 103h:03m:32s remains)
INFO - root - 2019-11-03 23:37:39.769763: step 35670, total loss = 0.58, predict loss = 0.14 (71.3 examples/sec; 0.056 sec/batch; 92h:56m:53s remains)
INFO - root - 2019-11-03 23:37:40.430971: step 35680, total loss = 0.60, predict loss = 0.13 (69.3 examples/sec; 0.058 sec/batch; 95h:41m:10s remains)
INFO - root - 2019-11-03 23:37:41.098905: step 35690, total loss = 0.69, predict loss = 0.17 (65.2 examples/sec; 0.061 sec/batch; 101h:41m:17s remains)
INFO - root - 2019-11-03 23:37:41.786878: step 35700, total loss = 0.76, predict loss = 0.17 (61.6 examples/sec; 0.065 sec/batch; 107h:40m:05s remains)
INFO - root - 2019-11-03 23:37:42.425766: step 35710, total loss = 0.85, predict loss = 0.20 (69.5 examples/sec; 0.058 sec/batch; 95h:23m:30s remains)
INFO - root - 2019-11-03 23:37:43.082474: step 35720, total loss = 0.80, predict loss = 0.18 (67.6 examples/sec; 0.059 sec/batch; 98h:02m:22s remains)
INFO - root - 2019-11-03 23:37:43.751456: step 35730, total loss = 0.77, predict loss = 0.18 (71.6 examples/sec; 0.056 sec/batch; 92h:33m:18s remains)
INFO - root - 2019-11-03 23:37:44.412865: step 35740, total loss = 0.83, predict loss = 0.18 (66.9 examples/sec; 0.060 sec/batch; 99h:05m:09s remains)
INFO - root - 2019-11-03 23:37:45.069347: step 35750, total loss = 0.81, predict loss = 0.18 (65.3 examples/sec; 0.061 sec/batch; 101h:31m:30s remains)
INFO - root - 2019-11-03 23:37:45.731508: step 35760, total loss = 0.76, predict loss = 0.17 (71.0 examples/sec; 0.056 sec/batch; 93h:17m:31s remains)
INFO - root - 2019-11-03 23:37:46.355563: step 35770, total loss = 0.67, predict loss = 0.15 (69.3 examples/sec; 0.058 sec/batch; 95h:34m:40s remains)
INFO - root - 2019-11-03 23:37:47.002446: step 35780, total loss = 0.81, predict loss = 0.18 (67.9 examples/sec; 0.059 sec/batch; 97h:36m:38s remains)
INFO - root - 2019-11-03 23:37:47.649046: step 35790, total loss = 0.71, predict loss = 0.16 (64.5 examples/sec; 0.062 sec/batch; 102h:45m:24s remains)
INFO - root - 2019-11-03 23:37:48.285064: step 35800, total loss = 0.70, predict loss = 0.16 (68.8 examples/sec; 0.058 sec/batch; 96h:21m:50s remains)
INFO - root - 2019-11-03 23:37:48.888612: step 35810, total loss = 0.72, predict loss = 0.17 (71.2 examples/sec; 0.056 sec/batch; 93h:00m:49s remains)
INFO - root - 2019-11-03 23:37:49.497134: step 35820, total loss = 0.67, predict loss = 0.14 (79.1 examples/sec; 0.051 sec/batch; 83h:47m:49s remains)
INFO - root - 2019-11-03 23:37:50.147539: step 35830, total loss = 0.64, predict loss = 0.14 (64.1 examples/sec; 0.062 sec/batch; 103h:24m:00s remains)
INFO - root - 2019-11-03 23:37:50.807772: step 35840, total loss = 0.54, predict loss = 0.12 (67.3 examples/sec; 0.059 sec/batch; 98h:25m:15s remains)
INFO - root - 2019-11-03 23:37:51.434721: step 35850, total loss = 0.59, predict loss = 0.13 (62.4 examples/sec; 0.064 sec/batch; 106h:17m:03s remains)
INFO - root - 2019-11-03 23:37:52.122934: step 35860, total loss = 0.69, predict loss = 0.16 (58.2 examples/sec; 0.069 sec/batch; 113h:56m:30s remains)
INFO - root - 2019-11-03 23:37:52.758551: step 35870, total loss = 1.23, predict loss = 0.28 (73.8 examples/sec; 0.054 sec/batch; 89h:51m:00s remains)
INFO - root - 2019-11-03 23:37:53.385097: step 35880, total loss = 0.86, predict loss = 0.20 (77.6 examples/sec; 0.052 sec/batch; 85h:26m:53s remains)
INFO - root - 2019-11-03 23:37:54.046274: step 35890, total loss = 0.73, predict loss = 0.17 (70.4 examples/sec; 0.057 sec/batch; 94h:10m:12s remains)
INFO - root - 2019-11-03 23:37:54.719965: step 35900, total loss = 0.55, predict loss = 0.14 (62.2 examples/sec; 0.064 sec/batch; 106h:35m:57s remains)
INFO - root - 2019-11-03 23:37:55.397880: step 35910, total loss = 0.70, predict loss = 0.17 (74.2 examples/sec; 0.054 sec/batch; 89h:21m:23s remains)
INFO - root - 2019-11-03 23:37:56.055367: step 35920, total loss = 0.80, predict loss = 0.20 (77.8 examples/sec; 0.051 sec/batch; 85h:08m:26s remains)
INFO - root - 2019-11-03 23:37:56.666257: step 35930, total loss = 0.27, predict loss = 0.05 (69.7 examples/sec; 0.057 sec/batch; 95h:02m:59s remains)
INFO - root - 2019-11-03 23:37:57.332174: step 35940, total loss = 0.69, predict loss = 0.18 (67.9 examples/sec; 0.059 sec/batch; 97h:38m:28s remains)
INFO - root - 2019-11-03 23:37:57.990348: step 35950, total loss = 0.54, predict loss = 0.12 (65.9 examples/sec; 0.061 sec/batch; 100h:37m:07s remains)
INFO - root - 2019-11-03 23:37:58.652337: step 35960, total loss = 0.41, predict loss = 0.08 (66.6 examples/sec; 0.060 sec/batch; 99h:29m:20s remains)
INFO - root - 2019-11-03 23:37:59.357954: step 35970, total loss = 0.75, predict loss = 0.17 (58.9 examples/sec; 0.068 sec/batch; 112h:35m:36s remains)
INFO - root - 2019-11-03 23:38:00.074597: step 35980, total loss = 0.68, predict loss = 0.16 (69.8 examples/sec; 0.057 sec/batch; 94h:54m:39s remains)
INFO - root - 2019-11-03 23:38:00.774850: step 35990, total loss = 0.68, predict loss = 0.16 (61.3 examples/sec; 0.065 sec/batch; 108h:01m:36s remains)
INFO - root - 2019-11-03 23:38:01.442086: step 36000, total loss = 0.64, predict loss = 0.15 (68.0 examples/sec; 0.059 sec/batch; 97h:29m:36s remains)
INFO - root - 2019-11-03 23:38:02.120856: step 36010, total loss = 0.66, predict loss = 0.15 (59.8 examples/sec; 0.067 sec/batch; 110h:46m:50s remains)
INFO - root - 2019-11-03 23:38:02.755948: step 36020, total loss = 0.74, predict loss = 0.18 (76.7 examples/sec; 0.052 sec/batch; 86h:21m:39s remains)
INFO - root - 2019-11-03 23:38:03.381314: step 36030, total loss = 0.61, predict loss = 0.15 (68.8 examples/sec; 0.058 sec/batch; 96h:17m:21s remains)
INFO - root - 2019-11-03 23:38:03.993428: step 36040, total loss = 0.59, predict loss = 0.14 (72.0 examples/sec; 0.056 sec/batch; 91h:58m:42s remains)
INFO - root - 2019-11-03 23:38:04.614200: step 36050, total loss = 0.35, predict loss = 0.08 (79.1 examples/sec; 0.051 sec/batch; 83h:45m:14s remains)
INFO - root - 2019-11-03 23:38:05.220944: step 36060, total loss = 0.44, predict loss = 0.09 (72.2 examples/sec; 0.055 sec/batch; 91h:49m:09s remains)
INFO - root - 2019-11-03 23:38:05.835548: step 36070, total loss = 0.45, predict loss = 0.10 (83.8 examples/sec; 0.048 sec/batch; 79h:05m:50s remains)
INFO - root - 2019-11-03 23:38:06.477828: step 36080, total loss = 0.84, predict loss = 0.20 (68.9 examples/sec; 0.058 sec/batch; 96h:13m:09s remains)
INFO - root - 2019-11-03 23:38:07.114985: step 36090, total loss = 0.62, predict loss = 0.14 (69.9 examples/sec; 0.057 sec/batch; 94h:49m:45s remains)
INFO - root - 2019-11-03 23:38:07.731433: step 36100, total loss = 0.49, predict loss = 0.10 (77.3 examples/sec; 0.052 sec/batch; 85h:43m:23s remains)
INFO - root - 2019-11-03 23:38:08.376370: step 36110, total loss = 0.63, predict loss = 0.13 (61.3 examples/sec; 0.065 sec/batch; 108h:10m:12s remains)
INFO - root - 2019-11-03 23:38:09.004476: step 36120, total loss = 0.68, predict loss = 0.19 (72.8 examples/sec; 0.055 sec/batch; 90h:59m:55s remains)
INFO - root - 2019-11-03 23:38:09.629996: step 36130, total loss = 0.76, predict loss = 0.16 (74.3 examples/sec; 0.054 sec/batch; 89h:12m:51s remains)
INFO - root - 2019-11-03 23:38:10.255277: step 36140, total loss = 0.71, predict loss = 0.16 (75.4 examples/sec; 0.053 sec/batch; 87h:52m:41s remains)
INFO - root - 2019-11-03 23:38:10.859066: step 36150, total loss = 0.62, predict loss = 0.16 (75.0 examples/sec; 0.053 sec/batch; 88h:21m:48s remains)
INFO - root - 2019-11-03 23:38:11.428545: step 36160, total loss = 0.73, predict loss = 0.17 (81.8 examples/sec; 0.049 sec/batch; 80h:59m:45s remains)
INFO - root - 2019-11-03 23:38:12.053159: step 36170, total loss = 0.76, predict loss = 0.19 (69.5 examples/sec; 0.058 sec/batch; 95h:24m:46s remains)
INFO - root - 2019-11-03 23:38:12.679398: step 36180, total loss = 0.78, predict loss = 0.19 (74.4 examples/sec; 0.054 sec/batch; 89h:01m:15s remains)
INFO - root - 2019-11-03 23:38:13.291369: step 36190, total loss = 0.60, predict loss = 0.14 (64.5 examples/sec; 0.062 sec/batch; 102h:48m:16s remains)
INFO - root - 2019-11-03 23:38:13.904622: step 36200, total loss = 0.43, predict loss = 0.09 (76.3 examples/sec; 0.052 sec/batch; 86h:52m:41s remains)
INFO - root - 2019-11-03 23:38:14.552015: step 36210, total loss = 0.71, predict loss = 0.15 (73.8 examples/sec; 0.054 sec/batch; 89h:47m:44s remains)
INFO - root - 2019-11-03 23:38:15.192247: step 36220, total loss = 0.77, predict loss = 0.19 (70.0 examples/sec; 0.057 sec/batch; 94h:37m:15s remains)
INFO - root - 2019-11-03 23:38:15.853440: step 36230, total loss = 0.59, predict loss = 0.13 (73.3 examples/sec; 0.055 sec/batch; 90h:22m:26s remains)
INFO - root - 2019-11-03 23:38:16.490183: step 36240, total loss = 0.59, predict loss = 0.15 (73.7 examples/sec; 0.054 sec/batch; 89h:55m:00s remains)
INFO - root - 2019-11-03 23:38:17.129359: step 36250, total loss = 0.81, predict loss = 0.19 (66.2 examples/sec; 0.060 sec/batch; 100h:09m:48s remains)
INFO - root - 2019-11-03 23:38:17.793107: step 36260, total loss = 0.66, predict loss = 0.15 (68.1 examples/sec; 0.059 sec/batch; 97h:19m:02s remains)
INFO - root - 2019-11-03 23:38:18.446920: step 36270, total loss = 0.66, predict loss = 0.16 (68.0 examples/sec; 0.059 sec/batch; 97h:30m:39s remains)
INFO - root - 2019-11-03 23:38:19.108932: step 36280, total loss = 0.59, predict loss = 0.15 (65.2 examples/sec; 0.061 sec/batch; 101h:38m:40s remains)
INFO - root - 2019-11-03 23:38:19.764259: step 36290, total loss = 0.46, predict loss = 0.10 (69.8 examples/sec; 0.057 sec/batch; 94h:52m:33s remains)
INFO - root - 2019-11-03 23:38:20.419739: step 36300, total loss = 0.31, predict loss = 0.06 (68.5 examples/sec; 0.058 sec/batch; 96h:43m:02s remains)
INFO - root - 2019-11-03 23:38:21.026772: step 36310, total loss = 0.32, predict loss = 0.07 (74.9 examples/sec; 0.053 sec/batch; 88h:26m:30s remains)
INFO - root - 2019-11-03 23:38:21.648487: step 36320, total loss = 0.37, predict loss = 0.08 (65.6 examples/sec; 0.061 sec/batch; 101h:04m:53s remains)
INFO - root - 2019-11-03 23:38:22.850052: step 36330, total loss = 0.46, predict loss = 0.10 (69.2 examples/sec; 0.058 sec/batch; 95h:46m:58s remains)
INFO - root - 2019-11-03 23:38:23.495995: step 36340, total loss = 0.43, predict loss = 0.10 (74.2 examples/sec; 0.054 sec/batch; 89h:17m:42s remains)
INFO - root - 2019-11-03 23:38:24.123539: step 36350, total loss = 0.47, predict loss = 0.11 (71.3 examples/sec; 0.056 sec/batch; 92h:55m:48s remains)
INFO - root - 2019-11-03 23:38:24.757198: step 36360, total loss = 0.48, predict loss = 0.11 (76.3 examples/sec; 0.052 sec/batch; 86h:53m:15s remains)
INFO - root - 2019-11-03 23:38:25.434704: step 36370, total loss = 0.79, predict loss = 0.21 (62.0 examples/sec; 0.064 sec/batch; 106h:49m:43s remains)
INFO - root - 2019-11-03 23:38:26.082508: step 36380, total loss = 0.59, predict loss = 0.14 (74.0 examples/sec; 0.054 sec/batch; 89h:33m:11s remains)
INFO - root - 2019-11-03 23:38:26.751647: step 36390, total loss = 0.54, predict loss = 0.12 (72.0 examples/sec; 0.056 sec/batch; 91h:59m:01s remains)
INFO - root - 2019-11-03 23:38:27.391483: step 36400, total loss = 0.45, predict loss = 0.10 (66.8 examples/sec; 0.060 sec/batch; 99h:13m:27s remains)
INFO - root - 2019-11-03 23:38:28.042607: step 36410, total loss = 0.40, predict loss = 0.09 (78.2 examples/sec; 0.051 sec/batch; 84h:41m:14s remains)
INFO - root - 2019-11-03 23:38:28.702169: step 36420, total loss = 0.62, predict loss = 0.15 (71.2 examples/sec; 0.056 sec/batch; 93h:04m:30s remains)
INFO - root - 2019-11-03 23:38:29.326091: step 36430, total loss = 0.59, predict loss = 0.13 (71.7 examples/sec; 0.056 sec/batch; 92h:27m:22s remains)
INFO - root - 2019-11-03 23:38:29.979343: step 36440, total loss = 0.47, predict loss = 0.10 (70.6 examples/sec; 0.057 sec/batch; 93h:49m:26s remains)
INFO - root - 2019-11-03 23:38:30.640149: step 36450, total loss = 0.51, predict loss = 0.11 (67.7 examples/sec; 0.059 sec/batch; 97h:50m:29s remains)
INFO - root - 2019-11-03 23:38:31.257734: step 36460, total loss = 0.61, predict loss = 0.15 (78.3 examples/sec; 0.051 sec/batch; 84h:38m:49s remains)
INFO - root - 2019-11-03 23:38:31.881248: step 36470, total loss = 0.65, predict loss = 0.16 (68.6 examples/sec; 0.058 sec/batch; 96h:33m:32s remains)
INFO - root - 2019-11-03 23:38:32.560965: step 36480, total loss = 0.69, predict loss = 0.16 (66.0 examples/sec; 0.061 sec/batch; 100h:20m:27s remains)
INFO - root - 2019-11-03 23:38:33.218610: step 36490, total loss = 0.43, predict loss = 0.11 (68.6 examples/sec; 0.058 sec/batch; 96h:38m:45s remains)
INFO - root - 2019-11-03 23:38:33.842101: step 36500, total loss = 0.59, predict loss = 0.13 (79.7 examples/sec; 0.050 sec/batch; 83h:08m:03s remains)
INFO - root - 2019-11-03 23:38:34.469323: step 36510, total loss = 0.43, predict loss = 0.10 (67.8 examples/sec; 0.059 sec/batch; 97h:44m:42s remains)
INFO - root - 2019-11-03 23:38:35.080886: step 36520, total loss = 0.55, predict loss = 0.13 (78.1 examples/sec; 0.051 sec/batch; 84h:48m:51s remains)
INFO - root - 2019-11-03 23:38:35.687144: step 36530, total loss = 0.68, predict loss = 0.16 (73.4 examples/sec; 0.054 sec/batch; 90h:15m:04s remains)
INFO - root - 2019-11-03 23:38:36.300642: step 36540, total loss = 0.45, predict loss = 0.11 (69.2 examples/sec; 0.058 sec/batch; 95h:44m:25s remains)
INFO - root - 2019-11-03 23:38:36.916639: step 36550, total loss = 0.35, predict loss = 0.08 (68.4 examples/sec; 0.059 sec/batch; 96h:54m:44s remains)
INFO - root - 2019-11-03 23:38:37.521416: step 36560, total loss = 0.55, predict loss = 0.14 (76.3 examples/sec; 0.052 sec/batch; 86h:48m:31s remains)
INFO - root - 2019-11-03 23:38:38.135191: step 36570, total loss = 0.47, predict loss = 0.11 (72.1 examples/sec; 0.056 sec/batch; 91h:56m:38s remains)
INFO - root - 2019-11-03 23:38:38.785564: step 36580, total loss = 0.49, predict loss = 0.11 (67.9 examples/sec; 0.059 sec/batch; 97h:36m:24s remains)
INFO - root - 2019-11-03 23:38:39.415725: step 36590, total loss = 0.67, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 91h:43m:58s remains)
INFO - root - 2019-11-03 23:38:40.056636: step 36600, total loss = 0.42, predict loss = 0.09 (78.4 examples/sec; 0.051 sec/batch; 84h:33m:10s remains)
INFO - root - 2019-11-03 23:38:40.669840: step 36610, total loss = 0.63, predict loss = 0.15 (75.1 examples/sec; 0.053 sec/batch; 88h:11m:36s remains)
INFO - root - 2019-11-03 23:38:41.292916: step 36620, total loss = 0.63, predict loss = 0.15 (73.8 examples/sec; 0.054 sec/batch; 89h:44m:29s remains)
INFO - root - 2019-11-03 23:38:41.932548: step 36630, total loss = 0.47, predict loss = 0.10 (67.0 examples/sec; 0.060 sec/batch; 98h:51m:16s remains)
INFO - root - 2019-11-03 23:38:42.544922: step 36640, total loss = 0.55, predict loss = 0.12 (81.2 examples/sec; 0.049 sec/batch; 81h:38m:40s remains)
INFO - root - 2019-11-03 23:38:43.221954: step 36650, total loss = 0.42, predict loss = 0.09 (65.1 examples/sec; 0.061 sec/batch; 101h:51m:02s remains)
INFO - root - 2019-11-03 23:38:43.859151: step 36660, total loss = 0.69, predict loss = 0.16 (68.0 examples/sec; 0.059 sec/batch; 97h:26m:10s remains)
INFO - root - 2019-11-03 23:38:44.497985: step 36670, total loss = 0.72, predict loss = 0.17 (78.8 examples/sec; 0.051 sec/batch; 84h:07m:55s remains)
INFO - root - 2019-11-03 23:38:45.165330: step 36680, total loss = 0.60, predict loss = 0.14 (67.7 examples/sec; 0.059 sec/batch; 97h:53m:47s remains)
INFO - root - 2019-11-03 23:38:45.839193: step 36690, total loss = 0.61, predict loss = 0.14 (65.8 examples/sec; 0.061 sec/batch; 100h:41m:24s remains)
INFO - root - 2019-11-03 23:38:46.435527: step 36700, total loss = 0.64, predict loss = 0.15 (78.9 examples/sec; 0.051 sec/batch; 83h:58m:39s remains)
INFO - root - 2019-11-03 23:38:47.024920: step 36710, total loss = 0.62, predict loss = 0.14 (76.0 examples/sec; 0.053 sec/batch; 87h:12m:10s remains)
INFO - root - 2019-11-03 23:38:47.629455: step 36720, total loss = 0.56, predict loss = 0.13 (70.3 examples/sec; 0.057 sec/batch; 94h:12m:45s remains)
INFO - root - 2019-11-03 23:38:48.282763: step 36730, total loss = 0.48, predict loss = 0.11 (69.5 examples/sec; 0.058 sec/batch; 95h:20m:28s remains)
INFO - root - 2019-11-03 23:38:48.921458: step 36740, total loss = 0.64, predict loss = 0.15 (64.0 examples/sec; 0.062 sec/batch; 103h:28m:11s remains)
INFO - root - 2019-11-03 23:38:49.566815: step 36750, total loss = 0.69, predict loss = 0.16 (64.3 examples/sec; 0.062 sec/batch; 103h:07m:22s remains)
INFO - root - 2019-11-03 23:38:50.230923: step 36760, total loss = 0.70, predict loss = 0.16 (66.2 examples/sec; 0.060 sec/batch; 100h:02m:09s remains)
INFO - root - 2019-11-03 23:38:50.891943: step 36770, total loss = 0.44, predict loss = 0.09 (58.2 examples/sec; 0.069 sec/batch; 113h:49m:29s remains)
INFO - root - 2019-11-03 23:38:51.541716: step 36780, total loss = 0.55, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 93h:57m:13s remains)
INFO - root - 2019-11-03 23:38:52.199364: step 36790, total loss = 0.37, predict loss = 0.09 (56.9 examples/sec; 0.070 sec/batch; 116h:29m:17s remains)
INFO - root - 2019-11-03 23:38:52.791881: step 36800, total loss = 0.39, predict loss = 0.08 (76.1 examples/sec; 0.053 sec/batch; 87h:02m:11s remains)
INFO - root - 2019-11-03 23:38:53.462881: step 36810, total loss = 0.49, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 92h:50m:49s remains)
INFO - root - 2019-11-03 23:38:54.111380: step 36820, total loss = 0.38, predict loss = 0.08 (64.7 examples/sec; 0.062 sec/batch; 102h:28m:58s remains)
INFO - root - 2019-11-03 23:38:54.768450: step 36830, total loss = 0.58, predict loss = 0.13 (72.6 examples/sec; 0.055 sec/batch; 91h:14m:49s remains)
INFO - root - 2019-11-03 23:38:55.390707: step 36840, total loss = 0.51, predict loss = 0.12 (69.9 examples/sec; 0.057 sec/batch; 94h:49m:21s remains)
INFO - root - 2019-11-03 23:38:56.034409: step 36850, total loss = 0.60, predict loss = 0.13 (61.7 examples/sec; 0.065 sec/batch; 107h:20m:25s remains)
INFO - root - 2019-11-03 23:38:56.645080: step 36860, total loss = 0.80, predict loss = 0.17 (67.5 examples/sec; 0.059 sec/batch; 98h:06m:45s remains)
INFO - root - 2019-11-03 23:38:57.230642: step 36870, total loss = 0.69, predict loss = 0.16 (74.8 examples/sec; 0.053 sec/batch; 88h:35m:07s remains)
INFO - root - 2019-11-03 23:38:57.840746: step 36880, total loss = 0.79, predict loss = 0.19 (73.0 examples/sec; 0.055 sec/batch; 90h:45m:52s remains)
INFO - root - 2019-11-03 23:38:58.458814: step 36890, total loss = 0.78, predict loss = 0.18 (77.1 examples/sec; 0.052 sec/batch; 85h:58m:31s remains)
INFO - root - 2019-11-03 23:38:59.072968: step 36900, total loss = 0.82, predict loss = 0.19 (69.1 examples/sec; 0.058 sec/batch; 95h:56m:05s remains)
INFO - root - 2019-11-03 23:38:59.689840: step 36910, total loss = 0.98, predict loss = 0.24 (80.0 examples/sec; 0.050 sec/batch; 82h:50m:36s remains)
INFO - root - 2019-11-03 23:39:00.323321: step 36920, total loss = 0.81, predict loss = 0.19 (81.1 examples/sec; 0.049 sec/batch; 81h:43m:59s remains)
INFO - root - 2019-11-03 23:39:00.952427: step 36930, total loss = 0.92, predict loss = 0.21 (68.2 examples/sec; 0.059 sec/batch; 97h:11m:12s remains)
INFO - root - 2019-11-03 23:39:01.618786: step 36940, total loss = 0.96, predict loss = 0.24 (67.4 examples/sec; 0.059 sec/batch; 98h:21m:53s remains)
INFO - root - 2019-11-03 23:39:02.235297: step 36950, total loss = 0.76, predict loss = 0.18 (69.9 examples/sec; 0.057 sec/batch; 94h:46m:21s remains)
INFO - root - 2019-11-03 23:39:02.849796: step 36960, total loss = 0.70, predict loss = 0.16 (66.7 examples/sec; 0.060 sec/batch; 99h:16m:38s remains)
INFO - root - 2019-11-03 23:39:03.502041: step 36970, total loss = 0.68, predict loss = 0.17 (70.1 examples/sec; 0.057 sec/batch; 94h:33m:01s remains)
INFO - root - 2019-11-03 23:39:04.154649: step 36980, total loss = 0.65, predict loss = 0.14 (74.4 examples/sec; 0.054 sec/batch; 89h:01m:49s remains)
INFO - root - 2019-11-03 23:39:04.804227: step 36990, total loss = 0.62, predict loss = 0.14 (74.3 examples/sec; 0.054 sec/batch; 89h:07m:06s remains)
INFO - root - 2019-11-03 23:39:05.445235: step 37000, total loss = 0.75, predict loss = 0.17 (77.4 examples/sec; 0.052 sec/batch; 85h:34m:20s remains)
INFO - root - 2019-11-03 23:39:06.098869: step 37010, total loss = 0.68, predict loss = 0.16 (65.0 examples/sec; 0.062 sec/batch; 101h:55m:36s remains)
INFO - root - 2019-11-03 23:39:06.725378: step 37020, total loss = 0.68, predict loss = 0.15 (75.5 examples/sec; 0.053 sec/batch; 87h:47m:24s remains)
INFO - root - 2019-11-03 23:39:07.327043: step 37030, total loss = 0.73, predict loss = 0.17 (72.4 examples/sec; 0.055 sec/batch; 91h:33m:14s remains)
INFO - root - 2019-11-03 23:39:07.970789: step 37040, total loss = 0.62, predict loss = 0.14 (75.5 examples/sec; 0.053 sec/batch; 87h:43m:33s remains)
INFO - root - 2019-11-03 23:39:08.664384: step 37050, total loss = 0.64, predict loss = 0.15 (59.4 examples/sec; 0.067 sec/batch; 111h:37m:58s remains)
INFO - root - 2019-11-03 23:39:09.327206: step 37060, total loss = 0.59, predict loss = 0.14 (76.7 examples/sec; 0.052 sec/batch; 86h:23m:57s remains)
INFO - root - 2019-11-03 23:39:09.957451: step 37070, total loss = 0.69, predict loss = 0.16 (68.1 examples/sec; 0.059 sec/batch; 97h:15m:38s remains)
INFO - root - 2019-11-03 23:39:10.595801: step 37080, total loss = 0.58, predict loss = 0.14 (69.6 examples/sec; 0.057 sec/batch; 95h:13m:51s remains)
INFO - root - 2019-11-03 23:39:11.215202: step 37090, total loss = 0.57, predict loss = 0.12 (76.1 examples/sec; 0.053 sec/batch; 87h:01m:35s remains)
INFO - root - 2019-11-03 23:39:11.860126: step 37100, total loss = 0.57, predict loss = 0.13 (68.1 examples/sec; 0.059 sec/batch; 97h:18m:15s remains)
INFO - root - 2019-11-03 23:39:12.511456: step 37110, total loss = 0.65, predict loss = 0.15 (72.6 examples/sec; 0.055 sec/batch; 91h:13m:01s remains)
INFO - root - 2019-11-03 23:39:13.168764: step 37120, total loss = 0.72, predict loss = 0.17 (70.7 examples/sec; 0.057 sec/batch; 93h:43m:32s remains)
INFO - root - 2019-11-03 23:39:13.789115: step 37130, total loss = 0.51, predict loss = 0.12 (72.0 examples/sec; 0.056 sec/batch; 92h:01m:50s remains)
INFO - root - 2019-11-03 23:39:14.379874: step 37140, total loss = 0.40, predict loss = 0.09 (81.3 examples/sec; 0.049 sec/batch; 81h:30m:39s remains)
INFO - root - 2019-11-03 23:39:14.975534: step 37150, total loss = 0.42, predict loss = 0.09 (84.0 examples/sec; 0.048 sec/batch; 78h:51m:11s remains)
INFO - root - 2019-11-03 23:39:15.608269: step 37160, total loss = 0.43, predict loss = 0.10 (68.3 examples/sec; 0.059 sec/batch; 96h:57m:33s remains)
INFO - root - 2019-11-03 23:39:16.264174: step 37170, total loss = 0.47, predict loss = 0.10 (64.1 examples/sec; 0.062 sec/batch; 103h:26m:25s remains)
INFO - root - 2019-11-03 23:39:16.917785: step 37180, total loss = 0.56, predict loss = 0.13 (66.9 examples/sec; 0.060 sec/batch; 99h:00m:14s remains)
INFO - root - 2019-11-03 23:39:17.552207: step 37190, total loss = 0.57, predict loss = 0.12 (69.4 examples/sec; 0.058 sec/batch; 95h:25m:04s remains)
INFO - root - 2019-11-03 23:39:18.196252: step 37200, total loss = 0.67, predict loss = 0.15 (72.6 examples/sec; 0.055 sec/batch; 91h:12m:50s remains)
INFO - root - 2019-11-03 23:39:18.850208: step 37210, total loss = 0.55, predict loss = 0.13 (65.6 examples/sec; 0.061 sec/batch; 100h:58m:46s remains)
INFO - root - 2019-11-03 23:39:19.484066: step 37220, total loss = 0.53, predict loss = 0.13 (67.9 examples/sec; 0.059 sec/batch; 97h:35m:36s remains)
INFO - root - 2019-11-03 23:39:20.146629: step 37230, total loss = 0.63, predict loss = 0.15 (61.7 examples/sec; 0.065 sec/batch; 107h:20m:31s remains)
INFO - root - 2019-11-03 23:39:20.750150: step 37240, total loss = 0.46, predict loss = 0.09 (68.4 examples/sec; 0.058 sec/batch; 96h:50m:19s remains)
INFO - root - 2019-11-03 23:39:21.360436: step 37250, total loss = 0.64, predict loss = 0.15 (76.6 examples/sec; 0.052 sec/batch; 86h:26m:14s remains)
INFO - root - 2019-11-03 23:39:22.016121: step 37260, total loss = 0.57, predict loss = 0.13 (60.3 examples/sec; 0.066 sec/batch; 109h:51m:38s remains)
INFO - root - 2019-11-03 23:39:22.659705: step 37270, total loss = 0.60, predict loss = 0.13 (72.0 examples/sec; 0.056 sec/batch; 92h:03m:02s remains)
INFO - root - 2019-11-03 23:39:23.264029: step 37280, total loss = 0.53, predict loss = 0.13 (72.2 examples/sec; 0.055 sec/batch; 91h:47m:49s remains)
INFO - root - 2019-11-03 23:39:23.890933: step 37290, total loss = 0.83, predict loss = 0.21 (67.6 examples/sec; 0.059 sec/batch; 97h:58m:14s remains)
INFO - root - 2019-11-03 23:39:24.530596: step 37300, total loss = 0.53, predict loss = 0.12 (71.8 examples/sec; 0.056 sec/batch; 92h:17m:07s remains)
INFO - root - 2019-11-03 23:39:25.173903: step 37310, total loss = 0.80, predict loss = 0.20 (66.6 examples/sec; 0.060 sec/batch; 99h:25m:00s remains)
INFO - root - 2019-11-03 23:39:25.808631: step 37320, total loss = 0.67, predict loss = 0.16 (74.6 examples/sec; 0.054 sec/batch; 88h:50m:43s remains)
INFO - root - 2019-11-03 23:39:26.448905: step 37330, total loss = 0.80, predict loss = 0.20 (64.3 examples/sec; 0.062 sec/batch; 103h:04m:21s remains)
INFO - root - 2019-11-03 23:39:27.083131: step 37340, total loss = 0.87, predict loss = 0.20 (66.5 examples/sec; 0.060 sec/batch; 99h:42m:04s remains)
INFO - root - 2019-11-03 23:39:27.712211: step 37350, total loss = 0.69, predict loss = 0.16 (61.6 examples/sec; 0.065 sec/batch; 107h:37m:57s remains)
INFO - root - 2019-11-03 23:39:28.366343: step 37360, total loss = 0.72, predict loss = 0.17 (66.9 examples/sec; 0.060 sec/batch; 99h:01m:59s remains)
INFO - root - 2019-11-03 23:39:29.046396: step 37370, total loss = 0.75, predict loss = 0.18 (67.9 examples/sec; 0.059 sec/batch; 97h:32m:18s remains)
INFO - root - 2019-11-03 23:39:29.663786: step 37380, total loss = 0.58, predict loss = 0.14 (65.9 examples/sec; 0.061 sec/batch; 100h:31m:12s remains)
INFO - root - 2019-11-03 23:39:30.273373: step 37390, total loss = 0.60, predict loss = 0.15 (73.3 examples/sec; 0.055 sec/batch; 90h:24m:14s remains)
INFO - root - 2019-11-03 23:39:30.877237: step 37400, total loss = 0.65, predict loss = 0.16 (72.1 examples/sec; 0.055 sec/batch; 91h:55m:03s remains)
INFO - root - 2019-11-03 23:39:31.482540: step 37410, total loss = 0.44, predict loss = 0.10 (67.9 examples/sec; 0.059 sec/batch; 97h:36m:14s remains)
INFO - root - 2019-11-03 23:39:32.097938: step 37420, total loss = 0.52, predict loss = 0.11 (79.7 examples/sec; 0.050 sec/batch; 83h:07m:20s remains)
INFO - root - 2019-11-03 23:39:32.778090: step 37430, total loss = 0.55, predict loss = 0.13 (72.4 examples/sec; 0.055 sec/batch; 91h:31m:22s remains)
INFO - root - 2019-11-03 23:39:33.463770: step 37440, total loss = 0.53, predict loss = 0.12 (64.6 examples/sec; 0.062 sec/batch; 102h:28m:51s remains)
INFO - root - 2019-11-03 23:39:34.108974: step 37450, total loss = 0.49, predict loss = 0.11 (63.8 examples/sec; 0.063 sec/batch; 103h:48m:19s remains)
INFO - root - 2019-11-03 23:39:34.732784: step 37460, total loss = 0.51, predict loss = 0.11 (67.9 examples/sec; 0.059 sec/batch; 97h:35m:13s remains)
INFO - root - 2019-11-03 23:39:35.336246: step 37470, total loss = 0.51, predict loss = 0.12 (71.4 examples/sec; 0.056 sec/batch; 92h:45m:11s remains)
INFO - root - 2019-11-03 23:39:35.965112: step 37480, total loss = 0.54, predict loss = 0.12 (68.4 examples/sec; 0.059 sec/batch; 96h:53m:57s remains)
INFO - root - 2019-11-03 23:39:36.572913: step 37490, total loss = 0.63, predict loss = 0.14 (64.6 examples/sec; 0.062 sec/batch; 102h:29m:43s remains)
INFO - root - 2019-11-03 23:39:37.234936: step 37500, total loss = 0.65, predict loss = 0.17 (69.1 examples/sec; 0.058 sec/batch; 95h:49m:25s remains)
INFO - root - 2019-11-03 23:39:37.885734: step 37510, total loss = 0.65, predict loss = 0.15 (72.2 examples/sec; 0.055 sec/batch; 91h:49m:18s remains)
INFO - root - 2019-11-03 23:39:38.502605: step 37520, total loss = 0.54, predict loss = 0.12 (65.1 examples/sec; 0.061 sec/batch; 101h:50m:14s remains)
INFO - root - 2019-11-03 23:39:39.108586: step 37530, total loss = 0.66, predict loss = 0.16 (70.3 examples/sec; 0.057 sec/batch; 94h:17m:52s remains)
INFO - root - 2019-11-03 23:39:39.760055: step 37540, total loss = 0.73, predict loss = 0.17 (65.2 examples/sec; 0.061 sec/batch; 101h:35m:02s remains)
INFO - root - 2019-11-03 23:39:40.374556: step 37550, total loss = 0.53, predict loss = 0.12 (81.7 examples/sec; 0.049 sec/batch; 81h:05m:25s remains)
INFO - root - 2019-11-03 23:39:41.001851: step 37560, total loss = 0.70, predict loss = 0.17 (69.3 examples/sec; 0.058 sec/batch; 95h:32m:58s remains)
INFO - root - 2019-11-03 23:39:41.610005: step 37570, total loss = 0.63, predict loss = 0.15 (77.4 examples/sec; 0.052 sec/batch; 85h:33m:03s remains)
INFO - root - 2019-11-03 23:39:42.235126: step 37580, total loss = 0.78, predict loss = 0.18 (81.0 examples/sec; 0.049 sec/batch; 81h:45m:22s remains)
INFO - root - 2019-11-03 23:39:42.816513: step 37590, total loss = 0.75, predict loss = 0.19 (71.8 examples/sec; 0.056 sec/batch; 92h:12m:35s remains)
INFO - root - 2019-11-03 23:39:43.439326: step 37600, total loss = 0.66, predict loss = 0.16 (63.4 examples/sec; 0.063 sec/batch; 104h:25m:42s remains)
INFO - root - 2019-11-03 23:39:44.071325: step 37610, total loss = 0.61, predict loss = 0.14 (71.1 examples/sec; 0.056 sec/batch; 93h:11m:47s remains)
INFO - root - 2019-11-03 23:39:44.781920: step 37620, total loss = 0.60, predict loss = 0.14 (56.3 examples/sec; 0.071 sec/batch; 117h:35m:29s remains)
INFO - root - 2019-11-03 23:39:45.440182: step 37630, total loss = 0.61, predict loss = 0.14 (70.7 examples/sec; 0.057 sec/batch; 93h:38m:15s remains)
INFO - root - 2019-11-03 23:39:46.068125: step 37640, total loss = 0.69, predict loss = 0.16 (72.8 examples/sec; 0.055 sec/batch; 91h:02m:41s remains)
INFO - root - 2019-11-03 23:39:46.686334: step 37650, total loss = 0.73, predict loss = 0.17 (73.8 examples/sec; 0.054 sec/batch; 89h:44m:39s remains)
INFO - root - 2019-11-03 23:39:47.313374: step 37660, total loss = 0.57, predict loss = 0.13 (82.0 examples/sec; 0.049 sec/batch; 80h:45m:36s remains)
INFO - root - 2019-11-03 23:39:47.899997: step 37670, total loss = 0.54, predict loss = 0.12 (78.2 examples/sec; 0.051 sec/batch; 84h:42m:53s remains)
INFO - root - 2019-11-03 23:39:48.503996: step 37680, total loss = 0.42, predict loss = 0.09 (74.5 examples/sec; 0.054 sec/batch; 88h:56m:00s remains)
INFO - root - 2019-11-03 23:39:49.126913: step 37690, total loss = 0.40, predict loss = 0.09 (77.1 examples/sec; 0.052 sec/batch; 85h:53m:17s remains)
INFO - root - 2019-11-03 23:39:49.801187: step 37700, total loss = 0.49, predict loss = 0.10 (62.4 examples/sec; 0.064 sec/batch; 106h:07m:20s remains)
INFO - root - 2019-11-03 23:39:50.488044: step 37710, total loss = 0.68, predict loss = 0.16 (66.6 examples/sec; 0.060 sec/batch; 99h:25m:16s remains)
INFO - root - 2019-11-03 23:39:51.144467: step 37720, total loss = 0.48, predict loss = 0.11 (67.5 examples/sec; 0.059 sec/batch; 98h:11m:31s remains)
INFO - root - 2019-11-03 23:39:51.797157: step 37730, total loss = 0.60, predict loss = 0.15 (63.6 examples/sec; 0.063 sec/batch; 104h:05m:49s remains)
INFO - root - 2019-11-03 23:39:52.487598: step 37740, total loss = 0.57, predict loss = 0.13 (70.5 examples/sec; 0.057 sec/batch; 93h:54m:37s remains)
INFO - root - 2019-11-03 23:39:53.096294: step 37750, total loss = 0.53, predict loss = 0.13 (66.4 examples/sec; 0.060 sec/batch; 99h:42m:21s remains)
INFO - root - 2019-11-03 23:39:53.694497: step 37760, total loss = 0.50, predict loss = 0.12 (76.9 examples/sec; 0.052 sec/batch; 86h:12m:06s remains)
INFO - root - 2019-11-03 23:39:54.327557: step 37770, total loss = 0.59, predict loss = 0.15 (72.4 examples/sec; 0.055 sec/batch; 91h:30m:45s remains)
